id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
4685f8b44d1aa5cd1854bb48b9e751431be896da	misual: music visualization based on acoustic data	visualization;moving average;similarity measure;music	This paper describes a music visualization method called Misual. The goal of this research to provide objective and static information regarding musical impressions that will enable people to efficiently categorize a huge number of music pieces and select favorites from them. Features are directly extracted from music and then visualized in an image that suits human intuition. As a first step in this research, volume transitions and repetitions are focued on as the visualized objects. For the volume transition, power information is extracted and then smoothed by a moving average. This information is continuously visualized with 3D-like images, in which radii of circles reflect the volume information. Repetitions are visualized with color. These are detected on the basis of the frame-to-frame similarity measured by the Mel-frequency cepstrum. These two kinds of information were visualized for three pieces of real music. The visualizations will be useful for roughly grasping the musical features in a very short time and imagining the music intuitively.	acoustic cryptanalysis;bookmark (world wide web);categorization;color;mel-frequency cepstrum;music visualization;smoothing	Naoko Kosugi	2010		10.1145/1967486.1967581	computer vision;speech recognition;computer science;multimedia	HCI	-34.05294953219527	-45.8442815102228	43108
582dd3ae0ff34c4dbfbb5acbb2c9ff4c6cf6ead4	the yale human grasping dataset: grasp, object, and task data in household and machine shop environments	grasping;multifingered hands;dexterous;manipulation	This paper presents a dataset of human grasping behavior in unstructured environments. Wide-angle head-mounted camera video was recorded from two housekeepers and two machinists during their regular work activities, and the grasp types, objects, and tasks were analyzed and coded by study staff. The full dataset contains 27.7 hours of tagged video and represents a wide range of manipulative behaviors spanning much of the typical human hand usage. We provide the original videos, a spreadsheet including the tagged grasp type, object, and task parameters, time information for each successive grasp, and video screenshots for each instance. Example code is provided for MATLAB and R, demonstrating how to load in the dataset and produce simple plots.	camera phone;file spanning;matlab;r language;screenshot;spreadsheet	Ian M. Bullock;Thomas Feix;Aaron M. Dollar	2015	I. J. Robotics Res.	10.1177/0278364914555720	computer vision;simulation;computer science;artificial intelligence	Robotics	-39.38634319722406	-39.25385253721215	43199
a8988ccd39203c2be2c4f151adf1a0aaebae1095	development of dual tactor capability for a soldier multisensory navigation and communication system	soldier performance;intuitive displays;haptic displays;multisensory displays;salience;tactile displays;soldier navigation	Development of new multisensory Soldier display systems requires context-driven evaluation of technology by expert users to assure generalizability to operations. The capture of Soldier performance demands is particularly challenging in this regard, as many factors converge to impact performance in actual usage. In this paper, we describe new capabilities for tactile communications that include an authoring system, use of android-driven displays for control and map-based information, and engineering tactors with differing salient characteristics. This allows development of a dual-tactor display that affords a larger variety of tactile patterns for communications, or TActions. These innovations are integrated in a prototype system. We used the system to present navigational signals to combat-experienced soldiers to guide development of tactile principles and the system itself. Feedback was positive for the concept, operational relevance, and for ease of interpretation.		Linda R. Elliott;Bruce J. P. Mortimer;Roger W. Cholewiak;Greg R. Mort;Gary A. Zets;Rodney Pittman	2013		10.1007/978-3-642-39215-3_6	computer vision;simulation;engineering;communication	Robotics	-45.46210053789302	-45.87475410514156	43203
859802f538bd816be8b71b410e27c0acf8df4b1e	visual interface system by character handwriting gestures in the air	computers;text analysis cameras gesture recognition handwritten character recognition light emitting diodes light pens stereo image processing;stereo method;multicamera system;handwriting recognition;image processing;light emitting diodes;text analysis;hand gesture;stereo method visual interface system character handwriting gesture air handwriting recognition japanese katakana character light emission diode pen tv camera led light trajectory direction code writing speed multicamera system 3d position;light pens;human interface;character handwriting gesture;direction code;trajectory;light emission diode pen;japanese katakana character;character recognition gesture recognition hand gesture human interface image processing;stereo image processing;3d position;mathematical model;led light trajectory;writing speed;visual interfaces;tv camera;character recognition;gesture recognition;air;cameras;handwritten character recognition;visual interface system;data models;cameras light emitting diodes trajectory mathematical model equations data models computers	A visual interface system that recognizes handwriting of Japanese katakana characters in the air has been developed. Characters written in a single stroke have both on-strokes and off-strokes. Thus, the shapes of the hand-gesture characters are different from the shapes of characters written on paper. It is difficult to trace the shapes of characters in air because the writer cannot see the trajectories. In this study, a light emission diode (LED) pen and a TV camera are used to capture the LED light trajectory, and the movements of the light are converted into direction codes correcting the slant of the handwriting character. The codes are normalized to 100 data items to eliminate the effect of writing speed. The 100 direction codes are compared with model data in which the direction codes of 46 Japanese characters are defined. Next, the system has expanded to a multi-camera system. Two of four cameras are selected and the 3-D positions of the gesture trajectories are calculated by the stereo method, and the position data are converted into front view data. In the experiments, we attained a recognition rate of 92.9% for the single-camera system. The multi-camera system has the advantage that it can recognize gestures regardless of the origin directions of the gestures. The system also has the ability to recognize the directions of the gesture commands with an accuracy of 9°.	code;diode;direction finding;experiment;the 100	Toshio Asano;Sachio Honda	2010	19th International Symposium in Robot and Human Interactive Communication	10.1109/ROMAN.2010.5598705	data modeling;computer vision;speech recognition;image processing;air;computer science;trajectory;professional video camera;mathematical model;gesture recognition;handwriting recognition;human interface device;computer graphics (images);light-emitting diode	Graphics	-38.40569768512285	-43.53914277033367	43290
3d6b11f97e38ab13803c3081894b7642da12de4b	understanding finger input above desktop devices	near surface;thickness;finger input;height;midair	Using the space above desktop input devices adds a rich new input channel to desktop interaction. Input in this elevated layer has been previously used to modify the granularity of a 2D slider, navigate layers of a 3D body scan above a multitouch table and access vertically stacked menus. However, designing these interactions is challenging because the lack of haptic and direct visual feedback easily leads to input errors. For bare finger input, the user's fingers needs to reliably enter and stay inside the interactive layer, and engagement techniques such as midair clicking have to be disambiguated from leaving the layer. These issues have been addressed for interactions in which users operate other devices in midair, but there is little guidance for the design of bare finger input in this space.  In this paper, we present the results of two user studies that inform the design of finger input above desktop devices. Our studies show that 2 cm is the minimum thickness of the above-surface volume that users can reliably remain within. We found that when accessing midair layers, users do not automatically move to the same height. To address this, we introduce a technique that dynamically determines the height at which the layer is placed, depending on the velocity profile of the user's initial finger movement into midair. Finally, we propose a technique that reliably distinguishes clicking from homing movements, based on the user's hand shape. We structure the presentation of our findings using Buxton's three-state input model, adding additional states and transitions for above-surface interactions.	desktop computer;haptic technology;input device;interaction;missile guidance;multi-touch;thickness (graph theory);three-state logic;usability testing;velocity (software development)	Chat Wacharamanotham;Kashyap Todi;Marty Pye;Jan O. Borchers	2014		10.1145/2556288.2557151	height;simulation;computer hardware	HCI	-45.789840129948594	-43.435884806498365	43425
eba197cd37a3ac038204f44a883d6f0fa004ff59	skinhaptics: ultrasound focused in the hand creates tactile sensations	human computer interaction haptic interfaces;interaction surface skinhaptics ultrasound tactile sensation on body interface;ultrasonic imaging skin acoustics receivers transmitters focusing tactile sensors	Recent developments in on-body interfaces have extended the interaction space of physical devices to the skin of our hands. While these interfaces can easily project graphical elements on the bare hand, they cannot give tactile feedback. Here we present a technology that could help to expand the output capability of on-body interfaces to provide tactile feedback without restricting the skin as an interaction surface. SkinHaptics works by focusing ultrasound in the hand using a phased array of ultrasound transmitters and the acoustic time-reversal signal processing technique. We present experimental results that show that this device can steer and focus ultrasound on the skin through the hand, which provides the basis for the envisioned technology. We then present results of a study that show that the focused energy can create sensations that are perceived under the skin and in the hand. We demonstrate the potential of SkinHaptics and discuss how our proof-of-concept device can be scaled beyond the prototype.	acoustic cryptanalysis;graphical user interface;phased array;prototype;skin (computing);time reversal signal processing;transmitter;under the skin	Daniel Spelmezan;Rafael Morales Gonzalez;Sriram Subramanian	2016	2016 IEEE Haptics Symposium (HAPTICS)	10.1109/HAPTICS.2016.7463162	computer vision;acoustics;engineering;communication;tactile sensor	HCI	-43.069335506640556	-41.5489331427867	43597
e5d694b1d1f2c65cacc1a267a898da6b872735d0	do you see what i see: towards a gaze-based surroundings query processing system	driving;map;query processing;gaze tracking;safety	A smart car can be defined in various dimensions. In this paper, we consider a gaze-based driver query processing system where the vehicle recognizes where the driver is looking at when she asks about roadside landmarks. By linking the tracked gaze of the driver with annotated maps and geo-location information, the system can let the vehicle provide immediate answers to the driver. We discuss the design and implementation of the system, along with the test results during real driving.	database;geolocation	Shinjae Kang;Byungjo Kim;Sangrok Han;Hyogon Kim	2015		10.1145/2799250.2799285	computer vision;simulation;geography;communication	DB	-39.209587375762595	-42.65411572336213	43966
c0ec690f4e5db6c0b350449e3e8dd54b7b521411	trends and vision of head mounted display in augmented reality	helmet mounted displays augmented reality computer displays;visualization;optical imaging;visualization head optical imaging cameras augmented reality educational institutions;head mounted display augmented reality mixed reality;computer displays;head;hmd head mounted display augmented reality mixed reality head mounted visual display head mounted multimodal display head mounted sensing technology mr ar;augmented reality;mixed reality;cameras;helmet mounted displays;head mounted display	This article introduces research trends and future visions of head mounted displays (HMDs) for mixed and augmented reality. Specifically, studies on head mounted visual displays, head mounted multi-modal displays, and head mounted sensing technologies for mixed and augmented reality are introduced, and challenges and visions are discussed for the realization of better MR/AR experience.	augmented reality;head-mounted display;modal logic	Kiyoshi Kiyokawa	2012	2012 International Symposium on Ubiquitous Virtual Reality	10.1109/ISUVR.2012.11	computer vision;augmented reality;simulation;visualization;computer science;optical head-mounted display;optical imaging;mixed reality;head;computer graphics (images)	Visualization	-40.96384387506013	-40.485719346555534	44203
0d37934e423485e3b8523f6eb024adc96133d636	serious games for rehabilitation of stroke patients with vibrotactile feedback		In this paper, we present a set of serious games with vibrotactile feedback developed for upper extremity motor function rehabilitation. These games are part of a framework called SIERRA that stands for Post-Stroke Interactive and Entertaining Rehabilitation with ReActive objects. The framework uses Augmented Reality technology to provide natural exercise environment and motivating virtual objects at the same time. It adopts serious games concept to provide patients with a more entertaining environment for treatments. We seamlessly superimpose virtual objects onto a real environment and patients can interact with them in a motivating game scenario. Since vibrotactile actuators are attached to the real objects, patient can experience haptic feedback as well as audiovisual feedback. Usability results show that the serious games with vibortactile feedback offers significant advantages both in terms of improving interests of the subjects for the game and in dealing with the realism of the games. SERIOUS GAME, STROKE REHABILITATION, VIBROTACTILE ACTUATORS, AUGMENTED REALITY	augmented reality;feedback;haptic technology;interactivity;usability	Atif Alamri;Heung-Nam Kim;Jongeun Cha;Abdulmotaleb El-Saddik	2010	Int. J. Comp. Sci. Sport		rehabilitation;physical therapy;physical medicine and rehabilitation;stroke;medicine	HCI	-42.073209547744966	-45.99028860034388	44716
27de5243f4a5f5d3faf49c0a3c2ac396bab92c90	evaluation of cognition of information which is stimulated by the sense of touch using phantom sensation and apparent movement	apparent movement;tactual sense navigation;phantom sensation;tactile motor		cognition;imaging phantom	Tota Mizuno;Hirotoshi Asano;Hideto Ide	2009	JRM	10.20965/jrm.2009.p0087	psychology;cognitive psychology;computer vision;communication	NLP	-45.044311769741405	-49.83276289545461	44832
152530d58f58a429318cb73f75b9e51c515a798b	measuring the difficulty of steering through corners	divided attention;user interface;fitts law;cognitive theory;movement time;menu navigation;steering law;gesturing	The steering law is intended to predict the performance of cursor manipulations in user interfaces, but the law has been verified for only a few path shapes and should be verified for more if it is to be generalized. This study extends the steering law to paths with corners. Two experiments compare the movement times of negotiating paths with corners to straight paths with the same width and movement amplitude. The experimental results show a significant effect on the movement times due to the corners, extending far into the legs of the path's corner. Modeling the results using resource theory, a cognitive theory for divided attention, suggests that steering through corners is two simultaneous tasks: steering along the legs of the corner and aiming at the corner.	cognitive science;cursor (databases);experiment;steering law;user interface	Robert Pastel	2006		10.1145/1124772.1124934	simulation;human–computer interaction;computer science;fitts's law;user interface;steering law	HCI	-45.55279814857852	-48.94763595778983	44994
f939c0a88285b9010f605d8be89e7e588e7a6cbf	hologram calculation for deep 3d scene from multi-view images	computer generated hologram;human vision;force represent;3d imaging;mechanical design;haptic displaye;satisfiability;three dimensional;human interface	Holography is a technique that records and reconstructs the wavefront using the interference and diffraction of light wave. Hologram can reproduce very realistic three-dimensional (3D) images that satisfy all depth cues in the 3D perception of human vision without any special observation devices. So the holography is expected as an ultimate technology for displaying 3D image. For the electronic display of holography, the hologram pattern is calculated from 3D data, using the technique called Computer Generated Hologram (CGH).	computer-generated holography;depth perception;display device;interference (communication);stereoscopy	Koki Wakunami;Masahiro Yamaguchi	2009		10.1145/1670252.1670312	stereoscopy;three-dimensional space;computer vision;computer science;operating system;human interface device;satisfiability;computer graphics (images)	Graphics	-41.309407800600106	-38.91156967044751	45169
19a899c94cceebbf0c43abafa8f611d942bb5717	food simulator	haptic;force sensor;food texture;human biting force;mechanical linkage;haptic interface;taste;food simulator;real food	The Food Simulator is a haptic interface that presents biting force. A taste of food arises from mixture of chemical, auditory, olfactory and haptic sensation. Haptic sensation while eating is a remained problem in taste display. The Food Simulator generates force to the user’s teeth to display food texture. The device is composed of four linkages. The mechanical configuration of the device is designed to fit to the mouth. A force sensor is attached to the end effecter. The Food Simulator generates force according to the force profile captured from a real food. The device is integrated with auditory and chemical display for multimodal sensation in taste.	experiment;haptic technology;modal logic;modality (human–computer interaction);multimodal interaction	Hiroo Iwata;Hiroaki Yano;Takahiro Uemura;Tetsuro Moriya	2003			embedded system;simulation;computer science	HCI	-41.316294525493795	-42.18218786828986	45350
6556d37225e4bc55af72b52cab2a90c8bf22658c	blindnavi: a navigation app for the visually impaired smartphone user	blind;micro location;navigation;accessibility;user experience;visually impaired;smartphone;user interfaces;orientation and mobility	"""These days, many of us frequently use mobile apps to help us navigate. However, these apps with touch screens are not user-friendly for visually impaired people who are eager to be able to leave their homes independently. Moreover, the most widely used apps are not specially made for the visually impaired, so they create much confusion and result in a problematic user experience. The main purpose of this research is to provide a new mobility-aid solution in the form of a navigation app that remembers meaningful information over the journey and makes the trip safer and smoother. Unlike those applications that provide visual guides, we want to refer to the way blind people recognize and remember their route, and provide multi-sensory messages combining familiar reference points that they have learned from O&M training. """"BlindNavi"""" is an app prototype with a 3-step simple search, flat flow design, voice feedback consisting of multi-sensory clues, combined with micro-location technology, to assist visually impaired people leave their homes and safely explore the outside world on their own."""	mobile app;prototype;smartphone;touchscreen;usability;user experience	Hsuan-Eng Chen;Yi-Ying Lin;Chien-Hsing Chen;I-Fang Wang	2015		10.1145/2702613.2726953	navigation;user experience design;simulation;human–computer interaction;computer science;accessibility;multimedia;user interface;world wide web	HCI	-46.57584298193083	-42.791899225753724	45441
cd29fe500a2cf83af1e3a1439275ca07dc2aef93	a practical evaluation of the influence of input devices on playability		Innovations being achieved with interactive devices (screens, sensors etc.) allow the development of new forms of interaction for many applications. Videogames played with these devices are completely changing how we use them and taking advantage of intuitive interfaces. Based on that, we ask “What aspects of playability are affected using different input devices for a certain gaming task and how is gaming performance affected?”. Our contribution is to present a practical evaluation of four different input devices (Mouse, Gamepad, Kinect and Touchscreen) used to interact with the same game, Fruit Ninja, with our data analysis indicating that changing input device brings significant differences in certain aspects of player experience for this game, such as sensation, challenge and control, while for others there was very little difference since this particular game rarely provides intense experiences for those aspects.		Lucas Machado;João Luiz Bernardes	2016		10.1007/978-3-319-39513-5_36	human–computer interaction;multimedia;computer science;input device;sensation;user experience design;touchscreen	HCI	-48.12622735453584	-45.38582802129259	45446
5a4973864bba6faadc49a54a482bde6627a38ede	a study on usability of human-robot interaction using a mobile computer and a human interface device	ubiquitous;personal computer;haptic device;mobile computer;human robot interaction;human interface;robot control	A variety of devices are used for robot control such as personal computers or other human interface devices, haptic devices, and so on. However, sometimes it is not easy to select a device which fits the specific character of varied kinds of robots while at the same time increasing the user's convenience. Under these circumstances, in this study, we have tried to measure user convenience. We tried to understand the characteristics of several devices used to achieve human robot interaction by using each of these devices that could be used with a personal computer: We used a button type device, a joystick, a driving device which consisted of a handle and pedals, and a motion-based human interface device including an acceleration sensor.	fits;haptic technology;human interface device;human–robot interaction;joystick;mobile computing;personal computer;robot control;usability	Tae Houn Song;Ji-Hwan Park;S. M. Chung;S. H. Hong;Key Ho Kwon;S. Lee;Jae Wook Jeon	2007		10.1145/1377999.1378055	human–robot interaction;embedded system;simulation;human–computer interaction;computer science;artificial intelligence;robot control;haptic technology;mobile computing;ubiquitous computing;personal robot;human interface device	HCI	-45.351130159097394	-43.8996440282844	45517
60f139e7afa67372169debb9b07c7449987bac8e	"""hyve-3d and rethinking the """"3d cursor"""": unfolding a natural interaction model for remote and local co-design in vr"""	gaze;alternative splicing;graphic novel;branching;attention;belief states;interactive;fixation;eye tracking	Hybrid Virtual Environment 3D (Hyve-3D) is a system to actively design inside Virtual Reality by a new model of interaction through a 3D cursor that is specially envisioned to facilitate local and remote collaboration. It introduces a novel approach to the concept of the cursor inside the 3D virtual space, rethinking it as a drawing and control plane. 3D cursors are intuitively manipulated by multi-touch handheld tablets that are tracked in 6DOF. The system also features a lightweight immersive 3D rendering technique that runs on a single laptop allowing texture and lighting effects on 3D geometry. Users can simultaneously access their individual complementary orthogonal views on the tablets, as personal windows into the shared display of the virtual environment; they can concurrently 3D sketch, select, edit, manipulate 3D objects using the tablets as tangible props, as well as collectively navigate the scene using the tablet as a 3D trackpad. With the notion of multiple 3D cursors the personal computer becomes a collaborative working environment. The first version was implemented in 2014 [Dorta et al. 2014]. A product level system is presented with the immersive and non immersive implementations. A variation of the interface which works without the need of an external tracker is also demostrated that opens the possibility of using the system with any handheld device such as smart phones and watches.	3d rendering;collaborative working environment;control plane;cursor (databases);handheld game console;laptop;microsoft windows;mobile device;multi-touch;personal computer;smartphone;smartwatch;tablet computer;touchpad;unfolding (dsp implementation);virtual reality	Tomás Dorta;Gokce Kinayoglu;Michael Hoffmann	2015		10.1145/2785585.2785586	fixation;computer vision;real-time computing;simulation;attention;eye tracking;branching;computer science;operating system;alternative splicing;multimedia;interactivity;computer graphics (images)	HCI	-45.64295358677561	-39.29667807255966	45565
1953f494651c1e756a5ea05acc019a9dd9ccd6fd	foot motion sensing: augmented game interface based on foot interaction for smartphone	smartphone game;interaktionsteknik;computer and information science;interaction technologies;augmented game;elektronik;media engineering;data och systemvetenskap;electronics;foot interaction;motion sensing;casual game;mediateknik	We designed and developmented two games: real-time augmented football game and augmented foot piano game to demonstrate a innovative interface based on foot motion sensing approach for smart phone. In the proposed novel interface, the computer vision based hybrid detection and tracking method provides a core support for foot interaction interface by accurately tracking the shoes. Based on the proposed interaction interface, wo demonstrations are developed, the applications employ augmented reality technology to render the game graphics and game status information on smart phones screen. The players interact with the game using foot interaction toward the rear camera, which triggers the interaction event. This interface supports basic foot motion sensing (i.e. direction of movement, velocity, rhythm).	augmented reality;computer vision;graphics;real-time transcription;shoes;smartphone;velocity (software development)	Zhihan Lv;Shengzhong Feng;Muhammad Sikandar Lal Khan;Shafiq ur Réhman;Haibo Li	2014		10.1145/2559206.2580096	electronics;simulation;human–computer interaction;multimedia	HCI	-44.20333077704439	-41.90081631589593	45847
2e4deee0497993b4ee987fd3f5556794d8cc56c5	conversational in-vehicle dialog systems: the past, present, and future	automotive engineering;sensors;intelligent vehicles smart devices speech recognition navigation data collection sensors automotive engineering natural languages vehicle safety;data collection;smart devices;navigation;intelligent vehicles;natural langauges;speech recognition;vehicle safety	Automotive technology rapidly advances with increasing connectivity and automation. These advancements aim to assist safe driving and improve user travel experience. Before the realization of a full automation, in-vehicle dialog systems may reduce the driver distraction from many services available through connectivity. Even when a full automation is realized, in-vehicle dialog systems still play a special role in assisting vehicle occupants to perform various tasks. On the other hand, in-vehicle use cases need to address very different user conditions, environments, and industry requirements than other uses. This makes the development of effective and efficient in-vehicle dialog systems challenging; it requires multidisciplinary expertise in automatic speech recognition, spoken language understanding, dialog management (DM), natural language generation, and application management, as well as field system and safety testing. In this article, we review research and development (R&amp;D) activities for in-vehicle dialog systems from both academic and industrial perspectives, examine findings, discuss key challenges, and share our visions for voice-enabled interaction and intelligent assistance for smart vehicles over the next decade.	application lifecycle management;dialog manager;dialog system;natural language generation;natural language understanding;requirement;speech recognition	Fuliang Weng;Pongtep Angkititrakul;Elizabeth Shriberg;Larry P. Heck;Stanley Peters;John H. L. Hansen	2016	IEEE Signal Processing Magazine	10.1109/MSP.2016.2599201	embedded system;navigation;simulation;speech recognition;computer science;sensor;electrical engineering;computer security;statistics;data collection	Robotics	-37.18158247777658	-41.86215303366812	45887
ae5858e9c4cb5a74438da94ad5e99de53e60787a	analysis of player motion in sport matches		The system for analysis of player motion during sport matches, developed at University of Ljubljana is presented. The system allows for non-intrusive measurement of positions of all players in indoor sports through whole match, using only inexpensive video equipment cameras mounted on the ceiling of the sports hall. Tracking process (obtaining trajectories from videos) is automatic and only supervised by operator, to initialize player positions at the beginning and correct the mistakes during the tracking. The software provides means for user friendly calibration of video data using court markings of each supported sport as reference coordinates. The system has been tested for European handball, basketball, squash and tennis. Manual annotations can be added, to complement the quantitative data. Software keeps synchronization between annotations and trajectory data and provides means to use custom annotation dictionaries. Due to calibration, the results are provided in court coordinates (meters, centimeters) and can be exported (synchronized with annotations in same file) for further analysis with any application (e.g. Microsoft Excel, SPSS). Software itself supports several types of graphical data presentation.	computer vision;dictionary;geographic coordinate system;indoor sports;spss;supervised learning;usability	Janez Pers;Matej Kristan;Matej Perse;Stanislav Kovacic	2008			operator (computer programming);ceiling (aeronautics);basketball;synchronization;software;computer graphics (images);trajectory;user friendly;computer science;annotation	Visualization	-39.27386770638013	-39.215347768699736	45957
437cbfcf5fef857579cab061788666abf7e4764e	toward intuitive 3d user interfaces for climbing, flying and stacking		In this paper, we propose 3D user interfaces (3DUI) that are adapted to specific Virtual Reality (VR) tasks: climbing a ladder using a puppet metaphor, piloting a drone thanks to a 3D virtual compass and stacking 3D objects with physics-based manipulation and time control. These metaphors have been designed to provide the user with an intuitive, playful and efficient way to perform each task.	3d computer graphics;3d user interaction;interaction technique;stacking;unmanned aerial vehicle;user interface;virtual reality	Antonin Bernardin;Guillaume Cortes;Rebecca Fribourg;Tiffany Luong;Florian Nouviale;Hakim Si-Mohammed	2018	2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)	10.1109/VR.2018.8446047	stacking;simulation;task analysis;computer science;virtual reality;compass;climbing;user interface	Visualization	-43.26903002909184	-38.54406748464628	46054
6ed3adaf23ac0a2f0dc10daf2a407b657b9f1ab1	selecting fiction in library catalogs: a gaze tracking study		It is studied how readers explore metadata in book pages when selecting fiction in a traditional and an enriched online catalog for fiction. The associations between attention devoted to metadata elements and selecting an interesting book were analyzed. Eye movements of 30 users selecting fiction for four search tasks were recorded. The results indicate that although participants paid most attention in book pages to content description and keywords, these had no bearing on selecting an interesting book. Author and title information received less attention, but were significant predictors of selection.	eye tracking	Janna Pöntinen;Pertti Vakkari	2013		10.1007/978-3-642-40501-3_8	computer vision;multimedia;world wide web	HCI	-35.64783652206219	-50.014046617041174	46156
39171b257d4d6dd5f8d97e2a483f796a39a677cd	third eye: a shopping assistant for the visually impaired	object recognition;pervasive computing;haptic gloves;asssistive technology;visual algorithms;navigation;visualization;visual augmentation;feature extraction;assistive technology;visual sensing;human augmentation;haptic interfaces;cameras;object detection	Through a combination of wearable cameras, hardware accelerators, and algorithms, a vision-based automatic shopping assistant allows users with limited or no sight to select products from grocery shelves.	algorithm;hardware acceleration;wearable computer	Peter A. Zientara;Sooyeon Lee;Gus H. Smith;Rorry Brenner;Laurent Itti;Mary Beth Rosson;John M. Carroll;Kevin M. Irick;Narayanan Vijaykrishnan	2017	Computer	10.1109/MC.2017.36	computer vision;navigation;simulation;visualization;feature extraction;computer science;cognitive neuroscience of visual object recognition;multimedia;ubiquitous computing	HCI	-40.1745634226555	-42.12817247389252	46760
9029a788a36faa19cbd1f324ad9d3602ac1855c6	human-machine conversations to support mission-oriented information provision	conversational interface;qa75 electronic computers computer science;mission oriented sensor networks;controlled natural language	Mission-oriented sensor networks present challenging problems in terms of human-machine collaboration. Human users need to task the network to help them achieve mission objectives, while humans (sometimes the same individuals) are also sources of mission-critical information. We propose a natural language-based conversational approach to supporting human-machine working in mission-oriented sensor networks. We present a model for human-machine and machine-machine interactions in a realistic mission context, and evaluate the model using an existing surveillance mission scenario. The model supports the flow of conversations from full natural language to a form of Controlled Natural Language (CNL) amenable to machine processing and automated reasoning, including high-level information fusion tasks. We introduce a mechanism for presenting the gist of verbose CNL expressions in a more convenient form for human users. We show how the conversational interactions supported by the model include requests for expansions and explanations of machine-processed information.	automated reasoning;computation;compute node linux;controlled natural language;gist;high- and low-level;interaction;mission critical	Alun D. Preece;Dave Braines;Diego Pizzocaro;Christos Parizas	2013		10.1145/2509338.2509342	natural language processing;simulation;computer science;communication	AI	-35.060713982653844	-41.576631896207864	46811
539f611490b193fbd5b0c76d1f46a5427b37efcf	collision avoidance interface for safe piloting of unmanned vehicles using a mobile device	uav;mobile;touchscreen;helicopters	Autonomous robots and vehicles can perform tasks that are unsafe or undesirable for humans to do themselves, such as investigate safety in nuclear reactors or assess structural damage to a building or bridge after an earthquake. In addition, improvements in autonomous modes of such vehicles are making it easier for minimally-trained individuals to operate the vehicles. As the autonomous capabilities advance, the user's role shifts from a direct teleoperator to a supervisory control role. Since the human operator is often better suited to make decisions in uncertain situations, it is important for the human operator to have awareness of the environment in which the vehicle is operating in order to prevent collisions and damage to the vehicle as well as the structures and people in the vicinity. In this paper, we present the Collision and Obstacle Detection and Alerting (CODA) display, a novel interface to enable safe piloting of a Micro Aerial Vehicle with a mobile device in real-world settings.	aerial photography;autonomous robot;human–computer interaction;mobile device;telerobotics;unmanned aerial vehicle	Erin Treacy Solovey;Kim Jackson;Mary L. Cummings	2012		10.1145/2380296.2380330	embedded system;simulation;computer science;operating system;mobile technology;computer security	Robotics	-46.57679165088033	-51.31161834554543	46902
bb88b6241b834948441ef297b0cd5f2f6df52276	user identification based on touch dynamics	touch dynamics;mice;user identification;human computer interaction;atmospheric measurements;standards;performance evaluation;mice atmospheric measurements particle measurements standards performance evaluation educational institutions timing;authorisation;particle measurements;biometrics;handheld device user identification biometrics touch interaction;human factors authorisation behavioural sciences haptic interfaces human computer interaction;human factors;touch interaction;vdp teknologi 500 informasjons og kommunikasjonsteknologi 550 datateknologi 551;behavioural sciences;handheld device;handheld touch based device security user identification touch dynamics touch interaction user touch behavior monitoring feature extraction left versus right hand dominance one handed versus bimanual operation stroke size stroke timing stroke symmetry stroke speed timing regularity automatic user interface customization;haptic interfaces;timing	Touch interaction has quickly become the de-facto means of interacting with handheld devices due to its perceived attractiveness and low hardware cost. This study proposes a strategy for identifying users based on touch dynamics. Users' touch behavior is monitored and several unique features are extracted including left versus right hand dominance, one-handed versus bimanual operation, stroke size, stroke timing, symmetry, stroke speed and timing regularity. An experiment involving 20 users reveals that the strategy is successful in identifying users and their traits according to the touch dynamics. The results can be used for automatic user interface customization. However, more research is needed before touch characteristics can be applied to increasing the security of handheld touch-based devices.	fax;handheld game console;interaction;mobile device;user interface	Frode Eika Sandnes;Xiaoli Zhang	2012	2012 9th International Conference on Ubiquitous Intelligence and Computing and 9th International Conference on Autonomic and Trusted Computing	10.1109/UIC-ATC.2012.45	simulation;human–computer interaction;behavioural sciences;computer science;mobile device;multimedia;authorization;biometrics	Robotics	-45.492681235418885	-45.256498624235235	47053
05f441ba80dba1b92dcc64594886f64c8c88029f	measurement of lens accommodation and convergence during the viewing of 3d images	convergent focus;character representation;dynamic character;theoretical virtual target;maximum distance;three-dimensional display technology;theoretical position;digital signage;virtual object;lens accommodation;age group	convergent focus;character representation;dynamic character;theoretical virtual target;maximum distance;three-dimensional display technology;theoretical position;digital signage;virtual object;lens accommodation;age group		Takumi Oohashi;Hiromu Ishio;Yuki Okada;Tomohiko Yanase;Takehito Kojima;Masaru Miyao	2013		10.1007/978-3-642-39191-0_35	computer vision;geography;communication;computer graphics (images)	Vision	-40.75417927823822	-39.965174938365145	47163
40d3b9249d50e60ef58ee439e96c04545c9d3ea4	quality of interaction experience in stereoscopic 3d tv	multidirectional tapping task interaction experience quality stereoscopic 3d tv quality of experience multimedia system high definition video content 3d video content technology pointing modality mouse based interaction virtual laser pointer hand gesture modality iso 9241 9 standard;human computer interaction;stereoscopic 3d video;three dimensional displays quality of service mice standards fatigue stereo image processing tv;quality of experience;iso 9241 9 quality of experience stereoscopic 3d video human computer interaction fitt s law;visual perception quality of experience stereo image processing three dimensional television;iso 9241 9;fitt s law	Quality of Experience has recently become a paramount research topic in multimedia systems, especially in the emerging areas of high-definition and 3D video content. There has been a significant amount of research focusing on 3D content production, compression and delivery. However very little research has been dedicated to the emerging challenges in assessing user experience when interacting with the 3D video content. Interaction tasks such as pointing and selection are critical to the consumer's experience of the 3D video technology. This paper studies the impact of pointing modalities on the quality of interaction experience with stereoscopic 3D television. The conducted user study compares and evaluates three pointing modalities: standard mouse-based interaction, virtual laser pointer and hand gesture modality, using the ISO 9241-9 standard for multi-directional tapping task. The results suggest that the virtual laser pointer modality can provide better quality of interaction experience than other modalities in terms of user performance and user satisfaction.	3d television;data compression;digital video;hdmi;interaction;modality (human–computer interaction);pointer (computer programming);stereoscopic video game;stereoscopy;usability testing;user experience	Haiyue Yuan;Janko Calic;Ahmet M. Kondoz	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025150	subjective video quality;computer vision;simulation;computer science;multimedia	HCI	-42.68815675078081	-47.1457639551521	47179
3fdeeed867be035413be4a0cc8823c3cf44c1132	usability analysis of 3d rotation techniques	virtual sphere;virtual manipulation;usability study;degree of freedom;user study;form factor;evaluation;3d input device;arcball;user acceptance;interactive 3d rotation;3d input devices	We report results from a formal user study of interactive 3D rotation using the mouse-driven Virtual Sphere and Arcball techniques, as well as multidimensional input techniques based on magnetic orientation sensors. Multidimensional input is often assumed to allow users to work quickly, but at the cost of precision, due to the instability of the hand moving in the open air. We show that, at least for the orientation matching task used in this experiment, users can take advantage of the integrated degrees of freedom provided by multidimensional input without necessarily sacrificing precision: using multidimensional input, users completed the experimental task up to 36% faster without any statistically detectable loss of accuracy. We also report detailed observations of common usability problems when first encountering the techniques. Our observations suggest some design issues for 3D input devices. For example, the physical form-factors of the 3D input device significantly influenced user acceptance of otherwise identical input sensors. The device should afford some tactile cues, so the user can feel its orientation without looking at it. In the absence of such cues, some test users were unsure of how to use the device.	input device;instability;sensor;usability testing	Ken Hinckley;Joe Tullio;Randy F. Pausch;Dennis Proffitt;Neal F. Kassell	1997		10.1145/263407.263408	simulation;form factor;human–computer interaction;computer science;evaluation;multimedia;degrees of freedom	HCI	-45.567188699409954	-46.43577328052982	47430
a9c68cdca1fe58cffde81673c0dd07e7fbada70c	haconiwa: a toolkit for introducing novice users to electronic circuits	modules electronic equipment manufacture;original miniature scenery haconiwa basic electronic circuit toolkit electronic devices basement modules object modules;light emitting diodes wires electronic circuits switches joining processes batteries connectors	"""This paper proposes """"Haconiwa,"""" a toolkit for learning how to build a basic electronic circuit intuitively. The toolkit is intended to motivate participants to develop an interest in electronic circuits. The toolkit aims at fostering the interest of participants in electronic devices and provides users with the pleasure involved in making products. The toolkit consists of two types of modules: basement modules and object modules (e.g., LED, battery, and switch). Users can customize the appearance of the object modules by decorating them with fancywork. By connecting the decorated modules with the basement modules, users can make their own original miniature scenery."""	electronic circuit	Saki Sakaguchi;Sayaka Shimada;Nanae Shirozu;Mitsunori Matsushita	2015	2015 IEEE 4th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2015.7398616	embedded system;simulation;engineering;engineering drawing	HCI	-42.55357675680439	-41.19045136909873	47701
8fc00cc73db678df783d145f5a61839abc9ded51	public space behavior modeling with video and sensor analytics		We present a review of technologies relevant to public space surveillance and describe a pilot study to explore the challenges. The general purpose of this study is to capture and analyze behavior patterns and anomalies of people behavior in a public space. On the capture side, we explore a small array of networked cameras as well as an ultrasonic sensor array for measuring the height of walking persons. After capture, video and ultrasound signals are analyzed and statistics calculated for such measurements, including the duration and speed of the trajectory of each tracked person, and a person's height which is a useful biometric feature for tracking the person across multiple, non-overlapping camera views. These statistics are first analyzed offline to determine the expected patterns of measured values over many captured events. Based on the expected patterns, anomalies can be detected as outliers in real time. Since this is a broad-based pilot study, conclusions relate to the effectiveness of the capture modalities and approaches investigated. We discuss how we use these findings to guide our future work. © 2012 Alcatel-Lucent.	behavior model;biometrics;image sensor;online and offline	Tin Kam Ho;Kim Matthews;Lawrence O'Gorman;Harald Steck	2012	Bell Labs Technical Journal	10.1002/bltj.20542	computer vision;simulation;multimedia	HCI	-37.222189865612066	-46.78702265345064	47884
156b857ad4321342e0750c6351c7706fb636ab7e	mobilepaperaccess: ubiquitous paper-based interfaces for mobile interactions	paper interaction;user interface;mobility;tangible user interface;input techniques;ubiquitous computing;finger tracking;wearable computing	MobilePaperAccess is a wearable camera-glasses system with tangible user interface allowing mobile interaction. We access the digital information from a paper interface extending the input space. The system is devised to validate our concepts of Environment Dependent Interface (EDI) and Environment Independent Interface (EII), which focus on enabling people to access their personal data as well as public resources at any time and in any place. In this paper, we propose a continuum from physical interface to digital interface in relation with EDI and EII, and we present design, implementation and evaluation aspects of our MobilePaperAccess system. We compare two interfaces (EDI and EII) and three input techniques (finger input, mask input and page input).	interaction	Yun Zhou;Bertrand David;René Chalon	2012		10.1016/j.procs.2012.06.038	embedded system;finger tracking;wearable computer;human–computer interaction;computer science;operating system;multimedia;natural user interface;user interface;ubiquitous computing	HCI	-46.308179165035185	-42.27066237286405	47907
177e3125c5ac40437928e3e29829660f528e3ab1	design and evaluation of virtual home objects with music interaction in smart homes	human computer interaction;smart home;virtual home object;rfid	Well-designed virtual home objects and human-computer interactions (HCIs) can provide convenient ways to easily make use of home services for home inhabitants in smart homes. This work tries to design the virtual home objects with the ability of playing the music and implement a particular HCI interface accomplishing the music interaction with virtual home objects for deploying virtual home music services in smart homes. Each virtual home object is connected to a list of songs. When a home inhabitant holds a virtual home object and moves it as operating gestures upon the implemented HCI interface, the virtual home music service will be activated and the smart home will automatically play the favorite songs for its inhabitant. In this way, a highly interactive home music service for home inhabitants can be realized through the convenient operating interactions between the virtual home objects and the implemented HCI interface in smart homes.		Jenq-Muh Hsu	2012	J. Intelligent Manufacturing	10.1007/s10845-010-0411-6	radio-frequency identification;simulation;computer science;engineering;multimedia	HCI	-46.94783594949117	-39.826353658896764	47913
0d73be7dfff2aa5d60c092933d8880121c2f9bad	textpursuits: using text for pursuits-based interaction and calibration on public displays	text;smooth pursuit;gaze interaction;public displays	In this paper we show how reading text on large display can be used to enable gaze interaction in public space. Our research is motivated by the fact that much of the content on public displays includes text. Hence, researchers and practitioners could greatly benefit from users being able to spontaneously interact as well as to implicitly calibrate an eye tracker while simply reading this text. In particular, we adapt Pursuits, a technique that correlates users' eye movements with moving on-screen targets. While prior work used abstract objects or dots as targets, we explore the use of Pursuits with text (read-and-pursue). Thereby we address the challenge that eye movements performed while reading interfere with the pursuit movements. Results from two user studies (N=37) show that Pursuits with text is feasible and can achieve similar accuracy as non text-based pursuit approaches. While calibration is less accurate, it integrates smoothly with reading and allows areas of the display the user is looking at to be identified.	eye tracking;smoothing;text-based (computing);usability testing	Mohamed Khamis;Ozan Saltuk;Alina Hang;Katharina Stolz;Andreas Bulling;Florian Alt	2016		10.1145/2971648.2971679	computer vision;smooth pursuit;multimedia	HCI	-45.66286260960532	-44.08371156779148	48169
36e6314603a6face44b340c808542fefb1cbd4bb	internet search using adaptive visualization	search engine;self organizing maps;search engines;information retrieval;clustering;intelligent searching;information representation;world wide web;interactive data exploration;self organized map;information seeking	Automatically created maps of concepts improve navigation in large collections of text documents. My research in progress on leveraging navigation by interactively providing the ability to modify the maps themselves has led me to believe that this functionality increases responsiveness to the user and makes searching more effective. I explored both what adaptive features users perceive to be most helpful and the overall effect of adaptation on achieving information seeking goals.	information seeking;interactivity;map;responsiveness	Dmitri Roussinov	1999		10.1145/632716.632760	computer science;data mining;world wide web;information retrieval;search engine	HCI	-33.986428152844844	-50.87905872464869	48281
3b0994388538189c8e88a8db3a0ad38a29491e6e	human - robot swarm interaction for entertainment : from animation display to gesture based control	robotics;multi agent;kinect;gesture based interaction;ipad;human swarm interaction;augmented reality;tablet;multi robot	This work shows experimental results with three systems that take real-time user input to direct a robot swarm formed by tens of small robots. These are: real-time drawing, gesture based interaction with an RGB-D sensor and control via a hand-held tablet computer.	human–robot interaction;mobile device;real-time clock;robot;swarm robotics;tablet computer	Javier Alonso-Mora;Roland Siegwart;Paul A. Beardsley	2014	2014 9th ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/2559636.2559645	computer vision;augmented reality;simulation;computer science;artificial intelligence;robot control;multimedia;natural user interface;robotics	Robotics	-38.15226189998249	-39.03105613956581	48322
374aa48e730a1bc19af8dafe4902875db7853783	careless touch: a comparative evaluation of mouse, pen, and touch input in shape tracing task	mouse;human computer interaction;manniska datorinteraktion interaktionsdesign;tracing;comparison;touch;shape;drawing;datavetenskap med inriktning mot manniska datorinteraktion;visual feedback;evaluation;work in progress;pen;method;freehand;computer science with specialization in human computer interaction	This short paper is a work-in-progress report on an experimental, exploratory comparison and evaluation of three input methods (mouse, pen, and touch-input) in a line-tracing task. A method to compare the original shape and user-generated version is presented. Measurements of user efficiency and accuracy showed that participants replicating a particular shape using touch-input performed the worst in terms of accuracy but were the fastest in comparison to the remaining input methods. No effect of controlled visual feedback was observed. Additionally, subjective operational biases were observed that, together with input method and expected shape related issues, might strongly affect the results.	fastest;input method;shape context;user-generated content	Stanislaw Zabramski	2011		10.1145/2071536.2071588	computer vision;method;simulation;tracing;human–computer interaction;shape;computer science;evaluation;drawing;computer graphics (images)	HCI	-44.851514435761004	-47.23946991587257	48364
99ed3ef0facccc6dc9146b253e57765ca1333108	"""""""i like to explore sometimes"""": adapting to dynamic user novelty preferences"""	diversity;dynamic user preferences;novelty;recommendation systems	Studies have shown that the recommendation of unseen, novel or serendipitous items is crucial for a satisfying and engaging user experience. As a result, recent developments in recommendation research have increasingly focused towards introducing novelty in user recommendation lists. While, existing solutions aim to find the right balance between the similarity and novelty of the recommended items, they largely ignore the user needs for novelty. In this paper, we show that there are large individual and temporal differences in the users' novelty preferences. We develop a regression model to predict these dynamic novelty preferences of users using features derived from their past interactions. Finally, we describe an adaptive recommender,~\emph{adaNov-R}, that adapts to the user needs for novel items and show that the model achieves better recommendation performance on a metric that considers both novel and familiar items.	interaction;user experience	Komal Kapoor;Vikas Kumar;Loren G. Terveen;Joseph A. Konstan;Paul R. Schrater	2015		10.1145/2792838.2800172	computer science;machine learning;data mining;multimedia;world wide web;recommender system	ML	-35.814460338283595	-51.2836344516028	48365
9f1e4af920fe8877cb2ad39631dfc92b7f050bff	evaluation of an indoor navigation approach based on approximate positions	indoor navigation;context based services indoor navigation mobile computing device whispering;prototypes;geometry;global position system;mobile computer;automatic generation;skeleton;device whispering;global positioning system;data structures;positional information;navigation system;skeleton prototypes geometry global positioning system data structures buildings;mobile computing;buildings;context based services	Until now navigation aids have primarily focused on outdoor scenarios, whether driving on highways or, more recently, walking through cities. These systems use the Global Positioning System (GPS) for position information. Indoor navigation however cannot rely on GPS data, as the signals do not penetrate building structure. Thus other techniques are required to provide position information indoors. In this article the approach to an indoor navigation system based on the position information provided by the Device Whispering technique is presented. The position information acquired by Device Whispering is less precise than information acquired by the Fingerprinting technique but more robust. To compensate the deficit of precision the position information is combined with a movement model. This movement model is automatically generated from the maps which are already required for navigation.	approximation algorithm;data structure;global positioning system;map;modulation;native-language identification;natural language generation;prototype;radio fingerprinting;wireless access point	Ory Chowaw-Liebman;Uta Christoph;Karl-Heinz Krempels;Christoph Terwelp	2010	2010 International Conference on Wireless Information Networks and Systems (WINSYS)		dead reckoning;turn-by-turn navigation;computer vision;gps/ins;simulation;global positioning system;computer science;operating system;prototype;mobile computing;skeleton;mobile robot navigation	Robotics	-38.833253614729124	-44.44720760083924	48367
7cd9f277f7ed1d41e2ea7d9b415a5bbaabb7311b	interactive representation of virtual object in hand-held box by finger-worn haptic display	haptic display;hand held box;haptic interfaces displays fingers shearing augmented reality force feedback thumb internal stresses acceleration testing;interactive devices augmented reality haptic interfaces;haptic i o;haptic device;interaction techniques;augmented reality system;virtual object;augmented reality haptic i o interaction techniques;finger worn haptic display;augmented reality;haptic interfaces;virtual ball;virtual ball interactive representation virtual object hand held box finger worn haptic display augmented reality system;interaction technique;physical simulation;interactive representation;interactive devices	To deliver a realistic presence of virtual objects with a simple haptic display in an augmented reality system, we have developed a wearable haptic display to present the sensation of weight and inertial force of the virtual objects. In this study, we developed an augmented reality application for our haptic device that represents the dynamics of the virtual objects inside a real box based on a physical simulation. We implemented the prototype system to represent a virtual ball in a real box, and evaluated the capability of our proposed method.	augmented reality;dynamical simulation;haptic technology;mobile device;prototype;wearable computer	Kouta Minamizawa;Souichiro Fukamachi;Naoki Kawakami;Susumu Tachi	2008	2008 Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems	10.1109/HAPTICS.2008.4479973	stereotaxy;computer vision;augmented reality;simulation;haptic technology;computer graphics (images)	Visualization	-42.66985079535872	-39.448603364400945	48520
af19264b3ffcf07066a40fdbfaa814b0ca34e8b4	see more: improving the usage of large display environments	tiled display;high resolution;large displays	Truly seamless tiled displays and stereoscopic large high-resolution displays are among the top research challenges in the area of large displays. In this paper we approach both topics by adding an additional projector to a tiled display scenario as well as to a stereoscopic environment. In both cases, we have developed new focus+context screen approaches: a multiple foci plus context metaphor in the tiled display setup and a 2D+3D focus+context metaphor in the stereoscopic scenario.		Achim Ebert;Hans Hagen;Torsten Bierz;Matthias Deller;Peter-Scott Olech;Daniel Steffen;Sebastian Thelen	2008		10.1007/978-3-211-99178-7_9	poggendorff illusion;computer vision;stereoscopy;metaphor;projector;artificial intelligence;virtual machine;computer science	HCI	-44.375874427917964	-40.488856800899704	48585
455f46b0fa5645a051538d9d6b4f9bbd9cf567ef	quickdraw: the impact of mobility and on-body placement on device access time	user mobility;mobile;wearables;mobile device;interface design;mobile phone;on the go;mobile phones;access time	We investigate the effect of placement and user mobility on the time required to access an on-body interface. In our study, a wrist-mounted system was significantly faster to access than a device stored in the pocket or mounted on the hip. In the latter two conditions, 78% of the time it took to access the device was spent retrieving the device from its holder. As mobile devices are beginning to include peripherals (for example, Bluetooth headsets and watches connected to a mobile phone stored in the pocket), these results may help guide interface designers with respect to distributing functions across the body between peripherals.	access time;bluetooth;mobile device;mobile phone;peripheral;quickdraw;usb on-the-go	Daniel Ashbrook;James Clawson;Kent Lyons;Thad Starner;Nirmal J. Patel	2008		10.1145/1357054.1357092	radio access network;embedded system;mobile identification number;mobile search;mobile web;wearable computer;human–computer interaction;computer hardware;access time;mobile database;computer science;interface design;operating system;mobile technology;mobile device;mobile station;mobile computing	HCI	-46.52961975362027	-42.88751716158952	48797
d4b029a8ee023bd3af7771d8c1bd31ac72c37ee3	text readability in head-worn displays: color and style optimization in video versus optical see-through devices	optimisation;video coding augmented reality data visualisation image colour analysis optimisation text analysis;text analysis;style guides;video coding;data visualisation;image colour analysis;vision i o;video see through;vision i o augmented reality optical see through style guides video see through;optical see through;augmented reality;color coding text readability color optimization style optimization video optical see through devices text visualization head worn augmented reality displays;augmented reality optical filters text processing computer vision	Efficient text visualization in head-worn augmented reality (AR) displays is critical because it is sensitive to display technology, text style and color, ambient illumination and so on. The main problem for the developer is to know the optimal text style for the specific display and for applications where color coding must be strictly followed because it is regulated by laws or internal practices. In this work, we experimented the effects on readability of two head-worn devices (optical and video see-through), two backgrounds (light and dark), five colors (white, black, red, green, and blue), and two text styles (plain text and billboarded text). Font type and size were kept constant. We measured the performance of 15 subjects by collecting about 5,000 measurements using a specific test application and followed by qualitative interviews. Readability turned out to be quicker on the optical see-through device. For the video see-through device, background affects readability only in case of text without billboard. Finally, our tests suggest that a good combination for indoor augmented reality applications, regardless of device and background, could be white text and blue billboard, while a mandatory color should be displayed as billboard with a white text message.	augmented reality;color;color balance;default;display device;head-mounted display;imagery;mandatory - hl7definedroseproperty;mega man x8;shadow volume;sprite (computer graphics);web site	Saverio Debernardis;Michele Fiorentino;Michele Gattullo;Giuseppe Monno;Antonio E. Uva	2014	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2013.86	computer vision;augmented reality;computer science;data mining;multimedia;data visualization;statistics;computer graphics (images)	Visualization	-43.18653535607865	-46.81322421467167	48929
70c1372e6cc334996ca868814083ee5ff482eb12	the reading desk: applying physical interactions to digital documents	digital documents;mobile device;user study;annotation;bookmarking;design and implementation;documents;note taking;physical interaction	Reading is increasingly being performed interactively on-screen; for instance, new novels are now routinely released in electronic format for viewing on PCs and mobile devices. Unfortunately, on-screen reading loses many of the natural features of conventional physical media, such as the ability to annotate, slip in bookmarks, turn page corners, and so on. How best should these features be represented electronically? Can computerized representations give benefits that excel the conventional benefits of paper? We describe the design and implementation of a novel reading system that mimics key properties of paper and surpasses them by incorporating digital techniques. A comparative user study evaluating the system confirmed the effectiveness of the features and the value of the system as a whole.	fundamental interaction;interactivity;mobile device;personal computer;screen reading;usability testing	Jennifer Pearson;George Buchanan;Harold W. Thimbleby	2011		10.1145/1978942.1979416	human–computer interaction;computer science;operating system;mobile device;multimedia;world wide web;bookmarking	HCI	-46.54495636171868	-41.787037537310574	49118
812c454c0b4ab6af7f20ac5268c4b6c706cb09a4	a browser-based multimodal interaction system	multimodal interaction system;web based system;multimodal interaction	In this paper, we propose a system that enables users to have multimodal interactions (MMI) with an anthropomorphic agent via a web browser. By using the system, a user can interact simply by accessing a web site from his/her web browser. A notable characteristic of the system is that the anthropomorphic agent is synthesized from a photograph of a real human face. This makes it possible to construct a web site whose owner's facial agent speaks with visitors to the site. This paper describes the structure of the system and provides a screen shot.	multimodal interaction;screenshot;web application	Kouichi Katsurada;Teruki Kirihata;Masashi Kudo;Junki Takada;Tsuneo Nitta	2008		10.1145/1452392.1452431	speech recognition;web-based simulation;human–computer interaction;computer science;multimodal interaction;web navigation;web page;multimedia;client-side scripting;world wide web	Web+IR	-48.043013145497035	-38.22616147387093	49250
197ff701f23c74037b9e6f4fc5322cd48cfa2ca8	wink it: investigating wink-based interactions for smartphones		Commodity mobile devices have front-facing cameras that can be used to precisely track facial expressions, such as winking, which can provide an additional input modality that co-exists with touchscreen input. We evaluate and compare three types of wink-based interactions: single wink, double wink, and long wink, in three mobile usage scenarios: sitting, walking, and lying down. Results show that single wink has similar error rate as touch input, and is preferred over touch input for targets in corner regions on a smartphone.	interaction;mobile device;modality (human–computer interaction);smartphone;touchscreen;wink (animated file)	Pin-Sung Ku;Te-Yen Wu;Ericka Andrea Valladares Bastias;Mike Y. Chen	2018		10.1145/3236112.3236133	computer hardware;wink;mobile device;word error rate;touchscreen;computer science	HCI	-45.740545804901146	-43.666381148346126	49629
26b82be43e67e9ff053a835f3925c873fdc271bd	automated lip synchronization for human-computer interaction and special effect animation	hidden markov models synchronisation computer animation real time systems speech recognition neural nets;automated special effect animation;human computer interaction;neural nets;hidden markov model;real time;computer graphical facial models;vocal soundtracks;facial modeling;animation system;speaker independent continuous speech;natural interface marti man machine animation real time interface automated lip synchronisation human computer interaction automated special effect animation engineering disciplines speech recognition facial modelling computer animation hybrid connectionist hidden markov model system phone recognition timing speaker independent continuous speech accurate facial models automated animation vocal soundtracks computer graphical facial models;automated animation;automated lip synchronisation;computer graphic;phone recognition;synchronisation;man machine animation real time interface;real world application;hidden markov models;speaker independent;marti;accurate facial models;speech recognition;computer animation;facial modelling;high performance;hybrid connectionist hidden markov model system;hidden markov models facial animation loudspeakers man machine systems human computer interaction speech recognition face recognition automatic speech recognition timing computer interfaces;natural interface;engineering disciplines;real time systems;timing	The research presents MARTI (Man-machine animation real-time interface) for the realisation of automated special effect animation and human computer interaction. The future developments of the Internet, video communications and multi-media, virtual reality, and animation will rely on the derivation of a natural humanmachine interface in order to submerse people, irrespective of technical know-how, into the latest technology, and allow them to interact with computers and one another using their own personality and idiosyncrasies. MARTI introduces novel research in a number of engineering fields to realise the first natural interface and animation system capable of high performance for real-users and real-world applications.	human computer;human–computer interaction;internet;real-time transcription;virtual reality	Christian Martyn Jones;Satnam Singh Dlay	1997		10.1109/MMCS.1997.609773	synchronization;computer vision;facial motion capture;speech recognition;computer facial animation;skeletal animation;computer science;interactive skeleton-driven simulation;machine learning;computer animation;hidden markov model	Visualization	-36.39121708395516	-43.677429969324706	49678
1c9912035fd9ccbff05a315624889c410edc79c6	extreme video retrieval: joint maximization of human and computer performance	human performance;real time;active learning;video retrieval;human performance optimization;human interface;relevance feedback;video search;image similarity	We present an efficient system for video search that maximizes the use of human bandwidth, while at the same time exploiting the machine's ability to learn in real-time from user selected relevant video clips. The system exploits the human capability for rapidly scanning imagery augmenting it with an active learning loop, which attempts to always present the most relevant material based on the current information. Two versions of the human interface were evaluated, one with variable page sizes and manual paging, the other with a fixed page size and automatic paging. Both require absolute attention and focus of the user for optimal performance. In either case, as users search and find relevant results, the system can invisibly re-rank its previous best guesses using a number of knowledge sources, such as image similarity, text similarity, and temporal proximity. Experimental evidence shows a significant improvement using the combined extremes of human and machine power over either approach alone.	bandwidth (signal processing);computer performance;expectation–maximization algorithm;page (computer memory);paging;real-time clock;user interface;video clip	Alexander G. Hauptmann;Wei-Hao Lin;Rong Yan;Jun Yang;Ming-yu Chen	2006		10.1145/1180639.1180721	human performance technology;computer vision;simulation;computer science;machine learning;pattern recognition;multimedia;active learning;world wide web;human interface device	ML	-38.107910958677714	-50.07742046416844	49850
a81571fbd6d77b82206663c76564ee29a6cabd22	the importance of choice design for low literate user experience		This research addresses a significant gap in our understanding of low literate behavior in online search. We explore how both lowand high-literate online consumers make decisions at the point of purchase in an online shopping task. We measured percentage fixation duration of AOIs during decision-making on four search engine results page (SERP) tasks. Qualitative and quantitative results combined suggest that tabular SERP may contribute to the success of low literate consumers making sophisticated trade-off decisions. Furthermore, we propose that tabular SERP choice design may improve low literate user experience for more general SERP choice design.	online search;online shopping;point of sale;search engine results page;table (information);user experience;web search engine	Lisa C. Harper;Melissa McMacken;Lianne Appelt;Kathryn Summers	2013		10.1007/978-3-642-39476-8_87	human–computer interaction;knowledge management;multimedia	HCI	-37.25111355918033	-51.93577512911974	49857
2a624422e1faba3c7fd87887bf5f79d0874b4344	wind tactor: an airflow-based wearable tactile display		Traditional wearable tactile displays transfer information through a firm contact between the tactile stimulator (tactor) and the skin. The firm contact, however, might limit the location of wearable tactile displays and might be the source of discomfort when the skin is being exposed to prolonged contact. This motivated us to find a non-contact wearable tactile display, which is able to transfer information without a contact. Based on the literature review, we concluded that we should focus on airflow-based tactile displays among various non-contact stimulation methods. In my previous work, I proposed the concept of a non-contact wearable tactile display using airflows and explored its feasibility. Focusing on an airflow-based wearable tactile display, I am investigating the expressivity and the feasibility of wearable airflow displays in real-world environments. I expect my dissertation will provide empirical grounds and guidelines for the design of an airflow-based wearable tactile display.	talking tactile tablet;wearable computer	Jaeyeon Lee	2017		10.1145/3131785.3131838	human–computer interaction;wearable computer;computer science;airflow;expressivity;computer vision;stimulation;artificial intelligence;tactile sensor	HCI	-46.8772897894575	-41.22033442170975	49943
2def47989c6f9143184b5eaaf3aca3f2833f3e05	learning from unscripted deictic gesture and language for human-robot interactions		As robots become more ubiquitous, it is increasingly important for untrained users to be able to interact with them intuitively. In this work, we investigate how people refer to objects in the world during relatively unstructured communication with robots. We collect a corpus of deictic interactions from users describing objects, which we use to train language and gesture models that allow our robot to determine what objects are being indicated. We introduce a temporal extension to stateof-the-art hierarchical matching pursuit features to support gesture understanding, and demonstrate that combining multiple communication modalities more effectively capture user intent than relying on a single type of input. Finally, we present initial interactions with a robot that uses the learned models to follow commands.	interaction;matching pursuit;robot	Cynthia Matuszek;Liefeng Bo;Luke S. Zettlemoyer;Dieter Fox	2014			computer vision;computer science;multimedia	AI	-34.573749183268326	-41.727630852541616	50272
cb78bb7066536a0b4899ae4965d5abd29953f2da	exploring interactions with a flexible tactile device for multi-context interaction	flexible interfaces;touch interaction;pressure;interaction design	The use of touch-based interaction now dominates the market as it enables a more natural form of interaction with our devices, but is largely limited to flat and rigid surfaces. The increasing availability of flexible interactive technologies offers us the opportunity to design devices that adapt to their context of use and opens up new avenues for the design of interaction on a flexible device for 'multi-context interaction'. In this paper, we present initial work in this direction through an explorative pilot study. Interestingly, the participants were rather influenced by standard tactile input gestures and envisioned using these for interaction while taking advantage of the flexible properties of the device for adaptation instead, either to the context or the object to which the device is attached (e.g. pressure input, positioning of the device and deformation).	interaction	Sabrina A. Panëels;Steven Strachan;Hanna Yousef;Sylvain Bouchigny	2016		10.1145/2851581.2892458	simulation;human–computer interaction;computer science;interaction design	HCI	-47.76227394504213	-41.45817026536951	50379
6769e38b646858dfbc35a9167854b4c25b34a0b0	surfaceconstellations: a modular hardware platform for ad-hoc reconfigurable cross-device workspaces		We contribute SurfaceConstellations, a modular hardware platform for linking multiple mobile devices to easily create novel cross-device workspace environments. Our platform combines the advantages of multi-monitor workspaces and multi-surface environments with the flexibility and extensibility of more recent cross-device setups. The SurfaceConstellations platform includes a comprehensive library of 3D-printed link modules to connect and arrange tablets into new workspaces, several strategies for designing setups, and a visual configuration tool for automatically generating link modules. We contribute a detailed design space of cross-device workspaces, a technique for capacitive links between tablets for automatic recognition of connected devices, designs of flexible joint connections, detailed explanations of the physical design of 3D printed brackets and support structures, and the design of a web-based tool for creating new SurfaceConstellation setups.		Nicolai Marquardt;Frederik Brudy;Can Liu;Ben Bengler;Christian Holz	2018		10.1145/3173574.3173928	computer hardware;capacitive sensing;workspace;physical design;mobile device;modular design;extensibility;computer science	HCI	-45.96862803923422	-38.29642634773644	50561
e0b3fae77290050bf7f6cd211d820a4e8b30cf44	a bidirectional haptic device for the training and assessment of handwriting capabilities	computer aided instruction;quantitative analysis bidirectional haptic device handwriting capability training handwriting capability assessment specific movement learning specific movement recovery writing skill improvement teacher haptic feedback;teaching computer aided instruction haptic interfaces;haptic interfaces writing vectors histograms kinematics robots force;haptic interfaces;teaching	Haptic devices have been used to help people to learn or recover specific movements. An interesting application is the handwriting. In this paper we present a haptic device designed to help people to improve their writing skills. Guided by a teacher, who controls one of the mechanisms providing the haptic feedback, a group of students were asked to write few words using their second hand. The test led to some interesting outcomes. A quantitative analysis is then carried out in order to support the considerations uniquely based on the qualitative observation of the results.	cyclic redundancy check;haptic technology;numerical analysis;operating system;optimal control;television	Nicolo Pedemonte;Thierry Laliberté;Clément Gosselin	2013	2013 World Haptics Conference (WHC)	10.1109/WHC.2013.6548476	education;computer vision;simulation;computer science;multimedia	HCI	-46.98702835013039	-50.0772041492748	50702
875d39c72e621aff21dc9aab9d8c3fda55ea2ecd	finding and emulating keyboard, mouse, and touch interactions and gestures while crawling rias	event handler analysis;ria;gesture emulation;crawling	Existing techniques for crawling Javascript-heavy Rich Internet Applications tend to ignore user interactions beyond mouse clicking, and therefore often fail to consider potential mouse, keyboard and touch interactions. We propose a new technique for automatically finding and exercising such interactions by analyzing and exercising event handlers registered in the DOM. A basic form of gesture emulation is employed to find states accessible via swiping and tapping. Testing the tool against 6 well-known gesture libraries and 5 actual RIAs, we find that the technique discovers many states and transitions resulting from such interactions, and could be useful for cases such as automatic test generation and error discovery, especially for mobile web applications.	emulator;interaction	Frederik Nakstad;Hironori Washizaki;Yoshiaki Fukazawa	2015	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194015710163	rich internet application;simulation;computer science;engineering;crawling;world wide web	SE	-47.91154967818852	-42.933074329325486	50715
adb277720fc05444c89b8fd72184894aeca65b06	"""""""boundary of illusion"""": an experiment of sensory integration with a pseudo-haptic system"""	springs haptic interfaces psychology computational modeling computer simulation imaging phantoms force feedback computer displays virtual environment process design;realistic behavior;haptic difference;point of subjective equality;visually oriented subjects sensory illusion boundary pseudo haptic feedback psychophysics virtual spring stiffness phantom force feedback device monoscopic computer screen realistic behavior visual displacement haptic displacement subjective equality point visual deformation compensation haptic difference visual dominance sensory integration strategy haptically oriented subjects;monoscopic computer screen;elastic constants haptic interfaces force feedback psychology visual perception human factors deformation;data collection;haptic displacement;phantom force feedback device;visually oriented subjects;psychology;visual deformation;virtual spring stiffness;process design;visual dominance;elastic constants;force feedback;imaging phantoms;springs;computational modeling;compensation;human factors;deformation;computer displays;sensory integration;haptic feedback;visual perception;subjective equality point;sensory illusion boundary;virtual environment;sensory integration strategy;haptic interfaces;computer simulation;pseudo haptic feedback;haptically oriented subjects;psychophysics;visual displacement	"""Describes a psychophysical experiment designed to study the phenomenon of illusion which occurs with pseudo-haptic feedback, and to identify the moment when this illusion occurs: the """"boundary of illusion"""". The subjects were given the task of deciding which of two virtual springs is the stiffer, these springs being simulated with a PHANToM/sup TM/ force feedback device and displayed on a monoscopic computer screen. The first spring had a realistic behavior, since its visual and haptic displacements were identical. The second spring-the pseudo-haptic one-was stiffer on a haptic basis, but sometimes less stiff on a visual basis. The data collected allowed us to calculate the visual point of subjective equality (PSE) between the two springs, which represents the boundary of the sensory illusion. On average, a high value of PSE turned out to be -24%. This value increased monotonically when the haptic difference between the springs increased. This implies that more visual deformation is necessary to compensate for large haptic differences and qualifies the notion of visual dominance. However, this boundary varies greatly depending on the subjects and their sensory integration strategy. The subjects were sensitive to this illusion to varying degrees. They were divided into different populations-from those who were """"haptically oriented"""" to those who were """"visually oriented""""."""	computer monitor;haptic technology;phantom reference;population	Anatole Lécuyer;Jean-Marie Burkhardt;Sabine Coquillart;Philippe Coiffet	2001	Proceedings IEEE Virtual Reality 2001	10.1109/VR.2001.913777	computer simulation;computer vision;simulation;computer science;artificial intelligence;haptic technology;psychophysics	Visualization	-43.79859243423077	-49.304627602870596	50905
de740087132230e9ccfe9898e5a5b83b09297c6a	a rapid screening and testing protocol for keyboard layout speed comparison	computers;layout keyboards testing protocols computers mobile handsets thumb;keypad experience level screening protocol testing protocol keyboard layout speed low overhead methodology typing speed standard layout comparator bias elimination familiarity finger memory mobile phone keypad layouts samsung phone variant common abc layout user specific layout pm training based method confidence interval test corpuses inherent conservatism user characteristics test group user age;protocols;keyboards;testing;layout;thumb;keyboards human factors;mobile handsets;layout comparison keypad keyboard	We propose an efficient low-overhead methodology for screening key layouts for ultimate typing speed. It is fast and complementary to existing protocols in other ways. For equal overhead, it allows testing over a wider range of users. It is subject to potential biases, but they can be quantified and adjusted. It assumes an existing standard layout which is used as a comparator. It eliminates bias from differing familiarity and finger memory by appropriately mapping both layouts. We illustrate it with two mobile phone keypad layouts: Samsung's variant of the common ABC layout, and a new user-specific layout (PM). We repeat a comparison previously undertaken by a training-based method, which used ten participants, and estimated a $54\%$ speedup for PM (SD = $35\%$). The new method used 116 participants and estimated a $19.5\%$ speedup (SD = $7.5\%$). Differences in speedup estimates can be explained by the wide confidence interval of the training-based methods, differences in test corpuses, and the inherent conservatism of the new method. Effects of user characteristics could be meaningfully tested due to the larger test group. Gender had no detectable effect on any of the measures, but age and keypad experience were significantly related to ABC and PM performance. However, the relative speedup was unaffected by age or keypad experience: the method can remove comparison biases arising from differing experience levels.	communications protocol;comparator;mobile phone;norm (social);overhead (computing);quantifier (logic);speedup;words per minute	Joonseok Lee;Hanggjun Cho;R. I. Bob McKay	2015	IEEE Transactions on Human-Machine Systems	10.1109/THMS.2014.2380641	layout;embedded system;communications protocol;simulation;computer hardware;computer science;operating system;software testing	Visualization	-47.61728046845381	-45.229498825423676	51080
03fb8e58efe1e27948740edf7b98adcef37c7560	elasticurves: exploiting stroke dynamics and inertia for the real-time neatening of sketched 2d curves	oscillations;fair curve design;real time;stroke based interfaces;natural variation;catching up;sketching	Elasticurves present a novel approach to neaten sketches in real-time, resulting in curves that combine smoothness with user-intended detail. Inspired by natural variations in stroke speed when drawing quickly or with precision, we exploit stroke dynamics to distinguish intentional fine detail from stroke noise. Combining inertia and stroke dynamics, elasticurves can be imagined as the trace of a pen attached to the user by an oscillation-free elastic band. Sketched quickly, the elasticurve spatially lags behind the stroke, smoothing over stroke detail, but catches up and matches the input stroke at slower speeds. Connectors, such as lines or circular-arcs link the evolving elasticurve to the next input point, growing the curve by a responsiveness fraction along the connector. Responsiveness is calibrated, to reflect drawing skill or device noise. Elasticurves are theoretically sound and robust to variations in stroke sampling. Practically, they neaten digital strokes in real-time while retaining the modeless and visceral feel of pen on paper.	fax;mode (computer interface);real-time clock;responsiveness;sampling (signal processing);smoothing	Yannick Thiel;Karan Singh;Ravin Balakrishnan	2011		10.1145/2047196.2047246	simulation;computer science;artificial intelligence;oscillation;computer graphics (images)	HCI	-38.70896033265901	-38.224355948358095	51093
f2c9e0080d9d2acd022b17731bd20d64c8b1546e	a multi-modal object attention system for a mobile robot	object recognition;mobile robot;robot companion object attention human robot interaction;man machine systems mobile robots object detection object recognition robot vision gesture recognition speech recognition;mobile robots;human robot interaction;mobile robots cognitive robotics object detection human robot interaction face detection layout information retrieval usability collaborative work humanoid robots;natural interaction;human robot interaction multimodal object attention system mobile robot robot companion object identification user gestures verbal instructions object detection object information;robot vision;speech recognition;man machine systems;gesture recognition;object detection;knowledge base	Robot companions are intended for operation in private homes with naive users. For this purpose, they need to be endowed with natural interaction capabilities. Additionally, such robots will need to be taught unknown objects that are present in private homes. We present a multi-modal object attention system that is able to identify objects referenced by the user with gestures and verbal instructions. The proposed system can detect known and unknown objects and stores newly acquired object information in a scene model for later retrieval. This way, the growing knowledge base of the robot companion improves the interaction quality as the robot can more easily focus its attention on objects it has been taught previously.	knowledge base;mobile robot;modal logic	Axel Haasch;Nils Hofemann;Jannik Fritsch;Gerhard Sagerer	2005	2005 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2005.1545191	mobile robot;robot learning;computer vision;knowledge base;simulation;computer science;artificial intelligence;social robot;gesture recognition;robot control;ubiquitous robot;mobile robot navigation;personal robot	Robotics	-36.41745103257994	-42.71711869756522	51303
1f6fbd4e4d0734f12e8558f9d36ae999e7735c87	an investigation of head motion and perceptual motion cues' influence on user depth perception of augmented reality neurosurgical simulators		Training and planning for neurosurgeries necessitate many requirements from junior neurosurgeons, including perceptual capacities. An effective method of deliberate training is to replicate the required procedures using neurosurgical simulation tools and visualizing a three-dimensional (3D) workspace. However, Augmented Reality (AR) neurosurgical simulators become obsolete for a variety of reasons, including usersu0027 distance underestimation. Few investigations have been conducted for improving usersu0027 depth perception in AR systems with perceptual motion cues through neurosurgical simulation tools for planning aid purposes. In this poster, we are reporting a user study about whether head motion and perceptual motion cues have any an influence on usersu0027 depth perception.	augmented reality;depth perception;effective method;requirement;self-replicating machine;simulation;usability testing;workspace	Hamza Ghandorh;Roy Eagleson;Sandrine de Ribaupierre	2018	2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)	10.1109/VR.2018.8446580	computer vision;workspace;simulation;task analysis;artificial intelligence;computer science;visualization;perception;effective method;augmented reality;depth perception	Visualization	-44.34615280617006	-47.96405283723761	51399
1f79e33a1ccde955da46e3ccfe17623b1fdb0bc2	performance evaluation of consumer decision support systems	electronic commerce;marketing data processing;performance evaluation;elicitation effort;electronic catalogs;e commerce;software performance evaluation;decision problem;decision support system;recommender system;extended effort accuracy framework;decision support systems;web sites;simulation environment;decision accuracy;multi attribute decision problem;evaluation framework	Consumer decision support systems (CDSSs) help online users make purchasing decisions in e-commerce Web sites. To more effectively compare the usefulness of the various functionalities and interface features of such systems, we have developed a simulation environment for decision tasks of any scale and structure. Furthermore, we have identified three criteria in an evaluation framework for assessing the quality of such CDSSs: users’ cognitive effort, preference expression effort, and decision accuracy. A set of experiments carried out in such simulation environments showed that most CDSSs employed in current e-commerce Web sites are suboptimal. On the other hand, a hybrid decision strategy based on four existing ones was found to be more effective. The interface improvements based on the new strategy correspond to some of the advanced tools already developed in the research field. This result is therefore consistent with our earlier work on evaluating CDSSs with real users. That is, some advanced tools do produce more accurate decisions while requiring a comparable amount of user effort. However, the simulation environment will enable us to efficiently compare more advanced tools among themselves, and indicate further opportunities for functionality and interface improvements.	decision support system;decision theory;e-commerce;experiment;parsing expression grammar;performance evaluation;purchasing;simulation	Jiyong Zhang;Pearl Pu	2006	IJEBR	10.4018/jebr.2006070103	e-commerce;decision support system;computer science;knowledge management;marketing;decision problem;data mining;world wide web	HCI	-37.805339915374525	-51.532654577662356	51581
8ed811ddfd0a1bdb125f4644695aebd82f8dd4d4	holo-haptics: haptic interaction with a see-through 3d display	tactile sensation holo haptics haptic interaction see through 3d display 3d virtual contents;virtual reality haptic interfaces three dimensional displays;virtual reality;haptic interfaces three dimensional displays stereo image processing thumb tracking indexes;three dimensional displays;haptic interfaces	In this paper, we present Holo-Haptics, which is an interactive haptic system to provide the illusion of touch that users are interacting directly and intuitively with 3D virtual contents. In order to provide haptic feedback, we use a thimble-formed pneumatic balloon display. It allows users to feel tactile sensation of contact, direction, and movement.	haptic technology;interaction;stereo display	Seungju Han;Joonah Park	2014	2014 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2014.6776110	stereotaxy;computer vision;computer science;virtual reality;multimedia;haptic technology;computer graphics (images)	Robotics	-42.50826345232196	-39.206334006316524	51618
a0d7f4e5ab66744646dc215c10b37ba4a04e7c59	predicting audio step feedback for real walking in virtual environments	step sound;gait;virtual reality;human walking;step prediction;auditory feedback	When navigating in virtual environments by using real walking, the correct auditory step feedback is usually ignored, although this could give more information to the user about the ground he is walking on. One reason for this are time constraints that hinder a replay of a walking sound synchronous to the haptic step feedback when walking. In order to add a matching step feedback to virtual environments, this paper introduces a calibration-free system which can predict the occurrence time of a stepdown event based on an analysis of the user’s gait. For detecting reliable characteristics of the gait, accelerometers and gyroscopes are used that are mounted on the user’s foot. Since the proposed system is capable of detecting the characteristic events in the foot’s swing phase, it allows a prediction that gives enough time to replay sound synchronous to the haptic sensation of walking. In order to find the best prediction regarding prediction time and accuracy, data gathered in an experiment is analyzed regarding reliably occurring characteristics in the human gait. Based on this, a suitable prediction algorithm is proposed.	algorithm;auditory processing disorder;feedback;haptic technology;sensor;virtual reality	Markus Zank;Thomas Nescher;Andreas M. Kunz	2015	Journal of Visualization and Computer Animation	10.1002/cav.1611	simulation;speech recognition;computer science;artificial intelligence;virtual reality;gait	Visualization	-46.22703712147099	-49.63691304757373	51623
15e84688cd89b71d437eb0cacd93d906f47ca515	input devices to support communication for people with serious disability	tactile sensors diseases handicapped aids image sensors interactive devices;input device;computer controlled;conferences mobile computing mobile communication ad hoc networks;infrared network system serious disability communication support intractable disease als amyotrophic lateral sclerosis spinal cord injury muscular function artificial respirators verbal communication touch sensor input devices capacitance sensor input devices vision input devices computer controlled environment;visio sensor;image sensors;conference paper;disability;communiction;handicapped aids;article authorversion;assistive;diseases;tactile sensors;communication;article;communication disability assistive communiction computer controlled visio sensor input device;interactive devices	Persons suffering from intractable disease like ALS (Amyotrophic Lateral Sclerosis) and spinal cord injury often show deterioration in muscular function. Some of these patients are obliged to use artificial respirators and cannot have verbal communication with others. In order to support their communication, there exist various input devices like touch sensor input devices, capacitance sensor input devices and also vision input devices. In addition, computer controlled environment is helpful for such patients. By developing proper computer controlled environment which is controlled by the proper input device, even the patient with serious disability can live sound life. Due to the infrared network system, the computer controlled environment could be well organized.	existential quantification;input device;lateral thinking	Chao Zhang;Shinichi Kumano;Kenichi Takashima;Motohiro Tanaka;Shunji Moromugi;Takakazu Ishimatsu	2013	2013 IEEE 9th International Conference on Mobile Ad-hoc and Sensor Networks	10.1109/MSN.2013.94	simulation;computer science;operating system;image sensor;tactile sensor;input device	Visualization	-41.242515771855395	-44.680592750336494	51676
177476e324fbdf4cbb685631db500dc7b9306b02	3d tracking in unknown environments using on-line keypoint learning for mobile augmented reality	computer graphics;degree of freedom;optical tracking augmented reality computer vision mobile computing;vision based tracking 3d tracking online keypoint learning mobile augmented reality natural feature tracking online boosting mobile computer;feature tracking;mobile computer;vision based tracking;planing;computer vision;online boosting;boosting;optical tracking;three dimensional displays;online keypoint learning;natural feature tracking;data visualization;mobile communication;3d tracking;augmented reality target tracking mobile computing boosting planing robustness data visualization computer graphics computer vision handheld computers;robustness;planning;augmented reality;target tracking;mobile computing;cameras;handheld computers;mobile augmented reality;tracking;on line learning;mobile user	In this paper we present a natural feature tracking algorithm based on on-line boosting used for localizing a mobile computer. Mobile augmented reality requires highly accurate and fast six degrees of freedom tracking in order to provide registered graphical overlays to a mobile user. With advances in mobile computer hardware, vision-based tracking approaches have the potential to provide efficient solutions that are non-invasive in contrast to the currently dominating marker-based approaches. We propose to use a tracking approach which can use in an unknown environment, i.e. the target has not be known beforehand. The core of the tracker is an on-line learning algorithm, which updates the tracker as new data becomes available. This is suitable in many mobile augmented reality applications. We demonstrate the applicability of our approach on tasks where the target objects are not known beforehand, i.e. interactive planing.	3d modeling;algorithm;augmented reality;bing maps platform;boosting (machine learning);computer hardware;computer vision;confluence;fundamental fysiks group;global positioning system;graphical user interface;graphics;microsoft windows;mobile computing;mobile operating system;model checking;motion estimation;online and offline;online machine learning;planar (computer graphics);planning;prototype;sensor;six degrees of separation;software deployment;virtual globe	Gerhard Schall;Helmut Grabner;Michael Grabner;Paul Wohlhart;Dieter Schmalstieg;Horst Bischof	2008	2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2008.4563134	planning;computer vision;augmented reality;simulation;computer science;multimedia;mobile computing	Vision	-39.82825198058511	-40.911580004212205	51913
d92be015a9f02c2efd17446675b97efed6e3af03	different strokes for different folks? revealing the physical characteristics of smartphone users from their swipe gestures	physical biometrics;touchscreen gestures;behavioural biometrics	Anthropometrics show that the lengths of many human body segments follow a common proportional relationship. To know the length of one body segment – such as a thumb – potentially provides a predictive route to other physical characteristics, such as overall standing height. In this study, we examined whether it is feasible that the length of a person's thumb could be revealed from the way in which they From a corpus of approx. 19,000 swipe gestures captured from 178 volunteers, we found that people with longer thumbs complete swipe gestures with shorter completion times, higher speeds and with higher accelerations than people with shorter thumbs. These differences were also observed to exist between our male and female volunteers, along with additional differences in the amount of touch pressure applied to the screen. Results are discussed in terms of linking behavioural and physical biometrics. & 2016 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).	anthropometry;approximation;biometrics;computer form factor;gesture recognition;human height;interaction;smartphone;text corpus;touchscreen	Chris Bevan;Danaë Stanton Fraser	2016	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2016.01.001	multimedia	HCI	-46.19365109184022	-45.088716384778984	51932
678738d4095301a6d731b38409a04e5d24e5f5cc	poster: evaluation of wayfinding aid techniques in multi-level virtual environments	topology;sign technique;electronic mail;virtual reality user interfaces;navigation in virtual environments;computer graphics;h 5 1 information interfaces and presentation multimedia information systems artificial;methodology and techniques;h 5 1 information interfaces and presentation multimedia information systemsâ artificial augmented and virtual realities;virtual reality;compass technique;testing;multimedia information system;multimedia systems;augmented;computer graphic;i 3 6 computer graphics methodology and techniques interaction techniques wayfinding aids techniques navigation in virtual environments virtual reality h 5 1 information interfaces and presentation multimedia information systems artificial augmented and virtual realities;navigation;wayfinding aid techniques;and virtual realities;compass technique wayfinding aid techniques multilevel virtual environments map technique landmark technique sign technique;virtual environment testing computer science navigation virtual reality floors multimedia systems computer graphics topology electronic mail;computer science;virtual environment;information interfaces and presentation;multilevel virtual environments;i 3 6 computer graphics methodology and techniquesâ interaction techniques;user interfaces;floors;interaction technique;landmark technique;i 3 6 computer graphics methodology and techniques interaction techniques;map technique;wayfinding aids techniques	This work deals with the evaluation of wayfinding aid techniques in multi-level virtual environments. The evaluation was accomplished through the development of two applications that implement four of the most commonly used wayfinding aid techniques (maps, compasses, landmarks and signs) and tested with two groups of users in two different experiments. The test results showed that compass is the technique which demanded less cognitive effort and that experience with games affects positively the users' performance.	experiment;map;virtual reality	Felipe Bacim;Andre Trombetta;Rafael Rieder;Márcio Sarroglia Pinho	2008	2008 IEEE Symposium on 3D User Interfaces	10.1109/3DUI.2008.4476608	human–computer interaction;computer science;multimedia;computer graphics (images)	SE	-44.166785532900086	-46.906825681265225	52282
f46b51936f31e987c578761553086ee087fd3708	acquario: a tangible spatially-aware tool for information interaction and visualization	information visualization;gestures;tangibles;spatial	"""Acquario is an interactive, spatially aware personal cubic display that can be used as an interaction tool for visualizations. Acquario uses the Pepper's ghost effect to transform a web-based visualization into a """"holographic"""" visualization that can be interacted with using gestures, proximity or custom laser printed tangible objects with embedded NFC tags. The aim of Acquario is to enable proximity, tangible and gestural interactions for designers for keyboard and mouse based interactions, allowing users an innovative and """"hands on"""" means."""	cubic function;embedded system;game controller;holography;interaction;near field communication;pepper's ghost;printing;web application	Sydney Pratte;Teddy Seyed;Frank Maurer	2016		10.1145/2983310.2989208	information visualization;human–computer interaction;engineering;multimedia;computer graphics (images)	HCI	-44.42919817033045	-39.558910308423776	52344
f5603ceaebe3caf6a812edef9c4b38def78cbf34	tailoring model-based techniques to facial expression interpretation	face recognition emotion recognition;human computer interaction;user interface;real time;humans image sequences hardware image databases application software face recognition facial features robustness knowledge based systems anthropometry;face and gesture recognition;emotion recognition;multi modal user interfaces;proof of concept;face recognition;face and gesture recognition emotion recognition multi modal user interfaces facial expression interpretation real time systems vision and scene understanding;human body;facial expression interpretation;image sequence;facial features;vision and scene understanding;experimental evaluation;facial expression;scene understanding;gesture recognition;real time systems;facial features model based techniques facial expression interpretation human computer interaction emotional state	Computers have been widely deployed to our daily lives, but human-computer interaction still lacks intuition. Researchers intend to resolve these shortcomings by augmenting traditional systems with human-like interaction capabilities. Knowledge about human emotion, behavior, and intention is necessary to construct convenient interaction mechanisms. Today, dedicated hardware often infers the emotional state from human body measures. Similar to humans interpreting facial expressions, our approach acquires video information using standard hardware that does not interfere with people to accomplish this task. It exploits model-based techniques that accurately localize facial features, seamlessly track them through image sequences, and finally interpret the visible information. We make use of state-of-the-art techniques and specifically adapt most of the components involved to this scenario, which provides high accuracy and real-time capability. We base our experimental evaluation on publicly available databases and compare its results to related approaches. Our proof-of-concept demonstrates the feasibility of our approach and shows promising for integration into various applications.	curve fitting;database;facial recognition system;human–computer interaction;information;loss function;mathematical optimization;microsoft outlook for mac;modal logic;optimization problem;real-time clock;user interface;world file	Matthias Wimmer;Christoph Mayer;Sylvia Pietzsch;Bernd Radig	2008	First International Conference on Advances in Computer-Human Interaction	10.1109/ACHI.2008.7	computer vision;human body;speech recognition;computer science;gesture recognition;user interface;proof of concept;facial expression	Robotics	-36.316235909419454	-43.05135060413282	52355
9b2fda13d22590695b2d015375d8aba2d69046c3	a comparative evaluation of mode switching techniques	azimuth;pressure mode switching;non preferred hand pen interfaces mode switching pressure tilt angle azimuth timeout;presses;non preferred hand;nonpreferred hand mode switching;timeout mode switching;error analysis;distance measurement;switches azimuth ink phase detection distributed processing application software mathematics computer science information systems systems engineering and theory;nonpreferred hand mode switching mode switching techniques tilt angle timeout mode switching barrel button mode switching pressure mode switching tilt mode switching azimuth mode switching multiple parameters pen;user interfaces interactive devices;analysis of variance;pen input;tilt mode switching;tilt angle;ink;mode switching;pen interfaces;switches;pressure;mode switching techniques;user interfaces;timeout;azimuth mode switching;multiple parameters pen;interactive devices;barrel button mode switching	In this study six different mode switching techniques (i.e. timeout mode switching, non-preferred hand mode switching, barrel button mode switching, pressure mode switching, tilt mode switching and azimuth mode switching) based on multiple parameters pen input are proposed. The results indicate that the techniques utilizing tilt angle and azimuth offer faster performance than the others.	fastest;usability;user interface	Chuanyi Liu;Xiangshi Ren;Dawei Li	2008	2008 IEEE International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2008.115	timeout;embedded system;optical burst switching;analysis of variance;computer hardware;network switch;computer science;operating system;azimuth;user interface	Embedded	-43.89145881223583	-46.04959839288843	52403
71bbf0fc9ee891190befbe216acaa8f4ca6c4b07	tele-embodied agent (tea) for video teleconferencing	human computer interaction;factory;manniska datorinteraktion interaktionsdesign;maintenance;communication systems;user centered design;user experience prototyping;kommunikationssystem;mobile and situated interface;evaluation	We propose a design of teleconference system which express nonverbal behavior (in our case head gesture) along with audio-video communication. Previous audio-video conferencing systems are abortive in presenting nonverbal behaviors which we, as human, usually use in face to face interaction. Recently, research in teleconferencing systems has expanded to include nonverbal cues of remote person in their distance communication. The accurate representation of non-verbal gestures for such systems is still challenging because they are dependent on hand-operated devices (like mouse or keyboard). Furthermore, they still lack in presenting accurate human gestures. We believe that incorporating embodied interaction in video teleconferencing, (i.e., using the physical world as a medium for interacting with digital technology) can result in nonverbal behavior representation. The experimental platform named Tele-Embodied Agent (TEA) is introduced which incorperates remote person's head gestures to study new paradigm of embodied interaction in video teleconferencing. Our preliminary test shows accuracy (with respect to pose angles) and efficiency (with respect to time) of our proposed design. TEA can be used in medical field, factories, offices, gaming industry, music industry and for training.	digital electronics;embodied agent;interaction;programming paradigm;tea;television	Muhammad Sikandar Lal Khan;Shafiq ur Réhman;Zhihan Lv;Haibo Li	2013		10.1145/2541831.2541876	user-centered design;simulation;human–computer interaction;computer science;factory;evaluation;operating system;multimedia;communications system	HCI	-47.38501036642418	-38.11734326072318	52587
3f7e87a47ee8d7d56cba93405b49b4001c024207	use of random dot pattern for achieving x-ray vision with stereoscopic augmented reality displays	x ray imaging;surface texture;observers;surface treatment;three dimensional displays;stereo image processing;augmented reality	This paper presents a possible solution to some of the challenges involved with creating a stereoscopic augmented reality ‘X-ray vision‘ display, which enables presentation of computer generated (virtual) objects as if they lie behind a real object surface, while maintaining the ability to effectively perceive any information that might be present on that surface. The method involves overlaying random dot patterns onto a real object surface prior to rendering a virtual object. Results from preliminary experiments have shown that the use of random dot patterns can be effective in contributing to the percept of transparency for the case of flat real surfaces with subtle textures. This suggests that the addition of such patterns may also help in perceiving the correct depth order of virtual objects in such images. Moreover, experimental results indicate that, by controlling the relative dot size and dot density of the patterns, it should be possible also to retain sufficient information about the real surface. Future research should be aimed towards the feasibility and effectiveness of applying this method to more realistic AR conditions.	ar (unix);augmented reality;dots per inch;experiment;stereoscopy	Sanaz Ghasemi;Mai Otsuki;Paul Milgram	2016	2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)	10.1109/ISMAR-Adjunct.2016.0086	surface finish;computer vision;augmented reality;simulation;computer science;computer graphics (images)	Visualization	-43.14500126573826	-48.606074511327826	52656
465e3ec5d536ed718e513b5fd824ba154731a82b	touch typing using thumbs: understanding the effect of mobility and hand posture	mobile;virtual keyboard;text entry;two handed interaction;text input;mobile text entry;walk;thumb;hand posture	Mobile touch devices have become increasingly popular, yet typing on virtual keyboards whilst walking is still an overwhelming task. In this paper we analyze; firstly, the negative effect of walking on text-input performance, particularly the users' main difficulties and error patterns. We focused our research on thumb typing, since this is a commonly used technique to interact with touch interfaces. Secondly, we analyze how these effects can be compensated by two-hand interaction and increasing target size. We asked 22 participants to input text under three mobility conditions (seated, slow walking, and normal walking) and three hand conditions (one-hand/portrait, two-hand/portrait, and two-hand/landscape). Results show that independently of hand condition, mobility significantly decreased input quality, leading to specific error patterns. Moreover, it was shown that target size can compensate the negative effect of walking, while two-hand interaction does not provide additional stability or input accuracy. We finish with implications for future designs.	poor posture;touch typing;virtual keyboard;while	Hugo Nicolau;Joaquim A. Jorge	2012		10.1145/2207676.2208661	simulation;computer science;operating system;mobile technology;multimedia	HCI	-46.73578210896878	-45.15476985982363	52659
137085baaca981a646984ce000e3646399c68122	low-cost gaze interaction: ready to deliver the promises	words per minute;pilot study;universal access;performance evaluation;gaze tracking;eye typing;gaze interaction;eye movement;error rate;eye tracking;target acquisition;low cost gaze tracking	Eye movements are the only means of communication for some severely disabled people. However, the high prices of commercial eye tracking systems limit the access to this technology. In this pilot study we compare the performance of a low-cost, webcam-based gaze tracker that we have developed with two commercial trackers in two different tasks: target acquisition and eye typing. From analyses on throughput, words per minute and error rates we conclude that a low-cost solution can be as efficient as expensive commercial systems.	eye tracking;throughput;tracking system;webcam;words per minute	Javier San Agustin;Henrik H. T. Skovsgaard;John Paulin Hansen;Dan Witzner Hansen	2009		10.1145/1520340.1520682	computer vision;simulation;universal design;eye tracking;word error rate;computer science;eye movement	HCI	-47.28964801436208	-45.12227388102655	52702
63d9d0b454b8ee56e201b8ff65a8105879d8d115	rethinking the progress bar	user interface;user perception;linear functionals;human centric;computer experiment;progress bar;peak and end;duration neglect;time perception	Progress bars are prevalent in modern user interfaces. Typically, a linear function is employed such that the progress of the bar is directly proportional to how much work has been completed. However, numerous factors cause progress bars to proceed at non-linear rates. Additionally, humans perceive time in a non-linear way. This paper explores the impact of various progress bar behaviors on user perception of process duration. The results are used to suggest several design considerations that can make progress bars appear faster and ultimately improve users' computing experience.	linear function;nonlinear system;user interface	Chris Harrison;Brian Amento;Stacey Kuznetsov;Robert Bell	2007		10.1145/1294211.1294231	simulation;computer experiment;human–computer interaction;computer science;artificial intelligence;operating system;user interface;time perception	HCI	-48.15690305555789	-50.25732972914394	52703
90895f2ca4fcc76bcf1549a0b2106c4d1590f6b5	machine learning of musical gestures		We present an overview of machine learning (ML) techniques and their application in interactive music and new digital instrument design. We first provide the non-specialist reader an introduction to two ML tasks, classification and regression, that are particularly relevant for gestural interaction. We then present a review of the literature in current NIME research that uses ML in musical gesture analysis and gestural sound control. We describe the ways in which machine learning is useful for creating expressive musical interaction, and in turn why live music performance presents a pertinent and challenging use case for machine learning.	dynamic music;gesture recognition;machine learning;new interfaces for musical expression;relevance	Baptiste Caramiaux;Atau Tanaka	2013			musical;multimedia;computer science;musical gesture;natural language processing;machine learning;gesture;artificial intelligence	HCI	-35.15592100289165	-45.88975655549136	52875
5bdac40aaa61073eb7c8a2b415a01682498d251f	autonomy in surgical robots and its meaningful human control				Fanny Ficuciello;Guglielmo Tamburrini;Alberto Arezzo;Luigi Villani;Bruno Siciliano	2019	Paladyn	10.1515/pjbr-2019-0002	simulation;computer science;robot;autonomy	Robotics	-35.80570388485008	-39.46357403488896	53063
3bbfe59598aa74129f450b717b7d57cf30ec915a	inspection mechanisms for community-based content discovery in microblogs		This paper presents a formative evaluation of an interface for inspecting microblog content. This novel interface introduces filters by communities, and network structure, as well as ranking of tweets. It aims to improving content discovery, while maintaining content relevance and sense of user control. Participants in the US and the UK interacted with the interface in semi-structured interviews. In two iterations of the same study (n=4, n=8), we found that the interface gave users a sense of control. Users asked for an active selection of communities, and a more fine-grained functionality for saving individual ‘favorite’ users. Users also highlighted unanticipated uses of the interface such as iteratively discovering new communities to follow, and organizing events. Informed by these studies, we propose improvements and a mock-up for an interface to be used for future larger scale experiments for exploring microblog content.	content discovery platform;experiment;iteration;mock object;organizing (structure);relevance;scientific visualization;semiconductor industry;tree traversal;user interface	Nava Tintarev;Byungkyu Kang;Tobias Höllerer;John O'Donovan	2015			microblogging;formative assessment;data mining;computer science;ranking;social media	HCI	-33.94362486472501	-49.96340483820225	53348
80163548ac42505f1616925fc0d2cc63f2836bba	qualitative and quantitive assessment of a virtual bowed string instrument		In this paper, we test the hypothesis proposed in a companion paper submitted to this conference (see [8]), namely that the playability of a bow stroke can be fully described by three parameters bow velocity, bow position and bow force. Moreover, the envelope of these parameters influences the quality of the attack. In this paper, we present a novel technique for measuring parameters of bowing. By coupling a haptic display to a computational model of a bowed string, we simulate the normal and frictional forces present during bow-string interaction.		M. Sile O'Modhrain;Stefania Serafin;Chris Chafe;Julius O. Smith	2000			computer science	HCI	-44.6038966584044	-50.63802188023715	53408
063ef8072e017c8fbb631a40735983b38f0a5b58	liveness detection using gaze collinearity	gaze;face liveness;face cameras face recognition security visualization feature extraction conferences;t technology;authorisation;biometrics;visualization;face recognition;feature extraction;face recognition authorisation computer animation;gaze face liveness biometrics;face;computer animation;security;cameras;conferences;gaze measurement gaze collinearity liveness detection method user gaze tracking face recognition system camera moving object visual animation display screen visual stimulus collinear point sets spoofing attacks	"""This paper presents a liveness detection method based on tracking the gaze of the user of a face recognition system using a single camera. The user is required to follow a visual animation of a moving object on a display screen while his/her gaze is measured. The visual stimulus is designed to direct the gaze of the user to sets of collinear points on the screen. Features based on the measured collinearity of the observed gaze are then used to discriminate between live attempts at responding to this challenge and those conducted by """"impostors"""" holding photographs and attempting to follow the stimulus. An initial set of experiments is reported that indicates the effectiveness of the proposed method in detecting this class of spoofing attacks."""	experiment;facial recognition system;liveness;sensor;spoofing attack	Asad Ali;Farzin Deravi;Sanaul Hoque	2012	2012 Third International Conference on Emerging Security Technologies	10.1109/EST.2012.12	computer vision;computer science;multimedia;communication	Robotics	-40.56463816508745	-51.30785697068241	53508
80ebf09f55f3402818d1c27c5dcb1e9a0a234147	smart m3-based robot interaction scenario for coalition work		The paper propose an interaction scenario for collaborative work of mobile robots for coalition creation and joint task solving. Scenario is considered as cyber-physical-social system that includes acting resources (mobile robots) that implements actions in physical space; information resources (robot control blocks, user mobile devices, computation services, etc.) that operate in informa- tion space; and social resources (users) that form tasks in social space. The following main operations have been identified for robot interaction scenario: mobile robot set forming that are ready to participate in scenario, coalition crea- tion, scenario operation. Scenario has been implemented based on Smart-M3 information sharing platform that provides possibilities for different resources to share information and knowledge in cyber space with each other based on RDF ontologies.	robot;smart tv	Alexander V. Smirnov;Alexey Kashevnik;Sergey Mikhailov;Mikhail Mironov;Mikhail Petrov	2016		10.1007/978-3-319-43955-6_24	simulation;engineering;knowledge management;scenario;computer security	Robotics	-34.469685576099934	-38.64316148308666	53534
3076c9386b39f41e948cbee47e24e64383114170	embedded auditory system for small mobile robots	digital signal processing;audio signal processing;mobile robot;real time;laptop computers;working environment noise;auditory system;mobile robots;array signal processing;laptop computer;human robot interaction;small mobile robot;auditory system mobile robots real time systems microphone arrays portable computers digital signal processing human robot interaction acoustic noise working environment noise array signal processing;embedded systems;digital signal processor audible embedded auditory system small mobile robot sound source localization sound tracking microphone array laptop computer signal processing technique;microphone array;portable computers;signal processing technique;signal processing;acoustic noise;sound source localization;digital signal processor;digital signal processing chips;sound tracking;microphone arrays;mobile robots audio signal processing digital signal processing chips embedded systems laptop computers microphone arrays;audible embedded auditory system;real time systems	Auditory capabilities would allow small robots interacting with people to act according to vocal cues. In our recent work, we have demonstrated AUDIBLE, an auditory system capable of sound source localization, tracking and separation in real-time, using an array of eight microphones and running on a laptop computer. The system is able to localize and track up to four sources, while separating up to three sources in real-time in noisy environments. Signal processing techniques can be quite computer intensive, and the question of making it possible for this system to run on platforms that cannot carry a laptop computer onboard can be raised. This paper reports our investigation of the appropriate compromises to be made to AUDIBLE's implementation in order to port the system on an embedded DSP (Digital Signal Processor) platform. The DSP implementation is fully functional and performs well with minor limitations compared to the original system i.e., limitations on sound source duration and on the number of sources that can be processed simultaneously. Results demonstrate that it is feasible to port AUDIBLE on embedded platforms, opening up its use in field applications such as human-robot interaction in real life settings.	algorithm;computer data storage;covox speech thing;digital signal processor;embedded system;field-programmable gate array;fixed-point arithmetic;human–robot interaction;internationalization and localization;laptop;microphone;mobile robot;performance;real life;real-time clock;real-time computing;real-time locating system;sampling (signal processing);signal processing	Simon Brière;Jean-Marc Valin;François Michaud;Dominic Létourneau	2008	2008 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2008.4543740	mobile robot;embedded system;electronic engineering;acoustics;computer science;engineering;artificial intelligence;signal processing	Robotics	-36.614576611917286	-43.05765808452664	53584
f1c0664af9eed603c41af2b8f5536d186572e134	preliminary environment mapping for redirected walking		Redirected walking applications allow a user to explore large virtual environments in a smaller physical space by employing so-called redirection techniques. To further improve the immersion of a virtual experience, path planner algorithms were developed which choose redirection techniques based on the current position and orientation of the user. In order to ensure a reliable performance, planning algorithms depend on accurate position tracking using an external tracking system. However, the disadvantage of such a tracking method is the time-consuming preparation of the physical environment which renders the system immobile. A possible solution to eliminate this dependency is to replace the external tracking system with a state-of-the-art inside-out tracker based on the concept of Simultaneous Localization and Mapping (SLAM). In this paper, we present an approach in which we attach a commercially available SLAM device to a head-mounted display to track the head motion of a user. From sensor recordings of the device, we construct a map of the surrounding environment for future processing in an existing path planner for redirected walking.		Christian Hirt;Markus Zank;Andreas M. Kunz	2018	2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)	10.1109/VR.2018.8446262	redirected walking;computer vision;artificial intelligence;computer science;cloud computing;reflection mapping;simultaneous localization and mapping;immersion (virtual reality);tracking system;virtual reality	Visualization	-39.617973213459045	-43.85671735837128	53626
39bcb3dd23abb4275ca16439041c5aafaffc4ed3	an emg-based mouse controller for a tetraplegic	emg based mouse controller;teeth clenching;user interfaces electromyography handicapped aids mouse controllers computers;user interfaces emg based mouse controller electromyogram tetraplegic teeth clenching;mouse controller;handicapped aids;operating system;tetraplegic;electromyography;mouse controllers computers;emg electromyogram;teeth clenching emg electromyogram tetraplegic mouse controller;electromyogram;user interfaces;control method;mice electromyography muscles control systems bioelectric phenomena pattern recognition prototypes neck signal processing keyboards	We propose an EMG-based mouse control method for helping the tetraplegic use computer by teeth-clenching. In this paper, we investigate useful actions and muscles as a means of the input method for the tetraplegic and a method of processing EMG signals for the purpose of controlling mouse control from the recognized patterns. Also, we developed a prototype system for acquiring and processing such EMGs based upon the specifications obtained from the study. Our experimental result shows that the proposed method shows promise of controlling the position of a mouse pointer and clicking mouse buttons in an operating system. In fact, our prototype system offers a great deal of convenience in wearing the prototype on a user's forehead. We verified our result by comparing the performance of the proposed system to that of a conventional mouse	electromyography;input method;mouse button;operating system;pointer (computer programming);pointer (user interface);prototype	Hyuk Jeong;Jong-Sung Kim;Wookho Son	2005	2005 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2005.1571314	simulation;computer science;user interface	Robotics	-41.56633971790207	-44.37318163721359	53852
1f99f7e35f20949fe8b59edf30d68f4e31c7cbc5	an architecture for rehabilitation task practice in socially assistive human-robot interaction	robot sensing systems;architectural design;sar architecture;pilot study;manipulators;social assistive human robot interaction;real time;patient rehabilitation;social assistive robotics;human robot interaction;servers robot sensing systems context manipulators monitoring books;books;medical robotics;feasibility study;servers;monitoring;rehabilitation task practice architecture;multiple task oriented interactions;robot agent;patient rehabilitation health care human robot interaction medical robotics;social assistance;context;robot agent rehabilitation task practice architecture social assistive human robot interaction health care social assistive robotics sar architecture multiple task oriented interactions;health care	New approaches to rehabilitation and health care have developed due to advances in technology and human robot interaction (HRI). Socially assistive robotics (SAR) is a subcategory of HRI that focuses on providing assistance through hands-off interactions. We have developed a SAR architecture that facilitates multiple task-oriented interactions between a user and a robot agent. The architecture accommodates a variety of inputs, tasks, and interaction modalities that are used to provide relevant, real-time feedback to the participant. We have implemented the architecture and validated its technological feasibility in a small pilot study in which a SAR agent led three post-stroke individuals through an exercise scenario. In the following, we present our architecture design, and the results of the feasibility study.	assistive technology;human–robot interaction;real-time transcription;robot;robotics	Ross Mead;Eric Wade;Pierre Johnson;Aaron St. Clair;Shuya Chen;Maja J. Mataric	2010	19th International Symposium in Robot and Human Interactive Communication	10.1109/ROMAN.2010.5598666	human–robot interaction;feasibility study;simulation;human–computer interaction;computer science;artificial intelligence;health care;server	Robotics	-34.42167417543163	-39.50052095390455	53874
ee87596d8e36199f4bbbdabf9318eeabf146b163	getting fast, free, and anonymous answers to questions asked by people with visual impairments		Much of the information we encounter in daily life is visual. For people with visual impairments, this information is not inherently accessible. Technology can be used to provide access to this visual information, through automated tools like currency identifiers, object recognizers, and optical character recognition, which allow for immediate access to visual information in a users' environment. However, these tools are often are limited to a specific domain, and can fail in non-ideal conditions (e.g., an optical character recognition tool trying to identify handwritten text).  By incorporating people into the information access pipeline, human-powered tools can allow for more complex information to be extracted from the user's surroundings. Human-powered access tools connect people who have disabilities to other people who can access information on their behalf. While people may not be as quick as automated tools, the person answering can make inferences based on prior knowledge and experiences, ask for clarification, and reason over the information provided.  In this article, we explore the long-term public deployment and lessons learned from VizWiz Social, a human-powered access tool that connects people with visual impairments to sighted workers or friends and family members who can answer their visual questions. By analyzing the use of this service, and running controlled experiments to determine how users value different answer sources, we have been able to build upon the original VizWiz design to create social microvolunteering, a method for getting fast, free, and anonymous answers to visual questions.	experience;experiment;finite-state machine;identifier;information access;optical character recognition;software deployment	Erin Brady	2015	ACM SIGACCESS	10.1145/2809915.2809918	computer science;data mining;internet privacy;world wide web	HCI	-36.87006577721317	-49.088443371324004	54000
c71f20bbdfc50667b9b6b127a20ed8101c95ca03	input devices & sensing technologies	input device;points;r tree;hierarchical spatial data structures;spatial databases;quadtrees;r tree image processing;rectangles;lines;octrees	"""In the """" future """" … Sensing technology can enable a wide variety of new interactions As hardware approaches """" free """" , we can afford a diversity of form factors d ll b d d l! Not every device will be used to do email! Devices can and should be pleasing to use, as well as useful."""	computer form factor;email;interaction	Andy Wilson	2007		10.1145/1281500.1281683	r-tree;discrete mathematics;computer science;theoretical computer science;operating system;line;database;input device;octree	HCI	-45.42521450067926	-40.63483979504601	54442
c61f4232438cf6f150dcd42f32aa34352a2196e0	shouldertap - pneumatic on-body cues to encode directions		A common way to display information via tactile output is to use vibration motors. However, vibration is often perceived as a rather artificial, even unpleasant cue. We explore a novel method based on pneumatic cues to provide a more natural tactile output. We use two airbags positioned at the back of the user, at shoulder height to give navigational cues. We utilize the shoulder tap metaphor to give directions to the left, right or ahead. We compare the pneumatic cue to vibro-tactile cue at the same position. Our Results show, that the pneumatic cue was rated as significantly less urgent than vibro-tactile cue. As there were no significant differences in error rate, annoyance and usability, we rate ShoulderTap as eligible alternative to vibro-tactile cues.	automation;cue tone;depth perception;encode;prototype;usability	Tim Claudius Stratmann;Shadan Sadeghian Borojeni;Wilko Heuten;Susanne C. J. Boll	2018		10.1145/3170427.3188624	annoyance;wearable computer;human–computer interaction;encode;less urgent;usability;word error rate;computer science	HCI	-46.51990204330869	-45.859643583482026	54484
b8070e7ad28e187acfaf739c82b410626fdfb517	performance comparisons between thumb-based and finger-based input on a small touch-screen under realistic variability		Performance Comparisons Between Thumb-Based and Finger-Based Input on a Small Touch-Screen Under Realistic Variability Inki Kim & Jang Hyeon Jo To cite this article: Inki Kim & Jang Hyeon Jo (2015) Performance Comparisons Between Thumb-Based and Finger-Based Input on a Small Touch-Screen Under Realistic Variability, International Journal of Human-Computer Interaction, 31:11, 746-760, DOI: 10.1080/10447318.2015.1045241 To link to this article: http://dx.doi.org/10.1080/10447318.2015.1045241	heart rate variability;human–computer interaction	Inki Kim;Jang Hyeon Jo	2015	Int. J. Hum. Comput. Interaction	10.1080/10447318.2015.1045241	real-time computing;simulation;multimedia	Arch	-47.20131831105988	-47.80855983599904	54548
ea1268b80e41ea459fbc1fb93912b103804d7804	applications of augmented reality for maintenance training	imperceptible projection blanking;segmentation;gestural interfaces;networked collaboration;mixed and augmented reality;augmented reality;categories and subject descriptors according to acm ccs i 4 9 image processing and computer vision applications	An Augmented Reality system that is designed for maintenance training is introduced. The proposed system adopts a projector to display the augmented video of the components to be repaired. With the augmented video, students can observe not only the real video of the components to be repaired, but also the additional information about the components such as contour, name and inner structure of the key parts etc. In order to accurately track the spatial relationship between the camera and the repaired components, several markers (small infrared LED) are attached with the components to be repaired, which ensure the dependability of the proposed system.	augmented reality;dependability;video projector	Dongdong Weng;Yongtian Wang;Yue Liu	2005		10.2312/EGVE/IPT_EGVE2005/219-220	computer vision;augmented reality;simulation;engineering;computer graphics (images)	HCI	-40.64416624305656	-38.135851360028035	54644
e50259d7ab9b92c7017bbeb9ec528244b2e804b8	remote control with switches on fingertips	glove input interface switches fingertips stand alone household remote control;remote control;user interfaces data gloves switches telecontrol;home appliances thumb resource management zigbee control systems prototypes;telecontrol;data gloves;switches;user interfaces	This paper describes a stand-alone household remote control. We previously proposed a glove input interface with switches on the fingertips, and added communication functions to the glove input interface. A prototype was prepared in order to evaluate the effectiveness of the proposed method.	input device;network switch;prototype;remote control	Shohei Mikami;Ryo Wada;Tomohiro Hase	2012	2012 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2012.6162029	embedded system;simulation;computer hardware;telecommunications;network switch;computer science;engineering;user interface;remote control	Robotics	-41.79669191153039	-43.52939540087877	54702
626dafd0517a7de8f9ccae46249396f16f6e1ed5	clickeraid: a tool for efficient clicking using intentional muscle contractions	human computer interaction;mouse clicks;tenosynovitis;eye tracker;layered windows;intentional muscle contractions	This is to propose a demo and poster about a tool designed to assist persons who are temporarily or permanently unable to reliably operate the buttons of a physical pointing device, for example because of tenosynovitis (TSV). It monitors a dedicated muscle of the user and emulates a click event at the current position of the mouse pointer in response to a contraction of that muscle (as small as raising the eyebrow). The ClickType (= type of the click) - left, right, single, double, drag - is selected by the user (who is also responsible for moving the mouse pointer) and stays valid until the selection of a new one.	emulator;pointer (computer programming);pointer (user interface);pointing device;through-silicon via	Torsten Felzer;Stephan Rinderknecht	2012		10.1145/2384916.2384981	simulation;human–computer interaction;eye tracking;computer science	HCI	-45.29539086497221	-42.634604333762255	54759
85aecede8a244357456e5316aa93f6294934989e	a process for anticipating and executing icon selection in graphical user interfaces	pointing device;user preferences;graphic user interface	This article presents a system for predicting the icon a user will select from an icon toolbar, based on command use frequency and mouse trajectory. The system differs from previous systems in two important ways: First, the prediction system does not initiate any action. Instead, it predicts where the mouse is moving and subtly “suggests” a command for the user to verify. Second, the system takes into account the relative likelihood of commands being used when making its predictions. Initial testing suggested that a system that only predicted the most frequently used icon choices was better than one that predicted all choices. A study with 12 test users using a mouse and 10 using a trackpad found substantial benefits of this “limited prediction system.” The system resulted in a mean reduction in time to issue a command of 41% for trackpad users and 25% for mouse users. Trackpad users, but not mouse users, preferred the prediction system to the traditional way of pointing. These results suggest that a prediction system such as the one described here has the potential to reduce the time and effort required to issue commands. The utility of the system appears to be especially great in laptops and other devices that use trackpads as their primary pointing devices.	computer mouse;graphical user interface;laptop;touchpad	David M. Lane;S. Camille Peres;Aniko Sandor;H. Albert Napier	2005	Int. J. Hum. Comput. Interaction	10.1207/s15327590ijhc1902_5	simulation;human–computer interaction;computer science;graphical user interface;world wide web;pointing device	HCI	-46.49250231392376	-45.949640788367084	54766
5579706ebb26a7d8ec2398dc45cfd81420244a99	fuzzy command grammars for intelligent interface design	langage commande;interfase usuario;langage entree;command language;intelligent interfaces;human computer interaction;fuzzy set;conception interface intelligente;user interface;ensemble flou;fuzzy set theory;robot kinematics manipulators command languages jacobian matrices equations robotics and automation motion control humans prosthetics man machine systems;intelligent interface design;fuzzy command grammar user interface intelligent interface fuzzy set theory;context sensitive grammars;system design;error correction;interaction homme machine;user interfaces context sensitive grammars fuzzy set theory;interface utilisateur;context dependent;conjunto borroso;user interfaces;input language;human computer interface	Human-computer interactions involve an input language with which users input their commands and a command language that is anticipated and recognized by the system. In general, the input language is a function of the command language, while the command language can be any language defined by the system designer. The relationship between these languages is modeled using fuzzy set theory, and a fuzzy command grammar that forms a coherent framework for intelligent human-computer interface designs is defined. To test the validity of these hypotheses, an interface was designed, using fuzzy command grammars, that is consistent, context-dependent, user-sensitive, adaptive, and responsive in interactions; and provides user and system initiated help; and that reduces unnecessary interruptions in dialogues by error correction. Experimental results indicate that a fuzzy command grammar is a viable alternative for designing such interfaces. >		Hikmet Senay	1992	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.179849	natural language processing;computer science;artificial intelligence;theoretical computer science;machine learning;fuzzy set;programming language;user interface;fuzzy control language	Robotics	-39.157656931168596	-49.0524733034279	55116
7dfe9bd5b9a058c7e011fd884d85f2d70da148ca	catching fireflies: a persuasive augmented reality game for android phones	persuasive software;operating system;persuasive technology;augmented reality;mobile application	This poster explains the development of the unique game Catching Fireflies and how it is an example of persuasive technology used to draw people outdoors. Catching Fireflies is a game for Android operating systems which uses multiple aspects of the phone including the camera, GPS, accelerometers, orientation and proximity sensors, vibration, and camera LEDs, among others.	android;augmented reality;global positioning system;operating system;persuasive technology;sensor	Amy Eubanks	2011		10.1145/2016039.2016149	embedded system;simulation;engineering;multimedia	HCI	-44.40297332457743	-41.97843300854599	55266
aaeb6ed2acd4abba38d46a15ba5c052d2f92beb4	comparing direct off-screen pointing, peephole, and flick & pinch interaction for map navigation	direct off screen pointing;spatial interaction;peephole displays	Navigating large workspaces with mobile devices often require users to access information that spatially lies beyond it's viewport. To browse information on such workspaces, two prominent spatially-aware navigation techniques, peephole, and direct off-screen pointing, have been proposed as alternatives to the standard on-screen flick and pinch gestures. Previous studies have shown that both techniques can outperform on-screen gestures in various user tasks, but no prior study has compared the three techniques in a map-based analytic task. In this paper, we examine these two spatially-aware techniques and compare their efficiency to on-screen gestures in a map navigation and exploration scenario. Our study demonstrates that peephole and direct off-screen pointing allows for 30% faster navigation times between workspace locations and that on-screen flick and pinch is superior for accurate retrieval of workspace content.	browsing;mobile device;peephole optimization;viewport;workspace	Khalad Hasan;David Ahlström;Pourang Irani	2015		10.1145/2788940.2788957	computer vision;simulation;engineering;computer graphics (images)	HCI	-45.330543679357916	-45.867624001849435	55293
bf537ac9887dfe5b36b519dd19930da96306cef3	mobile augmented reality in the museum: can a lace-like technology take you closer to works of art?	painting;mobile guide mobile augmented reality museum lace like technology markerless augmented reality sensitive activity analytical activity;art;instruments;ar mobile guide;museums augmented reality mobile computing mobile handsets;museums;visualization;activity of visiting a museum ar mobile guide;art painting augmented reality instruments mobile communication videos visualization;mobile guide;activity of visiting a museum;mobile communication;mobile handsets;augmented reality;mobile computing;mobile augmented reality;videos	This paper describes the results of experiments conducted during museum visits with a mobile guide using markerless augmented reality. It aims to study how such a tool can transform the activity of visiting a museum. First, we present a model of the activity of visiting a museum which has emerged both from the state of the art and also from a number of observations made by the author. This model is comprised of two types of activity experienced by the observer of an artwork: sensitive activity and analytical activity. We then explain the methodology of the experiments and the results. We identify three schemes of interaction between the visitor and the mobile guide, and we describe these schemes and their impact on the activity of visiting a museum. Finally this model allows us to determine future directions for the design of mobile augmented reality tools for visiting museums.	augmented reality;experiment	Anne Bationo Tillon;Isabelle Marchal;Pascal Houlier	2011	2011 IEEE International Symposium on Mixed and Augmented Reality - Arts, Media, and Humanities	10.1109/ISMAR-AMH.2011.6093655	augmented reality;simulation;visualization;painting;computer science;operating system;multimedia;mobile computing;computer graphics (images)	Visualization	-44.10184416914261	-41.43690864859347	55306
5374faaa78e9df79162b6b52d8ba75a2569f41b3	timing and accuracy of individuals with and without motor control disabilities completing a touch screen task	input device;touch screen;healthy subjects;motor control	As touch screen technology improves in functionality and decreases in price, these input devices are becoming much more prevalent. People are increasingly required to interact with touch screens at places ranging from their local grocery stores to airport check-in kiosks. Since it is becoming necessary for people to use touch screens in order to access needed products or services, we conducted an experiment to examine how individuals with varying motor control disabilities perform on a simple number entry task. We feel this research is important because, to date, most of the usability research related to touch screens has only included young, healthy subjects.		Curtis B. Irwin;Mary E. Sesto	2009		10.1007/978-3-642-02710-9_59	simulation;engineering;multimedia;communication	HCI	-47.175571656101866	-45.39690434007249	55322
4b242b3349fee60035c15d19ab21d8daa9441741	three-dimensional computer display	interactive stereographic terminal;computer graphics;three dimensional;computer graphic;swept raster display;color separation;video map;three dimensional display	A stereographic display terminal has been produced using the raster display (BRAD) recently developed at Brookhaven. The system uses a rotating refresh memory to feed standard television monitors. To produce a stereographic display the computer calculates the projected video images of an object, viewed from two separated points. The resulting video maps are stored on separate refresh bands of the rotating memory. The two output signals are connected to separate color guns of a color television monitor, thus creating a superimposed image on the screen. Optical separation is achieved by viewing the image through color filters. The display is interactive and can be viewed by a large group of people at the same time.	color television;computer monitor;display device;display resolution;map;raster graphics;stereoscopy	D. Ophir;Robert J. Spinrad	1969	Commun. ACM	10.1145/363011.363027	three-dimensional space;computer vision;computer display standard;computer science;multimedia;virtual retinal display;computer graphics;display device;computer graphics (images)	Graphics	-40.792527972364994	-39.272432892692414	55449
26d5567d03fcdd50d8c04fad7695db43dd69762c	a comparison of two wearable tactile interfaces with a complementary display in two orientations	pedestrian navigation;haptic i o;touch screen;user interface;wearable computers;line drawings;evaluation methodology;tactile display;tactile interface;user interfaces	Research has shown that two popular forms of wearable tactile displays, a back array and a waist belt, can aid pedestrian navigation by indicating direction. Each type has its proponents and each has been reported as successful in experimental trials, however, no direct experimental comparisons of the two approaches have been reported. We have therefore conducted a series of experiments directly comparing them on a range of measures. In this paper, we present results from a study in which we used a directional line drawing task to compare user performance with these two popular forms of wearable tactile display. We also investigated whether user performance was affected by a match between the plane of the tactile interface and the plane in which the users drew the perceived directions. Finally, we investigated the effect of adding a complementary visual display. The touch screen display on which participants drew the perceived directions presented either a blank display or a visual display of a map indicating eight directions from a central roundabout, corresponding to the eight directions indicated by the tactile stimuli. We found that participants performed significantly faster and more accurately with the belt than with the array whether they had a vertical screen or a horizontal screen. We found no difference in performance with the map display compared to the blank display.	experiment;line drawing algorithm;roundabout;tactile imaging;touchscreen;wearable computer	Mayuree Srikulwong;Eamonn O'Neill	2010		10.1007/978-3-642-15841-4_15	computer vision;simulation;engineering;communication	HCI	-45.84525615028186	-46.52463098938721	55486
15b4fac78258d6a79185d0f07e3d75cb76ef995e	image-based auto-positioning brush for lcd displays	liquid crystal display;liquid crystal displays;multimedia systems;multimedia systems computer vision interactive devices liquid crystal displays;video game;interactive multimedia;computer vision;image based auto positioning brush;human machine interface devices;human machine interface;ia brush;interactive multimedia services;brushes liquid crystal displays costs cameras humans games computer displays monitoring image processing painting;image processing techniques;human machine interface devices image based auto positioning brush lcd displays interactive devices interactive multimedia services;grid image ia brush video camera;video camera;grid image;interactive devices;lcd displays	The interactive devices, which help humans to easily access the computer and video game player, become an important technology to provide interactive multimedia services and entertainments in the 21st century. Considering the cost and the effectiveness of the human machine interface (HMI) devices, in this paper, we design a low cost image-based auto-positioning brush (IA-Brush), which can automatically locate the position of the window and modify the affection of angle displacement of camera. The proposed IA-Brush based on a low cost video camera can be easily implemented for low cost liquid crystal display monitor by using image processing techniques figure with a designated background grids. Simulation results with exhibition of interactive painting games show that we can achieve a low cost and effective HMI by using a video camera only.	computer monitor;displacement mapping;human–computer interaction;image processing;liquid-crystal display;polystation;simulation;user interface	Jun-Ren Ding;Jiun-Yu Chen;Fu-Chun Yang;Jar-Ferr Yang	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518075	computer vision;computer science;liquid-crystal display;multimedia;computer graphics (images)	Robotics	-41.393076349759056	-39.70674860499043	55514
4702e55a597e4d2906919aadd359e983ed32d7de	actui: using commodity mobile devices to build active tangible user interfaces	magic lens;touch screen;tangible;bench viewer;public records;websearch;touch pattern;ui design;pose tracking;mobile devices;rwth publications	We present the prototype design for a novel user interface, which extends the concept of tangible user interfaces from mostly specialized hardware components and studio deployment to commodity mobile devices in daily life. Our prototype enables mobile devices to be components of a tangible interface where each device can serve as both, a touch sensing display and as a tangible item for interaction. The only necessary modification is the attachment of a conductive 2D touch pattern on each device. Compared to existing approaches, our Active Commodity Tangible User Interfaces (ACTUI) can display graphical output directly on their built-in display paving the way to a plethora of innovative applications where the diverse combination of local and global active display area can significantly enhance the exibility and effectiveness of the interaction. We explore two exemplary application scenarios where we demonstrate the potential of ACTUI.	attachments;graphical user interface;mobile device;prototype;software deployment;tangible user interface	Ming Li;Leif Kobbelt	2015		10.1145/2786567.2792895	public records;simulation;human–computer interaction;computer science;operating system;mobile device;multimedia	HCI	-45.308737098059694	-39.44901865561565	55770
c7c146d5120f9d8315e148993da056df1fbd2697	a two-stage model for user's examination behavior in mobile search		With the rapid growth of mobile search, it is important to understand how users browse the mobile SERPs and allocate their limited attention to each result. To address this problem, we introduce a two-stage examination model that can separately capture the position bias with a skimming model and the attractiveness bias with an attractiveness model. The effectiveness of the proposed model is validated by using a dataset that contains explicit examination feedbacks from users. We further investigate user»s examination behaviors by analyzing the model parameters learned via EM algorithm. The results reveal some interesting findings such as how the skimming behavior is dependent on the previous examination sequence and what factors are associated with the attractiveness of search results on mobile SERPs.	bias–variance tradeoff;browsing;expectation–maximization algorithm;eye tracking;feedback;usability testing;viewport	Jiaxin Mao;Yiqun Liu;Noriko Kando;Zexue He;Min Zhang;Shaoping Ma	2018		10.1145/3176349.3176891	mobile search;data mining;computer science;attractiveness;expectation–maximization algorithm	Web+IR	-34.82680764759151	-50.912561421105906	55807
bf560ecbbac719dde20997532289d1f0fe307c67	virtual reality techniques for 3d data-warehouse exploration		This paper focuses on the evaluation of virtual reality (VR) interaction techniques for exploration of data warehouse (DW). The experimental DW involves hierarchical levels and contains information about customers profiles and related purchase items. A user study has been carried out to compare two navigation and selection techniques. Sixteen volunteers were instructed to explore the DW and look for information using the interaction techniques, involving either a single WiimoteTM (monomanual) or both WiimoteTM and NunchuckTM (bimanual). Results indicated that the bimanual interaction technique is more efficient in terms of speed and error rate. Moreover, most of the participants preferred the bimanual interaction technique and found it more appropriate for the exploration task. We also observed that males were faster and made less errors than females for both interaction techniques.	dreamwidth;graphics;haptic technology;interaction technique;multimodal interaction;usability testing;virtual reality	Hamza Hamdi;Eulalie Verhulst;Paul Richard	2017		10.5220/0006130400750083	computer-mediated reality;artificial reality;metaverse;mixed reality	HCI	-45.930388007701566	-47.186839747519905	56008
11c4b049a5a0cb2619f29a3cea5ae92773378d6c	identifying suitable projection parameters and display configurations for mobile true-3d displays	mobile true 3d;mid air pointing;intangible displays;displays;interface homme machine	We present a two-part exploration on mobile true-3D displays, i.e. displaying volumetric 3D content in mid-air. We first identify and study the parameters of a mobile true-3D projection, in terms of the projection's distance to the phone, angle to the phone, display volume and position within the display. We identify suitable parameters and constraints, which we propose as requirements for developing mobile true-3D systems. We build on the first outcomes to explore methods for coordinating the display configurations of the mobile true-3D setup. We explore the resulting design space through two applications: 3D map navigation and 3D interior design. We discuss the implications of our results for the future design of mobile true-3D displays.	requirement	Marcos Serrano;Dale Hildebrandt;Sriram Subramanian;Pourang Irani	2014		10.1145/2628363.2628375	computer vision;simulation;computer science;operating system;display device;computer graphics (images)	HCI	-45.08759166442222	-41.22910834144374	56050
06e6503a8d635d078573b595365e773665b123b0	tilt displays: designing display surfaces with multi-axis tilting and actuation	tilt displays;shape changing displays;spatially aware displays;physical actuation;non planar surface interaction;interaction;display surfaces;qa75 electronic computers computer science;m pspaces;actuated displays;deformable displays;mobile virtual workspace;pico projectors	We present a new type of actuatable display, called Tilt Displays, that provide visual feedback combined with multi-axis tilting and vertical actuation. Their ability to physically mutate provides users with an additional information channel that facilitates a range of new applications including collaboration and tangible entertainment while enhancing familiar applications such as terrain modelling by allowing 3D scenes to be rendered in a physical-3D manner. Through a mobile 3x3 custom built prototype, we examine the design space around Tilt Displays, categorise output modalities and conduct two user studies. The first, an exploratory study examines users' initial impressions of Tilt Displays and probes potential interactions and uses. The second takes a quantitative approach to understand interaction possibilities with such displays, resulting in the production of two user-defined gesture sets: one for manipulating the surface of the Tilt Display, the second for conducting everyday interactions.	apache axis;categorization;interaction;prototype;terrain rendering;usability testing	Jason Alexander;Andrés Lucero;Sriram Subramanian	2012		10.1145/2371574.2371600	computer vision;interaction;simulation;computer science;computer graphics (images)	HCI	-45.059044949614986	-41.21883105174964	56065
c96791fe470f0e92cc98bc4493ef1010d16af7c8	“wow! i have six fingers!”: would you accept structural changes of your hand in vr?	embodiment;virtual characters;virtual reality;ownership;avatars;perception;avatar	With the increasing demand in virtual reality applications and games, the need to understand how users perceive their virtual representation (avatar) is becoming more and more important. In particular, with the potential of virtual reality to alter and control avatars in different ways, the user representation in the virtual world does not always necessarily match the user body structure. In this context, this paper explores how users would accept as their own a six-digit realistic virtual hand. By measuring participants’ senses of ownership (i.e., the impression that the virtual hand is actually our own hand) and agency (i.e., the impression to be able to control the actions of the virtual hand), we somehow evaluate the possibility of creating a Six-Finger Illusion in VR. We measured these two dimensions of virtual embodiment in a virtual reality experiment where participants performed two tasks successively: (1) a self-manipulation task inducing visuomotor feedback, where participants mimicked finger movements presented in the virtual scene and (2) a visuotactile task inspired by Rubber Hand Illusion protocols, where an experimenter stroked the hand of the user with a brush. The real and virtual brushes were synchronously stroking the participants’ real and virtual hand, and in the case when the virtual brush was stroking the additional virtual digit, the real ring finger was also synchronously stroked to provide consistent tactile stimulation and elicit a sense of embodiment. Results of the experiment show that participants did experience high levels of ownership and agency of the six-digit virtual hand as a whole. We found higher levels of ownership and agency for the additional finger when the hand was fully animated, compared to a control group where the additional digit was not animated. Through the presented experiment, we found that participants responded positively to the possibility of controlling the six-digit hand despite the structural difference, and accepted the sixdigit virtual hand and individual digits as their own to some extent. These results bring preliminary insights about how avatar with structural differences can affect the senses of ownership and agency experienced by users in VR.	avatar (computing);virtual reality;virtual world	Ludovic Hoyet;Ferran Argelaguet;Corentin Nicole;Anatole Lécuyer	2016	Front. Robotics and AI	10.3389/frobt.2016.00027	simulation;computer science;virtual reality;multimedia;perception	Visualization	-46.318056284826845	-49.269794855537526	56386
cd57bc7775de3f6d7018171b4b936a82597cf860	place learning in humans: the role of distance and direction information	virtual reality;virtual environments;learning environment;landmark based navigation;virtual environment;landmarks;place learning	Although the process of establishing a memory of a location is necessary for navigation, relatively little is known about the information that humans use when forming place memories. We examined the relative importance of distance and angular information about landmarks in place learning. Participants repeatedly learned a target location in relation to three distinct landmarks in an immersive computer-generated (virtual) environment. Later, during testing, they attempted to return to that location. The configurations of landmarks used during testing were altered from those participants learned in order to separate the effects of metric distance information and information about inter-landmark angles. In general, participants showed greater reliance on distance information than angular information. This reliance was affected by nonmetric relationships present during learning, as well as by the degree to which the learned environment contained right or straight angles.	angularjs;computer-generated holography	David Waller;Jack M. Loomis;Reginald G. Golledge;Andrew C. Beall	2000	Spatial Cognition & Computation	10.1023/A:1015514424931	computer vision;simulation;computer science;virtual machine;virtual reality;communication	HCI	-44.042484613848046	-49.75578778718898	56440
60034340ec20c3f2b82aa379f250dd9b1c6f539d	combating latency in haptic collaborative virtual environments	surgical simulation;force feedback;long distance;surgical procedure;virtual environment;collaborative virtual environment;shared workspace;dynamic scenes	Haptic (force) feedback is increasingly being used in surgical-training simulators. The addition of touch is important extra information that can add another dimension to the realism of the experience. Progress in networking these systems together over long distances has been held back, principally because the latency of the network can induce severe instability in any dynamic objects in the scene. This paper describes techniques allowing long-distance sharing of haptic-enabled, dynamic scenes. At the CSIRO Virtual Environments Laboratory, we have successfully used this system to connect a prototype of a surgical-simulation application between participants on opposite sides of the world in Sweden and Australia, over a standard Internet connection spanning 3 continents and 2 oceans. The users were able to simultaneously manipulate pliable objects in a shared workspace, as well as guide each other's hands (and shake hands!) over 22,000 km (13620 miles) of Internet connection. The main obstacle to overcome was the latency-induced instability in the system, caused by the delays and jitter inherent in the network. Our system involved a combination of an event-collection mechanism, a network event-forwarding mechanism and a pseudophysics mechanism. We found that the resulting behavior of the interconnected body organs, under simultaneous-user manipulation, was sufficiently convincing to be considered for training surgical procedures.	algorithm;collaborative virtual environment;communications satellite;file spanning;haptic technology;instability;internet;interrupt latency;prototype;server (computing);shake;simulation;virtual reality;workspace	Chris Gunn	2005	Presence: Teleoperators & Virtual Environments	10.1162/105474605323384663	simulation;computer science;virtual machine;artificial intelligence;operating system;multimedia;haptic technology	Visualization	-41.75898854724464	-50.09604869667099	56483
2235be5a7c25b233390039640a80dba21dbfe3aa	situated communication with a simulated robot	simulated robot;situated communication		robot;simulation;situated	Simone Strippgen;Kornelia Peters;Jan-Torsten Milde	1998			simulation;situated;robot;computer science	Robotics	-35.752734955723454	-39.367332376023846	56625
36e06382256b3ded31f94410b7e269d999d40fe4	extracting tactile sensation information from multi-illuminated images of tangible cultural property	surface structure;tangible cultural property;vibrational signal;force feedback device tactile sensation information multi illuminated images tangible cultural property haptic devices multiple photographic images vibrational signal natural noise 3d visual display;vibrations;3d visualization;haptic device;multi illuminated image;virtual reality;tactile sensation information;image sensors;surface reconstruction;multiple photographic images;data mining;mesostructure;force feedback;vibrations image sensors tactile sensors;three dimensional displays;tactile sensation;natural noise;tactile sensors;haptic interfaces;3d visual display;haptic devices;mesostructure tactile sensation multi illuminated image tangible cultural property;force feedback device;data mining cultural differences signal synthesis haptic interfaces vibration measurement hardware surface structures lighting virtual reality three dimensional displays;cameras;multi illuminated images;noise	In recent years, haptic devices have been developed to give users the sensation of physically touching virtual objects. However, these studies have focused mainly on developing haptic hardware, and algorithms for synthesizing the tactile sensation signals have not been well studied. In this paper, a technique is proposed for synthesizing tactile vibrational signals based on the ability to measure surface structure by the use of multiple photographic images taken at different illumination angles. The vibrational signal of the tactile device is synthesized using natural noise. The virtual reality system consists of a 3D visual display, a force feedback device and a vibration tactile sensation device.	algorithm;haptic technology;virtual reality	Xin Yin;Kazuyoshi Nomura;Hiromi T. Tanaka	2009	2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2009.249	computer vision;computer science;virtual reality;multimedia;haptic technology	Robotics	-41.10388647672495	-40.70581515242815	56985
09e039d20cce992b7f10c5edf3e03077f9d98d07	exploring smartphone-based interaction with overview+detail interfaces on 3d public displays	informatique mobile;mid air interaction;3d interfaces;public display;overview detail;personal displays	As public displays integrate 3D content, Overview+Detail (O+D) interfaces on mobile devices will allow for a personal 3D exploration of the public display. In this paper we study the properties of mobile-based interaction with O+D interfaces on 3D public displays. We evaluate three types of existing interaction techniques for the 3D translation of the Detail view: touchscreen input, mid-air movement of the mobile device (Mid-Air Phone) and mid-air movement of the hand around the device (Mid-Air Hand). In a first experiment, we compare the performance and user preference of these three types of techniques with previous training. In a second experiment, we study how well the two mid-air techniques perform with no training or human help to imitate usual conditions in public context. Results reveal that Mid-Air Phone and Hand perform best with training. However, without training or human help Mid-Air Phone is more intuitive and performs better on the first trial. Interestingly, on both experiments users preferred Mid-Air Hand. We conclude with a discussion on the use of mobile devices to interact with public O+D interfaces.	air-ground radiotelephone service;experiment;interaction technique;mobile device;smartphone;touchscreen	L. Berge;Marcos Serrano;Gary Perelman;Emmanuel Dubois	2014		10.1145/2628363.2628374	simulation;human–computer interaction;multimedia	HCI	-46.84490949095199	-44.12057576709618	57086
4e61ddc6abd6fba1f5a009122dcf0ca71f6f1b3f	impact of virtual environments on motivation and engagement during exergames		Video games and sport are an essential part in the life of millions of people. With recent advances of immersive virtual reality devices such as the HTC Vive, Oculus Rift, or PlayStation VR, the use of virtual environments (VE) for exergames is becoming more and more popular. An exergame combines a physical activity with video game elements by tracking body movements or reactions of user, attempting to engage users in a more enjoyable system. In this paper, we present the results of a subjective experiment carried out with the aim to compare different kinds of virtual environments with each other. A rowing ergometer, connected either to a virtual reality system using a head-mounted display (HMD) or to a CAVE environment, was used as an exergame device. While for rowing experts, fitness and performance improvements are of major interest, we wanted to focus on the motivation and engagement of non-professionals. By means of a series of questionnaires and a follow-up interview, the Quality of Experience of participants using the system was assessed. Measurements include concepts such as flow, presence, video quality and well-being. Results show significant advantages of the HMD as well as of the CAVE compared to a system without a VE for the overall quality, system feedback, and flow. While the CAVE and HMD system mainly differed in their autotelic experience, the HMD was favored by the majority of participants due to a superior feeling of presence.	htc vive;head-mounted display;immersion (virtual reality);oculus rift;playstation vr;presence information;video;virtual reality	Steven Schmidt;Patrick Ehrenbrink;Benjamin Weiss;Jan-Niklas Voigt-Antons;Tanja Kojic;Andrew Johnston;Sebastian Möller	2018	2018 Tenth International Conference on Quality of Multimedia Experience (QoMEX)	10.1109/QoMEX.2018.8463389	multimedia;rowing;feeling;immersion (virtual reality);flow (psychology);quality of experience;video quality;virtual reality;computer science;autotelic	Visualization	-48.15585567619803	-50.67414332618934	57284
201487207e190c3ec816f004a39051b38f4e44f8	dynamic key frame presentation techniques for augmenting video browsing	display rate;frames per second;object recognition;divided attention;user study;user perception;interface design;representations;key frames;video browsing;visual interfaces;dynamic displays	"""Because of unique temporal and spatial properties of video data, different techniques for summarizing videos have been proposed. Key frames extracted directly from video inform users about content without requiring them to view the entire video. As part of ongoing work to develop video browsing interfaces, several interface displays based on key frames were investigated. Variations on dynamic key frame """"slide shows"""" were examined and compared to a static key frame """"filmstrip"""" display. The slide show mechanism displays key frames in rapid succession and is designed to facilitate visual browsing by exploiting human perceptual capabilities. User studies were conducted in a series of three experiments. Key frame display rate, number of simultaneous displays, and user perception were investigated as a function of user performance in object recognition and gist determination tasks. No significant performance degradation was detected at display rates up to 8 key frames per second, but performance degraded significantly at higher rates. Performance on gist determination tasks degraded less severely than performance on object recognition tasks as display rates increased. Furthermore, gist determination performance dropped significantly between three and four simultaneous slide shows in a single display. Users generally preferred key frame filmstrips to dynamic displays, although objective measures of performance were mixed. Implications for visual interface design and further questions for future research are provided."""	elegant degradation;experiment;gist;key frame;outline of object recognition;static key;succession;usability testing	Tony Tse;Gary Marchionini;Wei Ding;Laura A. Slaughter;Anita Komlodi	1998		10.1145/948496.948522	reference frame;computer vision;simulation;computer science;interface design;cognitive neuroscience of visual object recognition;multimedia;frame rate;world wide web	HCI	-38.08849138698292	-50.029061930354025	57571
343b42551d237a286cd35e6dcbcffade5d594610	tivoli: an electronic whiteboard for informal workgroup meetings	design process;interactive display;model based interface tools;uims;interface builders	This paper describes Tivoli, an electronic whiteboard application designed to support informal workgroup meetings and targeted to run on the Xerox Liveboard, a large screen, pen-based interactive display. Tivoli strives to provide its users with the simplicity, facile use, and easily understood functionality of conventional whiteboards, while at the same time taking advantage of the computational power of the Liveboard to support and augment its users' informal meeting practices. The paper presents the motivations for the design of Tivoli and briefly describes the current version in operation. It then reflects on several issues encountered in designing Tivoli, including the need to reconsider the basic assumptions behind the standard desktop GUI, the use of strokes as the fundamental object in the system, the generalized wipe interface technique, and the use of meta-strokes as gestural commands.	desktop computer;graphical user interface;interactive whiteboard	Elin Rønby Pedersen;Kim McCall;Thomas P. Moran;Frank G. Halasz	1993		10.1145/169059.169309	simulation;design process;human–computer interaction;computer science;operating system;multimedia;world wide web	HCI	-48.01292846796994	-39.62142549407366	57661
5acef5d6089de97978dc8dc3a08ecfb23332edc4	eye and pointer coordination in search and selection tasks	input device;multimodal interface;top down;eye hand coordination;eye movements;eye movement;graphic user interface;input devices;inproceedings;eye tracking;multimodal interfaces	Selecting a graphical item by pointing with a computer mouse is a ubiquitous task in many graphical user interfaces. Several techniques have been suggested to facilitate this task, for instance, by reducing the required movement distance. Here we measure the natural coordination of eye and mouse pointer control across several search and selection tasks. We find that users automatically minimize the distance to likely targets in an intelligent, task dependent way. When target location is highly predictable, top-down knowledge can enable users to initiate pointer movements prior to target fixation. These findings question the utility of existing assistive pointing techniques and suggest that alternative approaches might be more effective.	computer mouse;graphical user interface;pointer (computer programming);pointer (user interface);top-down and bottom-up design	Hans-Joachim Bieg;Lewis L. Chuang;Roland W. Fleming;Harald Reiterer;Heinrich H. Bülthoff	2010		10.1145/1743666.1743688	computer vision;simulation;pointer;computer science;communication;pointing device	HCI	-46.30489153813017	-46.319526407432306	57787
0c2fb4872fe09dcecec845e5d86b52ba55fee12a	gender recognition in walk gait through 3d motion by quadratic bezier curve and statistical techniques		Motion capture is the process of recording the movement of objects or people. It is used in military, entertainment, sports, and medical applications, and for validation of computer vision[2] and robotics. In filmmaking and video game development, it refers to recording actions of human actors, and using that information to animate digital character models in 2D or 3D computer animation. When it includes face and fingers or captures subtle	bézier curve;computer animation;computer graphics;motion capture;robotics;video game development	Sajid Ali	2012	CoRR		computer vision;motion capture;simulation;computer science;computer graphics (images)	Graphics	-38.57015376362599	-38.857994158541004	57833
b2b8af5487f576f373e11c944bdff49ceb3f5f8f	planning body gesture of android for multi-person human-robot interaction	databases;human impressions body gesture planning multiperson human robot interaction natural body gesture speech dialog human robot symbiosis real time gesture planning method motion parameterization function speaker locations object locations hri system android actroid sit;humanoid robot;humans planning androids humanoid robots databases accuracy;androids;path planning;real time;path planning control engineering computing human robot interaction;human robot interaction;accuracy;humanoid robots;planning;control engineering computing;humans	Natural body gesture, as well as speech dialog, is crucial for human-robot interaction and human-robot symbiosis. We have already proposed a real-time gesture planning method. In this paper, we afford this method more flexibility by adding motion parameterization function. Especially in multi-person HRI, this function becomes more important because of its adaptation to changes of a speaker's and/or object's locations. We implement our method for multi-person HRI system on the android Actroid-SIT, and conduct two experiments for estimating the precision of gestures and the human impressions about the Actroid. Through these experiments, we confirmed our method gives humans a more sophisticated impressions.	experiment;human–robot interaction;real-time clock;dialog	Yutaka Kondo;Kentaro Takemura;Jun Takamatsu;Tsukasa Ogasawara	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6224903	human–robot interaction;computer vision;simulation;computer science;engineering;humanoid robot;artificial intelligence	Robotics	-37.241512213842846	-42.783947648679344	57892
42799b44970d4d3d835c257cb5de467702b45222	multi-point vibrotactile feedback for an expressive musical interface		This paper describes the design of a hardware/software system for rendering multi-point, localized vibrotactile feedback in a multi-touch musical interface. A prototype was developed, based on the Madrona Labs Soundplane, which was chosen for it provides easy access to multi-touch data, including force, and its easily expandable layered construction. The proposed solution makes use of several piezo actuator discs, densely arranged in a honeycomb pattern on a thin PCB layer. Based on o↵-the-shelf components, custom amplifying and routing electronics were designed to drive each piezo element with standard audio signals. Features, as well as electronic and mechanical issues of the current prototype are discussed.	accessibility;multi-touch;piezoelectricity;printed circuit board;prototype;routing;software system	Stefano Papetti;Sébastien Schiesser;Martin Fröhlich	2015			embedded system;real-time computing;acoustics;computer hardware;operating system	HCI	-42.56956785498897	-41.29990486909502	57984
bfbcad5a58f29b6532342aa03a788f52143b63ae	context triggered visual episodic memory prosthesis	episodic memory;human memory context sensitive recorder and replay system retrospective episodic memory wearable camera visual content memory prosthesis augmented memory;context information;prosthetics;augmented memory;prosthetics information retrieval prototypes cameras image retrieval multimedia databases computer displays global positioning system transform coding image coding;portable computers;infrared;visual databases prosthetics portable computers;visual databases	This paper briefly describes a context sensitive recorder and replay system that supports retrospective episodic memory. The concept has been explored by implementing a prototype system, which utilizes simple context information based on infrared beacons to store and retrieve images taken with a wearable camera. Presenting visual content about a user's life, in mobile use, should help memory to recall other facts about the user's context.	prototype;wearable computer	Jyrki Hoisko	2000	Digest of Papers. Fourth International Symposium on Wearable Computers	10.1109/ISWC.2000.888493	embedded system;computer vision;infrared;computer science;episodic memory;multimedia;computer graphics (images)	HCI	-39.716226997412306	-42.495449472965504	58010
7b94558072b4a25c7670811cd282439d589598e1	wearable aid for the visually impaired	image scanners;visually impaired aid wearable vision aid video camera scanner headset laptop computer software implementation human interface technology lab;displays optical fibers portable computers light emitting diodes cameras prototypes costs legged locomotion optical filters cows;virtual fences;optical scanners;wearable computers;glacial sensors;computer vision;video cameras handicapped aids optical scanners microcomputer applications wearable computers image scanners computer vision;human interface;handicapped aids;video cameras;glascweb;visual impairment;microcomputer applications;wearable vision aid	For people with impaired vision, students at the University of Washington's Human Interface Technology Lab have created a prototype that just might solve their problem. We discuss about the wearable vision aid that alerts its users of stationary objects that are near enough for a collision. One of the group's main design objectives was to create something that could be developed inexpensively. The prototype combines a video camera, a scanner headset, a laptop computer, and software.	headset (audio);human–computer interaction;laptop;prototype;stationary process;wearable computer	Danna Voth	2004	IEEE Pervasive Computing	10.1109/MPRV.2004.1321018	computer vision;simulation;wearable computer;computer science;operating system;human interface device;computer graphics (images)	HCI	-40.62981828254434	-43.232232658981125	58352
5287866fd111a8b8d514b58c780cc8a274a78ffe	perceptual evaluation of cardboarding in 3d content visualization	non photorealistic rendering;stereo;stereo 3d	"""A pervasive artifact that occurs when visualizing 3D content is the so-called """"cardboarding"""" effect, where objects appear flat due to depth compression, with relatively little research conducted to perceptually quantify its effects. Our aim is to shed light on the subjective preferences and practical perceptual limits of stereo vision with respect to cardboarding. We present three experiments that explore the consequences of displaying simple scenes with reduced depths using both subjective ratings and adjustments and objective sensitivity metrics. Our results suggest that compressing depth to 80% or above is likely to be acceptable, whereas sensitivity to the cardboarding artifact below 30% is very high. These values could be used in practice as guidelines for commonplace depth mapping operations in 3D production pipelines."""	experiment;pipeline (computing);stereopsis;visual artifact	Alexandre Chapiro;Olga Diamanti;Steven Poulakos;Carol O'Sullivan;Aljoscha Smolic;Markus H. Gross	2014		10.1145/2628257.2628271	computer vision;geography;multimedia;computer graphics (images)	HCI	-42.37757391995527	-48.9105676466927	58406
0905d4774b1b8c2d8b3d563b94900dac0bf5c122	multimodal interaction with speech, gestures and haptic feedback in a media center application	pilot study;media center;speech;haptics;mobile phone;gestures;haptic feedback;multimodal interaction	We demonstrate interaction with a multimodal media center application. Mobile phone-based interface includes speech and gesture input and haptic feedback. The setup resembles our long-term public pilot study, where a living room environment containing the application was constructed inside a local media museum allowing visitors to freely test the system.	haptic technology;mobile phone;multimodal interaction;windows media center	Markku Turunen;Jaakko Hakulinen;Juho Hella;Juha-Pekka Rajaniemi;Aleksi Melto;Erno Mäkinen;Jussi Rantala;Tomi Heimonen;Tuuli Laivo;Hannu Soronen;Mervi Hansen;Pellervo Valkama;Toni Miettinen;Roope Raisamo	2009		10.1007/978-3-642-03658-3_88	human–computer interaction;computer science;multimedia;haptic technology	HCI	-47.8987293642484	-38.545107249413704	58743
744d07a91b8b8dc6f82bed866537d98f1d09742a	step aside: an initial exploration of gestural input for lateral movement during walking-in-place locomotion		Walking-in-place (WIP) techniques provide users with a relatively natural way of walking in virtual reality. However, previous research has primarily focused on WIP during forward movement and tasks involving turning. Thus, little is known about what gestures to use in combination with WIP in order to enable sidestepping. This paper presents two user studies comparing three different types of gestures based on movement of the hip, leaning of the torso, and actual sidesteps. The first study focuses on purely lateral movement while the second involves both forward and lateral movement. The results of both studies suggest that leaning yielded significantly more natural walking experiences and this gesture also produced significantly less positional drift.	drift plus penalty;experience;futures studies;in-place algorithm;lateral thinking;lean integration;obstacle avoidance;usability testing;virtual body;virtual reality	C Wang;Oana A. Dogaru;Patrick L. Strandholt;Niels C. Nilsson;Rolf Nordahl;Stefania Serafin	2018		10.1145/3281505.3281536	computer vision;torso;artificial intelligence;lateral movement;virtual reality;computer science;gesture;aside	HCI	-45.46872155130085	-48.07826751329164	58856
26ceab01086e0ca5b4a6313eb591e9a45f88dfcc	comparing presentation summaries: slides vs. reading vs. listening	video abstraction;user evaluation;video summarization;multimedia;user study;video skim;audio video;video browsing;digital video library	As more audio and video technical presentations go online, it becomes imperative to give users effective summarization and skimming tools so that they can find the presentation they want and browse through it quickly. In a previous study, we reported three automated methods for generating audio-video summaries and a user evaluation of those methods. An open question remained about how well various text/image only techniques will compare to the audio-video summarizations. This study attempts to fill that gap. This paper reports a user study that compares four possible ways of allowing a user to skim a presentation: 1) PowerPoint slides used by the speaker during the presentation, 2) the text transcript created by professional transcribers from the presentation, 3) the transcript with important points highlighted by the speaker, and 4) a audio-video summary created by the speaker. Results show that although some text-only conditions can match the audio-video summary, users have a marginal preference for audio-video (ANOVA f=3.067, p=0.087). Furthermore, different styles of slide-authoring (e.g., detailed vs. big-points only) can have a big impact on their effectiveness as summaries, raising a dilemma for some speakers in authoring for on-demand previewing versus that for live audiences.	browsing;imperative programming;marginal model;smart common input method;text-based user interface;usability testing	Li-wei He;Elizabeth Sanocki;Anoop Gupta;Jonathan Grudin	2000		10.1145/332040.332427	speech recognition;computer science;multimedia;world wide web;pevq	HCI	-37.198132302768535	-49.48857287454988	59006
354a97e00c6d51058ed76530627072eb466ce472	interaction and modeling techniques for desktop two-handed input	input device;modeling technique;map navigation;two handed interaction;touchpads;indexation;user testing;input devices;touchmouse;two handed input;three state model;tablests	We describe input devices and two-handed interaction techniques to support map navigation tasks. We discuss several design variations and user testing of two-handed navigation techniques, including puck and stylus input on a Wacom tablet, as well as a novel design incorporating a touchpad (for the nonpreferred hand) and a mouse (for the preferred hand). To support the latter technique, we introduce a new input device, the TouchMouse, which is a standard mouse augmented with a pair of one-bit touch sensors, one for the palm and one for the index finger. Finally, we propose several enhancements to Buxton’s three-state model of graphical input and extend this model to encompass two-handed input transactions as well.	desktop computer;graphical user interface;input device;interaction technique;sensor;stylus (computing);tablet computer;three-state logic;touchpad;user research	Ken Hinckley;Mary Czerwinski;Mike Sinclair	1998		10.1145/288392.288572	simulation;human–computer interaction;computer science;operating system;multimedia;input device	HCI	-46.287855868230764	-42.45609921547855	59097
abb18a88f290539e08eccb75c6f9865b4274b210	an hmm-based multi-sensor approach for continuous mobile authentication	multi sensor touch pattern continuous authentication hidden markov model behavioral biometric;hidden markov models authentication sensors data models mobile handsets training mobile communication;sensors;training;authentication;hidden markov models;mobile communication;mobile handsets;mobile device hmm based multisensor approach continuous mobile authentication smart phones robust authentication mechanism hidden markov model touch interface based mobile devices behavioral template training approach gesture patterns accelerometer gyroscope data continuous left right hmm;telecommunication security accelerometers gyroscopes hidden markov models mobile handsets mobile radio sensor fusion;data models	With the increased popularity of smart phones, there is a greater need to have a robust authentication mechanism that handles various security threats and privacy leakages effectively. This paper studies continuous authentication for touch interface based mobile devices. A Hidden Markov Model (HMM) based behavioral template training approach is presented, which does not require training data from other subjects other than the owner of the mobile device and can get updated with new data over time. The gesture patterns of the user are modeled from multiple sensors - touch, accelerometer and gyroscope data using a continuous left-right HMM. The approach models the tap and stroke patterns of a user since these are the basic and most frequently used interactions on a mobile device. To evaluate the effectiveness of the proposed method a new data set has been created from 42 users who interacted with off-the-shelf applications on their smart phones. Results show that the performance of the proposed approach is promising and potentially better than other state-of-the-art approaches.	authentication;gyroscope;hidden markov model;interaction;markov chain;mobile device;sensor;smartphone;touch user interface	Aditi Roy;Tzipora Halevi;Nasir D. Memon	2015	MILCOM 2015 - 2015 IEEE Military Communications Conference	10.1109/MILCOM.2015.7357626	embedded system;engineering;internet privacy;mobile computing;computer security	Mobile	-37.669222421821196	-46.30657556933545	59176
f23b99fabcce2a34e81bd8fdd9910eec566221ea	panoramic video techniques for improving presence in virtual environments	user studies and evaluation;interaction;categories and subject descriptors according to acmccs i 3 3 computer graphics virtual reality presence	Photo-realistic techniques that use sequences of images captured from a real environment can be used to create virtual environments (VEs). Unlike 3D modelling techniques, the required human work and computation are independent of the amounts of detail and complexity that exist in the scene, and in addition they provide great visual realism. In this study we created virtual environments using three different photo-realistic techniques: panoramic video, regular video, and a slide show of panoramic still images. While panoramic video offered continuous movement and the ability to interactively change the view, it was the most expensive and time consuming to produce among the three techniques. To assess whether the extra effort needed to create panoramic video is warranted, we analysed how effectively each of these techniques supported a sense of presence in participants. We analysed participants' subjective sense of presence in the context of a navigation task where they travelled along a route in a VE and tried to learn the relative locations of the landmarks on the route. Participants' sense of presence was highest in the panoramic video condition. This suggests that the effort in creating panoramic video might be warranted whenever high presence is desired.		Arefe Dalvandi;Bernhard E. Riecke;Thomas W. Calvert	2011		10.2312/EGVE/JVRC11/103-110	computer vision;simulation;engineering;multimedia	Visualization	-40.94239489847586	-51.725428932959055	59272
1ef00f4bdd85a81e91d8d0e16661aae23471831c	evaluating haptics for information discovery while walking	mobile computer;haptics;information discovery;haptic feedback;location awareness;mobile computing;visual system;location aware	In this article we describe and evaluate a novel, low interaction cost approach to supporting the spontaneous discovery of geo-tagged information while on the move. Our mobile haptic prototype helps users to explore their environment by providing directional vibrotactile feedback based on the presence of location data. We conducted a study to investigate whether users can find these targets while walking, comparing their performance when using only haptic feedback to that when using an equivalent visual system. The results are encouraging, and here we present our findings, discussing their significance and issues relevant to the design of future systems that combine haptics with location awareness.	haptic technology;information discovery;location awareness;prototype;spontaneous order	Simon Robinson;Parisa Eslambolchilar;Matt Jones	2009		10.1145/1671011.1671022	computer vision;simulation;engineering;multimedia	HCI	-46.676643049495894	-44.287049104021655	59384
49e30d1af8b7b52f416048e3be3ea888a643ea4c	fast tracking of hands and fingertips in infrared images for augmented desk interface	size specification;camara ir;desk interface;pistage;image segmentation;detecteur image;red www;augmented desk interface;object manipulation hand tracking fingertip tracking augmented desk interface infrared camera images user hand detection template matching hand gestures;image matching;infrared thermography;rastreo;infrared images;left handed;camera ir;tracking movable target;image contrast;thermographie ir;internet;infrared imaging;contraste image;pattern matching;segmentation image;world wide web;termografia ir;perceptual user interface;reseau www;detector imagen;poursuite;concordance forme;finger tracking;galibo;augmented reality;infrared camera;imagen contraste;template matching;gabarit;gesture recognition;image sensor;augmented reality tracking gesture recognition object detection image matching image sequences;tracking;object detection;persecucion y continuacion;image sequences	In this paper, we introduce a fast and robust method for tracking positions of the centers and the ngertips of both right and left hands. Our method makes use of infrared camera images for reliable detection of user's hands, and uses template matching strategy for nding ngertips. This method is an essential part of our augmented desk interface in which a user can, with natural hand gestures, simultaneously manipulate both physical objects and electronically projected objects on a desk, e.g., a textbook and related WWW pages. Previous tracking methods which are typically based on color segmentation or background subtraction simply do not perform well in this type of application because an observed color of human skin and image backgrounds may change signi cantly due to projection of various objects onto a desk. In contrast, our proposed method was shown to be e ective even in such a challenging situation through demonstration in our augmented desk interface. This paper describes the details of our tracking method as well as typical applications in our augmented desk interface.	background subtraction;general-purpose modeling;image processing;image segmentation;image subtraction;lcd projector;template matching;video projector;www	Yoichi Sato;Yoshinori Kobayashi;Hideki Koike	2000		10.1109/AFGR.2000.840675	computer vision;augmented reality;finger tracking;the internet;template matching;computer science;pattern matching;image sensor;gesture recognition;tracking;multimedia;image segmentation;computer graphics (images)	Vision	-38.738076600751235	-42.11858384667582	59710
f9e35c39225c93af6e7e3727f18a7ad56dae0c71	cardiolens: remote physiological monitoring in a mixed reality environment		Numerous vital signs can be captured through the measurement of blood flow; however, these signals are not visible to the unaided eye and measurement traditionally requires customized contact sensors. We present Cardiolens - a mixed reality application that enables real-time hands-free measurement and visualization of blood flow and vital signs. The system combines a front-facing camera, remote imaging photoplethysmography software and a heads up display allowing users to view the physiological state of a person simply by looking at them. Cardiolens provides the wearer with a new way to understand physiology and has applications in health care and affective computing.		Christophe Hurter;Daniel J. McDuff	2017		10.1145/3084822.3084834	mixed reality;computer vision;computer graphics (images);head-up display;visualization;artificial intelligence;computer-mediated reality;vital signs;affective computing;computer science;software;augmented reality	HCI	-43.21575771583066	-39.23606741821074	59765
bcfd0b1566a54ebfeb44932601147fdd52b74b21	modeling spatial referencing language for human-robot interaction	eigenvalues and eigenfunctions;range data;mobile robot;path planning;mobile robots;natural languages;human robot interaction;human subjects;left right;control engineering computing mobile robots eigenvalues and eigenfunctions path planning computational linguistics natural languages;spatial language;control engineering computing;science fiction;computational linguistics;computational linguistics spatial referencing language human robot interaction mobile robot eigenvectors contour points;orbital robotics humans mobile robots robot kinematics natural languages robot sensing systems grid computing computational linguistics extraterrestrial measurements communication system control;eigenvectors	"""It has long been a dream of science fiction to have a robot which understands the richness of spoken language. Part of attaining this goal is to exploit the ways that language structures space which would lead to a more natural way to interact with our robots. By investigating the application of spatial language as a modality of interacting with robots, we can create an interface that is more intuitive for a novice user. In this paper, we outline a method for computing target points to the FRONT, LEFT, RIGHT, and BEHIND segmented objects in evidence grid maps built using range data on a mobile robot. This method uses the segmented objects and their contour points to calculate eigenvectors which can be used to calculate the LEFT, RIGHT, FRONT, and BEHIND points. This allows the user to issue spatial referencing commands, such as """"go behind the desk"""" or """"look to the left of the table."""" We also show results for a human-subject experiment which provides validation of the algorithm."""	algorithm;human–robot interaction;map;mobile robot;modality (human–computer interaction)	Samuel Blisard;Marjorie Skubic	2005	ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.	10.1109/ROMAN.2005.1513861	human–robot interaction;mobile robot;simulation;computer science;artificial intelligence;computational linguistics	Robotics	-35.76282552446657	-42.18212353422413	60016
c3d4ab6d2cb7255c2e1e22bf6890c8c1c90c5810	wayfinding of users with visual impairments in haptically enhanced virtual environments		As a powerful interaction technology, haptically enhanced virtual environments have found many useful applications. However, few studies have examined how wayfinding of users with visual impairments is affected by VE characteristics. An empirical experiment was conducted to investigate how different environmental characteristics (number of objects inside the environment, layout of the objects and density) affect task performance (completion time, completion ratio and travel distance), perceived task difficulty and behavior pattern (short and long pause) of users with visual impairments when they perform a wayfinding task in a desktopD ow nl oa de d by [ Se lc uk U ni ve rs ite si ] at 2 2: 14 1 1 Fe br ua ry 2 01 5	haptic technology;virtual reality	Chang Soo Nam;Mincheol Whang;Shijing Liu;Matthew Moore	2015	Int. J. Hum. Comput. Interaction	10.1080/10447318.2015.1004151	simulation;human–computer interaction;multimedia	HCI	-46.00858772806943	-47.836955201072435	60119
3fb7bbcc32f6f5195ac48b8182479861dbf8c2c5	eyespyvr: interactive eye sensing using off-the-shelf, smartphone-based vr headsets		Low cost virtual reality (VR) headsets powered by smartphones are becoming ubiquitous. Their unique position on the useru0027s face opens interesting opportunities for interactive sensing. In this paper, we describe EyeSpyVR, a software-only eye sensing approach for smartphone-based VR, which uses a phoneu0027s front facing camera as a sensor and its display as a passive illuminator. Our proof-of-concept system, using a commodity Apple iPhone, enables four sensing modalities: detecting when the VR head set is worn, detecting blinks, recognizing the weareru0027s identity, and coarse gaze tracking - features typically found in high-end or specialty VR headsets. We demonstrate the utility and accuracy of EyeSpyVR in a series of studies with 70 participants, finding a worn detection of 100%, blink detection rate of 95.3%, family user identification accuracy of 81.4%, and mean gaze tracking error of 10.8° when calibrated to the wearer (12.9° without calibration). These sensing abilities can be used by developers to enable new interactive features and more immersive VR experiences on existing, off-the-shelf hardware.	eye tracking;illuminator (backlight);interactivity;prototype;sensor;smartphone;usability testing;virtual reality	Karan Ahuja;Rahul Islam;Varun Parashar;Kuntal Dey;Chris Harrison;Mayank Goel	2018	IMWUT	10.1145/3214260	immersion (virtual reality);virtual reality;computer vision;artificial intelligence;eye tracking;computer science	HCI	-45.105357149727055	-43.425745014512785	60155
80d8fcb6a62075860df9c761e1f1825bc4e89e1c	markerless motion capture in the crowd		This work uses crowdsourcing to obtain motion capture data from video recordings. The data is obtained by information workers who click repeatedly to indicate body configurations in the frames of a video, resulting in a model of 2D structure over time. We discuss techniques to optimize the tracking task and strategies for maximizing accuracy and efficiency. We show visualizations of a variety of motions captured with our pipeline then apply reconstruction techniques to derive 3D structure.	crowdsourcing;motion capture	Ian Spiro;Thomas Huston;Christoph Bregler	2012	CoRR		computer vision;simulation;computer science;multimedia	HCI	-34.94315373597997	-43.53853713810043	60401
5d055f082a09bdea7cd69fd30f862b20e0a6a051	vr study of human-multicopter interaction in a residential setting		We have built a virtual-reality simulator to design quadrotor robots that increase the independence of elderly persons living alone. The simulator lets us measure under what conditions a robot makes someone feel intrigued, or alarmed, or unconcerned, extending to nonhumanoids the studies of emotion that have been done for robots with facial expressions. This better model of how the virtual robot is perceived lets us agilely improve its appearance and behavior, containing the combinatorial explosion of parameters to study in costlier real-world experiments. The residence and the robot are rendered with Unity, and viewed with an Oculus Rift. Audio corresponding to flight maneuvers is resynthesized from recordings of an actual quadrotor. Flight dynamics using L1 adaptive control run in MATLAB and Simulink. Through menus operated by a Leap Motion hand tracker, the human subject commands the quadrotor to fly to various rooms (Unity waypoints), along precomputed paths that avoid collisions. The flight path is varied through a mathematical mapping to Laban Motion Factors (e.g., directness, sustainedness, lightness, and boundness) to provoke a range of responses. The subject’s emotion is estimated from heart rate, skin conductance, and head tilt from the Rift’s IMU (which is a quick metric for discomfort level). ∗e-mail: marinho@illinois.edu ACKNOWLEDGEMENTS This research is supported in part by the NSF’s National Robotics Initiative, and by the Graduate College at the University of Illinois at Urbana-Champaign. Additional help in making this video was provided by Naira Hovakimyan, Alex Kirlik, Frances Wang, Dusan Stipanovic, Amy LaViers, and Xiaofeng Wang.	audio signal processing;conductance (graph);experiment;ibm notes;matlab;oculus rift;precomputation;robot;robotics;simulation;simulink;unity;virtual reality;wang tile	Thiago Marinho;Arun Lakshmanan;Venanzio Cichella;Christopher Widdowson;Hang Cui;Robert Mitchell Jones;Bentic Sebastian;Camille Goudeseune	2016		10.1109/VR.2016.7504790	computer vision;simulation;artificial intelligence;operating system	Robotics	-45.123171052535284	-51.679537092941196	60419
a62218c6ef62a9207a1aba42ea348e41f1a9452f	a practical biosignal-based human interface applicable to the assistive systems for people with motor impairment	systeme commande;interfase usuario;sistema control;tecnologia electronica telecomunicaciones;entropia;control motor;integrated circuit;electroencefalografia;control maquina;user interface;taux erreur;man machine dialogue;implementation;electronic component;controle moteur;circuito integrado;membre;electroencephalographie;emg;machine control;human interface;control system;entropie;signal classification;classification signal;error rate;digital signal processor;dialogo hombre maquina;biosignal;contraccion;eeg;interface utilisateur;composant electronique;entropy;processeur signal numerique;commande machine;electroencephalography;tecnologias;procesador senal numerica;implementacion;grupo a;miembro;indice error;limb;circuit integre;eog;human computer interface;motor control;dialogue homme machine;componente electronico;contraction	An alternative human interface enabling the handicapped with severe motor disabilities to control an assistive system is presented. Since this interface relies on the biosignals originating from the contraction of muscles on the face during particular movements, even individuals with a paralyzed limb can use it with ease. For real-world application, a dedicated hardware module employing a general-purpose DSP was implemented and its validity tested on an electrically powered wheelchair. Furthermore, an additional attempt to reduce error rates to a minimum for stable operation was also made based on the entropy information inherent in the signals during the classification phase. In the experiments in which 11 subjects participated, it was found most of them could control the target system at their own will, and thus the proposed interface could be considered a potential alternative for the interaction of the severely handicapped with electronic systems.		Ki-Hong Kim;Jae-Kwon Yoo;Hong-Kee Kim;Wook-Ho Son;Soo-Young Lee	2006	IEICE Transactions	10.1093/ietisy/e89-d.10.2644	embedded system;entropy;simulation;electroencephalography;computer science;control system;artificial intelligence;operating system	HCI	-40.45509966588002	-45.38587524846365	60453
070cec02740885d9744ab500bdbf1acd415ff4d8	mining associations for interface design	association mining;user interface;personalization;online shopping;interface design;cluster analysis;clustering;rough sets;rough set;usability	Consumer research has indicated that consumers use compensatory and non-compensatory decision strategies when formulating their purchasing decisions. Compensatory decision-making strategies are used when the consumer fully rationalizes their decision outcome whereas non-compensatory decision-making strategies are used when the consumer considers only that information which has most meaning to them at the time of decision. When designing online shopping support tools, incorporating these decision-making strategies with the goal of personalizing the design of the user interface may enhance the overall quality and satisfaction of the consumer’s shopping experiences. This paper presents work towards this goal. The authors describe research that refines a previously developed procedure, using techniques in cluster analysis and rough sets, to obtain consumer information needed in support of designing customizable and personalized user interface enhancements. The authors further refine their procedure by examining and evaluating techniques in traditional association mining, specifically conducting experimentation using the Eclat algorithm for use with the authors’ previous work. A summary discussing previous work in relation to the new evaluation is provided. Results are analyzed and opportunities for future work are described.	algorithm;association rule learning;cluster analysis;decision support system;online shopping;personalization;purchasing;rough set;user interface	Timothy Maciag;Daryl H. Hepting;Dominik Slezak;Robert J. Hilderman	2007		10.1007/978-3-540-72458-2_13	rough set;computer science;knowledge management;machine learning;data mining;cluster analysis	HCI	-37.5643518061505	-51.510476188450475	60550
561cb703eab8b4fbf96c48116b6a2ff188f26a1d	sakurasensor: a system for realtime cherry-lined roads detection by in-vehicle smartphones	flowering cherries detection;participatory sensing;image analysis	SakuraSensor is a participatory sensing system for collecting short videos of cherry-lined roads by in-vehicle smartphones to provide scenery-based route recommendation services for comfortable driving. To detect good cherry-lined roads and record their videos automatically during driving, image analysis techniques are performed on user's smartphone, and the degree of flowering cherries (cherry intensity) along the roads are calculated in realtime. In this demonstration, we show how flowering cherries are detected from the videos along with the actual values of cherry intensity, and show a map-based user interface for viewing short videos recorded on the cherry-lined roads.	image analysis;participatory sensing;smartphone;user interface	Shogo Maenaka;Shigeya Morishita;Daichi Nagata;Morihiko Tamai;Keiichi Yasumoto;Toshinobu Fukukura;Keita Sato	2015		10.1145/2800835.2800892	image analysis;simulation;computer science;internet privacy	HCI	-38.9232587596137	-40.08369364106126	60563
58de1437df6add9286e8a2d309e637081d9d77a4	nosetapping: what else can you do with your nose?	gloves;touchscreen;design guidelines;winter;nosetapping	Touch-screen interfaces on smart devices became ubiquitous in our everyday lives. In specific contextual situations, capacitive touch interfaces used on current mobile devices are not accessible when, for example, wearing gloves during a cold winter. Although the market has responded by providing capacitive styluses or touchscreen-compatible gloves, these solutions are not widely accepted and appropriate in such particular situations. Using the nose instead of fingers is an easy way to overcome this problem. In this paper, we present in-depth results of a user study on nose-based interaction. The study was complemented by an online survey to elaborate the potential and acceptance of the nose-based interaction style. Based on the insights gained in the study, we identify the main challenges of nose-based interaction and contribute to the state of the art of design principles for this interaction style by adding two new design principles and refining one already existing principle. In addition, we investigated in the emotional effect of nose-based interaction based on the user experiences evolved during the user study.	mobile device;smart device;touchscreen;usability testing;wired glove	Ondrej Polácek;Thomas Grill;Manfred Tscheligi	2013		10.1145/2541831.2541867	simulation	HCI	-48.0328381855577	-41.54548737241113	60702
f4d6d843fa6476d2546400158bc27c4479c6304d	brake control assist on a four-castered walker for old people	adaptability;old people;four castered walker;brake control assist;gait analysis;assistive technology;it evaluation;point of view	Brake control assist for a four-castered walker for old people has been developed. The prototype system is equipped with a set of simple sensors, such as three-axis acceleration sensor and distance sensor. Our system estimates three walking states to control the application of the brake. Emergency strong brake can be predicted to prevent and/or forestall a fall. Individual characteristics related to brake control can be acquired within a short time and taken effect. In this paper, the implementation of brake control assist and its evaluations from performance and usability points of view are presented.		Tetsuya Hirotomi;Yasutomo Hosomi;Hiroyuki Yano	2008		10.1007/978-3-540-70540-6_190	control engineering;simulation;engineering;control theory	HCI	-40.44795938625021	-46.48327849156339	60883
3b3237a5a711fffab46a38e692287c0b4c58a2ff	spatial cognition of geometric figures in the context of proportional analogies	human cognition;spatial cognition;computer model;computational model for spatial cognition;adaptation;re representation;geometric proportional analogy;context	The cognition of spatial objects differs among people and is highly influenced by the context in which a spatial object is perceived. We investigated experimentally how humans perceive geometric figures in geometric proportional analogies and discovered that subjects perceive structures within the figures which are suitable for solving the analogy. Humans do not perceive the elements within a figure individually or separately, but cognize the figure as a structured whole. Furthermore, the perception of each figure in the series of analogous figures is influenced by the context of the whole analogy. A computational model which shall reflect human cognition of geometric figures must be flexible enough to adapt the representation of a geometric figure and produce a similarly structured representation as humans do while solving the analogy. Furthermore, it must be able to take into account the context, i.e. structures and transformations in other geometric figures in the analogy.	cognition;computation;computational model;do while loop;experiment;geometric modeling;humans	Angela Schwering;Kai-Uwe Kühnberger;Ulf Krumnack;Helmar Gust	2009		10.1007/978-3-642-03832-7_2	computer simulation;cognition;developmental psychology;computer science;artificial intelligence;communication;adaptation	Vision	-41.721281065082366	-51.99907623305053	60911
2d264711d395693d486e0a075cb20f49c65817b0	ambiculus: led-based low-resolution peripheral display extension for immersive head-mounted displays	peripheral vision;led displays;head mounted displays	Peripheral vision in immersive virtual environments is important for application fields that require high spatial awareness and veridical impressions of three-dimensional spaces. Head-mounted displays (HMDs), however, use displays and optical elements in front of a user's eyes, which often do not natively support a wide field of view to stimulate the entire human visual field. Such limited visual angles are often identified as causes of reduced navigation performance and sense of presence.  In this paper we present an approach to extend the visual field of HMDs towards the periphery by incorporating additional optical LED elements structured in an array, which provide additional low-resolution information in the periphery of a user's eyes. We detail our approach, technical realization, and present an experiment, in which we show that such far peripheral stimulation can increase subjective estimates of presence, and has the potential to change user behavior during navigation in a virtual environment.	anomalous experiences;head-mounted display;peripheral vision;spatial–temporal reasoning;virtual reality;visual basic[.net]	Paul Lubos;Gerd Bruder;Oscar Ariza;Frank Steinicke	2016		10.1145/2927929.2927939	computer vision;engineering;multimedia;computer graphics (images)	HCI	-44.27106495179764	-46.69403763619055	60977
6b6febc246ad6bd2e8945de06143a17c608138f9	advanced interaction in context	technologie communication;systeme intelligent;context aware;context information;personal digital assistant;real time;sistema inteligente;apprentissage conceptuel;telephone mobile;interactive method;mobile phone;user profile;aprendizaje conceptual;portable equipment;information appliance;intelligent system;situation awareness;concept learning;self organized map;communication technology;appareil portatif;aparato portatil;tecnologia comunicacion	Mobile information appliances are increasingly used in numerous different situations and locations, setting new requirements to their interaction methods. When the user's situation, place or activity changes, the functionality of the device should adapt to these changes. In this work we propose a layered real-time architecture for this kind of context-aware adaptation based on redundant collections of low-level sensors. Two kinds of sensors are distinguished: physical and logical sensors, which give cues from environment parameters and host information. A prototype board that consists of eight sensors was built for experimentation. The contexts are derived from cues using real-time recognition software, which was constructed after experiments with Kohonen's Self-Organizing Maps and its variants. A personal digital assistant (PDA) and a mobile phone were used with the prototype to demonstrate situational awareness. On the PDA font size and backlight were changed depending on the demonstrated contexts while in mobile phone the active user profile was changed. The experiments have shown that it is feasible to recognize contexts using sensors and that context information can be used to create new interaction metaphors.	backlight;british informatics olympiad;experiment;high- and low-level;information appliance;mobile phone;personal digital assistant;prototype;real-time clock;real-time locating system;requirement;scripting language;sensor;tea;teuvo kohonen;usability testing;user profile	Albrecht Schmidt;Kofi Asante Aidoo;Antti Takaluoma;Urpo Tuomela;Kristof Van Laerhoven;Walter Van de Velde	1999		10.1007/3-540-48157-5_10	situation awareness;computer vision;information and communications technology;simulation;concept learning;computer science;artificial intelligence;operating system;database;multimedia;information appliance;computer security	HCI	-34.824561234288964	-44.01999502210254	61276
0433438baf02894b0addfc57850ad39eccdfdd5f	knowledge engineering for unsupervised canine posture detection from imu data	classification algorithms;accelerometers;animal machine interfaces;canine training;animal computer interaction	"""Training animals is a process that requires a significant investment of time and energy on the part of the trainer. One of the most basic training tasks is to train dogs to perform postures on cue. While it might be easy for a human trainer to see when an animal has performed the desired posture, it is much more difficult for a computer to determine this. Most work in this area uses accelerometer and/or gyroscopic data to produce data from an animal's current state, but this has limitations. Take for example a normal standing posture. From an accelerometer's perspective, it closely resembles the """"laying down"""" posture, but the posture can look very different if the animal is standing still, versus walking, versus running, and might look completely different from a """"standing on incline"""" posture. A human trainer can instantly tell the difference between these postures and behaviors, but the process is much more difficult for a computer.  This paper demonstrates several algorithms for recognizing canine postures, as well as a system for building a computational model of a canine's potential postures, based solely on skeletal measurements. Existing techniques use labeled data, which can be difficult to acquire. We contribute a new technique for unsupervised posture detection, and compare the supervised technique to our new, unsupervised technique. Results indicate that the supervised technique performs with a mean 82.06% accuracy, while our unsupervised approach achieves a mean 74.25% accuracy, indicating that in some cases, our new unsupervised technique is capable of achieving comparable performance."""	algorithm;computation;computational model;cue tone;fibre optic gyroscope;knowledge engineering;poor posture;unsupervised learning	Michael Winters;Rita Brugarolas;John Majikes;Sean Mealin;Sherrie Yuschak;Barbara L. Sherman;Alper Bozkurt;David L. Roberts	2015		10.1145/2832932.2837015	computer vision;simulation;engineering;communication	HCI	-36.881708639855844	-44.45633327337453	61446
9f775dd228dec5f179bf4d80d054c5dc36f12191	taking shortcuts: embedded physical interfaces for spatial navigation	spatial mapping;interactive embedded interfaces;physical embodiment;physical interface;spatial relationships;physical interaction;point of interest;spatial navigation	Designing for embodied physical interaction is just as important at a coarse level of spatial navigation as in the minutiae of object exploration. We created interactive embedded interfaces called 'Navitiles' that can be suspended in a floor to support navigation of a building. Our design uses capacitance and RFID sensors to determine users' location and LEDs to indicate possible directions. We determine whether Navitile cues could help users understand spatial relationships between points of interest. We based our study on a previous experiment that used a simulated VR maze to test whether users were able to exhibit 'shortcut' behaviour that would indicate the formation of spatial maps. Our hypothesis was that the physicality of embodied spatial navigation directed by the Navitiles in a real maze would enable users to achieve similar spatial shortcut behaviours to those found in the virtual task. We found significant evidence that sufficient spatial knowledge was acquired to enable successful shortcut performance between unexplored routes. However, further work is required to measure the effect of physical body movement on spatial skills development.	embedded system;fundamental interaction;keyboard shortcut;map;minutiae;point of interest;sensor;spatial navigation	Douglas Boari;Mike Fraser	2009		10.1145/1517664.1517706	simulation;engineering;multimedia;communication	HCI	-45.5896928614506	-45.133301985143476	61909
0b206439bfa4862a80a9d1aad9bf556dd03a2c96	selection of unknown objects specified by speech using models constructed from web images	object recognition;real environment unknown objects web images human voice household robots image models speech recognition;web images;multimodality;multimodality object recognition web images;speech processing internet service robots;speech speech recognition robots image recognition accuracy hidden markov models logistics	To select an object requested by human voice among several unknown objects is one of the important tasks for household robots that assist human's daily lives. In this paper, we propose a method that can achieve this task. Using image models which are constructed by Web images collected from the results of speech recognition, the proposed method enables a robot to select an object specified by a speech from several unknown objects in a real environment.	robot;speech recognition	Hitoshi Nishimura;Yuko Ozasa;Yasuo Ariki;Mikio Nakano	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.91	speaker recognition;computer vision;speech recognition;computer science;cognitive neuroscience of visual object recognition;multimedia	Robotics	-36.03212264754382	-44.423816331208144	62179
20c2cb80e28197d29e65fd6662b2b416938694a9	dt controls: adding identity to physical interfaces	auditing;touch screen;sensory feedback;multi user;haptics;physical interfaces;diamondtouch;physical interface;identity	In this paper, we show how traditional physical interface components such as switches, levers, knobs and touch screens can be easily modified to identify who is activating each control. This allows us to change the function per-formed by the control, and the sensory feedback provided by the control itself, dependent upon the user. An auditing function is also available that logs each user's actions. We describe a number of example usage scenarios for our tech-nique, and present two sample implementations.	feedback;network switch;touchscreen	Paul H. Dietz;Bret Harsham;Clifton Forlines;Darren Leigh;William S. Yerazunis;Sam Shipman;Bent Schmidt-Nielsen;Kathy Ryall	2005		10.1145/1095034.1095075	human–computer interaction;computer science;artificial intelligence;multimedia;haptic technology;audit;world wide web	HCI	-44.81526659331385	-43.73151831830668	62182
82fc17b24dbd93f7f68c93f1adfe638c1e52a821	nailo: fingernails as an input surface	beauty technology;capacitive touch;printed electronics;wearable electronics;article;gesture recognition	We present NailO, a nail-mounted gestural input surface. Using capacitive sensing on printed electrodes, the interface can distinguish on-nail finger swipe gestures with high accuracy (>92%). NailO works in real-time: we miniaturized the system to fit on the fingernail, while wirelessly transmitting the sensor data to a mobile phone or PC. NailO allows one-handed and always-available input, while being unobtrusive and discrete. Inspired by commercial nail stickers, the device blends into the user's body, is customizable, fashionable and even removable. We show example applications of using the device as a remote controller when hands are busy and using the system to increase the input space of mobile phones.	capacitive sensing;mobile phone;printing;real-time clock;remote control;removable media;transmitter;unobtrusive javascript	Hsin-Liu Cindy Kao;Artem Dementyev;Joseph A. Paradiso;Chris Schmandt	2015		10.1145/2702123.2702572	embedded system;computer hardware;computer science;printed electronics;gesture recognition;capacitive sensing	HCI	-42.92608566509277	-41.878802019969996	62268
2a73a3c2eb977dda3a570b2c9e71b1a0970d166e	gestkeyboard: enabling gesture-based interaction on ordinary physical keyboard	modeless interaction;physical keyboard;gesture detection	Stroke gestures are intuitive and efficient but often require gesture-capable input hardware such as a touchscreen. In this paper, we present GestKeyboard, a novel technique for gesturing over an ordinary, unmodified physical keyboard that remains the major input modality for existing desktop and laptop computers. We discuss an exploratory study for understanding the design space of gesturing on a physical keyboard and our algorithms for detecting gestures in a modeless way, without interfering with the keyboard's major functionality such as text entry and shortcuts activation. We explored various features for detecting gestures from a keyboard event stream. Our experiment based on the data collected from 10 participants indicated it is feasible to reliably detect gestures from normal keyboard use, 95% detection accuracy within a maximum latency of 200ms.	algorithm;biconnected component;desktop computer;gesture recognition;input device;keyboard shortcut;language model;laptop;modality (human–computer interaction);mode (computer interface);sensor;touchscreen;usability testing	Haimo Zhang;Yang Li	2014		10.1145/2556288.2557362	footmouse;computer hardware;computer science;gesture recognition;multimedia	HCI	-46.56372911608935	-44.612041086355354	62309
16c307431c0009ca763a9613e5384cb46b8e1a96	special issue on learning for human-robot collaboration		Once isolated behind safety fences, robots are now making their way into spaces shared with people: not only on manufacturing production lines, but also our houses, museums, or hospitals. In these spaces, collaboration between humans and robots becomes vital, but also raises new challenges for robotics algorithms. A collaborative robot must be able to assist humans in a large diversity of tasks, understand its collaborator’s intentions as well as communicate its own, predict human actions to adapt its behavior accordingly, and decide when it can lead the task or instead follow the user. All these aspects require the robot to adapt. It needs to execute different tasks, and rapidly adapt to the user’s actions and requirements. This adaptation	algorithm;cobot;humans;requirement;robot;robotics	Leonel Dario Rozo;Heni Ben Amor;Sylvain Calinon;Anca D. Dragan;Dongheui Lee	2018	Auton. Robots	10.1007/s10514-018-9756-z	computer science;simulation;human–robot interaction	Robotics	-36.53138636728987	-40.353112479769855	62441
3efe32b372b0631f9611ef8262c1a1fe3cf8ba52	alternative strategies in processing 3d objects diagrams: static, animated and interactive presentation of a mental rotation test in an eye movements cued retrospective study		Spatial abilities involved in reasoning with diagrams have been assessed using tests supposed to require mental rotation (cube figures of the Vandenberg & Kruse). However, Hegarty (2010) described alternative strategies: Mental rotation is not always used; analytical strategies can be used instead. In this study, we compared three groups of participants in three external formats of presentation of the referent figure in the Vandenberg & Kruse test: static, animated, interactive. During the test, participants were eye tracked. After the test, they were interrogated on their strategies for each item during the viewing of the replay of their own eye movement in a cued retrospective verbal protocol session. Results showed participants used varied strategies, part of them similar to those shown by Hegarty. Presentation format influenced significantly the strategy. Participants with high performance at the test used more mental rotation. Participants with lower performance tended to use more analytical strategies than mental rotation.	diagram;interactivity;turing test	Jean-Michel Boucheix;Madeline Chevret	2014		10.1007/978-3-662-44043-8_17	simulation;computer science;communication;social psychology	HCI	-46.960023225368694	-48.59182019737562	62488
5b60c22ff53dc603ff1ddeec4558e38d986c4ef3	two layers action integration for hri - action integration with attention focusing for interactive robots	integrable system;motor system;action selection	Behavior architectures are widely used to program interactive robots. In these architectures multiple behaviors are usually running concurrently so a mechanism for integrating the resulting actuation commands from these behaviors into actual actuation commands sent to the robot’s motor system must be faced. Different architectures proposed different action integration mechanisms that range from distributed to central integration. In this paper an analysis of the special requirements that HRI imposes on the action integration system is given. Based on this analysis a novelle hybrid action integration mechanism that combines distributed attention focusing with a fast central integration algorithm is presented in the framework of the EICA architecture. The proposed system was tested in a simulation of a listener robot that aimed to achieve human-like nonverbal listening behavior in real world interactions. The analysis of the system showed that the proposed mechanism can generate coherent human-like behavior while being robust against signal correlated noise.	algorithm;artificial intelligence;autonomy;coherence (physics);data haven;embodied agent;human–robot interaction;pacific rim;requirement;robot;simulation;situated	Yasser F. O. Mohammad;Toyoaki Nishida	2008			integrable system;computer vision;simulation;action selection;computer science;artificial intelligence;motor system	Robotics	-35.34642813447021	-40.156824288891016	62633
3ed3befdad33381febd06f053ff04b6a527cac4b	creating virtual 3d see-through experiences on large-size 2d displays	i 3 7 computer graphics three dimensional graphics and realism virtual reality see through window infra red tracking distributed rendering tiled display h 5 1 information interfaces and presentation multimedia information systems artificial augmented and virtual realities h 5 2 information interfaces and presentation user interfaces input devices and strategies;tiled display;input devices and strategies;multiple infrared cameras;large size 2d displays;infra red tracking;user interface;h 5 1 information interfaces and presentation multimedia information systems artificial;prototypes;real time;h 5 1 information interfaces and presentation multimedia information systemsâ artificial augmented and virtual realities;virtual reality;two dimensional displays;see through window;layout;multimedia information system;infra red;augmented;computer graphic;3d position tracking;visualization;graphical user interfaces;engines;distributed rendering;virtual reality graphical user interfaces rendering computer graphics;three dimensional displays;i 3 7 computer graphics three dimensional graphics and realismâ virtual reality;and virtual realities;two dimensional displays large screen displays layout three dimensional displays tracking cameras rendering computer graphics graphics engines prototypes;3d graphics engines;i 3 7 computer graphics three dimensional graphics and realism virtual reality;virtual 3d see through experiences;h 5 2 information interfaces and presentation user interfacesâ input devices and strategies;three dimensional graphics and realism;display screen;h 5 2 information interfaces and presentation user interfaces input devices and strategies;information interfaces and presentation;large screen displays;rendering computer graphics;rendering virtual 3d see through experiences large size 2d displays display screen 3d position tracking 3d motion tracking multiple infrared cameras 3d graphics engines;3d graphics;cameras;graphics;tracking;3d motion tracking;rendering	This paper describes a novel approach for creating virtual 3D see-through experiences on large-size 2D displays. The approach aims at simulating the real-life experience of observing an outside world through one or multiple windows. Here, the display screen serves as a virtual window such that viewers moving in front of the display can observe different part of the scene which is also moving in the opposite direction. In order to generate this see-through experience, a virtual 3D scene is first created and placed behind the display. Then, the viewers' 3D positions and motion are tracked by a 3D viewer tracking method based on multiple infra-red cameras. The virtual scene is rendered by 3D graphics engines in real-time speed, based on the tracked viewer positions. A prototype system has been implemented on a large-size tiled display and was able to bring viewers a realistic and natural 3D see-through experience.	3d computer graphics;graphics library;microsoft windows;prototype;real life;real-time transcription;simulation;stereoscopy;virtual reality	Chang Yuan	2009	2009 IEEE Virtual Reality Conference	10.1109/VR.2009.4811033	layout;computer vision;visualization;infrared;rendering;computer science;graphics;graphical user interface;virtual reality;prototype;tracking;multimedia;virtual retinal display;user interface;computer graphics (images)	Visualization	-42.61804375154964	-38.48672591267216	62661
0cccc607dd106a6a0a115882106a893bea0f4df3	the magic barrier tape: a novel metaphor for infinite navigation in virtual worlds with a restricted walking workspace	3d interaction;virtual reality;interaction metaphor;rate control;navigation;hybrid position rate control;walking workspace;virtual worlds	"""In most virtual reality simulations the virtual world is larger than the real walking workspace. The workspace is often bounded by the tracking area or the display devices. This paper describes a novel interaction metaphor called the Magic Barrier Tape, which allows a user to navigate in a potentially infinite virtual scene while confined to a restricted walking workspace. The technique relies on the barrier tape metaphor and its """"do not cross"""" implicit message by surrounding the walking workspace with a virtual barrier tape in the scene. Therefore, the technique informs the user about the boundaries of his walking workspace, providing an environment safe from collisions and tracking problems. It uses a hybrid position/rate control mechanism to enable real walking inside the workspace and rate control navigation to move beyond the boundaries by """"pushing"""" on the virtual barrier tape. It provides an easy, intuitive and safe way of navigating in a virtual scene, without break of immersion. Two experiments were conducted in order to evaluate the Magic Barrier Tape by comparing it to two state-of-the-art navigation techniques. Results showed that the Magic Barrier Tape was faster and more appreciated than the compared techniques, while being more natural and less tiring. Considering it can be used in many different virtual reality systems, it is an interaction metaphor suitable for many different applications, from the entertainment field to training simulations scenarios."""	display device;experiment;haptic technology;immersion (virtual reality);institute for operations research and the management sciences;simulation;virtual reality;virtual world;workspace	Gabriel Cirio;Maud Marchal;Tony Regia-Corte;Anatole Lécuyer	2009		10.1145/1643928.1643965	navigation;simulation;computer science;artificial intelligence;virtual reality;multimedia;computer graphics (images)	Visualization	-45.55654501202445	-43.257188558570675	62690
6071ecd9ce51863d128830e9764cbe7d2eef0115	pervasive computing		Emerging electric-drive vehicles, such as hybrid electric vehicles (HEVs) and plug-in HEVs (PHEVs), hold the potential for substantial reduction of fuel consumption and greenhouse gas emissions. User driving behavior, which varies from person to person, can significantly affect (P)HEV operation and the corresponding energy and environmental impacts. Although some studies exist that investigate vehicle performance under different driving behaviors, either directed by vehicle manufacturers or via on-board diagnostic (OBD) devices, they are typically vehicle-specific and require extra device/effort. Moreover, there is no or very limited feedback to an individual driver regarding how his/her personalized driving behavior affects (P)HEV performance. This paper presents a personalized driving behavior monitoring and analysis system for emerging hybrid vehicles. Our design is fully automated and non-intrusive. We propose phone-based multi-modality sensing that captures precise driver–vehicle information through de-noise, calibration, synchronization, and disorientation compensation. We also provide quantitative driver-specific (P)HEV analysis through operation mode classification, energy use and fuel use modeling. The proposed system has been deployed and evaluated with real-world user studies. System evaluation demonstrates highly-accurate (0.88-0.996 correlation and low error) driving behavior sensing, mode classification, energy use and fuel use modeling.	modality (human–computer interaction);on-board data handling;personalization;plug-in (computing);ubiquitous computing;usability testing	Judy Kay Paul Lukowicz Hideyuki Tokuda;P. Krüger	2012		10.1007/978-3-642-31205-2		HCI	-42.70749425581283	-45.825186181917665	62741
a2863b6d4a72279446c3e75087f96a0918592699	smart signage: an interactive signage system with multiple displays	advertising data processing;smart phones;smart phones azimuth wireless communication algorithms time factors intelligent sensors;many to many interaction digital signs traditional static signs replacement out of home advertising smartphones interactive signage systems advertising purpose drag gable cyber physical broadcast multicast media system intuitive dragging hand gesture smart signage infrastructure smartphone embedded orientation sensors target signage display one to many interaction;smart phones advertising data processing	"""Digital signs (e.g. electronic billboards), as the replacement of traditional static signs (e.g. paper posters), have gained increasing popularity, especially for out-of-home advertising. The pervasiveness of the smartphones makes it possible to implement new interactive signage systems for advertising purpose. A new drag gable cyber-physical broadcast/multicast media system, Smart Signage, is proposed in our previous work. It allows multiples users simultaneously obtaining the displayed content with an intuitive ``dragging"""" hand gesture. With the increasing number of deployed digital signs, it is possible that there are multiple signs in one location. In this paper, a new approach is proposed using the existing Smart Signage infrastructure to allow smartphones to interact with multiple digital signs in one location. This approach differentiates signs by the orientations of the signage displays, which are measured by smartphone-embedded orientation sensors. A user can interact with the intended sign by simply pointing his/her smartphone at the target signage display. With this new approach, Smart Signage successfully extends from one-to-many interaction to many-to-many interaction."""	bit error rate;cyber-physical system;degraded mode;drag and drop;embedded system;experiment;many-to-many;multicast;one-to-many (data model);response time (technology);sensor;signage systems;smart tv;smartphone	James She;Jon A Crowcroft;Hao Fu;Pin-Han Ho	2013	2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing	10.1109/GreenCom-iThings-CPSCom.2013.133	embedded system;multimedia;internet privacy;computer security	Visualization	-42.72281528368699	-42.323596186913676	62974
e5e27bd9cd7e148ea1a84ab570064b3812b73046	sensecap: synchronized data collection with microsoft kinect2 and leapmotion	skeletal tracking;rgb d;leapmotion;multi media data collection;kinect2	We present a new recording tool to capture synchronized video and skeletal data streams from cheap sensors such as the Microsoft Kinect2, and LeapMotion. While other recording tools act as virtual playback devices for testing on-line real-time applications, we target multi-media data collection for off-line processing. Images are encoded in common video formats, and skeletal data as flat text tables. This approach enables long duration recordings (e.g. over 30 minutes), and supports post-hoc mapping of the Kinect2 depth video to the color space if needed. By using common file formats, the data can be played back and analyzed on any other computer, without requiring sensor specific SDKs to be installed. The project is released under a 3-clause BSD license, and consists of an extensible C++11 framework, with support for the official Microsoft Kinect 2 and LeapMotion APIs to record, a command-line interface, and a Matlab GUI to initiate, inspect, and load Kinect2 recordings.	bsd;c++11;color space;command-line interface;graphical user interface;hoc (programming language);kinect;matlab;online and offline;real-time clock;sensor;software development kit;video file format	Julian F. P. Kooij	2016		10.1145/2964284.2973805	microsoft video 1;computer hardware;computer science;operating system;world wide web	OS	-39.16109194301853	-39.08421732356502	63001
06a76a1674589b77312a2d48e7812371640626f3	getting off the treadmill: evaluating walking user interfaces for mobile devices in public spaces	dynamic change;walking user interface;pilot study;user evaluation;mobile device;user interface;design knowledge;user interface adaptation;public space;media player;adaptive user interface;situational impairments	Using a mobile device while moving limits attention and motor ability and can result in reduced performance. Mobile devices that can sense and adapt to contextual factors such as movement may reduce this performance deficit. We performed two studies evaluating the feasibility of walking user interfaces (WUIs) that adapt their layout when the user is moving. In a pilot study with 6 users, we evaluated the effects of different button sizes on performance when walking while using a portable music player. Results showed significant interactions between size and movement. In the second study, 29 users evaluated the performance of a WUI that dynamically changed button sizes as the user moved. Results show that our dynamic user interface performs at the level of its component static interfaces without any additional penalty due to adaptation. This work adds to our design knowledge about walking user interfaces and provides lessons learned in evaluating mobile devices while walking in public spaces.	interaction;mobile device;user interface	Shaun K. Kane;Jacob O. Wobbrock;Ian E. Smith	2008		10.1145/1409240.1409253	user interface design;user;simulation;user modeling;human–computer interaction;computer science;operating system;mobile device;multimedia;natural user interface;user interface	HCI	-46.946196948730666	-45.00034483286897	63047
3b0ad9132e16f2d7e91e1a951cc012c26d9a61f2	haptic attributes and human motor skills	motor skills;haptic attributes motor skill training dynamic models;training;dynamic model;haptic attributes;human subjects;dynamic models;motor skill;haptic interfaces humans virtual reality writing education force feedback instruments veins motor drives muscles	Most human fine motor skills involve the use of a tool to complete the given task. The task to be completed is often defined in terms of some desired trajectory and humans generate certain forces to achieve the desired trajectory. In this paper we describe our efforts to experimentally and theoretically classify forces associated with these skilled tasks. The idea is then to develop a system that can capture and playback exhibition of skill through haptics from person to person. Our preliminary results from experiments show that the forces generated by an individual are unique for a given task. Furthermore, for a given person, these forces show little variance over repeated exhibition of the same task. A virtual writing simulator was used to collect data from human subjects. The significance of this research is that not only was the forces generated by a person for a given skill unique, but is also verifiably different from another human subject performing the same task.	experiment;haptic technology;simulation	Govindarajan Srimathveeravalli;Venkatraghavan Gourishankar;Thenkurussi Kesavadas	2006	2006 14th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems	10.1109/HAPTICS.2006.163	simulation;engineering;multimedia;communication	Robotics	-47.58946680333497	-50.36420046302096	63122
00f787b0b34e8a0bcdd4e29af3e9d814ba77b84b	visual search	experimental lab study;reaction time;reading;visual perception	Broadly defined, visual search is the act of looking for something or a number of things. It is reasonable, at the start of a survey of the topic, to ask why we have to search at all. Part of the answer is obvious, part is more involved, and all of the answer has to do with the fact that we are creatures with limitations. To begin with the obvious, our eyes are seeing only part of the world around us at the present moment. The world extends 360 degrees around our heads, but our eyes have a visual field of a bit over 180 degrees in the horizontal dimension and, depending on the configuration of our faces, about 90 degrees in the vertical dimension. If the target of search is outside the current visual field, we will need to move to point our eyes in the right direction. Within that large visual field, processing is hugely uneven. In the fovea, each individual photoreceptor commands what amounts to its own optic nerve fiber. If there were one optic nerve fiber for every photoreceptor in the retina, the optic nerve would be too fat to be practical. This constraint and others result in extensive pooling of information in the peripheral visual field. As a result, visual resolution falls off dramatically as we move away from the fovea (Green, 1970). If detection requires more than very coarse processing—for example, if one wants to read any letter on this page—the target must be brought to the fovea by moving the eyes. It is important to note that while fixation of the targets of search is common and often necessary, it is not required in all cases. Targets can be detected in peripheral vision as long as their defining features can be resolved in peripheral vision. Indeed, many studies of search involve requiring observers to hold fixation at one point while targets and distractors are presented at peripheral locations (e.g., Braun & Julesz, 1998; Carrasco, Evert, Chang, & Katz, 1995; Wolfe, O’Neill, & Bennett, 1998). On the right side of Figure 13.1, if you fixate on the central “x,” you should be able to search successfully for the letter “c” without moving your eyes. When a basketball player is praised for his peripheral vision, this does not mean that he can read the newspaper 15 degrees from fixation. It is more likely to mean that he has trained himself to fixate at one location while successfully searching for the receiver of his next pass by deploying his attention to his peripheral visual field. The decline in resolution is only part of the problem in peripheral vision. Targets that are large enough to be resolved may still be difficult or impossible to identify in peripheral vision because of the crowding effects of other, nearby contours (Levi, Klein, & Aitsebaomo, 1985). If you look at the “x” at the center of the array on the left of	accidental falls;crowding;eye;face;fiber-optic communication;nerve fibers;optic nerve (gchq);optical fiber;peripheral vision;platelet glycoprotein 4, human;retina;structure of fovea centralis	Jeremy M. Wolfe;Todd Horowitz	2008	Current Biology	10.1016/j.cub.2010.02.016		Vision	-41.91396287525471	-50.421569549018685	63394
5795de251c6b2a9accffbd8a3cc35e88b2ecb47a	finger or stylus: their impact on the performance of on-line signature verification systems			stylus (computing)	Margit Antal;András Bandi	2017		10.1515/macro-2017-0002	stylus;computer hardware;computer science	Logic	-42.84003199848111	-41.471957503803104	63569
a5eed1967d635f58b83993b3fcaa55ed73653b21	multiview shooting geometry for multiscopic rendering with controlled distortion		A fundamental element of stereoscopic and/or autostereoscopic image production is the geometrical analysis of the shooting and viewing conditions in order to obtain a qualitative 3D perception experience. This paper firstly compares the perceived depth with the shot scene depth from the viewing and shooting geometries for a couple of shooting and rendering devices. This yields a depth distortion model whose parameters are expressed from the geometrical characteristics of shooting and rendering devices. Secondly, these expressions are inverted in order to design convenient shooting layouts yielding chosen distortions on specific rendering devices. Thirdly, this design scheme provides three shooting technologies (3D computer graphics software, photo rail, and camera box system) producing qualitative 3D content for various kinds of scenes (real or virtual, still or animated), complying with any prechosen distortion when rendered on any specific multiscopic technology or device formerly specified.		Jessica Prévoteau;Sylvia Chalençon-Piotin;Didier Debons;Laurent Lucas;Yannick Rémion	2010	Int. J. Digital Multimedia Broadcasting	10.1155/2010/975674	computer vision;simulation;computer graphics (images)	Graphics	-41.2841590783449	-39.55122418441679	63652
2b230aaddc530a7a782ea6c7b24045eddf9289a9	mind over mouse: the effect of cognitive load on mouse movement behavior	human computer interaction;cognitive load;mouse dynamics	Identifying when users are experiencing heightened cognitive load has a number of important applications for businesses, online education, and personal computer use. In this paper, we present a method for detecting heightened cognitive load using a technique known as mouse dynamics (MD). We provide an overview of cognitive science literature linking changes in cognitive states to interruptions of fine motor control to provide a theoretical base for our work. We then describe a laboratory experiment in which low, medium, and high cognitive load were elicited while MD features were captured. We found that participants exhibited longer task duration, longer mouse movements, more direction changes, and slower speed when under high cognitive load.		Mark Grimes;Joseph S. Valacich	2015			simulation;computer science;artificial intelligence;cognitive load	HCI	-48.21353885223073	-50.38800072200751	63656
d964efb74aeb69ac7151919922ddfa343e02f798	particle display system - virtually perceivable pixels with randomly distributed physical pixels		In this study, the authors propose and implement a particle display system (PDS) that consists of hundreds of randomly distributed pixels. The wireless capability of this system enables each node to move freely without distant limitation of the use of wire cables. The authors also propose effective visual presentation techniques for a display system with randomly distributed pixels. One of the optimization techniques involves the extension of a well-known phenomenon where humans can perceive two-dimensional static or moving images from a set of high-frequency flashing one-dimensional pixel arrays, such as LED arrays, as a characteristic of a human's vision system. While this technique can only extend the virtual resolution of a display in a direction perpendicular to the aligned pixels, our technique enables the display of multi-directional scrolling of two-dimensional images with randomly distributed pixels. In addition, the advantages of presenting information on a display with nonuniform pixel distribution and virtual pixels with fast flash of pixels are discussed. The proposed techniques help in reducing the cost of installing a large-scale display and the time taken for the initial preparation of the setup, which involves carrying large pixel arrays and determining the precise size and shape of the display.	pixel;randomness	Munehiko Sato;Atsushi Hiyama;Tomohiro Tanikawa;Michitaka Hirose	2009	JIP	10.2197/ipsjjip.17.280	computer vision;computer hardware;computer science;960h technology;native resolution;computer graphics (images)	HCI	-40.666641730996	-39.188401110473755	63676
40c22110be970c78cc18ce97e12d87a1a1548f10	designing an immersive and entertaining pervasive gameplay experience with spheros as game and interface elements	natural mappings;gaming experience;immersion;pervasive gaming;controllers;sphero;robotic ball	The Sphero is a robotic remote-controlled ball capable of rolling around on its own in any direction at multiple speeds. Numerous games have been designed for the Sphero for smartphones and tablets. However, most of these games provide an interface for controlling the Sphero that is far from natural. These games also do not put a strong focus on the physical environment around the Sphero. This work discusses a control scheme used to control a Sphero with another Sphero, and a pervasive game leveraging this scheme that emphasizes physical properties of the environment to create an immersive experience.	pervasive informatics;remote control;robot;smartphone;tablet computer	Brennan Jones;Kody R. Dillman;Setareh Aghel Manesh;Ehud Sharlin;Anthony Tang	2014		10.1145/2658537.2661301	simulation;human–computer interaction;engineering;multimedia	HCI	-45.63772223910078	-39.797665026495615	63699
ba4dd6651a0177d6af80ce0e9a549590b28ae745	low force pressure measurement: pressure sensor matrices for gesture analysis , stiffness recognition and augmented instruments		The described project is a new approach to use high ly sensitive low force pressure sensor matrices for malposition, cramping and tension of hands and fingers, gesture and keystroke analysis and for new musical expression. In the latter, sensors are used as additional touch sensitive switches and keys. In pe dagogical issues, new ways of technology enhanced teaching, s elf teaching and exercising are described. The used sensors are custom made in collaboration with the ReactiveS Sensorlab.	event (computing);network switch;sensor;stiffness;touchscreen	Tobias Großhauser	2008				HCI	-42.04381772601906	-43.3698571512156	63869
e89f24c36ed99970c9bee94909ebe4c618f1cc0d	a multimodal human-machine interface enabling situation-adaptive control inputs for highly automated vehicles		Intelligent vehicles operating in different levels of automation require the driver to fully or partially conduct the dynamic driving task (DDT) and to conduct fallback performance of the DDT, during a trip. Such vehicles create the need for novel human-machine interfaces (HMIs) designed to conduct high-level vehicle control tasks. Multimodal interfaces (MMIs) have advantages such as improved recognition, faster interaction, and situation-adaptability, over unimodal interfaces. In this study, we developed and evaluated a MMI system with three input modalities; touchscreen, hand-gesture, and haptic to input tactical-level control commands (e.g. lane-changing, overtaking, and parking). We conducted driving experiments in a driving simulator to evaluate the effectiveness of the MMI system. The results show that multimodal HMI significantly reduced the driver workload, improved the efficiency of interaction, and minimized input errors compared with unimodal interfaces. Moreover, we discovered relationships between input types and modalities: location-based inputs-touchscreen interface, time-critical inputs-haptic interface. The results proved the functional advantages and effectiveness of multimodal interface system over its unimodal components for conducting tactical-level driving tasks.	device driver;driving simulator;experiment;haptic technology;high- and low-level;human–computer interaction;lateral thinking;multimodal interaction;simulation;touchscreen;user interface;window of opportunity	Udara E. Manawadu;Mitsuhiro Kamezaki;Masaaki Ishikawa;Takahiro Kawano;Shigeki Sugano	2017	2017 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2017.7995875	vehicle dynamics;human–machine interface;automation;workload;adaptive control;haptic technology;touchscreen;control engineering;driving simulator;engineering	HCI	-46.92792038026297	-46.83278121242604	63914
0ddfd32f52eee85f20a19b72b680a5096b52536a	tactile sensing system including bidirectionality and enhancement of haptic perception by tactile feedback to distant part	tactile sensors haptic interfaces white noise surgery prototypes;tactile sensors haptic interfaces surgery;laparoscopic surgery;bidirectionality;tactile sensor;prototypes;forceps manipulation;laparoscopic surgery tactile sensor bidirectionality haptic perception;exploratory movements;tactile sensing system;tactile feedback;tactile display;surgery;exploration;tactile sensors;forceps manipulation tactile sensing system bidirectionality haptic perception tactile feedback exploratory movements exploration laparoscopic surgery tactile sensor tactile display;haptic interfaces;haptic perception;white noise	Haptic perception is bidirectionally related to the exploratory movements. This means that exploration influences perception, but also perception influences exploration. We can optimize/change the exploratory movements according to the perception and/or the task, consciously or unconsciously. In this paper, a tactile sensing system including bidirectionality is proposed and its potential and application to laparoscopic surgery are discussed. The proposed system is composed of a tactile sensor and a tactile display. The tactile sensor is mounted on a forceps, and the tactile feedback based on the sensor output is provided by using the tactile display. The user can feel sensor outputs and adjust the forceps manipulation according to the environment and the target. This leads to collect adequate sensor outputs. Implementing the bidirectionality gets the most out of the capability of the tactile sensor. Furthermore, the developed system might have an additional effect due to using the tactile display to the other hand, not to the hand with the forceps. It is an enhancement of haptic perception by tactile feedback to distant part. The developed system is introduced and establishment of the bidirectionality and its effect are discussed. Psychophysical experiments on the enhancing effect are conducted. Experimental results show a potential of the proposed tactile sensing system.	bi-directional text;consciousness;duplex (telecommunications);experiment;javaserver pages;lumped element model;numerical analysis;prototype;signal processing;tactile sensor	Yoshihiro Tanaka;Takanori Nagai;Masamichi Sakaguchi;Michitaka Fujiwara;Akihito Sano	2013	2013 World Haptics Conference (WHC)	10.1109/WHC.2013.6548399	computer vision;computer science;tactile sensor;statistics	Robotics	-42.58906383636645	-44.39134075050093	63983
3872603efdf9ced266876129f31581aa5126f8e9	multi-view operator control unit to improve situation awareness in usar missions	cameras feeds collision avoidance robot vision systems training;high fidelity tunnel accident simulation multiview operator control unit situation awareness usar missions urban search and rescue multiview multimodal ocu man portable ground robot fighting training center;mobile robots;human robot interaction;accidents;mobile robots accidents emergency services human robot interaction;emergency services	In urban search and rescue (USAR) missions, manually controlling robots is difficult, in large part due to low situation awareness (SA) provided by operator control units (OCUs). This paper looks at state-of-the-art OCUs to identify seven fundamental problems to be resolved. Next, the design and implementation of a multi-view multi-modal OCU are presented. This OCU follows a large set of design guidlines and also features novel techniques for a human operator to remotely interact with a man-portable ground robot. The system was evaluated in a high fidelity tunnel accident simulation at a fire fighting training center. The OCU allowed training and collisions to remain low, while SA was improved. Qualitative observations are also discussed, such that end-users do not often choose the optimal views in the OCU for the tasks at hand.	camera phone;collision detection;control unit;experiment;modal logic;multi-touch;robot;simulation;unmanned aerial vehicle;user modeling	Benoit Larochelle;Geert-Jan M. Kruijff	2012	2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2012.6343896	human–robot interaction;mobile robot;computer vision;simulation;computer science;artificial intelligence;computer security	Robotics	-36.8861170540983	-41.5173559680054	64026
a94b11ed2141d9dd61a185908e3ae0624c219f05	the design of a gui paradigm based on tablets, two-hands, and transparency	marking menu;divided attention;tablets;marking menus;toolglass;lessons learned;task integration;transparency;two handed input;visual attention	An experimental GUI paradigm is presented which is based on the design goals of maximizing the amount of screen used for application data, reducing the amount that the UI diverts visual attentions from the application data, and increasing the quality of input. In pursuit of these goals, we integrated the non-standard UI technologies of multi-sensor tablets, toolglass, transparent UI components, and marking menus. We describe a working prototype of our new paradigm, the rationale behind it and our experiences introducing it into an existing application. Finally, we presents some ot the lessons learned: prototypes are useful to break the barriers imposed by conventional GUI design and some of their ideas can still be retrofitted seamlessly into products. Furthermore, the added functionality is not measured only in terms of user performance, but also by the quality of interaction, which allows artists to create new graphic vocabularies and graphic styles.	design rationale;graphical user interface;item unique identification;programming paradigm;prototype;tablet computer;vocabulary	Gordon Kurtenbach;George W. Fitzmaurice;Thomas Baudel;William Buxton	1997		10.1145/258549.258574	simulation;human–computer interaction;operating system;multimedia;transparency;world wide web;computer graphics (images)	HCI	-45.065867449805175	-39.26518760154052	64035
4d8ed5f2b3c61ded158c4d2e1de90bf467c1394d	an emotional viseme compiler for facial animation	behavioural sciences computing computer animation image processing speech processing feature extraction;image processing;behavioural sciences computing;speech processing;rule based system emotional viseme compiler facial animation 3d synthetic human face lip movements generation speech soundtrack phonemes extraction speech signal visual lip shapes human emotional expressions emotional cues emotional visual speech synthetic human face;three dimensional;facial animation humans face signal generators signal processing muscles gold shape speech processing computer graphics;feature extraction;facial animation;computer animation;emotional expression	The animation of a three dimensional synthetic human face has been the object of much research in the past few years. Many systems now exist for this purpose, which rely on the artistic and animation skills of animators. Methods for the generation of lip movements to accompany a speech soundtrack have also been developed. These systems rely on the extraction of phonemes from the speech signal and converting them to 'visemes' or visual lip shapes for a synthetic human face. The generation of human emotional expressions has also been developed in the recent past. This paper combines some of these developments to present a system, which is capable of appropriately combining emotional cues automatically with phonemes to generate emotional visual speech on a synthetic human face.	compiler;norm (social);synthetic intelligence	Savant Karunaratne;Hong Yan	1999		10.1109/ISSPA.1999.818211	three-dimensional space;computer vision;speech recognition;computer facial animation;image processing;feature extraction;computer science;emotional expression;viseme;speech processing;computer animation	Graphics	-35.85244700522723	-43.55327573331634	64133
4aa42152921ca37792b49fdd3c2584bdd94ab8d5	whitewater slalom pseudo experience device using 3-dof motion base and vr goggles		"""We generally visit a stadium, watch TV programs, or view online broadcasts to enjoy watching sports. However, the position of the seat or camera for broadcast in the stadium is generally fixed, and the motion of players cannot be experienced. A sports-watching system, called """"Synchro-athlete,"""" is developed in our study, with a 3 DOF motion base and a virtual reality (VR) goggle. The motion base is driven by electric motors and is small and lightweight. Synchro-athlete provides 360° video, sound, and motion, which gives the viewer the impression of being a player. The control of the 3-DOF motion base and the application to canoe slalom are described in this paper."""	goggles;synchro;virtual reality	Senri Yoshikawa;Kazunori Takishima;Toshiki Tomihira;Yunosuke Sato;Akira Homma;Akihiro Yamashita;Katsushi Matsubayashi	2017	2017 Conference on Technologies and Applications of Artificial Intelligence (TAAI)	10.1109/TAAI.2017.46	simulation;impression;stadium;virtual reality;broadcasting;computer science	Visualization	-41.34545784456325	-40.43991530865357	64184
f6744283a3710198cc03fc4cd58dcaed88d0def8	responsive design for personalised subtitles	conference publications;web;captions;view on demand;accessibility;audio;video on demand;vod;video;subtitles	The Internet has continued to evolve, becoming increasingly media rich. It is now a major platform for video content, which is available to a variety of users across a range of devices. Subtitles enhance this experience for many users. However, subtitling techniques are still based on early television systems, which impose limitations on font type, size and line length. These are no longer appropriate in the context of a modern web-based culture.  In this paper we describe a new approach to displaying subtitles alongside the video content. This follows the responsive web design paradigm enabling subtitles to be formatted appropriately for different devices whilst respecting the requirements and preferences of the viewer. We present a prototype responsive video player, and report initial results from a study to evaluate the value perceived by regular subtitle users.	digital video;internet;norm (social);programming paradigm;prototype;requirement;responsive web design;television;web application;while	Chris J. Hughes;Mike Armstrong;Rhianne Jones;Michael Crabb	2015		10.1145/2745555.2746650	computer science;multimedia;advertising;world wide web	HCI	-35.213449460444885	-47.89615056936544	64188
344512cde9b8b656c4841e50ea6d250c126e1aa5	rwc multimodal database for interactions by integration of spoken language and visual information	computer graphics;image recognition;natural language interfaces;speech recognition;speech synthesis;rwc multimodal database;real world computing program multimodal database;cost control;database specification;design policy;face;facial expressions;hand gestures;hands;human computer interactions;human-like computer graphics agent;image recognition;image synthesis;nonverbal communication;prototype data collection;speech recognition;speech synthesis;spoken language/visual information integration;variability control	This paper describes our design policy and prototype data collection of RWC (Real World Computing Program) multimodal database. The database is intended for research and development on the integration of spoken language and visual information for human computer interactions. The interactions are supposed to use image recognition, image synthesis, speech recognition, and speech synthesis. Visual information also includes non-verbal communication such as interactions using hand gestures and facial expressions between human and a human-like CG (Computer Graphics) agent with a face and hands. Based on the experiments of interactions with these modes, speci cations of the database are discussed from the viewpoint of controlling the variability and cost for the collection.	computer graphics;computer vision;experiment;human computer;multimodal interaction;prototype;rendering (computer graphics);spatial variability;speech recognition;speech synthesis	Satoru Hayamizu;Osamu Hasegawa;Katunobu Itou;Katsuhiko Sakaue;Kazuyo Tanaka;Shigeki Nagaya;Masayuki Nakazawa;T. Endoh;Fumio Togawa;Kenji Sakamoto;Kazuhiko Yamamoto	1996			natural language processing;speech recognition;speech corpus;computer science;gesture recognition;communication;speech synthesis;speech analytics	AI	-36.39639887429899	-42.61298715199307	64202
1283b1b8f95520d33aed97dd47ffaa9718731858	effects of feedback on eye typing with a short dwell time	disabled users;text entry;dwell time;eye typing;feedback;error rate	Eye typing provides means of communication especially for people with severe disabilities. Recent research indicates that the type of feedback impacts typing speed, error rate, and the user's need to switch her gaze between the on-screen keyboard and the typed text field. The current study focuses on the issues of feedback when a short dwell time (450 ms vs. 900 ms in a previous study) is used. Results show that the findings obtained using longer dwell times only partly apply for shorter dwell times. For example, with a short dwell time, spoken feedback results in slower text entry speed and double entry errors. A short dwell time requires sharp and clear feedback that supports the typing rhythm.	bit error rate;feedback;keystroke dynamics;type system;virtual keyboard;words per minute	Päivi Majaranta;Anne Aula;Kari-Jouko Räihä	2004		10.1145/968363.968390	real-time computing;simulation;computer science;communication	HCI	-47.15314139173271	-45.45438132121562	64289
bf0198156096b812ae63bc4532912fe8b44ef8f5	the virtual dressing room: a perspective on recent developments		This paper presents a review of recent developments and future perspectives, addressing the problem of creating a virtual dressing room. First, we review the current state-of-the-art of exiting solutions and discuss their applicability and limitations. We categorize the exiting solutions into three kinds: (1) virtual real-time 2D image/video techniques, where the consumer gets to superimpose the clothes on their real-time video to visualize themselves wearing the clothes. (2) 2D and 3D mannequins, where a web-application uses the body measurements provided by the customer, to superimpose the standard sizes to fit a customized 2D or 3D mannequin before buying. (3) 3D camera and laser technologies which acquire 3D information of the costumer, enabling estimation of the body shape and measurements. Additionally, we conduct user studies to investigate the user behavior when buying clothes and their demands to a virtual dressing room.	virtual dressing room	Michael B. Holte	2013		10.1007/978-3-642-39420-1_26	computer-mediated reality;mixed reality	Vision	-48.195436192134096	-41.152088490128	64465
2f64fc5832a01c4de0392a79dd84cc7cbee58a05	realistic electronic books	wikipedia;human computer interaction;usability evaluation;digital library;within document search;flash application;journal article;electronic book;thesis;within document navigation;page turning techniques;usability studies	We describe a software book model that emulates a range of properties associated with physical books— analog page turning, visual location cues, bookmarks and annotations—and, furthermore, incorporates many advantages of digital environments—hyperlinks, multimedia, full-text search, automatic identification of synonyms, cross-referencing of key terms with an online encyclopedia, and an automatically generated backof-the-book index. Usability studies were conducted to compare performance using these books for various reading tasks with HTML, PDF and physical books. Participants completed the tasks more efficiently with the new interface without any loss in accuracy; they also preferred it.	automatic identification and data capture;book;cross-reference;e-book;emulator;html;hyperlink;portable document format;usability	Veronica Liesaputra;Ian H. Witten	2012	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2012.02.003	digital library;usability;human–computer interaction;computer science;artificial intelligence;brand;multimedia;world wide web;usability lab;information retrieval	HCI	-35.63203320420895	-49.76699025687229	64468
a78b21474003be0dd3659d1bae58870895b7f1ed	user-created marker based on character recognition for intuitive augmented reality interacion	human computer interaction;user interface;pattern recognition;augmented reality;user interfaces;character recognition	This paper proposes a novel concept of markers with alphabet combinations for Augmented Reality (AR) applications. Compared to traditional markers with square patterns, the proposed markers composed of alphabets have several advantages for interaction. The proposed markers are based on English alphabet letters, allowing users to interact intuitively by combining the alphabetized markers to from words. The marker recognition system operates at a robust 58.8 fps, recognizing the markers in a fast and effective method. In this paper, we verify the effectiveness of the proposed markers by implementing an AR application.	augmented reality;effective method;optical character recognition	Seiheui Han;Eun Joo Rhee;Junyeong Choi;Jong-Il Park	2011		10.1145/2087756.2087839	computer vision;augmented reality;speech recognition;computer science;operating system;user interface	HCI	-37.757792395971	-43.75919387794146	64529
71955648962b1b4de059f65941de6d42b61aac21	enabling tangible interaction on capacitive touch panels	interactive surface;touch screen;tangible;markers;time domain;tangible interaction;off the shelf;high speed;high frequency;physical interaction	We propose two approaches to sense tangible objects on capacitive touch screens, which are used in off-the-shelf multi-touch devices such as Apple iPad, iPhone, and 3M's multi-touch displays. We seek for the approaches that do not require modifications to the panels: spatial tag and frequency tag. Spatial tag is similar to fiducial tag used by tangible tabletop surface interaction, and uses multi-point, geometric patterns to encode object IDs. Frequency tag simulates high-frequency touches in the time domain to encode object IDs, using modulation circuits embedded inside tangible objects to simulate high-speed touches in varying frequency. We will show several demo applications. The first combines simultaneous tangible + touch input system. This explores how tangible inputs (e.g., pen, easer, etc.) and some simple gestures work together on capacitive touch panels.	encode;embedded system;fiducial marker;modulation;multi-touch;simulation;tangible user interface;touchscreen;ipad;ipod	Neng-Hao Yu;Li-Wei Chan;Lung-Pan Cheng;Mike Y. Chen;Yi-Ping Hung	2010		10.1145/1866218.1866269	human–computer interaction;computer hardware;time domain;computer science;high frequency;computer graphics (images)	HCI	-43.33349431783085	-40.818361002728885	64624
18be63ec30f237616a5e3c19d8f64026b2fa0910	exploring tangible and direct touch interfaces for manipulating 2d and 3d information on a digital table	spatially aware displays;interaction techniques;information visualization;tangible user interface;multi layer interaction;object manipulation;three dimensional space;tangible interaction;tabletop display;physical interaction	On traditional tables, people often manipulate a variety of physical objects, both 2D in nature (e.g., paper) and 3D in nature (e.g., books, pens, models, etc.). Current advances in hardware technology for tabletop displays introduce the possibility of mimicking these physical interactions through direct-touch or tangible user interfaces. While both promise intuitive physical interaction, they are rarely discussed in combination in the literature. In this paper, we present a study that explores the advantages and disadvantages of tangible and touch interfaces, specifically in relation to one another. We discuss our results in terms of how effective each technique was for accomplishing both a 3D object manipulation task and a 2D information visualization exploration task. Results suggest that people can more quickly move and rotate objects in 2D with our touch interaction, but more effectively navigate the visualization using tangible interaction. We discuss how our results can be used to inform future designs of tangible and touch interaction.	3d computer graphics;book;fundamental interaction;human–computer interaction;information visualization;tangible user interface	Mark S. Hancock;Otmar Hilliges;Christopher Collins;Dominikus Baur;M. Sheelagh T. Carpendale	2009		10.1145/1731903.1731921	human–computer interaction;engineering;multimedia;computer graphics (images)	HCI	-45.231732415950006	-39.46111971273721	64798
05fd21314460f0ca3b20026925c4f8c9c4b12bbd	fighting technology dumb down: our cognitive capacity for effortful ar navigation tools	hit;technology;lab;hitlab;interface;human;nz	By overlaying virtual guidance information directly over the surrounding environment, Augmented Reality (AR) is seen as an easy alternative to maps for pedestrians navigating in unfamiliar urban environments. It is hypothesized, however, that easing navigation tasks would result in weaker cognitive maps, leaving users more vulnerable to becoming lost should their navigation device fail. We describe an outdoor navigation study that highlighted the gap between theoretical expectations and real world testing with navigation tools. We addressed the issues by creating a simulation system for testing navigation tools and report on the results of a study comparing AR with maps. We then extended the system to support simultaneous secondary tasks to assess relative workload. We present this as a way of objectively measuring relative cognitive effort expended on navigation tool use. Our findings are helpful in the design of mobile pedestrian navigation tools seeking to balance navigational efficiency with mental map formation.		James Wen;Agnes Deneka;William S. Helton;Andreas Dünser;Mark Billinghurst	2014		10.1007/978-3-319-07227-2_50	simulation;computer science;artificial intelligence;interface;technology	HCI	-45.58901990138996	-48.416665806321724	64835
e499b6c7b8c0e5e7a195148a35e4d719ba686a8e	swimming performance and technique evaluation with wearable acceleration sensors	performance evaluation;wearable system;swimmodel;swimming;context recognition	We are working towards a wearable computing system called SwimMaster, that will support swimmers in achieving their desired exercise goals bymonitoring their swimming performance and technique and providing the necessary feedback. In this article, we describe our methods to extract the most relevant swimming performance and technique parameters from acceleration sensors worn at the wrist and at the back. We analyze the data and our methods with a SwimModel. Finally, we present the results of our evaluation studieswith 18 swimmers—seven elite, eight recreational and three occasional swimmers. © 2011 Elsevier B.V. All rights reserved.		Marc Bächlin;Gerhard Tröster	2012	Pervasive and Mobile Computing	10.1016/j.pmcj.2011.05.003	embedded system;simulation;artificial intelligence	HCI	-40.84223518747387	-46.03204788130492	64866
1942ba755160aa1770bf109118e3055ee1cd4d8b	chameleon tables: using context information in everyday objects	context aware;context information;musical instruments;context aware design;information appliance;ubiquitous computing;sensor design;information appliances	The Chameleon Table project created a set of hexagonal tables. They are modular and are able to snap together. The design portrays some goals that can be achieved by having a table that is aware of changes in its surroundings and includes this as part of its technology. By creating this infastructure, we have been able to make several scenarios including musical instruments, sending messages between tables, and menus that change with apparent use in a food scenario. This paper also shows the use of a network for broadcasting context information.		Ted Selker;Ernesto Arroyo;Winslow S. Burleson	2002		10.1145/506443.506493	human–computer interaction;computer science;database;multimedia;information appliance;world wide web;ubiquitous computing	HCI	-47.14465252807836	-39.29587367322303	64976
3b30cb89cb47742d8672ac3e857fd832cef7a18e	a virtual reality platform for assessment and rehabilitation of neglect using a kinect	rehabilitation;virtual reality;settore m psi 01 psicologia generale;kinect;m psi 01 psicologia generale;neurovirtual 3d;neglect	Unilateral Spatial Neglect (USN) is normally assessed with paper-and-pencil tests. Virtual reality can be an effective neuropsychological tool for a more ecological and functional assessment and rehabilitation of neglect. We developed a 3D Virtual Reality platform - NeuroVirtual 3D - for the assessment and rehabilitation of cognitive deficits, in particular for USN. Within the virtual environments it is possible to interact with virtual objects and execute specific exercises using a Microsoft Kinect. Through the analysis of different grasping tasks it is possible to evaluate in an ecological way the patients' ability to find and handle objects in both sides of the virtual space.	cognition disorders;hemimegalencephaly;kinect;patients;physical object;physical therapy exercises;virtual reality;functional assessment	Pietro Cipresso;Silvia Serino;Elisa Pedroli;Andrea Gaggioli;Giuseppe Riva	2014	Studies in health technology and informatics	10.3233/978-1-61499-375-9-66	simulation;computer science;artificial intelligence;virtual reality;natural user interface	HCI	-47.44006913581599	-49.57374641072713	65045
ab6768b983564977afaf791615e101e20889b458	perception of springs with visual and proprioceptive motion cues: implications for prosthetics	mechanoception;elasticity;springs visualization prosthetics force haptic interfaces indexes humans;object manipulation spring perception visual motion cues proprioceptive motion cues nonvisual sensory channel sensory substitution devices virtual prosthetic limb real limb movement virtual prosthetic finger one degree of freedom rotational spring discrimination custom haptic device experimental sensory conditions constant stimuli method stiffness proprioceptive information visual attention reduction upper limb prosthesis control;prosthetic limbs;biomechanics;prosthetics;maximum likelihood estimation;force;elastic constants;force feedback;indexes;springs mechanical;visualization;springs;springs mechanical artificial limbs biomechanics elastic constants elasticity haptic interfaces mechanoception;robot motion;artificial limbs;humans;haptic interfaces;robot motion force feedback maximum likelihood estimation prosthetic limbs	Manipulating objects with an upper limb prosthesis requires significantly more visual attention than doing the same task with an intact limb. Prior work and comments from individuals lacking proprioception indicate that conveying prosthesis motion through a nonvisual sensory channel would reduce and possibly remove the need to watch the prosthesis. To motivate the design of suitable sensory substitution devices, this study investigates the difference between seeing a virtual prosthetic limb move and feeling one's real limb move. Fifteen intact subjects controlled a virtual prosthetic finger in a one-degree-of-freedom rotational spring discrimination task. A custom haptic device was used to measure both real finger position and applied finger force, and the resulting prosthetic finger movement was displayed visually (on a computer screen) and/or proprioceptively (by allowing the subject's real finger to move). Spring discrimination performance was tested for three experimental sensory conditions-visual motion, proprioceptive motion, and visual and proprioceptive motion-using the method of constant stimuli, with a reference stiffness of 290 N/m. During each trial, subjects sequentially pressed the right index finger on a pair of hard-surfaced virtual springs and decided which was stiffer. No significant performance differences were found between the three experimental sensory conditions, but subjects perceived proprioceptive motion to be significantly more useful than visual motion. These results imply that relaying proprioceptive information through a nonvisual channel could reduce visual attention during prosthesis control while maintaining task performance, thus improving the upper limb prosthesis experience.	computer monitor;haptic technology;sensory substitution	Netta Gurari;Katherine J. Kuchenbecker;Allison M. Okamura	2013	IEEE Transactions on Human-Machine Systems	10.1109/TSMCA.2012.2221038	database index;computer vision;simulation;visualization;computer science;artificial intelligence;biomechanics;maximum likelihood;haptic technology;elasticity;force;statistics	HCI	-44.78609917794648	-50.59488274183839	65273
8acbf906efe2ee6f30d4862dcc2950556118b0c3	player control in a real-time mobile augmented reality game	ar;mobile gaming;gestures;augmented reality;user interaction	A virtual joys ck is on the screen. The posi on of a touch in rela on to the center of the joys ck defi nes its posi on. The movement of the avatar on the screen directly follows the direc on of the joys ck. Modern smartphones off er addi onal sensors like gyroscopes or accelerometers which can be used as input in games. They react to movements like l ng, rota ng or shaking of the device. The sensor input is mapped to the movement of the avatar.	augmented reality;avatar (computing);real-time transcription;sensor;smartphone	Mareike Picklum;Georg Modzelewski;Susanne Knoop;Toke Lichtenberg;Philipp Dittmann;Tammo Böhme;Volker Fehn;Christian John;Johannes Kenkel;Philipp Krieter;Patrick Niethen;Nicole Pampuch;Marcel Schnelle;Yvonne Schwarte;Sanja Stark;Alexander Steenbergen;Malte Stehr;Henning Wielenberg;Merve Yildirim	2012		10.1007/978-3-642-33542-6_36	augmented reality;simulation;human–computer interaction;engineering;mixed reality;multimedia	HCI	-44.3081558113369	-41.70735632761054	65559
1c83cbb2338ba97232ce98993a819b2937c6049e	gaze direction in a virtual environment via a dynamic full-image color effect		For developers of immersive 360-degree virtual environments, directing the viewer's gaze towards Points of Interest (POIs) is a challenge. Limited research exists testing the effectiveness of various gaze direction techniques. However, there is a lack of empirical research evaluating real-time color effects designed to direct the viewer's gaze. We developed a novel VR gaze-directing stimulus using a dynamic real-time color effect and tested its effectiveness in a user study. The stimulus was influenced by color psychology research and chosen by an informal pilot study. Results suggest that the stimulus encouraged participants to direct their gaze back towards POIs. In the majority of subjects who encountered the stimulus, their gaze was successfully directed back to POIs within a few seconds. While the task of holding viewer gaze in VR remains a challenge, this experiment has uncovered new information about the potential of color effect-based VR gaze direction.	color;point of interest;real-time clock;real-time locating system;usability testing;virtual reality	Mason Smith;Ann McNamara	2018	2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)	10.1109/VR.2018.8446420	computer vision;gaze;empirical research;artificial intelligence;point of interest;color psychology;immersion (virtual reality);computer science;virtual machine	Visualization	-45.294995054014855	-47.37270322117014	65647
448374e096b66a2ea862afbc5cb051ab4b2c100d	a multi-modal platform for semantic music analysis: visualizing audio-and score-based tension	music representation;music analysis;multimodal system;tension;article;online interface	Musicologists, music cognition scientists and others have long studied music in all of its facets. During the last few decades, research in both score and audio technology has opened the doors for automated, or (in many cases) semi-automated analysis. There remains a big gap, however, between the field of audio (performance) and score-based systems. In this research, we propose a web-based Interactive system for Multi-modal Music Analysis (IMMA), that provides musicologists with an intuitive interface for a joint analysis of performance and score. As an initial use-case, we implemented a tension analysis module in the system. Tension is a semantic characteristic of music that directly shapes the music experience and thus forms a crucial topic for researchers in musicology and music cognition. The module includes methods for calculating tonal tension (from the score) and timbral tension (from the performance). An audio-to-score alignment algorithm based on dynamic time warping was implemented to automate the synchronization between the audio and score analysis. The resulting system was tested on three performances (violin, flute, and guitar) of Paganini's Caprice No. 24 and four piano performances of Beethoven's Moonlight Sonata. We statistically analyzed the results of tonal and timbral tension and found correlations between them. A clustering algorithm was implemented to find segments of music (both within and between performances) with similar shape in their tension curve. These similar segments are visualized in IMMA. By displaying selected audio and score characteristics together with musical score following in sync with the performance playback, IMMA offers a user-friendly intuitive interface to bridge the gap between audio and score analysis.	algorithm;cluster analysis;cognition;dynamic time warping;modal logic;performance;semiconductor industry;usability;web application	Dorien Herremans;Ching-Hua Chuan	2017	2017 IEEE 11th International Conference on Semantic Computing (ICSC)	10.1109/ICSC.2017.49	simulation;speech recognition;computer science;artificial intelligence;tension;music theory;multimedia;world wide web	Visualization	-34.67618725885062	-46.01114125901734	65768
ac7ce8f2953d8dcd1cdecbd553660832c674c6e2	yes, right there!: a self-portrait application with sensor-assisted guiding for smartphones	smartphone;face detection;inertial sensor;camera	1. ABSTRACT Taking self-portrait on a smartphone is a lot of fun and can be easy when we know how (Fig. 1a). In addition, thank to selftimer, giving a delay between pressing the shutter release and the shutter’s firing, we are able to take photos of ourselves when nobody on hand to take it (Fig. 1b). However, there are rare cases that we can get satisfied snapshots at our first try. The reason is that we usually had no idea whether we were in the right position of the camera frame (Figs. 1c, 2a and 2c). Although the front camera can help us to check position in the frame, its generated snapshot quality is much lower than that taken by the back camera. In this demo we will introduce a self-portrait App – “Yes, right there!” The App enables a camera to prevent faces from being cut out of the camera frame by giving suggestions to users until they are in a suitable position in the frame, as shown in Fig. 1d, Fig. 2b, and Fig. 2d. The suggestions are voice commands including “Raise your hand!”, “Come closer!”, “Please move to the left”, and so on. They depend on the face’s position in the camera frame [1] and the inertial sensing data on the phone. In the end, a good picture is then taken after the beep sounds. More concretely, “Yes, right there!” supports two modes: selfportrait mode and self-timer mode. In self-portrait mode, our App initially asks users to take favorite photos as pre-training references. In the meantime, the application measures the yaw, pitch, and roll values from accelerometer, electronic compass, and gyro. Note that the users usually like to take photos with a special angle. Once a user wants to take self-portrait, she points the lens at herself so that her face is reflected in the camera frame. The App detects the face’s position in the camera frame and measures the yaw, pitch, and roll values. Then the face’s position and the inertial sensing data are being compared against the pre-training references while the App suggests the user to change her posture by voice commands. If the face moves to the right position and the yaw, pitch, and roll values are suitable, a good picture is then taken automatically; otherwise, the App repeats the suggestions. Note that the self-portrait mode can be easily extended for multiple users. In self-timer mode, the user firstly specifies an area where she wants her face to appear in the frame. When the App works, it compares the position of her face against the area specified, and suggests the user to change her location by voice commands. As shown in Fig. 2a, the user’s face is detected appearing in the grid 1 (outside the specified area in the frame). The voice interactive function is triggered to lead the user by saying “Please move to the left and then come closer, etc.” until she moves to the suitable position (Fig. 2b). When the App works for multiple users, as shown in Fig. 2c, it detects that a user in the left side is not in the area specified (gird 5), and then the voice interactive function will give suggestions (e.g., The user in the right side please moves to the left, etc.) until all the users are appearing in the specified area in the frame (Fig. 2d). Fig. 3 shows our system model. We will distribute Android phones to demo visitors, allowing real-time interaction with user interface on these phones. We will also show how “Yes, right there!” recommends users taking selfportrait without assistance from others. Moreover, users can set timer for different waiting intervals, and specify the number and range of the grid which is considered as suitable position. To the best of our knowledge, this is the first work that shows how to help users take self-portrait by detecting the face’s position in the camera frame and measuring the inertial sensing data on the phone.	android;beep;gyro;human–computer interaction;inertial navigation system;movie projector;multi-user;poor posture;raise;real-time transcription;sensor;smartphone;snapshot (computer storage);timer;user interface;xfig;yaws	Chi-Chung Lo;Sz-Pin Huang;Yi Cheng Ren;Yu-Chee Tseng	2013		10.1145/2462456.2465705	embedded system;computer vision;face detection;computer science	HCI	-44.40661132745862	-42.09424619640435	65977
6db0fc9779beec03c75f66a814aa51ffa13d0a7e	accessibility of non-trivial remote controlled models and toys	radio transmitters prototypes helicopters educational institutions protocols adaptation models standards;protocols;standards;sip and puff 4d joystick accessibility remote controlled toys asterics;prototypes;radio transmitters;telecontrol handicapped aids interactive devices;4d joystick nontrivial remote controlled models accessible off the shelf toys nontrivial toys physical disabilities rc hobby shops alternative control interfaces;adaptation models;helicopters	Nowadays there hardly exist accessible off the shelf toys for children and adults with disabilities. One group of non-trivial toys that are very interesting for people with physical disabilities are remote controlled (RC) models because the remotes can be easily exchanged by custom ones. If the controller is once adapted to the individual needs of a user, he can buy models in RC hobby shops without the need to adapt or modify it. This paper describes and discusses alternative control interfaces accessible for people with physical disabilities to accurately control model helicopters, airplanes boats or cars.	accessibility;remote control;toys	David Thaller;Gerhard Nussbaum	2013	Fourth International Conference on Information and Communication Technology and Accessibility (ICTA)	10.1109/ICTA.2013.6815314	communications protocol;transmitter;simulation;telecommunications;computer science;artificial intelligence;operating system;prototype;multimedia;world wide web	Robotics	-42.32928030163726	-42.712834127304475	66149
11df03dc0ce1ce592939f327ac8805bfd267fbef	demo abstract: inviz: low-power personalized gesture recognition using wearable textile capacitive sensor arrays	yarn;prototypes;capacitive sensors yarn sensor arrays prototypes gesture recognition fabrics;wearable computers capacitive sensors domestic appliances embedded systems gesture recognition home automation interactive systems textiles;fabrics;home automation gateway low power personalized gesture recognition wearable textile capacitive sensor arrays flexible textile based capacitive sensors proximity based movement detection inviz user interface home automation systems environmental control limited mobility user paralysis paresis degenerative diseases proximity based sensing physical contact skin abrasion limited to no sensitivity wireless module form factor hierarchical sensing technique gesture learning just in time embedded computational resources power consumption smart home environment appliance controls;gesture recognition;sensor arrays;capacitive sensors	This demonstration presents Inviz, a low-cost gesture recognition system that uses flexible textile-based capacitive sensors. Gestures are recognized using proximity-based movement detection using flexible capacitive sensor arrays that can be built into the environment or placed on to the body or be integrated into clothing. Inviz provides an innovative interface to home automation systems to simplify environmental control for individuals with limited-mobility resulting from paralysis, paresis, and degenerative diseases. Proximity-based sensing obviates the need for physical contact which can result in skin abrasion which is particularly deleterious to people with limited-to-no sensitivity in their extremities. A custom-designed wireless module maintains a small form factor facilitating placement based on an individual's needs. Our system leverages a hierarchical sensing technique which facilitates learning gestures based on the individual and placement of the sensors. Classification uses just-in-time embedded computational resources to provide accurate responses while maintaining a low average power consumption, in turn reducing the impact of batteries on the form factor. To illustrate the use of Inviz in a smart home environment, we demonstrate an end-to-end home automation system that controls small appliances. We will interface our system with a home automation gateway to demonstrate a subset of potential applications. This interactive demonstration highlights the intuitiveness and extensibility of the Inviz prototype.	capacitive sensing;computational resource;embedded system;end-to-end principle;extensibility;gesture recognition;home automation;just-in-time compilation;personalization;prototype;sensor;small form factor;wearable computer	Gurashish Singh;Alexander Nelson;Ryan Robucci;Chintan Patel;Nilanjan Banerjee	2015	2015 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops)	10.1109/PERCOMW.2015.7134029	embedded system;simulation;computer science;gesture recognition;prototype;capacitive sensing	Mobile	-42.255687089985585	-42.17502067760001	66203
df5b08d629fef7b0e8dd3c021422cf292acb7756	fingertip force displaying device using pneumatic negative pressure	rehabilitation;negative pressure;virtual reality;fingertip force display;pneumatic system			Masahiro Takaiwa;Toshiro Noritsugu;Daisuke Sasaki;Takahiro Nogami	2014	IJAT	10.20965/ijat.2014.p0208	embedded system;computer hardware	HCI	-41.63170183058199	-44.093014244173375	66572
b34870283333487cec15a39c79bb974d3fb349d5	near eyes-free chauffeur computer interaction with chording and visual text mnemonics	human computer interaction;in car user interface;mobile text entry;chording;limited visual feedback;vdp teknologi 500 informasjons og kommunikasjonsteknologi 550 datateknologi 551;spatial mnemonics;ubiquity	Modern cars are equipped with advanced technology requiring cognitively complex operation that is reliant on the user’s visual attention. It is therefore hazardous for drivers to operate such devices while driving. In this paper a user interface interaction style for in-car user interfaces are proposed. Users interact with the in-car computer using three chording keys and chording pattern sequences are derived based on visual mnemonics. Cases are illustrated for an in-car multimedia system, a mobile phone and a GPS-navigation system. Experimental results demonstrate that the technique is easy to learn, efficient to use and require low visual attention.	global positioning system;mobile phone;user interface	Frode Eika Sandnes;Yo-Ping Huang;Yueh-Min Huang	2010	J. UCS	10.3217/jucs-016-10-1311	simulation;human–computer interaction;computer science;multimedia;world wide web	HCI	-47.45892654333222	-43.272191560937486	66652
d28dbdc17208bdf3e4528d8f8b31c8d8960c429b	design case infracam	user study;user centered design;user studies;infrared;usability;infrared camera;interaction design	This is a design case covering the user-centered design of a new generation of an easy-to-use handheld infrared camera.	handheld game console;user-centered design	Bengt Göransson	2006		10.1145/1182475.1182548	user interface design;user-centered design;simulation;usability;infrared;human–computer interaction;computer science;interaction design;multimedia	Robotics	-45.08806104259008	-41.28665484493018	66757
f82a0beedf74c1700b02753bdc0bde2bd5733569	effect of camera and object motion on visual load in 3d games	motion;game design;user experience;visual perception;visual composition	3D video games are a popular form of entertainment that features elaborate visual compositions and settings. Occasionally, players are thrown into situations with a high amount of visual complexity, which may cause players, especially novice players, to misinterpret important game goals or mechanics, which may, in turn, lead to a diminished experience. Previous research investigated visual design, but only in terms of brightness and color contrast, no research investigated attributes of motion, their complexity in regards to visual design, and their effect on the game experience. Informed by cinema and visual perception domains, we embark on a study of 4 motion attributes: flicker, shape, speed, and repetition, and investigate their design within 6 games. We rate these games based on their complexity. We use video coding with a kappa reliability measure to identify these attributes. We then investigate the relationship between the use of these motion attributes and the rated complexity of the visual scene. We present this analysis as a contribution, and design lessons extrapolated based on the analysis.	3d film;bios;cinema 4d;data compression;experiment;extrapolation;focal (programming language);fear, uncertainty and doubt;flicker (screen);futures studies;stationary process;visual language	David Milam;Magy Seif El-Nasr;Dinara Moura;Lyn Bartram	2011		10.1007/978-3-642-24500-8_12	visual rhetoric;computer vision;simulation;computer science;multimedia	HCI	-42.95297045974721	-50.24545560087398	66783
3c8ba9427bf9b7f2990fab87a2ee0c97e5a525de	impact of numerical and graphical formats on dynamic decision making performance: an eye-tracking study	learning curve;information presentation;dynamic environment;eye tracking;computer simulation;dynamic decision making	This paper presents a study in which we manipulated the interface of a computer simulation into: graphical and numerical formats. We obtained both performance and eye-tracking learning curves from individuals assigned to one of these two conditions. Our findings indicate that although performance is not different between the two interfaces, the amount of attention as measured by the number of eye-tracking points was very different in the graphical and numerical conditions. Attention increased over time in the numerical condition, but was stable in the graphical condition. These results showed that the strategies used to make decisions in dynamic environments vary according to the form of information presentation.	computer simulation;eye tracking;graphical user interface;numerical analysis	Cleotilde Gonzalez;Janice Golenbock	2003			computer vision;simulation;human–computer interaction;computer science	HCI	-47.123767300274	-48.187616489032614	66899
a9171ae2ed73bccafc63f7556586a93f47cb9150	on-time measurement of subjective anxiety of a passenger in an autonomous vehicle: gradually changing sounds decreases anxiety of passenger		The current study examined the possibility of measuring the subjective anxiety in real-time by means of a novel handle-shaped device, caused by riding an autonomous car. In our experiment, a participant was shown computer graphics (CG) animation, which gave the person a virtual experience of riding a new autonomous car. The CG animation stimuli were made with three variables: maximum speed (19 km/h, 160 km/h, or 320 km/h), acceleration/deceleration pattern (linear or exponential), and with and without ascending/descending sounds (sound, or no sound). The participants grasped the handle and moved it in the longitudinal direction, i.e., pulling when they experienced anxiety and pushing when they felt relaxed. Results of experiments by 16 participants showed that they moved the handle depending on the stimulus of speed at that instant, which indicated that our handle-shaped device was useful in assessing the participants’ anxiety on time. In addition, results indicated that sounds, especially those which gradually ascending with acceleration, could diminish the subjective anxiety under some conditions.		Akitoshi Tomita;Etsuko T. Harada;Satoshi Ando;Kozue Miyashiro;Maito Ohmori;Hiroaki Yano	2017		10.1007/978-3-319-58472-0_17	computer animation;acceleration;animation;simulation;anxiety;computer graphics;computer science	Robotics	-46.145095790665195	-50.05768123124208	67151
a46a24d753bdce191c16ee03c059eeecab03ad01	mobile magic hand: camera phone based interaction using visual code and optical flow	mobile;gestural interface;user study;visual code;camera phone;optical flow	"""We propose the """"Mobile Magic Hand"""" interface; it is an extension of our previous visual code-based interface system. Once the user acquires the visual code of interest, the user can then manipulate the related virtual object/system without having to keep the camera centered on the visual code. Our new interface does this analyzing the optical flow as captured by the camera. For example, consider a visual code that represents a 3D object, such as a dial. After selecting the code, the user can freely rotate and/or move the virtual object without having to keep the camera pointed at the code. This interface is much more user friendly and is more intuitive since the user's hand gestures can be more relaxed, more natural, and more extensive. In this paper, we describe """"Mobile Magic Hand"""", some applications, and a preliminary user study of a prototype system."""	camera phone;optical flow	Yuichi Yoshida;Kento Miyaoku;Takashi Satou	2007		10.1007/978-3-540-73107-8_58	smart camera;computer vision;computer science;mobile technology;optical flow;multimedia;camera phone;computer graphics (images)	Vision	-44.11605109297897	-41.11583262440024	67437
cea9ab1135a1f5f2dc271dafdf439363652be810	dynamic reconfigurable screen keyboard generation method using probe key test	single screen keyboard dynamic reconfigurable screen keyboard generation probe key test touch screen mobile phones mobile applications touch screen ui;keyboards;touch sensitive screens;touch screen;dynamic reconfiguration;single screen keyboard;layout;touch screen mobile phones;touch screen ui;probes;dynamic reconfigurable screen keyboard generation;mobile phone;mobile applications;engines;general methods;monitoring;user interfaces keyboards mobile handsets touch sensitive screens;games;mobile communication;time use;mobile handsets;layout probes keyboards monitoring mobile communication games engines;user interfaces;probe key test;mobile application;dynamic configuration	As touch-screen mobile phones pour into the market, demands for reusing existing mobile applications by adding a touch-screen UI are increasing. Up until this point, the method of defining a single screen keyboard and redirecting touch inputs to key inputs was used. However, this method diminishes the efficiency of touch-screens because a fixed-layout is applied to every application even though different keys are used for different UI states of an application. This study proposes a method that investigates the type of keys used in an application during run-time using the probe key and dynamically configures the layout of screen keyboards. Test results of the proposed method showed that an optimized touch-screen UI was generated every time the UI state of an application changed.	mobile app;mobile phone;touchscreen;user interface	Seokhoon Ko;Seman Oh	2010	2010 Fifth International Conference on Digital Information Management (ICDIM)	10.1109/ICDIM.2010.5664625	layout;games;embedded system;mobile telephony;human–computer interaction;computer hardware;computer science;operating system;user interface	HCI	-45.99320512723465	-43.07072931886788	67546
b23b96a5a40d796cf66138ce27b5503e0b3c5d96	human factors in instructional augmented reality for intravehicular spaceflight activities and how gravity influences the setup of interfaces operated by direct object selection			augmented reality;human factors and ergonomics	Daniela Markov-Vetter	2016				HCI	-43.905061599539	-39.18054174580757	67548
a03482acbbbf0864f7cdfe6712aad008854164d4	eye-based head gestures	interaction;user study;head gestures;mobile phone;gaze interaction;eye tracker;eye movement;public display;gesture recognition	A novel method for video-based head gesture recognition using eye information by an eye tracker has been proposed. The method uses a combination of gaze and eye movement to infer head gestures. Compared to other gesture-based methods a major advantage of the method is that the user keeps the gaze on the interaction object while interacting. This method has been implemented on a head-mounted eye tracker for detecting a set of predefined head gestures. The accuracy of the gesture classifier is evaluated and verified for gaze-based interaction in applications intended for both large public displays and small mobile phone screens. The user study shows that the method detects a set of defined gestures reliably.	eye tracking;gesture recognition;interaction;mobile phone;sensor;usability testing	Diako Mardanbegi;Dan Witzner Hansen;Thomas Pederson	2012		10.1145/2168556.2168578	computer vision;eye tracking;gesture recognition;communication;computer graphics (images)	HCI	-44.28053561742167	-43.27892094491376	67681
633d427652398afd05a7f0180c680b255df3d3f3	choosing rendering parameters for effective communication of 3d shape	rendering parameters;graphical interface;interface properties rendering parameters 3d shape communication perceptual experiments three dimensional objects graphically rendered displays rotating objects shape perception discrimination judgments operational definition rendering factors occluding contour smooth shading specular highlights graphic interfaces rendering conditions shape discrimination judgments specular component reasoned manipulation;three dimensional;shape perception;visualization;human factors;communication effectiveness shape displays rendering computer graphics ellipsoids equations ground support psychology marine vehicles;ergonomics rendering computer graphics user interfaces human factors;rendering computer graphics;user interfaces;ergonomics	We conducted a series of perceptual experiments to assess the contributions of rendering parameters to the perception of the shape of three-dimensional objects. For the experiments, observers viewed graphically rendered displays consisting of pairs of rotating objects and judged whether their shapes were identical. For some pairs they were, while for other pairs they differed by varying amounts. We determined the accuracy of shape perception from these discrimination judgments. We provide background information for the operational definition of shape used throughout the experiments, as well as for the rendering factors under experimental investigation: occluding contour, smooth shading, and specular highlights. Following that, we describe a series of experiments. Experiment 1 demonstrated the effectiveness of our new technique for the exploration of perceptual issues related to graphic interfaces. An additional four experiments produced results concerning the effects of rendering parameters on the communication of 3D shape. Experiments 2 and 3 investigated the contributions of basic rendering conditions such as the presence of occluding contours and smooth surface shading. In Experiments 4 and 5, the manipulation of specular highlighting revealed that accurate shape discrimination judgments were possible either with or without the specular component. These results lay a foundation for reasoned manipulation of interface properties when accurate communication of 3D shape is a primary goal of the display.		James C. Rodger;Roger A. Browse	2000	IEEE Computer Graphics and Applications	10.1109/38.824528	three-dimensional space;computer vision;image-based modeling and rendering;visualization;3d rendering;rendering;computer science;human factors and ergonomics;operating system;graphical user interface;multimedia;real-time rendering;user interface;alternate frame rendering;computer graphics (images);mechanical engineering	Visualization	-43.26273393545127	-48.4063558590095	67728
3d171815c002434e1ef78b205ed2c4f21fc31997	evaluation of line-tracing controller of intelligently controllable walker	passive motion control;gait rehabilitation;mr fluid brake;walking aid;caster walker	Caster walkers are supporting frames with casters and wheels that are regularly used as walking-aids in hospitals, nursing homes, and by individuals. The casters and wheels of the walker provide mobility to elderly users. However, when the walker moves in unexpected directions, falling accidents often occur. These accidents pose a very serious problem and are one of the leading causes of secondary injuries and disorders. Caregivers support patients with impaired motor or sensory functions by standing next to them when they walk. The main interests of this study are the development of an intelligent walker and the realization of a safe walker for the elderly and disabled. In our previous study, we developed the intelligently controllable walker (i-Walker) that utilized compact MR fluid brakes in its wheels and a web camera. In addition, we had proposed a line-tracing controller for i-Walker, whose performance was evaluated by healthy subjects. In this study, three elderly subjects suffering from different ...	amiga walker	Takehito Kikuchi;Toshimasa Tanaka;Kenichi Anzai;Sensi Kawakami;Masayuki Hosaka;Kazumi Niino	2013	Advanced Robotics	10.1080/01691864.2013.776940	control engineering;simulation;engineering	Robotics	-40.47486915913471	-46.68478141616644	67806
7a22427607f9b25f0f328368845a1ef33f8d10a0	can a paper-based sketching interface improve the gamer experience in strategy computer games?	webcam paper based sketching interface gamer experience strategy computer games video game paper based midi musical interface freepad;games shape keyboards mice buildings computers real time systems;h 5 2 user interfaces evaluation methodology k 8 0 personal computing general games;computer games;interactive systems;interactive systems computer games	The field of sketching interface design in regards to video game is relatively young and has not been investigated in great depth. Freepad is a custom paper-based MIDI musical interface. We describe an extension to Freepad that supports user customization for real time strategy games. Using only a webcam, a pen and a sheet of paper, players can design their own interface by drawing shapes and linking them to simple or complex actions in the game. In an user study, we use this extended Freepad to explore the potential of sketching interfaces in strategy video games. Our results indicate that using Freepad improves the efficiency of players and their enjoyment in this kind of games.	battle.net;bespoke;conquest;constraint satisfaction problem;graphics software;kerrison predictor;midi;pc game;parallel computing;starcraft;usability testing;webcam	Matthieu Macret;Alissa Nicole Antle;Philippe Pasquier	2012	2012 4th International Conference on Intelligent Human Computer Interaction (IHCI)	10.1109/IHCI.2012.6481817	video game design;video game graphics;simulation;human–computer interaction;turns, rounds and time-keeping systems in games;computer science;emergent gameplay;game mechanics;full motion video;multimedia;video game development	HCI	-45.48869534209717	-41.68973185985219	67970
0381be4d23c53399cdb08daef6dd59c616218208	tv scout: lowering the entry barrier to personalized tv program recommendation	entry barrier	In this paper, we present TV Scout, a recommendation system providing users with personalized TV schedules. The TV Scout architecture addresses the “cold-start” problem of information filtering systems, i.e. that filtering systems have to gather information about the user’s interests before they can compute personalized recommendations. Traditionally, gathering this information involves upfront user effort, resulting in a substantial entry barrier. TV Scout is designed to avoid this problem by presenting itself to new users not as a filtering system, but as a retrieval system where all user effort leads to an immediate result. While users are dealing with this retrieval functionality, the system continuously and unobtrusively gathers information about the user’s interests from implicit feedback and gradually evolves into a filtering system. An analysis of log file data gathered with over 10,000 registered online users shows that over 85% of all first-time users logged in again, suggesting that the described architecture is successful in lowering the entry barrier.1	cold start;experiment;gerrit;information filtering system;information seeking;kelly criterion;login;personalization;recommender system;scout;television;tom proulx;web application	Patrick Baudisch;Lars Brueckner	2005		10.1007/978-3-540-31842-2_30	simulation;computer science;multimedia;world wide web	Web+IR	-34.781336004851845	-51.56009045607327	68008
bb97759717b2b40ac61084fb5274c6d65ae7d702	virtual surface discrimination via an anisotropic-stiffness contact model	tactile;grasping;virtual reality;perception;contact	In haptically enabled virtual reality, most existing devices render kinesthetic feedback via one 3DoF single-contact-point, thus they cannot stimulate tactily teh fingertip skin. This lack of information prevents the perception of contact surface orientation in absence of vision and of free exploratory movements. In this work we experimentally investigate the rendering performance of a contact model which exploits anisotropic contact stiffness to convey such information.	stiffness	Alessandro Formaglio;Gabriel Baud-Bovy;Domenico Prattichizzo	2010		10.1007/978-3-642-14075-4_42	computer vision;simulation;engineering;communication	Robotics	-44.94239022154697	-49.46988106248161	68083
edb155dc413cc0534646d4d13c7c6644d154905f	s-helmet: a ski helmet for augmenting peripheral perception	peripheral vision;distance tracking;peripheral perception;ski safety;lidar	The growing popularity of winter sports, as well as the trend towards high speed carving skies, have increased the risk of accidents on today's ski slopes. While many skiers now wear ski helmets, their bulk might in turn lower a skier's ability to sense their surroundings, potentially leading to dangerous situations. In this demo paper, we describe our Smart Ski Helmet (s-Helmet) prototype. s-Helmet uses a set of laser range finders mounted on the back to detect skiers approaching from behind and warn the wearer about potential collisions using three LEDs. Below, we describe our motivation and how the system works.	peripheral;prototype	Evangelos Niforatos;Ivan Elhart;Anton Fedosov;Marc Langheinrich	2016		10.1145/2875194.2875233	simulation;engineering;forensic engineering;cartography	Networks	-40.588277406048704	-42.35320540454177	68279
df713d2c0112c71d3dfab8cf6e2b6df8966fd49a	implementation of electromyogram interface in cabin immersive multiscreen display	biomedical measurements;electromagnetic measurements;cave;real time control;working environment noise;virtual reality;virtual reality vr;human interface;computer human interface;signal processing;electromyogram emg;computer displays;electromyography computer displays signal processing biomedical computing computer interfaces electromagnetic interference working environment noise biomedical measurements electromagnetic measurements signal to noise ratio;biosignal;electromyography;electromagnetic interference;signal to noise ratio;cabin;computer interfaces;electromyogram;virtual reality vr electromyogram emg computer human interface cave cabin biosignal;biomedical computing	The electromyogram (EMG) signal, which is a kind of biomedical information, might be promising as a computer-human interface. The purpose in this paper is to make a brief report on implementation of the EMG interface in CAVE-clone display and propose the advantages of it. In general, it is expected that the environmental electromagnetic noise would disturb the novel EMG signals. We measured the EMG signals and obtained high signal-to-noise ratio in CABIN immersive multiscreen display constructed at the University of Tokyo. The observed signal-tonoise ratio would make EMG signals a reliable input channel with simple signal processing, and real time control of virtual objects was successfully performed. The advantages of the EMG signals will be proposed as a promising human interface in immersive multiscreen environments. The user would control virtual objects reflecting the activations of the user’s muscles to realize fine operations. Furthermore, by using this additional biosignal channel, the user might interact with virtual objects without definite motion.	electromyography;human–computer interaction;multi-screen video;signal processing;signal-to-noise ratio;user interface	Hideaki Touyama;Koichi Hirota;Michitaka Hirose	2006	IEEE Virtual Reality Conference (VR 2006)	10.1109/VR.2006.83	electromagnetic interference;embedded system;simulation;real-time control system;human–computer interaction;computer science;artificial intelligence;operating system;signal processing;virtual reality;cave;signal-to-noise ratio;human interface device	Visualization	-39.609288768819866	-50.47813111714896	68519
f9065e7096a3061910749e7a682d710aaa209279	the everywhere displays projector: a device to create ubiquitous graphical interfaces	interfaz grafica;affichage;visualizacion;graphical interface;liquid crystal devices;display devices;dispositif cristaux liquides;senal video;signal video;display;dispositif affichage;video cameras;camera video;video signal;bri;interface graphique	"""This paper introduces the Everywhere Displays proj ector, a device that uses a rotating mirror to steer the light from an LCD/DLP projector onto different surfaces of an environment. Issues of bri ghtness, oblique projection distortion, focus, obstruction, and display resolut ion are examined. Solutions to some of these problems are described, together with a plan to use a video camera to allow device-free interaction with the pr ojected images. The ED-projector is a practical way to create ubiquitous g raphical interfaces to access computational power and networked data. In particul ar, it is envisioned as an alternative to the carrying of laptops and to the i nstallation of displays in furniture, objects, and walls. In addition, the use of ED-projectors to augment reality without the use of goggles is examined and illustrated with examples. 11Introduction Ubiquitous computing envisions a world where it is possible to have access to computer resources anywhere and anytime to the data and services available through the Internet [1]. Since most of current software and Internet data is designed to b e accessed through a high-resolution graphical interf ace, to truly ubiquitously compute todayusers need devices with reasonable graphical capabi lities. This means carrying laptops everywhere, wearing computer graphics goggl es, or installing monitors and displays on the surfaces of spaces and objects, suc h as desks, fridges, and entrance doors. Or, simply, toresign to the low-resolution displays of mobile pho nes or PDAs. In this paper we explore an alternative approach to create ubiquitous graphical interfaces. Our idea is to couple an LCD/DLP projec tor to a motorized rotating mirror and to a computer graphics system that can correct the distortion caused by oblique projection. As the mirror moves, different surfaces become available to be used as displays. Also, we plan to employ a video camera to detect hand interaction with the projected image using computer vision techniques. Our target is to develop a projection-based system that creates interactive displays everywhere in an environment by transforming a surf ace into a projected """" touch screen. """" Such an Everywhere Displays projector can be installed, for example, on the ceiling of a space, to provide a generic computer i nterface to users in that environment (see Fig. 1)."""	anytime algorithm;automatic computing engine;computer graphics;computer vision;digital light processing;distortion;goggles;graphical user interface;image resolution;laptop;liquid-crystal display;movie projector;oblique projection;personal digital assistant;robertson–seymour theorem;speeded up robust features;tor messenger;touchscreen;ubiquitous computing;video projector	Claudio S. Pinhanez	2001		10.1007/3-540-45427-6_27	computer vision;computer science;basic rate interface;operating system;graphical user interface;multimedia;display device;computer graphics (images)	HCI	-43.51434729323653	-40.411685391311245	68527
99f00b9def17b606b74f90da9ecb97670b3f82ec	acoustruments: passive, acoustically-driven, interactive controls for handheld devices	projection;large display	Smartphones and handheld devices are increasingly being used in interactive applications beyond their conventional touchscreens. For example, tangibles allow users to interact with mobile devices using physical objects both on screen and around the device. Similarly, there is a growing class of auxiliary devices that require a smartphone to be docked, transforming an otherwise simple object into something with rich interactivity. However, these auxiliary devices still require numerous components, including mechanical mechanisms, PCBs, and sometimes batteries. This increases manufacturing costs, and reduces physical robustness.	mobile device	Gierad Laput;Eric Brockmeyer;Moshe Mahler;Scott E. Hudson;Chris Harrison	2015		10.1145/2782782.2792490	embedded system;simulation;projection;computer science;computer graphics (images)	HCI	-45.0139908817306	-39.73419503115217	68838
bc27dcea24d2d8ec842663a5aa4067b861469498	contactless finger knuckle identification using smartphones	image segmentation;biometrics access control;authorisation;smart phones;c language;graphical user interfaces;data privacy;smartphone camera contactless finger knuckle identification finger knuckle authentication smartphone application online system automatic person identification finger knuckle image user friendly biometric privacy concern data security enhancement android operating system version 2 3 3 finger knuckle detection image pre processing region segmentation finger knuckle pattern phase information encoding log gabor filters c programming language c programming language opencv library user friendly graphical user interface;smart phones image segmentation cameras thumb gabor filters feature extraction;operating systems computers;smart phones authorisation biometrics access control c language data privacy graphical user interfaces image segmentation object detection operating systems computers;object detection	This paper details the development of a smartphone based online system to automatically identify a person by using their finger knuckle image. The key objective is to exploit user-friendly biometric, with least privacy concern, to enhance security of the data in smartphone. The final product from this research is a finger knuckle authentication smartphone application, which is developed under Android operating system with environment version 2.3.3. This paper has developed some specialized algorithms for the finger knuckle detection, image pre-processing and region segmentation. Automatically detected and segmented finger knuckle images are used to encode finger knuckle pattern phase information using a pair of log-Gabor filters. Efficient implementation of various modules is achieved in C/C++ programming language, with OpenCV library, for online application. We also developed a user-friendly graphical user interface for the users to enroll and authenticate themselves. The developed system can therefore acquire finger knuckle image from the smartphone camera and automatically authenticate the genuine users. This paper has also developed a new smartphone based finger knuckle image database of 561 finger knuckle images of 187 different fingers from 109 users, in real imaging environment. In the best of our knowledge, this is the first attempt to develop a mobile phone based finger knuckle identification which has shown highly promising results in automatically identifying the users from their finger knuckle images.	algorithm;android;authentication;biometrics;c++;contactless payment;encode;finger tree;gabor filter;graphical user interface;mobile app;mobile phone;opencv;operating system;preprocessor;programming language;smartphone;usability;web application;on-line system	KamYuen Cheng;Ajay Kumar	2012	2012 BIOSIG - Proceedings of the International Conference of Biometrics Special Interest Group (BIOSIG)		computer vision;speech recognition;information privacy;computer science;graphical user interface;authorization;image segmentation;computer security	Mobile	-34.557961399565386	-44.95648292866229	69058
464700a0f9d199c0e856841ec57fced0c3a37586	analyzing intended use effects in target acquisition	fitts law;pointing;motion kinematics;intention	Recent work by Mandryk and Lough demonstrated that the movement time of Fitts-style pointing tasks varies based on intended use of a target, suggesting major implications for HCI research that models pointing using Fitts' Law. We replicate the study of Mandryk and Lough to determine exactly how and why observed movement times vary. We demonstrate that any variation in movement time is the result of differences in additive factors (a in Fitts' equation) and can be attributed to changes in the time a user spends over their primary target.	air traffic control radar beacon system;fitts's law;human–computer interaction;self-replication;utility functions on indivisible goods	Jaime Ruiz;Edward Lank	2014		10.1145/2598153.2598158	simulation;human–computer interaction;computer science;artificial intelligence;fitts's law	HCI	-46.15534108812412	-48.812095788657835	69149
f0db4b119201b6c2dcf987c72a2913d275d18fcb	the effects of teleportation on recollection of the structure of a virtual world	h 5 2 information interfaces and presentation e g hci user inter faces evaluation methodology;categories and subject descriptors according to acm ccs i 3 7 computer graphics three dimensional graphics and realism virtual reality;virtual worlds	Teleportation is a virtual world navigation technique that allows users to travel at an infinite velocity from one lo-cation to another. Unfortunately, teleportation is known to cause disorientation in many users. This paper reports on an experiment designed to explore the relationship between teleportation and recollection of the structure of a virtual world when users are provided with a map navigation aid. Thirty-six subjects were divided into two groups (teleportation and free roam) and asked to collect objects in a virtual world. The results of the study showed that subjects who navigated with teleportation completed the task significantly faster than those who free roamed, with no difference between groups in the number of errors. However, when the map was removed, subjects who previ-ously teleported committed significantly more errors and took longer than those in the free roam group. There were no differences between groups on either of the post trial map drawing or map labeling exercises.	virtual world	Daniel C. Cliburn;Stacy Rilea;David Parsons;Prakash Surya;Jessica Semler	2009		10.2312/EGVE/JVRC09/117-120	simulation;computer science;multimedia;communication	HCI	-45.05508068386956	-46.87588544940283	69262
6ce361edfd6e2e304385d386e56e0aa6839da8db	real-time vision plus remote-brained design opens a new world for experimental robotics	vision system;real time tracking;software systems;robot vision;real time vision;new world;wireless technology;visual processing	We present our approach for experimental robotics based on real-time tracking vision and remote-brained design. A robot with remote-brained design does not bring its own brain within the body but leaves the brain in the mother environment. The robot talks with it by radio links. The brain is raised in the mother environment inherited over generations. The key idea of the remote-brained approach is that of interfacing intelligent software systems with real robot bodies through wireless technology. In this framework the robot system can have a powerful vision system in the brain environment. We have applied this approach toward the creation of vision-based dynamic and intelligent behaviors in various robot configurations. In this paper we introduce our robot vision system and the remote-brained approach and describe visual processes for vision-based behaviors with remote-brained robots.	real-time transcription;robotics	Masayuki Inaba;Satoshi Kagami;Hirochika Inoue	1995		10.1007/BFb0035201	computer vision;simulation;active vision;machine vision;computer science;software system;computer graphics (images)	Robotics	-36.784795128141994	-40.07034545726288	69282
a4e3d34a333031b9d924fc34b807fe6e51c2a618	a web-based system for transformer design	information space;power supply;web based system	Despite the recent use of computer software to aid in the design of power supply components such as transformers and inductors, there has been little work done on investigating the usefulness of a webbased environment for the design of these magnetic components. Such an environment would offer many advantages, including the potential to share and view previous designs easily along with platform/OS independence. This paper presents a web-based transformer design system whereby users can create new optimised transformer designs and collaborate or comment on previous designs through a shared information space.	heterogeneous database system;list of collaborative software;mathematical optimization;operating system;planar (computer graphics);power supply;switched-mode power supply;transformer;transformers;usability;web application	John G. Breslin;W. G. Hurley	2003		10.1007/978-3-540-45224-9_97	embedded system;electronic engineering;engineering;electrical engineering	HCI	-45.44579878575613	-38.69941916625261	69385
d443bfa836f1c84542ec4ca85890c50dafbb583c	going beyond the surface: studying multi-layer interaction above the tabletop	spatially aware displays;user study;information space;above the tabletop;tangible magic lens;design guideline;multi layer interaction	Lightweight spatially aware displays (Tangible Magic Lenses) are an effective approach for exploring complex information spaces within a tabletop environment. One way of using the 3D space above a horizontal surface is to divide it into discrete parallel layers stacked upon each other. Horizontal and vertical lens movements are essential tasks for the style of multi-layer interaction associated with it. We conducted a comprehensive user study with 18 participants investigating fundamental issues such as optimal number of layers and their thickness, movement and holding accuracies, and physical boundaries of the interaction volume. Findings include a rather limited overall interaction height (44 cm), a different minimal layer thickness for vertical and horizontal search tasks (1 cm/4 cm), a reasonable maximum number of layers depending on the primary task, and a convenience zone in the middle for horizontal search. Derived from that, design guidelines are also presented.	interaction technique;layer (electronics);multidimensional digital pre-distortion;nonlinear system;thickness (graph theory);usability testing	Martin Spindler;Marcel Martsch;Raimund Dachselt	2012		10.1145/2207676.2208583	simulation;human–computer interaction;multimedia	HCI	-45.141527382819106	-45.247325665812376	69399
b0e73adfd8bd387f227d811aa4e4fb097eca8413	automatic zooming interface for tangible augmented reality applications	hit;technology;lab;tangible augmented reality;zooming interface;conference contribution paper in published proceedings;interaction method;hitlab;interface;human;nz	Tangible Augmented Reality (AR) interfaces use physical objects as a medium for interacting with virtual objects. In many cases, they track physical objects using computer vision techniques to attach corresponding virtual objects on them. However, when a user tries to have a closer look at the virtual content, the tracking can fail as the viewpoint gets too close to the physical object. To prevent this, we propose an automatic zooming method that helps users to achieve a closer view to the scene without losing tracking. By updating the zoom factor based on the distance between the viewpoint and the target object, a natural and intuitive zooming interaction is achieved. In a user study evaluating the technique, we found that the proposed method is not only effective but also easy and natural to use.	augmented reality;computer vision;interaction;usability testing;zooming user interface	Gun A. Lee;Huidong Bai;Mark Billinghurst	2012		10.1145/2407516.2407518	human–computer interaction;computer science;interface;mixed reality;multimedia;computer graphics (images);technology	HCI	-43.049338655724156	-39.808941410742136	69432
4556e314007fe5b22c9047bcca1976c070a182be	hotpaper: multimedia interaction with paper using mobile phones	frames per second;markerless linking;real time;multimedia application;tangible interface;mobile imaging;mobile phone;camera phone;indexation;multimedia data;linking paper to electronic data;mobile interaction;reading and writing	The popularity of camera phones enables many exciting multimedia applications. In this paper, we present a novel technology and several applications that allow users to interact with paper documents, books, and magazines. This interaction is in the form of reading and writing electronic information, such as images, web urls, video, and audio, to the paper medium by pointing a camera phone at a patch of text on a document. Our application does not require any special markings, barcodes, or watermarks on the paper document. Instead, we propose a document recognition algorithm that automatically determines the location of a patch of text in a large collection of document images given a small document image. This is very challenging because the majority of phone cameras lack autofocus and macro capabilities and they produce low quality images and video. We developed a novel algorithm, Brick Wall Coding (BWC), that performs image-based document recognition using the mobile phone video frames. Given a document patch image, BWC utilizes the layout, i.e. relative locations, of word boxes in order to determine the original file, page, and the location on the page. BWC runs real-time (4 frames per second) on a Treo 700w smartphone with a 312 MHz processor and 64MB RAM. Using our method we can recognize blurry document patch frames that contain as little as 4-5 lines of text and a video resolution as low as 176x144. We performed experiments by indexing 4397 document pages and querying this database with 533 document patches. Besides describing the basic algorithm, this paper also describes several applications that are enabled by mobile phone-paper interaction, such as inserting electronic annotations to paper, using paper as a tangible interface to collect and communicate multimedia data, and collaborative homework.	algorithm;barcode;book;camera phone;display resolution;experiment;frame (video);html;human–computer interaction;image resolution;microsoft outlook for mac;mobile phone;patch (computing);random-access memory;real-time data;real-time transcription;real-time web;smartphone;tangible user interface;usability testing;web page	Berna Erol;Emilio R. Antúnez;Jonathan J. Hull	2008		10.1145/1459359.1459413	computer vision;mobile interaction;computer science;operating system;multimedia;camera phone;frame rate;world wide web;computer graphics (images)	Web+IR	-40.112374897057684	-39.996990481103815	69475
6b53b4c10007f45d0ff955b469154c5f36013365	efficient interactive visualization of crowd scenes on mobile devices	crowd simulation;mocap;mobile graphics;compression;perceptual factors	The ability to view crowded public spaces in real-time has a variety of applications including virtual tourism, surveillance and sports. In addition, given the prevalence of handheld wireless devices, visualization on mobile handheld devices is beneficial for a large proportion of users. In this work we summarize our research on crowd visualization on wireless and handheld devices combining compression, 3D modeling and interactive visualization. We also synthesize crowds to validate our preliminary implementations. In order to achieve very high compression rates we use a novel perceptually adaptive approach for motion capture data compression. Our visualization interface can be used for simulation of crowds, and evaluating the design and capacity tolerances to crowds for public transit systems.	3d modeling;data compression;handheld game console;information visualization;interactive visualization;mobile device;motion capture;real-time clock;simulation;virtual tour	L. Irene Cheng;Amirhossein Firouzmanesh;Anup Basu	2014		10.1145/2669062.2669067	computer vision;motion capture;simulation;computer science;artificial intelligence;crowd simulation;multimedia;compression;computer graphics (images)	Visualization	-38.766821288442515	-40.76178437327324	69478
d6156cfd6e52b8bd83e83c179ba09927908a3eac	a method of selective stimulation to epidermal skin receptors for realistic touch feedback	force feedback haptic interfaces virtual reality;tactile texture;tactile feeling display;virtual reality;chip;force feedback;touch feedback;virtual textures selective stimulation epidermal skin receptors realistic touch feedback superficial mechanoreceptors air pressure realistic touch;epidermis skin feedback displays electrical capacitance tomography pins fingers humans agricultural engineering agriculture;haptic interfaces;haptic interface	In this paper, we propose a device to stimulate only the superficial mechanoreceptors in the skin, and report the feeling caused by the stimulus. We describe the principle of the selective stimulation using air pressure, and we show the selectivity is more advanced than that of our previous system using magnet chips which was presented last year. We experimentally confirmed that a sparse array of the superficial stimulators could display realistic touch on objects including finer virtual textures than the stimulator spacing.	experiment;selectivity (electronic);sparse matrix;the superficial	Naoya Asamura;Nozomu Yokoyama;Hiroyuki Shinoda	1999		10.1109/VR.1999.756962	computer vision;simulation;computer science;artificial intelligence;virtual reality;haptic technology	HCI	-41.08122302252616	-41.91135867613917	69627
3f63129e4f8deb0b15e2ced89ebe5feb4c478150	exploring the use of visual annotations in a remote assistance platform	instant messaging;remote assistance;task assembly;visual annotation	In this paper, we report on the evaluation of a remote assistance platform (RAP) that is designed to enable an expert to remotely assist a field operator. A user study with 16 participants was conducted to evaluate its usability with two assembly tasks that varied in their complexity. As part of the assessment, we compared the interaction behavior of our platform with a commercial instant messaging application, which lacked the ability to augment or view video imagery. The results identified differences in the completion times between the two conditions, as we examined the use of visual augmentation, including recommendations to improve the platform.	adobe freehand;canonical quantization;embedded system;instant messaging;rapid refresh;recommender system;reference architecture;remote application platform;usability testing	Mark D. Rice;Shue-Ching Chia;Hong Huei Tay;Marcus Wan;Liyuan Li;Jamie Ng;Joo-Hwee Lim	2016		10.1145/2851581.2892346	simulation;computer science;multimedia;world wide web	HCI	-44.5726684435569	-46.29898903845576	69889
9ed498c6768ff582089ed7c18bd2af641c76bef2	force and torque simulation in virtual tennis	face tracking;video conferencing;vision based control;human robot interface	This work presents an overview of the development of various ungrounded haptic devices to render a variety of tennis ball impact accelerations and torques in an interactive virtual tennis system. The primary virtual tennis system, shown in Figure 1, consists of a four wall display with front, left, right and floor, high-resolution stereoscopic projection displays, to produce an immersive experience. The secondary system, shown in Figure 2, consisted of a single stereoscopic projector display, designed to be portable and easily assembled by two persons. Both systems use an in-house developed low-latency, low-jitter, all optical tracking system. The primary system has been demonstrated to several hundred visitors, while the secondary portable system has been demonstrated at the Emerging Technologies at both SIGGRAPH 2012 [Fong et al. 2012] in Los Angeles and SIGGRAPH Asia 2011 [Fong et al. 2011] in Hong Kong. The current prototype is the V-shaped actuator based haptic racket, shown in Figure 3.	haptic technology;image resolution;immersive technology;prototype;racket;siggraph;simulation;stereoscopy;tracking system;video projector	Wee Teck Fong	2012		10.1145/2425296.2425321	human–robot interaction;computer vision;facial motion capture;simulation;computer science;artificial intelligence;multimedia;videoconferencing;computer graphics (images)	HCI	-40.73697363899397	-39.201191028553914	70040
0773c320713dae62848fceac5a0ac346ba224eca	digital facial augmentation for interactive entertainment	mirrors;eye protection;light emitting diodes;virtual object interaction digital facial augmentation interactive entertainment digital projection technology spatial augmented reality applications object tracking commodity cameras depth sensors 3d position object pose dynamic graphical content arbitrary surfaces person face 2d rigid body integration fluid integration gravity simulation integration;three dimensional displays;projection mapping;graphical user interfaces augmented reality cameras entertainment;face;augmented reality projection mapping;augmented reality;cameras;face cameras three dimensional displays tracking eye protection light emitting diodes mirrors;tracking	Digital projection technology allows for effective and entertaining spatial augmented reality applications. Leveraging the capabilities of reasonably accurate object tracking using commodity cameras and/or depth sensors to determine the 3D position and pose of objects in real time, it is possible to project dynamic graphical content on arbitrary surfaces, such as a person's face. Coupling these capabilities with a simple drawing application, participants can have the experience of “painting” on someone's face, or even on their own, by observing the projection in a mirror. Similarly, integrating 2D rigid-body, fluid and gravity simulation, one may interact with virtual objects projected on their own face or body.	3d rendering;algorithm;augmented reality;display device;dynamical simulation;fluid animation;graphical user interface;map;parallax;physics engine;prototype;sensor;shader	Naoto Hieda;Jeremy R. Cooperstock	2015	2015 7th International Conference on Intelligent Technologies for Interactive Entertainment (INTETAIN)	10.4108/icst.intetain.2015.259444	face;computer vision;augmented reality;computer science;tracking;multimedia;computer graphics (images);light-emitting diode	HCI	-42.5034901255931	-38.0193721440952	70211
4129f49813f8e80ce4de73ae0a3aad229341ee82	user interface providing support for creating artistic patterns	intelligent user interface;user interface;fuzzy associative memory;idea support;kansei information	As design work using computer spreads, usage environments are appearing which have operations involving aesthetic sensibilities via the GUIs in a variety of graphics software and have the representational capacity to represent three-dimensional images and images with the feel of oil paintings. However, their operation still relies on the users sensibilities and knowledge. In this paper, we propose a user interface for a user support system for creating artistic patterns. Its features are summarized by the following three points. First, the system has knowledge about designs. The result of design analysis provided the following knowledge. (1) The elements producing a design are the colors, shapes, and composition. (2) To turn an image into a pattern, the image is conceived of as a specific scene, then the pattern can be drawn by extracting the colors, shapes, and composition based on the scene by combining these elements. This knowledge is held in the system. Second, there is a user interface to easily transmit the image to the system. The media to convey kansei (affective information) are considered to be both linguistic and nonlinguistic. And an interface was designed to enable direct manipulation by pen or mouse to select from a list box of a three-level kansei language and to select from a pattern candidate group. Another user interface was designed which accounts for the inability of a person to grasp the kansei from the start to completion and allows repeating the kansei specification by the user and the presentation of patterns by the system. Third, there is a mechanism that automatically creates the pattern in response to the users kansei. We designed an algorithm to create a design by (1) using fuzzy logic to deduce the users image and (2) creating the pattern by using the parameters needed to draw the pattern which were extracted from the deduced image. Furthermore, we developed a support system for creating artistic patterns that has the above functions, and had multiple testers test its operation to verify the systems effectiveness in application tests on real problems. © 2001 Scripta Technica, Syst Comp Jpn, 32(11): 2037, 2001	algorithm;color;digi-comp i;direct manipulation interface;fuzzy logic;graphics software;user interface	Junko Ichino;Shun'ichi Tano	2001	Systems and Computers in Japan	10.1002/scj.1067	user interface design;user;computer vision;simulation;computer science;electrical engineering;artificial intelligence;operating system;machine learning;multimedia;user interface;algorithm	Graphics	-42.34344555065073	-38.40586226016819	70457
48684be32c9c4a50ba791ec6200262918edfd840	development of a rating system for all tennis players		A mathematically based method for evaluating the ability of tennis players is recommended to replace the ad hoc methods now followed. This method involves deriving a numerical rating (TCAP) for each player by utilizing match scores. Such ratings can be put to many uses, such as tournament placement, handicapping, match and tournament result prediction, and player progress evaluation. Extensive results have been obtained for both professional and amateur players, and indications are that eventually all serious tennis players may be rated by the same system. Finally, these results lead to insights into the game, including the conclusion that the professionals have nearly the same variability within their group as do the amateurs within theirs.		Samuel S. Blackman;James W. Casey	1980	Operations Research	10.1287/opre.28.3.489	simulation;multimedia;advertising	Robotics	-34.58243845378392	-48.52340020728817	70520
59b9fa73d56223f789d246298ff95217e6b714ed	cloud media dj platform: functional perspective		Content services have been provided to people in a variety of ways. Jukebox provides an automated music-playing service. User inserts a coin and presses a music button desired, the jukebox automatically selects and plays the record. DJs in Korean cafes receive the contents they want and play it through the speakers in the store. In this paper, we propose a service platform that reinvents the Korean cafe DJ in an integrated environment of IoT and cloud computing. In addition, we analyze the functional aspects of the services provided by the proposed platform. The user in a store requests contents (music, video, message) through the service platform. The contents are provided through the public screen and speaker in the store where the user is located. This allows people in the same location store to enjoy the contents together. The user information and the usage history are collected and managed in the public cloud. Therefore, users can receive customized services regardless of stores. Based on the implementation results of the platform, it is shown that the proposed platform provides more functions and advantages than other streaming services. Also proposed platform can provide contents efficiently to concurrent users.		Joohyun Lee;Jinwoong Jung;Sanggil Yeoum;Junghyun Bum;Thien-Binh Dang;Hyunseung Choo	2018		10.1007/978-3-030-03192-3_25	multimedia;user information;cloud computing;computer science;internet of things	Vision	-47.50717225671538	-39.42305632030955	70663
10f248284008a214e6b5ee70ce25590ae9ef6ab7	tactile data entry for extravehicular activity	space suit gloves;integrated information system;keyboards gesture recognition graphical user interfaces haptic interfaces human computer interaction;standard computing interface;keyboards;touch based context;microelectromechanical systems;human computer interaction;humans keyboards tactile sensors nasa fingers accelerometers target tracking;astronauts;data entry performance tactile feedback extravehicular activity space suit gloves;tactile sensor;extravehicular activity;gloves;space suits;virtual keyboard;light emitting diodes;simulation;data processing;graphical user interface;motion tracking;human subjects;hand anatomy;instrumented eva gloves;graphical user interfaces;data entry performance;tactile feedback;vibrotactile information;fingers;tactile data entry;suit integrated information system;tactile sensors;graphic user interface;task saturated environment;instrumented eva gloves tactile data entry task saturated environment extravehicular activity astronaut ability suit integrated information system standard computing interface hand motion tracking finger gesture recognition virtual keyboard tactile feedback touch based context graphical user interface vibrotactile information;humans;computer components;tactile sensors robotics;target tracking;haptic interfaces;d haptics;finger gesture recognition;accelerometers;nasa;gesture recognition;astronaut ability;hand motion tracking;human computer interface	In the task-saturated environment of extravehicular activity (EVA), an astronaut's ability to leverage suit-integrated information systems is limited by a lack of options for data entry. In particular, bulky gloves inhibit the ability to interact with standard computing interfaces such as a mouse or keyboard. This paper presents the results of a preliminary investigation into a system that permits the space suit gloves themselves to be used as data entry devices. Hand motion tracking is combined with simple finger gesture recognition to enable use of a virtual keyboard, while tactile feedback provides touch-based context to the graphical user interface (GUI) and positive confirmation of keystroke events. In human subject trials, conducted with twenty participants using a prototype system, participants entered text significantly faster with tactile feedback than without (p = 0.02). The results support incorporation of vibrotactile information in a future system that will enable full touch typing and general mouse interactions using instrumented EVA gloves.	activity recognition;computer keyboard;computer mouse;display device;event (computing);gesture recognition;graphical user interface;helmet-mounted display;information system;interaction;line-of-sight (missile);point and click;prototype;smartphone;touch typing;virtual keyboard;wired glove	Richard J. Adams;Aaron B. Olowin;Blake Hannaford;O. Scott Sands	2011	2011 IEEE World Haptics Conference	10.1109/WHC.2011.5945503	embedded system;computer vision;simulation;data processing;computer science;engineering;operating system;gesture recognition;graphical user interface;microelectromechanical systems	HCI	-43.228246460960015	-43.8229430346202	70702
737bf1899b4092b79b8ec03df3844ffa3d3c4134	investigating touch interactions for an augmented world	user evaluation;convergence;unobtrusive;portable devices;touch screen;natural interaction;touch interaction;user testing;private scope;augmented reality	Touch screen interaction usually requires the user to view the input surface in order to make their selections. When the interaction platform is purposefully occluded to allow for natural interaction with an augmented reality (AR) system new issues are raised in regard to the usability of the touch sensitive interface.  This paper details a user evaluation scenario that we have conducted looking at pen-based selection techniques for a personal, light-weight AR system and introduces a trial for manipulation testing that we are currently conducting. By testing various techniques we are identifying a combination of operations that will enable effective and usable communication with an unobtrusive, mobile AR system.	augmented reality;bmc remedy action request system;computer user satisfaction;interaction;interaction technique;realization (linguistics);touchscreen;unobtrusive javascript;usability	Brett Wilkinson;Paul R. Calder	2008		10.1145/1517744.1517752	augmented reality;simulation;convergence;human–computer interaction;computer science;multimedia	HCI	-46.941030384835024	-43.60792225670886	70973
39045a8259f9a07cf8b93481097bb5518eca70c8	naturalistic human-robot collaboration mediated by shared communicational modality in teleoperation system	robot movil;robot humanoide;humanoid robot;operateur humain;evaluation systeme;operador humano;interaction;evaluacion sistema;robotics;remote operation;system evaluation;robot mobile;mixed initiative interaction;teleaccion;human operator;robotica;interaccion;robotique;moving robot;teleoperation	"""This paper presents a new style of human-robot collaboration in a teleoperation system where the robot has the autonomy to control its behavior. Our model provides the """"shared communicational modality"""" between the human operator and the robot autonomy to promote their mixed-initiative interactions. This paper describes the results of experiments using our developing system to evaluate our model, and discusses the interactions between the two autonomies based upon the Lens model framework known as a judgment analysis method."""	modality (human–computer interaction)	Yukio Horiguchi;Tetsuo Sawaragi	2001		10.1007/3-540-45336-9_7	computer vision;teleoperation;interaction;simulation;computer science;humanoid robot;artificial intelligence;robotics	Robotics	-39.92252307947922	-48.62161838344736	71077
f282cad5b1c30ca1b31ceda0fdcc7ffda6396132	tablet fish tank virtual reality: a usability study		In this paper, we describe the development a tablet FTVR prototype that incorporates both motion parallax and stereo cues with the use of easy-to-find hardware. We also present findings of a usability study based on the prototype. Experiment: We conducted an experiment on the usability of our tablet FTVR prototype using the visual search task from the comparative study between CAVE and FTVR [DJK*06]. To perform the task, participants had to identify the location of a rectangular bump on the surface of a noisy potato-shaped object then move it under a pole by rotating the potato using the arrow keys at the bottom of the display. Tablet Fish Tank Virtual Reality: To achieve tablet FTVR without any enhancement to the hardware itself, we combine Anaglyph 3D for stereopsis with head position tracking from the tablet’s front camera. For stereo, we use Anaglyph 3D images. For motion parallax, following previous studies [FN11, Rek95]. See Algorithm 1 for more detail. We used the Unity game engine to develop the application, and ran it on an iPad Air (model number A1474). The application operates in four view modes: Normal 2D (2D), Head-coupled display (HCD), Anaglyph 3D (Anaglyph), and Combined view mode (Combined). Results: Here we present the results of the experiment. We dropped the data for one participant from all analyses because the time the individual took to complete the task was many standard deviations beyond the mean. The objective data are summarised in Table 1. The results of the comparison between the Normal 2D view mode and the combined view mode are summarized in Figure 2. Discussion & Conclusion: The first question is How effective is tablet FTVR? Although there were no statistically significant differences between the view modes for PQ scores, we suspect that this was more because of the visual discomfort from Anaglyph 3D and the front-facing camera-based tracking technique’s limitations than anything else. The comparison results suggest that participants perceived depth and felt that a virtual object existed in front of them more in the Combined view mode, when compared to the Normal 2D view mode. Our findings coincide with those of Li et al. [LPWL12]. We suspect that participants were unable to perform the task better in the Combined view mode because of the front-facing camera-based tracking technique’s limitations. This coincides with a study by Kongsilp and Dailey [KD17], who found that in desktop FTVR settings, the combination of motion parallax and stereopsis cues produces lower visual discomfort and higher subjective level of presence when compared to the stereopsis cue only. The last question is If it is useful, should we develop a new system or enhance existing devices? We believe that it would be best to develop a new system from scratch if we absolutely require stereoscopic displays. Both polarized 3D and active shutter 3D technologies would require a fair amount of hardware changes to today’s commodity tablets. Reference: ● [DJK*06] DEMIRALP C., JACKSON C. D., KARELITZ D. B., ZHANG S., LAIDLAW D. H.: Cave and fishtank virtual-reality displays: A qualitative and quantitative comparison. IEEE Transactions on Visualization and Computer Graphics 12, 3 (2006), 323–330. 1 ● [FN11] FRANCONE J., NIGAY L.: Using the user’s point of view for interaction on mobile devices. In Proceedings of 23rd French Speaking Conference on Human-Computer Interaction (2011), ACM, p. 4. 1 ● [KD17] KONGSILP S., DAILEY M. N.: Motion parallax from head movement enhances stereoscopic displays by improving presence and decreasing visual fatigue. Displays 49 (2017), 72–79. 2 ● [KLBL93] KENNEDY R. S., LANE N. E., BERBAUM K. S., LILIENTHAL M. G.: Simulator sickness questionnaire: An enhanced method for quantifying simulator sickness. The international journal of aviation psychology 3, 3 (1993), 203–220. 1 ● [LPWL12] LI I. K., PEEK E. M., WÜNSCHE B. C., LUTTEROTH C.: Enhancing 3d applications using stereoscopic 3d and motion parallax. In Proceedings of the Thirteenth Australasian User Interface ConferenceVolume 126 (2012), Australian Computer Society, Inc., pp. 59–68. 2 ● [Rek95] REKIMOTO J.: A vision-based head tracker for fish tank virtual reality-vr without head gear. In Proceedings of Virtual Reality Annual International Symposium, 1995. Proceedings. (1995), IEEE, pp. 94–100. 1 ● [WS98] WITMER B. G., SINGER M. J.: Measuring presence in virtual environments: A presence questionnaire. Presence: Teleoperators and virtual environments 7, 3 (1998), 225–240. 1 We recruited 40 participants (30 male and 10 female, age ranging from 17 to 31 years old). We used a 2 × 2 experimental design in which each participant was assigned to the Normal 2D group, the Head-coupled group, the Anaglyph 3D group, or the Combined group. There were 20 random trials for each participant (1 view mode × 4 difficulty levels × 5 repetitions, giving 20 trials). When the participant completed the task, the researcher immediately asked the participant to answer the Simulation Sickness Questionnaire (SSQ) [KLBL93], followed by the Presence Questionnaire (PQ) [WS98], and compare the two view modes and give his or her preference for each view mode. Figure 1: Test application and the four levels of noise. Figure 2: Users’ preference between the Normal 2D and the Combined view modes along the seven dimensions.	algorithm;anaglyph 3d;australasian conference on information systems;bump mapping;computer graphics;design of experiments;desktop computer;experiment;game engine;human–computer interaction;mobile device;movie projector;normal (geometry);parallax;polarized 3d system;prototype;simulation;stereopsis;stereoscopy;tablet computer;unity;usability testing;user interface;virtual reality;ipad	Sirisilp Kongsilp;Mintra Ruensuk;Matthew N. Dailey;Takashi Komuro	2017		10.2312/egve.20171377	heuristic evaluation;usability lab;human–computer interaction;pluralistic walkthrough;usability;usability engineering;multimedia;virtual reality;computer science	Visualization	-45.81214197310422	-47.410092715124954	71148
8edbb1f6813737e8e245863f3759117d8b83d0d9	exploring a novel inexpensive tangible interface for non-visual math and science		Tangible interaction enables physical manipulation of digital data, making it ideal to support visually impaired students. Visually impaired students are frequently integrated in mainstream courses, collaborating with sighted peers and instructors. This paper describes a tangible block localization and tracking method, using small inexpensive sensor packages that detect color placed on an interaction surface—i.e., a standard flat-screen display. The system recursively subdivides the display surface into regions of distinct colors that the sensor package can distinguish. Once located, the sensor package can be tracked by moving the color pattern underneath to follow it, re-expanding the pattern as needed to capture the sensor package if it moves too fast. The novel tracking infrastructure supports novel approaches to teach a number of science and math concepts to visually impaired students.	tangible user interface	Rhonda Stanton;Enrico Pontelli;Z. Toups;Muhanad S. Manshad	2018		10.1007/978-3-319-94277-3_96	digital data;recursion;computer vision;mathematics;artificial intelligence	HCI	-44.76422544492326	-40.01692654232169	71273
55ae93809375a45295c6a919a73b39f58584677d	a tricycle-style teleoperational interface that remotely controls a robot for classroom children	educational institutions acceleration sensors mobile robots internet;remote control;sensors;field test;mobile robots;satisfiability;acceleration;english learning schools tricycle style teleoperational interface classroom children telepresence robots childhood education teleoperational robot system;distant communication;internet;robotics for children;telerobotics education;telepresence robot;tricycle;early childhood education;teleoperational interface telerobotics telepresence robot distant communication distant education robotics for children early childhood education tricycle;telerobotics;teleoperational interface;distant education	We consider the application of telepresence robots for supporting childhood education. One challenge here is to develop a teleoperational robot system that can be manipulated by children themselves. There are two requirements for realizing such a system. First, the system has to be sufficiently intuitive so that child users can control it without the need for detailed instructions. Second, the control of the system should have some amount of enjoyment so that child users do not get bored. To satisfy these requirements, we introduce a tricycle-style teleoperational interface that remotely controls a robot. We also report field tests that are currently being conducted at English learning schools for children in Japan.	requirement;robot	Fumihide Tanaka;Toshimitsu Takahashi	2012	2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/2157689.2157782	telerobotics;simulation;early childhood education;computer science;artificial intelligence;multimedia	Robotics	-42.022006766872	-45.20032746861981	71316
9a2483aaaf4378b8f9c18b4a035784e186aaffe7	throw and tilt - seamless interaction across devices using mobile phone gestures		To overcome limitations of small screens and to provide intuitive ways of interacting with personal data, this work addresses the seamless combination of sensor-enabled phones with large displays. An intuitive basic set of tilt gestures is introduced for a stepwise or continuous interaction with both mobile applications and distant user interfaces by utilizing the handheld as a remote control. In addition, we introduce throwing gestures to transfer media documents to a large display. By means of these gestures, we also propose transferring a running interface from a mobile phone to a large screen (to improve usability) and back (to achieve mobility). We demonstrate the feasibility of the interaction methods with several application prototypes facilitating a very natural flow of interaction.	handheld game console;interaction;mobile app;mobile device;mobile phone;personally identifiable information;privacy;remote control;scrolling;seamless3d;software engineering;software prototyping;stepwise regression;usability;user interface	Raimund Dachselt;Robert Buchholz	2008			embedded system;human–computer interaction;mobile search;mobile technology;mobile computing;mobile phone tracking;mobile station;mobile web;gsm services;user interface;computer science	HCI	-46.290622720931296	-42.290492027647105	71537
11147fc8168512b956542837f06182f22bbb985a	a coherent assessment of visual ergonomics in flight deck impacted by color and luminance	performance;visual ergonomics;coherent assessment;psychological indices	This research proposed a coherent assessment method for evaluating visual ergonomics in simulated flight deck via evaluating psychological indices and performance during a series of experiments. A simulated flight deck environment was established according to the dimensions of a real commercial aircraft cockpit with back projection, and then a balanced sequence of pseudo random variates was generated with replicated Latin square design. After the experiment of 18 interior color levels and 2 luminance levels, a complex statistical analysis was conducted to examine the significance as well as correlations within different factors. The fluctuation of luminance can affects the results slightly, while the change of the interior color, both hue and saturation-intensity levels, can influence subjects' visual ergonomics significantly and interactively. This coherent assessment indicates that light blue was the best choice, whereas, vivid yellow was the last among the 18 colors.	coherent;human factors and ergonomics	Ye Zhou;Wei Zhang;Baofeng Li;Jinhai Yu;Zhi Ma	2013		10.1007/978-3-642-39354-9_25	computer vision;simulation;engineering;computer graphics (images)	Vision	-43.16273408449775	-51.76946552102227	71688
3ab1ad7a0772077371a2f7315026ab5e641b1f95	podcastle and songle: crowdsourcing-based web services for retrieval and browsing of speech and music content		This paper describes two web services, PodCastleandSongle, that collect voluntary contributions by anonymous users in order to improve the experiences of users listening to speech and music content available on the web. These services use automatic speechrecognition and music-understanding technologies to provide content analysis results, such as full-text speech transcriptions and music scene descriptions, that let users enjoy content-based multimedia retrieval and active browsing of speech and music signals without relying on metadata. When automatic content analysis is used, however, errors are inevitable. PodCastle and Songle therefore provide an efficient error correction interface that let users easily correct errors by selecting from a list of candidate alternatives.	browsing;crowdsourcing;error detection and correction;module file;web service	Masataka Goto;Jun Ogata;Kazuyoshi Yoshii;Hiromasa Fujihara;Matthias Mauch;Tomoyasu Nakano	2012			computer science;multimedia;communication;world wide web	Web+IR	-34.789619614708734	-47.636845568517295	72012
29bd6bd0565391db118440181f9402699adcfc10	error-aware gaze-based interfaces for robust mobile gaze interaction		Gaze estimation error can severely hamper usability and performance of mobile gaze-based interfaces given that the error varies constantly for different interaction positions. In this work, we explore error-aware gaze-based interfaces that estimate and adapt to gaze estimation error on-the-fly. We implement a sample erroraware user interface for gaze-based selection and different error compensation methods: a naïve approach that increases component size directly proportional to the absolute error, a recent model by Feit et al. that is based on the two-dimensional error distribution, and a novel predictive model that shifts gaze by a directional error estimate. We evaluate these models in a 12-participant user study and show that our predictive model significantly outperforms the others in terms of selection rate, particularly for small gaze targets. These results underline both the feasibility and potential of next generation error-aware gaze-based user interfaces.	approximation error;image scaling;naivety;predictive modelling;real-time clock;seamless3d;usability testing;user interface	Michael Barz;Florian Daiber;Daniel Sonntag;Andreas Bulling	2018		10.1145/3204493.3204536	gaze;eye tracking;machine learning;approximation error;usability;mobile interaction;user interface;artificial intelligence;computer science	HCI	-45.49121568102772	-45.44154672466069	72258
1d0982358ab8b938ae3f5d4ea31c77d52440ae0f	embodiment of an agent by anthropomorphization of a common object	printing;lasers;anthropomorphism character generation intelligent agent graphics humanoid robots humans joining processes natural languages refrigerators rfid tags;printers;anthropomorphization human agent interaction human interface;computer graphics;robovie common object anthropomorphization human agent interaction humanoid parts anthropomorphic agents computer graphics agents communication robots independent humanoid agent;human robot interaction;computer graphic;human robot interaction computer graphics humanoid robots;human interface;humanoid robots;robots;human agent interaction;humans;iris;robot kinematics;anthropomorphization	"""We propose a direct anthropomorphization to improve human-agent interaction. It agentizes an artifact by attaching humanoid parts to it. There have been many studies that can provide valuable information on using spoken directions and gestures via anthropomorphic agents such as CG(Computer graphics) agents and communication robots. Our method directly anthropomorphizes the artifact through robotic bodily parts shaped like those of humans. An anthropomorphized artifact with these parts can provide information to people by giving them spoken directions and expressing themselves through body language. This makes people pay more attentions to the artifact, than when using anthropomorphic CG or robot agents. We conducted an experiment to verify the difference between receiving an explanation of the functions of the artifact using the direct anthropomorphization method and that from using the independent humanoid agent """"Robovie"""". The results from participants' questionnaires and gazes during the experiment indicated that they noticed the target artifact and memorized the functions more quickly and easily from using the direct anthropomorphization method than from the """"Robovie""""."""	cg (programming language);computer graphics;robot	Hirotaka Osawa;Ren Ohmura;Michita Imai	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.129	human–robot interaction;robot;computer vision;simulation;laser;computer science;humanoid robot;artificial intelligence;computer graphics;robot kinematics;human interface device	Robotics	-42.52510237917044	-44.23524084606887	72283
9194c8eccf02e02fb3a048736593365e6487972d	on the usage of smart devices to augment the user interaction with multimedia applications		Wearable devices have recently gained a foothold in the market with the uptake of smartwatches. The strong tie between a smartwatch and its owner, the highly predictable position of a smartwatch on the body, and its internal sensors are enabling a wide array of applications that leverage the user context. In this paper we focus on a gesture recognition system to augment the user interaction with multimedia applications. We define a set of seven gestures that are relevant across several applications and we collect an extensive dataset with two smartwatches (the Motorola Moto360 and Apple's Watch). We use Long Short Term Memory neural networks for gesture recognition based on sensor data from both smartwatches. We provide an extensive evaluation of the classification accuracy of the system and provide a sensitivity analysis to find the Long Short Term Memory configuration that maximizes the classification accuracy. We also show the extent to which Long Short Term Memory neural networks outperform traditional machine learning approaches. We also illustrate an application we built for the Android and iOS platforms that allows developers to easily integrate the gesture recognition in their own systems. We conclude the paper with a description of use cases to underscore the potential impact of our contribution.		Alan Ferrari;Vanni Galli;Daniele Puccinelli;Silvia Giordano	2017	2017 IEEE 18th International Symposium on A World of Wireless, Mobile and Multimedia Networks (WoWMoM)	10.1109/WoWMoM.2017.7974316	knowledge representation and reasoning;gesture recognition;multimedia;artificial neural network;wearable technology;android (operating system);smartwatch;computer science;gesture;use case	Mobile	-36.50455549167165	-46.92045465657759	72710
56f49dc4de023058d97ce1c5057f70cd2779a4a9	accessible multimodal media center application for blind and partially sighted people	media center;speech synthesis;physical disability;digital television;interface design;accessibility;gestures;graphic user interface;haptic feedback;speech recognition	We present a multimodal media center interface designed for blind and partially sighted people. It features a zooming focus-plus-context graphical user interface coupled with speech output and haptic feedback. A multimodal combination of gestures, key input, and speech input is utilized to interact with the interface. The interface has been developed and evaluated in close cooperation with representatives from the target user groups. We discuss the results from longitudinal evaluations that took place in participants’ homes, and compare the results to other pilot and laboratory studies carried out previously with physically disabled and nondisabled users.	graphical user interface;haptic technology;multimodal interaction;windows media center	Markku Turunen;Hannu Soronen;Santtu Pakarinen;Juho Hella;Tuuli Laivo;Jaakko Hakulinen;Aleksi Melto;Juha-Pekka Rajaniemi;Erno Mäkinen;Tomi Heimonen;Jussi Rantala;Pellervo Valkama;Toni Miettinen;Roope Raisamo	2010	Computers in Entertainment	10.1145/1902593.1902595	digital television;human–computer interaction;computer science;interface design;accessibility;graphical user interface;multimedia;haptic technology;natural user interface;gesture;speech synthesis	HCI	-47.96257170985576	-39.12301241268581	72890
0dde04a7e0ff344e17b42d2f3a3d5ba97d5e7a16	conventionalized gestures for the interaction of people in traffic with autonomous vehicles	human computer interaction;autonomous vehicle;gestures	The first autonomous vehicles are already tested in the public traffic. The rapid development in bringing this technology on roads attracts growing attention of research in the human interaction with autonomous vehicles. This paper focuses on the interaction of other road users with autonomous vehicles. These road users may be pedestrians who negotiate their right of way, other human drivers sharing the same road, or human traffic control officers. In order to learn about these road users in general, this paper aims to identify first the formalized hand signals applied by officers. The paper answers the question whether there is a general and universal language to interact with traffic. If so, then future work can identify elements of this universal language in the gestures of other road users, and facilitate an understanding between them and autonomous vehicles.	autonomous robot	Surabhi Gupta;Maria Vasardani;Stephan Winter	2016		10.1145/3003965.3003967	simulation;engineering;transport engineering;communication	Robotics	-37.21482602766614	-38.479558169653835	72908
7e9103c40aab5c448815709c2b28cd362e4dede3	multi-modal integration for personalized conversation: towards a humanoid in daily life	humanoid robot;image processing;real time;human robot interaction;personalized conversation;multimodal integration;spoken dialogue system;distant speech recognition;robot vision;humanoid robots;speech recognition;robot eyes;spoken language communication ability;sound source separation;source separation;camera;turn taking timing control	Humanoid with spoken language communication ability is proposed and developed. To make humanoid live with people, spoken language communication is fundamental because we use this kind of communication every day. However, due to difficulties of speech recognition itself and implementation on the robot, a robot with such an ability has not been developed. In this study, we propose a robot with the technique implemented to overcome these problems. This proposed system includes three key features, image processing, sound source separation, and turn-taking timing control. Processing image captured with camera mounted on the robotpsilas eyes enables to find and identify whom the robot should talked to. Sound source separation enables distant speech recognition, so that people need no special device, such as head-set microphones. Turn-taking timing control is often lacked in many conventional spoken dialogue system, but this is fundamental because the conversation proceeds in real-time. The effectiveness of these elements as well as the example of conversation are shown in experiments.	covox speech thing;dialog system;experiment;image processing;microphone;modal logic;personalization;real-time transcription;robot;source separation;speech recognition;spoken dialog systems	Shinya Fujie;Daichi Watanabe;Yuhi Ichikawa;Hikaru Taniyama;Kosuke Hosoya;Yoichi Matsuyama;Tetsunori Kobayashi	2008	Humanoids 2008 - 8th IEEE-RAS International Conference on Humanoid Robots	10.1109/ICHR.2008.4756014	human–robot interaction;computer vision;speech recognition;image processing;computer science;humanoid robot;artificial intelligence	Robotics	-36.95569385785077	-42.97809758605428	73066
2428c8cd680d0c8239a9c8ce8921e12f368eac43	multimodal signal processing and learning aspects of human-robot interaction for an assistive bathing robot		We explore new aspects of assistive living on smart human-robot interaction (HRI) that involve automatic recognition and online validation of speech and gestures in a natural interface, providing social features for HRI. We introduce a whole framework and resources of a real-life scenario for elderly subjects supported by an assistive bathing robot, addressing health and hygiene care issues. We contribute a new dataset and a suite of tools used for data acquisition and a state-of-the-art pipeline for multimodal learning within the framework of the I -Support bathing robot, with emphasis on audio and RGB- D visual streams. We consider privacy issues by evaluating the depth visual stream along with the RGB, using Kinect sensors. The audio-gestural recognition task on this new dataset yields up to 84.5%, while the online validation of the I-Support system on elderly users accomplishes up to 84% when the two modalities are fused together. The results are promising enough to support further research in the area of multimodal recognition for assistive social HRI, considering the difficulties of the specific task.	data acquisition;human–robot interaction;kinect;multimodal interaction;multimodal learning;privacy;real life;robot;sensor;signal processing	Athanasia Zlatintsi;Isidoros Rodomagoulakis;Petros Koutras;A. C. Dometios;Vassilis Pitsikalis;Costas S. Tzafestas;Petros Maragos	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461568	artificial intelligence;task analysis;computer vision;robot learning;natural user interface;multimodal learning;robot;visualization;computer science;gesture;human–robot interaction	Robotics	-36.88886637254963	-45.321019874027705	73079
bb3fa73a825576d6774bfb66969e58de4e6b3e80	telepresence racing game	remote control;feature detection;camera phones;sensors and actuators;mobile games;3d graphics	"""Telepresence enables users to interact with and experience """"real"""" environments by providing the display and means of interaction using sensors and actuators. In this demonstration, we apply the concept of telepresence to the good old sports car racing game. The user remotely controls, from a desktop computer, a toy car that is situated in a remote environment and equipped with wireless stereo-cameras and other sensors. The experience of the user is enriched by the information displayed to the user, namely the live stereo image and artificial multimodal special effects generated from the information from the sensors. The proposed telepresence game is expected to offer a different kind of fun and excitement compared to the 3D graphics based counterparts because the user does not have to suspend one's disbelief, and has to be aware and conscious of the actual physical implications of one's action. In the actual demo, two telepresence toy cars will run on a small mock up race track (thus requiring two control PC's)."""	3d computer graphics;as-interface;desktop computer;experience;mock object;multimodal interaction;sensor;situated;stereo cameras	Yongjin Kim;Jaehoon Jung;Seokhee Jeon;Sangyoon Lee;Gerard Jounghyun Kim	2005		10.1145/1178477.1178567	simulation;engineering;multimedia;computer graphics (images)	HCI	-46.083766315165235	-39.670976905971514	73099
143ec793005d59b9908997a8af46a170e6cd7cd9	understanding interactive legends: a comparative evaluation with standard widgets	numerical technique;information visualization;data type;visual representation;h 5 information interfaces and presentation miscellaneous	Interactive information visualization systems rely on widgets to allow users to interact with the data and modify the representation. We define interactive legends as a class of controls combining the visual representation of static legends and interaction mechanisms of widgets. As interactive legends start to appear in popular websites, we categorize their designs for common data types and evaluate their effectiveness compare to standard widgets. Results suggest that 1) interactive legends can lead to faster perception of the mapping between data values and visual encodings and 2) interaction time is affected differently depending on the data type. Additionally, our study indicates superiority both in terms of perception and interaction of ordinal controls over numerical ones. Numerical techniques are mostly used in today’s systems. By providing solutions to allowing users to modify ranges interactively, we believe that interactive legends make it possible to increase the use of ordinal techniques for visual exploration.	categorization;global variable;information visualization;interaction technique;interactive storytelling;interactivity;numerical analysis;ordinal data	Nathalie Henry Riche;Bongshin Lee;Catherine Plaisant	2010	Comput. Graph. Forum	10.1111/j.1467-8659.2009.01678.x	computer vision;information visualization;human–computer interaction;data type;computer science;theoretical computer science;multimedia;programming language;computer graphics (images)	HCI	-47.486095390866936	-48.254736975192	73201
87965ea5c0b74a883c651f39c5f54a2b4fa1b09e	measuring the performance of laser spot clicking techniques	on off on technique laser spot clicking techniques remote human computer interaction laser pointer toggle switch remote pointing technique desktop mouse fitts test one direction tapping test iso ts 9241 411 procedure pointing device on off technique;human computer interaction;laser beam applications human computer interaction interactive devices;mice error analysis laser applications throughput iso standards performance evaluation;laser beam applications;interactive devices	Laser spot clicking technique is the term of remote interaction technique between human and computer using a laser pointer as a pointing device. This paper is focused on the performance test of two laser spot clicking techniques. An off-the-shelf laser pointer has a toggle switch to generate a laser spot, the presence (ON) or absence (OFF) of this spot and its combination are the candidates of the interaction technique. We conducted empirical study that compared remote pointing technique performed using combination of ON and OFF of the laser spot, ON-OFF and ON-OFF-ON, and using a desktop mouse as a baseline comparison. We present quantitative performance test based on Fitts' test using a one-direction tapping test in ISO/TS 9241-411 procedure; and assessment of comfort using a questionnaire. We hope this result give contribution to the interaction technique using laser pointer as a pointing device especially in selecting the appropriate clicking technique for real application. Our results suggest ON-OFF technique has positive advantages over ON-OFF-ON technique such as the throughput and comfort.	baseline (configuration management);desktop computer;fitts's law;interaction technique;pointer (computer programming);pointing device;switch;throughput	Romy Budhi Widodo;Takafumi Matsumaru	2013	2013 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2013.6739639	simulation;computer hardware;computer science;engineering;computer graphics (images)	Robotics	-46.13781325182361	-46.03735939896299	73246
2dddff4da3830f68e66b704cc24e3ce9cd8b2fc0	"""picopet: """"real world"""" digital pet on a handheld projector"""	digital pet;social interaction;handheld projector;design and implementation;game;physical environment	We created PicoPet, a digital pet game based on mobile handheld projectors. The player can project the pet into physical environments, and the pet behaves and evolves differently according to the physical surroundings. PicoPet creates a new form of gaming experience that is directly blended into the physical world, thus could become incorporated into the player's daily life as well as reflecting their lifestyle. Multiple pets projected by multiple players can also interact with each other, potentially triggering social interactions between players. In this paper, we present the design and implementation of PicoPet, as well as directions for future explorations.	digital pet;handheld game console;handheld projector;interaction;movie projector;video projector	Yuhang Zhao;Chao Xue;Xiang Cao;Yuanchun Shi	2011		10.1145/2046396.2046398	games;social relation;simulation;computer science;multimedia;computer graphics (images)	HCI	-46.569099513177406	-39.77845707665425	73247
067be324c355e8eec804b01c68d5150c03d73567	the entrance authentication and tracking systems using object extraction and the rfid tag	2-step authentication;rfid;image processing.	In this paper, the proposal system can achieve the more safety of RFID System with the 2-step authentication procedures for the enhancement about the security of general RFID systems. After au- thentication RFID Tag, additionally, the proposal system extract the characteristic information in the user image for acquisition of the addi- tional authentication information of the user with the camera. In this paper, the system which was proposed more enforce the security of the automatic entrance and exit authentication system with the cognitive characters of RFID Tag and the extracted characteristic information of the user image through the camera. The RFID system which use the active tag and reader with 2.4GHz bandwidth can recognize the tag of RFID in the various output manner. Additionally, when the RFID system have errors, the characteristic information of the user image is designed to replace the RFID system as it compare with the similarity of the color, outline and input image information which was recorded to the database previously. In the result of experiment, the system can acquire more exact results as compared with the single authentication system when it using RFID Tag and the information of color characteristics.	authentication;radio-frequency identification	Dae-Gi Min;Jae-Woo Kim;Moon-Seog Jun	2011		10.1007/978-3-642-20975-8_33	internet privacy;world wide web;computer security	HCI	-34.70077608592572	-44.986470084837535	73262
14e629ead9528a9554b249bdc44134a4ac7c22c9	haptic guidance: experimental evaluation of a haptic training method for a perceptual motor skill	motor skills;virtual reality haptic interfaces computer aided instruction computer based training;computer aided instruction;training;haptic interfaces virtual environment shape measurement humans feedback mechanical engineering surgery position measurement timing education;virtual reality;motor learning haptics skills training haptic guidance training conditions training perceptual motor skills virtual environments;shape measurement;virtual environments;haptics;training conditions;mechanical engineering;feedback;perceptual motor skills;computer based training;position measurement;surgery;guidance;humans;experimental evaluation;virtual environment;motor learning;haptic interfaces;skills training;haptic guidance;haptic interface;timing	In this paper we investigate a use of haptics for skills training which we call haptic guidance. In the haptic guidance paradigm, the subject is physically guided through the ideal motion by the haptic interface, thus giving the subject a kinesthetic understanding of what is required. Subjects learned a complex 3D motion under three training conditions (haptic, visual, haptic and visual) and were required to manually reproduce the movement under two recall conditions (with vision, without vision). Performance was measured in terms of position, shape, timing, and drift. Findings from this study indicate that haptic guidance is effective in training. While visual training was better for teaching the trajectory shape, temporal aspects of the task were more effectively learned from haptic guidance. This supports a possible role for haptics in the training of perceptual motor skills in virtual environments.	alternating occlusion training;cognition;experiment;haptic technology;programming paradigm;simulation;virtual reality	David Feygin;Madeleine Keehner;Frank Tendick	2002		10.1109/HAPTIC.2002.998939	stereotaxy;computer vision;simulation;computer science;multimedia	Robotics	-46.86894501568949	-49.940922327597946	73266
ad3f35a59892948a6ed3fe03c9a04569ac7580e3	intuitive substitute interface	interaction;wearable device;substitute;markerless	This paper proposes the “Substitute Interface” to utilize the flat surfaces of objects around us as part of an ad hoc mobile device. The substitute interface is established by the combination of wearable devices such as a head-mounted display with camera and a ring-type microphone. The camera recognizes which object the user intends to employ. When the user picks up and taps the object, such as a notebook, a virtual display is overlaid on the object, and the user can operate the ad hoc mobile device as if the object were part of the device. Display size can be changed easily by selecting a larger object. The user’s pointing/selection action is recognized by the combination of the camera and the ring-type microphone. We first investigate the usage scene of tablet devices and create a prototype that can operate as a tablet device. Experiments on the prototype confirm that the proposal functions as intended.	digital video;display size;head-mounted display;hoc (programming language);microphone;mobile device;mobile phone;portable object (computing);prototype;tablet computer;wearable technology	Mikiko Nakanishi;Tsutomu Horikoshi	2013	Personal and Ubiquitous Computing	10.1007/s00779-013-0651-5	interaction;simulation;computer hardware;computer science;machine learning;multimedia	HCI	-44.01127455480515	-41.18013680896544	73323
5001eaf68b3c9ee3885c17a32e6a1a640132a0bb	pointer delegation for group collaboration using telepointers	proxy;group awareness;voting;group collaboration;telepointer;interactive environment	Pointer delegation, a new function for a telepointer, allows people to delegate the rights of their own pointers to the pointer of someone who they can trust, which helps to achieve better group collaboration through a kind of fair voting system in interactive environments that include many people. A telepointer that has a pointer delegation function is called a delegate pointer. The appearance of delegate pointers in an interactive environment can show the weight of voting. Moreover, delegate pointers can be used to limit the manipulators of objects to only one or a small group in order to reduce any conflicting manipulation. Experiments were conducted to evaluate the appropriateness of the appearance of pointers to show the voting weight, and to test the feasibility of the delegate pointer.	experiment;norm (social);pointer (computer programming);telepointer	Noritaka Osawa	2007		10.1145/1240866.1241049	proxy;real-time computing;voting;computer science;delegate;distributed computing;computer security	HCI	-40.72646746011191	-50.53231918045631	73327
3f4e43d457d07b29a60530d57caea284c51a11f3	remotefusion: real time depth camera fusion for remote collaboration on physical tasks	fusion;visualization;spatial augmented reality;spatial user interfaces;remote guidance	Remote guidance systems allow humans to collaborate on physical tasks across large distances and have applications in fields such as medicine, maintenance and working with hazardous substances. Existing systems typically provide two dimensional video streams to remote participants, and these are restricted to viewpoint locations based on the placement of physical cameras. Recent systems have incorporated the ability of a remote expert to annotate their 2D view and for these annotations to be displayed in the physical workspace to the local worker. We present a prototype remote guidance system, called RemoteFusion, which is based on the volumetric fusion of commodity depth cameras. The system incorporates real-time 3D fusion with color, the ability to distinguish and render dynamic elements of a scene whether human or non-human, a multi-touch driven free 3D viewpoint, and a Spatial Augmented Reality (SAR) light annotation mechanism. We provide a physical overview of the system, including hardware and software configuration, and detail the implementation of each of the key features.	augmented reality;computer hardware;guidance system;multi-touch;prototype;real-time clock;streaming media;workspace	Matt Adcock;Stuart Anderson;Bruce H. Thomas	2013		10.1145/2534329.2534331	computer vision;simulation;visualization;fusion;computer science;multimedia;computer graphics (images)	HCI	-42.64268675045209	-38.129764696008614	73451
2b94ba0b9cb1ac3e6b142f745eaad908ea227eb7	human factors evaluation of a vision-based facial gesture interface		We adapted a vision-based face tracking system for cursor control by head movement. An additional vision-based algorithm allowed the user to enter a click by opening the mouth. The Fitts law information throughput rate of cursor movements was measured to be 2.0 bits/sec with the ISO 9241-9 international standard method for testing input devices. A usability assessment was also conducted and we report and discuss the results. A practical application of this facial gesture interface was studied: text input using the Dasher system, which allows a user to type by moving the cursor. The measured typing speed was 7-12 words/minute, depending on level of user expertise. Performance of the system is compared to a conventional mouse interface.	algorithm;computer mouse;cursor (databases);dasher;facial motion capture;fitts's law;gesture recognition;human factors and ergonomics;ieee 1394;input device;joystick;pointing device;throughput;tracking system;usb;usability;words per minute	Gamhewage Chaminda de Silva;Michael J. Lyons;Shinjiro Kawato;Nobuji Tetsutani	2003	2003 Conference on Computer Vision and Pattern Recognition Workshop	10.1109/CVPRW.2003.10055	facial motion capture;fitts's law;input device;typing;throughput;throughput (business);computer vision;artificial intelligence;usability;computer science;gesture	HCI	-46.93474819113545	-45.42872662048204	73483
ead58b00ac70d46c50d6cccaa98b4a9662964b37	gestyboard 2.0: a gesture-based text entry concept for high performance ten-finger touch-typing and blind typing on touchscreens		This paper presents the second version of the Gestyboard, which is an innovative approach of text entry on multi-touch devices like tabletops or tablets. To overcome the lack of tactile feedback, we use unique gesture-to-key mappings for each finger according to the ten-finger touch-typing method. As a key feature, the Gestyboard only accepts keystrokes when they are performed with the finger corresponding to the ten-finger touch-typing method. This way, missing a keystroke is not possible, and therefore blind typing is naturally supported by the concept. The first version of the Gestyboard was optimized according to the qualitative and quantitative results of our first formal evaluation. This paper presents two new evaluations which give new insights on the comparative performance and conceptual improvements of the Gestyboard. In the second evaluation, our participants reached a speed of 108 cpm (characters per minute [21.6wpm]) and an error rate of 4% which is close to the performance of standard users on classic touchscreen keyboards. The third evaluation additionally revealed that our participants increased their typing speed with the Gestyboard by 44% and decreased their error rate by 48% in just 3 trial sessions. This steep learning curve is mostly due to the familiarity to the QWERTY layout.	event (computing);multi-touch;touch typing;touchscreen;words per minute	Tayfur Coskun;Christian A. Wiesner;Eva Artinger;Amal Benzina;Patrick Maier;Manuel J. Huber;Claudia Grill;Philip Schmitt;Gudrun Klinker	2013		10.1007/978-3-642-39062-3_49	speech recognition;communication	HCI	-47.2123862427394	-45.04238405193796	73608
4eb29d6f7ba2e3fc2176d14507a6657ba7c8682e	the flashing right turn signal with pedestrian indication: a human factors study to assess driver comprehension		Given the increased fatality risk of older pedestrians, and the large and growing older adult population in the United States and around the world, many countermeasures to ensure aging pedestrian safety have been explored (e.g., different types of crosswalk markings). The present study sought to investigate the potential of an experimental countermeasure, the flashing pedestrian indicator (FPI). This signal, intended for right-turning drivers, alternates between a yellow arrow and a pedestrian symbol when a pedestrian calls for a walk phase at a signalized intersection. The purpose of this signal is to cue right-turning drivers to the potential presence of a pedestrian, encourage scanning to the right for crossing pedestrians, and promote driver yielding behaviors. We conducted a study to gauge the comprehension of drivers who were naive to the signal to explore if the FPI’s intended message was understood. Participants were presented with scenarios depicting the FPI and other signal states and were asked the meaning of the observed signal (open-ended and multiple choice questions). Comprehension was tested across a range of age groups: younger (21–35 years), middle-aged (50–64), and older adult (65+) drivers. While in general the signal was understood, some participants were confused regarding the meaning of the FPI in certain situations. Potential positive effects of the FPI need to be weighed against potential confusion before any further recommendations can be made regarding the FPI as a potential countermeasure to assist with pedestrian crashes.	bios;human factors and ergonomics;list comprehension	Nelson A. Roque;Walter R. Boot;Neil Charness;Kimberly Barajas;Jared Dirghalli;Ainsley Mitchum	2016		10.1007/978-3-319-39949-2_40	simulation;transport engineering;communication	HCI	-47.52074384732464	-51.70527975123682	73703
89477503ddaebb341565966f64e635021b5fcf65	evaluating gesture-based augmented reality annotation	spatial referencing;user study;annotations;hololens;augmented reality	Drawing annotations with 3D hand gestures in augmented reality are useful for creating visual and spatial references in the real world, especially when these gestures can be issued from a distance. Different techniques exist for highlighting physical objects with hand-drawn circle and arrow annotations from a distance, assuming an approximate 3D scene model (e.g., as provided by the Microsoft HoloLens). However, little is known about user preference and performance of such methods for annotating real-world 3D environments. In this paper, we compare different annotation methods using the HoloLens augmented reality development platform: Surface-Drawing and Air-Drawing, with either raw but smoothed or interpreted and beautified gesture input. For the Surface-Drawing method, users control a cursor that is projected onto the world model, allowing gesture input to occur directly on the surfaces of real-world objects. For the Air-Drawing method, gesture drawing occurs at the user's fingertip and is projected onto the world model on release. The methods have different characteristics regarding necessitated vergence switches and afforded cursor control. We performed an experiment in which users draw on two different real-world objects at different distances using the different methods. Results indicate that Surface-Drawing is more accurate than Air-Drawing and Beautified annotations are drawn faster than Non-Beautified; participants also preferred Surface-Drawing and Beautified.	approximation algorithm;ar (unix);augmented reality;cursor (databases);ibm notes;microsoft hololens;network switch;smoothing;user interface;vergence	YunSuk Chang;Benjamin Nuernberger;Bo Luan;Tobias Höllerer	2017	2017 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2017.7893337	computer vision;computer science;multimedia;computer graphics (images)	HCI	-44.41481188176405	-40.58352633179681	73855
424c46da5086dc08929a87cbdad1e987645881fb	sdm: selective dynamic manipulation of visualizations	visualizations;moving object;interactive techniques;3d visualization;direct manipulation;contextual information;interactive animation;interaction technique	In this paper we present a new set of interactive techniques for 2D and 3D visualizations. This set of techniques is called SDM (Selective Dynamic Manipulation). Selective, indicating our goal for providing a high degree of user control in selecting an object set, in selecting teractive techniques and the properties they affect, and in the degree to which a user action affects the visualization. Dynamic, indicating that the interactions all occur in real-time and that interactive animation is used to provide better contextual information to users in response to an action or operation. Manipulation, indicating the types of interactions we provide, where users can directly move objects and transform their appearance to perform different tasks. While many other approaches only provide interactive techniques in isolation, SDM supports a suite of techniques which users can combine to solve a wide variety of problems.	interaction;real-time transcription;user interface	Mei C. Chuah;Steven F. Roth;Joe Mattis;John Kolojejchick	1995		10.1145/215585.215654	computer vision;visualization;human–computer interaction;computer science;multimedia;interaction technique;computer graphics (images)	HCI	-44.16745873423634	-46.812172260956025	74029
5aaa4fd891d30e8a33047be5f0040d4565baf9a2	tof tactile stamps: drawing object silhouettes through depth segmentation by blind people		We present a workstation utilizing a simple depth segmentation approach to let visually impaired users transform real-world objects into their two-dimensional silhouette shape. Therefore, we use a Kinect TOF system for detection and a novel two-dimensional dynamic tactile pin-matrix display for presenting the captured result. The calibration, segmentation as well as some experimental findings about the used TOF system are explained. Based on the limitations of the TOF system, the segmentation is limited to objects being at least 29mm high. Furthermore, the segmented shapes are less detailed, ragged and sometimes optical deformed. A brief evaluation with three blind users showed that they would accept little such deformations. However, they would like to use such a system for drawing complex objects or share an object's impression with other users in a very fast and enjoyable way.	kinect;workstation	Jens Bornschein;Lukas Förster;Denise Bornschein	2017		10.1145/3056540.3064946	silhouette;computer science;impression;computer vision;computer graphics (images);workstation;segmentation;artificial intelligence	HCI	-42.368414664405584	-39.63158904244057	74096
66c747e8891d989f019770d37e2f5d2736d01ebd	assessing the readability of stacked graphs		Stacked graphs are a visualization technique popular in casual scenarios for representing multiple time-series. Variations of stacked graphs have been focused on reducing the distortion of individual streams because foundational perceptual studies suggest that variably curved slopes may make it difficult to accurately read and compare values. We contribute to this discussion by formally comparing the relative readability of basic stacked area charts, ThemeRivers, streamgraphs and our own interactive technique for straightening baselines of individual streams in a ThemeRiver. We used both real-world and randomly generated datasets and covered tasks at the elementary, intermediate and overall information levels. Results indicate that the decreased distortion of the newer techniques does appear to improve their readability, with streamgraphs performing best for value comparison tasks. We also found that when a variety of tasks is expected to be performed, using the interactive version of the themeriver leads to more correctness at the cost of being slower for value comparison tasks.	baseline (configuration management);call stack;chart;correctness (computer science);design of experiments;distortion;futures studies;graphical user interface;information theory;interaction;procedural generation;randomness;smart;time series	Alice Thudt;Jagoda Walny;Charles Perin;Fateme Rajabiyazdi;Lindsay MacDonald;Riane Vardeleon;Saul Greenberg;M. Sheelagh T. Carpendale	2016		10.20380/GI2016.21	streams;computer vision;machine learning;readability;computer science;correctness;distortion;interaction technique;multimedia;visualization;artificial intelligence;graph;casual	HCI	-41.978279377128636	-49.18395459901224	74209
675eb53a4793a53fb8e67e522a9d7cbc1af337b6	violin timbre navigator: real-time visual feedback of violin bowing based on audio analysis and machine learning		Bowing is the main control mechanism in sound production during a violin performance. The balance among bowing parameters such as acceleration, force, velocity or bow-bridge distance are continuously determining the characteristics of the sound. However, in traditional music pedagogy, approaches to teaching the mechanics of bowing are based on subjective and vague perception, rather than on accurate understanding of the principles of movement bowing. In the last years, advances in technology has allowed to measure bowing parameters in violin performances. However, sensing systems are generally very expensive, intrusive and require for very complex and time consuming setups, which makes it impossible to bring them into a classroom environment. Here, we propose an algorithm that is able to estimate bowing parameters from audio analysis in real-time, requiring just a microphone and a simple calibration process. Additionally, we present the Violin Palette, a prototype that uses the reported algorithm and presents bowing information in an intuitive way.		Alfonso Pérez	2019		10.1007/978-3-030-05716-9_15	artificial intelligence;acceleration;computer vision;computer science;audio analyzer;bowing;microphone;violin;perception;timbre	AI	-47.925659830182795	-50.45233079178203	74413
ec6ce9100465c5378fbf7d53b31afed2f644a49a	pitchmap: a mobile interaction prototype for exploring combinations of maps and images	maps and images;virtual exploration;mobile interaction	While maps and images complement each other when combined in a 3d environment for virtual exploration, mobile interaction concepts for navigation in 3d space are challenging. Due to the lack of input devices, most of the interaction has to be realised on small sized touch screens. We present a prototype combining well-known interaction techniques using a discrete and a continuous pitch gesture.		Dirk Wenig;Rainer Malaka	2011		10.1007/978-3-642-22571-0_23	computer vision;simulation;mobile interaction;human–computer interaction;computer science	HCI	-45.22166759245822	-40.919740914356034	74456
422e79403b4b81237aef4dd5b5c11009ba0d3684	music to motion: using music information to create expressive robot motion	robot theater;humanoid robot;robot animation;artistic robot;motion generation	In a robot theater, developing high-quality motions for a humanoid robot requires significant time and effort. People with artistic and animation skills are required to create expressive gestures and movements on the robots. We observe that dancers can translate music into motions that are perceived as expressive by the audience. In this study, we developed a program to exploit melodic and dynamics information from music to create expressive robot motion. The program was used on two applications: producing motions strictly from the music information and controlling the execution a pre-programmed sequence of actions. Both applications are intended for a robot theater. The former was applied on an arm robot, while the latter on a humanoid KHR-1 robot. Two surveys were done to analyze the impact of our method. The results suggest that the robot motions produced by our program can be perceived as being more expressive and more dynamic than the motions created by a person or without music information.		Mathias Sunardi;Marek A. Perkowski	2018	I. J. Social Robotics	10.1007/s12369-017-0432-9	humanoid robot;simulation;social robot;robot learning;mobile robot;robot control;psychology;animation;personal robot;gesture	Robotics	-38.997066163958245	-38.28997396588356	74682
fb2a796484ca83925313c0ece469774cb3e49fa1	spontaneous interaction on context-aware public display: an nfc and infrared sensor approach	field communication;automatic delivery;mobile keyboard;cell phone;intelligent environment;minimum interactive effort;context-aware public display;public display;excellent conduit;infrared sensor;infrared sensor approach;spontaneous interaction;nfc;interaction	Public Display offers an excellent conduit for automatic delivery of information in an intelligent environment that is tailored to the user. We have detected some situations in which some kind of interaction with public displays is needed. The objective should require minimum interactive effort on the part of the user. In this work we present our experience when we tested two forms of interacting with public displays: Infrared sensors, to detect a sweep of the hand over the sensor; and a Near Field Communication (NFC) enabled cell phone, which used a mobile keyboard to carry out the interaction.		Gabriel Chavira;Salvador W. Nava;Ramón Hervás;José Bravo;Carlos Sánchez	2007			optoelectronics;internet privacy;computer security	HCI	-47.89462443659909	-41.39383082604322	74732
4a844eefffa52041e4647d94be2cb8fd5e9f305a	visual search for triangles in wine labels	wine label;visual search;set size effect	Visual search for a downward-pointing triangle among upward-pointing triangles is faster than vice versa, a phenomenon referred to as the downward-pointing triangle superiority (DPTS) effect. Here, we report two new experiments designed to investigate whether this phenomenon also emerges when a triangle appears as a local feature within a wine label. The experimental task was to identify whether all of the wine bottles in a store display were the same or not, while each wine bottle had either a downward- or upward-pointing triangle displayed on its label. The results of Experiment 1 revealed that the participants responded more rapidly when searching for a wine bottle with a downward-pointing triangle on its label than when the target had a triangle pointing upward, indicating the presence of a DPTS effect. In Experiment 2, the DPTS effect was replicated while varying the set size. The magnitude of the DPTS effect increased with increasing set size. Taken together, these results revealed similar visual search results for pictorial stimuli with triangles as local features as for geometric triangular shapes. The implications of these findings for the design of product labels are discussed.	doppler effect;emergence;experiment;image	Hui Zhao;Charles Spence;Xiaoang Wan	2016		10.1145/3007577.3007582	mathematics;advertising;communication;engineering drawing	HCI	-43.68306728313407	-51.4491270550174	74823
00e995350156f375d13d2adda637d40e98f9a767	unsupervised work knowledge mining through mobility and physical activity sensing	behavioural sciences computing;data mining;mobile computing;personnel;unsupervised learning;automated knowledge mining;firefighters;housekeepers;janitor;knowledge extraction;mobility activity sensing;nurses;physical activity sensing;physically active professions;unsupervised work knowledge mining;waiters	Knowledge of working professionals gained through years of experience is invaluable for any organization. Extracting this knowledge allows an organization to optimize internal processes and facilitate training of new hires. Therefore, there has been a significant research effort in developing techniques for automated knowledge mining at workplaces. However, research in the past have been focused mainly on extracting knowledge of stationary professions such as office workers, who perform most of their day-to-day tasks at their desk. In this work, we propose an approach for mining work knowledge of physically active professions such as nurses, firefighters, waiters, housekeepers or janitors. We leverage the advances of mobile sensing to extract knowledge from workers with high level of mobility and physical activity patterns. We demonstrate the feasibility of the proposed approach on a real-world scenario of a janitor as a study subject. We show that using data collected from mobile devices carried by a janitor throughout their work, we are able to extract knowledge rules that describe generalized patterns of janitor's behavior. We expect the proposed method to be applied to other fields to mine knowledge from workplaces.	data mining;experiment;framing (world wide web);high-level programming language;mobile device;smartphone;smartwatch;stationary process;unsupervised learning;vii	Le T. Nguyen;Joy Zhang	2014	6th International Conference on Mobile Computing, Applications and Services		engineering;knowledge management;data science;data mining	HCI	-36.30457903895256	-46.77417862540436	75101
24d906e5815e76af466b15513e4a4baa26341444	a comparison of techniques for in-place toolbars	in place toolbars;targeting time;direct input	Selections are often carried out using toolbars that are located far away from the location of the cursor. To reduce the time to make these selections, researchers have proposed in-place toolbars such as Toolglasses or popup palettes. Even though in-place toolbars have been known for a long time, there are factors influencing their performance that have not been investigated. To explore the subtleties of different designs for in-place toolbars, we implemented and compared three approaches: warping the cursor to the toolbar, having the toolbar pop up over the cursor, and showing the toolbar on the trackpad itself to allow direct touch. Our study showed that all three new techniques were faster than traditional static toolbars, but also uncovered important differences between the three in-place versions. Participants spent significantly less time in the direct-touch trackpad, and warping the cursor’s location caused a time-consuming attentional shift. These results provide a better understanding of how small changes to in-place toolbar techniques can affect performance.	cursor (databases);fastest;image warping;in-place algorithm;touchpad	Andre Doucette;Carl Gutwin;Regan Lee Mandryk	2010			real-time computing;simulation;computer hardware	HCI	-45.925741804040655	-46.52253587638013	75107
2ca458993e3299bcbaf1ab4331eb5db0fb506803	mental visual indexing: towards fast video browsing	mental search;rnn;query intent;video browsing;temporal model	Video browsing describes an interactive process where users want to find a target shot in a long video. Therefore, it is crucial for a video browsing system to be fast and accurate with minimum user effort. In sharp contrast to traditional Relevance Feedback (RF), we propose a novel paradigm for fast video browsing dubbed Mental Visual Indexing (MVI). At each interactive round, the user only needs to select one of the displayed shots that is most visually similar to her mental target and then the user's choice will further tailor the search to the target. The search model update given a user feedback only requires vector inner products, which makes MVI highly responsive. MVI is underpinned by a sequence model in terms of Recurrent Neural Network (RNN), which is trained by automatically generated shot sequences from a rigorous Bayesian framework, which simulates user feedback process. Experimental results on three 3-hour movies conducted by real users demonstrate the effectiveness of the proposed approach.	browsing;programming paradigm;radio frequency;random neural network;recurrent neural network;relevance feedback	Richang Hong;Jun He;Hanwang Zhang;Tat-Seng Chua	2016		10.1145/2964284.2967296	computer vision;computer science;machine learning;multimedia;world wide web	Vision	-33.763748267520285	-49.79582884727584	75146
38b9df04b81a856c82db26d34d97f7a5ea8f0ed7	emotional and behavioral responses to haptic stimulation	forward backward;empirical method;quantitative;reaction time;empirical methods	A prototype of friction-based horizontally rotating fingertip stimulator was used to investigate emotional experiences and behavioral responses to haptic stimulation. The rotation style of 12 different stimuli was varied by burst length (i.e., 20, 50, 100 ms), continuity (i.e., continuous and discontinuous), and direction (e.g., forward and backward). Using these stimuli 528 stimulus pairs were presented to 12 subjects who were to distinguish if stimuli in each pair were the same or different. Then they rated the stimuli using four scales measuring the pleasantness, arousal, approachability, and dominance qualities of the 12 stimuli. The results showed that continuous forward-backward rotating stimuli were rated as significantly more unpleasant, arousing, avoidable, and dominating than other types of stimulations (e.g., discontinuous forward rotation). The reaction times to these stimuli were significantly faster than reaction times to discontinuous forward and backward rotating stimuli. The results clearly suggest that even simple haptic stimulation can carry emotional information. The results can be utilized when making use of haptics in human-technology interaction.	haptic technology;prototype;scott continuity	Katri Salminen;Veikko Surakka;Jani Lylykangas;Jukka Raisamo;Rami Saarinen;Roope Raisamo;Jussi Rantala;Grigori E. Evreinov	2008		10.1145/1357054.1357298	simulation;subliminal stimuli;empirical research	HCI	-45.77361413932288	-50.87744378491796	75289
99c6a98e6db7a36d75a3ecca74a09aff360b7f60	cooperative embodied communication emerged by interactive humanoid robots	robot humanoide;humanoid robot;detection basee environnement;interfase usuario;user interface;relacion hombre maquina;man machine relation;environment based sensing;human robot interaction;three dimensional;motion capture;cooperative behavior;evaluation subjective;subjective experiments;entrainment;interface utilisateur;relation homme machine;subjective evaluation;face to face;evaluacion subjetiva	Research on humanoid robots has produced various uses for their body properties in communication. In particular, mutual relationships of body movements between a robot and a human are considered to be important for smooth and natural communication, as they are in human-human communication. We have developed a semi-autonomous humanoid robot system that is capable of cooperative body movements with humans using environment-based sensors and switching communicative units. And we conducted an experiment using this robot system and verified the importance of cooperative behaviors in a route-guidance situation where a human gives directions to the robot. This result indicates that the cooperative body movements greatly enhance the emotional impressions of human in a route-guidance situation. We believe these results allow us to develop interactive humanoid robots that sociably communicate with humans.	autonomous robot;humanoid robot;humans;semiconductor industry;sensor;smoothing	Daisuke Sakamoto;Takayuki Kanda;Tetsuo Ono;Masayuki Kamashima;Michita Imai;Hiroshi Ishiguro	2004	RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication (IEEE Catalog No.04TH8759)	10.1016/j.ijhcs.2004.11.001	human–robot interaction;three-dimensional space;entrainment;computer vision;motion capture;simulation;computer science;humanoid robot;artificial intelligence;social robot;user interface	Robotics	-39.96979219956213	-48.66622700466928	75387
88686d6c026cbfbeb204c16cd8f8dafe941f6790	pick me!: getting noticed on google play	visual saliency;computational methods;app icon quality;consumer choice;mobile apps;visual complexity	Almost any search on Google Play returns numerous app suggestions. The user quickly skims through the list and picks a few apps for a closer look. The vast majority of the apps regardless of how well-made they are go unnoticed. App icons uniquely represent each app in Google Play and help apps to get noticed, as we demonstrate in the paper. We reviewed the visual qualities of icons that could make them noticeable and likable. We then computationally measured two of the qualities visual saliency and complexity for 930 icons and linked the computed scores to app popularity (the number of app ratings and installs). The measures explained 38% of variance in the number of ratings, if app genre was accounted for. Not only does such result assert the link between icon properties and app popularity, it also highlights the automatic prediction of app popularity as a promising research direction. HCI researchers, app creators and Google Play (or another mobile marketplace) will benefit from the paper insights on what antecedes app success and how to measure the antecedents.	complexity;computation;graphical user interface;human–computer interaction;image;mobile app;pixel;play store;quadtree	Aliaksei Miniukovich;Antonella De Angeli	2016		10.1145/2858036.2858552	computer science;multimedia;internet privacy;world wide web	Web+IR	-34.427490738757726	-48.7694099953653	75454
80652cff0b7091770376b09de2216ecef4dfa5e4	a mobile passive augmented reality device - mpard	usability testing;videoconference;application software;industrial applications mobile passive augmented reality device mpard rf based approach computational power graphics power wearable computers hand held display external mobile camera head mounted display usability tests;computer graphics;augmented reality wearable computers computer vision virtual reality cameras application software mobile computing computer graphics computer displays videoconference;wearable computers;computer graphic equipment;virtual reality;computational power;computer displays mobile computing portable computers augmented reality computer graphic equipment cameras helmet mounted displays human factors;computer vision;mpard;human factors;graphics power;usability tests;mobile passive augmented reality device;portable computers;hand held display;rf based approach;computer displays;industrial application;wearable computer;industrial applications;augmented reality;mobile computing;external mobile camera;cameras;mobile augmented reality;helmet mounted displays;head mounted display	Describes an RF-based approach to mobile augmented reality (AR), solving the problem of providing sufficient computational and graphics power on conventional wearable computers. Two devices are presented which work together and can be used either in a hand-held AR display version or as a wearable version using an external mobile camera and a head-mounted display. The process of evaluation and improvement is ongoing, but the results so far are presented, including usability tests in industrial applications.	augmented reality	Holger Regenbrecht;Ralf Specht	2000		10.1109/ISAR.2000.880926	augmented reality;computer-mediated reality;simulation;wearable computer;human–computer interaction;computer science;operating system;virtual reality;mobile computing;computer graphics (images)	HCI	-43.00739165715267	-39.76050276376515	75497
a32e246e8ac36b5e3447cd9bf0af44b98be33000	tactifloor: design and evaluation of vibration signals for doorway reminder systems		The forgetting of objects is an everyday problem. Current research projects develop doorway reminder systems which are supposed to remind the user of certain objects or events before leaving home. At present the focus of research is mainly on the technical design of the recognition of persons and objects. For this reason we are concentrating on the design and evaluation of possible system feedback. In particular, the target group of the digital natives is familiar with the concept of tactile feedback for reminder functions. Therefore known vibration patterns were used, which are to be transferred to the user via the floor. This solution was implemented in a first prototype. A user study with 19 volunteers showed a general acceptance of tactile feedback via floor panel and the cognitive link of the vibration patterns with reminder notifications.	digital native;floor and ceiling functions;prototype;usability testing	Daniela Reschke;Matthias Böhmer;Manuel Sorg	2017		10.1145/3152832.3156623	human–computer interaction;computer science;cognition;forgetting;vibration;digital native	HCI	-46.97241044166743	-42.7609545006917	75621
df19e217899b6bc8f50a745d5e575e65bb8203ce	situation-based indoor wayfinding system for the visually impaired	vocabulary tree;auditory interface;real time;field test;speeded up robust features;visually impaired people;optimal path;wayfinding system;situation awareness;visual impairment;2d color code;speeded up robust feature surf	This paper presents an indoor wayfinding system to help the visually impaired finding their way to a given destination in an unfamiliar environment. The main novelty is the use of the user's situation as the basis for designing color codes to explain the environmental information and for developing the wayfinding system to detect and recognize such color codes. Actually, people would require different information according to their situations. Therefore, situation-based color codes are designed, including location-specific codes and guide codes. These color codes are affixed in certain locations to provide information to the visually impaired, and their location and meaning are then recognized using the proposed wayfinding system. Consisting of three steps, the proposed wayfinding system first recognizes the current situation using a vocabulary tree that is built on the shape properties of images taken of various situations. Next, it detects and recognizes the necessary codes according to the current situation, based on color and edge information. Finally, it provides the user with environmental information and their path through an auditory interface. To assess the validity of the proposed wayfinding system, we have conducted field test with four visually impaired, then the results showed that they can find the optimal path in real-time with an accuracy of 95%.	color;qr code;real-time clock;vocabulary	Eunjeong Ko;Jinsun Ju;Eun Yi Kim	2011		10.1145/2049536.2049545	situation awareness;computer vision;simulation;computer science	HCI	-44.10399877812398	-45.52824855996242	75626
6c213744b0656e57658013f6fe408e1017e5f4e3	wireless displays in educational augmented reality applications		Augmented Reality (AR) as defined by Azuma [1] does not pose restrictions on output devices to be used for AR. Starting with light-weight notebooks and ultra mobile PCs, recently smartphones became favorite AR output devices. They represent a class of self contained computing units, providing (usually limited) computing power as well as input and output peripherals – all in one device.	ar (unix);augmented reality;compaq lte;data rate units;emergence;fifth generation computer;handheld game console;image resolution;mobile device;mobile phone;multi-user;output device;prototype;rollable display;stereoscopy;uncompressed video	Hannes Kaufmann;Mathis Csisinko	2011		10.1007/978-1-4614-0064-6_6	embedded system;computer-mediated reality;multimedia;computer graphics (images)	HCI	-43.48908916977916	-39.63086388426555	75752
fcab574038a64ddc4942634727c7813ddd9495d7	a user study with guis tailored for smartphones and tablet pcs	user study usability device tailored gui small touchscreen;user study;smart phones;layout correlation graphical user interfaces smart phones error analysis usability performance evaluation;graphical user interfaces;internet;small touchscreen;notebook computers;qualitative data web based graphical user interfaces gui smartphones tablet pc screen size navigation task completion time error rates;smart phones graphical user interfaces internet notebook computers;usability;device tailored gui	Usually, Web-based graphical user interfaces (GUIs) are not specifically tailored for different devices with touch-screens, such as smartphones and tablet PCs, where interaction is affected mainly by screen size. There is little scientific evidence on the conditions under which additional taps for navigation are better than scrolling or vice versa. Therefore, we conducted a user study in which we experimentally evaluated GUIs tailored for a smartphone and a tablet PC, respectively. Each participant performed the same task with two different layouts of the same GUI, either on a given smartphone or tablet PC. We collected quantitative data through measuring task completion time and error rates, as well as qualitative data through subjective questionnaires. The main result is that tailoring a GUI specifically for a smartphone or tablet PC, respectively, is important, since screen size matters. Users performed significantly better when they could use the tailored version on the given device. This preference was also reflected in their subjective opinions.	display size;experiment;graphical user interface;scrolling;smartphone;tablet computer;usability testing	David Raneburger;Roman Popp;David Alonso-Ríos;Hermann Kaindl;Jürgen Falb	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.635	the internet;usability;human–computer interaction;computer science;graphical user interface;multimedia;world wide web	HCI	-47.62623142672634	-45.56707015798617	75962
febd46503711304e0d378b4414cb057c0edf9ce3	remote collaboration with augmented and virtual reality interfaces	virtual environment;virtual reality;mixed reality;spatial relationships	Some applications require a person in a remote location (the trainer), to instruct another person (the trainee) on how to put together a number of objects according to a precise set of spatial relationships. In this paper we report the results of using a collaborative Mixed Reality application that allows the trainer to manipulate the virtual objects in a virtual environment. At the same time, the trainee observes the virtual object manipulation as means to move the real objects accordingly. The trainer uses a Virtual Reality interface and the trainee uses an Augmented Reality one. The results show that, even though the time to complete the task using the application is longer, users perceive that collaboration using a similar application in the real world could make collaboration easier.	ar (unix);augmented reality;computer vision;experiment;head-mounted display;mixed reality;reality lab;software house;virtual reality	Luis Montoya;Elizabeth Restrepo;Daniels Sjogreen;Helmuth Gomez	2007			immersion (virtual reality);computer-mediated reality;human–computer interaction;mixed reality;multimedia;instructional simulation;virtual reality;augmented reality;computer science;cave automatic virtual environment;trainer	HCI	-43.20969758237676	-38.63108045565753	76132
0a437ed486c114885ac822c56ef0f27c474eae8e	wearable laser pointer versus head-mounted display for tele-guidance applications?	wearable computers;tele pointing;tele presence;laser pointer;remote collaboration;head mounted display	Wearable camera and displechnology allow remote collaborators to guide activities performed by human agents located elsewhere. This kind of technology augments the range of human perception and actuation. In this paper we quantitatively determine if wearable laser pointers are viable alternatives to Head-Mounted Displays for indicating where in the physical environment the local agent should direct her/his attention. The potential benefit of the laser pointer would be reduced eye fatigue, due to the fact that the documented refocusing challenges associated with HMD use would be completely eliminated. 10 participants where asked to perform a short tele-guided pick-and drop task using both approaches. The quantitative analysis indicates that user performance in the laser pointer condition is higher than the HMD approach (P = .064, α = 0.1). While all 10 participants found the task easy in both conditions, 8 of 10 participants found the laser pointer system more convenient.	head-mounted display;pointer (computer programming);television;wearable computer	Shahram Jalaliniya;Thomas Pederson;Steven Houben	2014		10.1145/2641248.2641354	simulation;computer hardware;engineering;computer graphics (images)	HCI	-44.65872888209396	-47.574449596540035	76248
5d2697886ffe3f671d7773c99c760229a2c11347	where does the mouse go? an investigation into the placement of a body-attached touchpad mouse for wearable computers	input device;error rate;wearable computer;head mounted display	We investigated the effects of placement of a TouchPad input device on a user’s body for the control of a wearable computer. This study involved 25 subjects performing selection tasks with a TouchPad mouse while wearing a wearable computer on their back and using a head-mounted display. Each subject performed the tasks in 27 different combinations of four postures (sitting, kneeling, standing and prone) and seven different placements of the TouchPad mouse on the subject’s body (forearm, thigh by 2, torso by 2, and upper arm by 2). We measured the time and error rate to complete the selection of a circular target. The results for the effects due to posture showed that there were similar time effects for sitting, standing and kneeling. When examining the effects resulting from mouse position, the front of the thigh was shown to be the best position of the mouse. When the posturing and mouse position conditions were combined, the results indicated that the thigh front mouse position would be most appropriate for sitting, kneeling and standing postures, and the forearm mouse position would be best for the prone position.	attachments;bit error rate;cpu cache;comment (computer programming);cursor (databases);desktop computer;head-mounted display;input device;interaction;poor posture;touchpad;wearable computer	Bruce H. Thomas;Karen Grimmer-Somers;Joanne Zucco;Steve Milanese	2002	Personal and Ubiquitous Computing	10.1007/s007790200009	simulation;wearable computer;word error rate;computer science;optical head-mounted display;operating system;input device	HCI	-46.10986268731067	-45.421642890504465	76269
6672cc576f3b8b4ce6efd8e97ba614f315e92644	twist it, touch it, push it, swipe it: evaluating secondary input devices for use with an automotive touchscreen hmi	workload;steering wheel controls;touchpad;visual demand;touchscreen;rotary controller;driving performance;character recognition;preferences	Touchscreen Human-Machine Interfaces (HMIs) inherently demand some visual attention. By employing a secondary device, to work in unison with a touchscreen, some of this demand may be alleviated. In a medium-fidelity driving simulator, twenty-four drivers completed four typical in-vehicle tasks, utilising each of four devices -- touchscreen, rotary controller, steering wheel controls and touchpad (counterbalanced). Participants were then able to combine devices during a final 'free-choice' drive. Visual behaviour, driving/task performance and subjective ratings (workload, emotional response, preferences), indicated that in isolation the touchscreen was the most preferred/least demanding to use. In contrast, the touchpad was least preferred/most demanding, whereas the rotary controller and steering wheel controls were largely comparable across most measures. When provided with 'free-choice', the rotary controller and steering wheel controls presented as the most popular candidates, although this was task-dependent. Further work is required to explore these devices in greater depth and during extended periods of testing.	driving simulator;human–computer interaction;rotary system;simulation;steering wheel;touchpad;touchscreen;unison	David R. Large;Gary E. Burnett;Elizabeth Crundall;Glyn Lawson;Lee Skrypchuk	2016		10.1145/3003715.3005459	embedded system;simulation;computer hardware;engineering	HCI	-47.15413905715022	-46.817168192552856	76513
c5625313a941ec81cec9c79172f4f2e9becd6194	sensing keyboard input for computer activity recognition with a smartphone		Computer activities such as writing documents and playing games are becoming more and more popular in our daily life. These activities (especially if identified in a non-intrusive manner) can be used to facilitate context-aware services. In this paper, we propose to recognize computer activities through keyboard input sensing with a smart-phone. Specifically, we first utilize the microphone embedded in a smartphone to sense the acoustic signal of keystrokes on a computer keyboard. We then identify keystrokes using fingerprint identification techniques. The determined keystrokes are then corrected by using the proposed adjacent similarity matrix algorithm. Finally, by fusing both semantic and acoustic features, a classification model is constructed to recognize four typical computer activities: chatting, coding, writing documents, and playing games. We evaluated the proposed approach from multiple aspects in realistic environments. Experimental results validated the effectiveness of our approach.	acoustic cryptanalysis;activity recognition;algorithm;computer keyboard;context-aware network;embedded system;event (computing);fingerprint;microphone;plover;sensor;similarity measure;smartphone	He Du;Zhiwen Yu;Dong Xiao;Zhu Wang;Qi Han;Bin Guo	2017		10.1145/3123024.3123150	embedded system;computer vision;coding (social sciences);activity recognition;artificial intelligence;computer science;microphone;similarity matrix	HCI	-35.66551446221573	-45.26230114527374	76521
bdb986e489564cdb31a2eb3c121a31009c168220	multimodal interactive continuous scoring of subjective 3d video quality of experience	video communication protocols quality of experience three dimensional displays three dimensional television;protocols;empirical 3d distortion multimodal interactive continuous scoring of quality micsq 3d quality of experience qoe subjective assessment visual comfort evaluation interactive continuous subjective quality assessment;three dimensional television;stereo camera 3d video quality of experience 3d visual programs 3d cinema 3d tv 3d games multimodal interactive continuous scoring of quality 3d display conventional single stimulus continuous quality evaluation protocol wireless device interaction process depth of field;three dimensional displays visualization stereo image processing quality assessment reliability video recording protocols;quality of experience;three dimensional displays;video communication	People experience a variety of 3D visual programs, such as 3D cinema, 3D TV and 3D games, making it necessary to deploy reliable methodologies for predicting each viewer's subjective experience. We propose a new methodology that we call multimodal interactive continuous scoring of quality (MICSQ). MICSQ is composed of a device interaction process between the 3D display and a separate device (PC, tablet, etc.) used as an assessment tool, and a human interaction process between the subject(s) and the separate device. The scoring process is multimodal, using aural and tactile cues to help engage and focus the subject(s) on their tasks by enhancing neuroplasticity. Recorded human responses to 3D visualizations obtained via MICSQ correlate highly with measurements of spatial and temporal activity in the 3D video content. We have also found that 3D quality of experience (QoE) assessment results obtained using MICSQ are more reliable over a wide dynamic range of content than obtained by the conventional single stimulus continuous quality evaluation (SSCQE) protocol. Moreover, the wireless device interaction process makes it possible for multiple subjects to assess 3D QoE simultaneously in a large space such as a movie theater, at different viewing angles and distances. We conducted a series of interesting 3D experiments showing the accuracy and versatility of the new system, while yielding new findings on visual comfort in terms of disparity, motion and an interesting relation between the naturalness and depth of field (DOF) of a stereo camera.	3d television;binocular disparity;cinema 4d;control table;digital video;display size;dynamic range;experiment;flicker (screen);multi-user;multimodal interaction;rejection sampling;signal processing;stereo camera;stereo display;stereoscopic acuity;stereoscopy;tablet computer;tree accumulation;video projector;visualization (graphics)	Taewan Kim;Jiwoo Kang;Alan C. Bovik	2014	IEEE Transactions on Multimedia	10.1109/TMM.2013.2292592	subjective video quality;communications protocol;computer vision;simulation;computer science;video quality;multimedia;computer network	HCI	-42.53302170194399	-47.01147163312894	76540
3f7bf700147066abd98a3798f7dd1c5ddef80a3d	the whole world under your feet: field trial of embodied browsing of geotagged content	mobile;location based service;real time;field trial;mobile phone;geotag;mixed reality;embodied interaction	Location-based services are increasingly popular, and the Earth has become covered with geotagged data. To assess a novel approach to access this information, we conducted a field trial of a mobile mixed reality application called MAA, which operates on a mobile phone. MAA displays a view through the Earth and geospatial content in the direction to which the user is pointing the device. In this paper, we report the results of the two-week long field trial of MAA. We found that the embodied usage of MAA is experienced as engaging and surprising, but may also be cumbersome in some usage situations. Virtual viewing of locations around the planet was considered pleasant. MAA was often shown to friends, and was used for watching visual materials and searching for information about cities. MAA was found to be a promising platform for many kinds of location-based content, especially for real-time events and local information.	geotagging;location-based service;mixed reality;mobile phone;real-time locating system	Erika Reponen;Tiina Koponen;Jaakko Keränen;Kaisa Väänänen	2012		10.1145/2148131.2148192	simulation;geography;multimedia;advertising	HCI	-47.8637726205987	-40.380242352002426	76663
57229cfe025c79eda76f3ded2d3ab4ccc3b0c897	assisting the visually impaired using depth inference on mobile devices via stereo matching	smart phone visually impaired assistance depth inference mobile devices stereo matching depth to sound mapping real time performance inference quality touch interface;mobile devices visually impaired assistance stereo matching sonification;sonification;image matching;smart phones;handicapped aids;stereo matching;stereo image processing;stereo image processing handicapped aids haptic interfaces image matching real time systems smart phones;educational institutions abstracts indexes image resolution;visually impaired assistance;haptic interfaces;mobile devices;real time systems	A novel system for assisting the visually impaired, built upon the platform of a mobile device, is proposed. The system uses depth inference and a depth-to-sound mapping to inform the user of the surroundings via sound. Stereo matching is proposed as the appropriate tool for depth inference, and stereo matching approaches are assessed for their potential for real-time performance on a mobile device and their inference quality. A novel depth-to-sound mapping is proposed that can be adjusted by the user through the touch interface of the mobile device. This mapping allows the user to define the field of view and the location of inference. A smart phone “app” and demo will be produced. The system demonstrates the viability and great benefit of depth inference on mobile devices for assisting the visually impaired.	computer stereo vision;mobile device;real-time locating system;smartphone;touch user interface	Benjamin Chidester;Minh N. Do	2013	2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2013.6618381	computer vision;simulation;sonification;computer science;mobile device;multimedia	Mobile	-44.17020876494851	-42.35946922423873	76680
272a7020607b869a9a527418c88b6857ddfdcf45	a multi-player arcade video game platform with a wireless tangible user interface	arcade video game;wireless sensor networks computer games user interfaces;sensors;user interface;off the shelf wireless sensor nodes multiplayer arcade video game platform wireless tangible user interface motion tracking platform acceleration sensors ultrasonic signals radio frequency signals game players;tangible user interface;wireless sensor node;video game;acceleration;motion tracking;game development;wireless communication;ultrasonic signals;distance measurement;acceleration sensors;motion tracking arcade video game user interface;radio frequency;radio frequency signals;off the shelf wireless sensor nodes;games;games user interfaces wireless sensor networks acceleration radio transmitters receivers radio frequency rf signals physics computing prototypes;natural user interface;multiplayer arcade video game platform;wireless tangible user interface;game players;computer games;motion tracking platform;off the shelf;user interfaces;wireless sensor networks	A recent game development trend is natural user interfaces. This paper presents a motion-tracking platform, which provides natural interfaces for multi-player arcade video games. The platform is composed of acceleration sensors and transmitters/receivers of radio frequency and ultrasonic signals. The game players operate physical devices in the real world, and the positions and orientations of the devices are computed and represented in the virtual devices of the game world. The prototype proves that various off-the-shelf wireless sensor nodes can be integrated, resulting in a new type of natural interface for arcade video games.	arcade game;natural user interface;prototype;radio frequency;sensor;tangible user interface;transmitter;video game development	Doo Seop Eom;Taeyoung Kim;HyunHo Jee;Hyoil Lee;JungHyun Han	2008	IEEE Transactions on Consumer Electronics	10.1109/TCE.2008.4711240	video game graphics;embedded system;simulation;telecommunications;computer science;engineering;operating system;multimedia;user interface	Mobile	-42.43139864423063	-42.509407094792984	76729
7435bc3dc8ac087dee4f7008d2c006106ea54fea	the effect of multimodal feedback on perceived exertion on a vr exercise setting		This paper seeks to determine if multimodal feedback, from auditory and haptic stimuli, can affect a user’s perceived exertion in a virtual reality setting. A simple virtual environment was created in the style of a desert to minimize the amount of visual distractions; a head mounted display was used to display the environment. Users would in the real world drive a Combi Bike and the velocity generated was translated to a vehicle in the virtual environment, moving it forward on a predetermined path. Each user traversed a total of eight hills, two in each of the four conditions. The perceived exertion was measured several times during each condition using the Borg Scale. The results show that there is no significant difference between the four conditions, which had different combinations of auditory and haptic feedback.	multimodal interaction	Jon Ram Bruun-Pedersen;Morten G. Andersen;Mathias M. Clemmensen;Mads K. Didriksen;Emil J. Wittendorff;Stefania Serafin	2018		10.1007/978-3-319-91584-5_2	simulation;haptic technology;virtual reality;exertion;virtual machine;computer science	HCI	-45.736952015232646	-48.68502597123768	76956
2332883726691e0b42f4e265236ff38bdd1f2a1e	cristal: a collaborative home media and device controller based on a multi-touch display	remote control;user study;consumer electronics;remote controller;multi touch;collaborative interface;user interaction	While most homes are inherently social places, existing devices designed to control consumer electronics typically only support single user interaction. Further, as the number of consumer electronics in modern homes increases, people are often forced to switch between many controllers to interact with these devices. To simplify interaction with these devices and to enable more collaborative forms of device control, we propose an integrated remote control system, called CRISTAL (Control of Remotely Interfaced Systems using Touch-based Actions in Living spaces). CRISTAL enables people to control a wide variety of digital devices from a centralized, interactive tabletop system that provides an intuitive, gesture-based interface that enables multiple users to control home media devices through a virtually augmented video image of the surrounding environment. A preliminary user study of the CRISTAL system is presented, along with a discussion of future research directions.	augmented reality;centralized computing;control system;gesture recognition;iteration;multi-touch;multi-user;remote control;situated;usability testing	Thomas Seifried;Michael J Haller;Stacey D. Scott;Florian Perteneder;Christian Rendl;Daisuke Sakamoto;Masahiko Inami	2009		10.1145/1731903.1731911	simulation;human–computer interaction;engineering;multimedia	HCI	-45.75661213736711	-39.242000195844675	77039
19751ffa9996e0a35fca4ef9d62debcded82dc95	focused and casual interactions: allowing users to vary their level of engagement	sensing;peripheral interaction;interaction techniques;deliberate interaction;casual interaction;foreground background	We describe the focused-casual continuum, a framework for describing interaction techniques according to the degree to which they allow users to adapt how much attention and effort they choose to invest in an interaction conditioned on their current situation. Casual interactions are particularly appropriate in scenarios where full engagement with devices is frowned upon socially, is unsafe, physically challenging or too mentally taxing. Novel sensing approaches which go beyond direct touch enable wider use of casual interactions, which will often be 'around device' interactions. We consider the degree to which previous commercial products and research prototypes can be considered as fitting the focused-casual framework, and describe the properties using control theoretic concepts. In an experimental study we observe that users naturally apply more precise and more highly engaged interaction techniques when faced with a more challenging task and use more relaxed gestures in easier tasks.	apache continuum;experiment;interaction technique;theory	Henning Pohl;Roderick Murray-Smith	2013		10.1145/2470654.2481307	simulation;human–computer interaction;foreground-background	HCI	-47.631619281283065	-49.03426999038888	77299
276e936c1b87cea2d992cf1b096ea9896fb8d903	explorer based on brain computer interface	mouse controllers computers brain computer interfaces;hybrid brain computer interfaces bci speller bci mouse motor imagery hybrid bci system;mice computers brain computer interfaces electroencephalography graphical user interfaces feature extraction training;explorer brain computer interface bci bci mouse bci speller	In recent years, various applications which apply the hybrid brain computer interfaces (BCIs) have been studied. In this paper, we present a hybrid BCI system to operate the explorer with P300 and motor imagery, which is mainly composed of a BCI mouse, a BCI speller and an explorer. Through this system, the user can access to his computer and manipulate (open, close, copy, paste, delete) files such as documents, pictures, music, movies and so on. The system has been tested with 5 subjects, and the experimental results show that the explorer can be successfully operated according to subjects' intention with only a small number of mistakes.	brain–computer interface;closing (morphology);image;paste	Lijuan Bai;Tianyou Yu;Yuanqing Li	2014	2014 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2014.6889380	human–computer interaction;computer science;artificial intelligence	EDA	-43.4059774236153	-44.56566103812989	77410
410b5dd36100fd22d171c4f349e0249546c778c6	amara: the affective museum of art resource agent	online museum;art collection search;agent;keyword search;interactive system;emotion;subject matter expert;social tagging	This interactive system uses an embedded agent for question-based art collection search on the platform of the Indianapolis Museum of Art website. Unlike a keyword search box, AMARA helps users browse and search for artwork by asking them simple questions with answers mapped to social tags. Thus, the users do not need to be subject matter experts to input specific terms to search. In designing AMARA, we focused on creating an enjoyable browsing experience and helping users to determine their known and unknown art preferences.	browsing;embedded system;interactivity;search algorithm;subject matter expert turing test;subject-matter expert	S. Joon Park;Gunho Chae;Craig M. MacDonald;Robert Stein;Susan Wiedenbeck;Jungwha Kim	2012		10.1145/2212776.2212379	emotion;human–computer interaction;artificial intelligence;multimedia;world wide web	HCI	-35.74839059106242	-49.46431027388499	77709
89270c7c060d68d1b56391626adebb193e2ede28	"""3-d rehabilitation systems for upper limbs using er actuators/brakes with high safety: """"emul"""", """"robotherapist"""" and """"plemo"""""""	feedback mechanism;optical markers;multiplayer virtual ping pong game;dc motor;motion capture system;motion capture;force feedback;tactile feedback;game server;wireless bluetooth technology;haptic interfaces;direct current	"""Rehabilitation for upper limbs is important for elderly people, people who have suffered a stroke and so on. In recent years, the needs for rehabilitation support systems are increasing, which use robot technology and virtual reality technology. Applying these technologies make efficient rehabilitation possible. But there was few rehabilitation support robot system for 3-D training with high safety. So we have developed three different rehabilitation machines for upper limbs: """"EMUL"""", """"Robotherapist"""" and """"PLEMO"""". In this paper, we introduce the recent researches of each robot."""	robot;virtual reality	Makoto Haraguchi;Takehito Kikuchi;Ying Jin;Kazuki Fukushima;Junji Furusho;Akio Inoue;Kunihiko Oda	2007	17th International Conference on Artificial Reality and Telexistence (ICAT 2007)	10.1109/ICAT.2007.39	embedded system;simulation;computer hardware;engineering	Robotics	-41.04379863988758	-45.211522041853044	77718
b51ee85104194e0e077ff829033157e5f945880f	generating and describing affective eye behaviors	emotional eye movement;virtual agents;markup language;affective computing	The manner of a person’s eye movement conveys much about nonverbal information and emotional intent beyond speech. This paper describes work on expressing emotion through eye behaviors in virtual agents based on the parameters selected from the AU-Coded facial expression database and real-time eye movement data (pupil size, blink rate and saccade). A rule-based approach to generate primary (joyful, sad, angry, afraid, disgusted and surprise) and intermediate emotions (emotions that can be represented as the mixture of two primary emotions) utilized the MPEG4 FAPs (facial animation parameters) is introduced. Meanwhile, based on our research, a scripting tool, named EEMML (Emotional Eye Movement Markup Language) that enables authors to describe and generate emotional eye movement of virtual agents, is proposed. key words: affective computing, virtual agents, emotional eye movement, markup language	affective computing;intelligent agent;logic programming;markup language;real-time transcription	Xia Mao;Zhijun Li	2010	IEICE Transactions	10.1587/transinf.E93.D.1282	computer vision;computer science;emotional expression;artificial intelligence;affective computing;markup language	AI	-36.06233161684682	-44.70300960396508	78017
3c6be2beb3aa1d9a9090813787bc1d2a51ae2635	topicvis: a gui for topic-based feedback and navigation	information retrieval;latent dirichlet allocation;topic visualization;interactive computer systems	This paper describes a search system which includes topic model visualization to improve the user search experience. The system graphically renders the topics in a retrieved set of documents, enables a user to selectively refine search results and allows easy navigation through information on selective topics within documents.	graphical user interface;rendering (computer graphics);topic model	Debasis Ganguly;Manisha Ganguly;Johannes Leveling;Gareth J. F. Jones	2013		10.1145/2484028.2484202	latent dirichlet allocation;natural language processing;computer science;machine learning;multimedia;world wide web;information retrieval	Web+IR	-34.06457899585827	-50.08112951190076	78068
8a319728a47c3815895bd1a6cd7419812fe3c3c1	a state of the art on computational music performance	computational music;sound synthesis;music performance;real time control;computer modelling;machine learning;expressive performance;computer music	Musical expressivity can be defined as the deviation from a musical standard when a score is performed by a musician. This deviation is made in terms of intrinsic note attributes like pitch, timbre, timing and dynamics. The advances in computational power capabilities and digital sound synthesis have allowed real-time control of synthesized sounds. Expressive control becomes then an area of great interest in the sound and music computing field. Musical expressivity can be approached from different perspectives. One approach is the musicological analysis of music and the study of the different stylistic schools. This approach provides a valuable understanding about musical expressivity. Another perspective is the computational modelling of music performance by means of automatic analysis of recordings. It is known that music performance is a complex activity that involves complementary aspects from other disciplines such as psychology and acoustics. It requires creativity and eventually, some manual abilities, being a hard task even for humans. Therefore, using machines appears as a very interesting and fascinating issue. In this paper, we present an overall view of the works many researchers have done so far in the field of expressive music performance, with special attention to the computational approach.	computation	Miguel Delgado;Waldo Fajardo Contreras;Miguel Molina-Solana	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.06.033	programming;evolutionary music;speech recognition;real-time control system;music and artificial intelligence;music psychology;computer science;artificial intelligence;machine learning;multimedia;pop music automation;computer music;musicality	ML	-35.19972537350267	-45.84591977716839	78088
88ed468b6b4b21fb57c657cb41cc503da599eb42	optical versus video see-through head-mounted displays in medical visualization	medical visualization;human factors;medical application;eye tracking;augmented reality;point of view;head mounted display	We compare two technological approaches to augmented reality for 3-D medical visualization: optical and video see-through devices. We provide a context to discuss the technology by reviewing several medical applications of augmented-reality re search efforts driven by real needs in the medical field, both in the United States and in Europe. We then discuss the issues for each approach, optical versus video, from both a technology and human-factor point of view. Finally, we point to potentially promising future developments of such devices including eye tracking and multifocus planes capabilities, as well as hybrid optical/video technology.	augmented reality;eye tracking;head-mounted display;medical imaging;point of view (computer hardware company);synthetic intelligence	Jannick P. Rolland;Henry Fuchs	2000	Presence: Teleoperators & Virtual Environments	10.1162/105474600566808	computer vision;augmented reality;eye tracking;computer science;optical head-mounted display;video tracking;multimedia;computer graphics (images)	Visualization	-40.242194861296205	-40.77990832497238	78092
14a3f6427fbd5178fe3b40e8db3994df744a33a4	estimating tactile perception by observing explorative hand motion of others		When we acquire tactile information about an object's surface, we actively move our hands. Past studies have shown a correlation between participants’ (i.e., touchers’) hand motion in tactile exploration and the evaluated tactile attributes of the object, which suggests that tactile perception can be estimated from statistical analysis of touchers’ hand motion. Though it has been reported that the statistical analysis of hand motion can indeed estimate tactile perception, whether humans can estimate tactile perception by observing the hand motion of others remains unclear. To investigate this, we conducted experiments wherein observers watched point-light moving hands of touchers in tactile exploration and evaluated the material being touched. Our results show that, although observers’ estimation of touchers’ perception was not accurate, observers extracted information from touchers’ hand motion for estimation, and the correlations within observers’ estimation were high. These results suggest that human observers can estimate tactile perception through visual observation of the hand motion of others by adopting common strategies about the relationships between touchers’ hand motion and tactile perception.	estimated;experiment;extraction;touch perception;touch sensation;observers	Takumi Yokosaka;Scinob Kuroki;Junji Watanabe;Shin'ya Nishida	2018	IEEE Transactions on Haptics	10.1109/TOH.2017.2775631	visualization;computer vision;artificial intelligence;computer science;feature extraction;perception	Robotics	-43.581646464249516	-51.11877448779108	78399
e51fd4952e9aad4744fda5dd651f877bcd1ea8da	throwing versus walking as indicators of distance perception in similar real and virtual environments	task performance;virtual environments;head mounted displays;distance perception;blind walking;virtual environment;effective interaction;visual system;head mounted display	For humans to effectively interact with their environment, it is important for the visual system to determine the absolute size and distance of objects. Previous experiments performed in full-cue, real-world environments have demonstrated that blind walking to targets serves as an accurate indication of distance perception, up to about 25 m. In contrast, the same task performed in virtual environments (VEs) using head-mounted displays shows significant underestimation in walking. To date, blind walking is the only visually directed action task that has been used to evaluate distance perception in VEs beyond reaching distances. The possible influence of the response measure itself on absolute distance perception in virtual environments is currently an open question. Blind walking involves locomotion and the egocentric updating of the environment with one's own movement. We compared this measure to blind throwing, a task that involves the initiation of a movement directed by vision, but no further interaction within the environment. Both throwing and walking were compressed in the VE but accurate in the real world. We suggest that distance compression found in VEs may be a result of a general perceptual origin rather than specific to the response measure.	experiment;head-mounted display;image compression;the walking dead: season two;virtual reality	Cynthia S. Sahm;Sarah H. Creem-Regehr;William B. Thompson;Peter Willemsen	2005	TAP	10.1145/1048687.1048690	computer vision;simulation;visual system;computer science;virtual machine;optical head-mounted display;communication	HCI	-44.53049281279512	-49.0156293275937	78507
ffffcfadfaf6a3dcb78d6db13229292af6e2a5f1	"""""""shooting a bird"""": game system using facial feature for the handicapped people"""	facial feature tracking;hci;augmented game;eye movement;facial features;computer game;neural network	This paper presents a novel computer game system that controls a game using only the movement of human's facial features. Our system is specially designated for the handicapped people with severe disabilities and the people without experience of using the computer. Using a usual PC camera, the proposed game system detects the user's eye movement and mouse movement, and then interprets the communication intent to play a game. The game system is tested with 42 numbers of people, and then the result shows that our game system should be efficiently and effectively used as the interface for the disabled people.		Jinsun Ju;Yunhee Shin;Eun Yi Kim	2007		10.1007/978-3-540-73110-8_70	simulation;human–computer interaction;computer science;multimedia;artificial neural network;eye movement	HCI	-44.419811418942786	-44.15086515007315	78743
1c6af9b3d99965abd15823b1531adfb8a73d5836	mobile crowdsourcing - activation of smartphones users to elicit specialized knowledge through worker profile match		"""Crowdsourcing models applied to work on mobile devices continuously reach new ways of solving sophisticated problems, now with a use of portable advanced devices, where users are not limited to a stationary use. There exists an open problem of quality in crowdsourcing models due the inexperienced or malicious workers. In this paper, we propose a model and a short specification of a platform for a bundled widely available crowdsourcing mechanism, which tries to utilize workers individual characteristics to maximum. Analyzed solution relies on geographical data classified by localization category. Secondly, we profile mobile workers by precisely analyzing their activity history. Results of this research will make an impact on better understanding the latent potential of mobile devices users. It makes for not only better quality in results, but also opens a possibility of implementing a """"twitch crowdsourcing"""" or emergency relief systems. Special tasks assigned to owners of mobile devices can help those, which are in need of help, making them the task creators."""	crowdsourcing;experience;internationalization and localization;malware;mobile device;smartphone;stationary process	Oskar Jarczyk	2014	CoRR		simulation;crowdsourcing software development;multimedia;world wide web	Web+IR	-37.88058141338168	-45.91294718844922	78911
ffa1d37893f3c22829023104667b294daab36b7e	load control based on pic microcomputer for a training machine suited to elderly people	load control;microordenador;exercise;elderly;relacion hombre maquina;educational software program;personne âgee;hombre;man machine relation;hmi;training machines;jambe;didacticiel;physical training;commande charge;anciano;microordinateur;microcomputer;captador medida;pierna;ageing population;measurement sensor;capteur mesure;human machine interface;control carga;human;elderly people;relation homme machine;training machine;programa didactico;reverse current;motor controller;microcomputers;leg;motor control;homme	This paper presents a load control method based on PIC microcomputer for a training machine suited to elderly people. The training machine allows the elderly people to maintain their physical ability through exercise close to ordinary motion that appears in daily life such as extending or flexing legs. The system configuration of the training machine and the muscle stiffness sensor used for human machine interface are described. The hardware and software of the motor controller based on PIC microcomputer are introduced and the load control method with the control function of reverse current is proposed. Several experiments are conducted to evaluate the training system and the results indicate the effectiveness of the control system.	microcomputer;pic microcontroller	Hongbo Wang;Takakazu Ishimatsu;Fumio Kasagami;Shunji Moromugi	2009	IJISTA	10.1504/IJISTA.2009.028054	control engineering;embedded system;simulation;computer science;engineering;microcomputer	HCI	-40.13558259625431	-45.4986914100765	79138
7b118e8d764b14fe30a9e0d3de029529702f4ff7	manipulation practice for upper-limb amputees using virtual reality	virtual reality;pressure sensor;assessment tool;upper limb;shoulder joint	We developed a novel interface that gives upper-limb amputees a virtual hand that can manipulate objects in a challenging environment. The interface registers specific myokinetic activity of the residual limbs, and encodes the intended voluntary movements that are then actualized as virtual hand motions. The composite myokinetic interface-virtual reality (MKI-VR) system consists of an array of pressure sensors mounted in an arm sleeve, sensors of elbow- and shoulder-joint angles, a trained filter derived from the pseudoinverse of a response matrix, and a virtual hand model, programmed in Java 3D. Users can manipulate virtual objects such as balls and pegs in a 3D training environment, while their performance at various difficulty levels is scored. In preliminary tests, upper-limb amputees readily gained the ability to grasp and release virtual objects. We propose the utility of the MKI-VR system both as an assessment tool for rehabilitation engineers, and as a motivator for amputees to exercise and thereby maintain their residual motor ability.	control system;graphics software;java 3d;playstation vr;sensor;usability;virtual reality	Manjuladevi Kuttuva;Grigore C. Burdea;James A. Flint;William Craelius	2005	Presence: Teleoperators & Virtual Environments	10.1162/1054746053967049	simulation;computer science;artificial intelligence;pressure sensor;virtual reality	Visualization	-41.69974273733082	-46.65444414692723	79466
029bb75633d5e65d338f20d18d008c6ffa5d8237	accessible question types on a touch-screen device: the case of a mobile game app for blind people		This study investigates accessibility and usability via screen reader and gestures on touch-screen mobile devices. We specifically focus on interactive tasks performed to complete exercises, answer questionnaires or quizzes. These tools are frequently exploited for evaluation tests or in serious games. Single-choice, multiple-choice and matching questions may create difficulties when using gestures and screen readers to interact on a mobile device.	mobile game	Barbara Leporini;Eleonora Palmucci	2018		10.1007/978-3-319-94277-3_42	multimedia;screen reader;mobile device;usability;mobile interaction;computer science;gesture	HCI	-47.73731475692528	-43.9110628203199	79512
c2c1c6b5bb83a5e987664c6a9d239d50ce4e1274	comparison of simultaneous measurement while viewing real objects and 3d video clips		The use of 3-dimensional images has been spreading rapidly in recent years such as in 3D films and 3D televisions. However, the influence of stereoscopic vision on human visual function remains insufficiently understood. The public has come to understand that lens accommodation and convergence are mismatched while viewing 3D video clips, and this is the main reason for the visual fatigue caused by 3D. The aim in this study is to compare the fixation distance of accommodation and convergence in viewing real objects and 3D video clips. Real objects and 3D video clips perform the same movements. We measured accommodation and convergence in viewing real objects and 3D video clips. From the result of this experiment, we found that no discrepancy exists in viewing 3D video clips like real object. Therefore, we argue that the symptoms in viewing stereoscopic vision may not be due to the discrepancy between lens accommodation and convergence.	3d television;discrepancy function;stereopsis;stereoscopy;video clip	Tomoki Shiomi;Keita Uemoto;Takehito Kojima;Satoshi Hasegawa;Masako Omori;Hiromu Ishio;Hiroki Takada;Masaru Miyao	2013			computer vision;computer graphics (images)	Vision	-42.6865879116406	-50.159577381612955	79623
40cdacf91c525b64873e5a6de73ed8b5b1836194	tactile synthesis and perceptual inverse problems seen from the viewpoint of contact mechanics	haptics;tactile sensing;computational tactile perception;inverse problem;tactile display;tactile perception;contact mechanics;tactile transducers arrays;lateral skin deformation;tactile synthesis	A contact-mechanics analysis was used to explain a tactile illusion engendered by straining the fingertip skin tangentially in a progressive wave pattern resulting in the perception of a moving undulating surface. We derived the strain tensor field induced by a sinusoidal surface sliding on a finger as well as the field created by a tactile transducer array deforming the fingerpad skin by lateral traction. We found that the first field could be well approximated by the second. Our results have several implications. First, tactile displays using lateral skin deformation can generate tactile sensations similar to those using normal skin deformation. Second, a synthesis approach can achieve this result if some constraints on the design of tactile stimulators are met. Third, the mechanoreceptors embedded in the skin must respond to the deviatoric part of the strain tensor field and not to its volumetric part. Finally, many tactile stimuli might represent, for the brain, an inverse problem to be solved, such specific examples of “tactile metameres” are given.	approximation algorithm;embedded system;lateral computing;lateral thinking;tactile transducer;traction teampage	Qi Wang;Vincent Hayward	2008	TAP	10.1145/1279920.1279921	tactile imaging;computer vision;computer science;inverse problem;artificial intelligence;haptic technology;communication;contact mechanics	Robotics	-44.70137126206062	-50.266906041386754	79767
5a22b24cff42c12622cd12503e24db7a837afb29	integration of communicative hand movements into human-computer-interaction	hand;lenguaje natural;interfase usuario;movimiento;human computer interaction;cognitive psychology;touch screen;user interface;man machine dialogue;relacion hombre maquina;langage naturel;hombre;man machine relation;motion;modelo;simulation technique;natural language;mouvement;human;mano;dialogo hombre maquina;natural language interface;interface utilisateur;modele;relation homme machine;facial expression;main;communication non verbale;comunicacion no verbal;face to face;models;homme;dialogue homme machine;expert system;non verbal communication	During face-to-face communication, the dialog partners can see and hear each other. Each speaker produces a variety of phenomena parallel to speech. Some of them, e.g. intonation, are coded vocally, others are coded by motor responses (facial expression, gestures, etc.). If human-computer-interaction (HCI) tries to mimic this situation, at least some non-verbal phenomena have to be integrated into natural language input and output. A multitude of new devices (mouse, joystick, touch-screens, etc.) have enabled this transition to multimodal HCI. Gestures which illustrate the content of the verbal message are especially suitable for integration into HCI. A relevant subset of them is pointing gestures, which specify elements of the visual context. They are performed frequently because their use shortens and simplifies the verbal output. As an illustration of these considerations, the NL dialog system XTRA (University of Saarbrucken) is presented. It allows reference to elements of a tax form by the combination of textual input and simulated pointing gestures. In order to explore the regularities of this “form deixis,” an experiment has been carried out within the framework of the XTRA-project. Furthermore, its results were taken for an evaluation of the currently used simulation technique.		Dagmar Schmauks;Michael Wille	1991	Computers and the Humanities	10.1007/BF00124149	natural language processing;nonverbal communication;speech recognition;natural language user interface;computer science;artificial intelligence;motion;linguistics;natural language;user interface;literature;facial expression;expert system	NLP	-39.67429258759392	-48.96378288791671	79845
42c124a55423917e6d6ee1f36c27a3201f56d985	interest-point-based landmark computation for agents' spatial description coordination		In applications involving multiple conversational agents, each of these agents has its own view of a visual scene, and thus all the agents must establish common visual landmarks in order to coordinate their space understanding and to coherently share generated spatial descriptions of this scene. Whereas natural language processing approaches contribute to define the common ground through dialogues between these agents, we propose in this paper a computer-vision system to determine the object of reference for both agents efficiently and automatically. Our approach consists in processing each agent’s view by computing the related, visual interest points, and then by matching them in order to extract the salient and meaningful landmark. Our approach has been successfully tested on real-world data, and its performance and design allow its use for embedded robotic system communication.	computation;computer vision;dialog system;embedded system;natural language processing;robot;type system	Joanna Isabelle Olszewska	2016		10.5220/0005847705660569	computer vision;artificial intelligence;machine learning	AI	-34.03518281499002	-41.24846169719478	79852
330119d7e0a30011eb257897ee26e4cbbcb26d84	quantitative and qualitative evaluation of vision-based teleoperation of a mobile robot	mobile robot;degree of freedom;qualitative evaluation	This paper analyzes how performance of a basic teleoperation task are influenced by the viewpoint of the video feedback, using a remote mobile robot. Specifically, the viewpoint is varied in terms of height and tilt and the influence on a basic task, such as following some pre-defined paths, is analyzed. The operators are able to control one motor degree of freedom and up to two perceptive degrees of freedom. It is shown that performance vary depending both on the viewpoint and on the amount of perceptive freedom; in particular, the chosen metrics give better results when more perspective and, surprisingly, a more constrained perception is deployed. Furthermore, the contrast between the actual performance and the performance perceived by the operators is shown, which allows to discuss about the need of quantitative approaches in measuring the efficiency of a teleoperation task.	mobile robot	Luca Giulio Brayda;Jesús Ortiz;Nicolas Mollet;Ryad Chellali;Jean-Guy Fontaine	2009		10.1007/978-3-642-10817-4_78	control engineering;mobile robot;computer vision;simulation;computer science;artificial intelligence;degrees of freedom	Robotics	-45.19844586519414	-48.51392091365359	79853
a1b282ce05b5436173857093c0eff5420b315161	an interactive mobile control center for cyber-physical systems	smart home;mobile app;cyber physical systems;control center;dashboard	Cyber-physical Systems (CPS) connect the virtual world of software and services with the real world of humans, objects and things. Sensors and actuators enable the detection and manipulation of physical and virtual properties. However, ubiquitous access for controlling and monitoring of all CPS components is a major challenge due to the complexity of CPS--numerous control options, devices and high volumes of data. We present a user-friendly mobile control center for CPS. It allows non-expert users to ubiquitously access all sensors, actuators, processes and more complex components represented by service robots. We demonstrate the control center in the smart home domain.	cyber-physical system;home automation;robot;sensor;usability;virtual world	Ronny Seiger;Diana Lemme;Susann Struwe;Thomas Schlegel	2016		10.1145/2968219.2971410	embedded system;simulation;computer science;operating system;cyber-physical system;computer security;dashboard;computer network	Robotics	-43.590597507899936	-42.0934272748339	80124
139f837d44fd99fce29f0f4f1b34045749fd4118	cost effective smart remote controller based on invisible ir-led using image processing	image processing;human intent command cost effective smart remote controller invisible ir led image processing camera digital appliance infrared led electronic appliance digital smart tv air conditioner built in embedded processor command signal digital device;light emitting diodes tv home appliances cameras light sources gesture recognition conferences;light emitting diodes;telecontrol equipment;infrared imaging;telecontrol equipment cameras image processing infrared imaging light emitting diodes;cameras	We present a new cost effective smart remote controller using only a camera which may be installed or attached on digital appliances and infrared (IR) LED to control various electronic appliances, such as digital smart TV, air conditioner and so on. The users can easily operate the invisible IR LED to make the specific command by simply blinking the IR LED. Then, the proposed system can easily analyze the state of IR LED and understand the human intention by image processing using a built-in embedded processor within the digital appliances. Therefore, the proposed system can directly understand and generate specific command signal to control the digital appliances without any communication circuit between remote controller and digital devices unlike the previous remote controllers. Experimental results show that the proposed remote controller is easy to use and can successfully operate to execute human intent command.	canonical account;embedded system;image processing;remote control;smart tv	Yunjung Park;Minho Lee	2013	2013 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2013.6486964	embedded system;computer vision;electronic engineering;computer hardware;image processing;computer science;engineering;electrical engineering;light-emitting diode	EDA	-40.497562689831234	-43.330273214625905	80424
cc332227d52fa01105ec5395fb24a9e8f83ab332	microassembly planning using physical-based models in virtual environment	microassembly virtual environment virtual reality education workstations micromechanical devices safety visual databases robot sensing systems force feedback;microrobots;remote control;virtual force feedback microassembly planning remote control microrobotic workstation hybrid mems virtual reality technology cad cam databases virtual workcell;physics based modeling;virtual reality;force feedback;assembly planning;cad cam;telecontrol;microassembling;force feedback microassembling assembly planning telecontrol microrobots virtual reality cad cam;virtual environment;simulation environment	Remote control of microrobotic workstations dedicated to microassembly of hybrid MEMS is a challenging problem due to the lack of reliability and safety in microplanning procedures. It can be avoided by using virtual reality technology. Using the simulated environment in virtual reality, the operator can practice, explore and prevent the problems that occur during implementation. We proposed a concept of a physically behaved microrobotic workcell in virtual environment for microassembly teaching. In a first part, implementation of physical behaviors of virtual objects into VR for micro-task teaching is introduced. A virtual microworld is exactly reconstructed from the CAD-CAM databases of the real environment and simulates contact and elastic robotic interactions, virtual sensing of position and force, etc. Then, the virtual workcell is simulated and an example of micro-task teaching is given. The implementation includes view tracking using virtual force feedback, visual and audio rendering in virtual microenvironment.	computer-aided design;database;habitat;haptic technology;interaction;microelectromechanical systems;remote control;robot;virtual reality;workstation	Mustapha Hamdi;Antoine Ferreira	2004	2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)	10.1109/IROS.2004.1389937	embedded system;computer vision;simulation;computer science;engineering;virtual machine;artificial intelligence;virtual reality;haptic technology;computer-aided technologies;remote control	Robotics	-39.611252865283944	-46.57630354368229	80453
ba2417de24ba591ab220f4e562fce35cde13f830	emotion rendering in plantar vibro-tactile simulations of imagined walking styles	legged locomotion vibrations engines foot footwear haptic interfaces rendering computer graphics;solesound emotion rendering walking footstep sounds	This paper investigates the production and identification of emotional states of a walker using plantar vibro-tactile simulations. In a first experiment, participants were asked to render, according to imagined walking scenarios, five emotions (aggressive, happy, neutral, sad, and tender) by manipulating the parameters of synthetic footstep vibrations simulating various combinations of surface materials and shoes. Results allowed to identify, for the involved emotions and vibration conditions, the mean values and ranges of variation of two parameters, vibration amplitude and temporal distance between consecutive steps. Results were in accordance with those reported in previous studies on real walking, suggesting that the plantar vibro-tactile expression of emotions in walking is independent of the real or imagined motor activity. In a second experiment, participants were asked to identify the emotions portrayed by walking vibrations synthesized by setting the synthesis engine parameters to the mean values found in the first experiment. Results showed that the involved algorithms were successful in conveying the emotional information at a level comparable with previous studies. Results of both experiments revealed strong similarities with those of an analogous study on footstep sounds suggesting that emotionally expressive walking styles are consistently produced and recognized at auditory and plantar vibro-tactile level.	algorithm;amiga walker;applicative programming language;avatar (computing);computer simulation;desktop computer;ecology;experiment;image;multimodal interaction;neural oscillation;performance;sadness;shoes;synthetic intelligence;virtual reality	Luca Turchet;Damiano Zanotto;Simone Minto;Antonio Rod&#x00E0;;Sunil K. Agrawal	2017	IEEE Transactions on Affective Computing	10.1109/TAFFC.2016.2552515	computer vision;simulation;computer science;computer graphics (images)	Visualization	-47.974091893871	-49.86520703725958	80722
d0ba44e87f9cfc1fa55a135451737c023626e84d	a virtual environment for the simulation and programming of excavation trajectories	virtual environment	An excavator simulator has been developed to facilitate the training of human operators and to evaluate control strategies for heavy-duty hydraulic machines. The operator controls a virtual excavator by means of a joystick while experiencing visual and force feedback generated by environment and machine models. The simulator comprises an impedance model of the excavator arm, a model for the bucket-ground interaction forces, a graphically rendered visual environment, and a haptic interface. This paper describes the simulator components and their integration.	characteristic impedance;columbia (supercomputer);excavator (microarchitecture);experiment;haptic technology;human factors and ergonomics;joystick;performance evaluation;refinement (computing);simulation;virtual reality	Simon P. DiMaio;Septimiu E. Salcudean;Claude Reboulet	2001	Presence: Teleoperators & Virtual Environments	10.1162/105474601753132650	computer vision;simulation;human–computer interaction;computer science;virtual machine;operating system	Robotics	-36.36624625124006	-38.083703287313796	80761
97cfadccff30b87a885f1d6ff358688058266c6d	taplock: exploit finger tap events for enhancing attack resilience of smartphone passwords	vibrations delays entropy resilience fingers haptic interfaces information systems;haptic input feedback taplock finger tap events attack resilience smartphone password system capacitive touch screens password resilience shoulder surfing attacks phone screen authentication delay;touch sensitive screens capacitive sensors human computer interaction smart phones tactile sensors telecommunication security	In this paper, we present TapLock as a smartphone password system that exploits the finger tap events on capacitive touch screens for increasing the password's resilience to shoulder-surfing attacks (where the password input by a user can be easily observed by a bystander over the user's shoulder). TapLock captures the size and the axis length of the finger touch area on the phone screen for creating a password, which cannot be easily observed by a shoulder surfer. Our user study shows that TapLock has several advantages over existing smartphone password systems, including its strong attack resilience, small authentication delay, and haptic input feedback that improves the usability.	apache axis;authentication;haptic technology;password;smartphone;touchscreen;usability testing	Hongji Yang;Lin Chen;Kaigui Bian;Yang Tian;Fan Ye;Wei Yan;Tong Zhao;Xiaoming Li	2015	2015 IEEE International Conference on Communications (ICC)	10.1109/ICC.2015.7249465	embedded system;internet privacy;computer security	Mobile	-44.80765403354447	-44.70847785007135	80856
40ed2817b8b80dbc8d574b1ef9498b9eff38c122	hands-free operation of a small mobile robot	energy conservation;control systems;tecnologia industrial tecnologia mecanica;electro oculogram;electro myogram;radio equipment;mobile robot;user interface;mobile robotics;grupo de excelencia;emg;proof of concept;control system;research programs;robots;intelligent system;telerobotics;eeg;energy conservation consumption and utilization;tecnologias;electro encephalogram;user interfaces;brainwaves;control method;eog;teleoperation	The Intelligent Systems and Robotics Center of Sandia National Laboratories has an ongoing research program in advanced user interfaces. As part of thk research, promising new transduction devices, particularly handsfree devices, are being explored for the control of mobile and floor-mounted robotic systems. Brainwave control has been successfully demonstrated by other researchers in a variety of fields. In the research described here, Sandia developed and demonstrated a proof-of-concept brainwave-controlled mobile robot system. Preliminary results were encouraging. Additional work required to turn this into a reliable, fieldable system for mobile robotic control is identified. Used in conjunction with other controls, brainwave control could be an effective control method in certain circumstances.	mobile robot;neural oscillation;transduction (machine learning);user interface	Wendy Amai;Jill Fahrenholtz;Chris Leger	2001	Auton. Robots	10.1023/A:1011260229560	embedded system;simulation;computer science;control system;artificial intelligence;user interface	Robotics	-39.1895136083682	-47.58334782366208	80899
af276a07a28f23f35b221a6c4d56c0b9310ebaf1	ecosonic: auditory peripheral monitoring of fuel consumption for fuel-efficient driving	auditory interfaces;eco feedback;behavior change;sustainable hci	In this paper, we propose to make use of an auditory fuel efficiency display as means to support car drivers in adopting an energy-efficient driving style. We report on the development of the EcoSonic system as a platform for evaluating such displays and present five design approaches to guide their realization. In a study with 30 participants, we evaluated two prototype auditory displays against the baseline of visual-only eco-driving feedback in a within-subject study. Our selected designs are described in full detail. Key findings include a significantly reduced fuel consumption as well as lower engine speeds compared to the visual display. Furthermore, questionnaire analysis results confirm that the auditory conditions are less obtrusive and also seem to allow a more subconscious processing and comprehension of the provided information. Finally, we have found that the affectiveness of the display design seems to have a positive impact on its perceived helpfulness and the ability to absorb its information subconsciously.	baseline (configuration management);list comprehension;peripheral;prototype	Jan Hammerschmidt;Thomas Hermann	2017	Displays	10.1016/j.displa.2016.11.002	simulation;engineering;behavior change;multimedia	HCI	-47.50115499594973	-46.61239199803591	81169
a0ceb3d2f078f0e7c90cb80b9da729451b065f70	attention approximation of mobile users towards their environment	attention approximation;gaze modelling;navigation	Public environments are increasingly equipped with interactive features, such as electronic maps for way finding, dynamic information displays or animated advertisements. Understanding the attention patterns of users within these environments is important for the design and evaluation of such interactive elements. We present a modelling approach based on dynamic adaptation of the users' field-of-view, bottom-up visual saliency calculation and task-dependent semantic interest modeling that allows approximating the attention of users with regard to a photorealistic 3D-model. This improved availability of attention information can help to design more usable navigation systems, identify problems for user groups with special needs and support the design of seamless attention switches between information elements.	approximation;bottom-up proteomics;display device;interactivity;map;network switch;seamless3d	Johann Schrammel;Georg Regal;Manfred Tscheligi	2014		10.1145/2559206.2581295	computer vision;navigation;simulation;computer science;multimedia	HCI	-46.746614390005725	-40.57224981392437	81641
afde75ed2a2f69860e85b3782ff0a0fb2b588392	a live augmented reality tool for facilitating interpretation of 2d construction drawings		Construction consists of a complex set of tasks in the 3D world based on instructions encoded in 2D drawings. Although the process is facilitated by the availability of 3D models displayed on digital tablets on construction sites, it is not always clear what is the exact 3D location of specific elements in 2D drawings. In this preliminary study, we propose a method based on a computer tablet and a head mounted augmentation system that enables the user to display a 3D element by clicking on its 2D representation on a construction drawing. Early results show that the method has potential, but highlights perception issues. We proposed and tested solutions to alleviate those issues.	augmented reality	Stéphane Côté;Myriam Beauvais;Antoine Girard-Vallée;Rob Snyder	2014		10.1007/978-3-319-13969-2_32	human–computer interaction;engineering;multimedia;computer graphics (images)	HCI	-45.69424140075965	-40.087132607212546	81691
df368f6cc4df4b508788d0b3bed8eb4388322c1d	modelling human-humanoid robot interaction in soccer robotics domain using ngomsl	humanoid robot;human computer interaction;usability evaluation;robocup 2007 humanoid soccer league;ngomsl model;mobile robot;ngomsl;robo erectus junior version;selection rules language model;mobile robots;human robot interaction;multi robot systems human computer interaction humanoid robots mobile robots;robocup 2007 humanoid soccer league human humanoid robot interaction soccer robotics ngomsl human computer interaction selection rules language model cognitive processes rapid usability evaluation mobile robots human robot interaction robo erectus junior version humanoid robots;cognitive process;humanoid robots;rapid usability evaluation;human humanoid robot interaction;robots;multi robot systems;soccer robotics;cognitive processes;interaction design;usability evaluation ngomsl model humanoid robots human humanoid robot interaction	In the field of human-computer interaction, natural goals, operators, methods, and selection rules language (NGOMSL) model is one of the most popular method for modelling knowledge and cognitive processes for rapid usability evaluation. NGOMSL model is a description of the knowledge that a user must possess to operate the system represented as elementary actions for effective usability evaluations. In the last few years, mobile robots have been exhibiting a stronger presence in the commercial markets and very little work has been done with NGOMSL modelling for usability evaluations in human-robot interaction discipline. This paper focuses on applying NGOMSL modelling for usability evaluation of human-humanoid robot interaction in soccer domain. Usability evaluations were performed and adequate results were obtained. Evaluated interaction design was adopted by our Robo-Erectus Junior version of humanoid robots in the RoboCup 2007 humanoid soccer league.	cognition;fault detection and isolation;humanoid robot;human–computer interaction;human–robot interaction;interaction design;mobile robot;ngomsl;robotics;selection rule;usability	Rajesh Elara Mohan;Carlos Antonio Acosta Calderon;Changjiu Zhou;Pik Kong Yue;Lingyun Hu	2008	RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2008.4600700	human–robot interaction;mobile robot;simulation;cognition;computer science;humanoid robot;artificial intelligence	Robotics	-34.6522993109266	-40.213052587837105	81793
6606ffe52a9cd870ccf6cf8d3988f8d04dd0e83e	i'm there! the influence of virtual reality and mixed reality environments combined with two different navigation methods on presence	3d navigation;legged locomotion;navigation usability virtual environments legged locomotion mobile communication smart phones;virtual reality;smart phones;user studies;virtual environments;mixed reality presence user studies 3d navigation;navigation;kinect sensor virtual reality mixed reality environments navigation methods natural walking wii balance board;mobile communication;presence;usability;mixed reality	For various VR/MR/AR applications, such as virtual usability studies, it is very important that the participants have the feeling that they are really in the environment. This feeling of “being” in a mediated environment is described as presence. Two important factors that influence presence are the level of immersion and the navigation method. We developed two navigation methods to simulate natural walking using a Wii Balance Board and a Kinect Sensor. In this preliminary study we examined the effects of these navigation methods and the level of immersion on the participants' perceived presence in a 2×2 factorial between-subjects study with 32 participants in two different VEs (Powerwall and Mixed-Reality-See-Through-Glasses). The results indicate that reported presence is higher for the Kinect navigation and Powerwall for some facets of presence.	immersion (virtual reality);kinect;mixed reality;powerwall;simulation;usability;virtual reality;wii	Mario Lorenz;Marc Busch;Loukas Rentzos;Manfred Tscheligi;Philipp Klimant;Peter Fröhlich	2015	2015 IEEE Virtual Reality (VR)	10.1109/VR.2015.7223376	navigation;computer-mediated reality;simulation;mobile telephony;usability;human–computer interaction;computer science;artificial intelligence;virtual reality;mixed reality;multimedia;mobile robot navigation	Visualization	-45.728674024942386	-45.145564452631916	81889
98b20b6315018fd4704b7bd43e2bc236f8edfc64	from semantic models to cognitive buildings		Today’s operation of buildings is either based on simple dashboards that are not scalable to thousands of sensor data or on rules that provide very limited fault information only. In either case considerable manual effort is required for diagnosing building operation problems related to energy usage or occupant comfort. We present a Cognitive Building demo that uses (i) semantic reasoning to model physical relationships of sensors and systems, (ii) machine learning to predict and detect anomalies in energy flow, occupancy and user comfort, and (iii) speech-enabled Augmented Reality interfaces for immersive interaction with thousands of devices. Our demo analyzes data from more than 3,300 sensors and shows how we can automatically diagnose building operation problems.	augmented reality;machine learning;scalability;sensor	Joern Ploennigs;Anika Schumann	2017			machine learning;artificial intelligence;computer science;cognition	HCI	-36.27098247719444	-46.663109277888914	82337
6cd262e209961cbdf792ff09087f4a36021efcc0	movexp: a versatile visualization tool for human-computer interaction studies with 3d performance and biomechanical data	sra e vetenskap serc;human computer interaction;manniska datorinteraktion interaktionsdesign;biological system modeling;biomechanics;information visualization;datalogi;visual encodings movexp tool human computer interaction hci 3d performance data biomechanical data user interfaces physical ergonomics optical motion capture biomechanical simulation multidimensional data set;data visualization;data visualization human computer interaction ergonomics biomechanics biological system modeling;datavetenskap datalogi;design study;sra e science serc;manniska datorinteraktion;computer science;ergonomics;human computer interaction data visualisation	In Human-Computer Interaction (HCI), experts seek to evaluate and compare the performance and ergonomics of user interfaces. Recently, a novel cost-efficient method for estimating physical ergonomics and performance has been introduced to HCI. It is based on optical motion capture and biomechanical simulation. It provides a rich source for analyzing human movements summarized in a multidimensional data set. Existing visualization tools do not sufficiently support the HCI experts in analyzing this data. We identified two shortcomings. First, appropriate visual encodings are missing particularly for the biomechanical aspects of the data. Second, the physical setup of the user interface cannot be incorporated explicitly into existing tools. We present MovExp, a versatile visualization tool that supports the evaluation of user interfaces. In particular, it can be easily adapted by the HCI experts to include the physical setup that is being evaluated, and visualize the data on top of it. Furthermore, it provides a variety of visual encodings to communicate muscular loads, movement directions, and other specifics of HCI studies that employ motion capture and biomechanical simulation. In this design study, we follow a problem-driven research approach. Based on a formalization of the visualization needs and the data structure, we formulate technical requirements for the visualization tool and present novel solutions to the analysis needs of the HCI experts. We show the utility of our tool with four case studies from the daily work of our HCI experts.	area striata structure;cost efficiency;data structure;estimated;graphics visualization;human factors and ergonomics;human–computer interaction;imagery;motion capture;movement;muscle;optic nerve glioma, childhood;preparation;requirement;simulation;solutions;user interface device component;science of ergonomics	Gregorio Palmas;Myroslav Bachynskyi;Antti Oulasvirta;Hans-Peter Seidel;Tino Weinkauf	2014	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2014.2346311	computer vision;simulation;information visualization;human–computer interaction;computer science;biomechanics;data mining;multimedia;data visualization	Visualization	-42.650564916469584	-47.6303250334425	82359
908c8f43a0ee357ac1b26dcf389a836440c43aec	real walking increases simulator sickness in navigationally complex virtual environments	simulated walking;real walking;analytical models;walking;atmospheric measurements;legged locomotion;particle measurements;computer graphics;h 5 1 information interfaces and presentation multimedia information systems artificial;methodology and techniques;user study;h 5 1 information interfaces and presentation multimedia information systemsâ artificial augmented and virtual realities;virtual reality;locomotion;testing;disorientation;virtual environments;indexing terms;multimedia information system;multimedia systems;augmented;computer graphic;navigation;computational modeling;travel technique;navigationally complex virtual environment;three dimensional displays;i 3 7 computer graphics three dimensional graphics and realismâ virtual reality;occulomotor discomfort;solid modeling;and virtual realities;analysis of variance;legged locomotion navigation virtual environment computational modeling computer graphics testing analysis of variance analytical models multimedia systems virtual reality;i 3 7 computer graphics three dimensional graphics and realism virtual reality;nausea;three dimensional graphics and realism;user study h 5 1 information interfaces and presentation multimedia information systems artificial augmented and virtual realities i 3 6 computer graphics methodology and techniques interaction techniques i 3 7 computer graphics three dimensional graphics and realism virtual reality virtual environments locomotion navigation walking;virtual environment;virtual maze;natural walking;real maze;i 3 6 computer graphics methodology and techniquesâ interaction techniques;simulator sickness;occulomotor discomfort real walking simulator sickness navigationally complex virtual environment travel technique real maze virtual maze natural walking simulated walking disorientation nausea;interaction technique;i 3 6 computer graphics methodology and techniques interaction techniques	We report on a study in which we investigate the effects of travel technique on simulator sickness in a real and virtual environment. Participants explored either a real maze or a virtual maze using either natural walking or simulated walking. Reported scores for measures of overall simulator sickness, disorientation, nausea, and occulomotor discomfort were all higher in the natural walking condition than either the simulated walking or real world conditions. This indicates that simulated walking is a better choice for reducing simulator sickness during tasks requiring a navigationally complex environment and a long amount of time.	simulation;virtual reality	Evan A. Suma;Samantha L. Finkelstein;Myra Reid;Amy Banic;Larry F. Hodges	2009	2009 IEEE Virtual Reality Conference	10.1109/VR.2009.4811037	navigation;simulation;index term;analysis of variance;human–computer interaction;computer science;virtual machine;operating system;virtual reality;software testing;solid modeling;computer graphics;computational model;interaction technique;statistics;computer graphics (images)	Visualization	-43.56105852090313	-48.64125643142937	82492
041bd32da37543a01a910156147b1e0878908305	binocular disparity as an explanation for the moon illusion		We present another explanation for the moon illusion, in which the moon looks larger near the horizon than near the zenith. In our model, the sky is considered a spatially contiguous and geometrically smooth surface. When an object (like the moon) breaks the contiguity of the surface, humans perceive an occlusion of the surface rather than an object appearing through a hole. Binocular vision dictates that the moon is distant, but this perception model dictates that the moon is closer than the sky. To solve the dilemma, the brain distorts the projections of the moon to increase the binocular disparity, which results in increase of the angular size of the moon. The degree of the distortion depends upon the apparent distance to the sky, which is influenced by the surrounding objects and the condition of the sky. The closer the sky appears, the stronger the illusion. At the zenith, few distance cues are present, causing difficulty with distance estimation and weakening the illusion.	angularjs;binocular disparity;binocular vision;distortion;moon;user illusion	Joseph Antonides;Toshiro Kubota	2013	CoRR		computer vision	Vision	-41.95367179753548	-50.46892616523057	82595
f22b21212f7dfe76e4c837af419462042d6b1682	tsunami evacuation drill system using motion hazard map and smart devices	tsunami evacuation drill;motion hazard map;tablet computers;smart glasses	Tsunami resulting from large-scale earthquakes can cause significant damage. Therefore, a tsunami evacuation drill (TED) is important for disaster education. However, conventional TEDs do not necessarily produce the maximum effect because some participants move quickly to evacuation sites while simply imagining the tsunami and do not feel a sense of tension. We have developed a TED system that uses a motion hazard map (MHM) and smart devices (e.g. tablet computers and smart glasses). This system has three phases, i.e. simulation, evacuation and reflection phases. For the simulation, the system allows users to generate a MHM by configuring tsunami simulation using Google Maps. For the evacuation, the system presents the generated MHM by panoramically visualising the approaching tsunami (simulated animation) and plotting the participants' current locations on Google Maps. The participants can move quickly while feeling a sense of tension and viewing the simulated animation to determine the speed of and distance to the tsunami. For the reflection, the system superimposes recorded participants' evacuation routes onto the simulated animation.	cartography;computer animation;simulation;smart device;smartglasses;tablet computer;tsunami games	Junya Kawai;Hiroyuki Mitsuhara;Masami Shishibori	2016	2016 3rd International Conference on Information and Communication Technologies for Disaster Management (ICT-DM)	10.1109/ICT-DM.2016.7857221	seismology;simulation;geography;cartography	HCI	-42.56722540062506	-38.70044604948564	82626
d6f36a120db9e2d69fc9985485a80439ee20595a	using sound feedback to help blind people navigate		People can generate mental representations of one physical magnitude (e.g., distance) in terms of another (e.g., pitch). Capitalizing on this ability, we developed a system that could help visually impaired people navigate by presenting them with sounds of lower or higher pitch according to their distance from objects that they gaze at. Eight blind-folded participants completed a Hebb-Williams Maze with the system and twelve blind-folded participants completed the maze without it. Both groups completed the maze five times. We found that participants completed the maze faster in the last three trials compared to the first trial. There were no differences between the groups in the pace of improvement and in the time to complete the maze. However, participants who used the system made less errors in the maze than participants who did not use it. Our findings indicate that the system can potentially assist visually impaired people navigate. The contribution of the system to navigation will be further investigated with larger number of participants and with more extensive training.	hebbian theory;pitch (music)	Assaf Botzer;Nir Shvalb;Boaz Ben-Moshe	2018		10.1145/3232078.3232083	spatial memory;simulation;control engineering;engineering	HCI	-46.68582644434719	-49.1012817285855	82712
b9c99002c833b9247013abd34d8884b8075ff682	full-body movement in numerical trainings: a pilot study with an interactive whiteboard		In this pilot study, we introduce an effective spatial-numerical training to improve children’s arithmetic abilities. We designed this training based on previous successful trainings of spatial-numerical associations (such as number line estimation) and introduced a full-body response movement. Children responded to a number line estimation task presented on an interactive whiteboard by moving their whole body to the left or right. In a pilot study with a small group of children (total sample size N = 27), this experimental training was compared to two control trainings, one training the same task without the full-body movement and one training a different task with full-body movement. The experimental training led to significant improvement in all dependent measures and was most effective in enhancing performance in a spatial-numerical task. Furthermore, full-body movement helped children maintain their performance level in multi-digit addition. We conclude that full-body movement can enhance the efficiency of numerical trainings, which could also be successfully utilized in serious games and incorporated into the classroom.	interactive whiteboard	Ursula Fischer;Korbinian Moeller;Stefan Huber;Ulrike Cress;Hans-Christoph Nuerk	2015	Int. J. Serious Games		simulation;computer science;multimedia;communication	HCI	-46.85125946302655	-49.717631228111436	83158
7916d5d6965781d4d01e04c01207bf9485bba85e	inside looking out or outside looking in?: an evaluation of visualisation modalities to support the creation of a substitutional virtual environment		Current Virtual Reality systems only allow users to draw a rectangular perimeter to mark the room-scale area they intend to use. Domestic environments can include furniture and other obstacles that hinder the ease with which users can naturally walk. By leveraging the benefits of passive haptics, users can match physical objects with virtual counterparts, to create substitutional environments. In this paper we explore two visualisation modalities to aid in the creation of a coarse virtual representation of the physical environment, by marking out the volumes of space where physical obstacles are located, to support the substitution process. Our study investigates whether this process is better supported by an inside-looking-out 3D User Interface (that is, viewing the outside world while immersed in Virtual Reality) or from an outside-looking-in one (while viewing the Virtual Environment through an external device, such as a tablet). Results show that the immersive option resulted in better accuracy and was the one with the highest overall preference ratings.	3d user interaction;haptic technology;integrated circuit layout;interaction technique;item unique identification;perimeter;peripheral;scientific visualization;tablet computer;user interface;virtual reality;virtual world	Jose F. Garcia;Adalberto L. Simeone;Matthew Higgins;Wendy Powell;Vaughan Powell	2018		10.1145/3206505.3206529	modalities;computer science;multimedia;human–computer interaction;immersion (virtual reality);visualization;haptic technology;virtual reality;virtual machine;marking out;user interface	HCI	-45.28715204655329	-40.12085406440991	83265
84e86eda7014368e45b9366a9b8d6b609ac16c2a	a comparative study of assembly planning in traditional and virtual environments	product safety;design automation;rapid prototyping industrial;handling difficulties;design engineering;rapid prototyping assembly planning task nonimmersive desktop virtual reality environment traditional engineering environment immersive cave virtual reality environment cave automatic virtual environment subject performance time problematic assembly steps assembly sequences reorientation dissimilar assembly steps handling difficulties assembly operations computer aided manufacturing;assembly operations;virtual reality;tellurium;indexing terms;problematic assembly steps;assembly;immersive cave virtual reality environment;rapid prototyping;human factors;assembly planning;rapid phototyping;computer aided manufacturing;subject performance time;reorientation;assembly sequences;production planning;not significant;rapid prototyping industrial assembly planning virtual reality human factors;assembly planning task;cave automatic virtual environment;dissimilar assembly steps;virtual environment;assembly virtual reality tellurium computer aided manufacturing production planning design engineering virtual environment costs product safety design automation;nonimmersive desktop virtual reality environment;traditional engineering environment	Presents an experiment that investigated the potential benefits of virtual reality (VR) environments in supporting assembly planning. In the experiment, 15 subjects performed an assembly planning task in three different conditions: a traditional engineering (TE) environment, a nonimmersive desktop VR (DVR) environment, and an immersive CAVE (Cave Automatic Virtual Environment) VR (CVR) environment. The effects of the three conditions on the subjects' performance were analyzed. The subjects' performance time in the TE condition was significantly longer than that in the DVR condition and that in the CVR condition, whereas the difference in performance time between the DVR condition and the CVR condition was not significant. The total number of problematic assembly steps in the TE condition was significantly greater than that in the CVR condition. Specifically, the subjects' assembly sequences in the TE condition involved more reorientations than in the DVR condition. The number of difficult assembly steps in the TE condition was significantly greater than that in the DVR condition, which was significantly greater than that in the CVR condition. The number of dissimilar assembly steps in the TE condition was significantly greater than that in the CVR condition, which was significantly greater than that in the DVR condition. Hence, the results revealed advantages of the two VR environments over the TE environment in improving the subjects' overall assembly planning performance and in minimizing the handling difficulty, excessive reorientation and dissimilarity of assembly operations.	automated planning and scheduling;virtual reality	Nong Ye;Pat P. Banerjee;Amarnath Banerjee;Fred Dech	1999	IEEE Trans. Systems, Man, and Cybernetics, Part C	10.1109/5326.798768	cave automatic virtual environment;simulation;index term;computer science;virtual machine;artificial intelligence;tellurium;assembly;virtual reality	Robotics	-41.03968179329019	-47.85838966731	83357
2375e0a3e4e536cc7ba1c1d9860b87156eca125e	evaluation of the command and control cube	audio user interfaces;visual mode;application control;cyberglove;sound mode;technological innovation;interaction styles;metaphors;command and control systems feedback virtual environment technological innovation large screen displays acoustic testing auditory displays data gloves layout navigation;command and control cube;audio device;stereoscopic images;virtual reality;body basedinteraction;layout;virtual environments;auditory displays;blind mode;large screen display;acoustic testing;open field;navigation;feedback;graphical user interfaces;quick access menu;command and control;tactile feedback;large screen;data gloves;workbench;wearable computing;graphical display;multimodal interfaces;virtual environment;command and control cube virtual environments application control quick access menu workbench large screen stereoscopic images graphical display visual mode blind mode cyberglove tactile feedback virtual environment sound mode audio device;large screen displays;command and control systems;audio user interfaces data gloves graphical user interfaces virtual reality	Application control in virtual environments (VE) is still an open field of research. The Command and Control Cube (C3) developed by Grosjean et al. is a quick access menu forthe VE configuration called workbench (a large screen displaying stereoscopic images). The C3 presents two modes, one with the graphical display of the cubic structure associated to the C3 and a blind mode for expert users, with no feedback. In this paper we conduct formal tests of the C3 under four different conditions: the visual mode with the graphical display, the blind mode with no feedback and two additional conditions enhancing the expert blind mode: a tactile mode with the tactile feedback of a Cyberglove¿ and a sound mode with a standard audio device. Results show that the addition of sound and tactil feedback is more disturbing to the users than the blind mode. The visual mode performs the best although the blind mode achieves some promising results.	cube;cubic function;infographic;stereoscopy;virtual reality;wired glove;workbench	Jérôme Grosjean;Jean-Marie Burkhardt;Sabine Coquillart;Paul Richard	2002		10.1109/ICMI.2002.1167041	layout;command and control;computer vision;navigation;simulation;wearable computer;human–computer interaction;computer science;virtual machine;operating system;feedback;graphical user interface;virtual reality;multimedia	HCI	-43.049673752426806	-40.942762263401846	83450
84681ce05988d50b760a675e80ed6fc95fca252b	an advanced teleoperation testbed	low resolution	Due to the technology available, most previous work in teleoperated robotics used relatively low-resolution video links and provided limited perceptual feedback to the teleoperator. In most cases, these projects reported only limited teleoperator success compared to vehicles with human drivers on-board. We set out to build a high-fidelity teleoperation system which takes advantage of recent technological advances. This system permits highly capable teleoperation and has allowed us to begin to investigate the minimum system requirements for effective teleoperation.	on-board data handling;requirement;robotics;system requirements;telerobotics;testbed	Bill Ross;John Bares;David Stager;Larry D. Jackel;Michael Perschbacher	2007		10.1007/978-3-540-75404-6_28	embedded system;simulation;image resolution;computer science	Robotics	-36.898505040482476	-41.5290916612708	83463
88ce9a8741b395fea334718b2adede5052cdb10b	multimodal frustration detection on smartphones	multitask game;multimodal system;frustration detection	Detecting user's frustration on a smartphone could have a significant impact on applications such as intelligent tutoring systems or app testing systems. Our goal is to provide a multimodal frustration detection system using data exclusively retrieved from a smartphone; motion sensor readings, touch gestures and face videos recorded from the smartphone's front camera.	motion detector;multimodal interaction;smartphone	Esther Vasiete;Tom Yeh	2015		10.1145/2702613.2732867	computer vision;simulation	Mobile	-45.03606007460655	-43.75991807822319	83516
825d2f91e5d09bd59643728d9a5c579c6ab77d51	image-based gesture recognition for user interaction with mobile companion-based assistance systems	human computer interaction;image segmentation;image classification;complex product assembling image based gesture recognition user interaction mobile companion based assistance systems image based methods robust static hand gesture recognition robust dynamic hand gesture recognition real time gesture recognition skin tones hand segmentation feature extraction hmm classifier dynamic gesture extraction;mobile assistance system head mounted display gesture recognition companion system;real time systems assembling feature extraction gesture recognition hidden markov models human computer interaction image classification image segmentation mobile computing production engineering computing;production engineering computing;hidden markov models;feature extraction;assembling;cameras gesture recognition mobile communication dynamics feature extraction skin hidden markov models;mobile computing;gesture recognition;real time systems	In this paper, we present image-based methods for robust recognition of static and dynamic hand gestures in real-time. These methods are used for an intuitive interaction with an assistance-system in which the skin-tones are used to segment the hands. The segmentation builds the basis of feature extraction for the static and dynamic gestures. In the static gestures, the activation of particular region leads us to associated actions whereas HMM classifier is used to extract the dynamic gestures dependent upon the flow. The assistance-system supports the workers in manual working tasks in the context of assembling complex products. This paper is focused on the interaction of the user with this system and describes the work in progress with the initial results from an application scenario.	feature extraction;gesture recognition;hidden markov model;real-time locating system	Frerk Saxen;Omer Rashid Ahmed;Ayoub Al-Hamadi;Simon Adler;Alexa Kernchen;Rüdiger Mecke	2012	2012 12th International Conference on Intelligent Systems Design and Applications (ISDA)	10.1109/ISDA.2012.6416668	computer vision;contextual image classification;speech recognition;feature extraction;computer science;machine learning;gesture recognition;image segmentation;mobile computing;sketch recognition	Robotics	-37.990196313209104	-43.37590505286474	83675
0ac53683719cb3f631c5e7afb125cc5a71e5a62f	key frame preview techniques for video browsing	performance measure;real estate;digital library;statistical significance;interface design;storage capacity;user testing;digital video;video browsing;user satisfaction;object identification	Digitized video is an important format in digital libraries. Browsing video surrogates saves user time, storage capacity and avoids unnecessary downloading of large files. The study presented in this paper compared dynamic and static presentation techniques for key frames extracted from video documents. For this study key frames were automatically extracted and then a subset was manually selected to best represent the document. The three interface designs used were: 4 key frame static storyboard display, 12 key frame static storyboard display and 12 key frame dynamic slideshow display. The key frames in all displays were shown in temporal order. User performances on object, action identification, and gist comprehension and selection tasks were compared across treatments. Examination time and user satisfaction were also measured. Static storyboard displays proved to support object identification better, while other user performance measures showed no statistically significant differences. Using fewer key frames in static displays saved considerable amount of user time and screen real estate and user performance on gist comprehension and selection did not decrease when key frames were carefully selected to support queries. Implications for interface design and further research are discussed.	browsing;computer user satisfaction;digital library;digital video;download;gist;key frame;library (computing);performance;storyboard;surrogates	Anita Komlodi;Gary Marchionini	1998		10.1145/276675.276688	simulation;computer science;video tracking;multimedia;video processing;world wide web	HCI	-37.63153481133712	-50.137516309242386	83821
380c6e943917c6ea7b6985b98448e2bdb9dfab4c	designing gaze-supported multimodal interactions for the exploration of large image collections	user study;user centered design;dwell time;gaze control;multimodal interaction;eye tracking;interaction technique	While eye tracking is becoming more and more relevant as a promising input channel, diverse applications using gaze control in a more natural way are still rather limited. Though several researchers have indicated the particular high potential of gaze-based interaction for pointing tasks, often gaze-only approaches are investigated. However, time-consuming dwell-time activations limit this potential. To overcome this, we present a gaze-supported fisheye lens in combination with (1) a keyboard and (2) and a tilt-sensitive mobile multi-touch device. In a user-centered design approach, we elicited how users would use the aforementioned input combinations. Based on the received feedback we designed a prototype system for the interaction with a remote display using gaze and a touch-and-tilt device. This eliminates gaze dwell-time activations and the well-known Midas Touch problem (unintentionally issuing an action via gaze). A formative user study testing our prototype provided further insights into how well the elaborated gaze-supported interaction techniques were experienced by users.	eye tracking;fisheye;interaction technique;multi-touch;multimodal interaction;prototype;usability testing;user-centered design;whole earth 'lectronic link	Sophie Stellmach;Sebastian Stober;Andreas Nürnberger;Raimund Dachselt	2011		10.1145/1983302.1983303	computer vision;computer science;multimedia;communication	HCI	-46.66370854178853	-44.15404154048716	83860
d0a66de8f740e077c0c118e724b19718a194c81a	multi-use light engine - fast projection	engines hardware displays cameras page description languages light emitting diodes application software color optical polarization graphics;led illumination;frames per second;motion control;image processing equipment;binary image;display devices;image processing equipment display devices;real time;display device;on the fly;binary images;multi use light engine;led illumination multi use light engine display device binary images nvidia graphics card;nvidia graphics card	This demonstration shows a display device delivering thousands of binary frames per second. Binary images are delivered over a standard DVI interface and can be created on the fly in real time on an nVIDIA graphics Card. The images are synchronous to the input and triggers for LED illumination, camera shutters, or motion control can be created. This system can be configured into a projector of into other forms.	digital visual interface;display device;graphics;on the fly;video card;video projector	Ian McDowall	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383462	computer vision;binary image;computer hardware;computer science;display device;computer graphics (images)	Vision	-40.80494124994701	-39.10320797253597	83918
8049f961d6b3f6c44f8b48ea9035db4dd9ce2a2a	visual tracking of bare fingers for interactive surfaces	finger tracking with computer vision;direct manipulation;multi user;satisfiability;computer vision;visual tracking;user interaction;multi user multi hand interaction;large interactive surface	Visual tracking of bare fingers allows more direct manipulation of digital objects, multiple simultaneous users interacting with their two hands, and permits the interaction on large surfaces, using only commodity hardware. After presenting related work, we detail our implementation. Its design is based on our modeling of two classes of algorithms that are key to the tracker: Image Differencing Segmentation (IDS) and Fast Rejection Filters (FRF). We introduce a new chromatic distance for IDS and a FRF that is independent to finger rotation. The system runs at full frame rate (25 Hz) with an average total system latency of 80 ms, independently of the number of tracked fingers. When used in a controlled environment such as a meeting room, its robustness is satisfying for everyday use.	algorithm;commodity computing;direct manipulation interface;fast fourier transform;frequency response;gabor filter;image differencing;interaction;rejection sampling;video tracking	Julien Letessier;François Bérard	2004		10.1145/1029632.1029652	computer vision;finger tracking;simulation;eye tracking;computer science;operating system;satisfiability;computer graphics (images)	HCI	-42.22818087848573	-39.879875424701034	83953
68dbd3702f77bd35b87f1bae6c902ff350266d15	controlling a computer via facial aspect	moving image;twinkle approach;vision ordenador;estacion trabajo;image processing;face detection eyes communication system control workstations face recognition image recognition tracking working environment noise humans image processing;edge detection;station travail;sistema informatico;extraction forme;selection;menu;procesamiento imagen;computer system;nez;imagen movil;traitement image;image mobile;computer vision;deteccion contorno;detection contour;workstation;handicapped aids;user interfaces handicapped aids face recognition;face recognition;extraccion forma;vision ordinateur;chamfering transformation;face;systeme informatique;nariz;seleccion;communication;comunicacion;user interfaces;pattern extraction;nose;cara;workstation control computer workstation operation facial aspect face position facial gesturing disabilities	Control of a computer workstation via face position and facial gesturing would be an important advance for people with hand or body disabilities as well as for all users. Steps toward realization of such a system are reported here. A computer system has been developed to track the eyes and the nose of a subject and to compute the direction of the face. Face direction and movement is then used to control the cursor. Test results show that the resulting system is usable, although several improvements are needed.	computer;cursor (databases);workstation	Philippe Ballard;George C. Stockman	1995	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.370199	face;selection;computer vision;edge detection;workstation;image processing;computer science;artificial intelligence;user interface;computer graphics (images)	Graphics	-38.29237444704245	-42.364520799997095	84004
2f5f973290634d81f70480fae5f52d9af74400e7	osmfind: fast textual search on osm data -- on smartphones and servers	mobile;run length;text;search;low memory;openstreetmap;variable length;compression;encoding;framework;devices	While the visual rendering of map data is probably the first thing catching somebody's eyes e.g. in a route planning application, the actual usability is heavily dependent on the ability to efficiently search for textual information in the map data. In this paper we present a framework to allow for complex textual queries over the body of OpenStreetMap ([6]) data. Our framework supports searching for substrings, points-of-interest, and geo locations while also allowing all the usual set operations in a query. It is optimized for efficiency, providing near-instant query results even on low-capability hardware. The respective data structure can be stored locally (even on a smartphone) or accessed on a server via the internet. The versatility of our framework is demonstrated by respective implementations for the Android platform as well as a simple web-based geo location search engine.	android;data structure;openstreetmap;server (computing);smartphone;substring;usability;web application;web search engine	Daniel Bahrdt	2013		10.1145/2534190.2534200	computer science;data mining;database;world wide web	Mobile	-46.30273991112613	-40.87081845307372	84201
d0b79c726221ba42044afb320e48afdb00e7bb20	a development and evaluation platform for non-tactile power wheelchair controls	business process models;user centered design;human factors;worker guidance;assistive technology;j 3 computer applications life and medical sciences;design;h 5 2 information interfaces and presentation user interfaces input devices and strategies;experimentation;quality management	This paper presents an intelligent wheelchair designed to be used as a development and evaluation platform for alternative, non-tactile power wheelchair controls. The system is designed to be highly modular such that new human-computer interface devices and methods can be quickly integrated and evaluated as necessary. The current configuration provides full proportional steering and speed control outputs using a combination of voice commands, video-occulography (eye tracking), and a single point electrode based electroencephalography (EEG) brain-computer interface.	brain–computer interface;electroencephalography;eye tracking;human–computer interaction	Christopher McMurrough;Isura Ranatunga;Alexandros Papangelis;Dan O. Popa;Fillia Makedon	2013		10.1145/2504335.2504339	embedded system;design;quality management;simulation;human–computer interaction;operating system	Robotics	-40.36663667728033	-45.66990398294679	84268
11aaec273c1ba348d113ff799048fdb3ccccbf1e	user perception of next-track music recommendations		Many of today's music streaming websites and apps provide personalized next-track listening recommendations based on the user's current and past listening behavior. In the research literature, various algorithmic approaches to determine suitable next tracks can be found. However, almost all of them were evaluated in offline experiments using, for example, manually created playlists as a gold standard. In this work, we aim to check the external validity of insights that are obtained through such offline experiments on historical datasets. We conducted an online user study involving 277 subjects in which the participants evaluated the suitability of four different alternatives of continuing a given set of playlists. Our results indicate that manually created playlists can in fact represent a reasonable gold standard, an insight for which no evidence existed in the literature before. Furthermore, our work was able to confirm that considering playlist homogeneity aspects does not only lead to performance improvements in offline experiments -- as indicated by past research -- but also to a better quality perception by users. However, the observations also revealed that user studies of this type can be easily distorted by item familiarity biases, because the participants tend to evaluate continuation alternatives better when they know the track or the artist.	advanced stream redirector;algorithm;continuation;experiment;external validity;online and offline;personalization;recommender system;scientific literature;streaming media;usability testing	Iman Kamehkhosh;Dietmar Jannach	2017		10.1145/3079628.3079668	recommender system;homogeneity (statistics);information retrieval;active listening;multimedia;perception;continuation;external validity;computer science	HCI	-34.55235671043991	-50.936950151722826	84287
1630010a24aa190c362075d4080e8c9427005668	are we there yet? exploring with dynamic visualization	data visualization manipulator dynamics motion control geometry computer science education brightness iron proteins;color model;data visualisation;human factors;human factors data visualisation colour graphics user interfaces;color mapping dynamic visualization interactive graphics virtual object precomputed animation dynamic manipulation kinesthetic sense visual sense exploration experience dynamic display bivariate color mappings color models calico;interactive graphics;colour graphics;user interfaces	Interactive graphics practitioners have long understood that viewing a virtual object by controlling the viewpoint dynamically is more illuminating than viewing a still image or even a precomputed animation. Dynamic manipulation engages a viewer's kinesthetic sense in addition to his visual sense, adding an immediacy to the exploration experience. Finding the right way to represent data has been an active topic of much thought and discussion since the beginnings of visualization. To explore dynamic visualization's power, the author constructed a tool (Calico), for creating and manipulating bivariate color mappings using several different color models. Using Calico, she conducted two experimental studies of the effects of control over the color mapping on accuracy, confidence, and preference.	bivariate data;color mapping;graphics;precomputation;scientific visualization	Penny Rheingans	2002	IEEE Computer Graphics and Applications	10.1109/38.974511	computer vision;color model;information visualization;computer science;human factors and ergonomics;operating system;geometry;multimedia;user interface;data visualization;statistics;computer graphics (images);mechanical engineering	Visualization	-44.37237469040857	-38.98595625410895	84296
47f1e004729f2786132fbb105d862c07ff8796f2	sketched menus and iconic gestures, techniques designed in the context of shareable interfaces	co located collaboration;cscw;tabletop display;menu technique;interaction technique	Suppose a user is interacting with other persons around a digital tabletop or in front of a digital wall. A user wants to launch a new graphical component or an application in the part of the screen next to him. Traditional methods such as popup menus allow him to first open the application and afterwards let him move, resize and orient the component appropriately. Meanwhile, the component may cover some objects the others users are looking at or interacting with. How to avoid this disruption of other persons' activity?  This paper describes menu techniques for adding a new user's interface object on a shared device while preserving mutual awareness of the participants without disturbing them in their interaction. We present Sketched Menu, Abbreviated Sketched Menu and Iconic Gestures. These techniques let a user specify the shape, the size, the location and the orientation of the desired object before its creation.  Sketched menus and iconic gestures preserve the mutual awareness. These techniques allow both adaptation of a user to the current context -the actions of the other users and the spatial arrangement of the objects on the tabletop-and the others to be aware of the foreseen action and the related space claim.	denial-of-service attack;graphical user interface;interaction;norm (social);pop-up ad	Mohammed Belatar;François Coldefy	2010		10.1145/1936652.1936681	simulation;human–computer interaction;computer science;multimedia	HCI	-45.96840417571735	-38.667034481542935	84377
d40c2246a8ff6d2b51bd61ff6cb5ced689430bcc	flickboard: enabling trackpad interaction with automatic mode switching on a capacitive-sensing keyboard	touchpad;co located input devices;hand movement;interaction area;automatic mode;learning systems;capacitive sensing;artificial intelligence;keyboard;input devices;typewriter keyboards;user interfaces;soft keyboard	We present FlickBoard, which combines a trackpad and a keyboard into the same interaction area to reduce hand movement between separate keyboards and trackpads. It supports automatic input mode detection and switching (ie. trackpad vs keyboard mode) without explicit user input. We developed a prototype by embedding a 58x20 capacitive sensing grid into a soft keyboard cover, and uses machine learning to distinguish between moving a cursor (trackpad mode) and entering text (keyboard mode). Our prototype has a thin profile and can be placed over existing keyboards.	touchpad	Ying-Chao Tung;Ta Yang Cheng;Neng-Hao Yu;Mike Y. Chen	2014		10.1145/2658779.2658799	touchpad;footmouse;keyboard controller;embedded system;keyboard computer;real-time computing;computer hardware;computer science;operating system;capacitive sensing;user interface;input device	HCI	-43.908428134377274	-43.34330928295374	84744
0c95ea77efbaeaa371a32ab8dc1b6b686624eb98	teleoperation using google glass and ar, drone for structural inspection	unmanned aerial vehicle teleoperation google glass wearable device visualization uav control building structural inspection ar drone head position gestures image capture glass screen;ar drone teleoperation uav google glass;glass cameras google teleoperators inspection surveillance;telerobotics autonomous aerial vehicles buildings structures control engineering computing data visualisation gesture recognition helmet mounted displays image capture inspection	This paper proposes the use of a wearable device for visualization and control in association with an UAV applied to the structural inspection of buildings. More specifically, an AR. Drone is controlled through head positions and gestures performed by the operator wearing a Google Glass, and the images captured by the drone are visualized on Glass's screen. We discuss the problems that arise when such a solution is developed, along with the limitations that come from today's available technology and how to overcome them.	ar (unix);gesture recognition;glass;network packet;parrot ar.drone;software development kit;touchpad;unmanned aerial vehicle;wearable technology	João Marcelo X. N. Teixeira;Ronaldo Ferreira dos Anjos Filho;Matheus Santos;Veronica Teichrieb	2014	2014 XVI Symposium on Virtual and Augmented Reality	10.1109/SVR.2014.42	embedded system;computer vision;simulation;engineering	Visualization	-42.96108403354296	-42.702732342742614	84909
fb219f04595aaa4c8a623928e402bb97d2502d74	beyond the desktop: emerging technologies for supporting 3d collaborative teams	head mounted displays eyeglass displays;wireless network;emerging technology;see through displays;collaborative environment;head wom displays;mobile user;head mounted display	The emergence of several trends, including the increased availability of wireless networks, miniaturization of electronics and sensing technologies, and novel input and output devices, is creating a demand for integrated, fulltime displays for use across a wide range of applications, including collaborative environments. In this paper, we present and discuss emerging visualization methods we are developing particularly as they relate to deployable displays and displays worn on the body to support mobile users.	desktop computer;emergence;input/output;output device	Jannick P. Rolland;Ozan Cakmakci;Jeff Covelli;Cali M. Fidopiastis;Florian R Fournier;R. Martins;Felix G. Hamza-Lup;Denise M. Nicholson	2007	CoRR	10.1007/s12008-007-0027-z	simulation;human–computer interaction;computer science;engineering;artificial intelligence;optical head-mounted display;wireless network;multimedia;emerging technologies	HCI	-45.4097856101431	-38.220448955287836	85042
bb8f30c1b80eb8d5f9420598d64e3694f8ccffe2	pictac: a model for perceiving touch interaction through tagging context		A natural interface is one of three key technologies of Ambient Intelligence (AmI); one of its main objectives is to minimize the user’s interactive effort, which is the difficulty level that depends on the diversity and quantity of devices that surround people in existing environments. The worldwide penetration of mobile phones at present makes mobile phones excellent devices for delivering new services to users without requiring learning effort. An NFC-enabled mobile phone will allow a user to demand and obtain services by touching its different elements in a given smart environment. In this paper, we present a proposal in which we analyze the scope of touch interaction and develop a perceived touch interaction through tagging context (PICTAC) model.	ambient intelligence;mobile phone;near field communication;smart environment	Gabriel Chavira;José Bravo;Salvador W. Nava;Julio C. Rolon	2010	J. UCS	10.3217/jucs-016-12-1577	knowledge management;natural user interface;mobile phone;human–computer interaction;computer science;ambient intelligence;computer vision;smart environment;artificial intelligence	HCI	-48.09787793235777	-41.42373849518257	85217
93134086fabd8508b0199c4806ab227e6cc87e6d	3d virtual hand pointing with ems and vibration feedback	vibrations;three dimensional displays visualization thumb vibrations haptic interfaces environmental management;thumb;visualization;virtual reality force feedback user interfaces vibrations;three dimensional displays;user acceptance 3d virtual hand pointing vibration feedback ems 3d user interfaces visual feedback electrical muscle stimulation fitts task;haptic interfaces;environmental management;ems 3d pointing feedback vibration	Pointing is one of the most basic interaction methods for 3D user interfaces. Previous work has shown that visual feedback improves such actions. Here we investigate if electrical muscle stimulation (EMS) and vibration is beneficial for 3D virtual hand pointing. In our experiment we used a 3D version of a Fitts' task to compare visual feedback, EMS, vibration, with no feedback. The results demonstrate that both EMS and vibration provide reasonable addition to visual feedback. We also found good user acceptance for both technologies.	fitts's law;haptic technology;user interface	Max Pfeiffer;Wolfgang Stuerzlinger	2015	2015 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2015.7131735	control engineering;embedded system;simulation;engineering	Visualization	-42.86622467484515	-47.945130198584906	85320
8ec46b770580909d334062f4c32cdb2aeaa09fd8	how screen transitions influence touch and pointer interaction across angled display arrangements	pointing device;transition;interactive surfaces;dragging;display connection;user satisfaction;tabletops	Digital office environments often integrate multiple displays in a variety of arrangements. We investigated the combination of a horizontal and a directly connected vertical display, which together form a digital workspace. In particular, we were interested in the effect of the physical transition (bezel, edge or curve) on dragging. In a study participants performed dragging tasks across both display planes with direct touch as well as a pointing device. Contrary to our expectations, we found no significant effect on task completion time. Only regarding accuracy the curved transition performed better than edge and bezel. Interestingly, the subjective judgment did generally not match the objective results. These findings suggest that we need to rethink our understanding of display continuities in terms of usability as well as user satisfaction.	drag and drop;pointer (computer programming);pointing device;usability;workspace	Fabian Hennecke;Wolfgang Matzke;Andreas Butz	2012		10.1145/2207676.2207705	computer vision;simulation;computer science;transition;multimedia;pointing device	HCI	-45.33772813066207	-46.440921154098994	85526
f5ffd98d59e1ed88d101bf039dfea5d8025bb150	cognitive factors in the use of menus and trees: an experiment	human factors databases delay psychology computer interfaces telecommunication computing computer errors error analysis computer displays keyboards;teletext videotex computer interfaces database systems human factors;human factors;computer experiment;database systems;error rate;teletext videotex;computer interfaces	Absrrucr-Users without  prior  computer  experience  employed a menu-and-tree  interface  to  search a database relevant to telecommunications ervices.  Detailed  data were recorded  on  the latency of each response and whether an error was made. The depth, level, vertical position, number of options, and trial were found to significantly affect response latency. The level, content, and side of the screen on which the choice was displayed significantly affected error rate. The results of the experiment and additional issues in cognitive human factors are discussed.	bit error rate;experiment;human factors and ergonomics	Robert B. Allen	1983	IEEE Journal on Selected Areas in Communications	10.1109/JSAC.1983.1145925	simulation;computer experiment;human–computer interaction;word error rate;computer science;human factors and ergonomics;multimedia	HCI	-47.59910864827337	-46.10669019531813	85617
35c8710b6376fa1e1a65481005b8a3e781af778c	assessing comfortable 3d visual environment based on human factors			human factors and ergonomics	Makoto Yoshizawa	2012	Displays	10.1016/j.displa.2012.04.002	computer vision;artificial intelligence;engineering	HCI	-48.27156568881895	-46.36719113619261	85672
e81993db9b27fd0c0b05df4b0cbf14fe8fbfd431	unsupervised embrace pose recognition using k-means clustering		Embrace is an essential part of human-social interactions. It also gives positive health benefits as much as other touch gestures. However, in the recent years, studies of embrace recognition has been largely ignored when compared to recognition of touch gestures such as pat or rub. There are different kinds of embrace, with humans and pets, which can express different meanings. As embraces can be as important as touches, we are interested in what kind of embraces we can model, specially for human-robot interaction. In this paper, we investigate an unsupervised embrace pose recognition system based on a soft-stuffed robot platform. Our proposed method includes a hardware implementation of soft fabric-based capacitive touch sensors and a software algorithm comprising of k-means clustering based on locational features extracted from a sliding window. The result shows that our proposed method can model embrace patterns as different clusters. The method is capable of recognizing and clustering unseen data to a similar cluster's patterns, though there is a limitation when modeling two poses whose touches are similar but different in the alignment of the robot. In the next step, we plan to improve and test the proposed method in real-time environment, and make adjustments to our sensing system to cope with found limitations. If successful, the proposed method will be integrated with a touch gesture recognizer into a gesture recognition system for creating interactive and affective responses with stuffed-toy robot, which can become a medium for robot therapy or our own pets at home.	algorithm;cluster analysis;coat of arms;finite-state machine;gesture recognition;humans;human–robot interaction;k-means clustering;real-time computing;real-time transcription;robot;sensor;unsupervised learning	Nutnaree Kleawsirikul;Hironori Mitake;Shoichi Hasegawa	2017	2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)	10.1109/ROMAN.2017.8172407	artificial intelligence;gesture recognition;computer vision;feature extraction;cluster analysis;computer science;software;k-means clustering;affect (psychology);tactile sensor;gesture	Robotics	-36.9412636321609	-43.89010892314679	85689
038580ef558ae5f51071bad1cecb67e6f0e197ef	thinsight: versatile multi-touch sensing for thin form-factor displays	infrared sensing;mobile device;thin form factor displays;novel hardware;form factor;multi touch;zero forcing;infrared;tangible interaction;physical objects	ThinSight is a novel optical sensing system, fully integrated into a thin form factor display, capable of detecting multi-ple fingers placed on or near the display surface. We describe this new hardware in detail, and demonstrate how it can be embedded behind a regular LCD, allowing sensing without degradation of display capability. With our approach, fingertips and hands are clearly identifiable through the display. The approach of optical sensing also opens up the exciting possibility for detecting other physical objects and visual markers through the display, and some initial experiments are described. We also discuss other novel capabilities of our system: interaction at a distance using IR pointing devices, and IR-based communication with other electronic devices through the display. A major advantage of ThinSight over existing camera and projector based optical systems is its compact, thin form-factor making such systems even more deployable. We therefore envisage using ThinSight to capture rich sensor data through the display which can be processed using computer vision techniques to enable both multi-touch and tangible interaction.	computer vision;elegant degradation;embedded system;experiment;form factor (design);image sensor;liquid-crystal display;multi-touch;tangible user interface;video projector	Steve Hodges;Shahram Izadi;Alex Butler;Alban Rrustemi;William Buxton	2007		10.1145/1294211.1294258	zero forcing equalizer;computer vision;simulation;infrared;form factor;mobile device;computer graphics (images)	HCI	-43.16588559587866	-41.09569881269454	85729
9a923351ce9c62671d06f8096735df97eea5c7a1	programming industrial robots using advanced input-output devices: test-case example using a cad package and a digital pen based on the anoto technology	index terms — cad;human machine interfaces;robot programming.;industrial robots;human machine interface;input output;human robot interaction;indexing terms;input device	Interaction with robot systems for specification of manufacturing tasks and motions needs to be simple, to enable wide-spread use of robots in SMEs. In the best case, existing practices from manual work could be used, to smoothly let current employees start using robot technology as a natural part of their work. Our aim is to simplify the robot programming task by allowing the user to simply make technical drawings on a sheet of paper. Craftsman use paper and raw sketches for several situations; to share ideas, to get a better imagination or to remember the customer situation. Currently these sketches have either to be interpreted by the worker when producing the final product by hand, or transferred into CAD file using an according tool. The former means that no automation is included, the latter means extra work and much experience in using the CAD tool. Our approach is to use the digital pen and paper from Anoto as input devices for SME robotic tasks, thereby creating simpler and more user friendly alternatives for programming, parameterization and commanding actions. To this end, the basic technology has been investigated and fully working prototypes have been developed to explore the possibilities and limitation in the context of typical SME applications. Based on the encouraging experimental results, we believe that drawings on digital paper will, among other means of human-robot interaction, play an important role in manufacturing SMEs in the future. Index Terms — CAD, Human machine interfaces, Industrial Robots, Robot programming.	best, worst and average case;computer-aided design;digital paper;digital pen;human–robot interaction;industrial robot;input device;output device;smoothing;technical drawing;usability	J. Norberto Pires;Tiago Godinho;Klas Nilsson;Mathias Haage;Christian Meyer	2007	iJOE		human–machine interface;human–robot interaction;input/output;embedded system;computer vision;simulation;index term;computer science;engineering;electrical engineering;artificial intelligence;operating system;machine learning;world wide web;engineering drawing;input device	Robotics	-44.10908967397422	-39.0276099845913	85881
cf20d7f10ad897b2229e30142019ca713af82d1b	an evaluation of graphical context when the graphics are outside of the task area	augmented environments;sensor systems;human computer interaction;h 5 1 information interfaces and presentation multimedia information systems evaluation methodology;sensors;h 5 1 information interfaces and presentation multimedia information systems artificial;ar;virtual reality;maintenance engineering;psychology;multimedia information system;augmented;lego block placement task;heads up display;graphical context;evaluation methodology;field of view;and virtual realities;registration error minimization;human computer interaction h 5 1 information interfaces and presentation multimedia information systems artificial augmented and virtual realities h 5 1 information interfaces and presentation multimedia information systems evaluation methodology augmented reality communicative intent augmented environments;head up display;communicative intent;augmented reality;information interfaces and presentation;human computer interaction graphical context augmented reality ar registration error minimization lego block placement task heads up display;graphics augmented reality sensors maintenance engineering psychology sensor systems cameras;cameras;graphics	An ongoing research problem in Augmented Reality (AR) is to improve tracking and display technology in order to minimize registration errors. However, perfect registration is not always necessary for users to understand the intent of an augmentation. This paper describes the results of an experiment to evaluate the effects of graphical context in a Lego block placement task when the graphics are located outside of the task area. Four conditions were compared: fully registered AR; non-registered AR; a heads-up display (HUD) with the graphics always visible in the field of view; and a HUD with the graphics not always visible in the field of view. The results of this experiment indicated that registered AR outperforms both non-registered AR and graphics displayed on a HUD. The results also indicated that non-registered AR does not offer any significant performance advantages over a HUD, but is rated as less intrusive and can keep non-registered graphics from cluttering the task space.	augmented reality;display device;gvu center at georgia tech;graphical user interface;graphics;head-mounted display;head-up display;ibm notes;modeling language;virtual world;wap identity module;workspace	Cindy M. Robertson;Blair MacIntyre;Bruce N. Walker	2008	2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2008.4637328	maintenance engineering;computer vision;augmented reality;head-up display;human–computer interaction;computer science;artificial intelligence;operating system;virtual reality;multimedia;computer graphics (images)	Visualization	-43.411666863986206	-39.12649270717397	85950
2ea8e5b71ecc193f88747e8b85791459bcf0fd89	building a self-learning eye gaze model from user interaction data	supervised learning;implicit modeling;data validation;gaze estimation	Most eye gaze estimation systems rely on explicit calibration, which is inconvenient to the user, limits the amount of possible training data and consequently the performance. Since there is likely a strong correlation between gaze and interaction cues, such as cursor and caret locations, a supervised learning algorithm can learn the complex mapping between gaze features and the gaze point by training on incremental data collected implicitly from normal computer interactions. We develop a set of robust geometric gaze features and a corresponding data validation mechanism that identifies good training data from noisy interaction-informed data collected in real-use scenarios. Based on a study of gaze movement patterns, we apply behavior-informed validation to extract gaze features that correspond with the interaction cue, and data-driven validation provides another level of crosschecking using previous good data. Experimental evaluation shows that the proposed method achieves an average error of 4.06º, and demonstrates the effectiveness of the proposed gaze estimation method and corresponding validation mechanism.	algorithm;caret;cursor (databases);data validation;interaction;supervised learning	Michael Xuelin Huang;Tiffany C. K. Kwok;Grace Ngai;Hong Va Leong;Stephen Chi-fai Chan	2014		10.1145/2647868.2655031	computer vision;computer science;machine learning;data validation;supervised learning	Vision	-37.194211692005396	-47.60392021131076	85958
79b705a684d6a95fb954b75bca85f8e88b858f5b	if reality bites, bite back virtually: simulating perfection in augmented reality tracking	cognitive map;pedestrian navigation;user interface;hit;technology;lab;virtual reality;immersion;hitlab;interface;human;nz;augmented reality	Augmented Reality (AR) on smart phones can be used to overlay virtual tags in the real world to show points of interest that people may want to visit. However, field tests have failed to validate the belief that AR-based tools would outperform map-based tools for such pedestrian navigation tasks. Assuming this is due to inaccuracies in consumer GPS tracking used in handheld AR, we created a simulated environment that provided perfect tracking for AR and conducted experiments based on real world navigation studies. We measured time-on-task performance for guided traversals on both desktop and head-mounted display systems and found that accurate tracking did validate the superior performance of AR-based navigation tools. We also measured performance for unguided recall traversals of previously traversed paths in order to investigate into how navigation tools impact upon route memory.	augmented reality;desktop computer;experiment;gps tracking unit;global positioning system;handheld game console;head-mounted display;point of interest;smartphone;virtual reality	James Wen;William S. Helton;Mark Billinghurst	2013		10.1145/2542242.2542246	augmented reality;computer-mediated reality;simulation;human–computer interaction;cognitive map;computer science;operating system;interface;virtual reality;multimedia;programming language;user interface;immersion;computer graphics (images);technology	HCI	-45.108156494088384	-45.84858140846878	85961
31400ababb47f5a8c5f91c3824dca49fae1140fb	a framework for comparing task performance in real and virtual scenes	task performance;real and virtual environments;perforation;attention;virtual environments;conference paper;rapid prototyping;visual perception;eye tracking;virtual environment	In this paper, we describe a framework for comparing task performance in real and virtual environments. Realistic graphics, rear projection, haptics and rapid prototyping are used to match the virtual scene to the real scene. We describe some preliminary placement tasks which were evaluated using eye-tracking and discuss our future plans for this framework.	eye tracking;graphics;haptic technology;rapid prototyping;virtual reality	Sarah Howlett;Richard Lee;Carol O'Sullivan	2005		10.1145/1080402.1080423	psychology;computer vision;neuroscience;simulation;attention;eye tracking;visual perception;computer science;virtual machine;computer graphics (images)	Visualization	-44.11233743777725	-47.62841803826757	85990
7f606485d3d72427bb6199cc212f5a407da3820f	did jan van eyck build the first photocopier in 1432?	mirrors;oak;opacity;projection systems;projection devices;optical components	"""Recently it has been claimed that some early Renaissance painters used concave mirrors to project real inverted images onto their supports (paper, canvas, oak panel, ...) which they then traced or painted over, and that this was an important source of the increase in realism in European painting around 1420. Key exhibits adduced as evidence in support of this bold theory are a pair of portraits by Jan van Eyck of Cardinal Niccolo Albergati(&?) - a silverpoint study of 1431 and a larger oil of 1432. The contours in these two works bear striking resemblance in form (after being appropriately scaled) and at least one distinctive """"relative shift"""" - evidence that has led proponents of the projection theory to claim that the oil was copied by means of an epidiascope or primitive opaque projector, the shift due to an accidental """"bump"""" during the copying process. We find several difficulties with this optical explanation: there are at least two relative shifts (one horizontal and one vertical), the latter being somewhat unlikely given the putative projection equipment and setup; these shifts are in the ratio of distances of nearly 1:2, a ratio that has no natural role in the projection explanation; any accidental """"bump"""" would surely have been noticed by van Eyck, and if so desired, corrected by him; recent analysis shows physical evidence (tiny pinpricks presumably from mechanical compass) consistent with mechanical transfer that has no role in the optical explanation; and several other points. The fidelity of the copy as well as the direction and relative magnitudes of these shifts are, however, consistent with the use of a familiar grid construction and with mechanical transfer using drawing compass and ruler or Reductionszirkel . Further, there are prominent vertical Bruchkanten (fold or fraction) lines on the grounded paper in the silverpoint study whose orientation and separation have no natural role in an optical theory, but have a plausible role in other explanations. Our rebuttal to the projection theory for these works is supposed by considertaion of hte lack of documentary evidence from both artists and scientists, of surviving optical devices, and of the artistic goals and established painting praxis in the early Renaissance.© (2003) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only."""	photocopier	David G. Stork	2004		10.1117/12.538841	visual arts;engineering;optics;cartography	Theory	-41.78534110898109	-50.51838956588879	86227
00e7bb0750fd1648174daad989d1d7070547381b	tactile feedback as a sensory subtraction technique in haptics for needle insertion	haptic device;tactile feedback;indexation;visual feedback;virtual environment;sensory substitution	A sensory substitution technique is presented in which the kinesthetic and tactile feedback are substituted by tactile feedback only provided by two wearable devices able to apply forces to the index finger and thumb holding a handle during a needle insertion task. The force pattern fed back to the user while using the tactile device is similar, in terms of intensity and area of application, to that perceived while interacting with a haptic device providing both tactile and kinesthetic feedback and it can be thought as a subtraction between the complete haptic and kinesthetic feedback. For this reason we refer to this approach as sensory subtraction instead of sensory substitution. A needle insertion scenario is considered. The haptic device is connected to a virtual environment simulating a needle insertion task. Experiments show that the perception of inserting a needle using the tactile feedback only is nearly indistinguishable from the one felt by the user using both tactile and kinesthetic feedback. As most of the sensory substitution approaches, also this does not suffer from typical stability issues of teleoperation systems due for instance to delays. Moreover experiments show that the proposed sensory substitution technique outperforms sensory substitution with more conventional visual feedback.	experiment;haptic technology;interaction;sensory substitution;simulation;virtual reality;wearable technology	Domenico Prattichizzo;Claudio Pacchierotti;Giulio Rosati	2011	CoRR		computer vision;computer science;multimedia;communication	Robotics	-45.06356315906021	-49.16077223589238	86364
e62f009700d6bfe578ce043558f4e956cb9bc608	a survey and classification of 3d pointing techniques	keyboards;mice;3d pointing technique classification;three dimensional user interfaces;user interface;fitts law;three dimensional pointing;virtual reality;graphical user interface;virtual reality graphical user interfaces;three dimensional;virtual enhancement;3d line based cursor;navigation;engineering and technology;graphical user interfaces;teknik och teknologier;pattern classification 3d pointing technique classification 3d pointer based cursor 3d line based cursor fitts law virtual enhancement target acquisition graphical user interface;pattern classification;predictive models;fitts law three dimensional user interfaces three dimensional cursor three dimensional pointing classification target acquisition;standardization mice user interfaces predictive models graphical user interfaces keyboards navigation;classification target acquisition;user interfaces;3d pointer based cursor;three dimensional cursor;standardization;target acquisition	This paper introduces a survey and a classification of 3D pointing techniques. The survey presents a chronological view on the study of 3D pointing techniques. The classification is based on a proposed definition of 3D cursor. The paper shows that existing 3D pointing techniques can be either 3D pointer-based cursor or 3D line-based cursor. Based on recent results of 3D Fitts' law study and the definition of two types of 3D cursor, the paper discusses different virtual enhancements for improving existing 3D pointing techniques and for creating and evaluating new 3D pointing techniques which focus on decreasing the average target acquisition time.	cursor (databases);fitts's law;pointer (computer programming)	Nguyen Thong Dang	2007	2007 IEEE International Conference on Research, Innovation and Vision for the Future	10.1109/RIVF.2007.369138	computer vision;simulation;human–computer interaction;computer science	HCI	-44.10360925426649	-46.27465991985259	86394
97e6f4774f371de2e3aff432d34539bfec847ef7	investigating pre-touch for sound generation on multi-touch surfaces using blob area detection		We present a method to assist pre-touch on multi-touch surfaces using features extracted from the fingertips of the user's hands. Pixel area a feature obtained from the contour of a fingertip is computed and tracked using commodity cameras to obtain the velocity of approaching fingers. We target sound volume control for digital multi-touch instruments an area where pre-touch is demonstrated with our approach to approximating velocity of finger taps. We explore other areas of application with velocity and how it can help anticipate user actions in mobile touch displays. In order to evaluate the effectiveness of the proposed system, a system prototype was presented to users. We confirmed the satisfactory results by performing the recognition rate experiment for measuring the blob size according to the touch speed.	mobile device;multi-touch;pixel;prototype;touchscreen;velocity (software development);vii	James Park;Seungho Chae;Yoonsik Yang;Tack-Don Han	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122752	computer vision;machine learning;pixel;loudness;artificial intelligence;multi-touch;computer science	Robotics	-45.4736233700095	-42.63960405875307	86556
3ae571c91071c0eb00f900d1e8dfd7260c15494b	let me introduce myself: i am care-o-bot 4, a gentleman robot	service robot;talk	In this paper, we present the latest step in the evolution of the Care-O-bot: A general purpose service robot that assists users in various tasks where human-like behavior and interfaces are desired. We argue that technology, design and user experience have to match to create a robot that will blend not only visually, but also emotionally into the scenarios it will be used for. We describe the requirement priorities, design decisions and technical feasibility with which Care-O-bot 4 is able to fulfill tasks in human environments and additionally evokes positive emotions and thus furthering and enhancing human-robot interaction (HRI).	care-o-bot;human–robot interaction;service robot;user experience	Ralf Kittmann;Tim Fröhlich;Johannes Schäfer;Ulrich Reiser;Florian Weisshardt;Andreas Haug	2015			simulation;engineering;artificial intelligence	Robotics	-47.09561681164609	-41.72604688558557	86744
879856bd8208d516866352ab1066c89171de2f95	describing and assessing image descriptions for visually impaired web users with idat		People with visual impairments, particularly blind people face alot of difficulties browsing the web with assistive technologies such as screen readers, when websites do not conform to accessibility standards and are thus inaccessible. HTML is the basic language for website design but its ALT attribute on the IMG element does not adequately capture comprehensive image semantics and description in a way that can be accurately interpreted by screen readers, hence blind people do not usually get the complete description of the image. Most of the problems however arise from web designers and developers not including a description of an image or not comprehensively describing these images to people with visual impairments. In this paper, we propose the use of the Image Description Assessment Tool (IDAT), a Java-based tool containing some proposed heuristics for assessing how well an image description matches the real content of the image on the web. The tool also contains a speech interface which can enable a visually impaired individual to listen to the description of an image that has been uploaded unto the system.	accessibility;alt attribute;assistive technology;computer science;html element;heuristic (computer science);img;java;web accessibility;web design	Julius Tanyu Nganji;Mike Brayshaw;Brian Tompsett	2011		10.1007/978-3-642-31603-6_3	information retrieval;rdf;web accessibility;java;semantics;heuristics;upload;alt attribute;computer science;img	HCI	-35.53601037175291	-49.65093277695744	87146
373bcfba4997579046cd23e3207b07b8038fec60	comprehensive calibration and registration procedures for augmented reality	tracking system;virtual environment;augmented reality;virtual worlds;immersive virtual environment;head mounted display	Augmented Reality best described as adding computer-generated virtual content to the real environment – needs more adjustments to work properly than immersive virtual environments. To be perceived as an augmentation of reality, the virtual environment has to be properly aligned to the real world. This registration process has to be done at least once for every hardware set-up, but may have to be repeated in part or completely for each user, prop or device to be included both in the real and the virtual world. In this paper, we propose a comprehensive process for registration and calibration tasks necessary to implement correct augmentation. This includes procedures for calibrating projective and head-mounted displays, tracking systems, tracked input devices and props. Our method unifies the necessary tasks of world-toaugmentation alignment, display calibration and registration of tracked and static props in one, interactive set-up process, which can easily be conducted by the untrained user.	augmented reality;computer-generated holography;head-mounted display;input device;tracking system;virtual reality;virtual world	Anton L. Fuhrmann;Rainer Splechtna;Jan Prikryl	2001		10.2312/EGVE/EGVE01/219-228	computer vision;augmented reality;computer-mediated reality;simulation;computer science;computer graphics (images)	Visualization	-41.65529308770815	-39.09047621946342	87244
2ca0cfa483ba9b9aefa8819e11b3f0e19ca3460b	a glimpse of 3d acoustics for immersive communication	matching pursuit algorithms;speaker pattern optimization 3d sound field reproduction immersive communication loudspeaker patterns acoustic directivity higher order loudspeakers;acoustics;ear;loudspeakers;three dimensional displays;microphone arrays acoustic communication telecommunication audio signals loudspeakers;microphone arrays;sound field reproduction immersive communication 3d acoustics 3d visual communications mode acoustic technology audio layer audio signals microphones array video based talker location system remote listener location 3d audio listener experiences reverberant environment prescribed directivity loudspeakers;loudspeakers microphone arrays three dimensional displays acoustics ear matching pursuit algorithms	Future multimedia communications systems are expected to be increasingly immersive, driven by new technology for improving 3D visual and acoustic experiences. In a classical communications sense this seems a step backwards because a much larger bandwidth is required, but the return is a richer communications mode. Acoustic technology for the audio layer is presented here, although it is not entirely inseparable from the visual aspects. The talkers' audio signals are sensed by an array of microphones assisted by a video-based talker-location system. The data is transmitted to the remote listener's location, where the 3D audio is reproduced around the listeners' ears by an array of speakers in the room. The idea is that the listener experiences being in the same room as the talkers, but achieving this convincingly has raised challenges which require new research. As a step in this direction, we use simulation in a reverberant environment to investigate the improvement from using prescribed-directivity loudspeakers for the sound field reproduction. The strong performance improvement compared to using an array of conventional omni-directional loudspeakers contributes motivation for the development of such directional loudspeakers.	acoustic cryptanalysis;acoustic fingerprint;bandwidth (signal processing);computer monitor;decibel;experience;immersion (virtual reality);loudspeaker;microphone;peripheral;separable polynomial;simulation;software deployment	Hanieh Khalilian;Ivan V. Bajic;Rodney G. Vaughan	2016	2016 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)	10.1109/CCECE.2016.7726615	loudspeaker;speech recognition;engineering;directional sound;audio signal;sound reinforcement system	Visualization	-36.58429034790242	-45.722100636963695	87563
edd1be47185f26693e333924faad070c462b23f8	user centered framework for intuitive robot programming	solid modeling collision avoidance grasping service robots planning programming;grasping;intuitive robot programming;robot technologies;service robots;user centered design;human robot interaction;robotic workers;robotic system components;user centered framework;quality requirement;robotic system components user centered framework intuitive robot programming robotic workers autonomous planning user centered design human robot interaction perception industrial interface industrial robot task knowledge robot technologies;industrial robots;solid modeling;industrial interface;task knowledge;planning;control engineering computing;collision avoidance;perception;robot programming control engineering computing human robot interaction industrial robots;programming;autonomous planning;robot programming;industrial robot	In this paper, we present a prototypical system as platform for a new generation of robotic workers. Dealing with customization, individualization and finally customer driven batch sizes of products of highest quality requires robot technologies that are able to compensate uncertainties inherent in processes and environment. Furthermore, it requires technologies allowing intuitive access to robotic system components without being a robot expert. The main contribution of the paper is a novel framework for robotic workers combining perception, autonomous planning and a user-centered design of human-robot interaction. We also present an industrial interface that allows the operator to immerse into complex industrial robot processes and to transfer task knowledge and strategies for commanding robots on a high-level. The framework and its embedded methods have been implemented and evaluated in a bin picking scenario for small batch sizes.	autonomous robot;embedded system;high- and low-level;human–robot interaction;industrial robot;user-centered design	Andreas Pichler;Martin Ankerl	2010	2010 IEEE International Workshop on Robotic and Sensors Environments	10.1109/ROSE.2010.5675249	control engineering;computer vision;simulation;engineering	Robotics	-35.89318171538857	-40.477808447013025	87721
b4eb9f818fd67d964d4705f1e192e4025d873fef	fvision: interactive glasses-free tabletop 3d images floated by conical screen and modular projector arrays	multi user;light field reproduction;binocular parallax;mixed reality;glasses free 3d display;tabletop display	fVisiOn floats glasses-free 3D images on an empty, flat tabletop surface. A combination of a conical screen and circularly arranged projectors underneath the table reproduces a 3D shape light field. Multiple viewers can share the experience and enjoy full-color, 5-cm-tall characters dancing on the table.	autostereoscopy;light field;movie projector;single-precision floating-point format;video projector	Shunsuke Yoshida	2015		10.1145/2818466.2818472	computer vision;computer science;mixed reality;multimedia;computer graphics (images)	HCI	-41.48826048793881	-38.67695992633068	87811
f274e0f410a0e94a8bb219d78e35c591d8d3ae12	comparison of three implementations of headturn: a multimodal interaction technique with gaze and head turns	gaze interaction;haptic feedback;head moves	The best way to construct user interfaces for smart glasses is not yet known. We investigated the use of eye tracking in this context in two experiments. The eye and head movements were combined so that one can select the object to interact by looking at it and then change a setting in that object by turning the head horizontally. We compared three different techniques for mapping the head turn to scrolling a list of numbers with and without haptic feedback. We found that the haptic feedback had no noticeable effect in objective metrics, but it sometimes improved user experience. Direct mapping of head orientation to list position is fast and easy to understand, but the signal-to-noise ratio of eye and head position measurement limits the possible range. The technique with constant rate of change after crossing the head angle threshold was simple and functional, but slow when the rate of change is adjusted to suit beginners. Finally the rate of change dependent on the head angle tends to lead to fairly long task completion times, although in theory it offers a good combination of speed and accuracy.	experiment;eye tracking;haptic technology;interaction technique;multimodal interaction;scrolling;signal-to-noise ratio;smartglasses;user experience;user interface	Oleg Spakov;Poika Isokoski;Jari Kangas;Jussi Rantala;Deepak Akkil;Roope Raisamo	2016		10.1145/2993148.2993153	computer vision;simulation;computer science;artificial intelligence;haptic technology	HCI	-45.61306720860231	-46.25274807948361	88067
19ff7900c86d7395b96fa69df64ece710f8b6632	coordination of tilt and touch in one- and two-handed use	tilt;touchscreen;touch;gestures;mobile devices	Our goal is to enhance navigation in mobile interfaces with quick command gestures that do not make use of explicit mode-switching actions. TilTouch gestures extend the vocabulary of navigation interfaces by combining motion tilt with directional touch. We consider sixteen directional TilTouch gestures that rely on tilt and touch movements along the four main compass directions. An experiment explores their effectiveness for both one-handed and two-handed use. Results identify the best combinations of TilTouch gestures in terms of performance, motor coordination, and user preferences.	user (computing);vocabulary	Theophanis Tsandilas;Caroline Appert;Anastasia Bezerianos;David Bonnet	2014		10.1145/2556288.2557088	computer vision;simulation;computer science;mobile device;natural user interface;gesture	HCI	-46.14370025153687	-44.02391958314988	88262
60541652fb364293ec51754f8bfca152559bde28	movietweeters: an interactive interface to improve recommendation novelty		This paper introduces and evaluates a novel interface, MovieTweeters. It is a movie recommendation system which incorporates social information with a traditional recommendation algorithm to generate recommendations for users. Few previous studies have investigated the influence of using social information in interactive interfaces to improve the novelty of recommendations. To address this gap, we investigate whether social information can be incorporated effectively into an interactive interface to improve recommendation novelty and user satisfaction. Our initial results suggest that such an interactive interface does indeed help users discover more novel items. Also, we observed users who perceived that they discovered more novel and diverse items reported increased levels of user satisfaction. Surprisingly, we observed that even though we successfully were able to increase the system diversity of the recommendations, it had a negative correlation with users perception of novelty and diversity of the items highlighting the importance of improved user-centered approaches.	algorithm;computer user satisfaction;multi-master replication;online and offline;recommender system;user interface;user-centered design	Ishan Ghanmode;Nava Tintarev	2018				HCI	-35.87188320018794	-51.25735934606782	88290
9af3cae422f46aa6e92bc387496290c673855e97	poster: dynamic adaptation of 3d selection techniques for suitability across diverse scenarios	total attempts per scenario variable 3d selection techniques scope raycast bendcast hook object density object velocity completion time variable;human computer interaction;computer graphics;three dimensional displays visualization educational institutions user interfaces time measurement virtual environments velocity measurement;dense and dynamic environments interaction techniques 3d object selection;interactive systems computer graphics human computer interaction;interactive systems	We performed a user study that measured the effectiveness of our new 3D selection technique, Scope, which dynamically adapts to the environment by altering its activation area and visual appearance with relation to cursor velocity. Users tested our new technique against existing techniques Raycast, Bendcast, and Hook across a variety of different 3D scenarios which featured three different levels of object density and three different levels of object velocity. Our two dependent variables were completion time and total attempts per scenario. Users also completed a post-questionnaire which yielded qualitative insights on their experience. Our study shows that Bendcast, Scope, and Hook all performed similarly across all scenarios, yet were all significantly faster and less error-prone than Raycast. Despite this similar performance, users strongly favored Scope over the other three techniques, and over the second most preferred technique nearly two to one.	cognitive dimensions of notations;cursor (databases);usability testing;velocity (software development)	Jeffrey Cashion;Joseph J. LaViola	2014	2014 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2014.6798872	simulation;human–computer interaction;computer science;multimedia	HCI	-44.67474224297028	-47.24114183180495	88810
35ee678ba415701ffe538ec250a6e0412573fa72	tangible interaction in the dentist office		This paper presents the design efforts involved in making a system for supporting haptic communication between dentist and patient during dental treatment. We describe Grasp Live, a haptic interaction technology consisting of a tangible stone-like object connected to a vibro-tactile feedback device.	apple watch;haptic technology;tangible user interface	Frode Guribye;Tor Gjosater	2018		10.1145/3173225.3173287	computer science;human–computer interaction;multimedia;haptic communication;dental care;haptic technology;dentistry;grasp	HCI	-45.61346296026351	-42.2707995331227	88820
74d108f796cd59b035c5ca2baf6f75ec17798e0b	perspective cursor: perspective-based interaction for multi-display environments	direct manipulation;multi display interaction techniques;laser pointing;multi monitor environments;space use;direct manipulation interfaces;interaction technique	Multi-display environments and smart meeting rooms are now becoming more common. These environments build a shared display space from variety of devices: tablets, projected surfaces, tabletops, and traditional monitors. Since the different display surfaces are usually not organized in a single plane, traditional schemes for stitching the displays together can cause problems for interaction. However, there is a more natural way to compose display space -- using perspective. In this paper, we develop interaction techniques for multi-display environments that are based on the user's perspective on the room. We designed the Perspective Cursor, a mapping of cursor to display space that appears natural and logical from wherever the user is located. We conducted an experiment to compare two perspective-based techniques, the Perspective Cursor and a beam-based technique, with traditional stitched displays. We found that both perspective techniques were significantly faster for targeting tasks than the traditional technique, and that Perspective Cursor was the most preferred method. Our results show that integrating perspective into the design of multi-display environments can substantially improve performance.	cursor (databases);image stitching;interaction technique	Miguel A. Nacenta;Samer Sallam;Bernard Champoux;Sriram Subramanian;Carl Gutwin	2006		10.1145/1124772.1124817	computer vision;simulation;human–computer interaction;computer science;interaction technique	HCI	-43.90614180441936	-39.52233508758497	88858
8a99f59aa9822ecd1fc4a46a57cb252ebec2d8d8	ubiquitous memories: a memory externalization system using physical objects	human memory;rfid tag;memory systems;radio frequency identification	In this paper we propose an object-triggered human memory augmentation system named “Ubiquitous Memories” that enables a user to directly associate his/her experience data with physical objects by using a “touching” operation. A user conceptually encloses his/her experiences gathered through sense organs into physical objects by simply touching an object. The user can also disclose and re-experience for himself/herself the experiences accumulated in an object by the same operation. We implemented a prototype system composed basically of a radio frequency identification (RFID) device. Physical objects are also attached to RFID tags. We conducted two experiments. The first experiment confirms a succession of the “encoding specificity principle,” which is well known in the research field of psychology, to the Ubiquitous Memories system. The second experiment aims at a clarification of the system’s characteristics by comparing the system with other memory externalization strategies. The results show the Ubiquitous Memories system is effective for supporting memorization and recollection of contextual events.	experience;experiment;prototype;radio frequency;radio-frequency identification;sensitivity and specificity;succession	Tatsuyuki Kawamura;Tomohiro Fukuhara;Hideaki Takeda;Yasuyuki Kono;Masatsugu Kidode	2006	Personal and Ubiquitous Computing	10.1007/s00779-006-0085-4	radio-frequency identification;computer science;artificial intelligence	HCI	-43.6789790045892	-44.459364538419685	88868
a3518cf84235afc17629b48f151d1a6446ca027b	an analysis of human motion for control of a wearable power assist system	power assisting device;wearable robotics;electromiogram emg		wearable computer	Satoshi Kawai;Keitaro Naruse;Hiroshi Yokoi;Yukinori Kakazu	2004	JRM	10.20965/jrm.2004.p0237	control engineering;embedded system;simulation	Robotics	-40.70304694997286	-45.06485306388301	89005
49792d54dddd3ec7aeea3abcb85449cbf87093fa	head movement based interaction in mobility		ABSTRACTThe touchless techniques in human computer interaction (HCI) can effectively expand communication capabilities. In the paper we present the innovative touchless computer control method based on head movement analysis. The aim of our work was to replace the standard mouse with the movements of the user’s head. In contrast to the known solutions, our proposition does not require image recording of the user’s head and complex image analysis. The analysis of position in our solution is made using the camera worn by the user on the head. A project of such a solution has been developed and the research of it has been carried out. It has been shown that in this way it is possible to effectively move the screen cursor to the position which is identified by the user’s face orientation. Additionally, in this solution, the eye image analysis has been performed. Interpretation of blinking allowed executing system commands. Using the built prototype the experiments have been carried out in a group of 30 people...		Dariusz Sawicki;Piotr Kowalczyk	2018	Int. J. Hum. Comput. Interaction	10.1080/10447318.2017.1392078	human–computer interaction;cursor (user interface);computer science;computer vision;artificial intelligence	HCI	-44.542555603140876	-43.44453007108504	89041
8cf52709020b206ae97e285e0a68d69749c2f054	generic system for human-computer gesture interaction	human computer interaction;support vector machines computer vision gesture recognition human computer interaction image segmentation learning artificial intelligence;generic systems;machine learning human computer interaction gesture interfaces generic systems computer vision;computer vision;machine learning;human machine applications human computer gesture interaction hand gestures human communication human computer interaction vision based hand gesture recognition electronic devices generic system architecture computer vision machine learning preprocessing module hand segmentation module static gesture interface module dynamic gesture interface module vision based interaction systems hand posture recognition svm model dynamic gestures hmm model;gesture interfaces;hidden markov models gesture recognition real time systems support vector machines robots accuracy mathematical model	Hand gestures are a powerful way for human communication, with lots of potential applications in the area of human computer interaction. Vision-based hand gesture recognition techniques have many proven advantages compared with traditional devices, giving users a simpler and more natural way to communicate with electronic devices. This work proposes a generic system architecture based in computer vision and machine learning, able to be used with any interface for human-computer interaction. The proposed solution is mainly composed of three modules: a pre-processing and hand segmentation module, a static gesture interface module and a dynamic gesture interface module. The experiments showed that the core of vision-based interaction systems can be the same for all applications and thus facilitate the implementation. In order to test the proposed solutions, three prototypes were implemented. For hand posture recognition, a SVM model was trained and used, able to achieve a final accuracy of 99.4%. For dynamic gestures, an HMM model was trained for each gesture that the system could recognize with a final average accuracy of 93.7%. The proposed solution as the advantage of being generic enough with the trained models able to work in real-time, allowing its application in a wide range of human-machine applications.		Paulo Trigueiros;A. Fernando Ribeiro;Luís Paulo Reis	2014	2014 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)	10.1109/ICARSC.2014.6849782	computer vision;speech recognition;computer science;gesture recognition;communication;sketch recognition;interaction technique	Robotics	-37.730053830515814	-43.51033910252682	89084
9512c73bb4ea16fbbbc5a7a55c370c9b7fb4841e	unencumbered 3d interaction with see-through displays	3d;3d interaction;interaction;gesture;head tracking;see through;touch;qualitative user study;spatial display;interface;public display;datavetenskap datalogi;computer science;augmented reality;mixed reality;pose	Augmented Reality (AR) systems that employ user-worn display and sensor technology can be problematic for certain applications as the technology might, for instance, be encumbering to the user or limit the deployment options of the system. Spatial AR systems instead use stationary displays that provide augmentation to an on-looking user. They could avoid issues with damage, breakage and wear, while enabling ubiquitous installations in unmanned environments, through protected display and sensing technology.  Our contribution is an exploration of compatible interfaces for public AR environments. We investigate interactive technologies, such as touch, gesture and head tracking, which are specifically appropriate for spatial optical see-through displays. A prototype system for a digital museum display was implemented and evaluated. We present the feedback from domain experts, and the results from a qualitative user study of seven interfaces for public spatial optical see-through displays.	3d interaction;augmented reality;gesture recognition;motion capture;prototype;software deployment;stationary process;unmanned aerial vehicle;usability testing	Alex Olwal	2008		10.1145/1463160.1463236	computer vision;augmented reality;interaction;pose;human–computer interaction;computer science;interface;mixed reality;multimedia;gesture	HCI	-44.31289716280808	-42.80613768974203	89107
ce06644d1d71e258363407288638ce76b561fadc	intuitive gesture based user identification system	user identification accelerometer gesture human computer interaction non invasive;user identification;human computer interaction;non invasive;performance evaluation;authorisation;gesture;iris recognition;gesture recognition authorisation;acceleration;acceleration time series analysis accelerometers gesture recognition performance evaluation iris recognition;time series analysis;accelerometer;explicit identification gestures intuitive gesture based user identification system security wise noncritical applications user login multimedia services implicit gesture natural physical hand manipulation neutral motionless position pick up gesture explicit identification signature;accelerometers;gesture recognition	We present an intuitive and very easy to use implicit gesture based identification system that is especially suited for security-wise non-critical applications, such as the user login in the multimedia services. The term “implicit gesture” in our work refers to a natural physical hand manipulation performed by the user who takes hold of the control device and simply picks it up from its neutral motionless position - a “pick-up” gesture. For reference with other related systems, explicit and well defined personal name identification gestures were used as well. User evaluation study results show that 100% recognition accuracy was achieved when using explicit identification signature gestures and over 91% recognition accuracy was achieved when using the implicit “pick-up” gesture. Performance of the proposed system is comparable to results of other respectable related works when using explicit identification gestures, while also showing that implicit gesture based user identification is possible and viable.	login;personal identification number	Joze Guna;Iztok Humar;Matevz Pogacnik	2012	2012 35th International Conference on Telecommunications and Signal Processing (TSP)	10.1109/TSP.2012.6256373	computer vision;speech recognition;computer science;operating system;gesture recognition;accelerometer;statistics	Robotics	-38.10895541530164	-46.462568086807096	89215
ae8c96439da53c829f1f056b0f37fa898edbdf30	robot gardens: an augmented reality prototype for plant-robot biohybrid systems	interactive simulation;biohybrids;augmented reality	Robot Gardens are an augmented reality concept allowing a human user to design a biohybrid, plant-robot system. Plants growing from deliberately placed seeds are directed by robotic units that the user can position, configure and activate. For example, the robotic units may serve as physical shields or frames but they may also guide the plants' growth through emission of light. The biohybrid system evolves over time to redefine architectural spaces. This gives rise to the particular challenge of designing a biohybrid system before its actual implementation and potentially long before its developmental processes unfold. Here, an augmented reality interface featuring according simulation models of plants and robotic units allows one to explore the design space a priori. In this work, we present our first functional augmented reality prototype to design biohybrid systems. We provide details about its workings and elaborate on first empirical studies on its usability.	augmented reality;prototype;robot;simulation;usability	Sebastian von Mammen;Heiko Hamann;Michael Heider	2016		10.1145/2993369.2993400	augmented reality;simulation;computer science;multimedia	HCI	-45.49361418346323	-38.87771792465605	89250
44f2521f02b50a05ad8140788bd45212204432e4	checking indefinitely in chinese chess endgames		A device and method for locating two or more points on a horizontal plane is disclosed. The device emits an audio or visual signal when fluid in a tube reaches a designated level. When the signal is given, the fluid at the other end of the tube will be on the same horizontal plane, thereby enabling the user to locate points on the horizontal plane.		Haw-ren Fang;Tsan-sheng Hsu;Shun-chin Hsu	2004	ICGA Journal		simulation;computer graphics (images);chess endgame;horizontal plane;computer science	Crypto	-39.103221125752675	-42.06117280935859	89590
0020f7b5ff3bd39c322ae8f8f7927d116e77d322	a two-dimensional click model for query auto-completion	two dimensional click model;query auto completion	Query auto-completion (QAC) facilitates faster user query input by predicting users' intended queries. Most QAC algorithms take a learning-based approach to incorporate various signals for query relevance prediction. However, such models are trained on simulat- ed user inputs from query log data. The lack of real user interaction data in the QAC process prevents them from further improving the QAC performance. In this work, for the first time we collect a high-resolution QAC query log that records every keystroke in a QAC session. Based on this data, we discover two user behaviors, namely the horizontal skipping bias and vertical position bias which are crucial for rele- vance prediction in QAC. In order to better explain them, we pro- pose a novel two-dimensional click model for modeling the QAC process with emphasis on these behaviors. Extensive experiments on our QAC data set from both PC and mobile devices demonstrate that our proposed model can accurate- ly explain the users' behaviors in interacting with a QAC system, and the resulting relevance model significant improves the QAC performance over existing click models. Furthermore, the learned knowledge about the skipping behavior can be effectively incorpo- rated into existing learning-based models to further improve their performance.	algorithm;column (database);event (computing);experiment;image resolution;interaction;interaction information;interdependence;mobile device;personal computer;relevance	Yanen Li;Anlei Dong;Hongning Wang;Hongbo Deng;Yi Chang;ChengXiang Zhai	2014		10.1145/2600428.2609571	computer science;machine learning;data mining;multimedia;world wide web;information retrieval	Web+IR	-34.18367013641556	-50.46647821608367	89719
0933fd619dbbd166b3bcfd53bdd8e4675a05eeff	head first: living labs for ad-hoc search evaluation	living labs;information retrieval;universiteitsbibliotheek;evaluation	The information retrieval (IR) community strives to make evaluation more centered on real users and their needs. The living labs evaluation paradigm, i.e., observing users in their natural task environments, offers great promise in this regard. Yet, progress in an academic setting has been limited. This paper presents the first living labs for the IR community benchmarking campaign initiative, taking as test two use-cases: local domain search on a university website and product search on an e-commerce site. There are many challenges associated with this setting, including incorporating results from experimental search systems into live production systems, and obtaining sufficiently many impressions from relatively low traffic sites. We propose that head queries can be used to generate result lists offline, which are then interleaved with results of the production system for live evaluation. An API is developed to orchestrate the communication between commercial parties and benchmark participants. This campaign acts to progress the living labs for IR evaluation methodology, and offers important insight into the role of living labs in this space.	application programming interface;benchmark (computing);e-commerce;hoc (programming language);information retrieval;list of google products;living lab;online and offline;production system (computer science);programming paradigm	Krisztian Balog;Liadh Kelly;Anne Schuth	2014		10.1145/2661829.2661962	simulation;computer science;artificial intelligence;evaluation;data mining;database;multimedia;world wide web;information retrieval	Web+IR	-34.27204052191914	-51.63948117537251	89748
a84c8218d6ff2493a09d9385216c15b7efa0da83	the development of virtual 3d acoustic environment for training 'perception of crossability'	interfase usuario;vision disorder;reproduccion sonido;realite virtuelle;modelo 3 dimensiones;realidad virtual;automovil;reproduction son;user interface;modele 3 dimensions;educational software program;virtual reality;three dimensional model;blind;sound reproduction;percepcion;didacticiel;user assistance;assistance utilisateur;automobile;motor car;asistencia usuario;visual impairment;interface utilisateur;audition;audicion;programa didactico;perception;trouble vision;trastorno vision;ciego;audio acoustics;hearing;acoustique audio;aveugle	This study attempted to reveal the role of auditory information in the accurate perception of crossability for people with severe visual impairment ('the blind'). We created a 'virtual 3D acoustic environment' in which listeners feel a car passing in front of them to help them cross the street safely. An idea of this acoustic system originated from a previous research that showed that the blind make good use of reflected sounds or reverberations in identifying sources and in specifying distances from objects. The system is useful not only for analyzing critical cues of perception of 'crossability' but also for training the blind how to cross a street. Such auditory information can provide the blind with a safe training system for acquiring such auditory information.	acoustic cryptanalysis	Takayuki Shiose;Kiyohide Ito;Kazuhiko Mamada	2004		10.1007/978-3-540-27817-7_70	computer vision;simulation;speech recognition;computer science;virtual reality;user interface;perception	Visualization	-39.66668106400173	-49.05597936088239	89807
8acab2740c5b43f1d72009f849c1a8802d98aef2	smart car [application notes]	pedestrian tracking technique smart car ubiquitous computing human computer interaction computer vision low visibility scenes;image sensors;smart devices;ubiquitous computing computer vision human computer interaction intelligent transportation systems;automotive components;intelligent vehicles;smart devices intelligent sensors image sensors automotive components intelligent vehicles;intelligent sensors	In contrast to a traditional mechanical car, the Smart Car is a highly computerized automobile featuring ubiquitous computing, intuitive human-computer interaction and an open application platform. In this paper, we propose an advanced Smart Car demonstration platform with a transparent windshield display and various motion sensors where drivers can manipulate a variety of car-appropriate applications in augmented reality. Similar to smartphones, drivers can customize their Smart Car through free downloads of car-appropriate applications according to their needs. Additionally, three potential car-appropriate applications related to computer vision are investigated and implemented in our platform for increased driving safety. The first and second carappropriate applications aim to enhance the driving visual field by restoring the low-visibility scenes captured during inclement-weather or nighttime driving conditions to be high-visibility ones, respectively, and display them on a transparent windshield display. We also survey pedestrian tracking techniques that combine multiple driving recorders' information as a mobile surveillance network, including one proposed framework we have developed as the third car-appropriate application. By embedding these carappropriate applications, the Smart Car has the potential to increase safety of driving conditions both in daytime and nighttime, even in bad weather.	augmented reality;bidirectional texture function;computational intelligence;computer vision;datasheet;download;embedded system;global positioning system;human–computer interaction;sensor;smart tv;smartphone;ubiquitous computing;video tracking;virtual world	Shih-Chia Huang;Bo-Hao Chen;Sheng-Kai Chou;Jenq-Neng Hwang;Kuan-Hui Lee	2016	IEEE Computational Intelligence Magazine	10.1109/MCI.2016.2601758	embedded system;computer vision;simulation;image sensor;internet of things;intelligent sensor	HCI	-40.02031976159903	-42.03593421834022	89897
22a3929ece4b763a7aa3d3d01b5448d3cdcb47fe	in search of personal information: narrative-based interfaces	personal information management;narrative based interfaces;user study;document retrieval	Most computer users find organizing large amounts of personal information problematic. Often, hierarchies are the sole means to do it. However, users can remember a broader range of autobiographic contextual data about their personal items. Unfortunately, it can seldom be used to manage and retrieve them. Even when this is possible, it is often done by asking users to fill in values for arbitrary properties in dialog boxes or wizards.  We propose that narrative-based interfaces can be a natural and effective way to help users recall relevant autobiographic data about their personal items and convey it to the computer. Using Quill, a narrative-based personal document retrieval interface, as a case-study, we show how such an interface can be designed. We demonstrate the approach's validity based on a set of user studies, discussing how the problems raised by the evaluation of such an interface were overcome.	document retrieval;organizing (structure);personally identifiable information;usability testing;user (computing);wizard (software);dialog	Daniel Gonçalves;Joaquim A. Jorge	2008		10.1145/1378773.1378797	document retrieval;human–computer interaction;computer science;personal information management;multimedia;world wide web;personal information manager;information retrieval	HCI	-48.12644379716799	-42.387282945377066	90047
01be2aad2fc2a5a84ca8754262f0fbeef0646b20	the audiator: a device-independent active marker for spatially aware displays	interaction;device independence;computer vision;public information	The LightSense system [Olwal 2006] promotes printed public information sources, such as maps, into interactive surfaces enriched with digital information, which is accessed through a spatially aware display [Fitzmaurice 1993]. Using outside-in tracking, LED lights on cell phones are located using an instrumented environment with either a combination of a computer/camera or microcontroller/photosensors. The location is continuously sent to the phone over Bluetooth, such that context-sensitive information can be shown as the device is moved by the user.	bluetooth;context-sensitive help;digital data;information sensitivity;map;microcontroller;mobile phone;printing	Alex Olwal	2007		10.1145/1280720.1280886	embedded system;computer vision;interaction;computer science;operating system;multimedia;computer graphics (images)	HCI	-45.26046217087524	-41.41970203644999	90189
40f4b6c62a0713792152fb6902e5356cc430d707	the effect that haptically perceiving a projection augmented model has on the perception of size	spatially coincident device haptic perception projection augmented model size perception;haptic interfaces visual perception augmented reality;mice augmented reality haptic interfaces feedback information systems solid modeling computer applications computer displays physics computing shape;visual perception;augmented reality;haptic interfaces	This paper reports on a study that investigated the effect touching a projection augmented model, and interacting with it using a spatially-coincident device, has on the perception of size. It was found that touching increased the accuracy of size estimates, however interaction using a spatially-coincident device did not.	haptic technology;interaction;projection augmented model	Emily Bennett;Brett Stevens	2004	Third IEEE and ACM International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2004.59	computer vision;augmented reality;visual perception;computer science;multimedia;computer graphics (images)	Arch	-43.76641259595755	-48.55605960964026	90241
3b479b0c0dcbb09274070532ea548f2bb3b57371	culturally independent gestures for in-car interactions		In this paper we report on our ongoing work to introduce freehand gestures in cars as an alternative input modality. Contactless gestures have hardly been successful in cars so far, but have received attention in other contexts recently. We propose a way to achieve a better acceptance by both drivers and car manufacturers. Using a four-step process, we developed a small set of culturally independent and therefore easy-to-learn gestures, which can be used universally across different devices. We built a first prototype using distance sensors to detect the stop gesture in front of several devices. We conducted a user study during actual driving situations, testing the pragmatic and hedonic quality of the approach as well as its attractiveness. The results show a high acceptance of our approach and confirm the potential of freehand gestures as an alternative input modality in the car.	adobe freehand;contactless payment;interaction;modality (human–computer interaction);prototype;sensor;usability testing	Sebastian Löhmann;Martin Knobel;Melanie Lamara;Andreas Butz	2013		10.1007/978-3-642-40477-1_34	simulation;multimedia	HCI	-46.47021310571723	-44.631971322901514	90297
20b8133f24712fe52ab09cbdb27414aa9ee6ddb9	picture browsing non-touch interaction methods for smartphones using an accelerometer and camera with a focus on phone dialing	dialing;touch interaction;accelerometer;smartphone;mobile interaction	This paper proposes various interaction methods for smartphones that allow users to call an intended contact without touching the smartphone screen. Existing applications allow users to answer phone calls without touching the screen—by shaking the phone, for example—but do not allow users to make phone calls. The proposed interaction allows users to select and call an intended contact by utilizing the iPhone’s accelerometer. The interaction also involves video camera scanning for commands to switch between group-selection and individual-selection modes to facilitate the selection of the call candidate. Furthermore, the proposed interaction secures transparency by displaying the camera’s video stream on the smartphone screen. In order to evaluate the efficacy of the interaction, an application using the interaction was developed, and two simple experiments were conducted, in which participants were asked to make phone calls using the application. The success rate was 98 %, and user satisfaction with the proposed interaction was approximately 90 %. Therefore, the results showed that the proposed interaction could be an effective solution to allow users to make phone calls in situations where they cannot physically touch the iPhone screen. Furthermore, this solution could be used in many fields that need interactions with users in mobile applications.	experiment;interaction;mobile app;real life;smartphone;streaming media	Myoungbeom Chung;Hyunseung Choo	2013	Multimedia Tools and Applications	10.1007/s11042-013-1576-8	embedded system;mobile interaction;computer science;operating system;internet privacy;world wide web;accelerometer	HCI	-45.77560636521863	-43.63599194478073	90330
e68018647070f0c74a555e8f522fa607d6a83f94	passive haptic learning for vibrotactile skin reading		This paper investigates the effects of using passive haptic learning to train the skill of comprehending text from vibrotactile patterns. The method of transmitting messages, skin-reading, is effective at conveying rich information but its active training method requires full user attention, is demanding, time-consuming, and tedious. Passive haptic learning offers the possibility to learn in the background while performing another primary task. We present a study investigating the use of passive haptic learning to train for skin-reading.	haptic technology;teaching method;transmitter	Granit Luzhnica;Eduardo E. Veas;Caitlyn E. Seim	2018		10.1145/3267242.3267271	computer vision;artificial intelligence;wearable computer;haptic technology;human–computer interaction;computer science	HCI	-44.70828302272107	-44.10380523238351	90434
94f6716193c81c09e4ac1c42dd3f6c06e6f0dee3	interactive virtual aquarium with a smart device as a remote user interface		New applications of smart devices interacting with other computing devices are recently providing interesting and feasible solutions in ubiquitous computing environments. In this study, we propose an interactive virtual aquarium system that interacts with a smart device as a user interface. We developed a virtual aquarium graphic system and a remote interaction application of a smart device for building an interactive virtual aquarium system. We performed an experiment that demonstrates the feasibility and the effectiveness of the proposed system as an example of a new type of interactive application of smart display, where a smart device serves as a remote user interface.	smart device;user interface	Yongho Seo;Jin Choi	2011		10.1007/978-3-642-27204-2_39	human–computer interaction;multimedia;world wide web	HCI	-46.93612765006646	-39.069730565224496	90452
8f739c0eba6875440e601b350edd015969303888	haptic material: a holistic approach for haptic texture mapping		In this paper, we propose a new format for haptic texture mapping which is not dependent on the haptic rendering setup hardware. Our “haptic material” format encodes ten elementary haptic features in dedicated maps, similarly to “materials” used in computer graphics. These ten different features enable the expression of compliance, surface geometry and friction attributes through vibratory, cutaneous and kinesthetic cues, as well as thermal rendering. The diversity of haptic data allows various hardware to share this single format, each of them selecting which features to render depending on its capabilities.	complementarity theory;computer graphics;database;haptic technology;iteration;list of 3d rendering software;map;open research;point of view (computer hardware company);texture mapping;volume	Antoine Costes;Fabien Danieau;Ferran Argelaguet;Anatole Lécuyer;Philippe Guillotel	2018		10.1007/978-3-319-93399-3_4	rendering (computer graphics);haptic technology;computer vision;computer graphics;texture mapping;computer science;artificial intelligence;kinesthetic learning	Visualization	-41.48436162816859	-39.729811206894766	90659
ad48b8cd59b5e329bb53e6ed36c85b74a0b70fa0	novel indirect touch input techniques applied to finger-forming 3d models	indirect touch;qualitative data;perspective dependent gestures;two handed input;polygon modeling	We address novel two-handed interaction techniques in dual display interactive workspaces combining direct and indirect touch input. In particular, we introduce the notion of a horizontal tool space with task-dependent graphical input areas. These input areas are designed as single purpose control elements for specific functions and allow users to manipulate objects displayed on a vertical screen using simple one- and two-finger touch gestures and both hands. For demonstrating this concept, we use 3D modeling tasks as a specific application area. Initial feedback of six expert users indicates that our techniques are easy to use and stimulate exploration rather than precise modeling. Further, we gathered qualitative feedback during a multi-session observational study with five novices who learned to use our tool and were interviewed several times. Preliminary results indicate that working with our setup is easy to learn and remember. Participants liked the partitioning character of the dual-surface setup and agreed on the benefiting quality of touch input, giving them a 'hands-on feeling'.	3d modeling;feedback;hands-on computing;interaction technique;multi-monitor;touchscreen;workspace	Henri Palleis;Julie Wagner;Heinrich Hußmann	2016		10.1145/2909132.2909257	qualitative property;simulation;computer science;polygonal modeling;multimedia	HCI	-44.49522050852844	-39.39454302246628	90707
b7220839704ca086de6513e9b2b9c73e49dfa103	dicom plug-n-play, factory calibration internal to the flat panel	display calibration to dicom;dicom calibration of lcd display;factory calibration of lcd displays;data transfer;look up table;present value	Medical grade display systems started out with expensive graphic cards and equally expensive CRT monitors operating in the world of analog signals. With the advent of LCD displays and their migration to pure digital data transfer, many of the cost elements associated with the graphic cards have fallen dramatically. The ability to incorporate any commercial graphic card into a medical application and retain the required calibration (DICOM 3.14) standards has shifted the calibration burden to the LCD display in a positive way. The LCD becomes the logical location for expanded Look-Up-Tables (LUTS) to reside. In this way, presentation values (P-values) from the graphics card in an unambiguous digital format can be mapped within the panel to absolute luminance responses consistent with DICOM.	dicom;flat panel display;plug and play	K. Compton;R. Hansen	2004			embedded system;digital data;calibration;computer hardware;dicom;graphics;data transmission;analog signal;medicine;lookup table;liquid-crystal display	Robotics	-38.67570738182196	-40.361529749653826	90906
3f12315a92a28f556c88ddceaea2947b4930106a	investigation of the text entry speed and accuracy in mobile devices	human computer interaction;mobile device;personal computer;text entry;mobile computer;mobile computing	The paper presents an investigation of the Bulgarian and English text entry speed and accuracy in mobile devices. For this purpose specialised software was developed and used. A comparison with hand writing and text entry using a personal computer is made. The obtained results are compared with those of similar investigations. The appropriate conclusions are made.	mobile device;personal computer;rollable display;virtual keyboard	Tsvetozar Georgiev;Evgeniya Georgieva	2009		10.1145/1731740.1731810	mobile search;simulation;human–computer interaction;computer science;operating system;mobile technology;mobile device;multimedia;mobile computing	HCI	-47.78281816154444	-44.18438362281923	90966
4132abd6b8a9354706361254086f8c5477cd35e4	animated ui transitions and perception of time: a user study on animated effects on a mobile screen	ui transitions;mobile device;user study;user studies;ui design;user experience;ui animations	The capability to present advanced graphics in the present mobile devices can be utilized to improve their usability and overall user experience. Mobile devices have limitations compared to PCs due to their inferior computing power and small screens, but a successful design of animated transitions can hide processing delays and make the user experience smoother. In this paper, we describe the design of animated transitions and present a user study on how they are perceived. The results show that in the transition between two images, bringing up the next image earlier dominates the perception of a fast transition over other variables examined in the study.	graphics;mobile device;mobile phone;usability testing;user experience;user interface	Jussi Huhtala;Ari-Heikki Sarjanoja;Jani Mäntyjärvi;Minna Isomursu;Jonna Häkkilä	2010		10.1145/1753326.1753527	user interface design;user experience design;simulation;human–computer interaction;computer science;operating system;mobile device;multimedia	HCI	-47.49754867431135	-44.70781733334029	90984
8afe7d61e13ad7f096429716f94ff33ac5c59b2f	climbaware: investigating perception and acceptance of wearables in rock climbing	visual alerts;tactile alerts audible alerts;ambient information;tactile alerts;perception;wearable computing;sports technologies;audible alerts;climbing	Wearable sports devices like GPS watches and heart rate monitors are ubiquitous in sports like running or road cycling and enable the users to receive real-time performance feedback. Although rock climbing is a trending sport, there are little to no consumer electronics available to support rock climbing training during exercise. In this paper, we investigated the acceptance and appropriateness of wearables in climbing on different body parts. Based on an online survey with 54 climbers, we designed a wearable device and conducted a perception study with 12 participants in a climbing gym. Using vibro-tactile, audible, and visual cues while climbing an easy route and a hard route, requiring high physical and cognitive load, we found that the most suited notification channel is sound, directly followed by vibro-tactile output. Light has been found to be inappropriate for the use in the sport of climbing.	global positioning system;norm (social);pet rock;real-time web;rock's law;wearable computer;wearable technology	Felix Kosmalla;Frederik Wiehr;Florian Daiber;Antonio Krüger;Markus Löchtefeld	2016		10.1145/2858036.2858562	climbing;simulation;wearable computer;computer science;multimedia;perception	HCI	-45.78012425973525	-44.6053434698164	91017
c4c55bfaaf596e0c8f749c6b73ba89669b186fbb	user intention modeling for interactive image retrieval	computers;user modelling;user intention modeling;intention specific search schema;user relevance feedbacks;smart intention;user study;user feedback;interactive method;natural user feedback;interactive image retrieval;reference strokes interaction;image retrieval feature extraction computers user interfaces face humans;feature extraction;user experience;user modelling content based retrieval image retrieval interactive systems relevance feedback;user relevance feedbacks user intention modeling interactive image retrieval content based image retrieval smart intention intention specific search schema reference strokes interaction natural user feedback;user feedback content based image retrieval user intention;face;humans;content based image retrieval;interactive systems;relevance feedback;content based retrieval;user interfaces;user intention;image retrieval	We propose three innovative interactive methods to let computer better understand user intention in content-based image retrieval: 1. Smart intention list induces user intention, thereby improves search results by intention-specific search schema; 2. Reference strokes interaction allows user to specify in detail about the intention by pointing out interested regions; 3. Natural user feedback easily collects data of user relevance feedbacks to boost the performance of the system. Systematic user study shows that the proposed interactive mechanism improves search efficiency, reduces user workload, and enhances user experience.	content-based image retrieval;relevance;usability testing;user experience	Jingyu Cui;Fang Wen;Xiaoou Tang	2010	2010 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2010.5583220	user interface design;face;user;user experience design;user modeling;computer user satisfaction;feature extraction;image retrieval;computer science;user requirements document;multimedia;user interface;world wide web;information retrieval	Vision	-34.87208305107421	-48.891377152156785	91368
0c13d5441759534e40863b40d757cece53770a5c	ivu.kom: a framework for viewer-centric mobile location-based services	location based service;field of view	Users, especially when in unfamiliar areas, would appreciate services that provide them with information about their surroundings and objects in the neighborhood. A natural way to find such information is by looking at the objects. Therefore, it would be very useful if Location-based Services could provide a kind of viewer-centric interaction between the users and their surroundings. In this paper, we outline iVu.KOM as a framework for running efficient and prompt mobile viewercentric queries. We describe our concepts of efficiently modeling the geometry that is surrounding the user, and the modeling of the user’s field of view using orientation sensors existing in emerging smartphones. Based on real-world experiments, we provide also an evaluation of the framework capabilities to recognize and react to sensors’ uncertainties.	desktop metaphor;experiment;image processing;location-based service;sensor;smartphone	Farid Zaid;Parag S. Mogre;Andreas Reinhardt;Diego Costantini;Ralf Steinmetz	2010	Praxis der Informationsverarbeitung und Kommunikation	10.1515/piko.2010.048	field of view;computer science;location-based service;internet privacy;world wide web;computer security	Mobile	-46.97409280266371	-40.612847622121485	91423
710f0efc81ddd7ccbfc427995a7e2ed53da72046	a haptic atm interface to assist visually impaired users	c800 psychology;automated teller machine;vibro tactile;acceptability;p900 others in mass communications and documentation;accessibility;visual impairment;haptic;keypad	This paper outlines the design and evaluation of a haptic interface intended to convey non audio-visual directions to an ATM (Automated Teller Machine) user. The haptic user interface is incorporated into an ATM test apparatus on the keypad. The system adopts a well known 'clock face' metaphor and is designed to provide haptic prompts to the user in the form of directions to the current active device, e.g. card reader or cash dispenser. Results of an evaluation of the device are reported that indicate that users with varying levels of visual impairment are able to appropriately detect, distinguish and act on the prompts given to them by the haptic keypad. As well as reporting on how participants performed in the evaluation we also report the results of a semi structured interview designed to find out how acceptable participants found the technology for use on a cash machine. As a further contribution the paper also presents observations on how participants place their hands on the haptic device and compare this with their performance.	atm turbo;card reader;haptic technology;norm (social);semiconductor industry;user interface	Brendan Cassidy;Gilbert Cockton;Lynne M. Coventry	2013		10.1145/2513383.2513433	simulation;human–computer interaction;computer science;accessibility;operating system;multimedia;haptic technology;world wide web	HCI	-48.15747701041847	-45.38965134797298	91435
8518a5a4874026a79d9788d57dd31bf0cd68e4a1	hyperbaric oxygen chamber users may obtain immersive enjoyment by virtual reality glasses		As for the users of hyperbaric oxygen therapy apartment, this paper put forward a newfangled immersing entertainment system. As for this system, it is a mixture of software and hardware, and this paper has depicted the scheme. And a HMD (i.e. virtual reality glasses shell), a water-repellent bag and a smart-phone compose the hardware. Besides, as for the software, it can transform the tridimensional images of the three-dimensional game into the smart-phone’s screen at the same time. And in terms of the actual moving scene of the hospital hyperbaric oxygen treatment, people will talk about the option and comparison of the hardware. At last, it is likely to raise a rudimentary guideline to design this sort of system consequently.		Zhihan Lv	2015		10.1007/978-3-319-69694-2_10	simulation;immersion (virtual reality);software;multimedia;guideline;virtual reality;engineering	Visualization	-45.09721296944439	-42.27736880127063	91440
59c96d9725fd76b07196497f460d35c8a50488d0	using a vibro-tactile display for enhanced collision perception and presence	whole body;multimodal interface;vibrator;virtual reality;virtual environments;tactile sensing;multimodality;tactile display;tactile interface;spatial presence;presence;virtual environment;close range;vibration feedback model;sensory saltation	One of the goals and means of realizing virtual reality is through multimodal interfaces, leveraging on the many sensory organs that humans possess. Among them, the tactile sense is important and useful for close range interaction and manipulation tasks. In this paper, we explore this possibility using a vibro-tactile device on the whole body for simulating collision between the user and virtual environment. We first experimentally verify the effect of enhanced user felt presence by employing localized vibration feedback alone on collision, and further investigate how to effectively provide the sense of collision using the vibro-tactile display in different ways. In particular, we test the effects of using a vibration feedback model (for simulating collision with different object materials), saltation, and simultaneous use of 3D sound toward spatial presence and perceptual realism. The results have shown that employing the proposed vibro-tactile interface did enhance the sense of presence, especially when combined with 3D sound. Furthermore, the use of saltation also helped the user detect and localize the point of contact more correctly. The use of the vibration feedback model was not found to be significantly effective, and sometimes even hindered the correct sense of collision primarily due to the limitation of the vibrotactile display device.	anomalous experiences;display device;experiment;http 404;multimodal interaction;simulation;surround sound;virtual reality	Jonghyun Ryu;Gerard Jounghyun Kim	2004		10.1145/1077534.1077551	computer vision;simulation;computer science;virtual machine;artificial intelligence;operating system;virtual reality	HCI	-44.83970595557317	-47.90258869413632	91448
099132ed6b9df4cea72e206340693d38349b8e33	steady-state veps in cave for walking around the virtual world	remote control;3d imaging;online interaction;linear discriminate analysis;gaze direction;steady state visual evoked potential;human brain;steady state;virtual worlds;immersive virtual environment	The human brain activities of steady-state visual evoked potentials, induced by a virtual panorama and two objects, were recorded for two subjects in immersive virtual environment. The linear discriminant analysis with single trial EEG data for 1.0 seconds resulted in 74.2 % of averaged recognition rate in inferring three gaze directions. The possibility of online interaction with 3D images in CAVE will be addressed for walking application or remote control of a robotic camera.	virtual world	Hideaki Touyama;Michitaka Hirose	2007		10.1007/978-3-540-73281-5_78	computer vision;simulation;geography;communication	HCI	-40.712857582547386	-41.04478448965224	91524
08628ef2f18a379f0c9d57b7499358d28eccfce4	an investigation of eyes-free spatial auditory interfaces for mobile devices : supporting multitasking and location-based information			computer multitasking;mobile device	Yolanda Vazquez-Alvarez	2013				HCI	-47.510573682489905	-43.017429737221036	91618
4f7bf03bfe27341a16ea605597d31a5925e31ef4	emotional interaction model for a service robot	service robots human robot interaction cognitive robotics robot sensing systems intelligent robots appraisal communication industry intelligent sensors mechanical engineering research and development;cognitive systems;service robots;human robot interaction;indexing terms;action coloring emotional interaction model service robot emotional response cognitive emotion modeling appraisal based emotion expression;cognitive process;service robot;artificial intelligence;user interfaces artificial intelligence service robots cognitive systems;interaction model;user interfaces;emotional expression	"""This paper presents the emotional interaction model that receives classified inputs from user's response and decides what emotional response the robot should generate. Cognitive emotion modeling requires profound understanding about human's cognitive processes and ideas on how to implement each constitutional components of the model into the robot. The proposed model is composed of two layers: reactive and deliberative layers. Reactive layer is in charge of immediate emotional response of the robot. It works with pre-defined rules which relate input to its corresponding emotional expressions. This layer enables immediate display of the robot's emotional state to the user so that lifelike characteristics of the robot can be achieved. The deliberative layer is in charge of the appraisal-based emotion expression and carries out the function of """"action-coloring,"""" which adds flavor to the actions being taken. Emotional interaction scenarios which are considered to be possible with the proposed emotion model are also introduced."""	cognition;cognitive science;computation;graph coloring;service robot	Hyoung-Rock Kim;Kang-Woo Lee;Dong-Soo Kwon	2005	ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.	10.1109/ROMAN.2005.1513857	human–robot interaction;simulation;cognition;index term;computer science;emotional expression;artificial intelligence;social robot;user interface;personal robot	Robotics	-34.58117280730559	-40.25204078417942	91727
4068721cc90f2c4eaf9babfbae97ac24d7edb9a8	the task of walking straight as an interactive serious game for blind children		This paper describes “Following the Cuckoo Sound”, a responsive floor system designed to train blind children in walking straight. Its characteristics and potentialities are evaluated in the framework of the guidelines of instruction and assistance to blind children defined by two government agencies. Veering data analysis confirm the validity of the game in correcting both the initial orientation error and the errors in the step direction and provide further insights about veering and its relationship with walking speed and participants’ clinical or experiential conditions. Moreover the system provides an engaging sensorial experience which fits not only the orientation and mobility programs but also other areas of the curriculum for blind children such as sensory efficiency, recreation and leisure, and education in the use of technology. Received on 31 May 2018; accepted on 22 June 2018; published on 13 September 2018	experiment;fits;poor posture;streaming simd extensions	Marcella Mandanici;Antonio Rodà;Marco Ricca	2018	EAI Endorsed Trans. Serious Games	10.4108/eai.13-7-2018.155165		HCI	-47.50743820937298	-49.39840680222227	91812
c6b52a3e176a59a82e02bdca64ca98b42d7e6013	museum of the moment: a cooperation project between metropolitan culture and geomatics	indoor navigation;gyroscopes;wireless lan accelerometers fingerprint identification gyroscopes headphones magnetic sensors microcontrollers museums;magnetic sensors;low cost arduino art museum orientation;magnetic sensors compass gyroscopes ieee 802 11 standard accelerometers indoor navigation;ieee 802 11 standard;accelerometers;compass;people image museum of the moment metropolitan culture metropolitan geomatics low cost sensor people memories people stories aesthetic effects radio controlled headphone arduino acoustic signal mpu 9150 gyroscope accelerometer magnetic field sensor wi fi fingerprinting algorithm signal strength	In the art scene, low-cost sensors like Arduino are often being used to create or support a range of different effects. Live-Art installations must be often made very favorably due to low budget. In this article the function of a module for orientation is being described as it works in an artistic project, a Ph.D. - thesis is based on. The project is based on different people's memories and stories. “Museum of the moment” deals with the aesthetic effects of performative collections such as the various visual animations of people's memories. Different people are being videotaped while describing important moments in their lives. These portraits are being presented on large screens in the gallery. The viewers equipped with radio-controlled headphones look at the images and also listen to the different stories, not necessarily knowing who of the portrayed people has told which story. The orientation module based on the Arduino is supposed to identify the screen automatically and communicate with the acoustic server, in order to create the effect, that only the screen, that is being looked at by the viewer will trigger an acoustic signal which is the story that the viewer can listen to. Arduino is being used in this setting to enable orientation in the gallery space in the first place. Secondly, it makes sure to identify positions. Arduino-Nanos with a MPU 9150 are being used. These include an accelerometer, gyroscope and magnetic field sensor. To set and identify positions Wi-Fi is being used. Therefore signal strength and fingerprinting algorithms were examined. The final configuration of the screens does not necessarily need an estimation of position.	acoustic cryptanalysis;algorithm;arduino;fingerprint (computing);geomatics;gyroscope;headphones;mpu-401;radio control;sensor;server (computing)	Thomas Willemsen;Friedrich Keller;Stefanie Lorey;Harald Sternberg	2014	2014 International Conference on Indoor Positioning and Indoor Navigation (IPIN)	10.1109/IPIN.2014.7275505	simulation;telecommunications;engineering;electrical engineering	HCI	-41.5831272802677	-42.5832911184024	91895
2cb81314ce056ceadd73c0f3268f2378db95e35b	review-based screening interface for improving users’ decision process in e-commerce		Due to the important role of product reviews in e-commerce, some systems have employed different approaches to present reviews on product detail page or comparison page. However, little work has investigated how to present reviews on screening interface for facilitating users to screen out interesting alternatives from a set of options. In this paper, we have developed a novel review-based screening interface in terms of users’ behaviors. Concretely, there are two innovations in the interface design: (1) it enables users to eliminate items by sentiment attributes (i.e., the attributes extracted from reviews), and (2) it emphasizes on visualizing the value distribution of each attribute and tradeoffs among attributes. The results of a user study show that our review-based screening interface achieves significantly more positive assessments than traditional screening interface, in terms of users’ perceived decision accuracy, cognitive effort, pleasantness and intention to use.	e-commerce payment system;usability testing	Dongning Yan;Li Chen	2017		10.1007/978-3-319-58484-3_11	interface design;e-commerce;data mining;cognition;computer science	HCI	-37.430226005053065	-51.63060100100946	91897
1255be2482d22e659ac21d05240409f4276f8471	gesture-based object localization for robot applications in intelligent environments		Drawing attention to objects and their localization in the environment are essential building blocks for domestic robot applications, e.g. fetch-and-delivery or navigation tasks. For this purpose, human pointing gestures turned out to be a natural and intuitive interaction method to transfer the spatial data of an object from human to robot. Current approaches only use the robot's on-board sensors to perceive gesture-based instructions, which restricts them to the field of view of the robot's camera. The integration of mobile robots into intelligent environments, such as smart homes, opens new possibilities to overcome this limitation by utilizing components of the surrounding environment as additional sensors. We take advantage of these new possibilities and propose a multi-stage object localization system based on human pointing gestures that considers the whole intelligent environment as interaction partner. Our experimental results show that our multi-stage approach successfully refines the position initially proposed by a human pointing gesture by employing a distributed camera network integrated into the environment for object localization.		Dennis Sprute;Robin Rasch;Aljoscha Pörtner;Sven Battermann;Matthias König	2018	2018 14th International Conference on Intelligent Environments (IE)	10.1109/IE.2018.00015	human–computer interaction;spatial analysis;probabilistic logic;computer science;robot;field of view;mobile robot;intelligent environment;domestic robot;gesture	Robotics	-39.84174695684404	-43.64268345825747	92002
6ded58bd0c3921a6d3bca6f486f34cdb2f900210	exploratory results for a mission specialist interface in micro unmanned aerial systems	microrobots;human factors and evaluation methodologies;user interfaces autonomous aerial vehicles human robot interaction microrobots mobile robots telerobotics;human computer interaction guidelines humans navigation cameras robots payloads;mobile robots;human robot interaction;specialized emergency responders mission specialist interface human robot interaction micro unmanned aerial systems hri micro uas human robot team modeling human machine interaction technologies interaction principles system architecture;hri applications search and rescue;user interface designs and usability evaluations;hri applications search and rescue user interface designs and usability evaluations human factors and evaluation methodologies;telerobotics;user interfaces;autonomous aerial vehicles	This paper presents a human-robot interaction (HRI) exploratory study of a dedicated Mission Specialist interface for micro unmanned aerial systems (UAS). Current HRI findings from the micro UAS literature suggest that a Mission Specialist role requires a small, mobile, and visual interface that is dedicated and software-based. A literature survey of humanrobot team modeling, human-machine interaction technologies, and interaction principles applicable to micro UAS, resulted in an identified HRI investigation framework, five synthesized design guidelines, and a system architecture for a dedicated Mission Specialist interface. The interface was implemented and evaluated through an exploratory field study involving 16 specialized emergency responders. Observations from the study suggested that with refinements, a dedicated Mission Specialist interface could be a useful tool for future HRI studies to explore role performance in micro UAS.	aerial photography;algorithm;field research;human–computer interaction;human–robot interaction;micro isv;systems architecture;unmanned aerial vehicle;user interface	Joshua M. Peschel;Brittany A. Duncan;Robin R. Murphy	2012	2012 International Conference on Collaboration Technologies and Systems (CTS)	10.1109/CTS.2012.6261039	telerobotics;human–robot interaction;mobile robot;computer vision;simulation;human–computer interaction;computer science;artificial intelligence;user interface	HCI	-36.92682877605553	-41.586983734263306	92145
ed949cf7518baf3a40b15fc2137f0704273ec450	character input by gesture performed with grasping both ends of tablet pc		We developed a method to enter Hiragana (cursive in Japanese) characters and alphanumeric characters by thumb gestures for Japanese people. This method is optimized to be used while holding the left and right ends of a tablet PC with hands. Each gesture is designed by concatenating a few horizontal and/or vertical strokes, but path of the gesture remains inside the one-stroke square centering its starting position. A Hiragana character is input by selecting a row of the table of Japanese syllabary and then a column of the table. The alphabet characters are segmented into 6 groups in order of letters. Just like Hiragana, an alphabet character is entered by selecting a group and then selecting a member. However, in the alphanumeric mode, to reduce the cost of modifying the mistakenly selected group, an operation to move to the previous or next group have been added.	tablet computer	Toshimitsu Tanaka;Tetsuaki Mano;Yuuya Tanaka;Kohei Akita;Yuji Sagawa	2018		10.1007/978-3-319-91250-9_7	computer hardware;alphanumeric;multimedia;computer science;hiragana;cursive;concatenation;syllabary;alphabet;left and right;gesture	Robotics	-45.302351743219376	-44.74198180223026	92166
f8e26016a335137cac8e7f412060dd3b9cdaa052	hand movement prediction based collision-free human-robot interaction		We present a framework from vision based hand movement prediction in a real-world human-robot collaborative scenario for safety guarantee. We first propose a perception submodule that takes in visual data solely and predicts human collaborator's hand movement. Then a robot trajectory adaptive planning submodule is developed that takes the noisy movement prediction signal into consideration for optimization. We first collect a new human manipulation dataset that can supplement the previous publicly available dataset with motion capture data to serve as the ground truth of hand location. We then integrate the algorithm with a robot manipulator that can collaborate with human workers on a set of trained manipulation actions, and it is shown that such a robot system outperforms the one without movement prediction in terms of collision avoidance.	algorithm;ground truth;human–robot interaction;mathematical optimization;motion capture;robot	Yiwei Wang;Xin Ye;Yezhou Yang;Wenlong Zhang	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.72	artificial intelligence;computer vision;collision;visualization;robot;ground truth;computer science;trajectory;manipulator;human–robot interaction;motion capture	Vision	-34.76741791495076	-43.25602113065423	92331
beb8eb64f4f02af3c42cea81f398174ab63cb1bc	directional control of an omnidirectional walker for walking support with forearm pressures	walking support;distance type fuzzy reasoning directional control omnidirectional walker walking support forearm pressures walking disabilities fuzzy rules directional intention identification;fuzzy reasoning;legged locomotion;sensors;fuzzy rules;fuzzy set theory;fuzzy logic;handicapped aids;cognition;mobile communication;omnidirectional walker;humans;handicapped aids fuzzy reasoning fuzzy set theory;distance type fuzzy reasoning method;directional intention identification;force sensor;everyday life;distance type fuzzy reasoning method walking support omnidirectional walker directional intention identification;legged locomotion sensors fuzzy reasoning cognition fuzzy logic humans mobile communication	Walking is a fundamental human ability necessary for everyday life. We have developed an omnidirectional walker (ODW) for walking support to those who have walking disabilities. It is necessary for the ODW to know which direction the user is intending to go during walking support. A novel interface is proposed for the ODW to recognize directional intention according to the user's forearm pressures which are measured by force sensors embedded in the armrest. The relationship between forearm pressures and directional intention was extracted as fuzzy rules and an algorithm is proposed for directional intention identification based on distance type fuzzy reasoning method. In this paper, we conduct walking support experiments with the proposed method. The results show that the algorithm is applicable to directional control in walking support.	algorithm;amiga walker;armrest;embedded system;experiment;open desktop workstation;sensor	Yinlai Jiang;Kenji Ishida;Shuoyu Wang;Takeshi Ando;Masakatsu G. Fujie	2011	2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)	10.1109/FUZZY.2011.6007467	fuzzy logic;computer vision;cognition;mobile telephony;computer science;sensor;artificial intelligence;mathematics;fuzzy set;force-sensing resistor	Robotics	-41.443151488898806	-44.44361265799876	92439
a6aa420218f224374738b32452778ea9af8ea14d	recognizing affect in human touch of a robot	human robot interaction;affective interfaces;affect recognition;haptic;gesture recognition	A pet cat or dog's ability to respond to our emotional state opens an interaction channel with high visceral impact, which social robots may also be able to access. Touch is a key but understudied element; here, we explore its emotional content in the context of a furry robot pet. We asked participants to imagine feeling nine emotions located in a 2-D arousal-valence affect space, then to express them by touching a lap-sized robot prototype equipped with pressure sensors and accelerometer. We found overall correct classification (Random Forests) within the 2-D grid of 36% (all participants combined) and 48% (average of participants classified individually); chance 11%. Rates rose to 56% in the high arousal zone. To better understand classifier performance, we defined and analyzed new metrics that better indicate closeness of the gestural expressions. We also present a method to combine direct affect recognition with affect inferred from gesture recognition. This analysis provides a unique first insight into the nature and quality of affective touch, with implications as a design tool and for incorporating unintrusive affect sensing into deployed interactions.	robot	Kerem Altun;Karon E. MacLean	2015	Pattern Recognition Letters	10.1016/j.patrec.2014.10.016	computer vision;computer science;gesture recognition;haptic technology	Vision	-46.12835030469332	-44.915425248484006	92543
8f1fb59e99634fd97596d5823e91ffe6c6cf88ba	how tactor size and density of normal indentation tactile displays affects grating discrimination tasks	haptic i o and standardization;gratings;haptic i o and standardization user interfaces evaluation methodology;actuators;thumb;arrays;indexes;shape;evaluation methodology;virtual reality haptic interfaces;user interfaces;gratings shape arrays actuators thumb indexes;virtual reality tactor size normal indentation tactile displays grating discrimination tactile fingertip devices design parameters physical feedback human tactile perception tactor density sinusoidal gratings passive guided touch tactor spacings spatial period weber fraction	Tactile fingertip devices of vertically moving tactors have two important design parameters that significantly affect physical feedback and human tactile perception: tactor size and tactor density. However, there are a limited number of research studies that have evaluated the effect of these parameters on human tactile perception. This paper investigated the influence and interaction of these two parameters on a discrimination task. The task consisted of discriminating the spatial-period of sinusoidal gratings through the use of passive-guided touch. In two complementary experiments 40 participants performed the discrimination task under two different tactile conditions: (I) using direct bare fingertip sensing (baseline condition), and (II) using tactile displays with different tactor spacings and diameters. In both experiments differences between and within subjects were considered for Condition II, and for all test conditions the spatial-period Weber fraction for each participant was measured. Results from both experiments were consistent in indicating that tactile performance improves as tactor spacing is decreased and tactor diameter is increased. However, tactor spacings below 1.1 mm might not result in any significant further improvement. The findings of this study might help designers to choose design parameters for tactile displays based upon the cost-benefit of tactor density versus perceptual performance.	baseline (configuration management);diameter (qualifier value);experiment;fingertip dosing unit;haptic device component;haptic technology;large;less than;small;spacing;touch perception;weber	Nadia Vanessa Garcia-Hernandez;Federica Bertolotto;Ferdinando Cannella;Nikolaos G. Tsagarakis;Darwin G. Caldwell	2014	IEEE Transactions on Haptics	10.1109/TOH.2014.2309128	database index;computer vision;simulation;shape;computer science;engineering;user interface;engineering drawing;actuator	HCI	-43.91989978701597	-48.98242191725323	93268
39805143bd27b873ff9c463ddf47c2b15e718023	compensating the effect of communication delay in client-server-based shared haptic virtual environments	perceived transparency;shared haptic virtual environment;collaboration;multi user;haptic rendering;communication delay	Shared haptic virtual environments can be realized using a client-server architecture. In this architecture, each client maintains a local copy of the virtual environment (VE). A centralized physics simulation running on a server calculates the object states based on haptic device position information received from the clients. The object states are sent back to the clients to update the local copies of the VE, which are used to render interaction forces displayed to the user through a haptic device. Communication delay leads to delayed object state updates and increased force feedback rendered at the clients. In this article, we analyze the effect of communication delay on the magnitude of the rendered forces at the clients for cooperative multi-user interactions with rigid objects. The analysis reveals guidelines on the tolerable communication delay. If this delay is exceeded, the increased force magnitude becomes haptically perceivable. We propose an adaptive force rendering scheme to compensate for this effect, which dynamically changes the stiffness used in the force rendering at the clients. Our experimental results, including a subjective user study, verify the applicability of the analysis and the proposed scheme to compensate the effect of time-varying communication delay in a multi-user SHVE.	centralized computing;client–server model;dynamical simulation;haptic technology;interaction;multi-user;server (computing);usability testing;virtual reality	Clemens Schuwerk;Xiao Xu;Rahul Gopal Chaudhari;Eckehard G. Steinbach	2015	TAP	10.1145/2835176	real-time computing;simulation;computer science;multimedia;collaboration	Visualization	-41.30222708870387	-49.81893752872352	93528
1ff1d2912fb01e2787f7c3eac9e57aa5e4fa2a49	poster: spatial misregistration of virtual human audio: implications of the precedence effect	mouth;h 5 1 information systems multimedia information systems artificial;audio signal processing;electronic mail;virtual characters;ventriloquism effect;mixed reality characters;virtual reality;virtual human;precedence effect;virtual human audio;virtual human display systems virtual human audio precedence effect spatial misregistration mixed reality characters ventriloquism effect viewing distances stereo collapse stereo loudspeakers sound source;multimedia information system;data communication;three dimensional;augmented;character projection;stereo loudspeakers;input output;visualization;b 4 2 input output and data communications input output devices voice h 5 1 information systems multimedia information systems artificial augmented and virtual realities;loudspeakers;virtual human display systems;three dimensional displays;virtual reality audio signal processing hi fi equipment loudspeakers;and virtual realities;hi fi equipment;h 5 1 information systems multimedia information systems artificial augmented and virtual realities;humans;b 4 2 input output and data communications input output devices voice;information system;stereo collapse;sound source;mixed reality;spatial misregistration;loudspeakers humans virtual reality visualization electronic mail mouth three dimensional displays;viewing distances	Virtual humans are often presented as mixed reality characters projected onto screens that are blended into a physical setting. Stereo loudspeakers to the left and right of the screen are typically used for virtual human audio. Unfortunately, stereo pairs can produce an effect known as precedence, which causes users standing close to a particular loudspeaker to perceive a collapse of the stereo sound to that singular loudspeaker. We studied if this effect might degrade the presentation of a virtual character, or if this would be prevented by the ventriloquism effect. Our results demonstrate that from viewing distances common to virtual human scenarios, a movement equivalent to a single stride can induce a stereo collapse, creating conflicting perceived locations of the virtual human's voice. Users also expressed a preference for a sound source collocated with the virtual human's mouth rather than a stereo pair. These results provide several design implications for virtual human display systems.	covox speech thing;loudspeaker;mixed reality;precedence effect;virtual actor	David M. Krum;Evan A. Suma;Mark T. Bolas	2012	2012 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2012.6184204	computer vision;simulation;computer science;multimedia	Visualization	-43.31393526435453	-47.3007919389721	93637
0af08d4d009d43dda388556003c9c636c034a33f	a haptic memory game using the stress2 tactile display	memory game;tactile display;tactile graphics	A computer implementation of a classic memory card game was adapted to rely on touch rather than vision. Instead of memorizing pictures on cards, players explore tactile graphics on a computer-generated virtual surface. Tactile sensations are created by controlling dynamic, distributed lateral strain patterns on a fingerpad in contact with a tactile display called STRESS2. The tactile graphics are explored by moving the device within the workspace of a 2D planar carrier. Three tactile rendering methods were developed and used to create distinct tactile memory cards. The haptic memory game showcases the capabilities of this novel tactile display technology.	computer-generated holography;display device;graphics;lateral thinking;memory card;workspace	Qi Wang;Vincent Lévesque;Jérôme Pasquero;Vincent Hayward	2006		10.1145/1125451.1125510	computer vision;computer science;computer graphics (images)	HCI	-43.45118012438111	-40.12232880348473	93682
a0ffe51190d128f9d7a5f41e3920b907610068b2	3-d perception enhancement in autostereoscopic tv by depth cue for 3-d model interaction	tv solid modeling three dimensional displays conferences consumer electronics spatial resolution sensitivity;hvs 3d perception enhancement autostereoscopic 3d tv depth cue 3d model interaction system human visual system theory;3 d interaction autostereoscopic tv depth cue;consumer electronics;sensitivity;three dimensional displays;visual perception graphical user interfaces human computer interaction image enhancement solid modelling stereo image processing three dimensional television;solid modeling;tv;conferences;spatial resolution	Autostereoscopic TV provides users 3-D experience without wearing glasses. To overcome its poor resolution and to improve the 3-D perception, in this paper, based on Human Visual System (HVS) theory, some depth cues were added into the proposed system. It is shown in the result that the viewers can perceive the 3-D models twice as much as the conventional one. A 3-D model interaction system was built with the proposed framework.	3d computer graphics;3d modeling;autostereoscopy;computer monitor;depth perception;human visual system model	Yi-Ting Shen;Guan-Lin Liu;Sih-Sian Wu;Liang-Gee Chen	2016	2016 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2016.7430613	computer vision;image resolution;sensitivity;multimedia;solid modeling;computer graphics (images)	Robotics	-40.98370015273393	-40.58499159658327	93799
742e791b9430a1d68267a4a4fffb5afac2a38ffb	my tai-chi book: a virtual-physical social network platform	human interaction;tai chi;pervasive computing;sensor network;wireless sensor network;social network;motion recognition;human motion;body sensor network	While social networks on Web platforms have attracted a lot of interests, including more natural and physical inputs into such systems to enhance human interaction is desirable. Using body-area sensor networks (BSNs) to capture human motions opens up an opportunity toward this goal. These motivate us to design a novel virtual-physical social network platform1 with typical social network functions and capable of receiving various inputs from remote BSNs. Through our platform, users can share conventional messages and images as well as sensory data in several interesting ways. We demonstrate a Tai-Chi exercise social network and some testing results.	chi;social network	Fang-Jing Wu;Chen-Shao Huang;Yu-Chee Tseng	2010		10.1145/1791212.1791289	embedded system;simulation;intelligent computer network;wireless sensor network;computer science;artificial intelligence;network simulation;key distribution in wireless sensor networks;ubiquitous computing;visual sensor network	Security	-47.22153060483854	-38.55945180138387	94019
3dda66ce9f3b402f524d34a85ee9fb49c83c99d2	perceptual evaluation of space in virtual environments		Floor plan designs and their spatial analysis are typically constrained to blueprints and 2D projections of 3D models. Computing appropriate spatial measures from such representations provides a standard way of quantifying important aspects of the design. We wish to investigate whether a personu0027s perceptual exploration of a space would agree with such spatial measures, that is, whether a person can roughly infer such measures by exploring a space. We perform two studies, one involving novices and the other experts. First, we conduct a perceptual study to discover whether a novice useru0027s perception of spatial measures depends on the mode used to explore the space. Our analysis considers three spatial measures, grounded in Space-Syntax, that characterize key aspects of a design such as visibility, accessibility, and organization. We compare three modes of exploration: 2D blueprints, first-person view in a 3D simulation, and a 3D virtual reality simulation with teleportation. A correlation analysis between the usersu0027 perceptual ratings and the spatial measures, indicates that virtual reality is the most effective of the three methods, while 2D blueprints and 3D first-person exploration often fail entirely to convey the spatial measures. In the second study, experts are asked to evaluate and rank the design blueprints for each measure. The expert observations are in strong agreement with the spatial measures for accessibility and organization, but not for visibility in some cases. This indicates that even experts have difficulty understanding spatial aspects of an architecture design from 2D blueprints alone.	3d floor plan;3d modeling;accessibility;blueprint;color vision;hoc (programming language);marginal model;max;parsing;perceptual robotics;simulated reality;simulation;singular value decomposition;spatial analysis;virtual reality;visibility graph;visual basic[.net]	Muhammad Usman;M. Brandon Haworth;Glen Berseth;Mubbasir Kapadia;Petros Faloutsos	2017		10.1145/3136457.3136458	simulation;architecture;computer science;artificial intelligence;computer vision;perception;floor plan;virtual reality;teleportation;blueprint	HCI	-43.61496997051781	-48.86475306182351	94058
19b86125a2e1f3233bd5985e300293672e6c2b4e	navigation methods for an augmented reality system	groupware;ubiquitous;tangible;interaction;navigation;viewpoint control;bricks;design;augmented reality	BUILD-IT is a planning tool based on computer vision technology, supporting complex planning and composition tasks. A group of people, seated around a table, interact with objects in a virtual scene using real bricks. A plan view of the scene is projected onto the table, where object manipulation takes place. A perspective view is projected on the wall. The views are set by virtual cameras, having spatial attributes like shift, rotation and zoom. However, planar interaction with bricks provides only position and rotation information. This paper explores two alternative methods to bridge the gap between planar interaction and three-dimensional navigation.	augmented reality;computer vision	Morten Fjeld;Fred Voorhorst;Martin Bichsel;Helmut Krueger;Matthias Rauterberg	2000		10.1145/633292.633298	computer vision;design;augmented reality;navigation;interaction;simulation;human–computer interaction;computer science;multimedia;management;ubiquitous computing	Vision	-44.09102979969292	-39.0712244173987	94428
dd0da67b83e3c88baea9eef4c4f6e62bcdf3be34	tangible media control system for intuitive interactions with multimedia contents	sensibilidad contexto;context awareness;interfase usuario;tecnologia electronica telecomunicaciones;context aware;realite virtuelle;realidad virtual;systeme multimedia;user interface;personalized contents;virtual reality;tangible user interface;multimedia systems;interactive media controller;control system;interface haptique;interface utilisateur;sensibilite contexte;tecnologias;haptic interfaces;grupo a	In this paper, we present the Tangible Media Control System (TMCS), which allows users to manipulate media contents through physical objects in an intuitive way. Currently, most people access digital media contents by exploiting GUI. However, it only provides limited manipulation of the contents. The proposed system, instead of a mouse and a keyboard, adopts two types of tangible objects, i.e. a RFID-enabled object and a tracker-embedded object. The TMCS enables users to easily access and control digital media contents through tangible objects. In addition, it supports an interactive media controller which can be used to synthesize media contents according to users' taste. It also offers personalized contents, which suits users' preferences, by exploiting context such as the users' profile and situational information. Accordingly, the TMCS demonstrates that tangible interfaces with context can provide more effective interfaces to satisfy users' demands. Therefore, the proposed system can be applied to various interactive applications such as multimedia education, entertainment, multimedia editor, etc.		Sejin Oh;Woontack Woo	2006	IEICE Transactions	10.1093/ietisy/e89-d.1.53	human–computer interaction;computer science;control system;operating system;virtual reality;multimedia;user interface;world wide web	DB	-46.16398804093755	-38.94320746414115	94579
6c6b9fb724b3edfa6b823acd8484cf38a559661d	adaptive level of detail in dynamic, refreshable tactile graphics	task performance;haptic interfaces computer graphics;computer graphics;manuals haptic interfaces visualization presses skin educational institutions;user preferences;level of detail;visual impairment;haptic interfaces;concert hall illustration refreshable tactile graphic hierarchical spatial search task	We investigate gains in user appreciation and performance when the level of detail of tactile graphics is dynamically altered either at the press of a button or automatically, as a function of exploration speed. This concept was evaluated by asking 9 visually impaired participants to perform hierarchical spatial search tasks in a concert hall illustration. The tasks could be simplified by first searching for a section in a sparse illustration, and then a seat in a detailed illustration. The results show no improvement in task performance but indicate a user preference for explicitly controlling the level of details with the manual toggle.	columbia (supercomputer);digital recording;experiment;feature toggle;graphical user interface;graphics;interaction technique;level of detail;sparse matrix	Vincent Lévesque;Grégory Petit;Aude Dufresne;Vincent Hayward	2012	2012 IEEE Haptics Symposium (HAPTICS)	10.1109/HAPTIC.2012.6183752	human–computer interaction;computer science;multimedia;computer graphics (images)	Visualization	-43.81714834727056	-47.192563154820014	94592
754c4a614c3d4277f710f78052a19d80454d7932	a tactile-proprioceptive communication aid for users who are deafblind	sign language recognition;qualitative feedback tactile proprioceptive communication aid intervener deafblind individuals haptic gloves manual sign languages bimanual communication aid autosem display surface semaphores able bodied users;manuals games assistive technology gesture recognition sensors haptic interfaces mobile handsets;haptic interfaces;sign language recognition haptic interfaces	Users who are congenitally deafblind face major challenges in communicating with other people and often rely on an intervener with who they communicate with using a manual sign language. In recent years -to allow for deafblind individuals to communicate more independently- a number of haptic gloves have been developed that can recognize and/or convey manual sign languages. These gloves are expensive to construct and currently not commercially available. To address this issue we present a bimanual communication aid, called AUTOSEM that instead of using the hand as a display surface for manual signs, uses combinations of different orientations of both hands to define a set of semaphores that can represent an alphabet. A significant benefit of our technique is that it uses both hands and it can be implemented using low-cost motion sensing devices. User studies with fourteen able-bodied users evaluate both the output and input capabilities of our technique. A deafblind individual provided qualitative feedback on AUTOSEM using a case study.	haptic technology;ibm notes;input/output;orientation (graph theory);semaphore (programming);usability testing	Vinitha Khambadkar;Eelke Folmer	2014	2014 IEEE Haptics Symposium (HAPTICS)	10.1109/HAPTICS.2014.6775461	simulation;computer science;multimedia;communication	Visualization	-42.835307438333125	-43.973962613370965	94655
28ceda952f6b3e1a58ed611342bfee02c51bdbf6	usability testing of the interaction of novices with a multi-touch table in semi public space	intended action;additional gestures people;usability testing;multi-touch table;handheld device;touch-sensitive device;touch interaction;mobile phone;semi public space;bigger device;different gesture;virtual globe	Touch-sensitive devices become more and more common all-around. Many people use touch interaction, especially on small devices like iPhone or other mobile phones and handhelds. But do they really understand the different gestures, know which gesture is the correct one for the intended action and do they know how to transfer the gestures to bigger devices and surfaces? This paper reports about usability tests which are carried out in semi public space to explore peoples’ ability to find gestures to navigate on a virtual globe. The globe is presented on a multi-touch-table. Furthermore, it is studied which additional gestures people use intuitively to those ones which are implemented.	gesture recognition;mobile device;mobile phone;multi-touch;semiconductor industry;touchscreen;usability testing;virtual globe	Markus Jokisch;Thomas Bartoschek;Angela Schwering	2011		10.1007/978-3-642-21605-3_8	human–computer interaction;multimedia	HCI	-45.82882672191508	-43.31260655805031	94858
c34cbf26f19428fe77784dcd8a5a4170c551d7b0	perceptual attraction force: the sixth force	weight representation;shadow representation;motion tracking;indirect information;entertainment	We have utilized the nonlinear characteristics of human haptic perception to develop a handheld force feedback device; humans feel rapid acceleration more strongly than slow acceleration. We designed and built a crank-slider mechanism that creates a periodic prismatic motion with asymmetric acceleration (strong in one direction and weak in the other) leading to a ”virtual” force sensation. The prototype of the handheld device generates a sensation of unidirectional force; humans perceive a uni-directional force although the device physically generates a bi-directional force. By using a motor to pan the crank-slider mechanism, the device unit can create a virtual force in any arbitrary direction on a two dimensional plane. The direction of force can be controlled with angular sensors. We will present three applications of ”perceptual attraction force”; (1)attraction = magnet, (2)attraction = Newton’s apple, and (3)attraction = personal charm.	angularjs;crank (person);handheld game console;haptic technology;mobile device;newton;nonlinear system;prototype;sensor	Tomohiro Amemiya;Hideyuki Ando;Taro Maeda	2006		10.1145/1179133.1179160	computer vision;entertainment;simulation;mathematics	Robotics	-41.50585113139157	-43.168522746941484	94867
09fae6ba5132f2c17e8f9261277923b027009262	movers, shakers, and those who stand still: visual atten-tion-grabbing techniques in robot teleoperation	multi robot teleoperation;human robot interaction;attention	We designed and evaluated a series of teleoperation interface techniques that aim to draw operator attention while mitigating negative effects of interruption. Monitoring live teleoperation video feeds, for example to search for survivors in search and rescue, can be cognitively taxing, particularly for operators driving multiple robots or monitoring multiple cameras. To reduce workload, emerging computer vision techniques can automatically identify and indicate (cue) salient points of potential interest for the operator. However, it is not clear how to cue such points to a preoccupied operator -- whether cues would be distracting and a hindrance to operators -- and how the design of the cue may impact operator cognitive load, attention drawn, and primary task performance. In this paper, we detail our iterative design process for creating a range of visual attention-grabbing cues that are grounded in psychological literature on human attention, and two formal evaluations that measure attention-grabbing capability and impact on operator performance. Our results show that visually cueing on-screen points of interest does not distract operators, that operators perform poorly without the cues, and detail how particular cue design parameters impact operator cognitive load and task performance. Specifically, full-screen cues can lower cognitive load, but can increase response time; animated cues may improve accuracy, but increase cognitive load. Finally, from this design process we provide tested, and theoretically grounded cues for attention drawing in teleoperation.	computer vision;interrupt;iterative design;iterative method;point of interest;response time (technology);robot	Daniel J. Rea;Stela H. Seo;Neil D. B. Bruce;James Everett Young	2017	2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI	10.1145/2909824.3020246	human–robot interaction;computer vision;simulation;attention;computer science;artificial intelligence	Robotics	-47.47577503324366	-49.17246406849334	94935
8c89de940ad0d2b543bb3cd9dfc95d4642e4b4db	chairgest: a challenge for multimodal mid-air gesture recognition for close hci	performance evaluation;open challenge;hci;corpus;gesture spotting;algorithms;gesture recognition	In this paper, we present a research oriented open challenge focusing on multimodal gesture spotting and recognition from continuous sequences in the context of close human-computer interaction. We contextually outline the added value of the proposed challenge by presenting most recent and popular challenges and corpora available in the field. Then we present the procedures for data collection, corpus creation and the tools that have been developed for participants. Finally we introduce a novel single performance metric that has been developed to quantitatively evaluate the spotting and recognition task with multiple sensors.	gesture recognition;human–computer interaction;multimodal interaction;sensor;text corpus	Simon Ruffieux;Denis Lalanne;Elena Mugellini	2013		10.1145/2522848.2532590	speech recognition;human–computer interaction;computer science;gesture recognition;text corpus;multimedia	HCI	-36.89782848724597	-45.275821773509975	94999
bcb5c49ba9e82a61ea457b732018f2531b157b9f	haptic invitation of textures: perceptually prominent properties of materials determine human touch motions	probabilistic relationships textures haptic invitation human touch motions sensory evaluations bayesian network model;standards;surface roughness;material property;materials;rough surfaces;touch behavior;materials haptic interfaces weaving rough surfaces surface roughness standards correlation;textural prominence;weaving;haptic interfaces bayes methods;correlation;haptic interfaces;textural perception	In daily life, certain textures and materials invite our touch motions. To seek the nature of such haptic invitation, we conducted a series of experiments consisting of sensory evaluations and ranking tasks for 36 materials to ascertain their perceptual properties and their degrees of haptic invitation. In addition, we recorded the human touch motions elicited by these materials. The results showed high degrees of haptic invitation for materials with perceptually prominent textures, which indicates that such textures frequently invite human touch motions. We also developed a Bayesian network model that represented the probabilistic relationships between invited touch motions and the properties of textures. The model substantiated the observation that different types of textural prominence led to different types of invited touch motions. These results collectively suggest that materials with prominent textures frequently encourage humans to touch them, using appropriate or specified touch motions.	bayesian network;coefficient;evaluation;experiment;haptic device component;haptic technology;motion;network model;prominence of facial canal;tactile graphic	Hikaru Nagano;Shogo Okamoto;Yoji Yamada	2014	IEEE Transactions on Haptics	10.1109/TOH.2014.2321575	computer vision;surface roughness;engineering;multimedia;correlation;weaving	Visualization	-43.430495518623964	-51.25765694888829	95024
27a3a54399e1d3991bbe17c2809319658c0464cc	g-raff: an elevating tangible block for spatial tabletop interaction	actuated displays;interface devices;spatial interaction;tangible interface;user interfaces	This video introduces an elevating tangible block, G-raff which supports spatial interaction in a tabletop computing environment. The elevating head part of G-raff moves according to the given height and angle data. The two rollable metal tape structures creates large movements with a small volume block. Design details, key features and applications are introduced in the video	compact cassette	Chang-Min Kim;Tek-Jin Nam	2015		10.1145/2702613.2732488	simulation;human–computer interaction;computer graphics (images)	HCI	-43.004944393219176	-40.044911592120734	95104
fa85ee130bc64a3d57f0d38ddb866830e93a7115	change blindness phenomena for virtual reality display systems	blindness visualization virtual reality observers humans context visual perception;virtual reality;virtual reality change blindness stereoscopic display;indexing terms;observers;stereoscopic vision;visualization;blindness;vision defects;visual perception natural scenes stereo image processing three dimensional displays virtual reality vision defects;three dimensional displays;stereo image processing;monoscopic viewing conditions change blindness virtual reality display systems visual perception visual scene computer generated imagery visual disruptions stereoscopic vision;change blindness;virtual reality environment;visual perception;humans;saccadic eye movement;stereoscopic display;context;natural scenes;head mounted display	In visual perception, change blindness describes the phenomenon that persons viewing a visual scene may apparently fail to detect significant changes in that scene. These phenomena have been observed in both computer-generated imagery and real-world scenes. Several studies have demonstrated that change blindness effects occur primarily during visual disruptions such as blinks or saccadic eye movements. However, until now the influence of stereoscopic vision on change blindness has not been studied thoroughly in the context of visual perception research. In this paper, we introduce change blindness techniques for stereoscopic virtual reality (VR) systems, providing the ability to substantially modify a virtual scene in a manner that is difficult for observers to perceive. We evaluate techniques for semiimmersive VR systems, i.e., a passive and active stereoscopic projection system as well as an immersive VR system, i.e., a head-mounted display, and compare the results to those of monoscopic viewing conditions. For stereoscopic viewing conditions, we found that change blindness phenomena occur with the same magnitude as in monoscopic viewing conditions. Furthermore, we have evaluated the potential of the presented techniques for allowing abrupt, and yet significant, changes of a stereoscopically displayed virtual reality environment.	blinking;color vision;computer-generated holography;depth perception;eye movements;head-mounted display;movement;night blindness;saccades;stereopsis;stereoscopy;vr - veterans rand health survey;virtual reality;observers	Frank Steinicke;Gerd Bruder;Klaus H. Hinrichs;Pete Willemsen	2011	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2011.41	computer vision;visualization;index term;change blindness;visual perception;computer science;stereopsis;optical head-mounted display;virtual reality;multimedia;computer graphics (images)	Visualization	-42.83455637777583	-49.99931172337982	95111
81499bdbcde8360df77aa143aab61b0838d8b3cd	using machine learning to blend human and robot controls for assisted wheelchair navigation	robot sensing systems wheelchairs navigation mobile robots mathematical model aerospace electronics;doorway safe traversal human controls robot controls assisted wheelchair navigation collaborative control assistive semi autonomous wheelchair statistical machine learning technique shared control powered wheelchairs doorway navigation;wheelchairs handicapped aids learning artificial intelligence navigation robots safety statistical analysis;navigation;handicapped aids;statistical analysis;robots;safety;learning artificial intelligence;wheelchairs	This work presents an algorithm for collaborative control of an assistive semi-autonomous wheelchair. Our approach is based on a statistical machine learning technique to learn task variability from demonstration examples. The algorithm has been developed in the context of shared-control powered wheelchairs that provide assistance to individuals with impairments that affect their control in challenging driving scenarios, like doorway navigation. We validate our algorithm within a simulation environment, and find that with relatively few demonstrations, our approach allows for safe traversal of the doorway while maintaining a high level of user control.	algorithm;autonomous robot;heart rate variability;high-level programming language;machine learning;navigation;power (psychology);robot control;sample variance;semiconductor industry;simulation;usability testing;user interface;motorized wheelchair	Aditya Goil;Matthew Derry;Brenna Argall	2013	2013 IEEE 13th International Conference on Rehabilitation Robotics (ICORR)	10.1109/ICORR.2013.6650454	control engineering;computer vision;simulation;engineering;mobile robot navigation	Robotics	-40.6324201385397	-47.87990104231147	95563
af5e8bfc21ed0b13406f6f99edbd469cdda7e83e	attending and observing robot for crutch users		Improper usages of crutches can cause a secondary accident like a falling. In this paper, an instruction robot for crutch walk training is introduced. This robot moves along a walking crutch user, and it measures his/her body parts motions. Based on the measurements, the robot provides advices to the crutch user for proper walk motions. As a result, the crutch user can review his/her own walk motions. It is expected that crutch walk training with this robot will improve the crutch user»s walk motions, and it will decrease the possibility of accidents.	robot	Naoaki Tsuda;Susumu Tarao;Yoshihiko Nomura;Norihiko Kato	2018		10.1145/3173386.3176968	simulation;human–computer interaction;robot;computer science;crutch	Robotics	-46.75095927595648	-50.18992631546415	95694
90d80de957eca575f5e22cfcb795b0bc67a1bdfa	automatic creation of magazine-page-like social media visual summary for mobile browsing	mobile browsing magazine page design visual summary info maximization aesthetically appealing;feeds;media;visualization;image color analysis;mobile communication;media visualization mobile communication feeds image color analysis visual perception videos;visual perception;videos	Today, mobile users are struggling with accessing overloading and unstructured social media feeds on the severely constrained mobile display. To overcome the challenges associated with browsing social media feeds on mobile devices, we are developing an innovative scheme to automatically create and synthesize the mixed social media digest (pictures, texts and videos) into a magazine-page-like social media visual summary. Given a set of personalized social media digest, a multi-objective optimization is formulated to organize the digest into visual summary in a 9-block-partition fashion with consideration of informative delivery, aesthetic rules and visual perception principles. Each block will be optimized interactively in terms of size, position and color to best represent the overall social media digest. Extensive evaluation and analysis based on user studies demonstrate that the proposed approach is effective in presenting social media content in a visually appealing and compact way. It is expected that this visual summary will lead to much enhanced user experiences for browsing social media digest on mobile devices.	color;cryptographic hash function;display size;function overloading;information;interactivity;mathematical optimization;maximal set;mobile device;mobile phone;multi-objective optimization;personalization;social media;usability testing;user experience	Shuang Ma;Chang Wen Chen	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532401	computer vision;media;visualization;mobile telephony;visual perception;computer science;multimedia;world wide web;electronic media	HCI	-35.11638370246845	-48.76490370374633	95717
12e27a9f2da15193cab775bc8e089ba9019c63f5	retrospective cued recall: a method for accurately recalling previous user behaviors	natural user behavior unintrusively;retrospective cued recall method;retrospective cued recall;user behaviors;data loggers;user study;user behavior;previous user behaviors;participant screen;retrospective recall;long period;month long period;disruptive action;data handling;passive longitudinal studies;passive user observations;passive user observation	A common problem in many user studies is gathering natural user behavior unintrusively over a long period of time. We describe a methodology for conducting passive longitudinal studies, where the participant is able to go about their daily routine without taking any disruptive action such as writing a diary entry or responding to an interruption. Although passive user observations have been done through log analysis, our retrospective cued recall (RCR) method recorded participant screens over a month long period linked to logged trigger events, allowing for a later review that was is guided by retrospective recall as cued by the images of screen captures. We find this method of gathering user behaviors to be remarkably accurate, despite fairly lengthy delays between action and recall.	diary studies;interrupt;log analysis;precision and recall;retro city rampage;usability testing	Daniel M. Russell;Mike Oren	2009	2009 42nd Hawaii International Conference on System Sciences	10.1109/HICSS.2009.885	simulation;computer science;world wide web	HCI	-36.60804999643831	-50.276984620919755	96022
3c592dfa1fb92dda139cee0eca5388485a67c5d5	situated multi-modal dialog system in vehicles	spoken dialog systems;face direction estimation;multi modal interaction	In this paper, we address Townsurfer, a situated multi-modal dialog system in vehicles. The system integrates multi-modal inputs of speech, geo-location, gaze (face direction) and dialog history to answer drivers' queries about their surroundings. To select appropriate data source used to answer queries, we apply belief tracking across the above modalities. We conducted a preliminary data collection and an evaluation focusing on the effect of gaze (head irection) and geo-location estimations. We report the result and analysis on the data.	dialog system;geolocation;modal logic;situated	Teruhisa Misu;Antoine Raux;Ian Lane;Joan Devassy;Rakesh Gupta	2013		10.1145/2535948.2535951	natural language processing;speech recognition;computer science;dialog system;communication	NLP	-34.490184071726816	-41.838546301253935	96201
7fd35ec726d4ea742aa0222247a8f0cf56db1a54	performance measures of game controllers in a three-dimensional environment	performance measure;input device;performance evaluation;three dimensional;video game;performance metric;h 5 2 user interfaces;human factors;evaluation methodology;video game devices;computer game devices;performance measurement;target tracking;target tracking tasks;computer game;user machine systems	Little work exists on the testing and evaluation of computer-game related input devices. This paper presents five new performance metrics and utilizes two tasks from the literature to quantify differences between input devices in constrained threedimensional environments, similar to “first-person”-genre games. The metrics are Mean Speed Variance, Mean Acceleration Variance, Percent View Moving, Target Leading Analysis, and Mean Time-to-Reacquire. All measures are continuous, as they evaluate movement during a trial. The tasks involved tracking a moving target for several seconds, with and without target acceleration. An evaluation between an X-Box gamepad and a standard PC mouse demonstrated the ability of the metrics to help reveal and explain performance differences between the devices. CR Categories: H.1.2 [User/Machine Systems]: Human Factors; H.5.2 [User Interfaces]: Evaluation Methodology.	analog stick;category theory;cognitive dimensions of notations;computation;displacement mapping;first-order predicate;game controller;gamepad;human factors and ergonomics;input device;interaction;mean squared error;milgram experiment;pc game;pointing device;radek maneuver;throughput;trusted computer system evaluation criteria;user interface;velocity (software development)	Chris Klochek;I. Scott MacKenzie	2006		10.1145/1143079.1143092	performance measurement;three-dimensional space;real-time computing;simulation;computer science;human factors and ergonomics;operating system;multimedia;input device	HCI	-46.6766048776071	-47.86044151658743	96229
001a3342cf9ad7b70129709fffa1b8c204a1eede	a touch sensitive keypad layout for improved usability of smartphones for the blind and visually impaired persons		Blind users face a number of challenges in performing common operations of text-entry, text selection, and text manipulation on smartphones. The existing keypad layouts make it difficult for the users to easily operate a touch screen device even for entry-level activities. This necessitates the need for customizing the current keypad and dialer to enable a blind user to perform common activities of making a call, sending and receiving SMS messages and e-mails and browsing internet without visual feedback. Based on our prior study on screen division layouts, this paper proposes and evaluates a dialer and keyboard for blind users of a smartphone. The proposed keypad was tested on selected groups of blind users from both the countries where the research was performed. They were initially trained on using the proposed keypad. Their experiences were then recorded using interviews and observation. The responses were then tested and analyzed using standard statistical tests. The results were then compared with the existing ordinary and QWERTY keypads. These results show that the proposed keypad and dialer has a gentle learning curve and results in minimum typing errors thus reducing cognitive load on the blind user.	smartphone;usability	Badam Niazi;Shah Khusro;Akif Khan;Iftikhar Alam	2016		10.1007/978-3-319-33625-1_38	embedded system;internet privacy;world wide web	HCI	-48.17410917925072	-44.66467942417494	96277
24926c6efef09e7a64942e9464c01778a5f7be01	a semi-autonomous interactive robot	semi-autonomous interactive robot	Human-Robot Interaction is a dynamic and expanding research field. This paper presents the creation and concepts of a semi-autonomous interactive robot, TARO. TARO is designed to humanoid in appearance, and can entertain and interact with people through verbal communication and body language.	autonomous robot;human–robot interaction;interactivity;semiconductor industry	Brian Schlesinger;Michael Mensch;Christopher Rindosh;Joe Votta;Yunfeng Wang	2006			mobile robot;robot learning;social robot;robot control;mobile robot navigation;personal robot	Robotics	-37.870563738218095	-38.70104930506549	96445
b2d034c7b66715d499bf0527894cc60bc58aeadc	visibility experiment and evaluation of 3d character representation on mobile displays		In the experimental study, we measured visibility and readability of text characters presented on a small 3D liquid crystal display (LCD) and evaluated features of the 3D character representation for application to mobile devices. For the visibility evaluation, we focused on time lag for first recognition, time required for full reading, and maximum distance of 3D objects popping out from the fixed display. We also report the dependency of the results on the age of subjects in comparison with 2D and 3D representations.	experiment;liquid-crystal display;mobile device	Hiromu Ishio;Shunta Sano;Tomoki Shiomi;Tetsuya Kanda;Hiroki Hori;Keita Uemoto;Asei Sugiyama;Minami Niwa;Akira Hasegawa;Shohei Matsunuma;Masaru Miyao	2011		10.1007/978-3-642-22095-1_10	computer vision;multimedia;computer graphics (images)	HCI	-47.10751377606754	-43.77820630573929	96660
7c42d5d7b9d06db25b1a85336a206274b890ab13	design and evaluation of a head-mounted display for immersive 3d teleoperation of field robots	user study;human factors;urban search and rescue;stereopsis;article;teleoperation;head mounted display	This paper describes and evaluates the use of a head-mounted display (HMD) for the teleoperation of a field robot. The HMD presents a pair of video streams to the operator (one to each eye) originating from a pair of stereo cameras located on the front of the robot, thus providing him/her with a sense of depth (stereopsis). A tracker on the HMD captures 3-DOF head orientation data which is then used for adjusting the camera orientation by moving the robot and/or the camera position accordingly, and rotating the displayed images to compensate for the operator’s head rotation. This approach was implemented in a search and rescue robot (RAPOSA), and it was empirically validated in a series of short user studies. This evaluation involved four experiments covering two-dimensional perception, depth perception, scene perception, and performing a search and rescue task in a controlled scenario. The stereoscopic display and head tracking are shown to afford a number of performance benefits. However, one experiment also revealed that controlling robot orientation with yaw input from the head tracker negatively influenced task completion time. A possible explanation is a mismatch between the abilities of the robot and the human operator. This aside, the studies indicated that the use of an HMD to create a stereoscopic visualization of the camera feeds from a mobile robot enhanced the perception of cues in a static three-dimensional environment and also that such benefits transferred to simulated field scenarios in the form of enhanced task completion times.	augmented reality;cognitive science;control system;depth perception;display device;experiment;head-mounted display;human reliability;information;machine perception;mobile robot;motion capture;raposa;rescue robot;robotics;seamless3d;sensor;stereo camera;stereo cameras;stereopsis;stereoscopy;streaming media;yaws	Henrique Martins;Ian Oakley;Rodrigo Ventura	2015	Robotica	10.1017/S026357471400126X	computer vision;teleoperation;simulation;computer science;engineering;stereopsis;artificial intelligence;human factors and ergonomics;optical head-mounted display;computer graphics (images);mechanical engineering	Robotics	-44.59908435180931	-48.10857396667642	96667
6a9ae21fb46c792baa4111a5e33eec87bb6c2386	a novel distributed functional electrical stimulation and assessment system for hand movements using wearable technology	edison compute module;rehabilitation;wearable technology;smartphone based;electromyography;functional electrical stimulation fes;assessment;stroke	In this paper, a novel distributed functional electrical stimulation (FES) and assessment system for hand movement using wearable technology is proposed. It consists of an FES of wireless EMG-bridge (EMGB) type, a sensor glove and a smartphone-based application software for displaying the virtual 3D hand environment. Combined with the FES therapy and the sensor glove, the proposed system provides a way to evaluate individual responses to rehabilitation among the stroke patients. The smartphone-based application software for displaying the virtual 3D hand environment has been applied to enhance the engagement and motivation needed to drive neuroplastic changes. The prototype system has been validated by performing wrist and finger extension on one healthy subject.	diode bridge;electromyography;functional electrical stimulation;prototype;shin megami tensei: persona 3;smartphone;wearable technology	Hai-Peng Wang;Ai-Wen Guo;Zhengyang Bi;Yu-Xuan Zhou;Zhi-Gong Wang;Xiao-Ying Lü	2016	2016 IEEE Biomedical Circuits and Systems Conference (BioCAS)	10.1109/BioCAS.2016.7833728	simulation;physical medicine and rehabilitation;engineering;physical therapy	Visualization	-41.5704853086239	-44.30120632240985	96763
197b5e4d7d044cba13db5e59dd2819d28436192f	the influence of audio quality on the popularity of music videos: a youtube case study	overall listening experience;popularity;audio quality	Video-sharing websites like YouTube contain many music videos. On such websites, the audio quality of these music videos can differ from poor to very good since the content is uploaded by users. The results of a previous study indicated that music videos are very popular in general among the users. This paper addresses the question whether the audio quality of music videos has an influence on user ratings. A generic system for measuring the audio quality on video-sharing websites is described. The system has been implemented and was deployed for evaluating the relationship between audio quality and video ratings on YouTube. The analysis of the results indicate that, contrary to popular expectation, the audio quality of music videos has surprisingly little influence on its appreciation by the YouTube user.	sound card;sound quality	Michael Schoeffler;Jürgen Herre	2014		10.1145/2661714.2661725	psychology;multimedia;advertising;internet privacy	Web+IR	-34.033668824121285	-47.59475129137997	96817
5599cdfad30fdb74ab58159714ffb17e9c94c97e	occlusion handling in augmented reality system for human-assisted assembly task	occlusion;depth image;virtual assembly;augmented reality;human machine interaction	Augmented reality (AR) based human-machine interaction (HMI) provided a seamless interface between user and application environment, but occlusion handing remained as a tough problem in AR applications. Recently most AR occlusion handling algorithms aimed at general environment, more special research on AR occlusion handling for application in the context of industrial assembly, such as assembly training and assembly task guidance, is required. An occlusion handling method aimed at video see-through AR-based assembly system is presented based on the analysis of the occlusion between virtual part, visual assembly feature, navigation information, physical part and physical assembly environment in the context of virtual/physical assembly working space. The method was implemented in prototype AR-assembly system and was proved to be efficient in handling the virtual/physical occlusion in augmented assembly scene.	augmented reality	Chi Xu;Shiqi Li;Junfeng Wang;Tao Peng;Ming Xie	2008		10.1007/978-3-540-88518-4_14	computer vision;augmented reality;simulation;computer science;engineering;computer graphics (images)	HCI	-40.385355181673226	-38.26550376985771	97014
681a62b6b7cbd8c903aec23f9aa5a9375369058f	towards privacy-preserving activity recognition using extremely low temporal and spatial resolution cameras	image sensors image motion analysis image resolution;reliability;avatars cameras spatial resolution mobile communication gray scale reliability;gray scale;mobile communication;avatars;correct classification rate privacy preserving activity recognition extremely low temporal resolution camera spatial resolution camera action recognition graphics engine kinect v2 sensor smart room centric gestures;cameras;spatial resolution	Although extensive research on action recognition has been carried out using standard video cameras, little work has explored recognition performance at extremely low temporal or spatial camera resolutions. Reliable action recognition in such a “degraded” environment would promote the development of privacy-preserving smart rooms that would facilitate intelligent interaction with its occupants while mitigating privacy concerns. This paper aims to explore the trade-off between action recognition performance, number of cameras, and temporal and spatial resolution in a smart-room environment. As it is impractical to build a physical platform to test every combination of camera positions and resolutions, we use a graphics engine (Unity3D©) to simulate a room with various avatars animated using motions captured from real subjects with a Kinect v2 sensor. We study the performance impact of spatial resolutions from a single pixel up-to 10×10 pixels, the impact of temporal resolutions from 2 Hz up-to 30 Hz and the impact of using up-to 5 ceiling cameras. We found that reliable action recognition for smart-room centric gestures can still occur in environments with extremely low temporal and spatial resolutions. When using 5, single-pixel cameras at 30Hz we achieved a correct classification rate (CCR) of 75.70% across 9 actions, only 13.9% lower than the CCR for the same camera setup at 10×10 pixels. We also found that, in terms of the impact on action recognition performance, spatial resolution has the highest impact, followed by number of cameras, and temporal resolution (frame rate).	activity recognition;algorithm;best, worst and average case;game engine;graphics;image resolution;kinect;local consistency;pixel;privacy;simulation;unity;usability	Ji Dai;Jonathan Wu;Behrouz Saghafi;Janusz Konrad;Prakash Ishwar	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2015.7301356	computer vision;simulation;image resolution;mobile telephony;computer science;reliability;grayscale;statistics;computer graphics (images)	Vision	-38.755909298627905	-40.883587582523106	97296
3ce4e32903b78ad642a63df49eea837dcb17ac60	swifter: design and evaluation of a speech-based text input metaphor for immersive virtual environments	microphones;keyboards;virtual reality graphical user interfaces speech based user interfaces;h 5 2 information interfaces and presentation user interfaces graphical user interfaces;speech;virtual environments;websearch;h 5 2 information interfaces and presentation user interfaces graphical user interfaces i 3 7 computer graphics three dimensional graphics and realism virtual reality;speech recognition;i 3 7 computer graphics three dimensional graphics and realism virtual reality;cave like virtual environment swifter speech based text input metaphor immersive virtual environments data annotation process speech based multimodal text entry system;publications database;user interfaces;context;rwth publications;speech recognition keyboards speech user interfaces virtual environments context microphones	Text input is an important part of the data annotation process, where text is used to capture ideas and comments. For text entry in immersive virtual environments, for which standard keyboards usually do not work, various approaches have been proposed. While these solutions have mostly proven effective, there still remain certain shortcomings making further investigations worthwhile. Motivated by recent research, we propose the speech-based multimodal text entry system SWIFTER, which strives for simplicity while maintaining good performance. In an initial user study, we compared our approach to smartphone-based text entry within a CAVE-like virtual environment. Results indicate that SWIFTER reaches an average input rate of 23.6 words per minute and is positively received by users in terms of user experience.	a/ux;multimodal interaction;smartphone;usability testing;user experience;virtual reality;words per minute	Sebastian Pick;Andrew S. Puika;Torsten Kuhlen	2016	2016 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2016.7460039	speech recognition;human–computer interaction;computer science;multimedia	Visualization	-47.23287169491165	-44.97921563904548	97363
dea62730b2ff56b2507075ac0039601735509502	reaching to sound accuracy in the peri-personal space of blind and sighted humans	sound localization;assistive device;blindness;augmented reality	With the aim of designing an assistive device for the Blind, we compared the ability of blind and sighted subjects to accurately locate several types of sounds generated in the peri-personal space. Despite a putative lack of calibration of their auditory system with vision, blind subjects performed with a similar accuracy as sighted subjects. The average error was sufficiently low (10° in azimuth and 10 cm in distance) to orient a user towards a specific goal or to guide a hand grasping movement to a nearby object. Repeated white noise bursts of short duration induced better performance than continuous sounds of similar total duration. These types of sound could be advantageously used in an assistive device. They would provide indications about direction to follow or position of surrounding objects, with limited masking of environmental sounds, which are of primary importance for the Blind.	humans	Marc J.-M. Macé;Florian Dramas;Christophe Jouffrais	2012		10.1007/978-3-642-31534-3_93	computer vision;simulation;engineering;communication	HCI	-45.97819562300114	-49.64863309100118	97419
6fc760c73fb5966dd51085f16d390a71d751835b	meet ar-bot: meeting anywhere, anytime with movable spatial ar robot		Many kinds of preparations are needed when meeting. For example, projector, laptop, cables and ETC. As such, this video have constructed Meet AR-bot, which helps users to keep meeting going smoothly, based on the projection of Augmented Reality(AR). Our system can easily provide meeting room environment through the movable setting via wheel-based stand. Users do not need to carry a personal laptop and connect them to the projector. Robot reconstructs the 3D geometry information through pan-tilt system and compute projection areas to project information in the space. Users can also control through mobile devices. We offer presentation, table interaction, file sharing and virtual object registration by mobile device.	anytime algorithm;augmented reality;file sharing;laptop;mobile device;robot;smoothing;video game bot;video projector;wheel (unix term)	Yoonjung Park;Yoonsik Yang;Hyocheol Ro;Junghyun Byun;Alexandr Nagorny;Tack-Don Han	2018		10.1145/3240508.3241390	virtual image;laptop;multimedia;projector;robot;mobile device;file sharing;augmented reality;computer science	HCI	-43.90720020951864	-40.20533303224679	97430
9fb410dd6f85f3211a40549887a9d105632a694f	thunderpunch: a bare-hand, gesture-based, large interactive display interface with upper-body-part detection in a top view		We present a new bare-hand gesture interface for large-screen interaction in which multiple users can participate simultaneously and interact with virtual content directly. To better reflect the intent of our new interface, we have created a new type of hardware system with a large hybrid display, named ThunderPunch. Unlike the conventional method, which involves positioning the camera in front, the cameras are mounted on the ceiling so that they avoid covering the large screen. To achieve bare-hand interaction in this hardware structure, we propose real-time algorithms that detect multiple body poses and recognize punching and touching gestures from top-view depth images. A pointing and touching test shows that the proposed algorithm is usable and that it outperforms other algorithms. In addition, we created a game to make the best use of the proposed system.	application program interface;gesture recognition;interface device component;multi-user;name;new type;other named variants of lymphosarcoma and reticulosarcoma, lymph nodes of head, face, and neck;real-time clock;algorithm	Daehwan Kim;Hye-mi Kim;Yuka Terauchi;Deb A;Ki-Suk Lee;Ki-Hong Kim	2018	IEEE Computer Graphics and Applications	10.1109/MCG.2018.053491734	computer vision;usable;punching;artificial intelligence;upper body part;computer graphics;ceiling (aeronautics);gesture;computer science	Visualization	-43.95845029521246	-42.91730722234886	97481
21c6e3eb886a20e20269b1acab1633636b23a50d	demo: real-time object tagging and retrieval	real time computer vision;object tagging;augmented reality	We propose an augmented reality system on off-the-shelves smartphones which allows random physical object tagging. At later times, such tags could be retrieved from different locations and orientations. Our approach does not require any additional infrastructure support, localization scheme, specialized camera, or modification to smartphone's operating system. Designed and developed for current generation smartphones, our application shows promising initial results with retrieval accuracy of 82% in indoor environment without noticeable impact on the user experience.  If made commercially available, such system could be used in city tourism, infrastructure maintenance, and enabling new kind of social interactions.	augmented reality;interaction;internationalization and localization;operating system;real-time locating system;smartphone;user experience	Puneet Jain;Romit Roy Choudhury	2014		10.1145/2594368.2601472	computer vision;augmented reality;simulation;computer science;multimedia	HCI	-47.177759781748016	-40.72239228843411	97492
f4be9d6d0ca71391862fb883d1dac4f5f755ea28	an empirical approach for the evaluation of haptic algorithms	virtual reality;force feedback;computational complexity;haptic interfaces libraries virtual environment performance evaluation displays current measurement object detection shape change detection algorithms torque;haptic interfaces;rendering computer graphics;virtual reality haptic interfaces force feedback rendering computer graphics computational complexity;computational complexity haptic algorithm evaluation force feedback algorithm virtual environment	The number of haptic algorithms has been growing over the past few years. However, little research has been performed in evaluating these algorithms. This paper provides a discussion of how force-feedback algorithms can be empirically evaluated for correctness and performance.	algorithm;correctness (computer science);haptic technology	Chris Raymaekers;Joan De Boeck;Karin Coninx	2005	First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. World Haptics Conference	10.1109/WHC.2005.21	computer vision;simulation;computer science;artificial intelligence;virtual reality;multimedia;haptic technology;computational complexity theory	HPC	-43.16013886000538	-48.68822772012618	97599
9a302c0cfc184cc6d24c923f63962c6894e5236f	influence of visual feedback on passive tactile perception of speed and spacing of rotating gratings	texture;haptic device;virtual reality;touch;speed effect;tactile perception;illusion;visual feedback;perception;spacing effect	We studied the influence of visual feedback on the tactual perception of both speed and spatial period of a rotating texture. Participants were placed in a situation of perceptual conflict concerning the rotation speed of a cylindrical texture. Participants touched a cylindrical texture of gratings rotating around its axis at a constant speed, while they watched a cylinder without gratings rotating at a different speed on a computer screen. Participants were asked to estimate the speed of the gratings texture under the finger and the spacing (or spatial period) of the gratings. We observed that the tactual estimations of both speed and spacing co-varied with the speed of the visual stimulus, although the cylinder perceived tactually rotated at a constant speed. The first effect (speed effect) could correspond to the resolution of the perceptual conflict in favor of vision. The second effect (spacing effect) is apparently surprising, since no varying information about spacing was provided by vision. However, the physical relation between spacing and speed is well established according to every day experience. Thus, the parameter extraneous to the conflict could be influenced according to previous experience. Such cross-modal effects could be used by designers of virtual reality systems and haptic devices to improve the haptic sensations they can generate using simple (constant) tactile stimulations combined with visual feedback.	computer monitor;cylinder seal;doppler effect;haptic technology;mcgurk effect;modal logic;optic axis of a crystal;virtual reality	Anatole Lécuyer;Marco Congedo;Edouard Gentaz;Olivier Joly;Sabine Coquillart	2010		10.1007/978-3-642-14075-4_11	computer vision;simulation;computer science;communication	HCI	-44.67721754540526	-49.67430071402865	97854
ccd47b5b2f3f3e645c18a037e13b7ed132a7ce80	vibration perception and excitatory direction for haptic devices	haptic device;haptics;emotion;vibration;excitatory direction;absolute threshold;emotional expression;haptic interface	Vibration feedback is one of the most popular ways to communicate between human and haptic interfaces nowadays. In order to deliver a wider variety of information accurately and efficiently, significant design factors of the vibration need to be investigated and applied to haptic devices. In this study, the excitatory direction was examined as a design factor of the vibration in terms of sensitivity and emotion. We conducted two experiments. In the first experiment, the sensitivities of three excitatory directions--X (lateral), Y (fore-and-aft) and Z (vertical) axes were estimated by the absolute thresholds of the vibration perception with two frequency levels (150 and 280 Hz). Based on ten participants' estimated absolute thresholds, we conclude that the vibration with X axis is less sensitive than Z axis at the frequency of 150 Hz, while the vibration with Y axis is less sensitive than Z axis at the frequency of 280 Hz. In the second experiment, the agreeability of 29 emotional expressions to the vibrations was measured by a 7-point scale with a total of 12 conditions (2 frequencies × 2 amplitudes (i.e., 50 × 10?3 and 500 × 10?3 g) × 3 excitatory directions). Based on 20 participants' responses, it is concluded that at the frequency of 150 Hz and the amplitude of 50 × 10?3 g, the vibration is perceived as `light', and as even `lighter' if the vibration is with Y axis rather than with Z axis. Likewise, at the frequency of 150 Hz and the amplitude of 500 × 10?3 g, the vibration is perceived as `repulsive', and as even `more repulsive' if the vibration is with Y or Z axis rather than with X axis. Therefore, three excitatory directions can be selectively utilized to design the distinguishable vibration by its sensitivity and emotion.	haptic technology	Jihong Hwang;Wonil Hwang	2011	J. Intelligent Manufacturing	10.1007/s10845-009-0277-7	speech recognition;computer science;engineering;artificial intelligence;haptic technology	Robotics	-44.505745050669375	-50.235161858488624	97975
08bb6547142489a24185e57a2ef6be7f562f5b08	drag-and-pop and drag-and-pick: techniques for accessing remote screen content on touch- and pen-operated systems	user study;interaction style;interactive display;operating system;pen input;drag and drop;interaction technique	Drag-and-pop and drag-and-pick are interaction techniques designed for users of penand touchoperated display systems. They provide users with access to screen content that would otherwise be impossible or hard to reach, e.g., because it is located behind a bezel or far away from the user. Drag-and-pop is an extension of traditional drag-and-drop. As the user starts dragging an icon towards some target icon, drag-and-pop responds by temporarily moving potential target icons towards the user’s current cursor location, thereby allowing the user to interact with these icons using comparably small hand movements. Drag-and-Pick extends the drag-and-pop interaction style such that it allows activating icons, e.g., to open folders or launch applications. In this paper, we report the results of a user study comparing drag-and-pop with traditional drag-and-drop on a 15’ (4.50m) wide interactive display wall. Participants where able to file icons up to 3.7 times faster when using the drag-and-pop interface.	bridging (networking);cursor (databases);drag and drop;interaction technique;tablet computer;usability testing;user (computing);video wall	Patrick Baudisch;Edward Cutrell;Mary Czerwinski;Daniel C. Robbins;Peter Tandler;Benjamin B. Bederson;A. Zierlinger	2003			human–computer interaction;computer science;operating system;multimedia;interaction technique;computer graphics (images)	HCI	-45.80044010466846	-41.55487153018641	98350
0ef7e246e6091208aebfbac8f0c1c60c18334337	using virtual environments to evaluate assumptions of the human visual system	computer vision psychology perception;gravity;three dimensional displays computational modeling solid modeling visual systems gravity image color analysis gray scale;3d shapes human visual system virtual reality applications cave simulation virtual furniture objects gravity direction scene reconstruction computer vision model 3d real scene;psychology;gray scale;computer vision;computational modeling;three dimensional displays;image color analysis;virtual reality computer vision;solid modeling;visual systems;perception	Virtual reality applications provide an opportunity to test human vision in well-controlled scenarios that would be difficult or impossible to generate in real physical spaces. This paper presents a study intended to evaluate the importance of possible assumptions made by the human visual system. Using a CAVE simulation, participants viewed and counted virtual furniture objects in a variety of experimental manipulations. The assumption of uprightness against inversion, or the `gravity constraint,' was identified as a significant assumption of the visual system (p <; 0.001). Monocular vs. binocular vision was also demonstrated as an important factor in this study (p = 0.01), while color vs. grayscale did not have a significant impact on task performance (p = 0.16). By including the binocular cue, and the assumption about the direction of gravity, the scene reconstruction produced by our computer vision model is reliable. The model can detect and count symmetrical objects in a 3D real scene and then recover their 3D shapes.	binocular vision;cave story;computer vision;grayscale;habbo;simulation;virtual reality	Eric Palmer;Aaron Michaux;Zygmunt Pizlo	2016	2016 IEEE Virtual Reality (VR)	10.1109/VR.2016.7504751	3d reconstruction;computer vision;simulation;gravity;computer science;solid modeling;human visual system model;perception;computational model;grayscale;computer graphics (images)	Vision	-42.98833733687563	-49.32074557201311	98402
198353c8339b203a207ab55d746bbbc9e56de98b	tattoo antenna temporary transfers operating on-skin (tattoos)	tk7871 6 antennas and waveguides;body centric communication;book chapter;aesthetic design;tk6570 m6 mobile communication systems;conducting ink;rfid	This paper discusses the development of RFID logo antennas based on the logos of Loughborough University and the University of Kent which can be tattooed directly onto the skin’s surface. Hence, this paper uses aesthetic principles to create functional wearable technology. Simulations of possible designs for the tattoo tags have been carried out to optimize their performance. Prototypes of the tag designs were fabricated and read range measurements with the transfer tattoos on a volunteers arm were carried out to test the performance. Measured Read ranges of approximately 0.5 m have been achieved with the antenna 10 μm from the body.	computer simulation;logo;printing;radio-frequency identification;smart antenna;user experience;wearable technology	James Tribe;Dumtoochukwu Oyeka;John C. Batchelor;Navjot Kaur;Diana M. Segura Velandia;Andrew A. West;Robert Kay;Katia Vega;William G. Whittow	2015		10.1007/978-3-319-20898-5_65	visual arts;telecommunications;engineering;advertising	Mobile	-43.90583031286267	-42.20949820633455	98557
06a2cadf8231e66308a275adb1166af24e36f873	optimization of interactive visual-similarity-based search	integrable system;degree of freedom;active learning;user study;interactive visualization;interactive search;information visualization;spectrum;universiteitsbibliotheek;machine learning;similarity based visualization;evaluation criteria;similarity function;relevance feedback;content based retrieval;is research	At one end of the spectrum, research in interactive content-based retrieval concentrates on machine learning methods for effective use of relevance feedback. On the other end, the information visualization community focuses on effective methods for conveying information to the user. What is lacking is research considering the information visualization and interactive retrieval as truly integrated parts of one content-based search system. In such an integrated system, there are many degrees of freedom like the similarity function, the number of images to display, the image size, different visualization modes, and possible feedback modes. To base the optimal values for all of those on user studies is unfeasible. We therefore develop search scenarios in which tasks and user actions are simulated. From there, the proposed scheme is optimized based on objective constraints and evaluation criteria. In such a manner, the degrees of freedom are reduced and the remaining degrees can be evaluated in user studies. In this article, we present a system that integrates advanced similarity based visualization with active learning. We have performed extensive experimentation on interactive category search with different image collections. The results using the proposed simulation scheme show that indeed the use of advanced visualization and active learning pays off in all of these datasets.	common criteria;image resolution;information visualization;machine learning;relevance feedback;similarity measure;simulation;usability testing	Giang P. Nguyen;Marcel Worring	2008	TOMCCAP	10.1145/1324287.1324294	spectrum;integrable system;computer vision;information visualization;interactive visualization;computer science;artificial intelligence;theoretical computer science;machine learning;multimedia;active learning;degrees of freedom;world wide web;information retrieval	Visualization	-34.00546408437752	-50.6891270751565	98626
6e377c77350cbcec8a723163be7791dce84c8c5f	tennis serve correction using a performance improvement platform		Technology based solutions for analyzing various metrics related to the game of tennis are becoming popular in market. But there is lack of a complete solution to track the effectiveness of training methods adopted for improvement. A platform to assess the effectiveness of these training routines is discussed in this paper. The platform consists of wireless measurement sensors which work in conjunction with software analysis modules. The sensors can measure kinetic parameters in real-time and are attached to the racket and to the player's body. The software module enables visualization of sensor data and evaluates performance. The platform has been made versatile and customizable to enable easy integration with any training procedure involved in performance improvement. In this paper, the proposed platform is utilized to monitor the effectiveness of drills involved in improvement of tennis serves. Methods adopted with inputs from trainers to compute numerical performance indices for tracking progress are discussed. Monitoring these indices computed over several training sessions helps in tracking the player's progress.	automatic computing engine;norm (social);numerical analysis;racket;real-time clock;requirement;sensor;tweaking	R. Dhinesh;S. P. Preejith;Mohanasankar Sivaprakasam	2018	2018 IEEE 6th International Conference on Serious Games and Applications for Health (SeGAH)	10.1109/SeGAH.2018.8401370	visualization;simulation;software analysis pattern;racket;performance improvement;software;kinetic theory;computer science	Visualization	-38.04095605456658	-45.14564285875096	98782
1fbe6c032c321841d451b64032d1ba4ecdaca1b9	towards an empathic social robot for ambient assisted living		In the context of Ambient Assisted Living, assistance and care are delegated to the intelligence embedded in the environment that, in our opinion, should provide not only a task-oriented support but also an interface able to establish a social empathic relation with the user. This can be achieved, for instance, using a social assistive robot as interface towards the environment services. In the context of the NICA (Natural Interaction with a Caring Agent) project we developed the behavioral architecture of a social robot able to assist the user in the interaction with a smart home environment. In this paper we describe how this robot has been endowed with the capability of recognizing the user affective state from the combination of facial expressions and spoken utterances and to reason on in order to simulate an empathic behavior.	assistive technology;embedded system;home automation;simulation;social robot	Berardina De Carolis;Stefano Ferilli;Giuseppe Palestra;Valeria Carofiglio	2015			simulation;social robot;architecture;home automation;robot;affect (psychology);facial expression;psychology	Robotics	-34.70673537855633	-39.647321629255856	99032
803829c1eebbc521e1f192238c55f030b61a4bb4	single channel brain-computer interface control system based on tgam module		In order to overcome the limitations of manual control or speech recognition in traditional control mode, this paper designs a “mind control” system using single channel EEG signal extraction. This system is based on the brain-computer interface technology and the EEG signal extracted from the TGAM module is transmitted to the host computer through Bluetooth. After analyzing by the host computer, the attention value is obtained and then converted into the control command to run a car. The test shows that the average success rate of five students' controlling of car is 80.5%. This system can realize the “mind control” of a car by detecting the degree of users' attention.	bluetooth;brain–computer interface;control system;electroencephalography;host (network);sensor;speech recognition	Lu Zhang;Qingsong Lv;Yishen Xu	2017	2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2017.8302235	artificial intelligence;computer hardware;brain–computer interface;computer vision;electroencephalography;computer science;bluetooth;host (network);communication channel;control system	Mobile	-40.459877183297515	-44.819918611306946	99086
5c923dd9c35c3a7991681de9e319f6c4e6598368	the music room	artistic installation;musical interface;user experience	This paper presents The Music Room, an interactive installation where couples compose original music. The music is generated by Robin, an automatic composition system, according to relative distance between the users and the speed of their own movements. Proximity maps the pleasantness of music, while speed maps its intensity. The Music Room was exhibited during the EU Researchers' Night in Trento, where it met with a strong interest by visitors.	map	Fabio Morreale;Raul Masu;Antonella De Angeli;Paolo Rota	2013		10.1145/2468356.2479620	user experience design;human–computer interaction;computer science;multimedia	Web+IR	-47.89342589243022	-38.11314907992162	99176
005223bb57115157024547e8b512d5eb8cf11bd1	embodied interaction using non-planar projections in immersive virtual reality	immersive interaction;sense of embodiment;non planar projections	In this paper we evaluate the use of non-planar projections as a means to increase the Field of View (FoV) in embodied Virtual Reality (VR). Our main goal is to bring the virtual body into the user's FoV and to understand how this affects the virtual body/environment relation and quality of interaction. Subjects wore a Head Mounted Display (HMD) and were instructed to perform a selection and docking task while using either Perspective (≈ 106 ° vertical FoV), Hammer or Equirectangular (≈ 180 ° vertical FoV for both) projection. The increased FoV allowed for a shorter search time as well as less head movements. However, quality of interaction was generally inferior, requiring more time to dock, increasing docking error and producing more body/environment collisions. We also assessed cybersickness and the sense of embodiment toward the virtual body through questionnaires, for which the difference between projections seemed to be less pronounced.	docking (molecular);field of view in video games;head-mounted display;immersion (virtual reality);virtual body;virtual reality sickness	Henrique Debarba;Sami Perrin;Bruno Herbelin;Ronan Boulic	2015		10.1145/2821592.2821603	computer vision;simulation;computer graphics (images)	Visualization	-45.08381370755239	-48.34543801754166	99183
f13a1ee8b7e6045a6a8f61330640fca5549c6251	quantitative assessment of the effectiveness of using display techniques with a haptic device for manipulating 3d objects in virtual environments	virtual holography;virtual environments;3d manipulation tasks;display techniques;multimodal interaction;haptic interaction	Quantitative assessment is made of using two display techniques, providing two different levels of depth perception, in conjunction with a haptic device for manipulating 3D objects in virtual environments. The two display techniques are 2D display, and interactive 3D stereoscopic virtual holography display on a zSpace tablet. Experiments were conducted, by several users of different ages and computer training. The experiments involved selected pointing and manipulation tasks. The speed of performing the tasks using the two display techniques were recorded. Statistical analysis of the data is presented. As expected, the use of interactive 3D stereoscopic display resulted in faster performance of the tasks. The improvement in performance was particularly noticeable for the cases wherein the subjects needed to manipulate the haptic arm to reach objects / targets at different depths, and also when the objects / targets were occluded partially by the obstacles.	arm architecture;depth perception;experiment;haptic technology;holography;stereoscopy;tablet computer;virtual reality	Rifat Aras;Yuzhong Shen;Ahmed K. Noor	2014	Advances in Engineering Software	10.1016/j.advengsoft.2014.05.009	computer vision;computer science;engineering;multimodal interaction;multimedia;computer graphics (images)	HCI	-44.39605500753383	-47.64331918404864	99212
528eed628b232d2c92d7ed901fd1adb52ee30f45	unimorph: fabricating thin film composites for shape-changing interfaces	digital fabrication;organic user interface;rapid prototyping;unimorph actuation;radical atoms;shape changing interfaces;human material interaction	Researchers have been investigating shape-changing interfaces, however technologies for thin, reversible shape change remain complicated to fabricate. uniMorph is an enabling technology for rapid digital fabrication of customized thin-film shape-changing interfaces. By combining the thermoelectric characteristics of copper with the high thermal expansion rate of ultra-high molecular weight polyethylene, we are able to actuate the shape of flexible circuit composites directly. The shape-changing actuation is enabled by a temperature driven mechanism and reduces the complexity of fabrication for thin shape-changing interfaces. In this paper we describe how to design and fabricate thin uniMorph composites. We present composites that are actuated by either environmental temperature changes or active heating of embedded structures and provide a systematic overview of shape-changing primitives. Finally, we present different sensing techniques that leverage the existing copper structures or can be seamlessly embedded into the uniMorph composite. To demonstrate the wide applicability of uniMorph, we present several applications in ubiquitous and mobile computing.	digital modeling and fabrication;embedded system;flexible circuit;mobile computing;shape context	Felix Heibeck;Basheer Tome;Clark Della Silva;Hiroshi Ishii	2015		10.1145/2807442.2807472	unimorph;thin film;computer science;rapid prototyping;thermal expansion;composite material;organic user interface	HCI	-45.11081667176704	-38.55039247882568	99276
1ae17e45e8b32969a3127400eb6525d2ccd93c3a	hybrid user interfaces: breeding virtually bigger interfaces for physically smaller computers	hybrid user interfaces	While virtual worlds offer a compelling alternative to conventional interfaces, the technologies these systems currently use do not provide sufficient resolution and accuracy to support detailed work such as text editing. We describe a pragmatic approach to interface design that provides users with a large virtual world in which such high-resolution work can be performed. Our approach is based on combining heterogeneous display and interaction device technologies to produce a hybrid user interface. Display and interaction technologies that have relatively low resolution, but which cover a wide (visual and interactive) field are used to form an information surround. Display and interaction technologies that have relatively high resolution over a limited visual and interaction range are used to present concentrated information in one or more selected portions of the surround. These highresolution fields are embedded within the low-resolution surround by choosing and coordinating complementary devices that permit the user to see and interact with both simultaneously. This allows each embedded high-resolution interface to serve as a “sweet spot” within which intonation may be preferentially processed, We have developed a preliminary implementation, described in this paper, that uses a Reflection Technology Private Eye display and a Polhemus sensor to provide the secondary lowresohttion surround, and a flat-panel display and mouse to provide the primary high-resolution interface. CR	computer;cooperative breeding;embedded system;flat panel display;image resolution;text editor;user interface;virtual world	Steven K. Feiner;Ari Shamash	1991		10.1145/120782.120783	simulation;computer science;theoretical computer science;user interface	HCI	-45.18056203676372	-40.16692734140075	99342
6e30cdf83becd2b7764e9d5543055184a74e8be3	balancing exploration - exploitation in image retrieval		In recent years there has been an increased interest in developing exploration–exploitation algorithms for image search. However, little research has been done as to what type of image search such techniques might be most beneficial. We present an interactive image retrieval system that combines Reinforcement Learning with an interface designed to allow users to actively engage in directing the search. Reinforcement Learning is used to model the user interests by allowing the system to trade off between exploration (unseen types of image) and exploitation (images the system thinks are relevant). A task-based user study indicates that for certain types of searches a traditional exploitation-based system is more than adequate, while for others a more complex system trading off exploration and exploitation is more beneficial. Image retrieval techniques operating on meta-data, such as textual annotations, have become the industry standard. However, with the explosive growth of image collections, tagging new images quickly is not always possible. Secondly, there are many instances where image search by query is problematic, e.g. finding an illustration for an article about “youth”. A solution to such a problem is content-based image retrieval (CBIR) [6]. Early experiments show that CBIR can be improved through relevance feedback by involving the user in the search loop [1]. However, relevance feedback can lead to a context trap, where users specify the context so strictly that they can only exploit a limited area of information space. Combining exploration/exploitation strategies with relevance feedback is a popular attempt at avoiding the context trap [2, 3, 7]. However, few studies have been done showing the advantages (and disadvantages) of exploratory image retrieval systems. We report preliminary studies showing under what conditions exploratory image search might be most beneficial and where exploratory search may actually hinder the search results. For this purpose, we built a query-less image search system incorporating state of the art reinforcement learning (RL) techniques to allow the system to efficiently balance between exploration and exploitation. System Overview. The system assists users in finding images in a database of unannotated images without query typing. The RL methods and interactive interface allow users to direct the search according to their interests. The interface and an example search are presented in Figure 1. The search starts with a display of a collage of images. To ensure that the initial set is a good representation of the entire image space, we cluster all the images in k clusters, where k is the number of displayed images and then we sample an image from each cluster. Our pre-user study shows that this technique provides a good starting point for the search. When the mouse hovers over an image, 13 (Edited by Iván Cantador and Min Chi) Proceedings of UMAP 2014 posters, demonstrations and late-breaking results (a) Iteration 1 (b) Iteration 2 (c) Iteration 3 (d) Iteration 4 Fig. 1. The first four iterations of an example search for “City by night”. a slide bar appears at the bottom allowing the user to rate that particular image. The feedback ranges from -1 (no interest to the user) to 1 (highly relevant). Users can score as many images as they like. Images not rated by the user are assumed to have score of 0. Each image can be displayed at most once throughout the entire search session. We illustrate the interface and interaction design through a walkthrough exemple. The user wants to find an image to illustrate an article about “city by night”. Initially (Figure 1a), the user is presented with a collage of images uniformly selected from the database and marks the fifth image in the second row and the second image in the third row as highly relevant. The user moves to the next iteration by pressing the “Next” button at the top of the page. In the second iteration (Figure 1b), more images related to “night” are presented and the user selects four images. In iterations 3 and 4 (Figures 1c and 1d), more relevant images are presented and the user can further narrow down his search. To help the user to explore the image space, we use Gaussian Process bandits with Self-Organizing Maps (GP-SOM), with dependencies across arms, which in our system translates into similarities between images. The algorithm uses function f that makes predictions with regards to the relevance of all the images to the user’s interests. When selecting the next set of images to display, the system might select images with the highest estimated relevance score but since the estimate of f may be inaccurate, this exploitative choice might be suboptimal. Alternatively, the system might exploratively select an image for which the user feedback improves the accuracy of f , enabling better future image selections. A detailed description of the algorithm and the similarity measure between the images can be found in [5]. Experiments. We conducted a set of user studies to evaluate the impact of exploration on three types of searches [1]: (1) Target search looking for a particular image; (2) Category search looking for any image from a given category, e.g. image of a cat; (3) 14 (Edited by Iván Cantador and Min Chi) Proceedings of UMAP 2014 posters, demonstrations and late-breaking results Open search browsing a collection of images without knowing what the target may look like. The study included three conditions: 1) our Gaussian Process system (GP), 2) a version of our system that uses only exploitation (EXPLOIT), and 3) a system that presents random images at each iteration (RAND). In EXPLOIT, the exploration level was set to 0, which means that the system can only present images similar to the ones marked as relevant. The same interface was used in all settings. We used the MIRFLICKR-25000 dataset [4] consisting of 25000 images from the social photography site Flickr and commonly used in assessment of image retrieval and annotation tasks. We recruited 20 post-graduate students to run the experiments. Each participant was asked to perform three tasks for all three types of searches, i.e. each participant performed 9 searchers. We counterbalanced between the tasks and the systems for each subject so that each task was performed the same number of times with each system. The participants were asked to finish the task when they find the target image (in target search) or when they feel they found the ideal image in category and open searches. In all the tasks, the search was limited to 25 iterations. In target search, participants were presented with an image and a short description of that image and then asked to look for that image. In category and open searches, no example images were provided and participants were only given a short description of what to look for, e.g. red rose or illustration for an article about gardening.	algorithm;chi;coat of arms;cognitive walkthrough;complex system;content-based image retrieval;database;emoticon;experiment;exploit (computer security);exploratory search;flickr;gardening (cryptanalysis);gaussian process;inferring horizontal gene transfer;interaction design;iteration;reinforcement learning;relevance feedback;session (web analytics);similarity measure;slide rule;tag (metadata);technical standard;the industry standard;usability testing;user interface	Dorota Glowacka;Sayantan Hore	2014				ML	-34.61703689495763	-49.871628285414666	99374
efa60944fc296786d6c2250a6994bb72da6ed344	a shared control method for obstacle avoidance with mobile robots and its interaction with communication delay	teleoperation;shared control;obstacle avoidance;communication delay;human-robot interaction;mobile robot	Teleoperation allows human operators to safely extend themselves to remote environments that are typically difficult or dangerous to access. The remote environments are often unstructured (i.e. not having clear roads or paths to follow) and only accessible by wireless communication (introducing factors such as degraded signals and communication delay). Teleoperated driving under these conditions can result in slow operation speeds and unintended collisions with obstacles. Automating portions of the teleoperation task can help mitigate some of the negative effects of wireless communication. Shared control is used to combine inputs from the human teleoperator and automation. This work presents a new model predictive control based shared control method. We introduce a new representation for obstacle free regions that works well with unstructured robot environments and allows for an model predictive control problem formulation that can be solved rapidly. The shared control method is implemented in a robot simulator and tested with human subjects. Two user studies involving a search task with a mobile robot evaluate the effectiveness of the shared control method and explore its interaction with factors such as communication delay and input interface style. Communication delay is found to have the largest magnitude effect on performance and safety measures. Results demonstrate that the shared control method can improve both performance and safety when delays are present.	control system;gamepad;input device;justin (robot);mobile robot;obstacle avoidance;robotic mapping;simulation;tablet computer;technical support;telerobotics;usability testing;voronoi diagram	Justin G. Storms;Kevin Chen;Dawn M. Tilbury	2017	I. J. Robotics Res.	10.1177/0278364917693690	obstacle avoidance	Robotics	-46.504570947629695	-50.908097427836545	99610
cf8710d9d21a2b8a3be5aee9aa8af360a79e6cff	pda-assisted indoor-navigation with imprecise positioning: results of a desktop usability study		Although most of today’s navigation systems are used for guidance of cars, recent progress in mobile computing made it possible for research and industry to develop various prototypes of indoor-navigation systems in combination with PDAs. Independent of the presentation mode of route instructions, it is desirable that such real-time route guidance system automatically delivers the correct piece of information to the user at the right time. This requires that the PDA knows the user’s position and orientation, which is not always available due to technical limitations of indoor sensing and positioning techniques, and potential signal dropouts. Using a desktop usability study, this chapter extends previous work on route instructions with mobile devices. The study explores the preferred modes of interaction between user and PDA in case of diluted position and orientation accuracies.	desktop computer;global positioning system;guidance system;mobile computing;mobile device;personal digital assistant;real-time locating system;usability testing	Hartwig H. Hochmair	2008		10.1007/978-3-540-37110-6_11	cognitive walkthrough;usability;human–computer interaction;computer science;data mining;world wide web;usability lab;usability inspection	HCI	-46.72880234413313	-42.6292411561463	99712
e964d8ef614d9baedcb3a1352a905617ae979a7c	experimental testing of the coglaboration prototype system for fluent human-robot object handover interactions	subjective user evaluation coglaboration prototype system fluent human robot object handover interaction robot prototype system human human object handover experiments knowledge base movement dynamics;robot sensing systems;prototypes;handover;engines;robot dynamics human robot interaction;handover delays robot sensing systems prototypes engines;delays	This article presents the design and execution of the experiments used to develop and evaluate a robot prototype system for fluent Human-Robot object handover interactions. A key aspect of our experimental methodology is the deep integration between Human-Robot and Human-Human object handover experiments. This provides a solid baseline and knowledge base for the prototype evaluation, both in terms of movement dynamics and in subjective user evaluation.	baseline (configuration management);experiment;human–robot interaction;knowledge base;prototype;requirement;responsiveness;social robot;velocity (software development)	Ansgar R. Koene;Satoshi Endo;Anthony Remazeilles;Miguel Prada;Alan Wing	2014	The 23rd IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2014.6926261	real-time computing;simulation;computer science;handover;prototype	Robotics	-35.08919115678557	-41.22366388963042	99725
37e49ae340d5f79d08d55a2e36577eb9faa10c84	an architecture for immersive evaluation of complex human tasks	human movement;technology development;spatial variables measurement;real time;measurement systems;virtual reality;feedback;measurement systems virtual reality spatial variables measurement feedback real time systems tracking;humans motion measurement tracking performance evaluation electromagnetic measurements kinematics virtual reality feedback anthropometry real time systems;vibrotactile feedback immersive evaluation human movement measurement virtual reality real time systems body posture visual feedback;tracking;real time systems	Tom Molet, Ronan Boulic, Member IEEE RA, Serge Rezzonico, Daniel Thalmann Computer Graphics Laboratory, Swiss Federal Institute of Technology Contact Author: Dr Ronan Boulic, Mail : EPFL, DI-LIG, CH-1015 Lausanne, Switzerland Email : Ronan.Boulic@epfl.ch Tel: +.41.21.693.52.46 Fax: +.41.21.693.53.28 Abstract We investigate how the movement measurement technologies developed for Virtual Reality applications can be applied to track in real-time the full body posture of a human being. The accuracy of this information is of definite importance to evaluate the feasibility of complex tasks involving human beings. We present a full body movement measurement approach. It provides a realistic conversion in real-time with a reasonable number of sensors. Associated with the hand movement measurement and correction algorithms we provide a pertinent visual and vibrotactile feedback to the performer.	accessibility;algorithm;autonomous robot;collision detection;computer graphics;computer multitasking;cylinder seal;email;experiment;fax;graphical user interface;lateral thinking;motion capture;online and offline;performance;poor posture;real-time clock;real-time computing;real-time locating system;real-time transcription;relevance;sensor;simulation;spinal cord stimulator;switzerland;system of measurement;tom proulx;uniform resource identifier;usability;virtual reality	Tom Molet;Ronan Boulic;Serge Rezzonico;Daniel Thalmann	1999	IEEE Trans. Robotics and Automation	10.1109/70.768180	computer vision;simulation;computer science;engineering;system of measurement;feedback;virtual reality;tracking;multimedia	Visualization	-38.27425784347065	-41.83040254704532	99839
4cf21bedcd43c1823b0d9a041dc79bdbde0647c4	delay compensation in shared haptic virtual environments	virtual reality;client server systems;object motion constraints delay compensation shared haptic virtual environments shves client server architecture physics simulation engine position information object state information haptic device communication delay adaptive force rendering scheme object stiffness device velocity;virtual reality client server systems delays haptic interfaces rendering computer graphics;delays force haptic interfaces servers engines probes;haptic interfaces;rendering computer graphics;delays	Shared Haptic Virtual Environments (SHVEs) are often realized using a client-server architecture. At the server, a physics simulation engine calculates the object states based on the position information received from the clients. At the clients, the object state information, received from the server, is used to update the local copy of the virtual environment. Forces displayed to the user through a haptic device are computed locally at the client based on the interactions with the objects. Communication delay leads to delayed object state updates and increased interaction forces rendered at the clients. Users perceive this as increased object weight. In this paper, we systematically analyze the loss of transparency caused by communication delay and propose a novel adaptive force rendering scheme to compensate for it. The proposed scheme reduces the objects' stiffness at the clients, based on delay, device velocity and motion constraints of the objects, only if necessary to achieve perceptual transparency. Simulations and subjective evaluations show that the effect of increased weight of objects is successfully compensated for the tested delay range of up to 150 ms. At the same time, if the object is unmovable, the perception of interaction with a rigid object is preserved.	algorithm;approximation;centralized computing;client–server model;computer simulation;cube 2: sauerbraten;damping factor;dynamical simulation;experiment;haptic technology;interaction;physics engine;server (computing);steady state;sticky bit;stiffness;velocity (software development);virtual reality	Clemens Schuwerk;Rahul Gopal Chaudhari;Eckehard G. Steinbach	2014	2014 IEEE Haptics Symposium (HAPTICS)	10.1109/HAPTICS.2014.6775484	real-time computing;simulation;computer science;multimedia	Mobile	-41.26408610454086	-49.82104703105954	99875
5550b07be0fa35b0d7711ce991c1871845bcc7ac	a novel prototype for an optical see-through head-mounted display with addressable focus cues	focusing;user studies three dimensional displays mixed and augmented reality focus cues accommodation retinal blur convergence;eye accommodative response;real world 3d viewing condition optical see through head mounted display addressable focus cues liquid lens monocular bench prototype display focal distance varifocal plane mode accommodation cue time multiplexed multifocal plane mode retinal blur cues depth perception eye accommodative response;convergence;retinal blur cues;optical see through head mounted display;prototypes;user study;addressable focus cues;liquid lens;real world 3d viewing condition;user studies;focal planes;three dimensional;accommodation cue;monocular bench prototype;visualization;depth perception;design and implementation;three dimensional displays;retinal blur;retina;time multiplexed multifocal plane mode;displays;prototypes displays convergence focusing retina holography holographic optical components lenses augmented reality visualization;lenses;accommodation;mixed and augmented reality;holographic optical components;holography;focus cues;varifocal plane mode;augmented reality;three dimensional displays focal planes helmet mounted displays;display focal distance;helmet mounted displays;head mounted display	We present the design and implementation of an optical see-through head-mounted display (HMD) with addressable focus cues utilizing a liquid lens. We implemented a monocular bench prototype capable of addressing the focal distance of the display from infinity to as close as 8 diopters. Two operation modes of the system were demonstrated: a vari-focal plane mode in which the accommodation cue is addressable, and a time-multiplexed multi-focal plane mode in which both the accommodation and retinal blur cues can be rendered. We further performed experiments to assess the depth perception and eye accommodative response of the system operated in a vari-focal plane mode. Both subjective and objective measurements suggest that the perceived depths and accommodative responses of the user match with the rendered depths of the virtual display with addressable accommodation cues, approximating the real-world 3-D viewing condition.	airplane mode;authorization;convergence (action);depth perception;dual;experiment;focal (programming language);flicker (screen);flicker-free;gaussian blur;head-mounted display;human factors and ergonomics;ieee xplore;image;liquid crystal on silicon;morphologic artifacts;multiplexing;numerous;photopsia;prototype;requirement;retina;stereoscopy;usability;user interface device component;visual accommodation;visual artifact;diopters	Sheng Liu;Hong Hua;Dewen Cheng	2010	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2009.95	three-dimensional space;computer vision;augmented reality;accommodation;visualization;convergence;depth perception;computer science;optical head-mounted display;lens;prototype;holography;computer graphics (images)	Visualization	-41.3346630928189	-39.9956999494488	100094
f45d588ec54595c24439ac74691bf339d36facdd	a usability study of a gesture recognition system applied during the surgical procedures		Within an operating room, surgeons need to interact with a large amount of patient's medical information and data. In order to avoid misunder- standings among the staff and protecting the patient safety, the medical staff may use a touchless interaction system that allows the surgeons to directly interact with digital devices that visualize digital images. The RISO project aims to create a gesture recognition system for the visualization and manipulation of medical images, useful for the surgeons even during the surgical procedures. In this paper we show the main findings from a usability study carried out with the aim to evaluate, among others, the learnability of the system and the memora- bility of the gestures employed for the interaction.	gesture recognition	Antonio Opromolla;Valentina Volpi;Andrea Ingrosso;Stefano Fabri;Claudia Rapuano;Delia Passalacqua;Carlo Maria Medaglia	2015		10.1007/978-3-319-20889-3_63	simulation;engineering;multimedia;biological engineering	HCI	-42.730855879389466	-45.41510467191997	100251
38dc4fee6ee0540bd47a297aa76d3507ac2f4789	intentions to use smart textiles in aal home environments: comparing younger and older adults	ikz080013;public records;websearch;rwth publications	The vision of ubiquitous computing is increasingly picking up pace. An increasing number of everyday objects are equipped with smart technology and start to form the Internet of Things. Yet, interacting with these devices is based on conventional surfaces made of glass, metal, or plastic. We believe that textile interaction surfaces will be the next frontier of ubiquitous computing and identified many blank spots in the research landscape. Peoples’ perception and acceptance of smooth and soft interaction surfaces is insufficiently understood. In this paper we present a study in which 90 people of a wide age range evaluated the suitability of smart textiles in different usage scenarios in the home environment. Overall, a solid willingness to use smart textiles as input devices was found, even though there were conditional acceptance criteria which should be given before participants would be willing to buy them. In contrast to many other technology contexts, however, age is not decisive in the evaluation of the usefulness of smart textiles. Younger and older adults seem to have a quite similar evaluation, hinting at a quite generic acceptance pattern.	atm adaptation layer;e-textiles;input device;interaction;internet of things;ubiquitous computing	Martina Ziefle;Philipp Brauner;Julia van Heek	2016		10.1007/978-3-319-39943-0_26	public records;computer science;internet privacy	HCI	-48.00666747174245	-41.475631286561935	100274
57a2ec8b97cf3596bb10d299197208625b7f338f	hand occlusion with tablet-sized direct pen input	tablet pc;pen input;geometric model;model fitting;hand occlusion	We present results from an experiment examining the area occluded by the hand when using a tablet-sized direct pen input device. Our results show that the pen, hand, and forearm can occlude up to 47% of a 12 inch display. The shape of the occluded area varies between participants due to differences in pen grip rather than simply anatomical differences. For the most part, individuals adopt a consistent posture for long and short selection tasks. Overall, many occluded pixels are located higher relative to the pen than previously thought. From the experimental data, a five-parameter scalable circle and pivoting rectangle geometric model is presented which captures the general shape of the occluded area relative to the pen position. This model fits the experimental data much better than the simple bounding box model often used implicitly by designers. The space of fitted parameters also serves to quantify the shape of occlusion. Finally, an initial design for a predictive version of the model is discussed.	fits;geometric modeling;hidden surface determination;input device;minimum bounding box;pen computing;pixel;poor posture;scalability;tablet computer	Daniel Vogel;Matthew Cudmore;Géry Casiez;Ravin Balakrishnan;Liam Keliher	2009		10.1145/1518701.1518787	simulation;computer science;geometric modeling;computer graphics (images)	HCI	-45.015276340056694	-47.10841411765263	100711
6ba50832c755597037e6990207796c5395cfdd0e	the space between the notes: adding expressive pitch control to the piano keyboard	musical interfaces;capacitive touch sensing;piano keyboard;expressivity;digital arts;performance technique	This paper addresses the question of how to extend the capabilities of a well-established interface in a way that respects users' existing expertise. The piano-style keyboard is among the most widely used and versatile of digital musical interfaces. However, it lacks the ability to alter the pitch of a note after it has been played, a limitation which prevents the performer from executing common expressive techniques including vibrato and pitch bending. We present a system for controlling pitch from the keyboard surface using capacitive touch sensors to measure the locations of the player's fingers on the keys. The large community of trained pianists makes the keyboard a compelling target for augmentation, but it also poses a challenge: how can a musical interface be extended while making use of the existing techniques performers have spent thousands of hours learning? In this paper, user studies with conservatory pianists explore the constraints of traditional keyboard technique and evaluate the usability of the continuous pitch control system. The paper also discusses implications for the extension of other established interfaces in musical and non-musical contexts.	conservatory (greenhouse);control system;musical keyboard;pitch (music);sensor;usability	Andrew P. McPherson;Adrian Gierakowski;Adam M. Stark	2013		10.1145/2470654.2481302	speech recognition;digital art;expressivity;multimedia	HCI	-46.768103206296075	-44.01826988668569	101150
eb3c1c6292fb0bf718183d8b9975f8f09b07c9ee	sign language mms to make cell phones accessible to the deaf and hard-of-hearing community	mms;cell phone;sign language;avatar;deaf;accessibility;real time	Cell phones became extremely popular devices considering their vast utility world wide. However, making cell phones accessible to the deaf and hard-of-hearing community is still a challenge. Main available products on the market for this community offer no more than the possibility to boost/amplify volume. Many cellular provides individual cell phone models which are hearing aid compatible and possess speakerphone capabilities. However, if the user is completely deaf, these phones still tend to be somewhat complex or impossible to use. Another alternative, based on Video phones messages, are quickly widespread as the preferred method of communicating for the deaf and hard-of-hearing community. However Video phones require significant computer processing power to compress and decompress video in real time. Nevertheless, this alternative still has to overcome the various technological challenges associated with utilizing video phone technology, especially via low bandwidth network. In this context, this paper describes a new application allowing the use of MMS (Multimedia Messaging Service) to generate sign language animation in order to communicate with deaf people via cell phones. These animations are avatar based animation obtained by automatic interpretation of text into sign language. This application is a new component developed amongst WebSign kernel (Jemni et al., 2007; Jemni and Ellghoul, 2007).	avatar (computing);mobile phone	Mohamed Jemni;Oussama El Ghoul;Nour Ben Yahia;Mehrez Boulares	2007			speakerphone;multimedia messaging service;phone;animation;videotelephony;hearing aid;sign language;multimedia;computer science	HCI	-48.191288718414995	-38.14435924390935	101338
670ec714e5ea7b5b60e18d5fbc4d3299bc5e5846	robot behavioral exploration and multi-modal perception using dynamically constructed controllers				Saeid Amiri;Suhua Wei;Shiqi Zhang;Jivko Sinapov;Jesse Thomason;Peter Stone	2018				HCI	-35.6018979979121	-39.47805202212298	101400
f92e81a90d2dc0b719374d3477ab4d575b291169	real-time neuro-inspired sound source localization and tracking architecture applied to a robotic platform		Abstract This paper proposes a real-time sound source localization and tracking architecture based on the ability of the mammalian auditory system using the interaural intensity difference. We used an innovative binaural Neuromorphic Auditory Sensor to obtain spike rates similar to those generated by the inner hair cells of the human auditory system. The design of the component that obtains the interaural intensity difference is inspired by the lateral superior olive. The spike stream that represents the IID is used to turn a robotic platform towards the sound source direction. The architecture was implemented on FPGA devices using general purpose FPGA resources and was tested with pure tones (1-kHz, 2.5-kHz and 5-kHz sounds) with an average error of 2.32°. Our architecture demonstrates a potential practical application of sound localization for robots, and can be used to test paradigms for sound localization in the mammalian brain.	covox speech thing;real-time web;robot	Elena Cerezuela-Escudero;Fernando Perez-Peña;Rafael Paz-Vicente;Angel Jiménez-Fernandez;Gabriel Jiménez-Moreno;Arturo Morgado Estevez	2018	Neurocomputing	10.1016/j.neucom.2017.12.041	computer vision;field-programmable gate array;architecture;robot;artificial intelligence;binaural recording;acoustic source localization;auditory system;mathematics;pattern recognition;sound localization;neuromorphic engineering	Robotics	-36.20211097863538	-42.88676895631738	101457
68d4a655dd55c74ebc60ac29d558a399dd0de5f4	a study on wearable robotics — comfort is in the context	search and rescue;user needs;location estimation;loading;service robots;audio recording;human robot interaction;perceived comfort wearable indoor tracking unit wearable robotic device travel aid complex buildings museum guides external localization information gps search and rescue symbiotic relationship;robots;dementia humans context robots audio recording loading educational institutions;dementia;service robots human robot interaction;humans;context;conference proceeding	WITU (Wearable Indoor Tracking Unit) is a wearable robotic device that aids indoor navigation by building maps and localizing the user within them. Applications of such a device include search and rescue, travel aid in large and complex buildings, museum guides among others where external localization information such as from a GPS is not available. However, WITU relies on human intelligence both to maintain long term consistency of its location estimates and to efficiently manage its limited memory and processing capacity. This alludes to a symbiotic relationship between the user and the device and here we look at this symbiotic relationship from an end user perspective. Thus, in order to have a successful interaction, we argue that the user needs to feel comfortable wearing the device while carrying out the intended tasks. We hypothesize that this perceived comfort is dependent on the context in which the device is used. We test our hypothesis on three different scenarios; search and rescue worker, dementia patient in a long care facility and a person at a party which acts as the baseline. Results indicate an important consequence for the development of such wearable robotic systems.	baseline (configuration management);global positioning system;internationalization and localization;map;robot;wearable computer	Damith Chandana Herath;Tawna Chapman;Alethea Tomkins;Larissa Elliott;Michelle David;Amy Cooper;Denis Burnham;Sarath Kodagoda	2011	2011 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2011.6181757	human–robot interaction;robot;computer vision;simulation;computer science;engineering;artificial intelligence	Robotics	-34.617350657653844	-41.70213971700288	101623
176e8edc0d19d795e674d691cba34e5c8ea47da2	ownership illusion of the hand/arm through dynamic vr interactions and automatic vibrotactile stimulation	elbow;rubber;3d visualization;sensors;wrist;virtual reality;joints;right handed;fingers;humans;fingers joints sensors elbow humans rubber wrist;response rate	Body awareness has important implications for the use of Virtual Reality (VR) and its effectiveness. This involves the senses of agency and body ownership. The possibility of producing the Rubber Hand Illusion (RHI) in VE (the sense of feeling a fake virtual body part seems like the one¿s own part), has been proven in the past, by giving the user synchronous 3D visual stimulation and passive tactile stimulation on the hidden real right hand. In this paper we present a VR system consisting on a low cost data¿glove and a hand/arm model as a tool for study presence and body ownership in VE. We present also a novel study of the RHI within highly dynamic VE sessions with synchronous pure virtual vibrotactile stimulation of the fingers. The hand/arm participant¿s movements are realistically reproduced on the VE and tactile stimulations are self-inflicted by the participant through actively touching the virtual objects. The results of the proprioceptive drift and the response ratings of a questionnaire applied seem to reveal that the RHI is possible in active and fully multisensored VE sessions	interaction;sacral nerve stimulation;synthetic intelligence;translational drift;virtual body;virtual reality;wired glove	Silvia Pabon;Miguel A. Padilla Castañeda;Antonio Frisoli;Giovanni Greco;Massimo Bergamasco	2010	19th International Symposium in Robot and Human Interactive Communication	10.1109/ROMAN.2010.5654669	natural rubber;simulation;visualization;computer science;sensor;artificial intelligence;response rate;virtual reality	Visualization	-45.968852688435376	-50.11814652328589	101679
5d8bf58a44dc7436bc2cb2e7bf335d2e9995bf9d	a ball type vibro-tactile space mouse using one web camera	input device;interaction;magnetic sensor;computer vision;wireless communication;3 dimensional;image sensor;ultrasonic sensor	A mouse is a useful input device which helps a user to do some tasks using computers with ease. Recently, many 3-dimensional input devices were developed using various kinds of sensors, e.g., gyro-sensors, acceleration sensors, magnetic sensors, ultrasonic sensors, and image sensors, etc. We developed a ball type space mouse which use a web camera to sense the spatial position of the device and can give some vibro-tactile stimulation on the user's fingertip. By using a web camera, we tracked the ball(circle), covered with a certain color, as a marker. The diameter of the ball type mouse is 4cm, and all the circuits for controlling the vibro-tactile device and wireless communication with the host computer and a battery to supply the power are integrated in the ball.	computer;gyro;host (network);image sensor;input device;webcam	Dong-Jin Yoo;Ho-joong Yong;Jongwon Back;Tae-Jeong Jang	2007		10.1145/1280720.1280898	embedded system;three-dimensional space;computer vision;interaction;computer science;operating system;image sensor;ultrasonic sensor;input device;wireless	HCI	-42.70368877440118	-42.34850408906735	101733
bfc1e74e0a17bd0d04bf61c10da41ed55420f750	turn your mobile into the ball: rendering live football game using vibration	football game mobile computing dynamic content analysis tactile rendering video coding vibrotactile coding;analisis contenido;mobile handsets games streaming media sun usability mobile communication system testing displays 3g mobile communication video coding;interfase usuario;affichage;mobile radiocommunication;informatique mobile;image coding;usability testing;vibrations;visualizacion;securite;user interface;dynamic content analysis;telephone portable;soccer;vibrotactile coding;real time;vibracion;physical sciences;software engineering;radiocommunication service mobile;video coding mobile computing mobile handsets rendering computer graphics sport vibrations;tactile rendering;mobile phone;physics;video coding dynamic content analysis football game mobile communication tactile rendering usability vibrotactile coding;codage image;video coding;content analysis;telefono movil;3g mobile communication;display;codage video;streaming media;vibration;football;sensibilidad tactil;displays;games;temps reel;comportement utilisateur;safety;mobile communication;sun;mobile handsets;utilisabilite;tiempo real;system testing;interface utilisateur;jeu ordinateur;user behavior;programvaruteknik;dynamic content;sport;analyse contenu;usabilidad;computer games;radiocomunicacion servicio movil;rendering computer graphics;mobile computing;usability;seguridad;sensibilite tactile;tactile sensitivity;comportamiento usuario;football game;futbol;fysik	Vibration offers many potential benefits for the use of mobile phones. In this paper, we propose a new method of rendering live football game on mobile phones using vibration. A mobile phone is ldquosynchronizedrdquo with the ball in the real field. By holding the phone, users are able to experience dynamic movements of the ball, to know attacking directions and which team is leading the attack. The usability test of our system shows that vibrotactile display is suitable for rendering live football information on mobile phones by adopting designed coding schemes with a right training process.	mobile phone;usability testing	Shafiq ur Réhman;Jiong Sun;Li Liu;Haibo Li	2008	IEEE Transactions on Multimedia	10.1109/TMM.2008.2001352	embedded system;computer vision;mobile search;simulation;usability;content analysis;telecommunications;computer science;operating system;vibration;multimedia;mobile computing;world wide web	Mobile	-47.45354272496275	-42.33417073899541	101787
c1cbf78cb882ec8d036b42ccfe52082c0a7801ef	employing smartwatch for enhanced password authentication		This paper presents an enhanced password authentication scheme by systematically exploiting the motion sensors in a smartwatch. We extract unique features from the sensor data when a smartwatch bearer types his/her password (or PIN), and train certain machine learning classifiers using these features. We then implement smartwatch-aided password authentication using the classifiers. Our scheme is user-friendly since it does not require users to perform any additional actions when typing passwords or PINs other than wearing smartwatches. We conduct a user study involving 51 participants on the developed prototype so as to evaluate its feasibility and performance. Experimental results show that the best classifier for our system is the Bagged Decision Trees, for which the accuracy is 4.58% FRR and 0.12% FAR on the QWERTY keyboard, and 6.13% FRR and 0.16% FAR on the numeric keypad.	authentication;password;smartwatch	Bing Chang;Ximing Liu;Yingjiu Li;Pingjian Wang;Wen Tao Zhu;Zhan Wang	2017		10.1007/978-3-319-60033-8_59	numeric keypad;password;typing;wearable technology;computer hardware;computer science;decision tree;smartwatch;distributed computing;password authentication protocol	Crypto	-37.42321401115408	-46.24481259254433	101918
ed5250ac64e89015c9df0f28a9a315854bf794b6	ani-bot: a modular robotics system supporting creation, tweaking, and usage with mixed-reality interactions		Ani-Bot is a modular robotics system that allows users to control their DIY robots using Mixed-Reality Interaction (MRI). This system takes advantage of MRI to enable users to visually program the robot through the augmented view of a Head-Mounted Display (HMD). In this paper, we first explain the design of the Mixed-Reality (MR) ready modular robotics system, which allows users to instantly perform MRI once they finish assembling the robot. Then, we elaborate the augmentations provided by the MR system in the three primary phases of a construction kit's lifecycle: Creation, Tweaking, and Usage. Finally, we demonstrate Ani-Bot with four application examples and evaluate the system with a two-session user study. The results of our evaluation indicate that Ani-Bot does successfully embed MRI into the lifecycle (Creation, Tweaking, Usage) of DIY robotics and that it does show strong potential for delivering an enhanced user experience.	coherence (physics);head-mounted display;interaction;mixed reality;robotics;self-reconfiguring modular robot;tweaking;usability testing;user experience;video game bot	Yuanzhi Cao;Zhuangying Xu;Terrell Glenn;Ke Huo;Karthik Ramani	2018		10.1145/3173225.3173226	computer science;human–computer interaction;mixed reality;self-reconfiguring modular robot;robot;user experience design;tweaking;user interface;human–robot interaction;robotics;artificial intelligence	Robotics	-43.46499618945709	-38.2224734412452	102066
e550a838031a3b2fad36bb5bcca4065ffaaf6b34	affective, anxiety and behavioral effects of an aversive stimulation during a simulated navigation task within a virtual environment: a pilot study	afecto afectividad;navegacion;negative affect;pilot study;realite virtuelle;realidad virtual;virtual reality;hombre;movement behavior;affect affectivity;navigation;affective state;affect affectivite;human;negative affects;experimental validation;angustia ansiedad;angoisse anxiete;virtual environment;variability;anxiety;spatial configuration;homme	This study investigated the impact of an aversive environmental stimulation on self-reported affective and anxiety states and movement behaviors during a simulated navigation task in a virtual environment (VE). In the experimental task, participants were asked to virtually navigate (within two consecutive sessions), from a starting point to a destination location, across a spatial configuration consisting in three successive corridors (A–C). In the first session, all corridors were non-aversive. In the second session, the corridor B contained an aversive stimulation (i.e., fire, smokescreen, and warning alarm). Fourteen participants were involved in the experiment. Self-reported anxiety and affective states were measured at the end of each session. However, movement indicators (i.e., execution, time, average speed, speed and trajectory variability) were recorded on-line during the experiment. Results showed a significant increased (i) level of self-reported negative affects and state-anxiety between the two sessions, and (ii) speed and trajectory variability between the two sessions, while the participants were in corridor B. In conclusion, these results support the experimental validity of virtual reality for the induction of negative affects and state-anxiety. The relationships between reported negative affects and state-anxiety and behavior are discussed. 2010 Elsevier Ltd. All rights reserved.	heart rate variability;online and offline;spatial variability;transcranial magnetic stimulation;virtual reality	Christophe Maïano;Pierre Therme;Daniel Mestre	2011	Computers in Human Behavior	10.1016/j.chb.2010.07.020	psychology;navigation;simulation;computer science;virtual machine;virtual reality;communication;social psychology;affect;anxiety	HCI	-47.971512359746455	-51.77787288993436	102201
c84d499071e57e7d39a6100d1ae8026a81811384	mood-learning public display: adapting content design evolutionarily through viewers' involuntary gestures and movements	image processing;human behavior recognition;public display;genetic algorithm			Ken Nagao;Issei Fujishiro	2014	IEICE Transactions	10.1587/transinf.E97.D.1991	computer vision;genetic algorithm;image processing;computer science;machine learning;multimedia	HCI	-36.95067039827889	-38.92063007992415	102251
84dd82f61014acfcb66b633a4b35ad3935dc9c13	the number wheel: a tablet based valuator for interactive three-dimensional positioning	graphic input;input device;real time;numeric value input;3 d dynamic graphics;graphic display;three dimensional;interactive input;real time graphics;logical input device;tablet input;valuator input device;interactive graphics	A logical valuator device which provides interactive graphical input of numeric values is presented. This device, called the Number Wheel, was developed for interactive control of highly dynamic three-dimensional displays but is not limited to this use: It is a general purpose valuator device. The implementation of the Number Wheel described here is based upon a digitizer tablet as the physical input device.  The character of the Number Wheel is best explained by developing an analogy with a hypothetical physical device. The Number Wheel can be thought of as a wheel which has a portion of its circumference, or tread, protruding through a slot on the surface of the tablet, somewhat like a giant thumb wheel. Each value in the desired range of the valuator is represented by a point on the circumference of the wheel with the value of the device at any given time being the point at the top of the wheel. The valuator is changed by putting the pen on the wheel where it protrudes through the tablet and moving it back or forth in the direction of rotation. Whenever the pen leaves the rim of the wheel while still moving, the Number Wheel maintains the same speed of rotation until the pen returns to the wheel in order to change or stop its movement.	graphical user interface;graphics tablet;input device;peripheral;tablet computer	Robert W. Thornton	1979		10.1145/800249.807430	three-dimensional space;simulation;computer hardware;computer science;operating system;input device;computer graphics (images)	HCI	-44.40932260637855	-42.00661136816895	102264
fee48f76ed88079d596b0b2446ebb8e4456fe471	dynamic maps for future navigation systems: agile design exploration of user interface concepts	touch screen;user interface;user feedback;design space;input output;specific activity;interactive system;difference set;navigation system;route planning	"""Maps have traditionally been used to support orientation and navigation. Navigation systems shift the focus from printed maps to interactive systems. The key goal of navigation systems is to simplify specific tasks, e.g. route planning or route following. While users of navigation systems need less skills in navigation specific activities, e.g. reading maps or manual route planning, they must now interact with the user interface of the navigation device, which requires a different set of skills. Current navigation systems aim to simplify the interaction by providing interfaces that use basic interaction mechanisms (e.g. button based interfaces on a touch-screen), exploiting the fact that many users are already familiar with such techniques. In the presentation of the information most navigation systems employ map-like displays, possibly combined with additional information, again to exploit familiarity. While such an approach can help with early adoption, it can also limit usefulness and usability. There is, however, a large opportunity to improve input, output and functionality of navigation systems. In this paper we expand a model of classical map based communication to identify possibilities where """"dynamic maps"""" can enhance map based communication in navigation systems. We report on how an agile design exploration process was applied to examine the design space spanned by the new model and to develop system probes. We discuss the user feedback and its implications for future interface concepts for navigation systems."""	agile software development;map;user interface	Volker Paelke;Karsten Nebe	2009		10.1007/978-3-642-02580-8_19	turn-by-turn navigation;input/output;computer vision;simulation;human–computer interaction;computer science;operating system;specific activity;user interface;mobile robot navigation;difference set	Robotics	-47.11659372564273	-42.009450206534964	102734
a114895bc4fb8f001425a6e29bd7ddac36b80011	finding impressive social content creators: searching for sns illustrators using feedback on motifs and impressions	illustration sharing sns;social tag;tag expansion;illustrator search;relevance feedback;impression tag	We propose a method for finding impressive creators in online social network sites (SNSs). Many users are actively engaged in publishing their own works, sharing visual content on sites such as YouTube or Flickr. In this paper, we focus on the Japanese illustration-sharing SNS, Pixiv. We implement an illustrator search system based on user impression categories. The impressions of illustrators are estimated from clues in the crowdsourced social-tag annotations on their illustrations. We evaluated our system in terms of normalized discounted cumulative gain and found that using feedback on motifs and impressions for illustrations of relevant illustrators improved illustrator search by 11%.	crowdsourcing;flickr;sequence motif;social media;social network	Yohei Seki;Kiyoto Miyajima	2013		10.1145/2484028.2484133	computer science;multimedia;internet privacy;world wide web;information retrieval	HCI	-33.99113475668267	-48.63028727570215	102839
7cc54ba4907db11ac154081989526982bacd3122	a mobile head-mounted display for action sports	eye protection monitoring temperature sensors smart phones optical imaging optical sensors visualization;eye protection;temperature sensors;head mounted display hmd;smart phones;monocular display;visualization;optical imaging;monitoring;sport helmet mounted displays mobile computing;optical sensors;sport;wearable computing;mobile computing;display placement;wearable computing head mounted display hmd display placement monocular display;helmet mounted displays;power source mobile head mounted display action sports monocular display driving electronics	We describe a highly portable head-mounted display designed for use in action sports such as skiing and snowboarding. The low field-of-view, monocular display is designed to keep distraction to a minimum and not impede the mobility of the wearer. Implications of the location of the monocular display are discussed. The driving electronics and power source are small enough so that no tethering is needed, all electronics are attached to the display.	android;head-mounted display;microprocessor;propel;sensor;ubiquitous computing	Reynald Hoskinson;Etienne Naugle	2012	2012 Third FTRA International Conference on Mobile, Ubiquitous, and Intelligent Computing	10.1109/MUSIC.2012.9	computer vision;simulation;engineering;computer graphics (images)	HCI	-40.659780192747405	-42.271670409442805	103282
be895b7cfbb2701e0385f99d7953ed76fdbb16bf	control and applications of smart projector based tele-presence robot	mutual interaction;robot sensing systems mobile communication face three dimensional displays testing medical services;tele presence;haptic feedback tele presence mutual interaction smart projector coexistence space;haptic feedback;telerobotics control engineering computing human computer interaction human robot interaction;coexistence space;smart projector;symmetric camera projector module smart projector telepresence robot	To overcome the tedium of conventional tele-presence systems that are mainly based on `see and talk' between remote users, we developed a novel tele-presence system that has a camera-projector module at both sides: local and remote sides. This symmetric camera-projector module makes the mutual interactions more interactive and broadens feasible communication space out of the monitor. This paper presents some preliminary results and special features of the system that are differentiated from the conventional tele-presence systems.	interaction;robot;television;video projector	Dae-Keun Yoon;Shin-Young Kim;JaiHi Cho;Ji-Yong Lee;Jung-Heum Kwon;Kwang-Kyu Lee;Bum-Jae You	2015	2015 12th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2015.7358827	embedded system;computer vision;simulation;computer science;artificial intelligence;haptic technology	Robotics	-41.77630208857057	-43.26340526941171	103294
cfa35aff0e762b4a4efe68440a5ecf5d515fd5f4	visual, vibrotactile, and force feedback of collisions in virtual environments: effects on performance, mental workload and spatial orientation	perzeption und kognition	In a laboratory study with N = 42 participants (thirty novices and twelve virtual reality (VR) specialists), we evaluated different variants of colli-sion feedback in a virtual environment. Individuals had to perform several ob-ject manipulations (peg-in-hole, narrow passage) in a virtual assembly scenario with three different collision feedback modalities (visual vs. vibrotactile vs. force feedback) and two different task complexities (small vs. large peg or wide vs. narrow passage, respectively). The feedback modalities were evaluated in terms of assembly performance (completion time, movement precision) and subjective user ratings. Altogether, results indicate that high resolution force feedback provided by a robotic arm as input device is superior in terms of movement precision, mental workload, and spatial orientation compared to vi-brotactile and visual feedback systems.	haptic technology	Bernhard Weber;Mikel Sagardia;Thomas Hulin;Carsten Preusche	2013		10.1007/978-3-642-39405-8_28	computer vision;simulation;computer science;multimedia	Visualization	-45.26138081637589	-48.08353433591635	103360
982166e47016c550bd48e3e68c81ed0320c58f07	the bluetooth radio ball interface (brbi): a wireless interface for music/sound control and motion sonification		The Bluetooth Radio Ball Interface (BRBI) is a wireless interface for motion tracking and sonification. The device is embedded in a palm sized foam ball. A 3-dimensional sensor with a Bluetooth module seated in the center of the ball transmits acceleration/tilt measurement data to a computer. Data is then converted into Open Sound Control messages for use with other applications. This paper presents the design concept and implementation of BRBI. Details of its hardware and data handling software are also discussed. Applications include gesture control in music performance as well as sonification of athletics.	bluetooth;embedded system;natural user interface;sonification	Woon Seung Yeo	2006			embedded system;acoustics;computer hardware;engineering	Mobile	-41.878666543597724	-43.04169705495813	103409
7200d755219250be57cd6a5cf25591c50198d613	facial expression training system using bilinear shape model	computer vision;machine learning;game;facial expression;augmented reality	We introduce a facial expression training system using the bilinear shape model which helps people to practice making a facial expression. The user face on the camera preview screen is reconstructed into a 3D face model and the model is transformed to blend shape model which represents the facial expression. This way, the system can precisely analyze the facial expression of the user. With target 3D face model appearing on the screen, the 3D face model changes its facial expression, it leads the user to change his facial expression to become look like same. The system recognizes whether the facial expression of the user is same with the one of 3D face model. As the system gives the various missions to user to change his facial expression, user can practice the facial expression. It can be used for bell's palsy patient who needs face rehabilitation exercise or someone who need to practice unique facial expression such as stewardess smile or facial mimicry.	bilinear filtering;bilinear transform;regular expression	Byung-Hwa Park;Se-Young Oh	2015		10.1145/2814940.2814985	psychology;computer vision;facial action coding system;multimedia;communication;face hallucination	Vision	-38.7614039814252	-38.840487176926175	103551
ee19d52b7239af05810b6bbd2e415b3c7f2c96e7	can haptic feedback improve gesture recognition in 3d handwriting systems?			gesture recognition;haptic technology	Dennis Babu;Seonghwan Kim;Hikaru Nagano;Masashi Konyo;Satoshi Tadokoro	2016		10.1007/978-3-319-43506-0_41	speech recognition;intelligent character recognition;gesture recognition	HCI	-37.698901233169366	-43.36134162325145	103658
395b0ae8fca877cc3d266e22f3f6aa20faade33a	palm touch panel: providing touch sensation through the device	mobile device;touch screen;mobile computer;palm;tactile feedback;tactile display;visual cues;handheld device;on body interaction;visual attention;mobile computers;electro tactile display	We present a novel touch sensitive handheld device, called Palm Touch Panel, which provides electro-tactile feedback on the back of the device thus simulating the sensation of being able to touch the user's palm directly through the device. Users hold the mobile device, which has an electro-tactile display attached at the back. When a finger touches the visual cues on the front screen panel, such as a button or an icon, the electro-tactile display at the back transmits the unique tactile sensation associated with this behavior of the cues to the palm of the hand. As a result, we speculate that the user can manipulate visual information with less visual attention, or even potentially in an eyes-free manner. In this paper we discuss the creation of this unique mobile device that allows the palm to be used for tactile feedback, thus enhancing the touch screen experience.	interaction;mobile device;simulation;touchscreen;usability testing	Shogo Fukushima;Hiroyuki Kajimoto	2011		10.1145/2076354.2076370	computer vision;engineering;multimedia;communication;display device	HCI	-46.12287446668935	-43.178404159919985	103675
d353b01cad6dc217f69d07f6a8f03d89b9295ac6	contractvis highlighter: the visual assistant for the fine print		Navigating and comprehending the legal text of web shops’ general terms and conditions is a burden for consumers. This poster abstract describes work-in-progress to design a visualization environment specifically addressing the needs of online shoppers. This environment highlights keywords of relevance (e.g., returning items), provides visual overview, and supports comparison of two texts.	online shopping;relevance;world wide web	Alexander Rind;Florian Grassinger;Armin Kirchknopf;Christina Stoiber;Aslihan Özüyilmaz	2018			multimedia;fine print;art	HCI	-35.78612458437987	-49.768643471820745	103688
e76fb4b71ad5db36a2afa7a541bbc1817abaa9c7	responses of participants during an immersive virtual environment experience.	urban environment;ucl;virtual characters;stress response;discovery;theses;conference proceedings;indexing terms;digital web resources;ucl discovery;open access;ucl library;mental stress;book chapters;open access repository;physiological response;experience design;immersive virtual environment;ucl research	—This paper reports on the results of an experiment designed to study fine grain physiological responses of participants to an immersive virtual simulation of an urban environment. An experiment was carried out with 40 participants who were asked to walk through a virtual street, which had virtual characters walking through it. The analysis of differences in participant responses at various stages of the experiment (baseline recordings, training, first half and second half of the urban simulation) is examined in detail. It was found that participants typically show a stress response during the training phase and a stress response towards the end of the simulation of the urban experience. The impact of realism on the reported presence was evaluated. The results suggest that the lowest presence was achieved with the higher fidelity characters but the less varied textures. There is some evidence also that variations in the level of visual realism based the texture strategy used was associated with changes in mental stress.	baseline (configuration management);simulation;urban computing	Andrea Brogni;Vinoba Vinayagamoorthy;Anthony Steed;Mel Slater	2007	IJVR		fight-or-flight response;simulation;index term;human–computer interaction;experience design;computer science;artificial intelligence;multimedia;world wide web	HCI	-46.80480685960146	-49.45521544702352	103869
7d39f4032873fc21cdcaae20e0e1f27745954485	vibrotactile inputs to the feet can modulate vection	vibrations;virtual reality;i 2 10 vision and scene understanding motion perceptual reasoning h l 2 user interfaces haptic i o theory and methods;vibrotactile stimulus vibrotactile input central linear vection self motion visual field visual flow stimulation peripheral vision stimulation vibrotactile feet stimulation vibratory stimuli sinusoidal signal pink noise chirp signal sinusoidal vibration;virtual reality haptic interfaces vibrations;vibrations visualization chirp haptic interfaces foot virtual reality acceleration;haptic interfaces	Vection refers to the illusion of self-motion when a significant portion of the visual field is stimulated by visual flow, while body is still. Vection is known to be strong for peripheral vision stimulation and relatively weak for central vision. In this paper, the results of an experimental study of central linear vection with and without vibrotactile feet stimulation are presented. Three types of vibratory stimuli were used: a sinusoidal signal, pink noise, and a chirp signal. Six subjects faced a screen looking at a looming visual flow that suggested virtual forward motion. The results showed that the sensation of self-motion happened faster and its intensity was the strongest for sinusoidal vibrations at constant frequency. For some subjects, a vibrotactile stimulus with an increasing frequency (a chirp) elicited as well a stronger vection. The strength of sensation of self-motion was the lowest in the cases when pink noise vibrations and no vibrotactile stimulation accompanied the visual flow stimulation. Possible application areas are mentioned.	chirp;experiment;peripheral vision;pink noise;visualflow	Ildar Farkhatdinov;Nizar Ouarti;Vincent Hayward	2013	2013 World Haptics Conference (WHC)	10.1109/WHC.2013.6548490	computer vision;acoustics;computer science;engineering;artificial intelligence;vibration;virtual reality;physics	HCI	-45.1835632837544	-50.44412342391468	103988
014b0d6d8501b2703cfe0de5e99b7064a8228207	development of color qr code for increasing capacity	image color analysis decoding encoding standards cameras java image coding;hsv color model qr code two dimensional barcode qr code color qr code quick response code;qr codes android operating system java;color qr code android application java application mobile phone code reader qr code structure 2d barcode color quick response code;color qr code;hsv color model;two dimensional barcode;qr code;quick response code	Barcodes have been widely popular. Their popularity has encouraged an ongoing invention of decoding methods. Barcodes can be categorized into 2 main groups, namely one-dimension (1D) barcodes at which information is stored horizontally and two-dimension (2D) barcodes which contain information in both vertical and horizontal direction, promising a higher storage capacity compared to 1D barcodes. Despite high data density, an amount of information obtained in 2D barcodes still limited to some extent. This study selected QR Code (Quick Response Code) is a type of 2D barcode because firstly, it can handle a variety of information. Secondly, decoding is reasonably straightforward. Finally, the structure of QR code is specified clearly by its developer. This research aimed to increase QR Code capacity by proposing a color Quick Response Code (color QR code) encoding concept which can hold a larger amount of information than that of the traditional black and white QR Code regarding their physical size. A two-color (black and white) QR Code can store 1 bit in each module only, whereas a module of a color QR code with sixteen different colors can contain 4-bit data. In order to decode a color QR code, this study used a code reader equipped with at least an 8-megapixel camera and a decoding application was developed on Android (Android application on mobile phone) and Java (Java application on PC) platform.	1-bit architecture;4-bit;android;areal density (computer storage);barcode;categorization;color;decoding methods;java;mobile phone;qr code	Nutchanad Taveerad;Sartid Vongpradhip	2015	2015 11th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)	10.1109/SITIS.2015.42	embedded system;computer hardware;computer science;computer graphics (images)	SE	-39.86754368261532	-40.15785785340927	104094
72fa2caed386e3ad95bb8a9988f918f5e7c8767e	a new gesture control for zooming on tablets and smartphones for visually impaired people		We propose a new interaction model for zooming in and out the screen content of smartphones and tablets for visually impaired people. It consists in gestures that are captured by the front-facing camera to control the zooming factor and to scroll in the zoomed-in image. A first prototype involving video processing has been developed. As it avoids using a hand to touch the screen, this interaction model is interesting for other mobile applications. Human factor study and tests has been performed on visually impaired people and operators using assistance with augmented reality on tablets.	smartphone;tablet computer	Nicolas Girard;Salma Cherké;Hélène Soubaras;Kees Nieuwenhuis	2018		10.1007/978-3-319-94274-2_49	gesture recognition;video processing;operator (computer programming);computer vision;interaction model;augmented reality;zoom;gesture;artificial intelligence;computer science	HCI	-44.806112767110974	-42.19277620300504	104406
a0bff6eeaf915fbc363951113435f6ccca6fb763	auditory self-motion simulation is facilitated by haptic and vibrational cues suggesting the possibility of actual motion	individualized binaural recordings;vibrations;higher level cognitive influences;virtual reality;spatial sound;motion perception;human factors;cue integration;auditory vection;situation awareness;hrtf;circular vection;self motion simulation;self motion illusions;psychophysics	Sound fields rotating around stationary blindfolded listeners sometimes elicit auditory circular vection, the illusion that the listener is physically rotating. Experiment 1 investigated whether auditory circular vection depends on participants' situational awareness of “movability,” that is, whether they sense/know that actual motion is possible or not. While previous studies often seated participants on movable chairs to suspend the disbelief of self-motion, it has never been investigated whether this does, in fact, facilitate auditory vection. To this end, 23 blindfolded participants were seated on a hammock chair with their feet either on solid ground (“movement impossible”) or suspended (“movement possible”) while listening to individualized binaural recordings of two sound sources rotating synchronously at 60°/s. Although participants never physically moved, situational awareness of movability facilitated auditory vection. Moreover, adding slight vibrations like the ones resulting from actual chair rotation increased the frequency and intensity of vection. Experiment 2 extended these findings and showed that nonindividualized binaural recordings were as effective in inducing auditory circular vection as individualized recordings. These results have important implications both for our theoretical understanding of self-motion perception and for the applied field of self-motion simulations, where vibrations, nonindividualized binaural sound, and the cognitive/perceptual framework of movability can typically be provided at minimal cost and effort.	binaural beats;haptic technology;motion simulator;movable type;simulation;stationary process	Bernhard E. Riecke;Daniel Feuereissen;John J. Rieser	2009	TAP	10.1145/1577755.1577763	psychology;situation awareness;simulation;motion perception;computer science;vibration;head-related transfer function;virtual reality;communication;psychophysics	HCI	-45.41681940448167	-50.315496201834534	104554
0a66e531c7ba01fb37ea6f75f9ca631aa357ed5d	nobunaga: multicylinder-like pulse generator for kinesthetic illusion of being pulled smoothly	oscillations;design and development;nongrounding;continuous force sensation;illusion;human perception;sequential pulse	We have proposed a force perception method based on asymmetric oscillation that exploits the characteristics of human perception. Our previous findings indicate that the pulse frequency determines the effective generation of the kinesthetic illusion of being pulled. However, whether pulse frequency or pulse width for force perception has not been clarified. If the pulse width is more dominant, the force sensation induced by sequential pulses will be more continuous. This is important because many of those who have experienced the asymmetric oscillation pointed out that the force sensation induced by the stimuli was not felt smoothly compared to physical force. This paper describes the design and development of a new multicylinder-like mechanism for generating sequential pulses, which should enable us to determine which is dominant for force perception.	pulse generator;smoothing	Tomohiro Amemiya;Taro Maeda	2008		10.1007/978-3-540-69057-3_75	psychology;computer vision;simulation;communication	Crypto	-45.57412435497625	-50.74103749809577	104567
1f8be47cceb5540d90f3a4c861a1325364f14cd4	wearable interfaces for orientation and wayfinding	speech interfaces;street crossing;environmental conditions;orientation aid;blindness;universal usability;wayfinding;visual impairment	People with severe visual impairment need a means of remaining oriented to their environment as they move through it. Three wearable orientation interfaces were developed and evaluated toward this purpose: a stereophonic sonic guide (sonic “carrot”), speech output, and shoulder-tapping system. Street crossing was used as a critical test setting in which to evaluate these interfaces. The shoulder-tapping system was found most universally usable. Considering the great variety of co-morbidities within this population, the authors concluded that a combined tapping/speech interface would provide usability and flexibility to the greatest number of people under the widest range of environmental conditions.	usability;wearable computer	David A. Ross;Bruce B. Blasch	2000		10.1145/354324.354380	computer vision;human–computer interaction;computer science;universal usability	HCI	-48.25823374446506	-42.895892976896825	104644
6b7b2c285fd7d628ab81d227949756fd0f5c0e92	virtual, augmented and mixed reality		In this paper, we present the design of spatial visualization training system implemented by augmented reality (AR). Spatial visualization is the ability to mentally transform complex stimuli in space. However, this ability declines with human age, resulting in spatial problems in one’s normal life. Based on the fact that AR interface can reduce cognitive load and provide correct spatial information, we are devoted to designing an AR spatial visualization training system for older adults to use. The system consists of a manual controller and a visualization training task. In the process of manual controller design, think aloud experiment is adopted to generate intuitive manipulation, and morphological analysis is used to evaluate the most elderly-friendly controller. In the process of training task design, by analyzing spatial training factors, a new visualization training task is designed. In the process of AR integration, the system is implemented by Qualcomm AR in Unity3D with Vuforia protal, and the final AR based spatial visualization ability training system is completed.	augmented reality;controller (computing);mixed reality;think aloud protocol;unity	Randall Shumaker;Stephanie Lackey	2015		10.1007/978-3-319-21067-4	simulation;human–computer interaction;engineering;multimedia	Visualization	-47.3771371310703	-49.34927051956967	104759
015f6a1cb8420d22826951a9ccbaa9297b108a26	ubiquitous interaction with positioning and navigation using a novel light sensor-based information transmission system	pulse frequency modulation;electronic ballast;information transmission;mobile computer;fluorescent lamp;navigation;indoor environment;wearable computer;positional information;augmented reality;indoor positioning	This paper describes a novel light sensor-based information transmission system for indoor positioning and navigation with particular benefits for mobile and wearable computers. It can seamlessly extend outdoor GPS tracking to the indoor environment. In a novel manner, fluorescent light is used as the medium to transmit information, which is encoded by using a pulse-frequency modulation technique. The user receives the encoded light information through a photo-receiver. The information is passed into the wearable or mobile computer after the data is decoded. This information allows positioning information to be given to indoor mobile and wearable computers. We design an economical transmitter circuit by adding few components to a commercial electronic ballast circuit for fluorescent lamps with price of less than US $10. The propose system can be used in indoor guidance and navigation applications. Exemplary applications are included in the paper, with experimented results detailed.	electrical ballast;gps tracking unit;global positioning system;human–computer interaction;mobile computing;modulation;transmitter;wearable computer	Adrian David Cheok;Yue Li	2007	Personal and Ubiquitous Computing	10.1007/s00779-007-0140-9	embedded system;augmented reality;navigation;pulse-frequency modulation;wearable computer;human–computer interaction;telecommunications;computer science;operating system	Mobile	-44.56579987107938	-41.45424917831596	104907
5401769304d206ba3c581895369d5ab723f64173	perceiving texture gradients on an electrostatic friction display		Two experiments tested young adults' ability to discriminate the direction of friction-defined textural gradients rendered by the Senseg FeelScreen™. Gradients were particularly effective when they spanned the low end of the intensity range. This trend likely reflects saturation of the device's rendering capabilities at high intensities, as confirmed by measurements with a manual linear tribometer. The results show promise for use of gradients rendered with variable friction displays to aid non-visual navigation on tablets.	experiment;image gradient;machine vision	Roberta L. Klatzky;Sara Adkins;Prachi Bodas;Reza Haghighi Osgouei;Seungmoon Choi;Hong Z. Tan	2017	2017 IEEE World Haptics Conference (WHC)	10.1109/WHC.2017.7989893	tribometer;artificial intelligence;computer vision;rendering (computer graphics);computer science;saturation (chemistry);surface finish;optics	Visualization	-43.455441158526575	-49.82567840049903	105049
c582477108accd6cfe5200dc74bc6cccd9c28ce8	transparent tactile switch for touch screen interface				Fumihito Arai;Naoya Iwata;Toshio Fukuda	2006	JRM	10.20965/jrm.2006.p0375	human–machine interface;computer hardware;computer science	HCI	-43.07751206434387	-41.42455860070826	105055
66e7aa0eb41a3bc6c2ed15046ad934287fa933a0	a case study on human learning of four-dimensional objects in virtual reality: passive exploration and display techniques	behavioural sciences computing;virtual reality;mathematical cognition;euclidean distance;computer graphic;humans virtual reality computer graphics computational modeling cognition hypercubes psychology three dimensional displays computer science computer displays;4d intuition;projection technique human learning four dimensional objects virtual reality computer graphic techniques human perceptual abilities randomly shaped hyper tetrahedrons euclidean distance geometric objects slicing technique distance judgment;cognition;spatial representation;virtual reality 4d intuition spatial representation mathematical cognition;virtual reality behavioural sciences computing cognition	A recent study has shown that computer graphic techniques may be used to extend human perceptual abilities in comprehending four-dimensional space. The current case study explored conditions such learning can occur. An experienced human observer watched computer generated graphical simulations of randomly shaped hyper-tetrahedrons and made judgments about the Euclidean distance between two of the vertices in four-dimensional space. The geometric objects were displayed using a slicing technique with the objects either translating along the w-axis or rotating along the wx-plane, or using a projection technique. The results showed that learning four-dimensional objects for distance judgment does not require active manipulation of the objects and can be achieved through various display techniques.	apache axis;computer graphics;display device;euclidean distance;graphical user interface;randomness;simulation;virtual reality	Ranxiao Frances Wang	2009	2009 Fourth International Conference on Frontier of Computer Science and Technology	10.1109/FCST.2009.93	computer vision;simulation;cognition;computer science;artificial intelligence;machine learning;euclidean distance;virtual reality	Visualization	-43.29329894601574	-48.83254259763774	105399
aeee9f9e8699408e4aa52a389e544ba13237af1e	tangible lightscapes	light interface;tangible interface	"""The aim of my exploration is to design a vocabulary of light states to show people what their devices are doing. This vocabulary consists of light behaviours and gestures that can be applied to a wide range of contexts where devices (speakers, headphones, memory storage devices, cameras, laptops...) are communicating wirelessly. This """"device language"""" gives a concrete representation of the intangible and invisible events that are taking place. It allows users to feel more in control by providing them with a direct interaction with the objects they are using.  The vocabulary is described though 3 main tools: (1) a map of the light states/gestures matched with the most common activities of wireless devices, (2) a set of prototypes which represent three-dimensionally the vocabulary for demonstration and user testing purposes, (3) two scenarios visualizing the light behaviours as applied to different devices in everyday life situations."""	headphones;laptop;usability testing;vocabulary	Alice V. Pintus	2010		10.1145/1709886.1709988	human–computer interaction;engineering;multimedia;communication	HCI	-47.11593528972171	-39.581904933101505	105430
9b876d8b002e75883ffc3ad438e1be364c48caf3	an artificial neural network for the design of an adaptive multimodal interface	adaptive multimodal interface design;direct manipulation multimodal interface;image recognition;prototyping tool;multimodal interface;experimentation protocol;neural nets;software prototyping;experimentation protocol artificial neural network adaptive multimodal interface design direct manipulation multimodal interface force feedback multimodal user interface system prototyping tool;direct manipulation;prototypes;information technology;multimodal user interface;process design;artificial neural networks force feedback prototypes process design software tools software prototyping speech recognition image recognition haptic interfaces information technology;force feedback;artificial neural networks;feedback;adaptive systems software prototyping user interface management systems neural nets feedback tactile sensors interactive devices;adaptive systems;multimodal user interface system;tactile sensors;speech recognition;software tools;user interface management systems;haptic interfaces;artificial neural network;interactive devices;force feeback	This article describes the use of an artificial neural network (AW) for facilitating the design and evaluation of a direct manipulation multimodal interface with forcefeedback. This article: describes the multimodal interface prototyping tool (MUIS); discusses the design problem; develops the conceptual basis of artificial neural networks; and discusses the results of the experimentation protocol used for evaluating the pertinence of our approach.	adjusted winner procedure;artificial neural network;direct manipulation interface;multimodal interaction;relevance	Jean-François Arcand;Christophe Ramstein	1995		10.1109/TAI.1995.479843	process design;embedded system;simulation;human–computer interaction;computer science;artificial intelligence;machine learning;feedback;prototype;haptic technology;artificial neural network;tactile sensor	HCI	-36.16470046484929	-45.49538029795462	105447
93a9da8829aebe1c0d3292eafe0681227516afc0	wearable head-mounted 3d tactile display application scenarios	virtual reality;immersion;assistive technology;haptic feedback;guidance;vibrotactile;augmented reality	Current generation virtual reality (VR) and augmented reality (AR) head-mounted displays (HMDs) usually include no or only a single vibration motor for haptic feedback and do not use it for guidance. In a previous work, we presented HapticHead, a potentially mobile system utilizing vibration motors distributed in three concentric ellipses around the head to give intuitive haptic guidance hints and to increase immersion for VR and AR applications. The purpose of this paper is to explore potential application scenarios and aesthetic possibilities of the proposed concept in order to create an active discussion amongst workshop participants.	augmented reality;beanie babies;haptic technology;head-mounted display;immersion (virtual reality);mobile app;mobile device;smartwatch;virtual reality;wearable computer	Oliver Beren Kaul;Michael Rohs	2016		10.1145/2957265.2965022	computer vision;augmented reality;computer-mediated reality;simulation;computer science;artificial intelligence;virtual reality;multimedia;haptic technology;immersion	HCI	-46.524962785849766	-42.78826207185157	105527
3fa89b2de4d45f5dfdec9336eb9d7f822c11528f	reducing interference in single display groupware through transparency	input device;collaborative work;controlled experiment;single display groupware;face to face	Single Display Groupware (SDG) supports face-to-face collaborators working over a single shared display, where all people have their own input device. Although SDG is simple in concept, there are surprisingly many problems in how interactions within SDG are managed. One problem is the potential for interference, where one person can raise an interface component (such as a menu or dialog box) in a way that hinders what another person is doing i.e., by obscuring another person’s working area that happens to be underneath the raised component. We propose transparent interface components as one possible solution to interference: while one person can raise and interact with the component, others can see through it and can continue to work underneath it. To test this concept, we first implemented a simple SDG game using both opaque and transparent SDG menus. Through a controlled experiment, we then analysed how interference affects peoples’ performance across an opaque and transparent menu condition: a solo condition (where a person played alone) acts as our control. Our results show that the transparent menu did lessen the effect of interference, and that SDG players overwhelmingly preferred it to opaque menus.	collaborative software;experiment;input device;interaction;interference (communication);structure of observed learning outcome;widget (gui);dialog	Ana Zanella;Saul Greenberg	2001		10.1007/0-306-48019-0_18	simulation;human–computer interaction;computer science;operating system;communication;world wide web;input device	HCI	-46.614727478104534	-38.46143554236117	105618
bd80340dce611acc303b70e557a907596cc94718	measurement of individual changes in the performance of human stereoscopic vision for disparities at the limits of the zone of comfortable viewing	3d displays;stereoscopic stimuli;decision rates;human stereoscopic visual performance;convergence accommodation conflict;visualization time factors three dimensional displays educational institutions stereo image processing adaptive optics optical imaging;visual perception stereo image processing three dimensional displays;visual discomfort 3d perception stereopsis stereo vision 3d displays;individual change measurement;comfortable viewing zone;visualization;four alternative forced choice setup;time factors;optical imaging;3d content;three dimensional displays;stereo image processing;stereo vision;3d perception;visual perception;comfortable viewing zone 3d displays visual impressions human perception convergence accommodation conflict visual discomfort visual perception 3d content visual system human stereoscopic visual performance stereoscopic stimuli four alternative forced choice setup response times decision rates deciding disparities individual change measurement;deciding disparities;response times;stereopsis;visual system;visual impressions;human perception;visual discomfort;adaptive optics	3D displays enable immersive visual impressions but the impact on the human perception still is not fully understood. Viewing conditions like the convergence-accommodation (C-A) conflict have an unnatural influence on the visual system and might even lead to visual discomfort. As visual perception is individual we assumed the impact of simulated 3D content on the visual system to be as well. In this study we aimed to analyze the stereoscopic visual performance of 17 subjects for disparities inside and outside the in literature defined zone of comfortable viewing to provide an individual evaluation of the impact of increased disparities on the performance of the visual system. Stereoscopic stimuli were presented in a four-alternative forced choice (4AFC) setup in different disparities. The response times as well as the correct decision rates indicated the performance of stereoscopic vision. The results showed that increased disparities lead to a decline in performance. Further, the impact of the presented disparities is dependent on the difficulty of the task. The decline of performance as well as the deciding disparities for the decline were subject dependent.	color vision;stereo display;stereopsis;stereoscopy	Jan Paulus;Georg Michelson;Marcus Barkowsky;Joachim Hornegger;Bjoern M. Eskofier;Michael Schmidt	2013	2013 International Conference on 3D Vision	10.1109/3DV.2013.48	psychology;computer vision;simulation;multimedia	Visualization	-43.14529655353211	-49.66855319087183	105659
2f2a16d0f0fd1fd1626f59080abd6953bb356e37	avatar weight estimates based on footstep sounds in three presentation formats	computers;legged locomotion;footstep sound design avatar weight estimates virtual environment interactive immersive vr format sound design evaluation task perceived weight virtual avatar seen;virtual environments;games legged locomotion avatars virtual environments computers visualization monitoring;visualization;monitoring;games;hearing avatars;avatars	When evaluating a sound design for virtual environment, the context where it is to be implemented in may have an influence on how it may be perceived. In this paper we perform an experiment comparing three presentation formats (audio only, video with audio and an interactive immersive VR format) and their influences on a sound design evaluation task concerning footstep sounds. The evaluation involved estimating the perceived weight of a virtual avatar seen from a first person perspective, as well as the suitability of the sound effect relative to the context. The results show significant differences for three cases between the presentation formats, both for weight estimates and suitability ratings over all variations of the footstep sound design. The remaining 15 comparisons each for both weight estimates and suitability ratings were not significant. However there were noticeably more significant differences between the sound designs variations that were evaluated in the audio only condition than in the two other presentation formats.	audio filter;avatar (computing);coherence (physics);first-person (video games);pc game;sound quality;virtual reality	Erik Sikström;Amalia de Götzen;Stefania Serafin	2015	2015 IEEE 2nd VR Workshop on Sonic Interactions for Virtual Environments (SIVE)	10.1109/SIVE.2015.7361295	simulation;computer science;multimedia;computer graphics (images)	Visualization	-47.93342807698385	-50.076733878975325	105751
43d19b51a041792de7f71ab215e7624860c20d74	a survey on haptic technologies for mobile augmented reality		Augmented Reality (AR) and Mobile Augmented Reality (MAR) applications have gained much research and industry attention these days. The mobile nature of MAR applications limits users’ interaction capabilities such as inputs, and haptic feedbacks. This survey reviews current research issues in the area of human computer interaction for MAR and haptic devices. The survey first presents human sensing capabilities and their applicability in AR applications. We classify haptic devices into two groups according to the triggered sense: cutaneous/tactile: touch, active surfaces, and mid-air; kinesthetic: manipulandum, grasp, and exoskeleton. Due to the mobile capabilities of MAR applications, we mainly focus our study on wearable haptic devices for each category and their AR possibilities. To conclude, we discuss the future paths that haptic feedbacks should follow for MAR applications and their challenges.	a/ux;augmented reality;ecosystem;haptic technology;human computer;human–computer interaction;software deployment;wearable computer;wearable technology	Carlos Bermejo;Pan Hui	2017	CoRR		multimedia;human–computer interaction;computer science;haptic technology;computer-mediated reality;wearable computer;augmented reality;grasp;kinesthetic learning	HCI	-48.00912053073046	-41.37984851939109	105753
5f6820ef107cb4ce43f261de6b737075330150f9	evaluation of user-friendliness of a compact input device with simple tactile feedback	doigt;feedback mechanism;pointing device;input device;capteur tactile;electric stimulation;input output equipment;tactile sensor;user interface;sensor tactil;degree of freedom;ilusion;feedback;equipement entree sortie;tactile feedback;sensibilidad tactil;illusion;equipo entrada salida;haptic feedback;finger;boucle reaction;retroalimentacion;dedo;low power consumption;sensibilite tactile;tactile sensitivity;haptic interface	Compact input devices are rather user-unfriendly because their narrow key pitch confuses finger pressure at a key touch pad. We have attempted to improve user-friendliness of such input devices by introducing a mechanism that enhances tactile feedback and creating an illusion of more distinguishable tactile space than the actual one. A trial model with such a mechanism is demonstrated and a GUI in association with haptic feedback of the device is effectively introduced.	input device	Itsuo Kumazawa	2004		10.1007/978-3-540-24678-7_12	computer science;artificial intelligence;operating system;control theory;feedback;haptic technology;illusion	HCI	-44.947127011188485	-44.32193017869296	105970
b0c51be29243c35369e453a2301cfc1e56d91da1	the study of auditory and haptic signals in a virtual reality-based hand rehabilitation system	haptic signals;tecnologia industrial tecnologia mecanica;tecnologia electronica telecomunicaciones;virtual reality;hand rehabilitation;tecnologias;grupo a	The purpose of the present study is to assess the influence of auditory and haptic signals on the manipulation performance in a virtual reality-based hand rehabilitation system. A personal computer, a tracker, and a data glove were included in this system. Three-dimensional virtual environments were developed. Forty volunteers were recruited to participate in a pick-and-place procedure, with three levels of difficulty and four feedback modes. Task time and collision frequency were the parameters used to evaluate their manipulation performance. It can be concluded that the haptics is a significant signal for improving a subject's performance at the high difficulty level.	haptic technology;virtual reality	Chang-Yih Shing;Chin-Ping Fung;Tien-Yow Chuang;I-Wen Penn;Ji-Liang Doong	2003	Robotica	10.1017/S0263574702004708	embedded system;simulation;computer science;engineering;artificial intelligence;virtual reality	HCI	-46.36095301031976	-48.60036676885264	106003
2930c00e08e2b9bc7dbe0523e1c0cabcab5a4ffe	responsive lighting: the city becomes alive	experience;responsive lighting;urban lighting;public space;experiment;lighting;ineraction;mobile communications;interaction design;mobile interaction	We distributed fourteen controllable street lamps in a city square and recorded three comparative and one 'usual' condition, operating the public lighting as if it were an interactive stage. First tested was adaptive lighting that responded to people's occupancy patterns. Second was a mobile phone application that allowed people to customise color and responsive behaviours in the overhead lighting system. Third was ambient lighting, responding to wind velocity. The study extends the discussion on multiuser interaction design in public lighting by asking: how can interactions using mobile phones, thermal tracking and wind inputs afford new social behaviors, without disturbing the usual public functions of street lighting? This research lays foundational work on the affordances of mobile phones for engagement and interaction with public lighting. The study indicates the use of personal phones as a tool for interaction in this setting has potential to provide a stronger ownership to urban place.	interaction design;mobile app;mobile phone;multi-user;overhead (computing);the wall street journal;velocity (software development)	Esben Skouboe Poulsen;Ann Morrison;Hans Jørgen Andersen;Ole Bjarlin Jensen	2013		10.1145/2493190.2493218	experiment;simulation;mobile interaction;human–computer interaction;computer science;interaction design;lighting;multimedia;smart lighting	HCI	-47.04993039918075	-39.7285149593494	106074
838386c610c3b52c39634c8e6a73c81d0084bc98	[demo] displaying free-viewpoint video with user controlable head mounted display demo	billboard head mounted display free viewpoint video	In this paper, we propose a method to experience a free-viewpoint video and image with a head mounted display (HMD) and a game controller that enable to operate it intuitively. The free-viewpoint video is generated by multiple 4K resolution cameras in sport games such as soccer and american football. This method can provide us a player's perspective. We adopt a billboard method to make a free-viewpoint video which is consisted of multiple textures accoding to user specified viewpoint. For implementing the program and displaying images effectively we used a game development system, a HMD and a game controller that user can operate their own views in the high degree of freedom. Experiment results show that the proposed method can obviously achive effective view.		Yuko Yoshida;Tetsuya Kawamoto	2014		10.1109/ISMAR.2014.6948503	computer vision;computer science;multimedia;computer graphics (images)	HCI	-41.77338677033536	-38.3666393644395	106251
9d38c14de6ace6763bec9b115582e18f672ac0a2	shift: a technique for operating pen-based interfaces using touch	pen based interface;mobile device;occlusion;touch screen;user study;interaction techniques;precise target acquisition;touch screens;error rate;visual feedback;mobile devices;target acquisition;interaction technique	Retrieving the stylus of a pen-based device takes time and requires a second hand. Especially for short intermittent interactions many users therefore choose to use their bare fingers. Although convenient, this increases targeting times and error rates. We argue that the main reasons are the occlusion of the target by the user's finger and ambiguity about which part of the finger defines the selection point. We propose a pointing technique we call Shift that is designed to address these issues. When the user touches the screen, Shift creates a callout showing a copy of the occluded screen area and places it in a non-occluded location. The callout also shows a pointer representing the selection point of the finger. Using this visual feedback, users guide the pointer into the target by moving their finger on the screen surface and commit the target acquisition by lifting the finger. Unlike existing techniques, Shift is only invoked when necessary--over large targets no callout is created and users enjoy the full performance of an unaltered touch screen. We report the results of a user study showing that with Shift participants can select small targets with much lower error rates than an unaided touch screen and that Shift is faster than Offset Cursor for larger targets.	conditional (computer programming);dos;hidden surface determination;interaction;lambda lifting;microsoft research;personal digital assistant;pointer (computer programming);privilege escalation;raman scattering;software deployment;stylus (computing);touchscreen;usability testing	Daniel Vogel;Patrick Baudisch	2007		10.1145/1240624.1240727	computer vision;human–computer interaction;computer hardware;computer science;operating system;mobile device;computer graphics (images)	HCI	-46.240579134873904	-45.64347217040505	106592
2d26af9e788edddd4de9afa38bdd5f605b2c5730	space-shared communication based on truly 3d information space	image processing;3d imaging;computational geometry;information space;operator space;stereoscopic glasses 3d information space 3d geometric modeling space shared communication 3d images 3d image display parallax focus function;image generation;computer displays;geometric model;three dimensional displays character generation solid modeling computational geometry orbital robotics focusing eyes image generation lenses glass;computer displays image processing computational geometry;3d display	"""In order to achieve space-shared communication, it is essential to develop a truly 3D information space. Therefore, we aimed to construct a unified 3D CG space, that is, a unified 3D geometric modeling system. As for the basic coding method, """"Extended Geometry Scheme"""" is proposed. By using this scheme, we have realized the 3D CG space with unified structure. Another important aspect of the space-shared communication is the development of teleoperation using robots on networks. It is important that 3D images are displayed naturally without wrong feeling in such real operation space. We have developed a 3D image display method using the parallax and focus function of human's eyes simultaneously. The proposed 3D display method is a combination of real image generation by Fresnel lens and parallax display with stereoscopic glasses. By using this method, it is possible to display realistic 3D images."""		Yoshiki Arakawa;Hideki Kakeya;Mitsuo Isogai;Kenji Suzuki;Fujio Yamaguchi	1999		10.1109/ICIP.1999.817063	operator space;stereoscopy;computer vision;stereo display;image processing;computational geometry;computer science;geometric modeling;mathematics;computer graphics (images)	HPC	-41.40860006050109	-38.636577485543484	106709
04508cd6d1850be87480fa8173433c0ad0c37ace	interactive virtual exhibition: creating custom virtual art galleries using web technologies		This paper presents an immersive 3D virtual reality application accessed through the web that allows users to create their own custom virtual art galleries. The application allows users to select paintings based on a time range or country and then it dynamically generates the 3D virtual exhibit. Various features about the exhibit can be customized, such as the floor texture and wall color. Users can also save their exhibit, so it can be shared with others.	online exhibition;virtual art;virtual reality	Saadiq K. Shaik;Kyungjin Yoo	2018		10.1145/3281505.3281619	exhibition;computer vision;human–computer interaction;immersion (virtual reality);artificial intelligence;computer science;virtual reality;user interface;virtual art;painting	Visualization	-43.25172366117784	-38.29717974520426	107099
c2a01844868cbe617caf82d0f3d02d2f25c77046	the flat finger: exploring area touches on smartwatches	smartwatch;shape touch;input technique;area touch	Smartwatches are emerging device category that feature highly limited input and display surfaces. We explore how touch contact areas, such as lines generated by flat fingers, can be used to increase input expressivity in these diminutive systems in three ways. Firstly, we present four design themes that emerged from an ideation workshop in which five designers proposed concepts for smartwatch touch area interaction. Secondly, we describe a sensor unit and study that captured user performance with 31 area touches and contrasted this against standard targeting performance. Finally, we describe three demonstration applications that instantiate ideas from the workshop and deploy the most reliably and rapidly produced area touches. We report generally positive user reactions to these demonstrators: the area touch interactions were perceived as quick, convenient and easy to learn and remember. Together this work characterizes how designers can use area touches in watch UIs, which area touches are most appropriate and how users respond to this interaction style.	android;apple watch;application programming interface;bespoke;ecology;expressive power (computer science);interaction;machine learning;next-generation network;operating system;sensor;smartwatch;theme (computing);typical set;warez	Ian Oakley;Carina Lindahl;Khanh Le;Doyoung Lee;Md. Rasel Islam	2016		10.1145/2858036.2858179	simulation;human–computer interaction;computer science;operating system;smartwatch;multimedia	HCI	-47.59064034761639	-41.05748191495715	107170
66de645a45ce965208cddfe1cdd5ef0f44d7d6da	hearwear: the fashion of environmental noise display	environmental;urban fashion;wearable technology;noise pollution;fashion and technology;noise	HearWear is an electronic wearable, which is not only a fashion apparel but also reacts to urban noise with moving light patterns. HearWear keeps the wearer and all passers-by in touch with their environment through a playful display of urban sounds and noise pollution.	wearable technology	Milena Iossifova;Younghui Kim	2004		10.1145/1186155.1186163	noise pollution;noise;multimedia;wearable technology	HCI	-46.848372781896074	-39.956808707631716	107468
b67dc13327c20e547a253d079ca53fff8fb8f8ad	android application for sending sms messages with speech recognition interface	google;voice sms;hmm android application sms messages speech recognition interface voice sms internet google server android sdk mobile phone android operating system hidden markov models;hmm;smart phones;speech;user interfaces electronic messaging hidden markov models internet mobile computing operating systems computers speech recognition;android operating system;mobile phone;sms messages;hidden markov models;internet;electronic messaging;speech recognition;google server;speech recognition interface;android sdk;mobile computing;android application;user interfaces;operating systems computers;speech recognition speech hidden markov models smart phones google operating systems;operating systems	Voice SMS is an application developed in this work that allows a user to record and convert spoken messages into SMS text message. User can send messages to the entered phone number or the number of contact from the phonebook. Speech recognition is done via the Internet, connecting to Google's server. The application is adapted to input messages in English. Used tools are Android SDK and the installation is done on mobile phone with Android operating system. In this article we will give basic features of the speech recognition and used algorithm. Speech recognition for Voice SMS uses a technique based on hidden Markov models (HMM - Hidden Markov Model). It is currently the most successful and most flexible approach to speech recognition.	algorithm;android software development;hidden markov model;internet;markov chain;mobile phone;operating system;server (computing);software development kit;speech recognition;telephone number	Sanja Primorac;Mladen Russo	2012	2012 Proceedings of the 35th International Convention MIPRO		voice activity detection;the internet;speech recognition;concatenated sms;computer science;speech;operating system;internet privacy;user interface;world wide web;hidden markov model;short message service	ML	-36.30737360345414	-44.20441530637032	107510
63418ae348da7ab327ab0f4245a598731328e7bf	machine vision system to induct binocular wide-angle foveated information into both the human and computers - feature generation algorithm based on dft for binocular fixation -	binocular fixation;magnetic heads;feature generation;fovea;virtual reality;computer vision;stereo matching;three dimensional displays;3d hmd;machine vision;lgn device fovea stereo vision 3d hmd binocular fixation feature generation dft stereo matching;stereo vision;humans;signal processing algorithms;discrete fourier transforms;visual system;lgn device;dft;cameras;machine vision humans computer vision signal processing algorithms discrete fourier transforms cameras magnetic heads virtual reality three dimensional displays visual system	This paper introduces a machine vision system, which is suitable for cooperative works between the human and computer. This system provides images inputted from a stereo camera head not only to the processor but also to the user’s sight as binocular wide-angle foveated (WAF) information, thus it is applicable for Virtual Reality (VR) systems such as tele-existence or training experts. The stereo camera head plays a role to get required input images foveated by special wide-angle optics under camera view direction control and 3D head mount display (HMD) displays fused 3D images to the user. Moreover, an analog video signal processing device much inspired from a structure of the human visual system realizes a unique way to provide WAF information to plural processors and the user. Therefore, this developed vision system is also much expected to be applicable for the human brain and vision research, because the design concept is to mimic the human visual system. Further, an algorithm to generate features using Discrete Fourier Transform (DFT) for binocular fixation in order to provide well-fused 3D images to 3D HMD is proposed. This paper examines influences of applying this algorithm to space variant images such as WAF images, based on experimental results.	algorithm;binocular vision;central processing unit;computer stereo vision;discrete fourier transform;feature selection;head-mounted display;machine vision;sensor;shroud of the avatar:;signal processing;stereo camera;television;virtual reality;waf	Sota Shimizu;Shinsuke Shimojo;Hao Jiang;Joel W. Burdick	2005	Proceedings of the 2005 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2005.1570214	computer vision;simulation;visual system;machine vision;computer science;stereopsis;discrete fourier transform;virtual reality;computer graphics (images)	Robotics	-40.47842649971651	-39.217952519966005	107521
b69d0109264d36c5f1c560c781ee897c4e2ff177	diversity-enhanced recommendation interface and evaluation		The beyond accuracy user experience of using recommender system is drawing more and more attention. For example, the system interface has been shown to associate positively with overall levels of user satisfaction. However, little is known about how the interfaces can constitute the user experience and the social interactions. In this paper, I plan to propose a visual diversity-enhanced interface that supports the user to inspect and control the multi-relevance recommendations. The goal is to let the users explore the different relevance prospects of recommended items in parallel and to stress their diversity. Two preliminary user studies with real-life tasks were conducted to compare the visual interface to a standard ranked list interface. The users» subjective evaluations show significant improvement in many metrics. I further show that the users explored a diverse set of recommended items while experiencing an increase in overall user satisfaction. A user-centered evaluation was used to reveal the mediating effects between the subjective and objective conceptual components. The future plans are discussed to extend the current findings.	agi-plan;computer user satisfaction;interaction;real life;recommender system;relevance;usability testing;user experience;user-centered design	Chun-Hua Tsai	2018		10.1145/3176349.3176357	computer science;recommender system;user control;information retrieval;data mining;user experience design;ranking	HCI	-36.87130326599125	-51.716546601731174	107560
4c8da177ce486b1507a8b30541d812dfb5f1843b	supporting map-based wayfinding with tactile cues	pedestrian navigation;tactile display;wayfinding;field study	Paper maps are a proven means for navigating in unfamiliar environments, however, they do not prevent people from getting lost or taking unwanted detours. A well-known issue is interpreting the map's geocentric content, which is known to become prone to errors when the map is not aligned to the environment. In this paper we report our investigation of providing a cue about the destination's location from an egocentric perspective in order to improve the interpretation of the map. We used a vibrotactile belt to continuously indicate a destination's direction relative to the user's orientation. In an outdoor field study we compared the performance of map-based navigation with and without the added tactile cue. We found evidence that people take shorter routes, consult the map less often, and were less often disoriented with the tactile cue. Furthermore, females found the tactile cue more useful and used it more often.	belt machine;field research;map	Martin Pielot;Niels Henze;Susanne Boll	2009		10.1145/1613858.1613888	computer vision;simulation;field research	HCI	-44.50952770548759	-45.67890066305169	107793
55055503d0d411d604f64f91bd7b375b9b0728b0	simple eeg driven mouse cursor movement	oscillations;brain computer interface;operant conditioning	  This paper dwells on suggestion of direction control of computer mouse by electroencephalographic (EEG) device incorporated,  which makes it possible for the user to affect the direction of the cursor’s movement on the screen by the frequency of brain’s  oscillation. The motivation for solution of this problem is the effort to help to handicapped people to communicate with surrounding  world. Described approach uses technique called operant conditioning [1] and in the simpliest version is based on comparison  of signal magnitude in two narrow neighbouring bands. Computed difference influences direction and speed of cursor’s motion.  Promising results of one axis control were achieved and described approach will be considered in more complex brain-computer  interface.    	electroencephalography	Jan Knezík;Martin Drahanský	2008		10.1007/978-3-540-75175-5_66	brain–computer interface;operant conditioning;oscillation	HCI	-46.72139147751371	-48.56016769295911	107804
683384a72b936d1b252731195958a912b55ab72f	integrating multiple views with virtual mirrors to facilitate scene understanding	virtual mirror;scene identification;multiple views;attentional load;scene understanding;visual task;image integration;psychophysics	In this article, an image integration technique called Virtual Mirroring (VM) is evaluated. VM is a technique that combines multiple 2D views of a 3D scene into a single composite image by overlaying views onto virtual mirrors. Given multiple views of a scene, one view is augmented with the remaining views by placing virtual mirrors on the first view and overlaying onto them the corresponding remaining views. Unlike a standard array presentation, where 2D views are not integrated and simply placed adjacent to one another, the VM presentation preserves the relative location, orientation, and scale between views. As such, it is our contention that humans will fare better at performing certain visual tasks, such as scene identification, when viewing a 3D scene via a VM presentation than when viewing an array presentation. We performed an experiment on 12 participants, where participants were required to identify 96 scenes both with a VM and an array presentation and we compared their % correctness and response times. Moreover, we studied the effects of adding an auditory attentional load on performance. We found that regardless of load, participants were able to identify scenes using VM presentation with greater accuracy and at greater speeds.	auditory processing disorder;correctness (computer science);disk mirroring;standard array	Carmen E. Au;James J. Clark	2008	TAP	10.1145/2043603.2043610	cognitive psychology;computer vision;simulation;computer science;multimedia;psychophysics;computer graphics (images)	Visualization	-43.87097265955474	-47.23614834158005	107821
05926c5f78b1e6483d179b0fbb1f61add030ebf3	fingerreader: a wearable device to support text reading on the go	wearable camera;text reading;assistive technology;finger worn interface	Visually impaired people report numerous difficulties with accessing printed text using existing technology, including problems with alignment, focus, accuracy, mobility and efficiency. We present a finger worn device that assists the visually impaired with effectively and efficiently reading paper-printed text. We introduce a novel, local-sequential manner for scanning text which enables reading single lines, blocks of text or skimming the text for important sections while providing real-time auditory and tactile feedback. The design is motivated by preliminary studies with visually impaired people, and it is small-scale and mobile, which enables a more manageable operation with little setup.	printing;real-time clock;usb on-the-go;wearable technology	Roy Shilkrot;Jochen Huber;Connie Liu;Pattie Maes;Suranga Nanayakkara	2014		10.1145/2559206.2581220	computer vision;speech recognition;computer science;multimedia	HCI	-46.845031119798364	-43.42658538072079	107874
d228523149a44f231b034ceb43b3a5a00a0d3010	mr visualization of wheel trajectories of driving vehicle by seeing-through dashboard	comfort of passengers;transportation data visualisation mobile robots road vehicles traffic information systems;mixed reality physiological indices;mixed reality physiological indices autonomous vehicles comfort of passengers;sweat information mr visualization wheel trajectories driving vehicle seeing through dashboard autonomous vehicles general transportation industrial fields political fields academic fields advanced auto driving control mixed reality display system road surface physiological indices heart rate variability;stress roads vehicles wheels trajectory mobile robots virtual reality;autonomous vehicles	A lot of efforts aim to realize a society where autonomous vehicles become general transportation in industrial, political and academic fields. In order to make autonomous vehicles more familiar with the public, it is necessary to develop not only advanced auto driving control but also comfortable environments for the passengers. This paper proposes our trial to improve comfort of passengers on autonomous vehicles. We developed an experimental vehicle equipping Mixed Reality (MR) display system which aims to reduce anxiety using visual factors. Our proposed system visualizes a road surface that is out of the passenger's field of view by projecting the see-through image on the dashboard. Moreover, it overlays the computer graphics of the wheel trajectories on the displayed image using MR so that the passengers can easily confirm the auto driving control is working correctly. The displayed images enable passengers to comprehend the road condition and the expected vehicle route out of the passenger's field of view. We investigated change of the mental stress by introducing methods for measuring physiological indices, heart rate variability and sweat information.	autonomous car;autonomous robot;computer graphics;dashboard;heart rate variability;mixed reality	Shota Sasai;Itaru Kitahara;Yoshinari Kameda;Yuichi Ohta;Masayuki Kanbara;Luis Yoichi Morales Saiki;Norimichi Ukita;Norihiro Hagita;Tetsushi Ikeda;Kazuhiko Shinozawa	2015	2015 IEEE International Symposium on Mixed and Augmented Reality Workshops	10.1109/ISMARW.2015.17	simulation	Robotics	-39.74537994247668	-41.2957715333058	108325
cefe3e0c1b4a5dbc56d5e5ea363f2e724e7e2eeb	effects of virtual arm representations on interaction in virtual environments		"""Many techniques for visualization and interaction that potentially increase user performance have been studied in the growing field of virtual reality. However, the effects of virtual-arm representations on users' performance and perception in selection tasks have not been studied before. This paper presents the results of a user study of three different representations of the virtual arm: """"hand only,"""" """"hand+forearm,"""" and """"whole arm"""" which includes the upper arm. In addition to the representations' effects on performance and perception in selection tasks, we investigate how the users' performance changes depending on whether collisions with objects are allowed or not. The relationship between the virtual-arm representations and the senses of agency and ownership are also explored. Overall, we found that the """"whole arm"""" condition performed worst."""	arm architecture;usability testing;virtual reality	Tanh Quang Tran;HyunJu Shin;Wolfgang Stuerzlinger;JungHyun Han	2017		10.1145/3139131.3139149	simulation;visualization;computer science;perception;forearm;human–computer interaction;virtual reality;3d interaction	Visualization	-45.613729688489705	-48.09766001528968	108506
be57b9dc16f69ed0f84faa7090a74708f4aee93e	on planning and task achievement modalities for human-robot collaboration	control architectures;human robot interaction;robot supervi sion	In this paper we present a robot supervision system designed to be able to execute collaborative tasks with humans in a flexible and robust way. Our system is designed to take into account the different preferences of the human partners, providing three operation modalities to interact with them. The robot is able to assume a leader role, planning and monitoring the execution of the task for itself and the human, to act as assistent of the human partner, following his orders, and also to adapt its plans to the human actions. We present several experiments that show that the robot can execute collaborative tasks with humans.	experiment;human–robot interaction;modality (human–computer interaction);partially observable markov decision process;relevance;robot;usability testing;user (computing)	Michelangelo Fiore;Aurélie Clodic;Rachid Alami	2014		10.1007/978-3-319-23778-7_20	human–robot interaction;simulation;computer science;engineering;artificial intelligence;social robot	Robotics	-34.67881727449874	-39.90982127076073	108535
75e0ed2ba27b7bf19103563a611511b28bf1f32d	transparent surface acoustic wave tactile display	virtual reality transparent surface acoustic wave tactile display human interface;display devices;virtual reality;virtual reality haptic interfaces surface acoustic wave devices display devices;human interface;surface acoustic wave;tactile display;surface acoustic waves acoustic waves displays pulse modulation friction humans acoustic pulses voltage fingers skin;surface acoustic wave devices;distributed generators;haptic interfaces;surface acoustic wave tactile display human interface virtual reality ultrasonic	We have already proposed a novel method to provide human tactile sensation using surface acoustic wave (SAW). A pulse modulated driving voltage excites temporal distribution of standing SAW. The distribution generates friction shift on the surface of a SAW substrate. When the surface with the burst SAW is explored, the friction shift can be perceived as tactile sensation at mechanoreceptors in the finger skin. Controlling the burst frequency according to measured rubbing motion, reality of the displayed sensation can be enhanced. In this paper, we proposed a transparent tactile display. An experimental apparatus was fabricated on trial and controlled by using captured images. The apparatus demonstrated potentiality of the tactile display for various applications.	acoustic cryptanalysis;excited state;pulse-width modulation	Masaya Takasaki;Hiroyuki Kotani;Takeshi Mizuno;Takaaki Nara	2005	2005 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2005.1545129	computer vision;acoustics;surface acoustic wave sensor;computer science;engineering;surface acoustic wave;virtual reality	Robotics	-41.26657363726547	-41.926845491557714	108573
bddf4a7be6f0175a0141f5f00c55f51e5253be05	evaluating optical see-through head-mounted display calibration via frustum visualization	frustum visualization;on screen visuals optical see through head mounted display calibration frustum visualization ost hmd calibration user feedback user interaction methods touch pads online evaluation calibration correction visual feedback calibration quality corrective actions single point active alignment method spaam calibration eye location estimate system error sources rendering;standards;visualization;optical imaging;calibration visualization adaptive optics optical feedback augmented reality optical imaging standards;ost hmd;augmented reality;frustum visualization calibration ost hmd;rendering computer graphics augmented reality calibration data visualisation helmet mounted displays human computer interaction;calibration;optical feedback;adaptive optics	Summary form only given. Effectively evaluating optical see-through (OST) head-mounted display (HMD) calibration is problematic and largely relies on feedback from the user. Studies evaluating OST HMD calibration, such as those by McGarrity, Tang, and Navab et al. [2, 3, 1], utilize user interaction methods, such as touch pads, to facilitate on-line evaluation and correction of calibration results. In all of these studies, however, only the users themselves receive any visual feedback related to the calibration quality or the corrective actions taken to improve it. In this video, we present the use of standard frustum visualization to provide calibration quality information to the researcher in real time. We use a standard Single Point Active Alignment Method (SPAAM) calibration, [4], after which both the eye location estimate and resulting intrinsic values are displayed superimposed onto the user. Presenting the eye position relative to the user's head benefits studies on system error sources, and rendering on-screen visuals also allows outside observers to identify calibration issues and offer corrective suggestions. We believe that techniques, such as frustum visualization, will expand the amount of information available for evaluating calibration results, and will greatly aid those investigating new and improved calibration procedures.	frustum;head-mounted display;online and offline	Kenneth R. Moser;J. Edward Swan	2015	2015 IEEE Virtual Reality (VR)	10.1109/VR.2015.7223450	computer vision;augmented reality;video feedback;calibration;simulation;visualization;computer science;optical imaging;adaptive optics;computer graphics (images)	Visualization	-41.22864040649951	-39.549711207574944	108711
4a9741580f43d13a1f31e4c617e740353a469fba	search engine accessibility for low-literate users	low literate users;search engine accessibility;guidelines	Search engines are often used to retrieve content on the Web, but it is not a simple activity for low-literate users since they have to know the technology and create strategies to query and navigate. Their interaction with search engines differ from high-literate users on strategies used, perception, communication and performance. In order to improve search engines and create solutions, we need to understand these users' needs. This research aimed to identify how search engine features influence the interaction of low-literate users. We analyzed the interaction of ten users through user tests that were part of a case study. Based on a limited set of features of a specific search engine, we identified what features were used, the perception about them and some barriers faced by these users. This study led to a list of recommendations for the development of search interfaces focused on low-literate users.	accessibility;web search engine	Débora Maurmo Modesto;Simone Bacellar Leal Ferreira;Aline da Silva Alves	2013		10.1007/978-3-642-39265-8_36	metasearch engine;spamdexing;multimedia;search analytics;world wide web;information retrieval;search engine	HCI	-35.05574709452453	-52.07733438632153	108770
a2812f2790aef4177e239d539c96b3ec80ce12bb	tactile visualization with mobile ar on a handheld device	visually impaired users;haptic feedback;handheld device;visual system	This paper presents a tactile visualization system incorporating touch feedback to a mobile AR system realized on a handheld device. This system enables, for the first time, interactive haptic feedback though mobile and wearable interfaces. To demonstrate the proposed concept, an interactive scenario that helps a visually impaired user to recognize specific pictograms has been constructed. This system allows users to tactually recognize flat pictograms situated in the real world. Furthermore, it also opens the door to a wide range of applications which could be based on wearable tactile interaction.	mobile device	Beom-Chan Lee;Hyeshin Park;Junhun Lee;Jeha Ryu	2007		10.1007/978-3-540-76702-2_3	computer vision;human–computer interaction;engineering;multimedia	Visualization	-45.99625335446365	-42.02218868469165	108878
76bafa11bd6602801533fabdb31b1b01198acb0d	[poster] interacting with your own hands in a fully immersive mr system	natural interaction;intuitive interaction experience user interaction fully immersive mr system fully immersive mixed reality system virtual scenario hmd helmet mounted device kinect like camera virtual objects photorealistic capture synthetic 3d modelled avatar;user interfaces augmented reality avatars;training three dimensional displays virtual environments cameras avatars industries;natural interaction mixed reality hand gestures;hand gestures;mixed reality	This poster introduces a fully immersive Mixed Reality system we have recently developed, where the user is free to walk inside a virtual scenario while wearing a HMD. The novelty of the system lies in the fact that users can see and use their real hands - by means of a Kinect-like camera mounted on the HMD - in order to naturally interact with the virtual objects. Our working hypothesis are that the introduction of the photorealistic capture of users' hands in a coherently rendered virtual scenario induces in them a strong feeling of presence and embodiment without the need of using a synthetic 3D modelled avatar as a representation of the self. We also argue that the users' ability of grasping and manipulating virtual objects using their own hands not only provides an intuitive interaction experience, but also improves self-perception as well as the perception of the environment.	avatar (computing);head-mounted display;kinect;mixed reality;synthetic intelligence	Franco Tecchia;Giovanni Avveduto;Marcello Carrozzino;Raffaello Brondi;Massimo Bergamasco;Leila Alem	2014	2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)	10.1109/ISMAR.2014.6948466	immersive technology;computer vision;computer-mediated reality;computer science;mixed reality;multimedia;immersion;computer graphics (images)	Visualization	-43.37912700079392	-38.02788798703092	108984
fb62c7c961a6c42f3d1b0962e1e77512e2b1b66e	scenarios of use for a modular robotic mobility enhancement system for profoundly disabled children in an educational and institutional care environment	aide handicape;handicapped aid;robot movil;ayuda minusvalido;interfase usuario;movilidad;behavioral analysis;user interface;mobility;elderly;systeme modulaire;personne âgee;hombre;sistema modular;robotics;mobilite;anciano;user assistance;enfant;deficiencia fisica;assistance utilisateur;robot mobile;nino;institutional environment;analyse comportementale;modular system;asistencia usuario;human;child;robotica;interface utilisateur;analisis conductual;handicap physique;physical handicap;robotique;use case;moving robot;homme	In the framework of the EU funded MOVEMENT Project a novel modular robotic system is being developed which aims at supporting the mobility of elderly citizens and persons with disabilities. This paper outlines some of the use cases which were developed for the envisaged system with focus on assisting severely disabled children and their carers in an institutional environment. Six scenarios were developed and commented by nine professional carers. The paper presents and discusses the qualitative and quantitative data gained from user panel discussion. It was found that the concept and the use cases of MOVEMENT system were very well rated regarding utility which confirms the work of the consortium up to now. The future development activities towards the roll out and evaluation of the first MOVEMENT prototype platform are described		Peter Mayer;Paul Panek;Georg Edelmayer;Marnix Nuttin;Wolfgang L. Zagler	2006		10.1007/11788713_149	use case;simulation;el niño;computer science;operating system;robotics;user interface	Robotics	-39.75113010676457	-48.399980690301504	109032
1d78579bcb0ead83ec56e6d1d5b3d6119ca7add4	innovative technologies and learning		ARPiano uses a MIDI keyboard and a multifunction knob to create a novel mixed reality experience that supports visual music learning, music visu‐ alizations and music understanding. At its core, ARPiano provides a framework for extending a physical piano using augmented reality. ARPiano is able to precisely locate a physical keyboard in order to overlay various objects around the keyboard and on individual keys. These augmented objects are then used for music learning, visualization and understanding. Furthermore, ARPiano demon‐ strates a novel way to utilize the keys in a piano as an interface to interact with various augmented objects.	augmented reality;control knob;midi;mixed reality;multi-function printer	Ting-Ting Wu;Yueh-Min Huang;Rustam Shadieva;Lin Lin;Andreja Istenic Starcic	2018		10.1007/978-3-319-99737-7		HCI	-45.691581977145894	-39.37500663779347	109116
ab040e459e9220eccb6bf4414d4eb5256728acd2	beyond the cybernetic jam fantasy: the continuator	detectors;control systems;cybernetics;instruments;karma musical workstation;cybernetics learning systems detectors pattern analysis music real time systems workstations instruments control systems algorithm design and analysis;musical effects library cybernetic jam continuator musical instrument interactive system automatic learning system music generation system karma musical workstation risset interactive piano;musical instruments;cybernetic jam;learning systems;learning system;musical instrument;musical effects library;music generation system;interactive system;continuator;markov processes music interactive systems musical instruments;workstations;pattern analysis;risset interactive piano;automatic learning system;markov processes;interactive systems;music;algorithm design and analysis;real time systems	The Continuator is a usable musical instrument combining techniques from interactive and automatic learning systems. It learns and interactively plays with a user in the user's style. Music-generation systems have traditionally belonged to one of two categories: interactive systems in which players trigger musical phrases, events, or effects, such as the Karma musical workstation, and systems such as Risset's interactive piano, which allow for user input such as keystrokes or chords, but can't learn and use preprogrammed musical styles. Most of these systems propose musical effects libraries (a term used in the Karma workstation meaning a generation of music material based on user input). Although some of these effects are musically impressive, these systems can't be considered cybernetic musicians or even musical companions, because they use preprogrammed reactions and have no memory or facility for evolving.	cns disorder;categories;cybernetics;event (computing);interaction;interactivity;jam;libraries;phrases;side effect (computer science);software design;workstation	François Pachet	2004	IEEE Computer Graphics and Applications	10.1109/MCG.2004.1255806	new interfaces for musical expression;computer vision;simulation;cybernetics;computer science;artificial intelligence;operating system;music;multimedia;statistics;computer graphics (images)	Visualization	-36.072956322435374	-43.76451502144533	109117
3740f88331ad7317e06c35db33da17bb441acb4e	adaptive tradeoff explanations in conversational recommenders	explanation;construction process;user interface;user centered design;conversational recommenders;preference elicitation;graphic user interface;information interfaces and presentation	The completeness and certainty of a user's preferences may vary during her preference construction process in a conversational recommender. In order to more effectively support users to uncover their hidden criteria and/or solve preference conflicts, we propose to generate adaptive tradeoff explanations in organization-based recommender interfaces, to be conditional on the user's contextual needs. An experiment shows the adaptive element's higher potential to improve recommendation efficiency, relative to methods without this feature.	recommender system	Li Chen	2009		10.1145/1639714.1639754	user-centered design;computer science;knowledge management;data mining;graphical user interface;user interface;world wide web	HCI	-38.64777155733994	-50.79743826441913	109138
3ee336155637c5456338f6a8251b512427d5018b	presence, rather than prior exposure, is the more strongly indicated factor in the accurate perception of egocentric distances in real world co-located immersive virtual environments	fov;driving simulator;optical flow	"""In recent work [4,2], we have discovered that people are able to make surprisingly accurate judgments about egocentric distances in an immersive virtual environment (IVE) in the special case that the IVE represents a high fidelity model of the same physical space that the user is actually occupying, and the user has been able to unambiguously verify this by viewing the real space prior to donning the display upon which the corresponding virtual environment is presented. Through followup experiments in multiple locales, we have verified that the key factor enabling this distance perception accuracy is fact of co-location, rather than any particular characteristics of the physical environment [1].One possible interpretation of these intriguing results is that observers are better enabled to make accurate judgments of egocentric distance in an IVE when they are when they are cognitively 'immersed' or 'present' in the IVE -- i.e. when they readily accept the virtual environment as being 'equivalent' to the real world and are therefore prepared to act in the virtual world in the same way that they act in the real world [3]. However, another interpretation is also possible: it could be that people are able to make accurate judgments of egocentric distances in a virtual environment when they know that it exactly corresponds to a recently viewed real environment because they are able to form a metrically accurate mental model of the spatial structure of the real environment from their brief exposure to it, so that when they are subsequently presented with the virtual environment they simply calibrate their mental model of distances in the IVE to be consistent with their remembered model of the corresponding real environment.In order to disambiguate between the 'presence' hypothesis and the 'spatial memory' hypothesis, we conducted the following study. Using a between-subjects design, we asked observers to make judgments of egocentric distance via blind walking in a real room and in one of three different virtual models, each of which was described to the participants as representing a """"high fidelity virtual model of that same room"""". However, only one of the virtual models was actually an identical match in size to the real room. Nine of our 23 participants viewed a virtual model in which each of the walls had been surreptitiously moved 3 ft inward towards the center of the room, and another nine viewed a virtual model in which each of the walls had been surreptitiously moved 3.75 ft outwards from the center of the room. In each case, the textures were touched up in Photoshop to effect the changes without scaling anything. The remaining 5 participants viewed the same sized virtual model, replicating our earlier study. Acoustic cues were muffled for all participants by a radio playing static, and no training or feedback was given at any time.If the 'presence' hypothesis holds, we would either expect that the performance of participants in each group would be about the same, or that distances would be slightly underestimated in each of the artificially manipulated rooms. However, if the 'spatial memory' hypothesis holds, then we would expect that distance judgments would tend to deviate in opposite directions in the smaller and larger rooms, relative to the in the same room: participants who experience the smaller virtual room should overestimate distances in the virtual environment, and participants who experience the larger room should underestimate distances in the virtual environment, relative to in the real room.Figure 1 shows the three virtual environments used in this study, and figure 2 shows the average relative error in the distance judgments made by each participant in each virtual environment (vertical axis), compared to their error in the real environment (horizontal axis). We can see that most participants who experienced the accurately sized virtual room made distance judgments that were nearly equivalent in the real and virtual environments, consistent with our earlier findings [2]. However, many of the participants who experienced the smaller room model, and nearly all of the participants who experienced the larger room model, judged distances to be shorter, on average, in the virtual world than in the real world. These trends are statistically significant, and seem to support the 'presence' hypothesis more strongly than the 'remembered size' hypothesis."""	3d modeling;acoustic cryptanalysis;adobe photoshop;apache axis;approximation error;direct inward dial;expect;experiment;image scaling;memory management;mental model;virtual reality;virtual world	Brian Ries;Victoria Interrante;Lee Anderson;Jason Lindquist	2006		10.1145/1179622.1179841	computer vision;simulation;field of view;computer science;optical flow;computer graphics (images)	Visualization	-44.06902591276189	-49.25539033178668	109181
668354b467c47134e6e570caa00d306882726909	facial position and expression-based human–computer interface for persons with tetraplegia	mouth;medical image processing biomedical optical imaging decision trees face recognition human computer interaction medical disorders;mice;training;human face facial position expression based human computer interface expression mouse system tetraplegia monocular infrared depth camera nose position mouth status computer user input randomized decision tree facial information detection cursor motion nonlinear function color changes;mouth nose training cameras informatics mice;informatics;cameras;computer access camera mouse hand free control assistive technology at perceptual user interface fitts law humancomputer interaction hci severe disabilities;nose	A human-computer interface (namely Facial position and expression Mouse system, FM) for the persons with tetraplegia based on a monocular infrared depth camera is presented in this paper. The nose position along with the mouth status (close/open) is detected by the proposed algorithm to control and navigate the cursor as computer user input. The algorithm is based on an improved Randomized Decision Tree, which is capable of detecting the facial information efficiently and accurately. A more comfortable user experience is achieved by mapping the nose motion to the cursor motion via a nonlinear function. The infrared depth camera enables the system to be independent of illumination and color changes both from the background and on human face, which is a critical advantage over RGB camera-based options. Extensive experimental results show that the proposed system outperforms existing assistive technologies in terms of quantitative and qualitative assessments.	assistive technology;cursor (databases);decision tree;evaluation procedure;fm broadcasting;human–computer interaction;interface device component;nonlinear system;quadriplegia;randomized algorithm;self-help devices;sensor;user (computing);user experience	Zhen-Peng Bian;Junhui Hou;Lap-Pui Chau;Nadia Magnenat-Thalmann	2016	IEEE Journal of Biomedical and Health Informatics	10.1109/JBHI.2015.2412125	computer vision;simulation;informatics	Robotics	-42.54269042954626	-46.12470820413386	109345
6418e99945eab2a7f14c8861b3c28b83b6cfd2c4	a single layer 3-d touch sensing system for mobile devices application		Touch sensing has been widely implemented as a main methodology to bridge human and machine interactions. The traditional touch sensing range is 2-D and therefore limits the user experience. To overcome these limitations, we propose a novel 3-D contactless touch sensing called Airtouch system, which improves user experience by remotely detecting single/multi-finger position. A single layer touch panel with triangle-shaped electrodes is proposed to achieve multitouch detection capability as well as manufacturing cost reduction. Moreover, an oscillator-based-capacitive touch sensing circuit is implemented as the sensing hardware with the bootstrapping technique to eliminate the interchannel coupling effects. To further improve the system accuracy, a grouping algorithm is proposed to group the useful channels’ data and filter out hardware noise impact. Finally, improved algorithms are proposed to eliminate the fringing capacitance effect and achieve accurate finger position estimation. EM simulation proved that the proposed algorithm reduced the maximum systematic error by 11 dB in the horizontal position detection. The proposed system consumes 2.3 mW and is fully compatible with existing mobile device environments. A prototype is built to demonstrate that the system can successfully detect finger movement in a vertical direction up to 6 cm and achieve a horizontal resolution up to 0.6 cm at 1 cm finger-height. As a new interface for human and machine interactions, this system offers great potential in finger movement detection and gesture recognition for small-sized electronics and advanced human interactive games for mobile device.	algorithm;apple watch;capacitive sensing;contactless smart card;electronic circuit;gesture recognition;human–computer interaction;interaction;mobile device;multi-touch;prototype;sensor;simulation;smart tv;tiny basic;touchscreen;user experience;wearable technology;xslt/muenchian grouping	Li Du;Chun-Chen Liu;Yan Zhang;Yilei Li;Yuan Du;Yen-Cheng Kuan;Mau-Chung Frank Chang	2018	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2017.2702630	electronics;computer science;network layer;gesture recognition;electronic engineering;horizontal position representation;bootstrapping;mobile device;touchscreen;communication channel	Mobile	-42.613356792555976	-41.711509236897086	109555
debd1fa0bbd22600181ada9d3d9cf1f788923541	an evaluation of menu properties and pointing techniques in a projection-based vr environment	pediatrics;virtual reality pointing systems user interfaces;3d menus;wrist;virtual reality;pointing;projection based vr environment;layout;stability;error analysis;visualization;casting;three dimensional displays;virtual reality haptic interfaces layout command and control systems error analysis graphics stability tracking casting wrist;pointing technique evaluation;error rate;virtual reality 3d menus pointing;haptic interfaces;pointing systems;command and control systems;menu property evaluation;user interfaces;graphics;tracking;virtual reality menu property evaluation pointing technique evaluation projection based vr environment;ray casting	We studied menu performance for a rear-projected VR system. Our experiment considered layout (pie vs. linear list), placement (fixed vs. contextual), and pointing method (ray vs. alternatives that we call PAM: pointer-attached-to-menu). We also discuss results with respect to breadth (number of menu items) and depth (top-level and child menus). Standard ray pointing was usually faster than PAM, especially in second-level (child) menus, but error rates were lower for PAM in some cases. Subjective ratings were higher for ray-casting. Pie menus performed better than list layouts. Contextual pop-up menus were faster than fixed location.	deployment environment;image scaling;multi-level cell;pie menu;pointer (computer programming);ray casting;speedup	Kaushik Das;Christoph W. Borst	2010	2010 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2010.5444721	simulation;human–computer interaction;computer science;multimedia	Visualization	-44.03876670590011	-47.43969141673319	109728
b1e765d1f7d74004950e4c0dbb33d3c95ea4b596	computer simulated tests for lever controls with circular displays	indexing terms;computer simulation	Using computer simulated tests, this study examined direction-of-motion stereotypes and response times for different configurations of lever controls and circular displays. Quantitative measures of the strength and reversibility of stereotypes were used to analyze the effects of pointer position, direction of turn instruction, and control plane on movement compatibility. The results showed that strong and significantly reversible stereotypes were obtained for horizontal and vertical levers, at the 12 and 9 o’clock pointer positions, respectively. Response times were generally longer when there were no clear movement stereotypes. In the analysis of contributions of component principles to overall stereotypes, the results were explained in terms of a number of common control operating principles. Based on the experimental findings, recommendations for check reading or resetting purposes are that the pointer should be placed at 12 and 9 o’clock positions for the horizontal and vertical levers, respectively. Both the levers and the display should be positioned in the frontal plane. Due to weak response preferences and low reversibility, vertical and horizontal levers were found not suitable for use with other control/display configurations tested here. This study provided useful design guidance for improving the design of control panels used in person-machine interfaces.	computer simulation;control plane;plugboard;pointer (computer programming)	W. H. Chan;Alan H. S. Chan	2007	Engineering Letters		computer simulation;simulation;index term;computer science;artificial intelligence;world wide web	HCI	-45.86620571888911	-51.61198087231054	109863
6e6c336d3763ffe983e11b0f40665225795e5bc0	vibrotactile information for intuitive speed regulation	non visual interaction;speed regulation;tactile feedback;tactile icons	The present aim was to investigate if controlled vibrotactile stimulation can be used to inform users on how to regulate their behavior. 36 stimuli were varied by frequency modulation (i.e., ascending, constant, and descending), duration (i.e., 500, 1750, and 3000 ms), waveform (i.e., sine and sawtooth), and body location (i.e., wrist and chest), and presented to 12 participants. The participants were to evaluate without any training the meaning of each presented stimuli using three response options: ‘accelerate your speed’, ‘keep your speed constant’, and ‘decelerate your speed’. Participants rated also how emotionally pleasant and arousing the different stimulations were. The results showed that the stimuli were predominantly perceived analogously with the vibration frequency modulation. The best stimuli represented ‘accelerate your speed’, ‘keep your speed constant’, and ‘decelerate your speed’ information in accuracies of 88, 100, and 79 %, respectively. Stimulations were experienced as more pleasant in the wrist compared to the chest location. Both ascending and descending stimulations were rated as more arousing than stimuli with constant frequency. Our results suggest that tactile stimulation could be used in real life mobile applications, for example, in sports to inform the users on how they should regulate their performance.	mobile app;modulation;real life;sawtooth (cellular automaton);waveform	Jani Lylykangas;Veikko Surakka;Jussi Rantala;Jukka Raisamo;Roope Raisamo;Esa Tuulari	2009		10.1145/1671011.1671024	psychology;simulation;communication;social psychology	HCI	-46.55658783786319	-50.43748001462571	110162
79d453f4336f3a43d86c8da594f21f73f03e06e5	seeing eye phone: a smart phone-based indoor localization and guidance system for the visually impaired	indoor guidance system;smart phone;visually impaired;pose estimation	In order to help the visually impaired as they navigate unfamiliar environment such as public buildings, this paper presents a novel smart phone, vision-based indoor localization, and guidance system, called Seeing Eye Phone. This system requires a smart phone from the user and a server. The smart phone captures and transmits images of the user facing forward to the server. The server processes the phone images to detect and describe 2D features by SURF and then matches them to the 2D features of the stored map images that include their corresponding 3D information of the building. After features are matched, Direct Linear Transform runs on a subset of correspondences to find a rough initial pose estimate and the Levenberg–Marquardt algorithm further refines the pose estimate to find a more optimal solution. With the estimated pose and the camera’s intrinsic parameters, the location and orientation of the user are calculated using 3D location correspondence data stored for features of each image. Positional information is then transmitted back to the smart phone and communicated to the user via text-to-speech. This indoor guiding system uses efficient algorithms such as SURF, homographs, multi-view geometry, and 3D to 2D reprojection to solve a very unique problem that will benefit the visually impaired. The experimental results demonstrate the feasibility of using a simple machine vision system design to accomplish a complex task and the potential of building a commercial product based on this design.	camera resectioning;download;guidance system;levenberg–marquardt algorithm;machine vision;map projection;mobile device;mobile phone;prototype;requirement;router (computing);sensor;server (computing);smartphone;speech synthesis;speeded up robust features;systems design;virtual reality headset;wireless router	Dong Zhang;Dah-Jye Lee;Brandon Taylor	2013	Machine Vision and Applications	10.1007/s00138-013-0575-0	computer vision;simulation;pose;computer science;multimedia	Mobile	-38.78145469207831	-44.4412369356358	110305
36d567a5eb07fb66cf25d5999e887324f0f0a303	a haptic feedback device based on an active mesh	haptic display;active structures;free hand haptic feedback devices;haptic displays;smart material;actuators;smart materials;haptic feedback;virtual environment;active mesh;linear actuators;haptic interaction	This paper introduces a preliminary study about the Smart-Mesh, a novel type of active structure capable of deforming actively its shape and thus being able to form objects. It is a new approach to find a solution to the difficulties that are encountered in the field of haptic interaction in virtual environments. The paper describes the basic working principles of the structure, which allow the generation of overhanging surfaces. A first prototype based on 16 nodes and 48 linkages with prismatic joints is presented. The SmartMesh can be embedded into a table, into walls, ceilings and floors. The resolution of the SmartMesh depends on the amount of nodes and the length of the linkages. The linkages can be replaced by linear actuators.	embedded system;haptic technology;linear logic;prototype;smart tv;virtual reality	Andrea Mazzone;Christian P. Spagno;Andreas M. Kunz	2003		10.1145/1008653.1008687	embedded system;simulation;computer science;artificial intelligence;smart material	Robotics	-42.310211818526405	-40.901442458119384	110497
8fc85d5e37cab4c9af71e2ed5e4f7c1c425734b3	optical sight metaphor for virtual environments	hand based camera control;virtual instrument;camera control;virtual environments;optical sight metaphor;virtual environment head cameras optical control information systems jitter optical devices user interfaces optical variables control control systems;object selection;variable zoom;virtual environment;augmented reality;augmented reality systems;mixed reality system;object selection optical sight metaphor virtual environments ray casting hand based camera control variable zoom virtual instrument mixed reality system augmented reality systems;ray casting	Optical sight is a new metaphor for selecting distant objects or precisely pointing at close objects in virtual environments. Optical sight combines ray-casting, hand based camera control, and variable zoom into one virtual instrument that can be easily implemented for a variety of virtual, mixed, and augmented reality systems. The optical sight can be modified into a wide family of tools for viewing and selecting objects. Optical sight scales well from desktop environments to fully immersive systems	augmented reality;desktop computer;ray casting;virtual instrumentation;virtual reality	Andrei Sherstyuk;Jarrell Pair;Anton Treskunov	2007	2007 IEEE Symposium on 3D User Interfaces	10.1109/3DUI.2007.340772	computer vision;augmented reality;computer-mediated reality;simulation;engineering;computer graphics (images)	Visualization	-42.706009597027915	-38.74346355299193	110508
c55d5706e7d7a1e07c81acf7bf9c07bfa13f1c02	human detection and discrimination of tactile repeatability, mechanical backlash, and temporal delay in a combined tactile-kinesthetic haptic display system	haptic i o;performance evaluation;phantoms;haptic device design;visualization;haptic interfaces delays fingers performance evaluation visualization phantoms;fingers;psychophysics evaluation of haptic devices haptic device design haptic i o;haptic interfaces;delays;psychophysics;evaluation of haptic devices;device specifications human detection human discrimination tactile repeatability mechanical backlash temporal delay tactile kinesthetic haptic display system human perception levels device centric perception thresholds degree of freedom contact location feedback device tactile kinesthetic haptic devices tactile cues localization fingerpad minimum perceptible difference tactile element device backlash low curvature models high curvature models system delay user action device reaction	Many of the devices used in haptics research are over-engineered for the task and are designed with capabilities that go far beyond human perception levels. Designing devices that more closely match the limits of human perception will make them smaller, less expensive, and more useful. However, many device-centric perception thresholds have yet to be evaluated. To this end, three experiments were conducted, using one degree-of-freedom contact location feedback device in combination with a kinesthetic display, to provide a more explicit set of specifications for similar tactile-kinesthetic haptic devices. The first of these experiments evaluated the ability of humans to repeatedly localize tactile cues across the fingerpad. Subjects could localize cues to within 1.3 mm and showed bias toward the center of the fingerpad. The second experiment evaluated the minimum perceptible difference of backlash at the tactile element. Subjects were able to discriminate device backlash in excess of 0.46 mm on low-curvature models and 0.93 mm on high-curvature models. The last experiment evaluated the minimum perceptible difference of system delay between user action and device reaction. Subjects were able to discriminate delays in excess of 61 ms. The results from these studies can serve as the maximum (i.e., most demanding) device specifications for most tactile-kinesthetic haptic systems.	blinded;cns disorder;color layout descriptor;display device;experiment;haptic device component;haptic technology;large;millimeter per second;motion;offset binary;phase i/ii trial;repeatability;requirement;sensor;small;specification;tactile reflex epilepsy;velocity (software development)	Andrew J. Doxon;David E. Johnson;Hong Z. Tan;William R. Provancher	2013	IEEE Transactions on Haptics	10.1109/TOH.2013.50	stereotaxy;computer vision;simulation;visualization;computer science;engineering;psychophysics	Robotics	-44.24120426840157	-49.36734315857633	110552
0cae508c7df5b20c8009dde793f956af032fa76d	vision in bad weather	vision system;snow;application software;poor weather conditions vision systems computer vision haze fog rain hail snow atmospheric optics visual information coding three dimensional structure;encoding computer vision atmospheric optics;layout;computer vision;hail;poor weather conditions;atmospheric optics;machine vision;modulation coding;weather condition;rain;visual information coding;haze;optical modulation;vision systems;three dimensional structure;atmospheric modeling;layout machine vision application software computer vision rain snow optical modulation atmosphere modulation coding atmospheric modeling;atmosphere;encoding;fog	Current vision systems are designed to perform in clear weather. Needless to say, in any outdoor application, there is no escape from “bad” weather. Ultimately, computer vision systems must include mechanisms that enable them to function (even if somewhat less reliably) in the presence of haze, fog, rain, hail and snow. We begin by studying the visual manifestations of different weather conditions. For this, we draw on what is already known about atmospheric optics. Next, we identify effects caused by bad weather that can be turned to our advantage. Since the atmosphere modulates the information carried from a scene point to the observer, it can be viewed as a mechanism of visual information coding. Based on this observation, we develop models and methods for recovering pertinent scene properties, such as threedimensional structure, from images taken under poor weather conditions. 1 Vision and the Atmosphere Virtually all work in vision is based on the premise that the observer is immersed in a transparent medium (air). It is assumed that light rays reflected by scene objects travel to the observer without attenuation or alteration. Under this assumption, the brightness of an image point depends solely on the brightness of a single point in the scene. Quite simply, existing vision sensors and algorithms have been created only to function on “clear” days. A dependable vision system however must reckon with the entire spectrum of weather conditions, including, haze, fog, rain, hail and snow. The study of the interaction of light with the atmosphere (and hence weather) is widely known as atmospheric optics. Atmospheric optics lies at the heart of the most magnificent visual experiences known to man, including, the colors of sunrise and sunset, the blueness of the clear sky, and the rainbow (see[Minnaert, 1954]). The literature on this topic has been written over the past two centuries. A summary of where the subject as a whole stands would be too ambitious a pursuit. Instead, our objective will be to sieve out of this vast body of work, models of atmospheric optics that are of direct relevance to computational vision. Our most ∗This work was supported by the DARPA/ONR MURI Program under Grant N00014-95-1-0601 and the David and Lucile Packard Foundation. The authors thank Jan Koenderink of Utrecht University for pointers to early work on atmospheric optics. prominent sources of background material will be the works of McCartney[McCartney, 1976 ] and Middleton[Middleton, 1952] whose books, though dated, serve as excellent reviews of prior work. The key characteristics of light, such as its intensity and color, are altered by its interactions with the atmosphere. These interactions can be broadly classified into three categories, namely,scattering, absorptionandemission. Of these, scattering due to suspended particles is the most pertinent to us. As can be expected, this phenomenon leads to complex visual effects. So, at first glance, atmospheric scattering may be viewed as no more than a hindrance to an observer. However, it turns out that bad weather can be put to good use. The farther light has to travel from its source (say, a surface) to its destination (say, a camera), the greater it will be effected by the weather. Hence, bad weather could serve as a powerful means for coding and conveying scene structure. This observation lies at the core of our investigation; we wish to understand not only what bad weather does to vision but also what it can dofor vision. Surprisingly little work has been done in computer vision on weather related issues. An exception is the work of Cozman and Krotkov[Cozman and Krotkov, 1997 ] which uses the scattering models in [McCartney, 1976 ] to compute depth cues. Their algorithm assumes that all scene points used for depth estimation have the same intensity on a clear day. Since scene points can have their own reflectances and illuminations, this assumption is hard to satisfy in practice. In this paper, we develop algorithms that recover complete depth maps of scenes without making assumptions about the properties of the scene points or the atmospheric conditions. How do such scene recovery methods compare with existing ones? Unlike binocular stereo, they do not suffer from the problems of correspondence and discontinuities. Nor do they require tracking of image features as in structure from motion. Furthermore, they are particularly useful for scenes with distant objects (even miles away) which pose problems for stereo and motion. The techniques we present here only require changes in weather conditions and accurate measurement of image irradiance. 2 Bad Weather: Particles in Space Weather conditions differ mainly in the types and sizes of the particles involved and their concentrations in space. A great deal of effort has gone into measuring particle sizes and conCONDITION PARTICLE TYPE RADIUS CONCENTRATION ( μm ) ( cm ) -3	algorithm;binocular vision;book;color;computer vision;dependability;depth map;depth perception;experience;image sensor;interaction;jan bergstra;machine vision;numerical weather prediction;radius;ray (optics);relevance;structure from motion;visual effects	Shree K. Nayar;Srinivasa G. Narasimhan	1999		10.1109/ICCV.1999.790306	layout;atmospheric model;snow;application software;machine vision;fog;haze;computer science;atmosphere;atmospheric optics;encoding	Vision	-41.51202629650885	-50.916728866371926	110598
622535301b4fd62e9b61f39b252c090335081c3e	the two-user responsive workbench: support for collaboration through individual views of a shared space	virtual reality;virtual environments;shared virtual environment;responsive workbench;shared space;virtual environment	We present the two-user Responsive Workbench: a projectionbased virtual reality system that allows two people to simultaneously view individual stereoscopic image pairs from their own viewpoints. The system tracks the head positions of both users and computes four images one for each eye of each person. To display the four images as two stereo pairs, we must ensure each image is correctly presented to the appropriate eye. We describe a hardware solution to this display problem as well as registration and calibration procedures. These procedures ensure that when two users point to the same location on a virtual object, their fingers will physically touch. Since the stereo pairs are independent, we have the option of displaying specialized views of the shared virtual environment to each user. We present several scenarios in which specialized views might be useful. CR	responsive web design;stereoscopy;virtual reality;workbench	Maneesh Agrawala;Andrew C. Beers;Ian McDowall;Bernd Fröhlich;Mark T. Bolas;Pat Hanrahan	1997		10.1145/258734.258875	human–computer interaction;computer science;virtual machine;instructional simulation;operating system;virtual reality;multimedia;world wide web	Graphics	-43.23700642136737	-38.795005464927826	110839
f7b8ab3ae87b90f0a08212384a1884eef1d05d2a	make a riddle and telestory: designing children's applications for the siftables platform	tangible user interface tui;user interface;tangible user interface;digital manipulatives;sensor network;wireless communication;embodied media user interface;siftables;make a riddle;graphic user interface;ubiquitous computing;augmented reality;educational application;use case;telestory	We present the design of Make a Riddle and TeleStory, educational applications developed on the Siftables platform for children aged 4-7 years. Siftables are hybrid tangible-graphical user interface devices with motion and neighbor sensing, graphical display, and wireless communication. Siftables provide a unique opportunity to give children responsive feedback about the movement and arrangement of a distributed set of objects. We contrast the use case that includes an external display to their use as a standalone application platform. We outline design strategies for communicating information about the affordances of the Siftables and methods of providing dynamic feedback to encourage manipulation and to increase engagement during application use for hybrid tangible-graphical user interfaces.	graphical user interface;infographic;siftable	Seth E. Hunter;Jeevan J. Kalanithi;David Merrill	2010		10.1145/1810543.1810572	user interface design;user;10-foot user interface;simulation;interface metaphor;shell;human–computer interaction;engineering;multimedia;natural user interface;user interface	HCI	-46.29054557306777	-39.40418976729025	110941
bf24e3db4384833287e813f07d5efd7e5d174686	3d haptic rendering of tissues for epidural needle insertion using an electro-pneumatic 7 degrees of freedom device	force;pneumatic actuators;three dimensional displays;solid modeling;haptic interfaces;needles;anesthesia	Epidural anaesthesia is a medical gesture commonly performed by an anaesthesiologist. However, it remains one of the most difficult gestures to master for medical students. Given the lack of sufficiently realistic training devices available for future physicians, we propose a new haptic simulator which reproduces the haptic sensations felt by anaesthesiologists when performing this kind of operation. The originality of this simulator is the coupling of a Geomagic Touch® device with a pneumatic cylinder to reproduce the “Loss of Resistance” phenomenon which helps the physician to control the needle depth. In this paper, we introduce the parametric 3D model of the region of interest and the control laws used jointly. Even though this device could not reproduce the right level of forces required in this type of anaesthesia, an anaesthesiologist involved in the project gave positive feedback about its haptic tissue rendering.	cylinder seal;experiment;geomagic;haptic technology;parametric model;positive feedback;region of interest;simulation;spectral leakage	Pierre-Jean Ales;Nicolas Herzig;Arnaud Lelevé;Richard Moreau;Christian Bauer	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759760	simulation;pneumatic actuator;engineering;biological engineering;solid modeling;force	Robotics	-41.64041857932002	-47.20093170690033	110951
de83477a1359a1bce6946e7140e085158ab8c8e3	moving motion metaphors	3d interaction;universal design;metaphors;disability;3d environment;accessibility;data visualization	A class of visual metaphors is introduced in which simple animation conveys meaning about motion metaphorically. Antecedents in the form of static metaphors for visually conveying motion are discussed, followed by two examples of moving motion metaphors. The second is an integral part of a new heads-up flying interface for navigating 3D environments Some concluding remarks are made concerning the practical uses of moving motion metaphors.	floor and ceiling functions;motion capture	Colin Ware	1996		10.1145/257089.257293	universal design;human–computer interaction;computer science;accessibility;multimedia;world wide web;data visualization	Graphics	-44.59107256936182	-38.819811466385154	110975
3cba9922b798f017726f5320c3ca9434c4ff361c	interactive visual calibration of volumetric head-tracked 3d displays		Head-tracked 3D displays can provide a compelling 3D effect, but even small inaccuracies in the calibration of the participant's viewpoint to the display can disrupt the 3D illusion. We propose a novel interactive procedure for a participant to easily and accurately calibrate a head-tracked display by visually aligning patterns across a multi-screen display. Head-tracker measurements are then calibrated to these known viewpoints. We conducted a user study to evaluate the effectiveness of different visual patterns and different display shapes. We found that the easiest to align shape was the spherical display and the best calibration pattern was the combination of circles and lines. We performed a quantitative camera-based calibration of a cubic display and found visual calibration outperformed manual tuning and generated viewpoint calibrations accurate to within a degree. Our work removes the usual, burdensome step of manual calibration when using head-tracked displays and paves the way for wider adoption of this inexpensive and effective 3D display technology.	align (company);calibration (statistics);cubic function;display device;stereo display;usability testing	Andrew John Wagemakers;Dylan Brodie Fafard;Ian Stavness	2017		10.1145/3025453.3025685	calibration;stereo display;computer graphics (images);artificial intelligence;illusion;computer science;visual perception;computer vision	HCI	-41.379840169932315	-39.381140634695086	111000
cae544dd4816858429c253ea54e29046a89a9b92	a multimedia, augmented reality interactive system for the application of a guided school tour	human computer interaction;health risk;computer vision;interactive system;augmented reality	The paper describes an implementation of a multimedia, augmented reality system used for a guided school tour. The aim of this work is to improve the level of interactions between a viewer and the system by means of augmented reality. In the implemented system, hand motions are captured via computer vision based approaches and analyzed to extract representative actions which are used to interact with the system. In this manner, tactile peripheral hardware such as keyboard and mouse can be eliminated. In addition, the proposed system also aims to reduce hardware related costs and avoid health risks associated with contaminations by contact in public areas.	augmented reality;computer keyboard;computer mouse;computer vision;game controller;interaction;interactivity;map;overhead (computing);peripheral;touchscreen	Ko-Chun Lin;Sheng-Wen Huang;Sheng-Kai Chu;Ming-Wei Su;Chia-Yen Chen;Chi-Fa Chen	2009		10.1007/978-3-642-11577-6_29	computer vision;augmented reality;computer-mediated reality;simulation;human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-45.00735566903231	-42.29173016211057	111053
1b350f1d9d78bc810cccd97faa6b6d91264502f2	tactile display for the visually impaired using teslatouch	touch screen;teslatouch;tactile display;assistive technology;visual impairment	TeslaTouch is a technology that provides tactile sensation to moving fingers on touch screens. Based on TeslaTouch, we have developed applications for the visually impaired to interpret and create 2D tactile information. In this paper, we demonstrate these applications, present observations from the interaction, and discuss TeslaTouch's potential in supporting communication among visually impaired individuals.	touchscreen	Cheng Xu;Ali Israr;Ivan Poupyrev;Olivier Bau;Chris Harrison	2011		10.1145/1979742.1979705	computer vision;multimedia	HCI	-46.08885028793328	-42.71955437006512	111054
b1e0f06e0cbbe316ce10169acd4b579203e4b009	direct hand manipulation of constrained virtual objects		We propose a direct object manipulation system that a user can manipulate objects constrained to each other by his or her hands. Because there is no haptic feedback to a user, manipulation of constrained objects is much harder than that of a simple rigid body. In addition, many physics simulator usually suffers from instability in handling the constrained objects in a virtual system. We solve the problem by adding another micro simulator which enforce the constraint between the objects, and guarantees that the rendered image of an object and its physics counterpart always satisfy the given constraint. The experiment shows that the proposed method makes a user manipulate constrained objects with no difficulties in the way that he or she manipulates such objects in the real world.	experiment;haptic technology;head-mounted display;instability;mathematical optimization;principle of good enough;simulation;virtual reality;visualization (graphics)	Jun-Sik Kim;Jung-Min Park	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8202180	computer vision;artificial intelligence;computer science;haptic technology;rigid body;hand manipulation	Robotics	-40.78401629530724	-38.474099962079165	111060
d15521f9a73341b2bf5e3be73f2dfd110d8487bf	gamification of hand rehabilitation process using virtual reality tools: using leap motion for hand rehabilitation	robot sensing systems;virtual reality;leap motion virtual reality gamification modeling unity 3d bionic controlled robot arm 3d printing;three dimensional displays;games;games three dimensional displays virtual reality medical treatment hardware robot sensing systems;medical treatment;hardware	Nowadays virtual reality (VR) technology give us the considerable opportunities to develop new methods to supplement traditional physiotherapy with sustain beneficial quantity and quality of rehabilitation. VR tools, like Leap motion have received great attention in the recent few years because of their immeasurable applications, whish include gaming, robotics, education, medicine etc. In this paper we present a game for hand rehabilitation using the Leap Motion controller. The main idea of gamification of hand rehabilitation is to help develop the muscle tonus and increase precision in gestures using the opportunities that VR offer by making the rehabilitation process more effective and motivating for patients.	gamification;motion controller;virtual reality	Madina Alimanova;Saulet Borambayeva;Dinara Kozhamzharova;Nurgul Kurmangaiyeva;Dinara Ospanova;Gulnar Tyulepberdinova;Gulnur Gaziz;Aray Kassenkhan	2017	2017 First IEEE International Conference on Robotic Computing (IRC)	10.1109/IRC.2017.76	simulation;engineering;multimedia;computer graphics (images)	Visualization	-40.25619108458761	-46.615044920314475	111152
54b32fb4a6b6689fe8906a0e1a510e5bc79ebb8f	one-handed input through rotational motion for smartwatches		ABSTRACTOne-handed input for smartwatches is crucial when users’ hands are occupied. A rotational motion is leveraged, which can be detected by built-in motion sensors in most smartwatches, as input to provide item selection. Users control a cursor by rotating the smartwatch into different orientations using the proposed item selection gestures. To understand human wrist dexterity and the ability to perform rotational motion input along each degree of freedom on smartwatches, a human-factor study was performed. Except dexterity, users’ consensus and agreement of user-defined rotational motion input gestures for selection and commitment on smartwatches in an exploratory user study were also observed. Based on both the studies, gestures roll and tilt are proposed as selection gestures, and shake and nod are used as commitment gestures. The item selection performance using the proposed gestures in 2D layouts in a user study was further evaluated. The study results showed that tilt-nod and tilt-shake are suit...	smartwatch	Hsin-Ruey Tsai;Po-Chang Chen;Liwei Chan;Yi-Ping Hung	2018	Int. J. Hum. Comput. Interaction	10.1080/10447318.2017.1331534	human–computer interaction;computer vision;cursor (user interface);smartwatch;rotation around a fixed axis;computer science;gesture;artificial intelligence	HCI	-45.7346701613941	-47.54277699599359	111471
4f0f6a42b6666e14699b36ae42e6bc996e76d6ab	molebot: a robotic creature based on physical transformability	user interface;kinetic interaction;organic user interface;oui;game;physical transformability;kinetics;ludic experience	What would it be like to have a mole live under your table and push around objects on the table surface? We developed MoleBot, a robotic creature based on an Organic User Interface (OUI) that lives in a coffee table and interacts with small items placed on the table surface. To ensure fluid motions of the molehill cast by the MoleBot, the table surface combines its horizontal rigidity with the vertical flexibility of movable pins. The users are enabled to kinetically interact with this creature via a joystick or gestural commands.	joystick;organic user interface;robot	Narae Lee;Juwhan Kim;Jungsoo Lee;Myeongsoo Shin;Woohun Lee	2012		10.1145/2331714.2331734	simulation;human–computer interaction;engineering;computer graphics (images)	Robotics	-44.10112349548197	-39.38933355651577	111477
bbd744cbc1447de8633b3028912d4c905de451ab	multi-evaluation method of visual fatigue and motion sickness while viewing 2d/3d video clips on a liquid crystal display	stabilometry;stereoscopic images;visually induced motion sickness vims;visual fatigue;blurred images	It is physiologically known that the vestibular system and the autonomic nervous system interact with each other. The motion sickness can affect both these systems, and severity of the motion sickness is expected to be measured by dysfunction of the equilibrium system. We have proposed a new index, sparse density (SPD), of stationary stabilograms for detecting the metamorphism in the (temporally averaged) potential function of stochastic differential equations, which occurs when a human attempts to maintain an upright posture. It is known that a mathematical model of the body sway can be developed by a stochastic process. The authors have succeeded in finding the nonlinearity in the potential function. Subjects in a standing position were stimulated by a movie scrolling from left to right on a liquid crystal display (LCD) in measurement 1 and a stereoscopic video clip on an LCD in measurement 2. As a result, the dynamics of the body sway in the presence of the stimulus as well as in its absence were considered to be stochastic. The metamorphism in the potential function during exposure to blurred images and a stereoscopic video clip could be detected by using the SPD.	liquid-crystal display	Hiroki Takada;Kazuhiro Fujikake;Yasuyuki Matsuura;Masaru Miyao	2013		10.1007/978-3-642-39191-0_36	computer vision;simulation;geography;computer graphics (images)	Vision	-42.85457905580817	-50.4770718936654	111489
156463ee6c16d69787f688ae4586ae1bc6fee75a	real-time multiple speaker tracking by multi-modal integration for mobile robots	social interaction;interaural intensity difference;mobile robot;real time;real time processing;interaural phase difference;ip networks	In this paper, real-time multiple speaker tracking is addressed, because it is essential in robot perception and humanrobot social interaction. The difficulty lies in treating a mixture of sounds, occlusion (some talkers are hidden) and real-time processing. Our approach consists of three components; (1) the extraction of the direction of each speaker by using interaural phase difference and interaural intensity difference, (2) the resolution of each speaker’s direction by multi-modal integration of audition, vision and motion with canceling inevitable motor noises in motion in case of an unseen or silent speaker, and (3) the distributed implementation to three PCs connected by TCP/IP network to attain real-time processing. As a result, we attain robust real-time speaker tracking with 200 ms delay in a non-anechoic room, even when multiple speakers exist and the tracking person is visually occluded.	hidden surface determination;internet protocol suite;mobile robot;modal logic;real-time clock;real-time transcription	Kazuhiro Nakadai;Ken-ichi Hidai;Hiroshi G. Okuno;Hiroaki Kitano	2001			mobile robot;social relation;real-time computing;speech recognition;computer science	Robotics	-36.862763565481856	-42.94349880288839	111582
be2821e3e33787bd9886f9058d99fe5171a0f10d	head-up display for motorcycle navigation	virtual reality;head up display;motorcycle simulator	In this paper, we describe a navigation system for motorcycle using the technology of head-up display. While there are various navigation system products for automobiles, motorcyclist has issues in using currently provided navigation system. It is known from previous researches that motorcycle rider moves their viewpoint in a characterful way while driving, which makes it difficult for the rider to look at small liquid crystal displays. To solve this issue, we propose a navigation system utilizing the technology of head-up display. While considering the motorcycle rider's viewpoint behavior, we constructed a head-up display to conduct experiments and observe the effects by presenting information on the head-up display. Regarding safety, experiments were conducted inside an immersive projection environment using a motorcycle simulator. The results of experiments clarified the preferable display position and the amount of information to present on the head-up display.	experiment;head-up display;liquid-crystal display;simulation	Kenichiro Ito;Hidekazu Nishimura;Tetsuro Ogi	2015		10.1145/2818406.2818415	head-up display;simulation;computer science;artificial intelligence;virtual reality;computer graphics (images)	HCI	-44.91797832372684	-47.52046683525841	111819
6d0031e661c16519ea6f198208a07ab1b73c6f13	saliency-driven real-time video-to-tactile translation	visual saliency;4d film;artificial;multimedia tactile effect authoring visual saliency 4d film;haptic i o;h 5 1 d evaluation methodology;multimedia;motion pictures;augmented;visualization;streaming media;image color analysis;visualization spatiotemporal phenomena rendering computer graphics haptic interfaces streaming media motion pictures image color analysis;and virtual realities;tactile effect;spatiotemporal phenomena;authoring;automated authoring approach saliency driven real time video to tactile translation tactile feedback visual stimuli immersive multimodal experiences video images spatiotemporal features visuotactile rendering;haptic interfaces;rendering computer graphics;video signal processing haptic interfaces rendering computer graphics	Tactile feedback coordinated with visual stimuli has proven its worth in mediating immersive multimodal experiences, yet its authoring has relied on content artists. This article presents a fully automated framework of generating tactile cues from streaming images to provide synchronized visuotactile stimuli in real time. The spatiotemporal features of video images are analyzed on the basis of visual saliency and then mapped into the tactile cues that are rendered on tactors installed on a chair. We also conducted two user experiments for performance evaluation. The first experiment investigated the effects of visuotactile rendering against visual-only rendering, demonstrating that the visuotactile rendering improved the movie watching experience to be more interesting, immersive, and understandable. The second experiment was performed to compare the effectiveness of authoring methods and found that the automated authoring approach, used with care, can produce plausible tactile effects similar in quality to manual authoring.	experience;experiment;multimodal interaction;performance evaluation;real-time transcription;mapped	Myongchan Kim;Sungkil Lee;Seungmoon Choi	2014	IEEE Transactions on Haptics	10.1109/TOH.2013.58	computer vision;visualization;computer science;multimedia;computer graphics (images)	Visualization	-39.520306772181776	-38.76480298282132	111899
c7cccf383d96fb1b9adfbd0366408245af7516ba	gaze behavior and visual attention model when turning in virtual environments	input device;top down;data collection;virtual reality;perception model;head direction;gaze behavior;gaze tracking;interactive application;optical flow;prediction model;virtual environment;first person navigation;visual attention	In this paper we analyze and try to predict the gaze behavior of users navigating in virtual environments. We focus on first-person navigation in virtual environments which involves forward and backward motions on a ground-surface with turns toward the left or right. We found that gaze behavior in virtual reality, with input devices like mice and keyboards, is similar to the one observed in real life. Participants anticipated turns as in real life conditions, i.e. when they can actually move their body and head. We also found influences of visual occlusions and optic flow similar to the ones reported in existing literature on real navigations. Then, we propose three simple gaze prediction models taking as input: (1) the motion of the user as given by the rotation velocity of the camera on the yaw axis (considered here as the virtual heading direction), and/or (2) the optic flow on screen. These models were tested with data collected in various virtual environments. Results show that these models can significantly improve the prediction of gaze position on screen, especially when turning, in the virtual environment. The model based on rotation velocity of the camera seems to be the best trade-off between simplicity and efficiency. We suggest that these models could be used in several interactive applications using gaze point as input. They could also be used as a new top-down component in any existing visual attention model.	course (navigation);input device;optic axis of a crystal;optical flow;real life;top-down and bottom-up design;velocity (software development);virtual reality;yaws	Sébastien Hillaire;Anatole Lécuyer;Gaspard Breton;Tony Regia-Corte	2009		10.1145/1643928.1643941	computer vision;simulation;computer science;virtual machine;operating system;top-down and bottom-up design;optical flow;virtual reality;predictive modelling;input device;data collection	Visualization	-45.09188851238477	-47.57220537700275	112107
2e6a7a71d1438e8f2d7ff81832ed3e085e79f016	from tactile to virtual: using a smartwatch to improve spatial map exploration for visually impaired users	map exploration;visually impaired users;wearable devices;geospatial data	Tactile raised-line maps are paper maps widely used by visually impaired people. We designed a mobile technique, based on hand tracking and a smartwatch, in order to leverage pervasive access to virtual maps. We use the smartwatch to render localized text-to-speech and vibratory feedback during hand exploration, but also to provide filtering functions activated by swipe gestures. We conducted a first study to compare the usability of a raised-line map with three virtual maps (plain, with filter, with filter and grid). The results show that virtual maps are usable, and that adding a filter, or a filter and a grid, significantly speeds up data exploration and selection. The results of a following case study showed that visually impaired users were able to achieve a complex task with the device, i.e. finding spatial correlations between two sets of data.	filter (signal processing);map;smartwatch;speech synthesis;usability	Sandra Bardot;Marcos Serrano;Christophe Jouffrais	2016		10.1145/2935334.2935342	computer vision;simulation;geospatial analysis;multimedia;wearable technology	HCI	-46.034918263955966	-42.576995222098205	112113
5e641b0d9ae82df7f565e245758a1c11892c33ce	privacy visor: method based on light absorbing and reflecting properties for preventing face image detection	light absorbing material;object detection data privacy face recognition;haar like feature;eyeglasses privacy visor method light absorbing property light reflecting property face image detection unauthorized face image revelation face to face communication haar like features;light reflecting material unauthorized face image revelation face detection haar like feature light absorbing material;face recognition;unauthorized face image revelation;light reflecting material;data privacy;face detection;face materials feature extraction face detection privacy reflection face recognition;object detection	"""A method is proposed for preventing unauthorized face image revelation through unintentional capture of facial images. Methods such as covering the face and painting particular patterns on the face effectively prevent detection of facial images but hinder face-to-face communication. The proposed method overcomes this problem through the use of a device worn on the face that corrupts the Haar-like features through the use of light absorbing and reflecting materials, which makes faces in captured images mostly undetectable. The device is similar in appearance to a pair of eyeglasses, so face-to-face communication is only slightly hindered, and it does not need a power supply. Testing of a prototype """"privacy visor"""" showed that captured facial images are sufficiently corrupted to prevent unauthorized face image revelation by face detection."""	authorization;digital camera;face detection;haar wavelet;image;internet;mobile phone;power supply;privacy;prototype;real-time computing;smartphone	Takayuki Yamada;Seiichi Gohshi;Isao Echizen	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.271	facial recognition system;computer vision;face detection;speech recognition;object-class detection;information privacy;computer science	Vision	-34.217666330676316	-44.784884547219555	112247
d4346aaff51db79a9f1aa7f00177f90ec9877ebe	user preference for sharpview-enhanced virtual text during non-fixated viewing		For optical see-through head-mounted displays, the mismatch between a display's focal length and the real world scene inadvertently prevents users from simultaneously focusing on the presented virtual content and the scene. It has been shown that it is possible to ameliorate the out-of-focus blur for images with a known focus distance, by applying an algorithm called Sharp View. However, it remains unclear if Sharp View also improves the readability and clarity of text rendered on the display. In this study, we investigate whether users reported increased text clarity when Sharp View was applied to a text label, and how the focal demand of the display, the focal distance to real world content, and gaze condition affect the result. Our results indicate that, in non-fixated viewing, there is a significant user preference for Sharp View-enhanced text strings.	algorithm;ar (unix);augmented reality;bus mastering;context switch;focal (programming language);gaussian blur;head-mounted display;interaction;text-based (computing);usability	Trey Cook;Nate Phillips;Kristen Massey;Alexander Plopski;Christian Sandor	2018	2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)	10.1109/VR.2018.8446058	computer vision;optical imaging;gaze;artificial intelligence;readability;focal length;computer science;visualization;augmented reality;clarity;lens (optics)	Visualization	-43.599694731206945	-47.24121579508421	112298
54540960beb0ccc2aceb58e8e827e715ba7dfff7	virtual basketball	virtual basketball	V IS U A L P R O C E E D IN G S 78 In this cave-like virtual space, participants use visual, auditory, and tactile sensations to perceive and interact with objects. A new human-scale haptic device, Big SPIDAR, couples haptic sensations with vision and audition, and displays various aspects of force feedback associated mainly with contact, weight, and inertia. In Virtual Basketball, players feel the weight and the spherical shape of the virtual ball at any position inside the playing space and experience the illusion of natural control over the ball. Virtual Basketball Makoto Sato •	haptic technology;virtual reality	Makoto Sato	1997		10.1145/259081.259175	computer graphics (images);computer vision;artificial intelligence;basketball;computer science	HCI	-45.05987173008039	-49.05292986027738	112300
1cbbe0da104d10f5b056cd84397089f06c97bad4	moveme: 3d haptic support for a musical instrument	learning;remote teaching;haptic;music;hand manipulation	"""Fine motor skills like finger/hand manipulations are essential for playing musical instruments and these skills require a great amount of time and effort to acquire. Researchers have been introducing haptic feedback systems in order to facilitate the process of learning motor skills but little research has expanded the possibility of applying to the field of musical instruments. Hence, we developed a system called """"MoveMe"""" that provides three-dimensional haptic support for playing a musical instrument. The system guides a user's hands as if someone else was holding their hands to help a beginner play a musical instrument. With the system, an expert can pre-record his/her movements so that a beginner can play it back later as necessary. Alternatively, the system connects an expert and a beginner via two haptic robots and the expert can, in real time, guide and correct the beginner's movement. In addition to those functionalities, we introduce a new proficiency metric provided by force feedback. A master can evaluate how much a beginner has improved using both audio feedback as well as this new force-based metric. Through the experiments that we conducted, we found that our system is effective in terms of playing a song at a correct speed and rhythm."""	audio feedback;experiment;haptic technology;robot	Katsuya Fujii;Sophia S. Russo;Pattie Maes;Jun Rekimoto	2015		10.1145/2832932.2832947	simulation;engineering;multimedia;communication	HCI	-47.468329592697	-50.293934688719176	112493
dc8c21f9f6ece2d8510772fcd2bdb009bbadddab	scale impacts elicited gestures for manipulating holograms: implications for ar gesture design		Because gesture design for augmented reality (AR) remains idiosyncratic, people cannot necessarily use gestures learned in one AR application in another. To design discoverable gestures, we need to understand what gestures people expect to use. We explore how the scale of AR affects the gestures people expect to use to interact with 3D holograms. Using an elicitation study, we asked participants to generate gestures in response to holographic task referents, where we varied the scale of holograms from desktop-scale to room-scale objects. We found that the scale of objects and scenes in the AR experience moderates the generated gestures. Most gestures were informed by physical interaction, and when people interacted from a distance, they sought a good perspective on the target object before and during the interaction. These results suggest that gesture designers need to account for scale, and should not simply reuse gestures across different hologram sizes.	augmented reality;desktop computer;discoverability;gesture recognition;holography;human–computer interaction	Tran Pham;Jo Vermeulen;Anthony Tang;Lindsay MacDonald Vermeulen	2018		10.1145/3196709.3196719	engineering;holography;multimedia;human–computer interaction;augmented reality;gesture	HCI	-45.61153976427738	-46.637458862413744	112577
a883050c77c4017d46d7d42624167a9dce3d421f	balancing multiplayer first-person shooter games using aiming assistance	mice;magnetic confinement;visualization;deathmatch games multiplayer first person shooter game balancing techniques 3d fps games 3d environment aim assistance techniques bullet magnetism area cursor;game balancing aim assistance fps games;three dimensional displays;games;games three dimensional displays aerospace electronics visualization mice mathematical model magnetic confinement;aerospace electronics;mathematical model;computer games	When player skill levels are different in competitive First Person Shooter (FPS) games, the weaker player can end up feeling discouraged and the stronger player may get bored with the lack of challenge - ultimately yielding a non-optimal play experience for both players. Previous work has investigated how aiming assistance can be applied in a 3D environment to assist weaker players; however, there is little information on whether aiming assistance balances gameplay in a multiplayer environment or on how aim assistance affects player experience. We carried out a study to test the effectiveness of two aim assistance techniques (Bullet Magnetism and Area Cursor) that have been shown to help aiming in a 3D FPS. Our study had novice-expert pairs play deathmatch games in a multiplayer 3D FPS with and without the assistance techniques. The study showed that although Area Cursor and Bullet Magnetism resulted in better performance of the weaker player, it did not result in closer scores, and had no effect on the players' enjoyment of the game. Our results indicate that balancing performance in 3D FPS games is more complex than simply helping weaker players' aim, and we suggest possible reasons that warrant further investigation. This study is the first realistic evaluation of balancing techniques for 3D First Person Shooters, providing empirical evidence of the difficulty of using aim assistance techniques for balancing competitive gameplay.	assistive technology;competitive learning;first person shooter;first-person (video games);floating point systems	Rodrigo Vicencio-Moreira;Regan Lee Mandryk;Carl Gutwin	2014	2014 IEEE Games Media Entertainment	10.1109/GEM.2014.7048086	simulation;engineering;game mechanics;multimedia;computer graphics (images)	HCI	-45.63765969378344	-48.27694392597797	112589
d2202b1e545017ac24cac9a911c26000b38cfe87	achieving human presence in space exploration		One of the primary goals of human spaceflight has been putting human cognition on other worlds. This is at the heart of the premise of what we call space exploration. But Earth-controlled telerobotic facilities can now bring human senses to other worlds and, in that respect, the historical premise of exploration, of boots on the ground, no longer clearly applies. We have ways of achieving remote presence that we never used to have. But the distances over which this must be achieved, by humans based on the Earth, is such that the speed of light seriously handicaps their awareness and cognition. The highest quality telepresence can be achieved not only by having people on site, but also by having people close, and it is that requirement that truly mandates human spaceflight. In terms of cost, safety, and survival, getting people close is easier than getting people all the way there. It is suggested here that to the extent that space exploration is best accomplished by achieving a sense of real human off-Earth presence, that presence can be best achieved by optimally combining human spaceflight to mitigate latency, with telerobotics, to keep those humans secure. This is culturally a new perspective on exploration.	cognition;requirement;telerobotics	Dan Lester	2013	PRESENCE: Teleoperators and Virtual Environments	10.1162/PRES_a_00160	simulation;artificial intelligence	HCI	-45.38107663711418	-49.10857994623413	112599
40866f7d03567b23aa73e1ebb311654d4e05f4d8	combined endo- and exoscopic semi-robotic manipulator system for image guided operations	operating room;virtual reality;digital camera;multimodal user interface;robot manipulator;user testing;modes of operation	This paper describes the development of a robotic assistance system for image guided operations. To minimize operation time, a multimodal user interface enables freehand robotic manipulation of an extracorporeal stereoscopic digital camera (exoscope) and an endoscope. The surgeon thereby wears a head-mounted unit with a binocular display, a head tracker, a microphone and earphones. Different view positioning and adjustment modes can be selected by voice and controlled by head rotation while pressing a miniature confirmation button with a finger. Initial studies focused on the evaluation and optimization of the intuitiveness, comfort and precision of different modes of operation, including a user test with neurosurgeons in a virtual reality simulation. The first labtype of the system was then implemented and demonstrated in the operating room on a phantom together with the clinical partners.	adobe freehand;binocular vision;block cipher mode of operation;digital camera;endoscope device component;hl7publishingsubsection <operations>;head-mounted display;headphones;imaging phantom;mathematical optimization;microphone device component;multimodal interaction;operating room;operation time;phantoms, imaging;robot;semiconductor industry;simulated reality;simulation;stereoscopy;user interface device component;virtual reality;confirmation - responselevel	Stefanos Serefoglou;Wolfgang Lauer;Axel Perneczky;Theodor Lutze;Klaus Radermacher	2006	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/11866565_63	embedded system;computer vision;simulation;computer science;artificial intelligence;virtual reality	Robotics	-42.28382381701571	-44.845167009937796	112680
0dfc660a779063f3c6be4f41cd9be886c07286bf	immersive representation of objects in virtual reality environment implementing impicit properties	aura dowsing interaction technique object representation virtual reality environment material object model sensory channel virtual world engineering design tool information perception object exploration standardized properties sets;virtual reality computer graphics image representation;engineering design;multisensorial presentation implicit properties material objects virtual reality;computer graphics;color;virtual reality;virtual environments;observers;materials;visualization;material objects;image representation;solid modeling;materials virtual environments solid modeling visualization observers color;virtual reality environment;implicit properties;virtual environment;multisensorial presentation;geometric structure;interaction technique;object model;virtual worlds	"""In this work we propose a new concept of so called implicit (or """"hidden"""") properties implemented in material object models. These are some features of the material object model that cannot be perceived by the observer through his/her senses (i.e. magnetization, radiation, humidity). Exploration of such features requires additional means and techniques that expand the perception range of the observer's different sensory channels. Traditional approaches use only a few of the explicit properties of objects models (mainly its geometric, structural and topological characteristics) for their building. We believe that the supplement of implicit properties, the selection of an appropriate combination of stimuli for separate sensory channels, their effective presentation to the observer and the ability for their modification directly in the virtual world will turn this situation in a way that may lead to a disruption in the generally accepted practices for different research fields, thus allowing for the virtual reality to unfold its full potential as a significant engineering design tool. Respectively, this will considerably enhance and intensify the information perception of the observer during object exploration in the virtual environment. Here, a brief introduction in the Virtual Reality and its engineering application is presented. The concepts about the material objects and their properties, as well as systems and norms for standardized properties sets and classes, have been surveyed. The developed Aura-Dowsing interaction technique as a part of ongoing complex research on the possibility for implementation of the implicit properties concept is presented and discussed."""	denial-of-service attack;design tool;engineering design process;interaction technique;virtual reality;virtual world	Angel Bachvarov;Stoyan Maleshkov;Dimo Chotrov;Jurica Katicic	2011	2011 Developments in E-systems Engineering	10.1109/DeSE.2011.80	computer vision;simulation;computer science;multimedia	Visualization	-44.86560983197035	-39.40153217817315	112766
3c148e32802d09c6cf01febedfc33dc6a55f01b3	roman: a mobile robotic assistant for indoor service applications	mobile robot;mobile robots;multimodal human robot interface roman mobile robotic assistant indoor service applications mobile service robot health care applications domestic automation semi autonomous operation maneuverable locomotion platform anthropomorphous manipulator reliable multisensor system;mobile service;mobile robots robotics and automation service robots medical services design automation anthropomorphism manipulators multisensor systems information processing automatic control;information processing;service robot;system architecture;human robot interface;health care	The paper describes design issues of a mobile service robot for health care applications and domestic automation. Key components required for achieving semi-autonomous operation are surveyed, including: 1. a highly maneuverable locomotion platform, 2. an anthropomorphous manipulator, 3. a reliable multisensor system, and a 4. multimodal human-robot-interface. In addition, related information processing and control methodologies are presented. Special emphasis is put on a system architecture for integration of the individual components into a full-size service robot. Performance and usefulness of the proposed approaches are demonstrated through experiments in various real-world service scenarios.	robot	Uwe D. Hanebeck;Christian Fischer;Günther Schmidt	1997		10.1109/IROS.1997.655061	control engineering;mobile robot;embedded system;simulation;information processing;computer science;engineering;artificial intelligence;mobile manipulator;robot control	Mobile	-36.94875968540422	-41.79294177740163	112891
600d69e3de6f2177b48634f9cf9b697435263fe6	digital beauty: the good, the bad, and the (not-so) ugly.	face feature detection digital beauty digital retouching image acquisition face enhancement;digital systems smart phones photography digital cameras software development mobile communication;image enhancement face recognition feature extraction image capture	Today, digital retouching of your pictures is made possible in the latest smartphones and cameras at the touch of a button. What is more, all of this can be achieved transparently to the user, in real time, just as the image is acquired or added afterwards, allowing users to manipulate and enhance individual faces according to their personal preferences.	image;real-time computing;smartphone	Peter Corcoran;Cosmin Stan;Corneliu Florea;Mihai Ciuc;Petronel Bigioi	2014	IEEE Consumer Electronics Magazine	10.1109/MCE.2014.2338573	computer vision;digital photography;image analysis;computer science;digital image processing;multimedia;digital image;computer graphics (images)	Vision	-40.51750534986174	-40.19795483286579	112913
4cdf328790a486a464b804496218d495ed1757ab	a two-layer model for behavior and dialogue planning in conversational service robots	service robots humans intelligent robots speech recognition robot sensing systems emotion recognition text recognition multimodal sensors speech processing natural languages;intelligent robots;mixed initiative;planning artificial intelligence;service robots;honda asimo dialogue planning conversational service robots robot behavior planning spoken dialogue management mixed initiative dialogues task planning hierarchical planning;intelligent robots interactive systems service robots planning artificial intelligence;dialogue management conversational robot service robot behavior and dialogue planning;model integration;service robot;dialogue management;conversational robot;interactive systems;dialogue manager;behavior and dialogue planning	This paper presents a model for the behavior and dialogue planning module of conversational service robots. Most of the previously built conversational robots cannot perform dialogue management necessary for accurately recognizing human intentions and providing information to humans. This model integrates robot behavior planning models with spoken dialogue management that is robust enough to engage in mixed-initiative dialogues in specific domains. It has two layers; the upper layer is responsible for global task planning using hierarchical planning and the lower layer engages in local planning by utilizing modules called experts, which are specialized for performing certain kind of tasks by performing physical actions and engaging in dialogues. This model enables switching and canceling tasks based on recognized human intentions. A preliminary implementation of the model, which has been integrated with Honda ASIMO, has shown its effectiveness.	asimo;algorithm;automated planning and scheduling;dialog system;humanoid robot;reinforcement learning;robot;vii;vocabulary	Mikio Nakano;Yuji Hasegawa;Kazuhiro Nakadai;Takahiro Nakamura;Johane Takeuchi;Toyotaka Torii;Hiroshi Tsujino;Naoyuki Kanda;Hiroshi G. Okuno	2005	2005 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2005.1545198	simulation;computer science;artificial intelligence	Robotics	-34.40366136105874	-40.696647160775605	112988
c8dbfd75eacbaf7dbd0252ac1f134afefa5bebfa	modular gesture interface for people with severe motor dysfunction: foot recognition	gesture interface;gesture recognition;motor dysfunction and impairment;user interface	We have collected various gestures from persons with motor dysfunction who cannot use normal interface switches to develop contactless, non-constraining, and inexpensive gesture interfaces for operating PCs by utilizing a commercially available image range sensor. We describe the collection and classification of the gestures and the foot gesture recognition module.		Ikushi Yoda;Kazuyuki Itoh;Tsuyoshi Nakayama	2017	Studies in health technology and informatics	10.3233/978-1-61499-798-6-725	motor dysfunction;gesture;modular design;speech recognition;computer science	HCI	-41.030634014118405	-44.68384252419566	113244
eae024c5d4d8f4300ed904c300902763b54aa795	an adaptable rear-projection screen using digital pens and hand gestures	interactive devices gesture recognition;intoi adaptable rear projection screen digital pens accurate pen tracking hand gesture recognition rear projection foil infrared tracking system;displays cameras fingers hardware delay manufacturing size control robust control steel companies	INTOI is a rear-projection setup which combines accurate pen tracking with hand gesture recognition. The hardware consists of an Anoto pattern printed on a special rear-projection foil and an infrared tracking system. INTOI is a low-cost system that is scalable and provides highly accurate input (to less than 1mm). Finally, our setup supports a novel multi-user interaction that combines simultaneous interaction of both hand and pen gesture input.	digital pen;felix von leitner;fundamental fysiks group;gesture recognition;han unification;human–computer interaction;motion estimation;multi-user;printing;projection screen;scalability;tracking system;usability testing;video projector	Peter Brandl;Michael J Haller;Michael Hurnaus;Verena Lugmayr;Juergen Oberngruber;Claudia Oster;Christian Schafleitner;Mark Billinghurst	2007	17th International Conference on Artificial Reality and Telexistence (ICAT 2007)	10.1109/ICAT.2007.12	computer vision;simulation;engineering;computer graphics (images)	HCI	-42.09888161237188	-40.42725497680435	113332
ff9dc9ae32ab096972259852a0f63f8379814b31	marsui: malleable audio-reactive shape-retaining user interface	user interface;organic ui;shape retaining;sound;plastic;deformable ui;auditory feedback;prototype	MARSUI is a hardware deformable prototype exhibiting plastic (shape-retaining) behavior. It can track the shape that the user creates when deforming it. We envision that a set of predefined shapes could be mapped onto particular applications and functions. In its current implementation, we present three shapes that MARSUI can be deformed into: circular band, flat surface and sharp bent. These shapes map respectively onto the following applications: wristwatch, mobile phone and media player. Since the malleable interface can also take other forms, feedback plays an important role in guiding the user towards the predefined shapes. In this paper, we focus on investigating the possibilities that auditory feedback could offer in guiding the user towards reaching the intended shapes.	mobile phone;prototype;user interface;watch	Valtteri Wikström;Simon Overstall;Koray Tahiroglu;Johan Kildal;Teemu Tuomas Ahmaniemi	2013		10.1145/2468356.2479633	simulation;computer science;plastic;prototype;user interface;sound	HCI	-46.49759308412982	-40.21563898235268	113440
10bbeb0ea7ea488c3b0ad866a40114bc1e5101be	discrimination of virtual square gratings by dynamic touch on friction based tactile displays	tiled display;texture perception;texture resolution;touch physiological;co located tactile displays;discrimination thresholds;h 5 2 user interfaces haptic i o evaluation methodology tactile displays co located tactile displays friction based tactile displays jnd experiment discrimination thresholds;h 5 2 user interfaces haptic i o evaluation methodology;texture perception virtual square grating discrimination dynamic touch friction based tactile displays finely textured surfaces vision feedback tactile feedback texture resolution differential threshold weber fraction pin based array tactile rendering;dynamic touch;vision feedback;virtual square grating discrimination;friction based tactile displays;pin based array;controlled experiment;indexing terms;tactile rendering;image texture;touch physiological friction haptic interfaces image texture rendering computer graphics;h 5 2 user interfaces;finely textured surfaces;weber fraction;evaluation methodology;design guideline;tactile feedback;tactile display;haptic interfaces;rendering computer graphics;friction;differential threshold;tactile displays;jnd experiment;gratings friction displays surface texture rough surfaces surface roughness fingers feedback haptic interfaces brain modeling	In this paper, we investigate the use of friction based tactile displays for the simulation of finely textured surfaces, as such displays offer a promising way for the development of devices with co-located vision and tactile feedback. The resolution of the textures rendered with such devices and their matching to real textures have never been investigated. The paper first contributes to the evaluation of the texture resolution of friction based tactile displays. In a controlled experiment, we investigate the differential thresholds for square gratings simulated with a friction based tactile device by dynamic touch. Then we compare them to the differential thresholds of real square wave gratings. We found that the Weber fraction remains constant across the different spatial period at 9%, which is close to the Weber fraction found for corresponding real square gratings. This study inclines us to conclude that friction based tactile displays offers a realistic alternative to pin based arrays and can be used for co-located vision and tactile rendering. From the results of the experiment, we also give the design guidelines to improve the perception of textures on friction based tactile displays.	cloud fraction;image resolution;laptop;simulation;texture mapping;touchpad;trochoidal wave	Melisande Biet;Géry Casiez;Frédéric Giraud;Betty Lemaire-Semail	2008	2008 Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems	10.1109/HAPTICS.2008.4479912	computer vision;engineering;communication;computer graphics (images)	Robotics	-43.53431251883931	-49.057600923456306	113531
4069be2cfb6d76a46fb61156e23f8705d6f27dfe	an ontology-based multi-level robot architecture for learning from experiences	learning;datalogi;datavetenskap datalogi;computer science;robot;ontology;experiences	One way to improve the robustness and flexibility of robot performance is to let the robot learn from its experiences. In this paper, we describe the architecture and knowledge-representation framework for a service robot being developed in the EU project RACE, and present examples illustrating how learning from experiences will be achieved. As a unique innovative feature, the framework combines memory records of low-level robot activities with ontology-based high-level semantic descriptions.	experience;high- and low-level;knowledge representation and reasoning;service robot	Sebastian Rockel;Bernd Neumann;Jianwei Zhang;Krishna Sandeep Reddy Dubba;Anthony G. Cohn;Stefan Konecny;Masoumeh Mansouri;Federico Pecora;Alessandro Saffiotti;Martin Günther;Sebastian Stock;Joachim Hertzberg;Ana Maria Tomé;Armando J. Pinho;Luís Seabra Lopes;Stephanie von Riegen;Lothar Hotz	2013			robot;robot learning;simulation;computer science;knowledge management;artificial intelligence;ontology;personal robot	Robotics	-34.07331406484395	-39.67773383313554	113603
085ac245d93737c9ebe7a62075ca8b553ddb80f0	situated incremental natural language understanding using a multimodal, linguistically-driven update model		A common site of language use is interactive dialogue between two people situated together in shared time and space. In this paper, we present a statistical model for understanding natural human language that works incrementally (i.e., does not wait until the end of an utterance to begin processing), and is grounded by linking semantic entities with objects in a shared space. We describe our model, show how a semantic meaning representation is grounded with properties of real-world objects, and further show that it can ground with embodied, interactive cues such as pointing gestures or eye gaze.	entity;multimodal interaction;natural language understanding;situated;statistical model	Casey Redd Kennington;Spyros Kousidis;David Schlangen	2014			natural language processing;computer science;multimedia	NLP	-34.2446707283725	-41.589532327053455	113732
fb0485a16488e02a0dbb6ec3381cdd3500a6464c	viewport prediction method of 360 vr video using sound localization information		For 360 VR video, the user's viewport is portion, but the video needs to be sent to 360° spheres. So 360 VR video requires large bandwidth. In this paper, we propose a viewport prediction method to solve this problem. The proposed method predicts the user's viewport by utilizing the location information of the sound sources in 360 VR video. Especially, the proposed method is considered based on MPEG-DASH, and its feasibility is also shown by our head tracking simulation.	boulder dash;motion capture;simulation;viewport	Eunyoung Jeong;Dongho You;Changjong Hyun;Bong-Seok Seo;Namtae Kim;Dong Ho Kim;Ye Hoon Lee	2018	2018 Tenth International Conference on Ubiquitous and Future Networks (ICUFN)	10.1109/ICUFN.2018.8436981	viewport;transform coding;computer vision;spheres;quality of experience;sound localization;computer science;distributed computing;bandwidth (signal processing);artificial intelligence	EDA	-43.688323802734644	-40.36038330312935	113738
05f944791aceb8203e3401cb2a60e9d89aba634b	using mobile phones to control desktop multiplayer games	mobile phones bluetooth desktop games multiplayer games;j2se;protocols;j2me;testbed games mobile phones desktop multiplayer games control multiplatform architecture j2me j2se bluetooth communication wireless application independent remote controller;sensors;bluetooth communication;multiplayer games;mobile phone;desktop games;desktop multiplayer games control;servers;testbed games;design and implementation;multiplatform architecture;wireless application independent remote controller;games;telecontrol bluetooth computer games java mobile computing mobile handsets;multiplayer game;mobile handsets;telecontrol;communication delay;games bluetooth servers mobile handsets protocols java sensors;bluetooth;mobile phones;computer games;mobile computing;java	This work presents a multiplatform architecture to support the design and implementation of desktop multiplayer games controlled by mobile phones with Bluetooth capability. Our main objective was to demonstrate that by harnessing technologies such as J2ME, J2SE, and Bluetooth communication it is possible to transform any mobile phone that support these technologies into a wireless application-independent remote controller. To demonstrate the flexibility of this approach, we have focused on employing mobile phones as game controllers. We developed four games in different genres/styles that support various input modes among players. The results gathered from our testbed games are twofold: i) in terms of overall game performance there was no noticeable communication delays, and; ii) in terms of gameplay, we have observed that the nature of the game interaction supported by this communication architecture has enhanced the social aspect of games---each game offered a entertaining environment in which a group of people could engage in.	bluetooth;desktop computer;game controller;mobile phone;remote control;testbed	Silvano Maneck Malfatti;Fernando Ferreira dos Santos;Selan Rodrigues dos Santos	2010	2010 Brazilian Symposium on Games and Digital Entertainment	10.1109/SBGAMES.2010.32	embedded system;simulation;computer science;game mechanics;multimedia;video game development	HCI	-47.74434931733884	-38.8134727775346	113823
6a739500252163d0ed24e114fab363f5d30304d1	development of a semi-autonomous vehicle operable by the visually-impaired	software;audio systems;motor vehicles;odin;vibrations;autonomous vehicle;sensors;tactile interfaces;visually impaired person;virginia school for the blind semi autonomous vehicle visually impaired person blind driver challenge odin tactile interfaces audio interfaces;audio interfaces;driver circuits vehicles software acceleration sensors testing vibrations;testing;virginia school for the blind;acceleration;handicapped aids;road vehicles audio systems handicapped aids haptic interfaces;tactile interface;visual impairment;driver circuits;system development;semi autonomous vehicle;audio interface;vehicles;blind driver challenge;haptic interfaces;road vehicles	This paper presents the development of a system that will allow a visually-impaired person to safely operate a motor vehicle. Named the blind driver challenge, the purpose of the project is to improve the independence of the visually-impaired by allowing them to travel at their convenience. The system development is targeted to be deployed on Team Victor Tangopsilas DARPA Urban Challenge vehicle ldquoOdin.rdquo The system uses tactile and audio interfaces to relay information to the driver about vehicle heading and speed. The driver then corrects his steering and speed using a joystick. The tactile interface is a modified massage chair, which directs the driver to accelerate or brake. The audio interface is a pair of headphones, which directs the driver where to turn. Testing software has been developed to evaluate the effectiveness of the system by tracking the userspsila ability to follow signals generated by the blind driver challenge code. With these results the team hopes to improve the system, and eventually through a partnership with the Virginia School for the Blind, expand testing to include the visually-impaired.	autonomous robot;binary delta compression;course (navigation);darpa grand challenge (2007);device driver;experiment;headphones;joystick;modular programming;operability;relay;remote control;semiconductor industry;simulation;software system;steering wheel;tango;user interface;virtual reality	Dennis W. Hong;Shawn Kimmel;Rett Boehling;Nina Camoriano;Wes Cardwell;Greg Jannaman;Alex Purcell;Dan Ross;Eric Russel	2008	2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems	10.1109/MFI.2008.4648051	embedded system;simulation;engineering;communication	Robotics	-40.27197773007225	-44.73323071478167	113831
276620d27bec5dd14d5ce2a364f5210e3a479980	body ownership transfer to teleoperated android robot	tactile sensation;teleoperated android;three-way interaction;android motion;android robot;rubber hand illusion;teleoperated android robot;android robot teleoperation;similar phenomenon;tactile feedback;similar interaction;body ownership transfer	"""Teleoperators of android robots occasionally feel as if the robotic bodies are extensions of their own. When others touch the teleoperated android, even without tactile feedback, some operators feel as if they themselves have been touched. In the past, a similar phenomenon named """"Rubber Hand Illusion"""" has been studied for its reflection of a three-way interaction among vision, touch and proprioception.#R##N##R##N#In this study, we examined whether a similar interaction occurs when replacing a tactile sensation with android robot teleoperation; that is, whether the interaction among vision, motion and proprioception occurs. The result showed that when the operator and the android motions are synchronized, operators feel as if their sense of body ownership is transferred to the android robot."""		Shuichi Nishio;Tetsuya Watanabe;Kohei Ogawa;Hiroshi Ishiguro	2012		10.1007/978-3-642-34103-8_40	computer vision;simulation;engineering;communication	Robotics	-45.72700457136276	-50.558164025251244	113889
3f915f043e23be9481a03823f004bb62dcebee45	fast calibration for augmented reality	distortion;registration;augmented reality;virtual space;calibration;head mounted display	Augmented Reality overlays computer generated images over the real world. These images have to be generated using transformations which correctly project a point in virtual space onto its corresponding point in the real world. We present a simple and fast calibration scheme for head-mounted displays (HMDs), which does not require additional instrumentation or complicated procedures. The user is interactively guided through the calibration process, allowing even inexperienced users to calibrate the display to their eye distance and head geometry. The calibration is stable - meaning that slight errors made by the user do not result in gross miscalibrations - and easily applicable for see-through and video-based HMDs.	augmented reality;experience;head-mounted display;interactive media;virtual reality	Anton L. Fuhrmann;Dieter Schmalstieg;Werner Purgathofer	1999		10.1145/323663.323692	computer vision;augmented reality;calibration;simulation;distortion;computer science;optical head-mounted display;computer graphics (images)	Graphics	-41.36922401870155	-39.369846533203685	113945
07aaa00fc4a5565336b028d724ce604ce0547917	a dynamic and static microcomputer-based stereogram generator	microordenador;dynamic circuit;memoire;biological techniques and instruments;eye;bioelectric potentials;stereogramme;tv retina cathode ray tubes oscilloscopes optical pulse generation microcomputers optical receivers computerized monitoring electrophysiology solids;computer graphics;disparity;estrategia;taille;disparidad;generador;microordinateur;microcomputer;visual perception bioelectric potentials biological techniques and instruments computer graphics computerised picture processing eye microcomputer applications;strategy;generator;memoria;visual evoked response data red elements microcomputer based stereogram generator solid shaped target random element stereogram generator green elements color monitor sinusoidal triangle square wave eye movement;estereograma;eye movement;talla;stereogram;computerised picture processing;visual perception;microcomputer applications;size;generateur;strategie;materiel informatique;disparite;material informatica;circuit dynamique;circuito dinamico;memory;hardware	A dynamic and static generator for random element stereograms is presented. The system allows any solid shaped target to be programmed and vary in size or shape, movement and disparity. The random element stereogram generator (RESG) creates static or dynamic random red and green elements on a color monitor. Target size is dependent on overall monitor size, and programmable movement varies over a large range with three types of movement possible: sinusoidal, triangle, and square wave. The versatility of the system is demonstrated with eye movement and visual evoked-response data. >	microcomputer;stereoscopy	Mohammad S. Obaidat;Lawrence E. Leguire	1991	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.101152	computer vision;computer hardware;visual perception;strategy;computer science;microcomputer;size;memory;computer graphics;electric generator;eye movement;computer graphics (images)	Embedded	-42.65525435795123	-49.8843123020052	114073
7376bf56690b2a3435bc85b0ed1406fec8a718ea	effective information gathering on the web	user study;searching;information gathering;re finding;keeping;user behavior;web tasks	This paper presents Web Gad, a tool intended to improve how users perform information gathering tasks on the Web. Web Gad is meant to assist users with several subtasks under the information gathering task with specific emphasis on managing, organizing, keeping, and re-finding information during the task. The prototype system was designed based on recommendations derived in a previous user study (Alhenshiri et al., 2012) in which the user behavior, user activities, used tools, and encountered difficulties were observed and analyzed. Web Gad is ready for evaluation in a planned complete factorial and counterbalanced user study to demonstrate its effectiveness, efficiency, and enjoyments as a Web information gathering tool.	organizing (structure);prototype;usability testing;world wide web	Anwar Alhenshiri;Carolyn R. Watters;Michael A. Shepherd	2012		10.1145/2132176.2132230	web service;web development;web modeling;simulation;web mapping;web design;computer science;web navigation;multimedia;world wide web;web server	HCI	-34.107767948472166	-51.33580425634452	114122
5cd7bf10bfcd91b336c964bc9b20ec7edcbf8715	keyboard before head tracking depresses user success in remote camera control	automatic control;remote camera control;pan tilt zoom camera;physical games;soccer head tracking;human computer interaction;motion control;usability evaluation;book chapter;pan tilt zoom;head motion;head tracking;enhanced performance;camera control;camera motions;camera motion;keywords automatic control;joystick control;remote mining;complex machinery;survey data;difference set;camera controls;control methods;control method;teleoperation;remote cameras	In remote mining, operators of complex machinery have more tasks or devices to control than they have hands. For example, operating a rock breaker requires two handed joystick control to position and fire the jackhammer, leaving the camera control to either automatic control or require the operator to switch between controls. We modelled such a teleoperated setting by performing experiments using a simple physical game analogue, being a half size table soccer game with two handles. The complex camera angles of the mining application were modelled by obscuring the direct view of the play area and the use of a Pan-Tilt-Zoom (PTZ) camera. The camera control was via either a keyboard or via head tracking using two different sets of head gestures called “head motion” and “head flicking” for turning camera motion on/off. Our results show that the head motion control was able to provide a comparable performance to using a keyboard, while head flicking was significantly worse. In addition, the sequence of use of the three control methods is highly significant. It appears that use of the keyboard first depresses successful use of the head tracking methods, with significantly better results when one of the head tracking methods was used first. Analysis of the qualitative survey data collected supports that the worst (by performance) method was disliked by participants. Surprisingly, use of that worst method as the first control method significantly enhanced performance using the other two	automatic control;experiment;joystick;motion capture;pan–tilt–zoom camera	Dingyun Zhu;Tamás D. Gedeon;Ken Taylor	2009		10.1007/978-3-642-03658-3_37	motion control;computer vision;teleoperation;simulation;human–computer interaction;survey data collection;automatic control;difference set;computer graphics (images)	HCI	-44.582335599686296	-48.68179113351698	114217
17364226ed3245eaddc640810a48120db422880a	a cognitive model of visual path planning in a multi-robot control system	cognitive modeling path planning visual navigation robot control;perceptual path planning;user interface;path planning;visual navigation;satisfiability;data mining;path planning multi robot systems;navigation;visualization;robot control;visual planning cognitive model perceptual path planning multi robot control;robots;multi robot systems;visual planning;multi robot control;planning;humans;cognitive modeling;model fitting;individual difference;cognitive model;path planning control system synthesis humans biological system modeling cognitive robotics usa councils robot control navigation engines physics;evaluation model	We discuss an experiment involving visual path planning for multiple, remote robots in a partially visible building, with a partial 2D map available. Participants in the experiment defined waypoints for each robot to circumnavigate obstacles and explore the building. A cognitively plausible model of visual planning is evaluated using a normalized metric of the fit between model and subject itineraries. We discuss variation in the data and model fit, indicating individual differences in strategies to cope with task demands.	cognitive model;control system;motion planning;robot control	David Reitter;Christian Lebiere;Michael Lewis;Huadong Wang;Zheng Ma	2009		10.1109/ICSMC.2009.5346070	cognitive model;computer vision;simulation;computer science;artificial intelligence	Robotics	-36.28282230916205	-41.54992385148077	114694
70d1d05e4674febede6aa6e05605916a3b9be153	development and experimental validation of a master interface with vibrotactile feedback for robotic telesurgery	telerobotics actuators end effectors feedback medical robotics surgery;robot sensing systems;actuators;visualization;surgery;haptic interfaces;surgery haptic interfaces actuators robot kinematics visualization robot sensing systems;visual feedback master interface vibrotactile feedback robotic telesurgery haptic perception teleoperated robotic system vibrotactile actuators end effector haptic joystick novint falcon kuka light weight robot iii virtual environment;robot kinematics	This work wants to investigate the efficacy of a vibro-tactile feedback to convey a haptic perception to the surgeon in a teleoperated robotic system for surgery. To this purpose, vibrotactile actuators have been embedded in the end-effector of the master interface of a tele-operated robotic system made of the haptic joystick Novint Falcon and the Kuka Light Weight Robot III. Vibrotactile feedback can be used to support the surgeon during the surgical procedure, guiding him/her during the intervention, and to train unskilled surgeons with simulators. The development and the experimental validation of the master interface with the vibrotactile feedback is presented in this paper. The system has been validated on 12 subjects, who were requested to control the movement of a sphere along a desired path in a virtual environment. Results have been compared with the three cases of absence of feedback, visual feedback and combined vibrotactile and visual feedback. The obtained results demonstrate that a vibrotactile feedback can improve in a statistically significant manner the accuracy of the procedure with respect to the absence of feedback.	embedded system;falcon (video game series);feedback;haptic technology;joystick;modality (human–computer interaction);modulation;novint technologies;remote surgery;robot end effector;sensory substitution;simulation;television;virtual reality	Elena F. Gambaro;Loredana Zollo;Eugenio Guglielmelli	2014	5th IEEE RAS/EMBS International Conference on Biomedical Robotics and Biomechatronics	10.1109/BIOROB.2014.6913864	control engineering;computer vision;simulation;engineering	Robotics	-42.33167898452569	-46.231164129631594	114821
c76d257414ece7e44171e457137cff4fc2711bfa	synchronizing digital signage content with the movement of passerby	foot tracking;digital signage;3d content;pressure sensing mat device	The use of digital signage systems is increasingly common in public spaces such as shopping malls, train stations, airports and urban areas. In this paper, we present a new interaction scheme for digital signage that allows the passerby to keep him/her walking while getting a series of information. The innovative features of our system include estimation of the passerby's eye level based on features of gait on a pressure sensing floor mat device, and presentation of a 3D content considering his/her eye level.	digital signage;signage systems	Nozomi Tamaki;Masahito Hirakawa	2015		10.1145/2801040.2801050	simulation;engineering;multimedia;advertising	HCI	-44.41819531321318	-41.7305117821353	114826
259741f2137851a4a6aa925ac47db7829dc6f732	using bio-inspired algorithm to compensate web page color contrast for dichromat users		With this paper, we are focusing on improving web accessibility, more precisely on compensating the contrast loss for textual web content for dichromat users. A study over the entire sRGB color space showed that the loss experienced by a dichromat user may be significant. With the current approach, we assess the interest of using API for our problem. Several tests for different parameters settings were performed on both real and synthetic data in order to assess the algorithm efficiency.		Alina Mereuta;Sebastien Aupetit;Nicolas Monmarché;Mohamed Slimane	2014		10.1007/978-3-319-12970-9_9	multimedia;world wide web;information retrieval	Vision	-34.971776401270944	-49.84297022591446	114840
fc59bbcc7ce8edb9ed9fca11420e85f6d9171591	the smartphone as a 3d input device	human device interaction;3d navigation;smart phones;data visualisation;smart phones data visualisation interactive systems;acceleration navigation accelerometers gyroscopes data visualization sensors cameras;interactive systems;3d navigation mobile devices human device interaction;mobile devices;user interaction smart phone 3d input handheld device 3d visualization interaction 3d presentation interaction exhibitions museums business presentations computer games position sensors motion sensors 3d navigation	This paper presents a concept for the use of a smart phone as a handheld input device for the interaction with 3D visualizations and presentations. Applications are mainly in the area of exhibitions and museums to enable visitors to interact with certain exhibits. Moreover applications for business presentations or computer games are possible. The usability of motion and position sensors in modern smart phones for the purpose of 3D navigation is examined. An algorithm to prevent from position drifting on short distances is developed. Finally a demo application presents the navigation concept and the ability for multiple users to simultaneously interact with the same visualization.	algorithm;handheld game console;input device;multi-user;pc game;sensor;smartphone;usability	Henning Graf;Klaus Jung	2012	2012 IEEE Second International Conference on Consumer Electronics - Berlin (ICCE-Berlin)	10.1109/ICCE-Berlin.2012.6336487	simulation;human–computer interaction;engineering;multimedia	Visualization	-44.53029662435245	-41.64989194624685	114918
d10fc8de73ecd917cacd66d845a9a32158ff5c3d	feedback mechanisms for a natural language interface: an application of the critic paradigm	feedback mechanism;natural language interface	Users of natural language interfaces (NLI) to databases often build their own notions about the linguistic capabilities of the NLI and bear false expectations in mind with respect to the intelligence of the system. An NLI should provide feedback on what it does and what it understands. In addition, user queries and intermediate results have to be analyzed for errors, so that the interface can signal potential problem situations. This paper discusses the application of the critic paradigm for human-computer interaction to natural language interfaces, and presents a computer-based assistant for the NLI-Z39.50, a natural language interface for access to library databases. The search assistant, a so called critic system, spots errors in the natural language input, provides feedback on intermediate results, suggests search terms for query refinement, and aids the users when they do not know how to perform a search successfully or when they encounter common search problems.	database;feedback;human–computer interaction;native-language identification;natural language processing;natural language user interface;programming paradigm;recommender system;refinement (computing);search problem;spell checker;usability;web search engine;dialog	Johannes Leveling	2004			natural language processing;natural language programming;natural language user interface;computer science;artificial intelligence;communication	DB	-37.23247789314503	-50.82432935255734	114931
b765e3de4ce76c3251abeaf5acb4c217f0ea55b8	the room effect: metric spatial knowledge of local and separated regions	dual mode model;workable knowledge;spatial layout;early acquisition assumption;metric spatial knowledge;room effect;different room;route knowledge;separated regions;accurate metric spatial information;survey knowledge	Navigating through real or virtual worlds requires a workable knowledge of the spatial layout. According to the landmark-route-survey model, metric spatial knowledge (survey knowledge) is acquired only following the acquisition of landmark and route knowledge. A dual mode model was proposed that assumes that survey knowledge may be quickly acquired for local regions. Research was conducted to understand how people rapidly acquire survey knowledge. Participants in three experiments briefly navigated on one floor of a virtual building, moving down hallways and performing tasks by using objects in rooms. Participants were later asked to answer from memory about the direction of objects by using pointing and map-drawing measures. A room effect was found for both measures; the angular positions of two objects in the same room were more accurately reported than those in different rooms. Accurate metric spatial information was available for objects in the same room, supporting an early acquisition assumption.	angularjs;building information modeling;experiment;virtual world	Herbert A. Colle;Gary B. Reid	1998	Presence	10.1162/105474698565622	computer vision;simulation;computer science	HCI	-43.912855362432786	-49.88777774568472	115017
fad36feba37ca2323927a3267e9f146428d8b812	"""kinect-sign: teaching sign language to """"listeners"""" through a game"""	sign language;serious game;gesture recognition;kinect sensor	The sign language is widely used by deaf people around the globe. As the spoken languages, several sign languages do exist. The way sign language is learned by deaf people may have some details to be improved, but one can state that the existing learning mechanisms are effective when we talk about a deaf child, for example. The problem arises for the non-deaf persons that communicate with the deaf persons – the so-called listeners. If, for example, one couple has a new child that turns to be deaf, these two persons find a challenge to learn the sign language. In one hand, they cannot stop their working life, especially because of this sad news turns to be more costly, on the other hand, the existing mechanisms target the deaf-persons and are not prepared for the listeners. This paper proposes a new playful approach to help these listeners to learn the sign language. The proposal is a serious game composed of two modes: School-mode and Competition-mode. The first provides a school-like environment where the user learns the letter-signs and the second provides the user an environment used towards testing the learned skills. Behind the scenes, the proposal is based on two phases: 1 – the creation of a gestures library, relying on the Kinect depth camera; and 2 – the realtime recognition of gestures, by comparing what the depth camera information to the existing gestures previously stored in the library. A prototype system was developed – the Kinect-Sign – and tested in a Portuguese Sign-Language school resulting in a joyful acceptance of the approach. © 2014 The Authors. Published by Elsevier Ltd. Selection and peer-review under responsibility of ISEL – Instituto Superior de Engenharia de Lisboa.	kinect;prototype	João Gameiro;Tiago Cardoso;Yves Rybarczyk	2013		10.1007/978-3-642-55143-7_6	speech recognition;computer science;gesture recognition;multimedia;communication	AI	-47.94797535515237	-39.420564192596714	115057
911414dbe1129c361335ddcbccf374712757182a	rectangular stable power-aware mobile projection on planar surfaces	projector stabilization;keystone correction;projector calibration	Pico projectors are becoming increasingly popular and can be used in conjunction with handheld mobile devices, portable media players, digital cameras in the foreseeable future. Mobile projection provides new opportunities realizing mobile spatial augmented reality applications. Projection from handheld devices bring in challenges of stabilizing the imagery in the persence of hand movements which entails more complex problems like keystone, zoom and jitter correction. However, no suitable software interface exists today that integrates these pico-projectors with handheld mobile devices.  This paper develops the first power-efficient software methods and interface to project a rectangular, stable image from a pico-projector augmented to a handheld mobile device that can allow users to make contents legible on such devices via real-time solutions for keystone correction, zoom and jitter correction. To provide power efficient solutions in the context of mobile power-sensitive devices, we achieve the keystone correction, zoom and hand-movement stabilization mostly using data from an inertial measurement unit (IMU) while minimizing the use of less power-efficient sensor like camera. For zoom stabilization, we use a very inexpensive low power distance sensor. The camera is used only for initialization. We have developed two prototype systems to show our software interface: the first one is implemented on cellphone running Android OS; and the second one used a raspberry pi interface to the projectors.	android;augmented reality;digital camera;handheld game console;handheld projector;keystone effect;mobile device;mobile phone;movie projector;multi-user;operating system;output device;pico;power distance;prototype;range imaging;raspberry pi 3 model b (latest version);real-time transcription;sensor;video projector	Mehdi Rahimzadeh;Hung Nguyen;Ardalan Amiri Sani;Fadi J. Kurdahi;Aditi Majumder	2016		10.1145/3013971.3013989	embedded system;computer vision;keystone effect;computer graphics (images)	Mobile	-43.65985577976061	-40.95365182742772	115319
3ef2bef6c9345cc8e3ce97c26fcf08d01f32cab5	multimodal human-robot interaction	computers;robot sensing systems;emotional communications;human computer interaction;multimodal interface;gesture recognition multimodal human robot interaction web cameras microphone array distant speech recognition face tracking manual entry verbal dialog visual communications emotional communications task execution task completion;mobile informational robot;mobile informational robot human computer interaction multimodal interface speech recognition;speech;manual entry;mobile robots;visual communications;human robot interaction;speech recognition gesture recognition human robot interaction mobile robots;face tracking;distant speech recognition;web cameras;microphone array;verbal dialog;mobile communication;speech recognition;speech robot sensing systems mobile communication speech recognition humans computers;task execution;humans;interaction model;multimodal human robot interaction;task completion;gesture recognition;human computer interface	Multimodal human-computer interface provides a connection between a user and a computer in a natural manner. In order to determine position of the user and his speech queries, the robot uses a set of web-cameras, microphone array and technologies for distant speech recognition and face tracking. There are some methods for interaction between users and computers, such as manual entry, verbal dialog, visual and emotional communications. Interaction model of the proposed mobile informational robot has 3 main parts: beginning of the dialogue, task execution and completion of the dialogue. Further development of the model is aimed to implement a technology for gesture recognition for supporting a special group of disabled people.	computer;dialog system;facial motion capture;gestalt psychology;gesture recognition;human–computer interaction;human–robot interaction;microphone;multimodal interaction;robot;speech recognition;dialog	V. Yu. Budkov;Maria Prischepa;Andrey Ronzhin;Alexey Karpov	2010	International Congress on Ultra Modern Telecommunications and Control Systems	10.1109/ICUMT.2010.5676593	mobile robot;computer vision;facial motion capture;speech recognition;mobile telephony;computer science;speech;gesture recognition;visual communication	Robotics	-36.14811392045045	-42.29959867971674	115845
d8dadf0b938fb0a3fc5b2b57ffa949711b14ba8b	panel-type rt devices realizing user intuitive intelligent environment		Recently, the number of used house have been increased in Japan. However, it is difficult to create the new value for used houses. To solve the usage of used house, the robotics technology has large potential to improve the situations. Many researchers have been proposed and developed intelligent environment based on robotics technologies. However, it is difficult to construct intelligent environment based on previous research. Based on the back ground, in this paper, we proposed panel-type RT device, which can realize flexible and user intuitive intelligent environment based on robot technologies. In this paper, we show the concept of panel-type RT device and its prototype. And, several example implementation of RT panel and integration are introduced.	intelligent environment;prototype;robotics	Kenichi Ohara;Ryosuke Oe;Yuji Mizutani;Hideki Nishio;Akio Tomita	2017	2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2017.7992867	human–computer interaction;robot;middleware;robotics;artificial intelligence;intelligent environment;engineering;user interface	Robotics	-47.095799929573204	-41.48961806746527	116004
10fa439aea98a735daabf791515c934ed3102564	using onscreendualscribe to support text entry and targeting among individuals with physical disabilities	cerebral palsy;muscular dystrophy;assistive technology;input devices	This paper describes a study examining the usability of OnScreenDualScribe (OSDS), a tool to support individuals with physical disabilities with text entry and cursor movement. A portable numeric keypad is used to interact with the OSDS, which can either be held by the user, or can be affixed to a surface for interaction. A study to determine the feasibility of the system was conducted with three individuals with physical disabilities. While it was noted that the time taken was higher to complete a task compared to their existing methods of computer-based input, findings also indicate that the system offers potential for tasks involving a combination of text entry and cursor movement (e.g., completing online forms). Furthermore, as the keypad is smaller in size compared with a traditional keyboard, participants suggested that it offered potential to reduce effort spent in the fatiguing process of traversal.	cursor (databases);human–computer interaction;pointing device gesture;r language;tree traversal;usability;user interface	Sidas A. Saulynas;Lula Albar;Ravi Kuber;Torsten Felzer	2015		10.1145/2700648.2811348	simulation;computer science;operating system;multimedia;input device	HCI	-47.061811597987706	-45.22623605166591	116058
8362d6f31d9ce71b785f3991eb0ccb9bf5d2f473	route guidance for visually impaired based on haptic technology and their spatial cognition		Haptic (vibration) information expression is an effective humancomputer interaction mode and information transfer method. It makes up shortcomings of sound under certain conditions and be an important channel of information transfer for route guidance field. As a special kind of walkers, the visually impaired pedestrians have a specific type of cognition or perception of route guidance environment (including spatial orientation, distance and walking speed etc.). And they have more sensitive sense of touch than that the ordinary people have. This work integrated application of GPS, GIS and Haptic (vibration) technology to develop more reliable mode route guidance for the visually impaired. It provides different vibration to the user under two circumstances: key nodes of roads ahead and deviation planning path. It has several obvious advantages, such as higher anti-noise, sensitivity and effectiveness. Using HTC Legend phone, we developed the prototype and realized the designed functions, and verify the effectiveness of the system. We initially determined the thresholds of deviating from the path, those at road junctions and other nodes through experiments, interviews etc. And we used the thresholds for experiments testing and guiding. Then they were inspected, corrected and improved in the field practice. Finally, more reasonable thresholds were drawn out for future applications in reality.	cognition;experiment;geographic information system;global positioning system;haptic technology;prototype	Guansheng Wang;Jianghua Zheng;Hong Fan	2017		10.1007/978-981-10-6385-5_60	simulation;phone;spatial cognition;information transfer;perception;haptic technology;computer vision;computer science;cognition;communication channel;vibration;artificial intelligence	HCI	-44.432058495022346	-45.255630929865056	116127
5fd5f74fab7e0dc2c42bb22e1e4b6c369ce13617	learning algorithms for human–machine interfaces	hand;psychomotor performance;moore penrose pseudoinverse transformation;machine learning algorithms;control systems;remote control;instruments;electronic mail;algorithms artificial intelligence communication aids for disabled hand humans man machine systems multivariate analysis posture psychomotor performance robotics signal processing computer assisted user computer interface;multivariate analysis;least mean squares methods;simulated planar two link arm;data glove;communication aids for disabled;least squares approximation;robotics;human robot interaction;machine learning adaptive learning hand posture human machine interface;finger motions;simulated robot arm;signal processing computer assisted;machine learning algorithms machine learning fingers computational modeling least squares approximation control systems electronic mail instruments data gloves computer simulation;posture;biocybernetics;moore penrose pseudoinverse transformation human machine interfaces machine learning algorithms data glove finger motions high dimensional glove signals remote control simulated planar two link arm simulated robot arm least mean square gradient descent;computational modeling;high dimensional glove signals;machine learning;human machine interface;adaptive learning;human machine interfaces;fingers;telecontrol;gradient methods;artificial intelligence;algorithms;data gloves;hand posture;humans;user computer interface;learning artificial intelligence;least mean square gradient descent;computer simulation;man machine systems;telecontrol biocybernetics data gloves gradient methods human robot interaction learning artificial intelligence least mean squares methods	The goal of this study is to create and examine machine learning algorithms that adapt in a controlled and cadenced way to foster a harmonious learning environment between the user and the controlled device. To evaluate these algorithms, we have developed a simple experimental framework. Subjects wear an instrumented data glove that records finger motions. The high-dimensional glove signals remotely control the joint angles of a simulated planar two-link arm on a computer screen, which is used to acquire targets. A machine learning algorithm was applied to adaptively change the transformation between finger motion and the simulated robot arm. This algorithm was either LMS gradient descent or the Moore-Penrose (MP) pseudoinverse transformation. Both algorithms modified the glove-to-joint angle map so as to reduce the endpoint errors measured in past performance. The MP group performed worse than the control group (subjects not exposed to any machine learning), while the LMS group outperformed the control subjects. However, the LMS subjects failed to achieve better generalization than the control subjects, and after extensive training converged to the same level of performance as the control subjects. These results highlight the limitations of coadaptive learning using only endpoint error reduction.	algorithm;communication endpoint;computer monitor;gradient descent;machine learning;moore–penrose pseudoinverse;motion;robotic arm;wired glove	Zachary C Danziger;Alon Fishbach;Ferdinando A. Mussa-Ivaldi	2009	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2009.2013822	computer simulation;human–machine interface;computer vision;simulation;computer science;electrical engineering;artificial intelligence;wired glove;machine learning;biocybernetics;multivariate analysis;robotics;computational model;least squares;adaptive learning;remote control;generalization error	ML	-40.914612979499935	-48.29611018178109	116170
1f19e20d4fbf0ec8bb6fe94136f90e50b4715ad1	interactive omnidirectional video delivery: a bandwidth-effective approach	bandwidth constraint;live user interaction;video surveillance;quality video;uniform quality;cylindrical region;omnidirectional video;high-resolution video;rectangular panorama video;full panorama video;bandwidth-effective approach;interactive omnidirectional video delivery	Omnidirectional video (cylindrical or spherical) is a new media becoming more and more popular thanks to its interactivity for online multimedia applications such as Google Street View as well as for video surveillance and robotics applications. Interactivity in this context means that the user is able to explore and navigate audio-visual scenes by freely choosing viewpoint and viewing direction. In order to provide this key feature, omnidirectional video is typically represented as a classical two-dimensional (2D) rectangular panorama video that is mapped onto a (spherical or cylindrical) mesh and then rendered on the client's screen. Early transmission models of this full panorama video and mesh content simply deal with the panorama as a high-resolution video to be encoded at uniform quality. Generally the user can only view a restricted field-of-view of the content and then interact with pan-tilt-zoom commands. This means that a significant part of the bandwidth is wasted by transmitting quality video in regions that are not being visualized. In this paper we evaluate the relevance and optimality of a personalized transmission where quality is modulated in spherical or cylindrical regions depending on their likelihood to be viewed during a live user interaction. We show, based on interaction delay as well as bandwidth constraints, how tiling and predictive methods can improve on existing methods. © 2012 Alcatel-Lucent.	360-degree video;adobe flash player;client-side;closed-circuit television;digital distribution;digital video;google street view;image resolution;immersion (virtual reality);interactivity;internet access;mathematical optimization;modulation;new media;personalization;relevance;robotics;tiling window manager;transmitter;viewing cone;viewport	Patrice Rondao-Alface;Jean-François Macq;Nico Verzijp	2012	Bell Labs Technical Journal	10.1002/bltj.20538	computer vision;computer science;video tracking;multimedia;computer graphics (images)	Vision	-43.814363488901115	-40.2052639976774	116295
e8081aad0514a60307e5a7083a04aea91bcf33e2	reducing visual demand for gestural text input on touchscreen devices	automatic error correction;unistrokes;text entry;text input;gestural input;error correction;error rate;visual attention;graffiti;soft keyboard	We developed a text entry method for touchscreen devices using a Graffiti-like alphabet combined with automatic error correction. The method is novel in that the user does not receive the results of the recognition process, except at the end of a phrase. The method is justified over soft keyboards in terms of a Frame Model of Visual Attention, which reveals both the presence and advantage of reduced visual attention. With less on-going feedback to monitor, there is a tendency for the user to enter gestures more quickly. Preliminary testing reveals reasonably quick text entry speeds (>20 wpm) with low errors rates (<5%).	error detection and correction;forward error correction;graffiti (palm os);touchscreen;words per minute	I. Scott MacKenzie;Steven J. Castellucci	2012		10.1145/2212776.2223840	error detection and correction;speech recognition;word error rate;computer science;operating system;graffiti;multimedia	HCI	-47.13272848432865	-45.44546508553875	116452
9ac39f0b42bea29239271b5b5a744a50095936b3	personalizing a smartwatch-based gesture interface with transfer learning	abstracts computers;user interfaces gesture recognition haar transforms learning artificial intelligence smart phones;visual impairments gesture recognition smartwatch transfer learning haar features;haar coefficients smartwatch based gesture interface smartphone based solutions touch based interaction paradigm hands free interaction arm gestures gesture recognition system novel transfer metric learning algorithm	The widespread adoption of mobile devices has lead to an increased interest toward smartphone-based solutions for supporting visually impaired users. Unfortunately the touch-based interaction paradigm commonly adopted on most devices is not convenient for these users, motivating the study of different interaction technologies. In this paper, following up on our previous work, we consider a system where a smartwatch is exploited to provide hands-free interaction through arm gestures with an assistive application running on a smartphone. In particular we focus on the task of effortlessly customizing the gesture recognition system with new gestures specified by the user. To address this problem we propose an approach based on a novel transfer metric learning algorithm, which exploits prior knowledge about a predefined set of gestures to improve the recognition of user-defined ones, while requiring only few novel training samples. The effectiveness of the proposed method is demonstrated through an extensive experimental evaluation.	algorithm;gesture recognition;mobile device;programming paradigm;smartphone;smartwatch	Gabriele Costante;Lorenzo Porzi;Oswald Lanz;Paolo Valigi;Elisa Ricci	2014	2014 22nd European Signal Processing Conference (EUSIPCO)		computer vision;speech recognition;computer science;gesture recognition;communication	HCI	-37.88943862257785	-46.16577903091959	116486
cafda75aa5fe4d30c55b5a2bcfcdbd68e6d8af47	real-time markerless tracking the human hands for 3d interaction		xvii	3d interaction;real-time transcription	Markus Schlattmann	2011			computer vision;artificial intelligence;computer science;3d interaction	Vision	-40.04040140727254	-38.441785147770275	116545
200e0a1088db72e961dd042bd0868b43fa58f27f	construction of mixed reality story environment based on real space shape		We propose a system to construct Mixed Reality (MR) environments based on actual environments. Our goal is to develop MR content in which users and characters can coexist in an actual room and experience a story. This study represents a preliminary step in the development of such an MR experience. We proposed a method to construct an MR environment that uses three-dimensional information acquired from the actual environment based on rules. We extract valid plane information from three-dimensional information and use it. Virtual objects used for MR content can be placed in real space automatically. Rules are created by specifying the area and height of the plane of the real space. In this study, we placed seven types of objects in two types of rooms and confirmed that it is possible to construct an environment using a transmission-type head mount-ed display. However, under the current system, users cannot experience a story.	mixed reality	Kazuma Nagata;Soh Masuko;Junichi Hoshino	2018		10.1007/978-3-319-99426-0_27	simulation;mixed reality;computer science	Visualization	-42.543074128994505	-38.594611670144026	116665
2d3878f9bf4ca77737ee09c04d087e3a813094f3	controlling a wheelchair indoors using thought	sensor processing;paralysis;brain;medical signal processing diseases electroencephalography handicapped aids;motion guidance strategy;systeme nerveux central;wheelchair;guidage;brain controlled wheelchair;brain computer interface;electroencefalografia;disease;hombre;gaze tracking;intelligence artificielle;silla de ruedas;guiado;encefalo;motor neuron;electroencephalographie;wheelchair brain machine interface;captador medida;sistema nervioso central;cerebro;handicapped aids;measurement sensor;capteur mesure;encephale;sensor processing amyotrophic lateral sclerosis disease paralysis brain computer interface brain controlled wheelchair eeg signal motion guidance strategy;cerveau;human;amyotrophic lateral sclerosis;wheel chair;brain machine interface;diseases;guidance;artificial intelligence;physical interface;encephalon;inteligencia artificial;electroencephalography;eeg signal;wheelchairs hospitals prototypes navigation degenerative diseases neurons cost function brain computer interfaces control systems system performance;indoor installation;instalacion interior;installation interieure;medical signal processing;central nervous system;control strategy;homme;fauteuil roulant	Amyotrophic lateral sclerosis, or ALS, is a degenerative disease of the motor neurons that eventually leads to complete paralysis. We are developing a wheelchair system that can help ALS patients, and others who can't use physical interfaces such as joysticks or gaze tracking, regain some autonomy. The system must be usable in hospitals and homes with minimal infrastructure modification. It must be safe and relatively low cost and must provide optimal interaction between the user and the wheelchair within the constraints of the brain-computer interface. To this end, we have built the first working prototype of a brain-controlled wheelchair that can navigate inside a typical office or hospital environment. This article describes the BCW, our control strategy, and the system's performance in a typical building environment. This brain-controlled wheelchair prototype uses a P300 EEG signal and a motion guidance strategy to navigate in a building safely and efficiently without complex sensors or sensor processing	brain–computer interface;control theory;electroencephalography;eye tracking;joystick;lateral thinking;prototype;sensor	Brice Rebsamen;Etienne Burdet;Cuntai Guan;Haihong Zhang;Chee Leong Teo;Qiang Zeng;Christian Laugier;Marcelo H. Ang	2007	IEEE Intelligent Systems	10.1109/MIS.2007.26	brain–computer interface;simulation;artificial intelligence	Mobile	-40.15728050106932	-45.45860692607347	116715
8e453234f395cf5218e618423920fdd6bfb94548	long live the sensor! designing with energy harvesting		Energy harvesting is an emerging concept that could open new design vistas for sensor-embedded environments as it provides battery-independent capabilities to sensors. In this pictorial, we illustrate how the topic of energy harvesting is approached from the lens of designers. Through the dynamic use of images coupled with physical prototyping, a new design direction for energy harvesting has been formulated. We introduce Livefeed, a prototype series of sensor-embedded objects that live through ambient energy.	embedded system;image;prototype;sensor	Zhide Loh;Jung-Joo Lee;Kee Hong Song	2017		10.1145/3059454.3059488	simulation;energy harvesting;computer science	HCI	-45.99845612132917	-39.06094115221676	116750
8a0dfd273a8cd52ad58de02788324d3ae84a62d3	vos - designing a visual orientation system		While humans possess a well-developed sense of direction and can easily walk to a visible target, that ability is drastically reduced when lacking visual cues. In situations where people cannot depend on sight, orientation might become a critical issue, as when escaping a room filled with smoke, swimming in open waters, hiking in the fog or crossing the woods at night. In this paper, we present the design and implementation of VOS - a Visual Orientation System for providing an augmented sense of direction. Our system uses LEDs to offer cues on how to correct the current heading. Our findings consist of demonstrating the viability of such system, as well as its usability. We discuss the implications for designing technology that enables people to orient themselves and navigate places with little or no visual cues.	course (navigation);usability	Francisco Esteban Kiss;Albrecht Schmidt;Pawel W. Wozniak	2018		10.1145/3170427.3188673	human–computer interaction;usability;sight;sensory cue;computer science;sense of direction	HCI	-44.663239146777066	-45.319648788286926	116974
5dd8c90d97b37bd07c5db292d7583a7afa8aeb66	you're in control: a urinary user interface	user interface;control system;urinal;augmented reality;entertainment	The You're In Control system uses computation to enhance the act of urination. Sensors in the back of a urinal detect the position of impact of a stream of urine, enabling the user to play interactive games on a screen mounted above the urinal.	computation;control system;sensor;user interface	Dan Maynes-Aminzade;Hayes Raffle	2003		10.1145/765891.766108	augmented reality;entertainment;simulation;human–computer interaction;computer science;control system;multimedia;user interface	HCI	-43.715316536309444	-42.15549829640156	117219
c479aa129e2db7ac1d67b136b036b375ff64f9a5	the design and validation of the r1 personal humanoid		In recent years the robotics field has witnessed an interesting new trend. Several companies started the production of service robots whose aim is to cooperate with humans. The robots developed so far are either rather expensive or unsuitable for manipulation tasks. This article presents the result of a project which wishes to demonstrate the feasibility of an affordable humanoid robot. R1 is able to navigate, and interact with the environment (grasping and carrying objects, operating switches, opening doors etc). The robot is also equipped with a speaker, microphones and it mounts a display in the head to support interaction using natural channels like speech or (simulated) eye movements. The final cost of the robot is expected to range around that of a family car, possibly, when produced in large quantities, even significantly lower. This goal was tackled along three synergistic directions: use of polymeric materials, light-weight design and implementation of novel actuation solutions. These lines, as well as the robot with its main features, are described hereafter.	dependability;humanoid robot;humans;microphone;network switch;prototype;robotics;synergy	Alberto Parmiggiani;Luca Fiorio;Alessandro Scalzo;Anand Vazhapilli Sureshbabu;Marco Randazzo;Marco Maggiali;Ugo Pattacini;Hagen Lehmann;Vadim Tikhanoff;Daniele Domenichelli;Alberto Cardellino;Pierpaolo Congiu;Andrea Pagnin;Roberto Cingolani;Lorenzo Natale;Giorgio Metta	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8202224	computer science;humanoid robot;simulation;computer vision;artificial intelligence;robot;doors;robotics	Robotics	-37.151548739535556	-40.203931925646096	117391
ec395b4650e84b3cb63056b62a74511baa72acfc	an approach to develop a virtual 3d stereoscopic environment for smartphones	smart phones;resists;virtual environments;wii remote virtual 3d stereoscopic environment immersive three dimensional stereoscopic environment control education smartphone learning tool accelerometer;servers;three dimensional displays;stereo image processing;three dimensional displays servers stereo image processing smart phones virtual environments resists;virtual reality computer aided instruction control engineering computing control engineering education mobile computing smart phones	This paper presents an approach to develop an immersive virtual three-dimensional stereoscopic environment for control education. Thanks to the technological advances and the actual continuous reduction costs on electronic devices, it is possible to create immersive 3D virtual scenarios applying the side-by-side technique; and using standard gadgets like: smartphones, and head-mounted displays. Due to the smartphone built-in accelerometer, the user can easily and intuitively control the movements around the virtual world. Also, the Wii-remote® is used as an auxiliary input device to interact with the application. The proposed method permits to interact and visualize three-dimensional virtual scenarios without the need of specialized software or browser-plugins installed in the smartphone. By taking advantage of today's smartphone technological features, students feel highly motivated to use this device as an amusing learning tool. Since this application resembles three-dimensional computer games, it attracts students, increases their concentration, and keeps their attention during the learning process.	gesture recognition;head-mounted display;image resolution;input device;labview;pc game;plug-in (computing);real life;remote control;sensor;smartphone;stereoscopic video game;stereoscopy;virtual world;wii remote plus	Ernesto Granado;Julio Viola;Julio Zambrano;Flavio Quizhpi	2016	2016 IEEE Conference on Control Applications (CCA)	10.1109/CCA.2016.7587942	computer vision;simulation;engineering;instructional simulation;multimedia	Visualization	-45.2493916010223	-39.616931740923725	117615
91cbbe61ac39108a0b8d1afed961745232a89216	mobile interface for neuroprosthesis control aiming tetraplegic users	human computer interaction;smart phones;computational modeling;injuries;mobile communication;adaptation models;usability	This article proposes the development of a mobile interface for controlling a Neuroprosthesis, designed to restore grasp patterns, aiming tetraplegics users at C5 and C6 levels. Human Computer Interface paradigms and usability concepts guide its planning and development to garantee the quality of user's interaction with the system and thus, the sucess and controlability of the neuroprostheses. The number of screens and menus were optimized, thus the user may feel the interface as more intuitive, leading to fast learning and increasing the trust on it.	anatomical maturation;application program interface;description;heuristic (computer science);heuristics;human computer;human–computer interaction;interface device component;paper prototyping;population;prototype;smartphone;usability;neuroprosthesis	Renato G. Barelli;Plinio T. Aquino;Maria Cláudia Ferrari de Castro	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7591267	embedded system;simulation;mobile telephony;usability;human–computer interaction;computer science;engineering;computational model	Robotics	-42.87936471218357	-46.18538051766481	117795
041988a1025160141ea1d50741ef039ab481d17a	cyarm: an alternative aid device for blind persons	electric aid device;user interface;visual impairment;user interfaces;haptic sense;object detection;ultrasonic sensor	With the concept of 'human-machine interface', designed especially for visually impaired persons, we have developed an electric aid device for use in guiding orientation and locomotion. The device, which we call CyARM, measures the distance between a person and an object with an ultrasonic sensor and transmits the distance information to the user's haptic sense. In this report, we will: (1) outline the concept of CyARM, (2) describe its mechanism, and (3) demonstrate three preliminary experiments that verify the usability of CyARM. We conducted the experiments in terms of detection of objects, detection of space, and tracking object movement. As a result of these experiments, we have concluded that CyARM is potentially effective for visually impaired persons. Our study will encourage the related studies of user interfaces, particularly focusing on electric aid devices that guide visually impaired persons in detecting their environment.	experiment;sensor;usability;user interface	Kiyohide Ito;Makoto Okamoto;Junichi Akita;Tetsuo Ono;Ikuko Gyobu;Tomohito Takagi;Takahiro Hoshi;Yu Mishima	2005		10.1145/1056808.1056947	computer vision;simulation;human–computer interaction;computer science;operating system;user interface	HCI	-43.209503905079124	-43.92916705501189	117973
d5eed7dda018a3bcc4cc85940a4fa8571d5c6506	development of communication support application with blinks	fires complexity theory brightness face smart phones educational institutions iris;haar like eye ditection voca voice output communication aid phsically handicapped children opencv;voice output communication aid tool communication support application blink detection physically handicapped children limited body movements mental disorders smart phones eye area detection opencv image complexity image saturation eye chased program error reduction;voice communication gaze tracking handicapped aids object detection	In this study, we try to develop a new application for physically handicapped children to communicate with others by a blink. Because of limited body movements and mental disorders, many of them cannot communicate with their families or caregivers. We think if they can use applications of smart phones by a blink, it will be big help for them to tell caregivers what they really need or want to say. Fist, we try to detect an eye area by using OpenCv. Then we develop the way to detect opening and closing of eyes. We combine two methods, using saturation and using complexity of image, to get more accurate results of detecting a blink. The level of handicap is varied in children. So we are trying to develop the application to be able to customize depends on the situation of users. And also, we are trying to reduce the error in detecting a blink and pursue the high precision of the eye chased program.	blink;closing (morphology);opencv;sensor;smartphone	Ippei Torii;Shunki Takami;Kaoruko Ohtani;Naohiro Ishii	2014	IISA 2014, The 5th International Conference on Information, Intelligence, Systems and Applications	10.1109/IISA.2014.6878718	computer vision;speech recognition;computer science;communication	HCI	-36.95850478682844	-44.4372276357321	118152
b8f5de7122f885469a72590c93ddcd7c5b070f12	hand in hand: tools and techniques for understanding children's touch with a social robot		Robots that facilitate touch by children have special requirements in terms of safety and robustness, but little is known about how and when children actually use touch with robots. Tools and techniques are required to sense the variety of children's touch and to interpret the volumes of data generated. This explorative user study investigated children's patterns of touch during game play with a robot. We examined where the children touch the robot and their patterns of touch over time, using a raster-based visualisation of each child's time series of touches, recording patterns of touch across different games and children. We found that children readily engage with the robot, in particular spontaneously touching the robot's hands more than any other area. This user study and the tools developed may aid future designs of robots to autonomously detect when they have been touched.	raster graphics;requirement;social robot;time series;usability testing	Kristyn Hensby;Janet Wiles;Marie Bodén;Scott Heath;Mark Nielsen;Paul Pounds;Joshua Riddell;Kristopher Rogers;Nikodem Rybak;Virginia Slaughter;Michael Smith;Jonathon Taufatofua;Peter Worthy;Jason Weigel	2016	2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)		robot;games;computer vision;simulation;torso;computer science;artificial intelligence;prototype;multimedia;data visualization	Robotics	-45.528542207024344	-42.7086192217104	118288
a2dfe79fe5aa55866f78e8fd5df998e51c2b0147	focused web searching with pdas	personal digital assistant;real estate;handheld computer;real time;mobile computer;personal digital assistants;palm pilot;indexation;world wide web;web search;mobile computing;handheld computers	The Stanford Power Browser project addresses the problems of interacting with the World-Wide Web through wirelessly connected Personal Digital Assistants (PDAs). These problems include bandwidth limitations, screen real-estate shortage, battery capacity, and the time costs of pen-based search keyword input. As a way to address bandwidth and battery life limitations, we provide local site search facilities for all sites. We incrementally index Web sites in real time as the PDA user visits them. These indexes have narrow scope at first, and improve as the user dwells on the site, or as more users visit the site over time. We address the keyword input problem by providing site specific keyword completion, and indications of keyword selectivity within sites. The system is implemented on the Palm Pilot platform, using a Metricom radio link. We describe the user level experience, and then present the analyses that informed our technical decisions.	bandwidth (signal processing);interaction;personal digital assistant;selectivity (electronic);web search query;world wide web	Orkut Buyukkokten;Hector Garcia-Molina;Andreas Paepcke	2000	Computer Networks	10.1016/S1389-1286(00)00060-8	simulation;computer science;operating system;multimedia;mobile computing;world wide web;computer security;real estate	HCI	-33.86854975123936	-51.021967588289996	118295
119bcd29070369121ac246165d03ca7ae668f412	low cost 3d perception sensors	time of flight;social networking;enhanced museum experience;laser scanner;structure infrared;depth perception;laser range finder;stereo vision;edutainment;new genres of entertainment technology;off the shelf	Many sensors like the laser range finder, stereo vision cameras which help in building a depth perception of the world around it in 3D are very costly. Here I present the designs and prototypes of few 3D perception sensors which have been built low cost using components off the shelf. These perception sensors use structured infrared light projection. The design is miniature compared to other 3D sensors like LIDAR, Laser scanner and Time of flight cameras.	depth perception;image scanner;sensor;stereopsis	Sriranjan Rasakatla	2010		10.1145/1836845.1836912	laser scanning;computer vision;time of flight;simulation;depth perception;stereopsis;social network;computer graphics (images)	HCI	-40.07630242736704	-41.979876700375826	118299
07159620464c9c7ab0d2114d88c744fbacec2aeb	tap sensor: evaluating a new physical user interface for connected lighting	lighting control;connected lighting;user experience	The commercial introduction of connected lighting that can be integrated with sensors and other devices is opening up new possibilities for creating responsive and intelligent environments. However, the current approach for controlling such systems is to simply replace the light switch with a somewhat more sophisticated smartphone-based remote control. In this position paper we present a study of a new connected lighting control interface -- tap sensors. The tap sensor is a wireless device equipped with an accelerometer sensor that can detect a tap interaction and send a signal to the connected lighting system, for controlling light or other outputs. The tap sensors are small enough to be discretely attached to objects, such as a table, a picture frame, or a door, and it will then turn that object into an interactive control device. Therefore, by tapping on the object, the tap sensor that is attached to it will detect the tap input and an output can subsequently be triggered. These sensors were given to participants who lived with them in their own homes for a few weeks. The results of the study showed a great appreciation for this type of interaction from both practical and hedonic perspectives. Participants informed us that the tap sensors were effective, helpful, fun and playful ways of controlling their light.	intelligent environment;lighting control system;picture-in-picture;remote control;sensor;smartphone;table (database);user interface	Dzmitry Aliakseyeu;Jon Mason	2016		10.1145/2968219.2968522	embedded system;user experience design;real-time computing;human–computer interaction;computer hardware;computer science	HCI	-44.55938752815881	-41.80530616159887	118342
f67829ddb52d34bc3a65df1099b4f282d6df9cd0	applying models of visual search to menu design	minimal model;human computer interaction;visual search;optimal algorithm;simulation model	The Guided Search (GS) model, a quantitative model of visual search, was used to develop menu designs in a four-step process. First, a GS simulation model was defined for a menu search task. Second, model parameters were estimated to provide the best fit between model predictions and experimental data. Third, an optimization algorithm was used to identify the menu design that minimized model predicted search times based on predefined search frequencies of different menu items. Fourth, the design was tested. The results indicate that the GS model has the potential to be part of a system for predicting or automating the design of menus. # 2002 Elsevier Science Ltd.	algorithm;color;computer memory;confusion matrix;curve fitting;experiment;human–computer interaction;mathematical optimization;optimal design;roland gs;simulation;surround sound;top-down and bottom-up design	Baili Liu;Gregory Francis;Gavriel Salvendy	2002	Int. J. Hum.-Comput. Stud.	10.1006/ijhc.2002.0527	simulation;visual search;human–computer interaction;computer science;artificial intelligence;simulation modeling	Graphics	-47.31067962810683	-48.05957428842494	118370
9ec5ee952b5c66f0341c151e3b97ed08119068c8	automatic annotation of haptic exploratory procedures	virtual environments;spatial orientation	Active movement of the hand and fingers enables dynamic exploration of objects. Lederman and Klatzky [1987] have proposed a set of stereotyped movement patterns called Exploratory Procedures (EPs) that are linked to the acquisition of knowledge concerning specific object properties. For example, lateral motion is the optimal EP for acquiring roughness information and compliance is best estimated by employing pressure.	expectation propagation;exploratory testing;haptic technology;lateral thinking	Sander E. M. Jansen;Wouter M. Bergmann Tiest;Astrid M. L. Kappers	2013		10.1145/2492494.2501877	computer vision;simulation;computer science;communication	HCI	-44.83126447057419	-49.689849580692474	118403
46e634d943e149e839801d18cf0c1aa6d5b417e9	touch-shake: design and implementation of a physical contact support device for face-to-face communication	conferences consumer electronics;touch shake capacitive sensor physical attributes lighting pattern output sounds speaker led face to face communication physical contact support device;face to face communication physical contact;light emitting diodes;tactile sensors audio equipment capacitive sensors light emitting diodes;tactile sensors;audio equipment;capacitive sensors	In this study we developed the Touch-Shake, which supports physical contact for face-to-face communication. The Touch-Shake has a LED and a speaker, and is shaped like a baton. When each user holds a Touch-Shake and one user touches the body of the other, the LEDs of the Touch-Shakes turn on and the Touch-Shakes output sounds. Additionally, the output sounds and lighting pattern of LEDs vary based on how the user touches the other body, the users' body condition, and the users' physical attributes, which are measured using a capacitive sensor built into the Touch-Shake. The results of an evaluative experiment revealed that the subjects enjoyed physical contact with the Touch-Shake, and they were able to experience rich face-to-face communication.	baton;capacitive sensing;touch id	Yohei Yamaguchi;Hidekatsu Yanagi;Yoshinari Takegawa	2013	2013 IEEE 2nd Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2013.6664789	embedded system;electronic engineering;engineering;communication	HCI	-46.56526877613275	-40.45447911337127	118452
f7700ecbf5f669095bf9a10440d71339158e8036	multifaceted interaction with a virtual engineering environment using a scenegraph-oriented approach	3d interaction;user interface;virtual reality;virtual environment;interaction technique	To provide an adequate user interface to the large amount of geometrical and alphanumerical date coupled to the geometries, the 3D interaction in the virtual environment is combined with 2D interaction techniques using a portable touchscreen computer. Therefore, the consequences are analyzed, when the scenegraph structure of the underlying VR system is applied to the design of the 2D interface to the engineering data. This paper describes methods of using a scenegraph-oriented approach for structured interaction and information display in a virtual engineering application.	3d interaction;display device;interaction technique;scene graph;tablet computer;touchscreen;user interface;virtual engineering;virtual reality	Arnold Müller;Stefan Conrad;Ernst Kruijff	2003			simulation;interactive systems engineering;human–computer interaction;computer science;virtual machine;operating system;virtual reality;multimedia;user interface;interaction technique	HCI	-44.53310333390833	-38.438794312492064	118567
36025a6d26d8775db836e1e4182d62993dcf18c9	gesture interface magnifiers for low-vision users	gesture;low vision video magnifier	This study compared different types of magnification and navigation methods on low-vision handheld magnifiers to determine the feasibility of a touch screen gesture interface. The results show that despite the fact that participants had no experience using gestures for magnification or navigation, participants were more satisfied with them. Gestures were faster and more preferred than the indirect input methods for pushing a button or rotating a knob, which had previously been familiar to participants from other electronic device interfaces. The study suggests that the use of gestures may afford an alternative and more natural magnification and navigation method for a new user-centric low vision magnifier.	control knob;gesture recognition;handheld game console;input method;touchscreen	Seunghyun Tina Lee;Jon A. Sanford	2012		10.1145/2384916.2384993	computer vision;computer science;gesture recognition;multimedia;gesture	HCI	-46.17867817819927	-44.026380855906844	118571
0658a9dbd418f92b2e1a1801755c5bb4cc498f60	exploration and discovery of user-generated content in large information spaces	human computer interaction;analysis and design;information space;data mining;data analysis;photo collection;user interface design;user generated content;social media	The accumulation of large collections of social media data poses new challenges for the design of exploratory experiences, such as when a user browses through a collection to discover content (e.g. exploring photo collections, network of friends, etc). Cardinality and characteristics of the set, together with volatility of the information, resulting from fast and continuous creation, deletion and updating of entries, trigger novel research questions. In this context, we plan to investigate and contribute to the data analysis, and user interface design of exploratory experiences. The proposed approach is an iterative process where analysis and design phases are performed in cycles. The long-term vision is to understand the underlying reasoning in order to be able to automatically replicate it.	iterative method;self-replicating machine;social media;tree accumulation;user interface design;user-generated content;volatility	Luca Chiarandini	2012		10.1145/2124295.2124386	user interface design;user experience design;social media;computer science;data science;data mining;data analysis;user interface;user-generated content;world wide web;information retrieval	HCI	-34.30230897345712	-49.568902647139815	118643
38ff692d9960bc9c346f0ef426d28d4eb665df77	robust multimodal audio–visual processing for advanced context awareness in smart spaces	context aware application;context aware;context model;context aware service;smart spaces;audio visual;visual processing	Identifying people and tracking their locations is a key prerequisite to achieving context awareness in smart spaces. Moreover, in realistic context-aware applications, these tasks have to be carried out in a non-obtrusive fashion. In this paper we present a set of robust person-identification and tracking algorithms, based on audio and visual processing. A main characteristic of these algorithms is that they operate on far-field and un-constrained audio–visual streams, which ensure that they are non-intrusive. We also illustrate that the combination of their outputs can lead to composite multimodal tracking components, which are suitable for supporting a broad range of context-aware services. In combining audio–visual processing results, we exploit a context-modeling approach based on a graph of situations. Accordingly, we discuss the implementation of realistic prototype applications that make use of the full range of audio, visual and multimodal algorithms.	algorithm;context awareness;context-aware network;jog dial;middleware;multimodal interaction;prototype;scalability;sensor;smart tv;ubiquitous computing;user experience	Aristodemos Pnevmatikakis;John Soldatos;Fotios Talantzis;Lazaros Polymenakos	2007	Personal and Ubiquitous Computing	10.1007/s00779-007-0169-9	computer vision;real-time computing;computer science;multimedia;context model	HCI	-47.75754129907358	-40.650583811394306	118907
ed3b8877c457cc330e38b02c18227af2812be78d	voices of vlogging	nonverbal behavior;vlogging;youtube;size effect;verbal behavior	Vlogs have rapidly evolved from the ’chat from your bedroom’ format to a highly creative form of expression and communication. However, despite the high popularity of vlogging, automatic analysis of conversational vlogs have not been attempted in the literature. In this paper, we present a novel analysis of conversational vlogs based on the characterization of vloggers’ nonverbal behavior. We investigate the use of four nonverbal cues extracted automatically from the audio channel to measure the behavior of vloggers and explore the relation to their degree of popularity and that of their videos. Our study is validated on over 2200 videos and 150 hours of data, and shows that one nonverbal cue (speaking time) is correlated with levels of popularity with a medium size effect.	video blog	Joan-Isaac Biel;Daniel Gatica-Perez	2010			multimedia;social psychology	HCI	-33.75876927990662	-47.87345983443558	119069
4f5d24eb93a6d8ff5f6d76ae7717bc08614ef72e	when two hands are better than one: enhancing collaboration using single display groupware	pilot study;input device;kidpad;pad;children;cscw;input devices;single display groupware;educational application	In this paper, we describe Single Display Groupware, a software model that enables multiple users to work simultaneously at a single computer display. We discuss the collaborative benefits observed during a pilot study of the SDG application, KidPad.	collaborative software;computer monitor;multi-user	Jason Stewart;Elaine M. Raybourn;Benjamin B. Bederson;Allison Druin	1998		10.1145/286498.286766	simulation;human–computer interaction;computer science;operating system;multimedia;input device	HCI	-46.65776366697529	-38.673747387397185	119093
7da7ebaacc5f63210236f652f99065ce4f44c26d	issues and techniques in touch-sensitive tablet input	input device;human computer interaction;touch sensitive input devices	Touch-sensitive tablets and their use in human-computer interaction are discussed. It is shown that such devices have some important properties that differentiate them from other input devices (such as mice and joysticks). The analysis serves two purposes: (1) it sheds light on touch tablets, and (2) it demonstrates how other devices might be approached. Three specific distinctions between touch tablets and one button mice are drawn. These concern the signaling of events, multiple point sensing and the use of templates. These distinctions are reinforced, and possible uses of touch tablets are illustrated, in an example application. Potential enhancements to touch tablets and other input devices are discussed, as are some inherent problems. The paper concludes with recommendations for future work.	computer mouse;human–computer interaction;input device;joystick;tablet computer;touchscreen	William Buxton;Ralph Hill;Peter Rowley	1985		10.1145/325334.325239	simulation;computer science;operating system;input device	HCI	-47.2904238129292	-43.76267576420952	119125
c5e46ed0293e01183be9defc1183dec741ecd2de	effects of full/partial haptic guidance on handwriting skills development		It has been shown in previous studies that haptic guidance improves the learning outcomes of handwriting motor skills. In this paper, we present a comparison between full guidance and partial guidance using a haptic learning tool which supports these two modes. The full guidance mode leads the user along a pre-recorded trajectory, whereas the partial guidance mode allows the user a free movement and provides corrective forces if they deviate significantly from the desired path. Experimental results with 22 participants demonstrated that there is no significant difference between partial haptic guidance and full haptic guidance for improving learning outcomes, while both have significantly improved the learner's performance compared to no haptics feedback. However, when the two modes are combined, partial guidance followed by full guidance yielded better overall performance. We conclude the paper by summarizing our findings and providing perspectives for future work.	experiment;haptic technology;human factors and ergonomics;stylus (computing)	Akiko Teranishi;Timothy Mulumba;Georgios Karafotias;Jihad Mohamad Jaam;Mohamad A. Eid	2017	2017 IEEE World Haptics Conference (WHC)	10.1109/WHC.2017.7989886	computer science;computer vision;artificial intelligence;software;haptic technology;visualization;simulation;handwriting	HCI	-46.04499478177876	-48.00397776163979	119183
f7e5d3689aef9922dafffe648678d25bf32ae4c2	write-n-speak: authoring multimodal digital-paper materials for speech-language therapy	object recognition;older adult;speech language therapy;older adults;multimodal interaction;pen based computing;communication	Aphasia is characterized by a reduced ability to understand and/or generate speech and language. Speech-language therapy helps individuals with aphasia regain language and cope with changes in their communication abilities. The therapy process is largely paper-based, making multimodal digital pen technology a promising tool for supporting therapy activities. We report on ten months of field research where we examine the practice of speech-language therapy, implement Write-N-Speak, a digital-paper toolkit for end-user creation of custom therapy materials, and deploy this system for 12 weeks with one therapist-client dyad in a clinical setting. The therapist used Write-N-Speak to create a range of materials including custom interactive worksheets, photographs programmed with the client’s voice, and interactive stickers on household items to aid object recognition and naming. We conclude with a discussion of multimodal digital pen technology for this and other therapy activities.	digital paper;digital pen;field research;multimodal interaction;outline of object recognition	Anne Marie Piper;Nadir Weibel;James D. Hollan	2011	TACCESS	10.1145/2039339.2039341	speech recognition;computer science;cognitive neuroscience of visual object recognition;multimodal interaction;multimedia	HCI	-48.090371388051295	-42.88732733222471	119188
4ce1802f8f9e2ebd60cea34171695bed49c9ed2d	modeling user quality of experience of olfaction-enhanced multimedia		Research on modelling user quality of experience (QoE) to date has primarily focused the combination of traditional media components; audio and video, or the individual influence of each. However, multisensory experiences have recently gained significant traction in the research community as a novel method to enhance QoE beyond what is possible with traditional media. This paper presents a model developed based on empirical data. It estimates user QoE of olfaction-enhanced multimedia. A set of 12 olfaction enhanced video clips were viewed by 84 assessors. A strong age and gender balance produced 6048 user ratings across six questions. Employing this dataset, the proposed model considers the influence of: system factors, user factors and content factors on user perceived QoE. The model is instantiated and validated. The analysis indicates that: content factors have a 10% influence on user QoE; age factors have an 11% influence; and gender factors have an 8% influence on user QoE. Also, content factors had the highest number of statistically significant influences across all of the factors evaluated. These results suggest that human, and content in addition to system factors play a key role in perceptual multimedia quality of olfaction enhanced multimedia. Further work is required to understand the remaining factors as well as the relationship between the media components has on QoE.	digital video;experience;recommender system;traction teampage;user (computing);user profile;utility;video clip	Niall Murray;Gabriel-Miro Muntean;Yuansong Qiao;Sean Brennan;Brian Lee	2018	IEEE Transactions on Broadcasting	10.1109/TBC.2018.2825297	quality of experience;perception;quality of service;multimedia;computer science	Visualization	-34.01777320285662	-47.53157593925267	119251
74bf06dceb2ea7b501acd436fcba5afe3faf081a	a technique for precise depth representation in stereoscopic display	virtual reality depth representation stereoscopic display 3d virtual object three dimensional virtual object large sized screen viewing position screen position stereoscopic calculation experimental results;stereo image processing virtual reality user interfaces three dimensional displays;virtual reality;depth perception;three dimensional displays;stereo image processing;pupil distance;stereoscopic display;user interfaces	In observing a 3D virtual object displayed stereoscopically on a large-sized screen, there often exists a difference between the calculated depth and the perceived one of the object. This paper presents a method for reducing such differences of depth. This is performed by modification of both the viewing position and the screen position in stereoscopic calculation. The optimal amount of modification was decided using sample values of depth differences. Effectiveness of the proposed method is discussed with the experimental results.	stereoscopy	Shunsuke Yoshida;Shinya Miyazaki;Toshihito Hoshino;Toru Ozeki;Jun-ichi Hasegawa;Takami Yasuda;Shigeki Yokoi	1999		10.1109/CGI.1999.777919	computer vision;depth perception;computer science;artificial intelligence;operating system;virtual reality;multimedia;user interface;computer graphics (images)	HCI	-44.186709746439945	-47.203828058542435	119272
022eb7e90eda17461dab81dacffc5d4b9b99476d	intelligent objects to facilitate human participation in virtual institutions	sensor phenomena and characterization;human computer interaction;sensors;user participation;3d virtual world;virtual reality;virtual environments;immersive environment;intelligent objects;normative multiagent systems;data visualisation;humans intelligent agent visualization artificial intelligence avatars mathematics software agents multiagent systems mechanical factors middleware;visualization;multi agent systems;general solution;three dimensional displays;avatars;middleware;humans;virtual environment;middleware infrastructure intelligent objects human participation virtual institutions electronic institutions 3d virtual worlds iobjects visualization property;intelligent objects virtual environments normative multiagent systems virtual institutions;electronic institution;virtual institutions;virtual reality data visualisation human computer interaction middleware multi agent systems;virtual worlds	Our research combines electronic institutions and 3D virtual worlds for the construction of virtual institutions which are virtual worlds with normative regulation of interactions. That is, a virtual world where participants actions have to comply with predefined institutional rules. In this context, the actions a participant may perform depend on the institutional rules and the current execution state. We propose to include iObjects, intelligent objects, as entities having both visualization properties and decision mechanisms in the virtual institution. They are a new key element to improve users participation in virtual institutions. We situate them in a middleware infrastructure in order to be independent of 3D virtual world platform and to provide a general solution in which participants could be connected from different immersive environment platforms.	entity;immersion (virtual reality);interaction;middleware;situated cognition;virtual world	Inmaculada Rodríguez;Anna Puig;Marc Esteva;Carles Sierra;Anton Bogdanovych;Simeon J. Simoff	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.320	simulation;visualization;human–computer interaction;computer science;sensor;virtual machine;artificial intelligence;instructional simulation;multi-agent system;middleware;virtual reality;multimedia	Visualization	-34.53728673673253	-38.53445117396445	119323
f0254d2e344b8d84efd86c6a808307e19d055853	small inspection vehicles for non-destructive testing applications	electrical engineering electronics nuclear engineering	One of the fundamental questions in service robotics is how we can get those machines to work efficiently in the same environment with and under the command of the average user. The answer – not an easy one – is in the development of human-robot interfaces (HRI), which should be able to transmit the will of the user to the robot in a simple but effective way preferably using means that are natural to humans. The paper introduces a generic interface concept based on exchanging cognitive information between the robot and the user, both present in the same environment. The information is exchanged through a virtual world called “common situation awareness” or “common presence” of both entities. The virtual world is a simple map augmented by database of semantic information. Functions of the HRI are based on utilizing interactively the senses of both the human and machine entities, the human when perceiving the environment and commanding the machine and the machine when looking for working targets or moving in the environment. The syntax of the communication language utilizes the objects and work targets currently existing in the common presence. A multitasking humanoid robot, called WorkPartner, is used to demonstrate the interface principles. The interface is designed to support interaction, including teaching and learning, with the human user in various outdoors work tasks. Natural to human communication methods, such as speech and gestures, are promoted in order to ease the user‟s load. Methods and hardware presented have been designed for the WorkPartner robot, but many of them are generic in nature and not dependent upon the robot physics.		Markus Friedrich;L. Gatzoulis;Gordon Hayward;Walter Galbraith	2005		10.1007/3-540-26415-9_111	simulation;human–computer interaction;engineering;communication	HCI	-42.6674959605032	-43.19773227901137	119534
6b27ed12e5fe8b6a9d7033b0594cef6f7c7b6532	implementation of the stroop task using an interactive table: an experimental study	tangible interactions;interactive table;rfid;stroop;tangisense	"""We have implemented a psychological test, known as the Stroop task (in its reversed version), using the interactive TangiSense table that is equipped with RFID technology allowing the use of tangible objects. How the test was implemented, as well as the experimental study, are described in this paper. As a general description, participants moved a tangible object into one of four colored zones. The correct colored zone was indicated by a word for a color displayed in congruent or incongruent colored LED lights (e.g., """"yellow"""" displayed in yellow or red, respectively). Participants took more time to place the object in a zone and made more errors when the word was lit with an incongruent than congruent color. These results suggest that the TangiSense table has applicable value for psychological research."""	color;experiment;radio-frequency identification	Holly C. Miller;Sébastien Kubicki;Christophe Kolski;Janick Naveteur	2013		10.1145/2468356.2468402	radio-frequency identification;stroop effect;multimedia;computer graphics (images)	HCI	-44.973135853864136	-45.226895914698375	119557
042931ee34d4bc872167b977d8f5c147ae8e30b1	pressure or movement?		Despite considerable prior work exploring foot-based interaction techniques, direct comparisons of the performance of these approaches have been lacking. Here, we compare the performance of the two most common approaches found in previous studies: rocking (applying pressure to different parts of the foot) versus rotating and sliding, considering the use case of a hands-free interface intended for seated musicians. Participants performed a number of representative operations, such as setting the tempo of a metronome, using the two strategies. Results indicate superiority of the rotating and sliding approach, both in completion time and responses to NASA TLX questionnaires, although rocking was preferred by some participants due to its ergonomics and subtle movements required for parameter-controlling tasks. Beyond the comparison itself, the decisions we faced related to menu design and feedback for our use case may offer helpful insight for the design of future foot-based interfaces.	human factors and ergonomics;interaction technique;pedobarography	Taeyong Kim;Hao Ju;Jeremy R. Cooperstock	2018		10.1145/3196709.3196759		HCI	-46.247190663446474	-47.65403219518068	119675
3e63cc52832039f6d5b004687b2ffbfdfc62a865	fingerreader: a wearable device to explore printed text on the go	wearable interface;text reading;assistive technology	Accessing printed text in a mobile context is a major challenge for the blind. A preliminary study with blind people reveals numerous difficulties with existing state-of-the-art technologies including problems with alignment, focus, accuracy, mobility and efficiency. In this paper, we present a finger-worn device, FingerReader, that assists blind users with reading printed text on the go. We introduce a novel computer vision algorithm for local-sequential text scanning that enables reading single lines, blocks of text or skimming the text with complementary, multimodal feedback. This system is implemented in a small finger-worn form factor, that enables a more manageable eyes-free operation with trivial setup. We offer findings from three studies performed to determine the usability of the FingerReader.	algorithm;computer vision;multimodal interaction;printing;usb on-the-go;usability	Roy Shilkrot;Jochen Huber;Wong Meng Ee;Pattie Maes;Suranga Nanayakkara	2015		10.1145/2702123.2702421	speech recognition;human–computer interaction;computer science;multimedia;world wide web	HCI	-46.973901970807475	-43.44427887376909	119739
9c6cc4cb8f5cc8a084e1bdf04836218f0c1bef3c	protohole: prototyping interactive 3d printed objects using holes and acoustic sensing		A prototype process of a physical user interface includes not only connection of electronic parts and an enclosure design, but also the arrangement and configuration of the electronic parts in the enclosure. This process is complicated and difficult for people who do not have modeling skills. We propose ProtoHole, a system for prototyping interactive 3D printed objects using holes, internal cavity and swept-frequency acoustic sensing. By emitting a high-frequency sweep signal inside the object, our system enables to classify changes in resonance properties when closing holes using a machine learning technique. Therefore, an object can be easily made interactive without considering an arrangement of internal electronic parts and wiring. We show examples of prototypes created with our system.	3d printing;acoustic cryptanalysis;closing (morphology);machine learning;prototype;radio-frequency sweep;resonance;user interface;wiring	Shohei Katakura;Keita Watanabe	2018		10.1145/3170427.3188471	human–computer interaction;computer hardware;3d printing;resonance;enclosure;chirp;computer science;rapid prototyping;user interface	HCI	-42.49455334901215	-41.383802116219925	119758
e0e6634f1b5b5583fa7328aee52b3c566048354e	natcut: an interactive tangible editor for physical object fabrication	enclosure;personal fabrication;tangible interaction	While physical prototyping and personal fabrication is currently getting increasingly popular, many of the tools used to design 3D objects are still complex and cumbersome to use. In this paper, we address this issue and present a novel tabletop-based tangible editor, called NatCut, that allows the quick and easy design of physical enclosures for interactive prototypes. To generate an enclosure with NatCut, the user first chooses a basic geometric shape for it on the tabletop surface. By simply placing electronic components on the displayed 2D layout for the enclosure, respective cut-outs and holes are generated. Further, a number of user interactions on the tabletop screen are supported to modify, personalize, and enrich the casing. The resulting 2D layout contains all joints needed to assemble the parts after laser cutting. We discuss the results of a user study in which we tested the approach.	electronic component;interaction;personalization;usability testing	Stefan Schneegaß;Alireza Sahami Shirazi;Tanja Döring;David Schmid;Albrecht Schmidt	2014		10.1145/2559206.2581189	enclosure;human–computer interaction;multimedia;world wide web	HCI	-44.064092166665425	-39.5265814823776	119782
588be58fdcd2eda364378e33838ba3af0317e0c7	body-tracking camera control for demonstration videos	diy;motion tracking;kinect;capturing tools;how to;camera;videos	A large community of users creates and shares how-to videos online. Many of these videos show demonstrations of physical tasks, such as fixing a machine, assembling furniture, or demonstrating dance steps. It is often difficult for the authors of these videos to control camera focus, view, and position while performing their tasks. To help authors produce videos, we introduce Kinectograph, a recording device that automatically pans and tilts to follow specific body parts, e.g., hands, of a user in a video. It utilizes a Kinect depth sensor to track skeletal data and adjusts the camera angle via a 2D pan-tilt gimbal mount. Users control and configure Kinectograph through a tablet application with real-time video preview. An informal user study suggests that users prefer to record and share videos with Kinectograph, as it enables authors to focus on performing their demonstration tasks.	kinect;real-time transcription;structured-light 3d scanner;tablet computer;usability testing;video clip	Derrick Cheng;Pei-Yu Chi;T. C. Kwak;Björn Hartmann;Paul K. Wright	2013		10.1145/2468356.2468568	computer vision;simulation;computer science;multimedia;natural user interface;world wide web	HCI	-39.65347940283321	-39.43651097236387	119809
ee34dae79607b211d292a1918e093ad73660f244	setting the table for the blind	gestural interaction;nonvisual communication;haptic device;information access;haptics;visual impairment;visually impaired;tables	We report on research concerning the rendering of tables for blind individuals with an emphasis on exploring the potential of a new planar haptic device in combination with sound.	haptic technology	Mechmet Chiousemoglou;Helmut Jürgensen	2011		10.1145/2141622.2141624	computer vision;computer science;artificial intelligence;multimedia;haptic technology	HCI	-46.362575279422735	-41.98707409607052	119818
9f792b60ffe9f0db315d81e876c56655a419e5c2	benchmarking news recommendations: the clef newsreel use case	interactive information retrieval;personal information management;user studies;user interfaces	"""The CLEF NewsREEL challenge is a campaign-style evaluation lab allowing participants to evaluate and optimize news recommender algorithms. The goal is to create an algorithm that is able to generate news items that users would click, respecting a strict time constraint. The lab challenges participants to compete in either a """"living lab"""" (Task 1) or perform an evaluation that replays recorded streams (Task 2). In this report, we discuss the objectives and challenges of the NewsREEL lab, summarize last year's campaign and outline the main research challenges that can be addressed by participating in NewsREEL 2016."""	algorithm;living lab;recommender system	Frank Hopfgartner;Torben Brodt;Jonas Seiler;Benjamin Kille;Andreas Lommatzsch;Martha Larson;Roberto Turrin;András Serény	2015	SIGIR Forum	10.1145/2888422.2888443	computer science;personal information management;data mining;multimedia;user interface;world wide web;information retrieval	HCI	-34.18392985521164	-51.6652271394398	119830
c15a4f8cba3a4d9c6b4de350e57872be457b86be	an interactive image clipping system using hand motion recognition	kinect;image clipping;hand recognition;region recognition	We present an efficient hand recognition algorithm for an interactive image clipping system, which is widely used for environments such as public facilities and security environments where personal capturing devices including mobile phones are not allowed. User-friendly interface and accurate image capturing function are required for an image clipping system. We build the system by combining Microsoft Kinect, HD webcam and projector. The Kinect and webcam are used to capture the motions of users' hand and project is to display the user-selected area from the capturing material. Hand recognition is composed of three steps: (i) the region occupied by users' hand is extracted from an image, (ii) the fingertips of the extracted hand region are analyzed using k-curvature algorithm, and (iii) the height of the fingertip is estimated using the depth image from Kinect. The height of the fingertip informs whether users' finger touched the surface of the target. The region captured by the fingertip is clipped from the image and stored as the target image. The excellence of our hand recognition algorithm is proved through a user test.		Kowoon Lee;Kyungha Min	2015	Inf. Syst.	10.1016/j.is.2014.05.011	computer vision;simulation;computer science;clipping;natural user interface;computer graphics (images)	Vision	-41.830553437849105	-40.41579466564241	119992
a4e5fd0d86ef40858d640e8257fc27acb0885247	a qr code-based on-street parking fee payment mechanism	e bill;traffic engineering computing internet mobile computing mobile handsets qr codes road pricing tolls;parking payment methods qr code based on street parking fee payment mechanism mobile phone users smartphone camera reader application contact information web hyperlink remote server fee collectors paper consumption reduction e bill payment taiwan;qr code;licenses navigation servers mobile handsets mobile communication vehicles encoding;web services;license plate recognition;web services qr code e bill histogram equalization license plate recognition;histogram equalization	Quick response (QR) code is a convenient product for mobile phone users. People can use a smartphone camera to capture the code, and then decode it through a dedicated reader application. Specifically, that code stands for concise text, contact information, or a web hyperlink. Its existence assists phone users in keypad typing more easily. This paper proposes an on-street parking fee payment mechanism based on the QR code of an E-bill. People can regard the code as a bill to pay their parking fee, where the parking information is recorded into a remote server by the fee collectors. The main idea of this mechanism is to save on resources such as reducing paper consumption. Simulation results showed that the proposed mobile application provides a new mode for E-bill payment for on-street parking in Taiwan. Furthermore, the said application also serves as an exemplary model for other parking payment methods.	hyperlink;mobile app;mobile phone;qr code;server (computing);simulation;smartphone	Wen Chuan Wu	2014	2014 Tenth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2014.33	web service;computer vision;computer science;operating system;parking guidance and information;world wide web;computer security;histogram equalization;mobile payment	Mobile	-39.883755637119336	-40.02511358212016	120140
4101fef5ff72dd90691281ed266332f2452e1b76	touchviz: (multi)touching multivariate data	user touch;multitouch interaction;actionable finding;physical model;finger gesture;user interaction;multivariate data;force-based tool;real-world force;actual physical object	In this paper we describe TouchViz, an information visualization system for tablets that encourages rich interaction, exploration, and play through references to physical models. TouchViz turns data into physical objects that experience forces and respond to the user. We describe the design of the system.	information visualization;speedrun	Jeffrey M. Rzeszotarski;Aniket Kittur	2013		10.1145/2468356.2479625	information visualization;visualization;exploration;human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-45.25043669737885	-40.01120908763397	120159
9c060d7d57e9699d81dd82afadb5309c791e84c6	assessing visual search performance in ocular compared to cerebral visual impairment using a virtual reality simulation of human dynamic movement		Virtual reality (VR) can provide robust assessment of cognitive spatial processing skills in individuals with visual impairment. VR combined with objective measures of behavioral performance (i.e., eye tracking) affords a high degree of experimental control, task flexibility, participant engagement, and enhanced data capture. Individuals with visual impairment typically have difficulties identifying people in crowded environments; these difficulties may differ depending on the origin of the visual impairment. Specifically, individuals with cortical/cerebral visual impairment (CVI) may show a greater sensitivity to scenarios of high dynamic visual complexity compared to those with ocular based visual impairment (OVI). To test potential differences in visual search performance, we have developed a first-person perspective VR environment integrated with eye tracking designed to simulate the dynamic movement of humans in a hallway. Participants were tasked with locating a specific target individual walking among a crowd of people moving in various directions in the hallway. To assess the effect of task difficulty, we manipulated factors of crowd density and presence of object clutter within the hallway. Results to date show that both visually impaired groups demonstrate increased variability in search patterns and reaction times as compared to controls. Additionally, performance of the CVI group fluctuates greater as a function of task difficulty. Findings from the current work demonstrate a successful interaction between individuals with visual impairments and VR simulations in assessing high level visual function. Further studies will serve as a theoretical foundation for the creation of new assessment and training paradigms for visually impaired individuals.	clutter;eye tracking;first-person (video games);high-level programming language;labwindows/cvi;ovi (nokia);simulation;spatial variability;virtual reality;visual basic[.net]	Christopher R. Bennett;Emma S. Bailin;Timothy K. Gottlieb;Corinna M. Bauer;Peter J. Bex;Lotfi B. Merabet	2018		10.1145/3183654.3183674	visual search;cerebral visual impairment;cognitive psychology;visual impairment;eye tracking;virtual reality;cognition;automatic identification and data capture;psychology	HCI	-48.02881109886775	-49.35186602589099	120176
87ada87cd75e97b86ac2892b7d0e4964f5d7f32c	assessing real world imagery in virtual environments for people with cognitive disabilities	image processing augmented reality cognition handicapped aids;training in virtual reality;research outputs;research publications;navigation;navigation training google visualization mobile communication databases augmented reality;navigation cognitive disability augmented reality training in virtual reality;static videos real world imagery assessment virtual environments people with cognitive disability augmented reality virtual reality real time guidance feedback people with down syndrome spatial perception visual perception visual feedback virtual intelligent system;augmented reality;cognitive disability	People with cognitive disabilities are often socially excluded. We propose a system based on Virtual and Augmented Reality that has the potential to act as an educational and support tool in everyday tasks for people with cognitive disabilities. Our solution consists of two components: the first that enables users to train for several essential quotidian activities and the second that is meant to offer real time guidance feedback for immediate support. In order to illustrate the functionality of our proposed system, we chose to train and support navigation skills. Thus, we conducted a preliminary study on people with Down Syndrome (DS) based on a navigation task. Our experiment was aimed at evaluating the visual and spatial perception of people with DS when interacting with different elements of our system. We provide a preliminary evaluation that illustrates how people with DS perceive different landmarks and types of visual feedback, in static images and videos. Although we focused our study on people with DS, people with different cognitive disabilities could also benefit from the features of our solution. This analysis is mandatory in the design of a virtual intelligent system with several functionalities that aims at helping disabled people in developing basic knowledge in every day tasks.	artificial intelligence;augmented reality;interaction	Alexandra Covaci;Dean Kramer;Juan Carlos Augusto;Silvia Rus;Andreas Braun	2015	2015 International Conference on Intelligent Environments	10.1109/IE.2015.14	computer vision;computer-mediated reality;simulation;engineering;multimedia	HCI	-43.06139388205363	-45.35627648984777	120276
70b8be9fa370f750518437e70edbae81e0e43205	compensating for distance compression in audiovisual virtual environments using incongruence	binaural audio;distance perception;incongruent display;virtual environment;spatial audio;head mounted display	A key requirement for a sense of presence in Virtual Environments (VEs) is for a user to perceive space as naturally as possible. One critical aspect is distance perception. When judging distances, compression is a phenomenon where humans tend to underestimate the distance between themselves and target objects (termed egocentric or absolute compression), and between other objects (exocentric or relative compression). Results of studies in virtual worlds rendered through head mounted displays are striking, demonstrating significant distance compression error. Distance compression is a multisensory phenomenon, where both audio and visual stimuli are often compressed with respect to their distances from the observer. In this paper, we propose and test a method for reducing crossmodal distance compression in VEs. We report an empirical evaluation of our method via a study of 3D spatial perception within a virtual reality (VR) head mounted display. Applying our method resulted in more accurate distance perception in a VE at longer range, and suggests a modification that could adaptively compensate for distance compression at both shorter and longer ranges. Our results have a significant and intriguing implication for designers of VEs: an incongruent audiovisual display, i.e. where the audio and visual information is intentionally misaligned, may lead to better spatial perception of a virtual scene.	anomalous experiences;head-mounted display;virtual reality;virtual world	Daniel J. Finnegan;Eamonn O'Neill;Michael J. Proulx	2016		10.1145/2858036.2858065	computer vision;simulation;binaural recording;computer science;virtual machine;optical head-mounted display;operating system	HCI	-44.31698770427899	-48.87981436791128	120427
47281c8f5f6baa3352cd45602be0673e86e87162	developing steady clicks: : a method of cursor assistance for people with motor impairments	mouse;task performance;pointing and selection tasks;empirical study;t technology general;clicking errors;disability;qa75 electronic computers computer science;user input;clicking;target acquisition;ta engineering general civil engineering general	Slipping while clicking and accidental clicks are a source of errors for mouse users with motor impairments. The Steady Clicks assistance feature suppresses these errors by freezing the cursor during mouse clicks, preventing overlapping button presses and suppressing clicks made while the mouse is moving at a high velocity. Evaluation with eleven target users found that Steady Clicks enabled participants to select targets using significantly fewer attempts. Overall task performance times were significantly improved for the five participants with the highest slip rates. Blocking of overlapping and high velocity clicks also shows promise as an error filter. Nine participants preferred Steady Clicks to the unassisted condition. If used in conjunction with existing techniques for cursor positioning, all of the major sources of clicking errors observed in empirical studies would be addressed, enabling faster and more effective mouse use for those who currently struggle with the standard mouse.	cursor (databases);error-tolerant design;slip (programming language);velocity (software development)	Shari Trewin;Simeon Keates;Karyn Moffatt	2006		10.1145/1168987.1168993	simulation;speech recognition;empirical research	HCI	-46.68542207105543	-45.87832753432296	120647
7d9cfd92f5a5882d4cef597dca8007f6b00ee785	pointing and speech: comparison of various voice commands	pointing;voice commands;large wall display;multimodal interaction;touchless interaction	This poster presents an experiment that was conducted to investigate pointing combined with voice commands. Three different voice commands are compared with each other during a simple task where subjects had to drag & drop, rotate and resize objects. It turned out that shorter voice commands were more preferred by the subjects than voice commands that were equivalent to human-human communication.	drag and drop;speech recognition	Monika Elepfandt	2012		10.1145/2399016.2399158	speech recognition;computer science;voice tag;voice command device;multimodal interaction	HCI	-45.58709785201595	-44.430040889705346	120666
4e1545cf38eec4560f60ca6333fe6dc978a7a067	photochat: communication support system based on sharing photos and notes	communication support system;real time;digital camera;collaborative photo annotation;support system;interaction pattern;experience sharing;chat on photos;experimental evaluation	This paper proposes PhotoChat, a system that facilitates communication among users who want to share experiences by enabling them to share photos and notes. PhotoChat is designed to be used as a digital camera and to run on mobile PCs with a camera module. PhotoChat users can comment on the shared photos with a pen interface. The data, i.e., photos and comments, are distributed among PhotoChat users in real time to enable them to learn others' interests and to chat easily. In this paper, we show an implementation of our PhotoChat system and interaction patterns among PhotoChat users observed during our experimental evaluations.	camera module;digital camera	Yasuyuki Sumi;Jun Ito;Toyoaki Nishida	2008		10.1145/1358628.1358837	human–computer interaction;computer science;multimedia;world wide web	HCI	-48.21751540998156	-38.324665648512706	120785
a1d050c9225b2dd90947baab3a882e628a48c62c	design of a vision substitution vibrotactile vest for the visually impaired		In this paper, we create a low-cost, discrete, vibrotactile vest to address the error-prone task of navigation for the visually impaired. Our implementation is based upon sensory substitution principle, which dictates that information captured by a sensor is fed to user through a different sensory channel. In our case, a depth sensor is mounted on user's chest which scans the environment in vicinity. The depth information, after being processed, is translated to a tactile image on torso rendered by a two dimension array of actuators. In this way, a mental representation of the obstacles in the scene is created.	cognitive dimensions of notations;mental representation;sensory substitution;structured-light 3d scanner	Dimitris Kalampalikis;Konstantinos Moustakas	2018		10.1145/3200947.3201055	computer vision;mental representation;image processing;machine learning;artificial intelligence;computer science;sensory system;wearable computer;vest;sensory substitution;communication channel	HCI	-42.614897447861914	-42.53846813941456	120860
5f74e9d1263d0c4897c8da693f28e2d7458ccf21	temporal properties of illusory-surface perception probed with poggendorff configuration	illusory surface;time course;human visual system;temporal properties;poggendorff illusion;surface perception	Temporal properties of illusory surface perception were investigated by using the probing method of the Poggendorff configuration. We used real lines and an opaque illusory surface to compose the Poggendorff configuration, which was presented in an intermittent display method so that the real lines were displayed continuously and the opaque illusory surface was displayed periodically with various duration and interval times. The results showed that the opaque illusory surface required a minimum duration of approximately 220 msec for sustained perception. An interval of as much as 2200 msec was needed to obliterate the perception of the opaque illusory surface. We decided the intermittent display method was effective to directly examine the time course of illusory surface perception. Furthermore, we concluded that we could achieve better understanding of the surface perception mechanism of the human visual system by utilizing the intermittent display method and probing method of the Poggendorff configuration.		Qin Wang;Masanori Idesawa	2008		10.1007/978-3-540-87732-5_9	computer vision;computer science;human visual system model	Robotics	-44.405111788082735	-50.743934429694136	120945
1524df5b8b9baa5706ebdb5809b1ba61b889bdc3	entropy-based correction of eye tracking data for static scenes	tracking system;systematic error;prior knowledge;drift correction;entropy;eye tracking	In a typical head-mounted eye tracking system, any small slippage of the eye tracker headband on the participant's head leads to a systematic error in the recorded gaze positions. While various approaches exist that reduce these errors at recording time, only few methods reduce the errors of a given tracking system after recording. In this paper we introduce a novel correction algorithm that can significantly reduce the drift in recorded gaze data for eye tracking experiments that use static stimuli. The algorithm is entropy-based and needs no prior knowledge about the stimuli shown or the tasks participants accomplish during the experiment.	algorithm;entropy (information theory);experiment;eye tracking;tracking system	Samuel John;Erik Weitnauer;Hendrik Koesling	2012		10.1145/2168556.2168620	computer vision;simulation;computer science;communication	HCI	-43.09377701191834	-51.12880001372293	120969
b21dd6d1d682db6a849b88a0290f4745bffbf94f	a new design on multi-modal robotic focus attention	image motion analysis;speech processing;laser scanner;service robots;mobile robots;speech processing collision avoidance humanoid robots image motion analysis man machine systems mobile robots object detection optical tracking robot vision;multi modal system;human robot interaction;motion tracking;human tracking;speech command;robot vision;obstacle avoidance;multimodal robotic focus attention;humanoid robots;optical tracking;speech command multimodal robotic focus attention human detection human tracking human robot interaction sensory information motion tracking obstacle avoidance;human detection;robots;proceedings paper;focus attention;service robot;sensory information;collision avoidance;man machine systems;human tracking human robot interaction focus attention multi modal system service robots;object detection;multi modal interaction	Human detection and tracking is important for user-friendly human-robot interaction. The robot should be able to find the user autonomously and keep its attention to the user in a human-like manner. In this paper, a design and experimental study of robust human detection and tracking is presented through fusion several modalities of sensory information. The multi-modal interaction design utilizes a combination of visual, audio, and laser scanner data for reliable detection and tracking of an interested user. During tracking motion, obstacle avoidance behavior will be activated any time required to ensure safety. Furthermore, user can further assign the robot to interact with other user by speech command. Experimental results show that the robot can robustly tracks person under complex scenarios.	agent-based model;emotion recognition;experiment;face detection;hands-free computing;human–robot interaction;interaction design;interrupt;modal logic;obstacle avoidance;robot;robotic mapping;usability	Chia-How Lin;Chia-Hsing Yang;Cheng-Kang Wang;Kai-Tai Song;Jwu-Sheng Hu	2008	RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2008.4600732	laser scanning;human–robot interaction;robot;sensory system;mobile robot;computer vision;simulation;computer science;humanoid robot;artificial intelligence;speech processing;obstacle avoidance	Robotics	-37.15549254657473	-42.90128404062651	121176
d1a8f7cebabb6bd75c2998bbbe0e70777c218b9b	head-tracked off-axis perspective projection improves gaze readability of 3d virtual avatars.		Virtual avatars have been employed in many contexts, from simple conversational agents to communicating the internal state and intentions of large robots when interacting with humans. Rarely, however, are they employed in scenarios which require non-verbal communication of spatial information or dynamic interaction from a variety of perspectives. When presented on a flat screen, many illusions and visual artifacts interfere with such applications, which leads to a strong preference for physically-actuated heads and faces. By adjusting the perspective projection used to render 3D avatars to match a vieweru0027s physical perspective, they could provide a useful middle ground between typical 2D/3D avatar representations, which are often ambiguous in their spatial relationships, and physically-actuated heads/faces, which can be difficult to construct or impractical to use in some environments. A user study was conducted to determine to what extent a head-tracked perspective projection scheme was able to mitigate the issues in readability of a 3D avataru0027s expression or gaze target compared to use of a standard perspective projection. To the authorsu0027 knowledge, this is the first user study to perform such a comparison, and the results show not only an overall improvement in viewersu0027 accuracy when attempting to follow the avataru0027s gaze, but a reduction in spatial biases in predictions made from oblique viewing angles.		Tamas Bates;Jens Kober;Michael Gienger	2018		10.1145/3283254.3283271	computer vision;gaze;visual artifact;oblique case;spatial analysis;augmented reality;eye tracking;illusion;perspective (graphical);artificial intelligence;computer science	HCI	-43.7642451246117	-48.16353976760043	121254
ef7c204df1165584b47a3d4c8b33b323ceb093f8	integrating real space and virtual space in the 'invisible person' communication support system	mobile computer;support system;system design;system architecture;virtual space	The real space computing technologies, such as the mobile computing technology, enable users to make use of computers anywhere in the world. On the other hand, the virtual space computing technologies enable users to use remote computer resources from their desktop environments through intuitive operations. By combining these two kinds of computing technologies, we can construct a more flexible and general platform for computing in either space. Based on this viewpoint, we have realized a communication environment, called the 'invisible person' environment, where virtual space and real space are strongly associated. In this paper, we discuss the system architecture of this environment. The policies that we took in its design are 1) reduction to a feasible design at present, 2) wide-spread popularity to become an invisible person, and 3) emphasis on the realization of communication rather than the concrete analysis and accurate presentation of the real space. These policies are reflected on our system design where we provide users with several kinds of browsers for the flexibility of their operations.		Masahiko Tsukamoto	1998		10.1007/3-540-48962-2_5	simulation;human–computer interaction;engineering;multimedia	HCI	-47.11083098206532	-39.972963932405605	121292
b146ee1e65901aa1d6bf880d8f3891eec4060c9e	placing and recalling virtual items on the skin		The human skin provides an ample, always-on surface for input to smart watches, mobile phones, and remote displays. Using touch on bare skin to issue commands, however, requires users to recall the location of items without direct visual feedback. We present an in-depth study in which participants placed 30 items on the hand and forearm and attempted to recall their locations. We found that participants used a variety of landmarks, personal associations, and semantic groupings in placing the items on the skin. Although participants most frequently used anatomical landmarks (e.g., fingers, joints, and nails), recall rates were higher for items placed on personal landmarks, including scars and tattoos. We further found that personal associations between items improved recall, and that participants often grouped important items in similar areas, such as family members on the nails. We conclude by discussing the implications of our findings for design of skin-based interfaces.	high availability;mobile phone;scar (physics);skin (computing);smartwatch	Joanna Bergstrom-Lehtovirta;Sebastian Boring;Kasper Hornbæk	2017		10.1145/3025453.3026030	multimedia;interface design;recall;computer science	HCI	-45.663887267195875	-44.009097024954826	121490
9d5b043f74f4a2c91e0c2acf03aa3dfd01dc712d	detection thresholds for label motion in visually cluttered displays	label movement;label placement;air traffic control;label overlap;electronic mail;i 3 computing methodologies computer graphics;stereoscopic depth;application software;user interface;computer graphics;technology;h 5 2 information systems;computer and information science;virtual reality;h 5 2 information systems user interfaces;teknikvetenskap;motion detection displays virtual reality virtual environment application software user interfaces air traffic control humans nasa algorithm design and analysis;computer graphic;detection threshold;i 3 computing methodologies computer graphics h 5 2 information systems user interfaces;natural sciences;visualization;label motion;visual clutter label motion visually cluttered displays label overlap label movement dynamic displays motion detection threshold virtual environment stereoscopic depth lateral monoscopic movement 2d label placement visual periphery reduced sensitivity realistic viewing condition visual masking;lateral monoscopic movement;three dimensional displays;heuristic algorithms;displays;visual clutter;datavetenskap datalogi;realistic viewing condition;2d label placement;motion detection threshold;humans;reduced sensitivity;i 3 computing methodologies;computer science;information system;virtual environment;augmented reality;visually cluttered displays;visual attention;nasa;user interfaces;motion detection;algorithm design and analysis;visual periphery;visual masking;dynamic displays	While label placement algorithms are generally successful in managing visual clutter by preventing label overlap, they can also cause significant label movement in dynamic displays. This study investigates motion detection thresholds for various types of label movement in realistic and complex virtual environments, which can be helpful for designing less salient and disturbing algorithms. Our results show that label movement in stereoscopic depth is shown to be less noticeable than similar lateral monoscopic movement, inherent to 2D label placement algorithms. Furthermore, label movement can be introduced more readily into the visual periphery (over 15° eccentricity) because of reduced sensitivity in this region. Moreover, under the realistic viewing conditions that we used, motion of isolated labels is more easily detected than that of overlapping labels. This perhaps counterintuitive finding may be explained by visual masking due to the visual clutter arising from the label overlap. The quantitative description of the findings presented in this paper should be useful not only for label placement applications, but also for any cluttered AR or VR application in which designers wish to control the users' visual attention, either making text labels more or less noticeable as needed.	algorithm;ar (unix);automatic label placement;clutter;distance (graph theory);lateral computing;lateral thinking;stereoscopy;virtual reality	Stephen D. O'Connell;Magnus Axholt;Matthew D. Cooper;Stephen R. Ellis	2010	2010 IEEE Virtual Reality Conference (VR)	10.1109/VR.2010.5444788	computer vision;augmented reality;simulation;computer science;operating system;virtual reality;user interface;computer graphics (images)	Visualization	-43.30833590850363	-48.13652107634616	121554
e7a96b2f1c82bd13b3167bb87d11c7623bd6d14e	a preliminary usability evaluation of strategies for seeking online information with elderly people	usability evaluation;information overload;older people;elderly people;seeking online information strategies;usability	This short paper describes an experimental study with elderly users comparing three strategies for seeking online information, Google basic search, the Yahoo! Directory and Google advanced search. The effect or three general usability criteria for the elderly, simplicity, difficulties using the mouse and cautious clicking and reading, on the total search time older people spend seeking complex online information with the three strategies has been studied. The hypothesis that basic search is the fastest strategy because it meets the three usability criteria, unlike the other two strategies, is confirmed. Older people were 3 times faster in basic search than in either advanced search or directory. Advanced search was slower than basic search due to information overload but faster than the directory, which was the slowest strategy primarily due to difficulties using the mouse and information overload.	directory (computing);experiment;fastest;information overload;usability	Sergio Sayago;Josep Blat	2007		10.1145/1243441.1243457	simulation;computer science;multimedia;world wide web	HCI	-35.82584590687576	-51.95711244344758	121713
22cd3c2c1c96783a725bb59c06f3b82279326180	the effects of operator spatial perception and sensory feedback on human-robot teleoperation performance	sensory feedback;spatial perception	Teleoperation requires a complex combination of the operator's cognitive, perceptual, and motor skills. Our experiment tested the ability of subjects to teleoperate a remote robot under different conditions of increasing sensory feedback. We also evaluated each operator's spatial perception skills using a battery of tests to understand the effect of spatial perception on the operator's ability to perform the teleoperation task. The experiment showed that the spatial ability of an operatoras reflected by a test battery of two spatial recognition and two spatial manipulation testswas significantly correlated with the ability to teleoperate the robot through a maze. Surprisingly, providing different combinations of visual, auditory, and vibrotactile feedback to the operator did not significantly change performance. However, there was an interaction between spatial ability and feedback condition that affected teleoperation performance.	feedback;robot;simulation;virtual reality;aptitude	Corinna E. Lathan;Michael Tracey	2002	Presence: Teleoperators & Virtual Environments	10.1162/105474602760204282	computer vision;simulation;computer science	Robotics	-46.108372417915795	-48.531860355381134	121821
47a7f5c0effecd2e4e373459527653999d4d0098	stroke rehabilitation with a sensing surface	rehabilitation;tabletop;hci;gesture recognition;stroke	"""This paper presents a new sensing and interaction environment for post-stroke and upper extremity limb rehabilitation. The device is a combination of camera-based multitouch sensing and a supporting therapeutic software application that advances the treatment, provides feedback, and records a user's progress. The image-based analysis of hand position provided by a Microsoft Surface is used as an input into a tabletop game environment. Tailored image analysis algorithms assess rehabilitative hand movements. Visual feedback is provided in a game context. Experiments were conducted in a sub-acute rehabilitation center. Preliminary user studies with a stroke-afflicted population determined essential design criteria. Hand and wrist sensing, as well as the goals of the supporting game environment, engage therapeutic flexion and extension as defined by consulted physicians. Participants valued personalization of the activity, novelty, reward and the ability to work at their own pace in an otherwise repetitive therapeutic task. A """"character"""" - game element personifying the participant's movement - was uniquely motivating relative to the media available in the typical therapeutic routine."""	algorithm;image analysis;multi-touch;personalization;usability testing	Cati N. Boulanger;Adam Boulanger;Lilian de Greef;Andy Kearney;Kiley Sobel;Russell Transue;Elizabeth Sweedyk;Paul H. Dietz;Steven Bathiche	2013		10.1145/2470654.2466160	simulation;stroke;human–computer interaction;computer science;gesture recognition;multimedia	HCI	-46.20508067094453	-44.40966143993918	121994
fca17595ab54c36ee97a6047e9c7c40cb63d50ba	robotic gesture generation based on a cognitive basis for non-verbal communication	robot sensing systems;human robot interaction robotic gesture generation nonverbal communication semantic synthesis method cognitive behavior recognition emotional behavior recognition humanoid robot virtual space robotic hand robot cognitive process motion set;cognitive and emotional behavior semantic representation gesture generation non verbal communication;thumb humanoid robots shape robot sensing systems joints;joints;thumb;shape;humanoid robots;motion control cognitive systems gesture recognition humanoid robots human robot interaction	This paper introduces a semantic synthesis method that enables robots to generate human-like gestures by recognizing cognitive and emotional behaviors based on a given situation. Assuming that the human cognitive process is represented as a series of associated events, we proposed a virtually touchable space associated with robotic hands. Additionally, in a humanoid robot, the motions of two arms are considered as a crucial non-verbal communication channel because large spatial changes capture the attention of a human agent. Additionally, virtual spaces related to certain events are described by robotic hands. The concept of virtual spaces is tested with regard to the expression of the robot's cognitive process with a combination of predefined motion sets.	channel (communications);coat of arms;cognition;humanoid robot;robotic arm	Jeong-Yean Yang;Dong-Soo Kwon	2014	2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2014.7057497	computer vision;simulation;shape;computer science;humanoid robot;artificial intelligence;social robot;robot control	Robotics	-34.25695488581999	-40.44417237905373	122019
ec5b7d78adf9dbcefbb6a010d79577af6253c7ca	aesthetic measure of alignment and regularity	the hough transform;aesthetic rules;regularity;automatic layout evaluation;design knowledge;hough transform;designer grid;quality control;alignment	To be effective as communications or sales tools, documents that are personalized and customized for each customer must be visually appealing and aesthetically pleasing. Producing perhaps millions of unique versions of essentially the same document not only presents challenges to the printing process but also disrupts the standard quality control procedures. The quality of the alignment in each document can easily distinguish professionally looking documents from amateur designs and some computer generated layouts. A multicomponent measure of document alignment and regularity, derived directly from designer knowledge, is developed and presented in computable form. The measure includes: edge quality, page connectivity, grid regularity and alignment statistics. It is clear that these components may have different levels of importance, relevance and acceptability for various document types and classes, thus the proposed measure should always be evaluated against the requirements of the desired class of documents.	computable function;display resolution;document;information extraction;path ordering (term rewriting);personalization;printing;relevance;requirement;semi-supervised learning;semiconductor industry;supervised learning;while	Helen Balinsky;Anthony Wiley;Matthew C. Roberts	2009		10.1145/1600193.1600207	hough transform;quality control;computer science;data mining;database;multimedia;world wide web	Web+IR	-35.799918166315386	-48.385792622994074	122087
f183cf83be498cafb2daa612287e35a5929ca833	a perceptual interface for vision substitution in a color matching experiment	perceptual auditory coding;auditory channel;image coding;color matching;image matching;perceptual auditory coding perceptual interface vision substitution color matching experiment auditory channel see color visually impaired people colored video image;serveur institutionnel;joints;musical instruments;vision substitution;see color;artificial neural networks;visually impaired people;handicapped aids;archive institutionnelle;image colour analysis;open access;visual impairment;color matching experiment;archive ouverte unige;artificial neural networks conferences joints;cybertheses;perceptual interface;image matching handicapped aids image coding image colour analysis;institutional repository;conferences;colored video image	In the context of vision substitution by the auditory channel several systems have been introduced. One such system that is presented here, See ColOr, is a dedicated interface part of a mobility aid for visually impaired people. It transforms a small portion of a colored video image into spatialized instrument sounds. In this work the purpose is to verify the hypothesis that sounds from musical instruments provide an alternative way to vision for obtaining color information from the environment. We introduce an experiment in which several participants try to match pairs of colored socks by pointing a head mounted camera and by listening to the generated sounds. Our experiments demonstrated that blindfolded individuals were able to accurately match pairs of colored socks. The advantage of the See ColOr interface is that it allows the user to receive a feed-back auditory signal from the environment and its colors, promptly. Our perceptual auditory coding of pixel values opens the opportunity to achieve more complicated experiments related to vision tasks, such as perceiving the environment by interpreting its colors.	color;experiment;feedback;pixel;socks	Guido Bologna;Benoît Deville;Michel Vinckenbosch;Thierry Pun	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4634014	computer vision;computer science;machine learning;multimedia;artificial neural network	Vision	-43.00281873349263	-44.69932281211396	122310
e93a2a062a838949059c4bbaf6128843a4055664	an indoor navigation system using signpost metaphor for smartphone environments		This manuscript introduces an indoor navigation system that uses signpost metaphor. A signpost is usually installed at each fork of a path and tells people which fork they should take. Our system locates the user’s position by reading visual markers, which represent signposts. After that, it retrieves floor maps from the database and allows the user to turn around seeing labels from the perspective of the user’s position, which are overlaid on the real world. It also navigates the user’s way not only at the signpost but also between signposts by allowing the user to move around in the corresponding virtual world. From an experiment in usability, our system demonstrated good results for indoor navigation.		Daiki Aono;Makio Ishihara	2014		10.1007/978-3-319-07857-1_98	human–computer interaction;multimedia;fork (system call);usability;computer science;navigation system;metaphor;virtual reality;augmented reality	HCI	-45.94415342232595	-41.89108565557324	122495
538d2efcf11f159c20d7f20b9c6058643d10f039	intelligent illumination model-based lighting control	simplistic virtual room model;stress;time varying nondeterministic daylight contributions;wall;texture;electric lighting;lighting control;floor;furniture colour;wireless sensors;sensors;fixtures;lighting lighting control solid modeling sensors artificial intelligence light sources aerospace electronics;glare;daylight infiltration;wireless control intelligent lighting machine learning illumination modeling;open loop systems;intelligent open loop lighting control;intelligent lighting;intelligent control;windows;machine learning;solid modeling;aerospace electronics;illumination modeling;intelligent lighting control system;artificial intelligence;lighting;intelligent illumination model based lighting control;productivity;commercial office environments;wireless sensor networks daylighting intelligent control lighting control open loop systems;daylighting;wireless sensor networks;light sources;wireless control;luminaires;simplistic virtual room model intelligent illumination model based lighting control commercial office environments workplace comfort productivity stress luminaires fixtures wall floor furniture colour texture daylight infiltration electric lighting intelligent lighting control system time varying nondeterministic daylight contributions windows glare wireless sensors intelligent open loop lighting control;workplace comfort	"""Lighting in commercial office environments is a major factor in workplace comfort, productivity, and stress. Modern work environments strive to improve these conditions by better selection of luminaires (fixtures), wall, floor, and furniture colour and texture, and intelligent lighting control that ensures that occupants each receive the proper amount of light needed at their workstations for the task at hand. Daylight infiltration is generally good for occupant comfort, and provides opportunities to """"harvest"""" this daylight and save energy on electric lighting, provided that it is harnessed in a way that it does not over illuminate or cause glare. Building an intelligent lighting control system is challenging due to incompatible or sometimes conflicting lighting preferences from adjacent people or areas. Some office environments can be additionally challenging because of complex geometries, time-varying nondeterministic daylight contributions through windows, and glare. Here we address the challenges of meeting users' individual lighting preferences using a highly accurate illumination model to enable balancing the various lighting requirements among spatially grouped task areas, while minimizing the energy needed to do so (and in the future, minimizing the number of wireless sensors needed for this application). We propose an illumination model-based method and algorithm for intelligent open-loop lighting control, and present the results of a simulation study using a simplistic virtual room model to demonstrate the validity of our method. Daylight infiltration is to be addressed in future work."""	algorithm;daylight;lighting control system;list of common shading algorithms;microsoft windows;requirement;sensor;simulation;workstation	Michael Fischer;Kui Wu;Pan Agathoklis	2012	2012 32nd International Conference on Distributed Computing Systems Workshops	10.1109/ICDCSW.2012.75	electric light;open-loop controller;embedded system;productivity;simulation;wireless sensor network;glare;daylighting;computer science;sensor;lighting;solid modeling;texture;stress;smart lighting;intelligent control;intelligent lighting	Robotics	-40.43355907649367	-50.94999914767044	122554
33e64675538a4c5568107035e563ebe76b079c75	the benefits of multimodal information: a meta-analysis comparing visual and visual-tactile feedback	human cognition;meta analysis;tactile feedback;multimodal;visual feedback;visual tactile feedback;reaction time	Information display systems have become increasingly complex and more difficult for human cognition to process effectively. Based upon Wicken's Multiple Resource Theory (MRT), information delivered using multiple modalities (i.e., visual and tactile) could be more effective than communicating the same information through a single modality. The purpose of this meta-analysis is to compare user effectiveness when using visual-tactile task feedback (a multimodality) to using only visual task feedback (a single modality). Results indicate that using visual-tactile feedback enhances task effectiveness more so than visual feedback (g = .38). When assessing different criteria, visual-tactile feedback is particularly effective at reducing reaction time (g = .631) and increasing performance (g = .618). Follow up moderator analyses indicate that visual-tactile feedback is more effective when workload is high (g = .844) and multiple tasks are being performed (g = .767). Implications of results are discussed in the paper.	cognition;google moderator;information display systems;modality (human–computer interaction);multimodal interaction;visual basic[.net]	Matthew S. Prewett;Liuquin Yang;Frederick R. B. Stilson;Ashley A. Gray;Michael D. Coovert;Jennifer L. Burke;Elizabeth S. Redden;Linda R. Elliott	2006		10.1145/1180995.1181057	mental chronometry;computer vision;meta-analysis;speech recognition;cognition;computer science;multimodal interaction;multimedia	HCI	-47.00843916544766	-47.20416144274888	122559
407e1099bc7ad7bf72a6e5d9623339ca29afd735	magic stick: a tangible interface for the edutainment of young children	animals;pediatrics;magic stick;young children;computer aided instruction;edutainment tool;rfid tag;tangible interface;development tool;media;visualization;rfid tags;graphical user interfaces;visual representations;educational aids;visual representation;games;young children edutainment tangible interface;gui;gui magic stick tangible interface young children edutainment tool visual representations rfid tags;edutainment;bluetooth;rfid tags brushes user interfaces multimedia communication information technology joining processes visualization graphical user interfaces atmosphere paints;radiofrequency identification;radiofrequency identification computer aided instruction educational aids graphical user interfaces	Recently, there has been a high demand for developing tools that promote education through learning. We introduce our edutainment tool called Magic Stick that helps children learn about new objects by providing their names associated by visual representations regarding these objects. Children's parents or teachers can pick the entities they would like their children to learn about by simply attaching RFID tags to these entities. Afterwards, they can customize the type of information and visualizations related to these entities through the use of a friendly GUI designed for this purpose. In our study with young children, we found that the Magic Stick created an entertaining atmosphere among children and greatly engaged them in learning.	educational entertainment;entity;graphical user interface;radio-frequency identification;tangible user interface	Ali Karime;M. Anwar Hossain;Wail Gueaieb;Abdulmotaleb El-Saddik	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202750	radio-frequency identification;human–computer interaction;computer science;operating system;graphical user interface;multimedia	HCI	-46.01187517549008	-39.093422838166596	122795
620af8f809ff39ec220dccef470df4f4da586802	comparison of force and vibrotactile feedback with direct stimulation for texture recognition	paper palpable prototype force feedback vibrotactile feedback direct stimulation texture recognition haptic feedback texture discrimination virtual environment phantom device vibrotactile data glove;vibrations;phantoms;virtual reality data gloves force feedback;virtual reality;thumb;force feedback;dataglove force feedback vibrotactile textures;force feedback vibrations thumb phantoms tactile sensors;tactile sensors;dataglove;data gloves;vibrotactile;textures	In this paper a study is conducted in order to evaluate three different strategies of haptic feedback for texture discrimination in virtual environments. Specifically, both force and vibrotactile feedback have been evaluated, as well as the direct use of the sense of touch, to detect different textures. To this end, a force feedback Phantom device, a custom built vibrotactile data glove and paper palpable prototypes, which represent an ideal model of tactile feedback, have been compared. These three methods have been used to detect two types of patterns, one formed by different geometrical shapes, and the other with different grooves width. Results show that the vibrotactile data glove has a notable behaviour in the detection of textures where the frequency of tactile stimuli varies, and it is even useful to detect more complex textures.	algorithm;design of experiments;dynamic problem (algorithms);effective method;feedback;haptic technology;imaging phantom;virtual reality;wired glove	Jonatan Martínez;Diego Martínez;José Pascual Molina;Pascual González;Arturo S. García	2011	2011 International Conference on Cyberworlds	10.1109/CW.2011.23	computer vision;simulation;computer science;artificial intelligence;vibration;virtual reality;haptic technology;tactile sensor	Robotics	-43.99461812910376	-49.6601108573728	122838
6b6fc72d9c0b9b8bceea32c73c62e6cff890791b	a replai of soccer: recognizing intentions in the domain of soccer games				Gudula Retz-Schmidt	1988			computer science;artificial intelligence;machine learning	AI	-36.66927448442873	-39.0008128148022	122841
bccadd7f79a4d09e488fccedc987c2966ca1f433	using real objects for interaction in virtual reality		"""Output devices to view Virtual Reality (VR), like Head Mount Displays, are getting common lately. However, there is no standard input devices to tell computers user's intention in VR yet. According to the naive intuition, using real objects that has similar shapes as in VR is the better way than using controllers like mouses or game controllers to manipulate objects showed in VR. But is it true? To make it clear, we develop an game system like tower defense. Users played the game in two ways, one is using real objects, the other is using mouse to place """"Towers"""". As the result of questionnaire after game plays, """"easy to use"""" factor was lower for the real objects operation mainly because of the technical difficulties like object detection failure. But the """"fun to use"""" factor was still higher for the real objects than the controllers. It show that using real objects has high potential for interaction between users and the VR system."""	computer;experiment;experimental system;game controller;input device;interaction design;object detection;standard streams;virtual reality	Ryota Yoshimoto;Mariko Sasakura	2017	2017 21st International Conference Information Visualisation (IV)	10.1109/iV.2017.57	simulation;input device;mixed reality;object detection;immersion (virtual reality);augmented reality;computer-mediated reality;metaverse;virtual reality;computer science	Visualization	-43.181816624520174	-38.37114194764017	122867
154dc339f1de7806306436cb8a3ec6501c7db07d	how natural is a natural interface? an evaluation procedure based on action breakdowns	action breakdown;naturalness;usability;systematic video analysis	This paper describes an issue-based method to evaluate the naturalness of an interface. The method consists of the execution of a series of tasks on that interface, which is subsequently systematically analyzed to identify breakdowns in the users’ actions. The systematic analysis of breakdowns is allowed by the support of video-coding software (The Observer by Noldus). This method is described on its theoretical bases and then applied to the evaluation of a natural interface, a walk-in-place locomotion system for virtual spaces called Superfeet. The procedure is comparative, since Superfeet is compared to two locomotion devices, Superfeet enhanced with headtracker and a more traditional Joypad. The test involves 36 participants (mean age = 23.68, SD = 3.14). The outcomes of the breakdown analysis are illustrated at a progressively finer level of granularity from the amount and length of breakdowns, to the circumstances of the breakdowns, to the type of actions involved in the breakdowns. The potential of this procedure for usability studies is finally synthesized.	in-place algorithm;joystick;natural mapping (interface design);natural user interface;top-down and bottom-up design;unfolding (dsp implementation);unintended consequences;usability testing;vagueness;video content analysis	Luciano Gamberini;Anna Spagnolli;Lisa Prontu;Sarah Furlan;Francesco Martino;Beatriz Rey;Mariano Alcañiz Raya;José Antonio Lozano	2011	Personal and Ubiquitous Computing	10.1007/s00779-011-0476-z	simulation;naturalness;usability;human–computer interaction;computer science;artificial intelligence	HCI	-47.56015772533584	-47.523146271887065	123123
d965f4444695383e12ecdcb319fffcca7145bd01	quick viewpoint switching for manipulating virtual objects in hand-held augmented reality using stored snapshots	legged locomotion;feeds;quick viewpoint switching;user interfaces augmented reality;arrays;visualization;cameras switches handheld computers visualization arrays legged locomotion feeds;augmented reality;switches;viewpoint alignment task quick viewpoint switching virtual object manipulation hand held augmented reality stored snapshots magic lens style augmented reality visual feedback augmented scene snapshot condition live mode condition virtual object physical object;user interfaces;cameras;handheld computers;virtual travel;virtual travel augmented reality quick viewpoint switching	Magic-lens style augmented reality applications allow users to control camera pose easily by manipulating a portable hand-held device and provide immediate visual feedback. However, strategic vantage points must often be revisited repeatedly, adding time and error and taxing memory. We describe a new approach that allows users to take snapshots of augmented scenes that can be virtually revisited at later times. The system stores still images of scenes along with camera poses, so that augmentations remain dynamic and interactive. Users can manipulate virtual objects while viewing snapshots, instead of moving to real-world views. We present a study comparing performance in snapshot and live mode conditions in a task in which a virtual object must be aligned with two pairs of physical objects. Proper alignment requires sequentially visiting two viewpoints. Participants completed the alignment task significantly faster and more accurately using snapshots than when using the live mode. Moreover, participants preferred manipulating virtual objects using snapshots to the live mode.	ar (unix);augmented reality;broadcast domain;habbo;ibm notes;interaction technique;microsoft xna;mobile device;overhead (computing);prototype;snapshot (computer storage);usability testing	Mengu Sukan;Steven K. Feiner;Barbara Tversky;Semih Energin	2012	2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)	10.1109/ISMAR.2012.6402560	computer vision;augmented reality;computer-mediated reality;simulation;visualization;network switch;computer science;operating system;user interface;computer graphics (images)	Visualization	-44.43721921165536	-40.59640546011648	123177
c069432bbbb9a58d98b0c76c488f14cf563307ef	evaluating user performance in 3d stereo and motion enabled video games	3d interaction;video games;motion control;control group;move controller;spatial interaction;playstation 3;game design;stereoscopic vision;video game;user performance experience;3d stereo;experimental group	We present a study that investigates user performance benefits of playing video games using 3D motion controllers in 3D stereoscopic vision in comparison to monoscopic viewing. Using the PlayStation 3 game console coupled with the PlayStation Move Controller, we explored five different games that combine 3D stereo and 3D spatial interaction. For each game, quantitative and qualitative measures were taken to determine if users performed better and learned faster in the experimental group (3D stereo display) than in the control group (2D display). A game expertise pre-questionnaire was used to classify participants into beginners and expert game player categories to analyze a possible impact on performance differences. The results show two cases where the 3D stereo display did help participants perform significantly better than with a 2D display. For the first time, we can report a positive effect on gaming performance based on stereoscopic vision, although reserved to isolated tasks and depending on game expertise. We discuss the reasons behind these findings and provide recommendations for game designers who want to make use of 3D stereoscopic vision and 3D motion control to enhance game experiences.	experiment;motion capture;motion controller;playstation 3;playstation move;polystation;stereo display;stereopsis;stereoscopy	Arun Kulshreshth;Jonas Schild;Joseph J. LaViola	2012		10.1145/2282338.2282350	game design;motion control;video game graphics;computer vision;simulation;computer science;stereopsis;game art design;multimedia;scientific control	HCI	-45.591779767577115	-47.739461833445255	123203
2df9e2a6fcf389067784ba3b1917b0c34153d0f4	experiences with digital pen, keyboard and mouse usability	software tool;usability testing;multi modal interface;natural computing;tablet pc;operating system;pen input;pen based computing	Pen input provides intuitive and natural computer interaction for tasks such as annotating documents and sketching. However interfaces that use a pen alone can be slow and inefficient. Thus most pen interfaces also support keyboard and mouse input. Multi-modal input exponentially increases the complexity of the design and usability of these systems. Here we describe our usability testing experiences of four different pen-dominant software tools. One is designed for a digital whiteboard, two for a Tablet PC and the last for a Tablet PC coupled to a haptic pen. Our experiences suggest that pen only input is difficult to achieve with a standard operating system because the pen is appreciably overloaded. However, in situations where a keyboard and mouse is convenient, users tolerate the inefficient of pen interaction in exchange for convenience.	computer mouse;digital pen;game controller;haptic technology;interactive whiteboard;modal logic;operating system;pen computing;tablet computer;usability testing	Beryl Plimmer	2008	Journal on Multimodal User Interfaces	10.1007/s12193-008-0002-4	active pen;natural computing;human–computer interaction;computer science;operating system;multimedia;handwriting recognition;computer graphics (images)	HCI	-48.21302733575818	-41.87100870873743	123228
ebb86836f557e016b37209295f0e6bb0990d9950	rsvp on the go: implicit reading support on smart watches through eye tracking	mental load;eye gaze interaction;reading interfaces;eye tracking;rsvp;comprehension	While smartwatches have become common for mobile interaction, one of their main limitation is the limited screen size. To facilitate reading activities despite these limitations, reading with Rapid Serial Visual Presentation (RSVP) has been shown to be feasible. However, when text is presented in rapid sequence, single words are easily missed due to blinking or briefly glancing up from the screen. This gets worse the more the reader is engaged in a secondary task, such as walking. To give implicit control over the reading flow we combined an RSVP reading application on a smartwatch with a head-worn eye tracker. When the reading flow is briefly interrupted, the text presentation automatically pauses or backtracks. In a user study with 15 participants we show that using eye tracking in combination with RSVP increases users' comprehension compared to a touch-based UI to control the text presentation. We argue that eye tracking will be a valuable extension for future smartwatch interaction.	backtracking;display size;eye tracking;interrupt;mobile device;mobile interaction;modality (human–computer interaction);prototype;smartwatch;usb on-the-go;usability testing;user interface	Tilman Dingler;Rufat Rzayev;Valentin Schwind;Niels Henze	2016		10.1145/2971763.2971794	computer vision;eye tracking;computer science;multimedia;communication	HCI	-46.88901277091498	-43.88360233574902	123406
f66201f58734dcc5c458c4e45f80d08119aebaaf	architectural concepts of a semi-autonomous wheelchair	aide handicape;handicapped aid;robot movil;ayuda minusvalido;systeme commande;sistema control;sensor system;rehabilitation;architecture systeme;wheelchair;concepcion sistema;autonomous system;rehabilitacion;robotics;silla de ruedas;smart sensor;sistema autonomo;500 naturwissenschaften und mathematik;control system;robot mobile;human machine interface;system design;sensorics;systeme autonome;wheel chair;robotica;arquitectura sistema;control;rehabilitation technology;robotique;mobile systems;system architecture;vocational rehabilitation;conception systeme;moving robot;mobile system;fauteuil roulant	A new smart, sensor-assisted wheelchair system for the vocational rehabilitation of people with severe and multiple handicap has been developed in the research project OMNI within the CEC TIDE programme. The project finished in December 1996. The objective of the project was the development of an advanced wheelchair with omnidirectional manoeuvrability and navigational intelligence that is well suited for vocational rehabilitation. It provides an opportunity of intuitive wheelchair control to people with severe physical or multiple (incl. mental) handicap. Cramped offices are made accessible by the small outline, high mobility and navigational support of the chair. The user’s safety and driving accuracy are guaranteed by a novel sensor system and navigation modules. A wide range of control devices can be used with the adaptable human-machine interface which also controls environmental devices. Within this project, the user focused principle has strongly been followed (this includes the involvement of users from the preparatory work to the evaluation of the results).	autonomous robot;semiconductor industry	Ulrich Borgolte;Helmut Hoyer;Christian Bühler;Helmut Heck;Ralf Hoelper	1998	Journal of Intelligent and Robotic Systems	10.1023/A:1007944531532	human–machine interface;embedded system;simulation;engineering;autonomous system;control system;artificial intelligence;robotics;scientific control;systems design	Robotics	-39.22889430233046	-47.62923090968787	123444
2050efe66c349f90fb5d01b2d4caaaafb0fd1c9f	high-precision pointing on large wall displays using small handheld devices	pointing;wall displays;handheld devices	Rich interaction with high-resolution wall displays is not limited to remotely pointing at targets. Other relevant types of interaction include virtual navigation, text entry, and direct manipulation of control widgets. However, most techniques for remotely acquiring targets with high precision have studied remote pointing in isolation, focusing on pointing efficiency and ignoring the need to support these other types of interaction. We investigate high-precision pointing techniques capable of acquiring targets as small as 4 millimeters on a 5.5 meters wide display while leaving up to 93 % of a typical tablet device's screen space available for task-specific widgets. We compare these techniques to state-of-the-art distant pointing techniques and show that two of our techniques, a purely relative one and one that uses head orientation, perform as well or better than the best pointing-only input techniques while using a fraction of the interaction resources.	direct manipulation interface;glossary of computer graphics;image resolution;mobile device;tablet computer	Mathieu Nancel;Olivier Chapuis;Emmanuel Pietriga;Xing-Dong Yang;Pourang Irani;Michel Beaudouin-Lafon	2013		10.1145/2470654.2470773	computer vision;simulation;computer science;operating system;mobile device	HCI	-44.35630312487869	-42.49873589626929	123979
84cd4bcb5c7ae9db1935b9758c64c91d45da73d1	the recovery from task execution errors during time delayed teleoperation	feedback task execution error time delayed teleoperation teleprogramming system supervisory control operator interaction virtual environment remote manipulator error detection fault diagnosis error recovery task level reasoning task planning action level reasoning;robot sensing systems;error recovery;yarn;motion control;supervisory control;force sensors;virtual reality;planning artificial intelligence;task level reasoning;delay effects;inference mechanisms;remote manipulator;time delay;operator interaction;teleprogramming system;force feedback;task execution error;feedback;system recovery;system design;system recovery telerobotics man machine systems delays inference mechanisms fault diagnosis planning artificial intelligence user interfaces virtual reality;mechanical sensors;normal operator;task planning;telerobotics;displacement control;time delayed teleoperation;delay effects force control mechanical sensors virtual environment motion control displacement control force feedback force sensors robot sensing systems yarn;action level reasoning;error detection;virtual environment;user interfaces;man machine systems;delays;fault diagnosis;force control	"""The teleprogramming system employs a supervisory control approach where operator interaction with a virtual environment directs the actions of a remote manipulator operating semi-autonomously. We focus on the detection, diagnosis, and recovery from unexpected situations frequently arising during normal operation. The teleprogramming system employs a single thread approach to error recovery that is inherent in the system design. We perform two experimental tasks, each highlighting a different role for the operator, using the teleprogramming system with a simulated communication time delay of 10 seconds. In the first task of inserting and extracting bolts, the operator assumes the role of """"task level reasoning"""" or task planning and sequencing, task state reasoning, and diagnostic procedure generation. In the second task, puncturing and slicing duct tape, the operator performs """"action level reasoning"""" or diagnosing and correcting unexpected situations and determining success of actions. Our experimental results indicate that complex tasks can be accomplished when the operator is provided multiple and overlapping forms of feedback. >"""		Matthew R. Stein;Craig Sayers;Richard P. Paul	1994		10.1109/IROS.1994.407425	telerobotics;control engineering;motion control;real-time computing;simulation;error detection and correction;computer science;engineering;virtual machine;artificial intelligence;control theory;feedback;virtual reality;supervisory control;haptic technology;user interface;remote manipulator;normal operator;systems design	NLP	-40.03585593386062	-47.6422968676008	124097
cc52f9973cd56a03babde38e5905cf6ca6eb0377	mems inertial sensors: a tutorial overview	mechanical sensors micromechanical devices consumer electronics electrostatics sensor systems integrated circuits	Inertial sensors based on MEMS technology are fast becoming ubiquitous with their adoption into many types of consumer electronics products, including smart phones, tablets, gaming systems, TV remotes, toys, and even (more recently) power tools and wearable sensors. Now a standard feature of most smart phones, MEMS-based motion tracking enhances the user interface by allowing response to user motions, complements the GPS receiver by providing dead-reckoning indoor navigation and supporting location-based services, and holds the promise of enabling handset optical image stabilization in next-generation handsets by virtue of its lower cost and small form factor. This tutorial provides an overview of MEMS technology and describes the essential features of the mechanical systems underlying the most common sensors accelerometers and gyroscopes. It also highlights some fundamental trade-offs related to mechanical system dynamics, force and charge transduction methods, and their implications for the mixed-signal systems that process the sensor outputs. The presentation of an energy-based metric allows a comparison of the performance of competing sensor solutions. For each type of sensor, descriptions of the underlying mechanical theory, canonical sensor architectures, and key design challenges are also presented. Finally, the tutorial reviews multisensor silicon MEMS/CMOS monolithic integration, which is driving the cost and form factor reduction behind the current proliferation of these devices.	cmos;dead reckoning;global positioning system;image;location-based service;microelectromechanical systems;mixed-signal integrated circuit;sensor;small form factor;smartphone;system dynamics;tablet computer;television;toys;transduction (machine learning);user interface;wearable computer	Derek K. Shaeffer	2013	IEEE Communications Magazine	10.1109/MCOM.2013.6495768	embedded system;telecommunications;computer science	Mobile	-44.633561446071276	-41.14792156189189	124202
6f12c67ed416a75dbb30827cc673d62e8bb3b2e2	adaptive layout template for effective web content presentation in large-screen contexts	web pages;real estate;new web standards;web design;adaptive layout;large displays;large display settings	Despite the fact that average screen size and resolution have dramatically increased, many of today's web sites still do not scale well in larger viewing contexts. The upcoming HTML5 and CSS3 standards propose features that can be used to build more flexible web page layouts, but their potential to accommodate a wider range of display environments is currently relatively unexplored. We examine the proposed standards to identify the most promising features and report on experiments with a number of adaptive layout mechanisms that support the required forms of adaptation to take advantage of greater screen real estates, such as automated scaling of text and media. Special attention is given to the effective use of multi-column layout, a brand new feature for web design that contributes to optimising the space occupied by text, but at the same time still poses problems in predominantly continuous vertical-scrolling browsing behaviours. The proposed solutions were integrated in a flexible layout template that was then applied to an existing news web site and tested on users to identify the adaptive features that best support reading comfort and efficiency.	blog;bundle adjustment;cascading style sheets;client-side;display size;experiment;html5;image resolution;image scaling;scrolling;task manager;user experience;web application;web content;web design;web developer;web page;webmail;wiki	Michael Nebeling;Fabrice Matulic;Lucas Streit;Moira C. Norrie	2011		10.1145/2034691.2034737	web service;web modeling;web mapping;web design;web standards;computer science;web navigation;web page;data mining;database;multimedia;world wide web;real estate	Web+IR	-35.4371095030626	-48.23896602230287	124245
696d295c61e59c2acca6e0a94b0a0d1a078b41a2	advanced interactive t-learning system with variable remote control	remote control	T-Learning was passive, for a long time, because of the restrictions imposed on TVs. The relevant restrictions are a result of the input function, i.e. the normal TV remote control. Researchers have proposed various solutions for improving the normal TV remote control. However, these are difficult to be in practice now. Nowadays, it has been established that dual-device services may be helpful for TLearning. These studies usually lay more emphasis and stress on the cooperation of output function, while ignoring the input function of the cooperative device. Thus, this study is aimed at improving the normal TV remote control for advanced T-Learning. In the present study, an advanced T-Learning framework named as “DDRC T-Learning Framework” is proposed which is based on M2M cooperation to improve the input function in T-Learning., In the DDRC T-Learning Framework, the touch screen of a handheld device can be effectively used by the learner as an advanced TV remote controller for interactive T-Learning.	m2m (eclipse);mobile device;remote control;touchscreen	Tzu-Chih Chung;Li-Ya Tseng	2010	JDCTA		computer science;remote control	HCI	-41.66459599398853	-45.35103861093937	124324
ae780caca8f9dcb4be5c3f796334ef580b941dac	virtual pads: decoupling motor space and visual space for flexible manipulation of 2d windows within ves	3d interaction;control systems;speed accuracy tradeoff;application software;user preference virtual pad motor space decoupling visual space flexible manipulation 2d windows graphical user interface texture mapped rectangle visual representation;adaptive control;texture mapping;virtual reality;programmable control;two dimensional displays;user preferences;indexing terms;multimedia information system;multimedia systems;interactive devices graphical user interfaces;visualization;flexible manipulator;graphical user interfaces;visual representation;virtual reality application software graphical user interfaces control systems two dimensional displays space technology visualization programmable control adaptive control multimedia systems;space technology;information interfaces and presentation;interaction technique;interactive devices	The ability to access external 2D applications from within 3D worlds can greatly enhance the possibilities of many VE applications. In this paper we present a new interaction metaphor for fast, accurate and comfortable manipulation of external GUIs displayed as texture-mapped rectangles. The main idea is to decouple the motor space from the visual space so that the external application can be manipulated within a user-defined working volume whose location and size is completely independent from the application's visual representation. This decoupling is accomplished through a virtual pad which receives user actions and maps them into cursor movements. The main advantage of our approach is that both the working space and the visual space can be adjusted independently to suit user preferences. This allows users to seamlessly balance speed and accuracy without affecting the visual representation of the application's GUI. We have implemented an interaction technique adopting our metaphor in combination with a pointing technique and we have evaluated its effectiveness in terms of task performance and user preference. Our experiments indicate that the proposed technique increases user's comfort while providing dynamic management of speed/accuracy tradeoff	coupling (computer programming);cursor (databases);experiment;graphical user interface;interaction technique;map;microsoft windows;texture mapping;user (computing);workspace	Carlos Andújar;Ferran Argelaguet	2007	2007 IEEE Symposium on 3D User Interfaces	10.1109/3DUI.2007.340781	computer vision;simulation;human–computer interaction;computer science	Visualization	-43.2361209994352	-40.03266941957139	124342
5b794e673e923ef33a2e14b44973b90eac30e796	withyou - a communication system to provide out together feeling	communication system;wearable mobile and human robot interaction;pan tilt zoom;human robot interaction;communication support;tele presence	In this paper, we present a video-based communication system that provides an Out Together Feeling. In other words, it makes a pair of users, one outdoors and the other indoors, feel as if they are going outside together. To achieve this, it is important that both users (1) can freely peruse the outdoor user's surroundings, (2) can see what the outdoor user is looking at, and (3) can focus together on the same point. To realize these features, we have designed and implemented a system called WithYou. It consists of two subsystems: a wearable system for the outdoor user and an immersive space for the indoor user. The indoor user wears an HMD and watches video from a Pan/Tilt/Zoom camera mounted on the outdoor user's chest. Thus, the indoor user can look around by simply turning his/her head to the left or right. The orientation of the outdoor user's face is also displayed on the HMD screen to indicate where he/she is looking. In the preliminary test, both users experienced the Out Together Feeling to some extent.	head-mounted display;pan–tilt–zoom camera;wearable computer	Ching-Tzun Chang;Shin Takahashi;Jiro Tanaka	2012		10.1145/2254556.2254617	human–robot interaction;computer vision;simulation;computer science;artificial intelligence;multimedia;communications system	HCI	-43.19857785924723	-44.25683197205171	124403
7c18e615d22a8bc28401c7709e387a84328e82f1	a projection-based approach for real-time assessment and playability check for physics-based games		This paper introduces an authoring tool for physics-based puzzle games that supports game designers through providing visual feedback about the space of interactions. The underlying algorithm accounts for the type and physical properties of the different game components. An area of influence, which identifies the possible space of interaction, is identified for each component. The influence areas of all components in a given design are then merged considering the components’ type and the context information. The tool can be used offline where complete designs are analyzed and the final interactive space is projected, and online where edits in the interactive space are projected on the canvas in realtime permitting continuous assistance for game designers and providing informative feedback about playability.	airplane mode;algorithm;artificial intelligence;canvas element;entity;information;interaction;internet access;online and offline;real-time web;virtual world	Mohammad Shaker;Noor Shaker;Mohamed Abou-Zleikha;Julian Togelius	2015		10.1007/978-3-319-16549-3_35	simulation;video game design;dykstra's projection algorithm	HCI	-44.68199501048305	-38.986820619556745	124618
d22de8b34a11bcdb6612c4e48e40243d36e97c0d	a screen composition method for mobile interactive multi-display environment		To date, a mobile multi-screen system is emerged to satisfy customer's requirement for playing various interactive contents with bigger screen size. However, it is costful job to configure visual mapping for screen locations. To resolve this problem, we propose an intuitive composition method for an interactive multi-screen system. With proposed method, we present scenarios that compose a visual screen with multiple display devices which have various screen size.	display size	Sanghong Ahn;Hyeontaek Oh;Seokhyun Song;Jinhong Yang;Jun Kyun Choi	2015	2015 IEEE 4th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2015.7398594	simulation;engineering;multimedia;computer graphics (images)	Visualization	-45.336856331016634	-40.79276145417212	124699
48a273257f63032d71e9e7f52e7f014a867b3846	icenergy: augmented reality display for intuitive energy monitoring	smartphones;posture;mobile gaming;motion sensors	"""Energy saving is the main goal in most building energy monitoring applications. These systems, however, are operated by people. For this reason, an intuitive user interface is an essential element that will affect users' data understandability, thereby determining the system's usability. Traditional energy monitoring generally focuses on getting energy information by utilizing graphs or static text interfaces. However, these approaches are not related to the physical space. With the increase of information in energy monitoring systems, intuitive and efficient ways of displaying information are needed. In this demo, we present iCEnergy, a vision-based mobile information interface that provides power monitoring using augmented reality. Using existing system data, the system overlays an interactive """"energy cloud"""" over corresponding devices in order to illustrate information about the physical environment. This approach aims to provide users with a comfortable interaction experience through its intuitive information display."""	augmented reality;cloud computing;display device;usability;user interface	Shijia Pan;Bo Liu;Lin Zhang;Pei Zhang	2012		10.1145/2426656.2426719	embedded system;simulation;human–computer interaction;multimedia	HCI	-47.18496665950343	-41.02990623568789	124730
46df24189e184f8c19cff8e34412437b9b8e7b60	improving gui accessibility for people with low vision	low vision	We present UnWindows VI, a set of tools designed to assist low vision users of X Windows in effectively accomplishing two mundane yet critical interaction tasks: selectively magnifying areas of the screen so that the contents can be seen comfortably, and keeping track of the location of the mouse pointer. We describe our software from both the end user's and implementor's points of view, with particular emphasis on issues related to screen magnification techniques. We conclude with details regarding software availability and plans for future extensions.	accessibility;graphical user interface;microsoft windows;pointer (computer programming);pointer (user interface);x window system	Richard L. Kline;Ephraim P. Glinert	1995		10.1145/223904.223919	computer vision;human–computer interaction;computer science;computer graphics (images)	HCI	-45.9419578982824	-41.38344980070606	124803
6aa573cb0b019aa27577f4b8c584ea3f4e42c58c	enhanced pressure-based multimodal immersive experiences		Haptic feedback to the feet has been explored in the context of ground surface simulation, but existing solutions have generally not taken into account the effects of variable foot pressure during naturalistic stepping movements. We present here our approach to simulating the surface of a frozen pond, including ice cracking under increased foot pressure. This serves as a compelling example of a wearable mobile augmented foot-based surface simulator, whose response varies as a function of applied foot pressure. To enhance the illusion, we render multi-sensory feedback, comprising audio, visual, and haptic effects. We describe the hardware employed, the algorithm used to determine the distribution of foot pressure, and suggest potential possibilities for such a foot-pressure-based augmented reality system.	algorithm;augmented reality;feedback;haptic technology;multimodal interaction;password cracking;pedobarography;simulation;stepping level;wearable computer	Taeyong Kim;Jeremy R. Cooperstock	2018		10.1145/3174910.3174928	computer vision;simulation;immersion (virtual reality);wearable computer;artificial intelligence;haptic technology;computer science;augmented reality;illusion	HCI	-44.98330679802807	-49.85049096297613	124940
5fdbcf379a577378da883048c50c972e1a7d9fd5	a field study on spontaneous gaze-based interaction with a public display using pursuits	pursuits;smooth pursuit eye movement;public displays;pervasive displays;field study	Smooth pursuit eye movements were recently introduced as a promising technique for calibration-free and thus spontaneous and natural gaze interaction. While pursuits have been evaluated in controlled laboratory studies, the technique has not yet been evaluated with respect to usability in the wild. We report on a field study in which we deployed a game on a public display where participants used pursuits to select fish moving in linear and circular trajectories at different speeds. The study ran for two days in a busy computer lab resulting in a total of 56 interactions. Results from our study show that linear trajectories are statistically faster to select via pursuits than circular trajectories. We also found that pursuits is well perceived by users who find it fast and responsive.	computer lab;field research;interaction;kinetic data structure;spontaneous order;usability	Mohamed Khamis;Florian Alt;Andreas Bulling	2015		10.1145/2800835.2804335	computer vision;simulation;smooth pursuit;field research	HCI	-46.062120647621065	-43.68608983792438	125051
7bbd3ce5dd75cea2dcb3a4af01933ec2a718e8b5	designing for feel: contrasts between human and automated parametric capture of knob physics	model identification;knob physics;detent spacing parameterization;haptic i o;automobiles;estimation method;objective physical model identification method;software psychology;information technology;ergonomics improvement;human identification performance;indexing terms;mechanical reference knobs;physics;power plant;feedback;human factors;boilers;rendered knob model parameter;evaluation methodology;design guideline;physics vehicle dynamics human factors usability safety power generation boilers feedback automobiles ergonomics;automated parametric capture;identification;safety;identification ergonomics friction handles;detent strength;software psychology haptic i o evaluation methodology human factors;human identification;power generation;knob dynamics;physical interface;physical model;usability;friction;human experts estimation methods;handles;ergonomics;vehicle dynamics;ergonomics improvement automated parametric capture knob physics objective physical model identification method human identification performance mechanical reference knobs rendered knob model parameter inertia friction detent strength detent spacing parameterization automatic estimation method human experts estimation methods knob dynamics;automatic estimation method;inertia	We examine a crucial aspect of a tool intended to support designing for feel: the ability of an objective physical-model identification method to capture perceptually relevant parameters, relative to human identification performance. The feel of manual controls, such as knobs, sliders, and buttons, becomes critical when these controls are used in certain settings. Appropriate feel enables designers to create consistent control behaviors that lead to improved usability and safety. For example, a heavy knob with stiff detents for a power plant boiler setting may afford better feedback and safer operations, whereas subtle detents in an automobile radio volume knob may afford improved ergonomics and driver attention to the road. To assess the quality of our identification method, we compared previously reported automated model captures for five real mechanical reference knobs with captures by novice and expert human participants who were asked to adjust four parameters of a rendered knob model to match the feel of each reference knob. Participants indicated their satisfaction with the matches their renderings produced. We observed similar relative inertia, friction, detent strength, and detent spacing parameterizations by human experts and our automatic estimation methods. Qualitative results provided insight on users' strategies and confidence. While experts (but not novices) were better able to ascertain an underlying model in the presence of unmodeled dynamics, the objective algorithm outperformed all humans when an appropriate physical model was used. Our studies demonstrate that automated model identification can capture knob dynamics as perceived by a human, and they also establish limits to that ability; they comprise a step towards pragmatic design guidelines for embedded physical interfaces in which methodological expedience is informed by human perceptual requirements.	algorithm;behavior;control knob;embedded system;embedding;friction;hl7publishingsubsection <operations>;human factors and ergonomics;knob device component;potentiometer;power plants;requirement;spacing;system identification;usability	Colin Swindells;Karon E. MacLean;Kellogg S. Booth	2009	IEEE Transactions on Haptics	10.1109/TOH.2009.23	identification;power station;electricity generation;inertia;vehicle dynamics;simulation;index term;usability;system identification;physical model;computer science;engineering;artificial intelligence;human factors and ergonomics;friction;feedback;engineering drawing	HCI	-42.72388242614801	-48.16216703998464	125085
f9095fc1695c7d306bffbe787b74af9b34d0f8b2	efficient audio rendering using angular region-wise source enhancement for 360 $^{\circ }$ video		"""In virtual reality, 360<inline-formula><tex-math notation=""""LaTeX"""">$^{\circ }$</tex-math></inline-formula> video services provided through head-mounted displays or smartphones are widely available. Among these, some state-of-the-art devices are able to render varying auditory location of an object perceived by the user when the visual location of the object in the video moves along with the change of the user's looking direction. Nevertheless, an acoustic immersion technology that generates binaural sound to maintain a good match between the auditory and visual localization of an object in 360<inline-formula><tex-math notation=""""LaTeX"""">$^{\circ }$</tex-math> </inline-formula> video has not been studied sufficiently. This study focuses on an approach that synthesizes semibinaural sound being composed of <italic>virtual sources</italic> located in each angular region and the representative head related transfer functions of each angular region. To minimize the calculation cost on audio rendering and to reduce latency in downloading data from servers, the number of angular regions should be reduced while maintaining a good match between the auditory and visual localization of an object. In this paper, we investigate the minimum number of angular regions at which it is possible to maintain a good match by conducting subjective tests using a 360<inline-formula><tex-math notation=""""LaTeX"""">$^{\circ }$</tex-math></inline-formula> video viewing system composed of virtual images and sound sources. From the subjective tests, it was confirmed that the acoustic field should be divided into more than six equispaced angular regions so as to achieve natural auditory localization that matches an object's location in 360<inline-formula><tex-math notation=""""LaTeX"""">$^{\circ }$ </tex-math></inline-formula> video."""	acoustic coupler;acoustic cryptanalysis;angularjs;binaural beats;download;head-mounted display;immersion (virtual reality);smartphone;transfer function;virtual reality	Yusuke Hioka;Hisashi Uematsu	2018	IEEE Transactions on Multimedia	10.1109/TMM.2018.2829187	virtual image;visualization;computer science;artificial intelligence;computer vision;rendering (computer graphics);binaural recording;immersion (virtual reality);source separation;virtual reality;server	Visualization	-43.80095091474116	-40.45255970136823	125109
17959c2c25659fb292082b1db437bc49d55f3e18	persona-ization: searching on behalf of others		Many information retrieval tasks involve searching on behalf of others. Example scenarios include searching for a present to give a friend, trying to find “cool” clothes for a teenage child, looking for medical supplies for an elderly relative [1], or planning a group activity that many friends will enjoy. In this paper, we use demographically annotated web search logs to present a large-scale study of such “on behalf of” searches. We develop an exploratory technique for recognizing such searches, and present information to describe and understand the phenomenon, including the demographics of who is searching, who they are searching for and on what topics.	information retrieval;mozilla persona;web search engine	Paul N. Bennett;Emre Kiciman	2015			medicine;artificial intelligence;genealogy;world wide web	HCI	-35.91599827580973	-49.78127426090247	125157
e810c66a73d37318aaac8b20fe55c6e2a53f5646	semantic transcoding: making the handicapped and the aged free from their barriers in obtaining information on the web		Transcoding refers to the process of converting HTML files in order to adapt the physical attributes of each device. We call our transcoding “Semantic Transcoding” because it transcodes at the deep semantic level. The proxy server consists of translation, text summarization, video summarization, text-to-speech, speech recognition, and so on. External annotation is also possible. Semantic transcoding enables the visually challenged to browse Web pages by changing text to speech, and to understand the pictures by reading external annotated comments. The hearing impaired are provided with superimposed dialog generated by the recognition of speech and scene. Furthermore, we will propose a rapid “diagonal” reading method by listening to speech converted from text on the basis of linguistic information such as syntax, new and old information for the blind and visually impaired in the internet speech browser realized by semantic transcoding.	automatic summarization;browsing;html;proxy server;server (computing);speech recognition;speech synthesis;user (computing);world wide web;dialog	Shinichi Torihara;Katashi Nagao	2000			world wide web;computer science;multimedia;transcoding	NLP	-35.05720990648324	-47.57558313490344	125257
62b137180eb6aff01cb791b3cd06abcebd3586e4	authentication analysis using input gestures in touch-based mobile devices		The smartphones have become the basic necessity of life. Users are being authenticated on mobile devices using PINs, password, swipe, etc. However, the existing authentication mechanisms are not resilient against modern security attacks. With the increase of touch devices, gesture-based authentication behaviour becomes more important. This paper analyzes the distinctness of a gesture in a touch base mobile device. Analysis show that specific user has distinct gesture of different fingers. Experimental study reveals that finger accuracy in a gesture-based authentication is increased by an individual's index finger and thumbs. Furthermore, the results show that the accuracy and efficiency of a gesture-based lock depends on the phone's position, portrait mode, left or right-handed user, and single or double hand user style.	authentication;mobile device;password;personal identification number;smartphone	Anwar Ur Rehman;Muhammad Awais;Munam Ali Shah	2017	2017 23rd International Conference on Automation and Computing (ICAC)	10.23919/IConAC.2017.8082062	password;engineering;control engineering;index finger;swipe;gesture recognition;phone;computer hardware;internet privacy;mobile device;authentication;gesture	HCI	-45.14687673316522	-45.00064980668177	125607
587c06c2cda1bb3e6a8011c15a3c5f1e98984587	repro3d: full-parallax 3d display using retro-reflective projection technology	3d interaction;object interaction;3d imaging;infra red projection;tabletop system;motion parallax;3d display	Motion parallax is important to recognize the depth of a 3D image. In recent years, many 3D display methods that enable parallax images to be seen with the naked eye have been developed. In addition, there has been an increase in research to design interfaces that enable humans to intuitively interact with and operate 3D objects using their hands. However, realizing 3D object interaction as if the user is actually touching the object in the real world is quite difficult. One of the reasons for this is that the screen shape in conventional methods is restricted to a flat panel. In addition, it is difficult to achieve a balance between displaying the 3D image and sensing the user input. Therefore, we propose a novel full-parallax 3D display system that is suitable for interactive 3D applications. We call this system RePro3D. Our approach is based on a retro-reflective projection technology[Inami et al. 2000]. A number of images from a projector array are projected onto the retro-reflective screen. When a user looks at the screen through a half mirror, he or she, without the use of glasses, can view a 3D image that has motion parallax. We can choose the screen shape depending on the application. Image correction according to the screen shape is not required. Consequently, we can design a touch-sensitive soft screen, a complexly curved screen, or a screen with an automatically moving surface. RePro3D has a sensor function to recognize the user input. Some interactive features, such as operation of 3D objects, can be achieved by using it.	curved screen;flat panel display;interactivity;parallax barrier;stereo display;stereoscopy;touchscreen;video projector	Takumi Yoshida;Sho Kamuro;Kouta Minamizawa;Hideaki Nii;Susumu Tachi	2010		10.1145/1836821.1836841	parallax;parallax barrier;stereoscopy;computer vision;simulation;stereo display;computer graphics (images)	HCI	-42.28195010196626	-39.69512656842912	125804
704b0d905fda7ac516ab6caee95e5be06f3d2a45	an ir local positioning system for smart items and devices	optical distortion;audio centric user terminal;nonlinear optics;cameras optical distortion nonlinear distortion calibration lenses pixel space technology geometrical optics nonlinear optics solid modeling;stationary mounted stereo camera;nonlinear distortion;optical tracking;pixel;solid modeling;lenses;positioning system;ir local positioning system;local positioning system;space technology;infrared;mobile computing;off the shelf;calibration;intelligent sensors;cameras;tracking;audio centric user terminal ir local positioning system stationary mounted stereo camera tracking;geometrical optics;intelligent sensors mobile computing optical tracking	This paper describes IRIS-LPS (InfraRed Indoor Scout), an optical infrared local positioning system. The tracked objects carry active tags that emit infrared signals which are received by a stationary mounted stereo-camera. The system is based on cheap off-the-shelf components, is easy to deploy, and features a large range of coverage. It is capable of tracking a large number of tags without significant performance impact, since the sampling rate remains constant with an increasing number of tags. A test installation of the system has been evaluated in a lecture hall. The positioning system is utilized by our audio-centric user terminal called Talking Assistant. This and other application scenarios are described at the end.	hybrid system;lightweight portable security;positioning system;sampling (signal processing);scout;stationary process;stereo camera;ubiquitous computing	Erwin Aitenbichler;Max Mühlhäuser	2003		10.1109/ICDCSW.2003.1203576	nonlinear optics;geometrical optics;computer vision;nonlinear distortion;calibration;infrared;local positioning system;lens;tracking;space technology;solid modeling;mobile computing;pixel;intelligent sensor	Mobile	-41.38513109164746	-42.49359833282986	126289
f4dbcab636f2ee03466f32a27cebb5777629aa5d	isee: an android application for the assistance of the visually impaired		Smart phone technology and mobile applications have become an indispensable part of our daily life. The primary use however, is targeted towards social media and photography. While some camera-based approaches provided partial solutions for the visually impaired, they still constitute a cumbersome process for the user. iSee is an Android based application that benefits from the commercially available technology to help the visually impaired people improve their day-to-day activities. A single screen tap in iSee is able to serve as a virtual eye by providing a sense of seeing to the blind person by audibly communicating the object(s) names and description. iSee employs efficient object recognition algorithms based on FAST and BRIEF. Implementation results are promising and allow iSee to constitute a basis for more advanced applications.		Milad Ghantous;Michel Nahas;Maya Ghamloush;Maya Rida	2014		10.1007/978-3-319-13461-1_4	simulation;human–computer interaction;engineering;multimedia	HCI	-47.364358264616406	-41.57618561846247	126331
764193fc925e817a3afe5930e1399268c9eaa136	quick bootstrapping of a personalized gaze model from real-use interactions		Understanding human visual attention is essential for understanding human cognition, which in turn benefits human--computer interaction. Recent work has demonstrated a Personalized, Auto-Calibrating Eye-tracking (PACE) system, which makes it possible to achieve accurate gaze estimation using only an off-the-shelf webcam by identifying and collecting data implicitly from user interaction events. However, this method is constrained by the need for large amounts of well-annotated data. We thus present fast-PACE, an adaptation to PACE that exploits knowledge from existing data from different users to accelerate the learning speed of the personalized model. The result is an adaptive, data-driven approach that continuously “learns” its user and recalibrates, adapts, and improves with additional usage by a user. Experimental evaluations of fast-PACE demonstrate its competitive accuracy in iris localization, validity of alignment identification between gaze and interactions, and effectiveness of gaze transfer. In general, fast-PACE achieves an initial visual error of 3.98 degrees and then steadily improves to 2.52 degrees given incremental interaction-informed data. Our performance is comparable to state-of-the-art, but without the need for explicit training or calibration. Our technique addresses the data quality and quantity problems. It therefore has the potential to enable comprehensive gaze-aware applications in the wild.	aggregate data;cognition;data acquisition;data quality;data validation;eye tracking;human–computer interaction;interactive computing;personalization;tracking system;webcam	Michael Xuelin Huang;Jiajia Li;Grace Ngai;Hong Va Leong	2018	ACM TIST	10.1145/3156682	machine learning;gaze;data mining;artificial intelligence;data validation;bootstrapping;exploit;computer science;cognition;data quality	HCI	-37.17558708155553	-47.6017849574747	126363
cc67cb5a338cd96f114097a0c5143575be621922	finger tracking for the digital desk	motion analysis;mouse;input device tracking;electrical capacitance tomography;user s hand;input device;mice;fitts law;pervasive computing;motion estimation interactive devices augmented reality tracking real time systems active vision;real time;virtual reality;motion estimation;natural interaction;user transparent hardware;augmented reality environments;fitts law digital desk real time finger tracking natural interaction user transparent hardware ubiquitous computing input device tracking user s fingertip mouse augmented reality environments vision motion analysis user s hand;fingers electrical capacitance tomography computer displays mice pervasive computing cameras ubiquitous computing virtual reality augmented reality computer science;real time finger tracking;user s fingertip;computer displays;fingers;ubiquitous computing;computer science;finger tracking;augmented reality;digital desk;vision;cameras;tracking;interactive devices;active vision;real time systems	A trend in computing environments today is to move towards more ‘natural’ interaction, another is to make hardware invisible to the user. Both these ideas converge into ubiquitous computing the Digital Desk is an example of this idea. In this paper we concentrate on an input device for the Digital Desk, namely the user’s fingertip, which is made to act like a mouse. Tracking such an input device is common to a number of augmented reality environments and involves vision and motion analysis. However, previous attempts have focused more on the vision aspect of tracking general objects than on using the information already known about the user’s hand, which is the approach taken here. We adopted the goal of tracking the user’s fingertip as fast as possible in real-time so the system could be compared with other input devices, using models such as Fitts’ Law. Our system is shown to comply with the law adequately.	augmented reality;computer mouse;converge;finger tracking;fitts's law;input device;real-time clock;ubiquitous computing	Thomas Brown;Richard C. Thomas	2000		10.1109/AUIC.2000.822058	vision;computer vision;finger tracking;simulation;active vision;human–computer interaction;computer science;motion estimation;tracking;fitts's law;ubiquitous computing;input device;computer graphics (images)	HCI	-42.088715131144454	-39.14530169716444	126372
18c2323b2641143810f074fc816f3eed116ed7f6	development of haptic device using flexible sheet			haptic technology	Kenji Inoue;Reiko Uesugi;Tatsuo Arai;Yasushi Mae	2003	JRM	10.20965/jrm.2003.p0121	computer hardware;haptic technology;computer science	HCI	-42.794742518808356	-41.22175706090891	126516
b2c2277fd8cdea79c2f3fa87bc508c789085e4fd	beats: tapping gestures for smart watches	smartwatch;wearable;tapping;multitouch;qa76 computer software	Interacting with smartwatches poses new challenges. Although capable of displaying complex content, their extremely small screens poorly match many of the touchscreen interaction techniques dominant on larger mobile devices. Addressing this problem, this paper presents beating gestures, a novel form of input based on pairs of simultaneous or rapidly sequential and overlapping screen taps made by the index and middle finger of one hand. Distinguished simply by their temporal sequence and relative left/right position these gestures are designed explicitly for the very small screens (approx. 40mm square) of smartwatches and to operate without interfering with regular single touch input. This paper presents the design of beating gestures and a rigorous empirical study that characterizes how users perform them -- in a mean of 355ms and with an error rate of 5.5%. We also derive thresholds for reliably distinguishing between simultaneous (under 30ms) and sequential (under 400ms) pairs of screen touches or releases. We then present five interface designs and evaluate them in a qualitative study in which users report valuing the speed and ready availability of beating gestures.	approximation;interaction technique;mobile device;smartwatch;touchscreen	Ian Oakley;Doyoung Lee;Md. Rasel Islam;Augusto Esteves	2015		10.1145/2702123.2702226	simulation;wearable computer;computer hardware;computer science;smartwatch	HCI	-46.48150368288374	-45.01298514212208	126657
e8a771ec9d0c16a22e43f1780ffa8ce4cbb472b7	rfig lamps: interacting with a self-describing world via photosensing wireless tags and projectors	projector;projected information;radio frequency identity;augmented reality;rfig lamp;human-machine communication;wireless tag;image stabilization;direct projection;self-describing world;enabling technology;interactive projection;user annotation;tag data;active radio frequency;radio frequency identification;unpowered passive-rfid;stucture from motion	This paper describes how to instrument the physical world so that objects become self-describing, communicating their identity, geometry, and other information such as history or user annotation. The enabling technology is a wireless tag which acts as a radio frequency identity and geometry (RFIG) transponder. We show how addition of a photo-sensor to a wireless tag significantly extends its functionality to allow geometric operations - such as finding the 3D position of a tag, or detecting change in the shape of a tagged object. Tag data is presented to the user by direct projection using a handheld locale-aware mobile projector. We introduce a novel technique that we call interactive projection to allow a user to interact with projected information e.g. to navigate or update the projected information.The ideas are demonstrated using objects with active radio frequency (RF) tags. But the work was motivated by the advent of unpowered passive-RFID, a technology that promises to have significant impact in real-world applications. We discuss how our current prototypes could evolve to passive-RFID in the future.	interaction;movie projector;self-documenting code;shader lamps	Ramesh Raskar;Paul A. Beardsley;Jeroen van Baar;Yao Wang;Paul H. Dietz;Johnny C. Lee;Darren Leigh;Thomas Willwacher	2005		10.1145/1198555.1198717	handheld projector;radio-frequency identification;computer graphics (images);computer vision;projector;wireless;augmented reality;artificial intelligence;mobile device;transponder (aeronautics);radio frequency;computer science	HCI	-44.00909413090462	-40.68215088913822	126871
776b3f262dd0cd913aa40a5a81e200081f6d1532	multi-layered interfaces to improve older adults' initial learnability of mobile applications	negative affect;mobile device;menu design;user study;smart phone;mobile computer;older adult;controlled experiment;age related differences;older adults;multi layered interfaces;mobile application;mobile devices;learnability	Mobile computing devices can offer older adults (ages 65+) support in their daily lives, but older adults often find such devices difficult to learn and use. One potential design approach to improve the learnability of mobile devices is a Multi-Layered (ML) interface, where novice users start with a reduced-functionality interface layer that only allows them to perform basic tasks, before progressing to a more complex interface layer when they are comfortable. We studied the effects of a ML interface on older adults’ performance in learning tasks on a mobile device. We conducted a controlled experiment with 16 older (ages 65--81) and 16 younger participants (age 21--36), who performed tasks on either a 2-layer or a nonlayered (control) address book application, implemented on a commercial smart phone. We found that the ML interface’s Reduced-Functionality layer, compared to the control’s Full-Functionality layer, better helped users to master a set of basic tasks and to retain that ability 30 minutes later. When users transitioned from the Reduced-Functionality to the Full-Functionality interface layer, their performance on the previously learned tasks was negatively affected, but no negative impact was found on learning new, advanced tasks. Overall, the ML interface provided greater benefit for older participants than for younger participants in terms of task completion time during initial learning, perceived complexity, and preference. We discuss how the ML interface approach is suitable for improving the learnability of mobile applications, particularly for older adults.	learnability;mobile app;mobile computing;mobile device;smartphone	Rock Leung;Leah Findlater;Joanna McGrenere;Peter Graf;Justine Yang	2010	TACCESS	10.1145/1838562.1838563	simulation;human–computer interaction;computer science;operating system;mobile device;multimedia;mobile computing	HCI	-47.42764805960017	-45.282908958425566	126951
4ad953bb67848c80d00641ac9668ddbda8ce113b	headgesture: hands-free input approach leveraging head movements for hmd devices		We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.		Yukang Yan;Chun Yu;Xin Yi;Yuanchun Shi	2018	IMWUT	10.1145/3287076	gesture recognition;human–computer interaction;feature extraction;optical head-mounted display;virtual reality;space exploration;unification;segmentation;computer science;gesture	HCI	-45.68763809727377	-44.810483726335804	127007
47fdd65cea3c67339cc4c43e9114e4d2d2be7c4f	the design of tools for sketching sensor-based interaction	design tool;wireless;pervasive computing;ease of use;tangible interaction design;audio;sensor based interaction;wearable computer;wearable computing;tangible interaction;embodied interaction	In this paper we briefly motivate, present, and give an initial evaluation of DUL Radio, a small wireless toolkit for sketching sensor-based interaction. In the motivation, we state the purpose of this specific platform, which aims to balance ease-of-use (learning, setup, initialization), size, speed, flexibility and cost, aimed at wearable and ultra-mobile prototyping where fast reaction is needed (e.g. in controlling sound), and we mention general issues facing this category of embodied interaction design tools. We briefly present the platform, both regarding hardware and software. In the evaluation, we present our experiences with the platform both in design projects and in teaching. We conclude that DUL Radio was the preferred platform for sketching sensor-based interaction compared to other solutions, and that it does seem to be a relatively easy-to-use tool, but that there are many ways to improve it. Target users include designers, students, artists etc. with minimal programming and hardware skills, but this paper addresses the issues with designing the tools, which includes some technical considerations.	computer hardware;dialup users list;interaction design;sensor;wearable computer	Martin Brynskov;Rasmus B. Lunding;Lasse Steenbock Vestergaard	2012		10.1145/2148131.2148178	simulation;human–computer interaction;engineering;multimedia	HCI	-48.09937534957023	-40.2204417286489	127033
280669ffe5cda497b11423f69871f830f76d03d3	creating a common operation picture in realtime with user-centered interfaces for mass casualty incidents	lead;ems;user interfaces;irrigation;scalability;psychology	Accurate, accessible, and realtime information on the number, location, and medical condition of patients are critical for the successful management of mass casualty incidents (MCIs), where the number of patients exceeds the capacity of the emergency management service (EMS). We present a concept of a collaborative infrastructure which generates a common operation picture in realtime. A complex, stressful and uncommon situation like an MCI creates strong psychological influences and burdens on the rescue workers. Based on our psychological findings we derived eleven special requirements for efficient and intuitive user interfaces in unstable, time-critical emergency situations. Taking the requirements into consideration we developed a concept to overcome the MCI through the combination of multiple devices. The devices are carefully chosen according to the task of the EMS personnel in the field as well as in the incident command post. Three different interfaces - PDAs for the rescue units in the field, tablet PCs for the incident commanders and a multitouch table in the incident command post - help the entire rescue team to gain efficient situational awareness.	control theory;multi-touch;personal digital assistant;requirement;rugged computer;tablet computer;user interface;user-centered design;vii;window of opportunity	Eva Artinger;Patrick Maier;Tayfur Coskun;Simon Nestler;Mareike Mähler;Yeliz Yildirim-Krannig;Fabian Wucholt;Florian Echtler;Gudrun Klinker	2012	2012 6th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth) and Workshops		embedded system;lead;scalability;simulation;human–computer interaction;computer science;operating system;irrigation;user interface;computer security	HCI	-48.216965428770294	-46.80817539774647	127171
19d2e7fa56b8766e22949744866dede779038c4e	virtual locomotion system for human-scale virtual environments	virtual locomotion;human scale;turntable;computer vision;pressure sensor;visual feedback;virtual environment;surrounding projection	This paper presents a new virtual locomotion interface based on step-in-place action and a smart-turntable system. The interface provides a turntable as walking platform, on top of which users will stand at its center, and facing a large screen, to perform life-like walking actions that steer their navigation through the virtual environment. Steering actions are tracked seamlessly without attachment to the body through a set of pressure sensors embedded within the turntable and a computer vision system. For instance, in place stepping is treated as a gesture indicating the intention to move forward. Rotation about the body's vertical axis is treated as a gesture changing the walking direction. However, as large screens are usually limited in size and do not allow a surrounding projection, a large turning action may put users in a visual-less situation, which hamper considerably the effectiveness of the walking experience. To avoid such case and keep users always provided with sufficient visual feedback, the turntable will passively and smoothly rotate in opposite direction of users' turning. Rotation speed and acceleration of the turntable are well optimized to keep users well balanced and easily withstand the passive rotation. The interface is shown to be easy and simple to use in virtual environments equipped with large screen.	attachments;computer vision;embedded system;gesture recognition;in-place algorithm;optic axis of a crystal;sensor;smoothing;stepping level;virtual reality	Laroussi Bouguila;Masahiro Ishii;Makoto Sato	2002		10.1145/1556262.1556299	computer vision;simulation;computer science;virtual machine;operating system;pressure sensor;computer graphics (images)	HCI	-44.502886562706806	-48.38176195602664	127192
8f49cf19202da7e150dfa09319b823e663a5d617	factors influencing operator interaction with virtual objects viewed via head-mounted see-through displays: viewing conditions and rendering latency	visually guided manual assembly;visual requirements;computer graphics;biocular display;system latency;stereo viewing;virtual reality;viewing conditions;system response latency;head referenced cursor;aircraft wire harnesses;experimental tasks;computer displays rendering computer graphics assembly delay wire computer graphics three dimensional displays laboratories aircraft hardware;wire;assembly;operator interaction;adverse effect;virtual paths;viewing difficulty;computer generated nearby wire like virtual objects;human factors;three dimensional displays;human factors virtual reality interactive devices three dimensional displays interactive systems rendering computer graphics;stereo disparity cues;computer displays;path complexity;high precision tracing;ring shaped virtual objects;system latency operator interaction head mounted see through displays viewing conditions rendering latency see through format computer generated nearby wire like virtual objects visual requirements experimental tasks visually guided manual assembly aircraft wire harnesses head referenced cursor stereo viewing viewing difficulty biocular display stereo disparity cues ring shaped virtual objects virtual paths path complexity system response latency high precision tracing;rendering computer graphics;head mounted see through displays;interactive systems;see through format;virtual path;aircraft;interactive devices;hardware;rendering latency	A head-mounted visual display was used in a see-through format to present computer generated, space-stabilized, nearby wire-like virtual objects to 14 subjects. The visual requirements of their experimental tasks were similar to those needed for visually-guided manual assembly of aircraft wire harnesses. In the first experiment subjects visually traced wire paths with a head-referenced cursor, subjectively rated aspects of viewing, and had their vision tested before and after monocular, biocular, or stereo viewing. Only the viewing dificulty with the biocular display was adversely effected by the visual task. This viewing dtfficulty is likely due to conflict between looming and stereo disparity cues. A second experiment examined the precision with which operators could manually move ringshaped virtual objects over virtual paths without collision. Accuracy of per$ormance was studied as a function of required precision, path complexity, and system response latency. Results show that high precision tracing is most sensitive to increasing latency. Ring placement with less than 1.8 cm precision will require system latency less than 50 msec before asymptotic per$ormance is found.	binocular disparity;cursor (databases);requirement	Stephen R. Ellis;F. Breant;B. Manges;Richard H. Jacoby;Bernard D. Adelstein	1997		10.1109/VRAIS.1997.583063	computer vision;simulation;adverse effect;computer science;assembly;virtual reality;computer graphics;computer graphics (images)	Visualization	-43.30292471142339	-47.753802200865906	127333
67fae72c0180697ee69ebdb94ddf6470afbf5760	spotlight: directing users' attention on large displays	user study;attention;field of view;large displays;visual attention;interaction technique	We describe a new interaction technique, called a spotlight, for directing the visual attention of an audience when viewing data or presentations on large wall-sized displays. A spotlight is simply a region of the display where the contents are displayed normally while the remainder of the display is somewhat darkened. In this paper we define the behavior of spotlights, show unique affordances of the technique, and discuss design characteristics. We also report on experiments that show the benefit of using the spotlight a large display and standard desktop configuration. Our results suggest that the spotlight is preferred over the standard cursor and outperforms it by a factor of 3.4 on a wall-sized display.	cursor (databases);dbpedia;desktop computer;experiment;interaction technique	Azam Khan;Justin Matejka;George W. Fitzmaurice;Gordon Kurtenbach	2005		10.1145/1054972.1055082	computer vision;attention;field of view;human–computer interaction;computer science;multimedia;interaction technique;computer graphics (images)	HCI	-44.594425380365756	-46.13921999160747	127443
2283456dd3e1c17c1375517c0c458cf6bb98587d	wearable gesture control of agile micro quadrotors		Quadrotor unmanned aerial vehicles (UAVs) have seen a surge of use in various applications due to its structural simplicity and high maneuverability. However, conventional control methods using joysticks prohibit novices from getting used to maneuvering quadrotors in short time. In this paper, we suggest the use of a wearable device, such as a smart watch, as a new remote-controller for a quadrotor. The user's command is recognized as gestures using the 9-DoF inertial measurement unit (IMU) of a wearable device through a recurrent neural network (RNN) with long short-term memory (LSTM) cells. Our implementation also makes it possible to align the heading of a quadrotor with the heading of the user. Our implementation allows nine different gestures and the trained RNN is used for real-time gesture recognition for controlling a micro quadrotor. The proposed system exploits available sensors in a wearable device and a quadrotor as much as possible to make the gesture-based control intuitive. We have experimentally validated the performance of the proposed system by using a Samsung Gear S smart watch and a Crazyflie Nano Quadcopter.	aerial photography;agile software development;align (company);artificial neural network;course (navigation);experiment;gnu nano;gesture recognition;joystick;long short-term memory;natural user interface;random neural network;real-time transcription;recurrent neural network;samsung gear;sensor;server (computing);smartphone;smartwatch;unmanned aerial vehicle;wearable technology	Yunho Choi;Inhwan Hwang;Songhwai Oh	2017	2017 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)	10.1109/MFI.2017.8170439	computer vision;gesture recognition;joystick;artificial intelligence;embedded system;wearable computer;computer science;smartwatch;inertial measurement unit;recurrent neural network;quadcopter;gesture	Robotics	-37.961021191254595	-45.23109966591145	127471
8150f267cd2852f27639d4d85c3a311360346c88	salient montages from unconstrained videos	video summarization;video saliency detection	We present a novel method to generate salient montages from unconstrained videos, by finding “montageable moments” and identifying the salient people and actions to depict in each montage. Our method addresses the need for generating concise visualizations from the increasingly large number of videos being captured from portable devices. Our main contributions are (1) the process of finding salient people and moments to form a montage, and (2) the application of this method to videos taken “in the wild” where the camera moves freely. As such, we demonstrate results on head-mounted cameras, where the camera moves constantly, as well as on videos downloaded from YouTube. Our approach can operate on videos of any length; some will contain many montageable moments, while others may have none. We demonstrate that a novel “montageability” score can be used to retrieve results with relatively high precision which allows us to present high quality montages to users.	digital camera;display resolution;head-mounted display;high- and low-level;ibm notes;mobile device;montagejs;wearable technology	Min Sun;Ali Farhadi;Ben Taskar;Steven M. Seitz	2014		10.1007/978-3-319-10584-0_31	computer vision;computer science	Vision	-34.88093576873526	-43.59652123482963	127497
90cdd21ecd5bee6ca86ce5392350a5ced41c50c3	freedom of movement: generative responses to motion control				Kate Compton;Michael Mateas	2018				ML	-35.799092902436584	-39.359846897168545	127652
12eae0d433f39f830fa4c75bb170fe85e4fb13b4	3d contour perception for flow visualization	depth cues;stereo viewing;depth perception;flow field;data visualization;motion parallax;flow visualization	One of the most challenging problems in data visualization is the perception of 3D flow fields because judging the orientation of 3D paths is perceptually difficult. It is hypothesized that perception of the orientations of streamlines in space can greatly benefit from stereoscopic depth cues and motion parallax. In addition, because stereoscopic depth perception is a super-acuity and relies on such factors as small-scale stereoscopic disparity gradient based on texture, stereoscopic depth judgments will be exceptionally sensitive to display quality. In conventional displays, the aliasing of pixels gives completely spurious texture information to the mechanisms of stereoscopic depth perception. We carried out a study to evaluate the importance of 3D cues in perceiving the orientation of curved contours. The result showed that stereo and motion cues are essential to perceiving the orientation of 3D lines. If, however, the contours are rendered as shaded tubes, good orientation information is available even without stereo or motion depth cues.	aliasing;binocular disparity;data visualization;depth perception;gradient;parallax;pixel;shading;stereopsis;stereoscopy	Colin Ware	2006		10.1145/1140491.1140512	computer vision;depth perception;multimedia;optics;data visualization;kinetic depth effect;computer graphics (images)	HCI	-43.15644176726698	-49.469265913981545	127689
a511c87f8a6af96d14aa45ef3ea93ed6c11d4102	issues of controlling public kiosks and other self service machines using gesture recognition	keyboards;mice;motion control;static model information;motion pictures;real time;airports;dynamic system;keyboards displays mice speech recognition multimedia systems internet educational institutions motion pictures airports motion control;multimedia systems;linear in parameters dynamic system public kiosks self service machines gesture recognition static model information;linear in parameters dynamic system;internet;self service machines;displays;security of data gesture recognition;speech recognition;public kiosks;security of data;gesture recognition	We discuss the issues involved in controlling self service machines via gestures composed of both static symbols and dynamic motions. Each gesture is modeled from either static model information or a linear-in-parameters dynamic system. Recognition occurs in a real-time environment using a small amount of processing time and memory. We will examine which gestures are appropriate, how the gestures can be recognized, and which commands the gestures should control.	gesture recognition	Charles J. Cohen;Glenn J. Beach;George V. Paul;Jay Obermark;Gene Foulk	1998		10.1109/ICSMC.1998.728074	motion control;computer vision;the internet;computer science;dynamical system;gesture recognition;multimedia	HCI	-36.39184261214981	-43.92039725983987	127854
332066e3523e28c3c8eddb0a016900f251be64cc	hssim: an objective haptic quality assessment measure for force-feedback signals		Recent advances in haptic communication cast light onto the promise of full immersion into remote real or virtual environments. The quality of compressed haptic signals is crucial to fulfill this promise. Traditionally, the quality of haptic signals is evaluated through a series of subjective experiments. So far, only very limited attention was directed toward developing objective quality measures for haptic communication. As touch applications continue to grow, the need for efficient haptic quality assessment (HQA) measures is becoming more pronounced. In this work, we attempt to pragmatically approach this problem inspired by recent advances in the visual domain. More specifically, we design a HQA measure based on our knowledge of human haptic perception represented by Stevens power law and a similarity comparison between a reference and a compressed haptic force-feedback signal represented by the SSIM index as a full-reference quality measure. The proposed HSSIM measure provides clear advantages over existing approaches. We validate the performance of the proposed HQA measure with subjective experiments which show high correlation with human quality assessment results.	distortion;experiment;feedback;haptic technology;immersion (virtual reality);internet protocol suite;lossy compression;network packet;nonlinear system;real-time clock;similarity measure;structural similarity;virtual reality	Rania Hassen;Eckehard G. Steinbach	2018	2018 Tenth International Conference on Quality of Multimedia Experience (QoMEX)	10.1109/QoMEX.2018.8463365	computer vision;structural similarity;haptic technology;artificial intelligence;haptic communication;haptic perception;immersion (virtual reality);distortion;stevens' power law;computer science	Visualization	-43.21707849554792	-50.51551675794329	127911
0efaf0b1088019262a27b9b2cb7f7add90c96727	improvement of the design quality of 3d-input devices using motion analyses and biomechanical comparisons	reachable space of motion;input device;3d input;evaluation software;ergonomically motivated equipment;design quality;motion analyses of input tasks;motion capture;user centred design;3d input device	Due to the lack of investigations and standards describing the design of real 3D input devices a real 3D input device was develop. To compare seve ral devices a test task was created and performed with combination of a motion capturing system. During the experiment 19 attendees with different levels of experience performed the test with this setup. Several intra-individual motion patterns and using strategies belonging to different input devices could be observed.		Tobias Nowack;Stefan Lutherdt;Manuel Möller;Peter Kurtz;Hartmut Witte	2009		10.1007/978-3-642-02731-4_33	control engineering;real-time computing;simulation;computer science	HCI	-46.6994497079228	-47.71328662393523	128220
fbc63a7def9dcdd3f26dc4526a1babbc2634ccf2	a multimodal air gesture interface for in vehicle menu navigation	multimodal feedback;in vehicle interaction;infotainment;hand gesture recognition;auditory feedback	Multimodal and visual-only air gesture systems for navigating menus in the vehicle were developed and compared to a conventional direct touch system in a driving simulator using various distraction metrics. Participants using the multimodal air gesture system exhibited safer secondary task dwell patterns, but took longer to complete tasks and reported higher workload compared to the touch system.	driving simulator;gesture recognition;multimodal interaction;simulation	Keenan R. May;Thomas M. Gable;Bruce N. Walker	2014		10.1145/2667239.2667280	computer vision;simulation;engineering;gesture recognition;communication	HCI	-46.670470310967545	-46.89326371085593	128384
c893f3912174ddd24dbd97bf9342182f1a8b6dd0	boosthand : distance-free object manipulation system with switchable non-linear mapping for augmented reality classrooms		In this paper, we propose BoostHand, a freehand, distance-free object-manipulation system that supports simple trigger gestures using Leap Motion. In AR classrooms, it is necessary to allow both lecturers and students to utilize virtual teaching materials without any spatial restrictions, while handling virtual objects easily, regardless of distance. To provide efficient and accurate methods of handling AR classroom objects, our system requires only simple intuitive freehand gestures to control the users virtual hands in an enlarged, shared control space of users. We modified the GoGo interaction technique [5] by adding simple trigger gestures, and we evaluated performance against gaze-assisted selection (GaS) capabilities. Our proposed system enables both lecturers and students to utilize virtual teaching materials easily from their remote positions.	adobe freehand;augmented reality;experiment;interaction technique;microsoft hololens;user interface	Whie Jung;Woojin Cho;Hayun Kim;Woontack Woo	2017	2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)	10.1109/ISMAR-Adjunct.2017.96	artificial intelligence;computer science;multimedia;free object;computer vision;human–computer interaction;interaction technique;computer graphics (images);nonlinear system;augmented reality;gesture	Visualization	-43.382400368249414	-38.746136153004784	128479
a0a9efbe34679fb47391d62979e6efcd34617ae4	context-based visual feedback recognition	electrical engineering and computer science;thesis	During face-to-face conversation, people use visual feedback (e.g., head and eye gesture) to communicate relevant information and to synchronize rhythm between participants. When recognizing visual feedback, people often rely on more than their visual perception. For instance, knowledge about the current topic and from previous utterances help guide the recognition of nonverbal cues. The goal of this thesis is to augment computer interfaces with the ability to perceive visual feedback gestures and to enable the exploitation of contextual information from the current interaction state to improve visual feedback recognition. We introduce the concept of visual feedback anticipation where contextual knowledge from an interactive system (e.g. last spoken utterance from the robot or system events from the GUI interface) is analyzed online to anticipate visual feedback from a human participant and improve visual feedback recognition. Our multi-modal framework for context-based visual feedback recognition was successfully tested on conversational and non-embodied interfaces for head and eye gesture recognition. We also introduce Frame-based Hidden-state Conditional Random Field model, a new discriminative model for visual gesture recognition which can model the substructure of a gesture sequence, learn the dynamics between gesture labels, and can be directly applied to label unsegmented sequences. The FHCRF model outperforms previous approaches (i.e. HMM, SVM and CRF) for visual gesture recognition and can efficiently learn relevant contextual information necessary for visual feedback anticipation. A real-time visual feedback recognition library for interactive interfaces (called Watson) was developed to recognize head gaze, head gestures, and eye gaze using the images from a monocular or stereo camera and the context information from the interactive system. Watson was downloaded by more then 70 researchers around the world and was successfully used by MERL, USC, NTT, MIT Media Lab and many other research groups. Thesis Supervisor: Trevor Darrell Title: Associate Professor		Louis-Philippe Morency	2006			computer science;electrical engineering;artificial intelligence	HCI	-35.050735584526606	-42.4004798646394	128553
18a24a02ef510c94d2b1cb319fc27ea1b811f8bb	multimodal communication for human-friendly robot partners in informationally structured space	speech recognition gesture recognition human robot interaction image colour analysis neural nets robot vision;neural nets;human robot interaction;robot vision;image colour analysis;speech recognition;ubiquitous computing computational intelligence human robot interaction information services intelligent robots;robots human robot interaction user interfaces multimodal sensors man machine systems neural networks three dimensional displays gesture recognition;gesture recognition;human hand positions multimodal communication human friendly robot partners informationally structured space sensors cognitive capabilities environmental systems touch interface voice recognition human detection gesture recognition color image 3d distance information multilayered spiking neural network	This paper proposes a multimodal communication method for human-friendly robot partners based on various types of sensors. First, we explain informationally structured space to extend the cognitive capabilities of robot partners based on environmental systems. Next, we discuss the suitable measurement range for recognition technologies of touch interface, voice recognition, human detection, gesture recognition, and others. Based on the suitable measurement ranges, we propose an integration method to estimate human behaviors based on the human detection using color image and 3-D distance information, and gesture recognition by the multilayered spiking neural network using the time series of human-hand positions. Furthermore, we propose a conversation system to realize the multimodal communication with a person. Finally, we show several experimental results of the proposed method, and discuss the future direction of this research.	algorithm;artificial neural network;color image;environment (systems);gesture recognition;multimodal interaction;prototype;robot;sensor;signal processing;speech recognition;spiking neural network;time series;touch user interface	Naoyuki Kubota;Yuichiro Toda	2012	IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)	10.1109/TSMCC.2012.2213810	computer vision;computer science;artificial intelligence;social robot;gesture recognition;sketch recognition;artificial neural network	Robotics	-36.72283223207825	-42.51831820611916	128584
a4ebe9959cf9b9a0e7061640f87fdc32b4e6e110	stacked half-pie menus: navigating nested menus on interactive tabletops	touch screen;interactive surfaces;data selection;hierarchical pie menus;tabletops	"""Hierarchical menus can be found in many of today's software applications. However, these menus are often optimized for mouse or keyboard interaction and their suitability for touch screen-based interactive tabletops is questionable. On touch based interfaces, screen occlusion by the user, menu item size and the usage of intuitive navigation paradigms are essential aspects that need to be considered. In this paper we present our approach: """"Stacked Half-Pie menus"""" that allow visualization of an unlimited number of hierarchical menu items as well as interactive navigation and selection of these items by touch. Our evaluation shows a fairly high usability of touchable half-pie menus, making them an interesting alternative to other established menu types on interactive tabletops."""	interactive media;pie menu;touchscreen;usability	Tobias Hesselmann;Stefan Flöring;Marwin Schmitt	2009		10.1145/1731903.1731936	human–computer interaction;engineering;multimedia;menu bar;world wide web	HCI	-45.73075877394999	-41.2801271509118	128879
b08984414fc3621898af224ef0e404583c7b555b	xpaaand: interaction techniques for rollable displays	dynamic change;mobile device;scroll;rollable display;screen;form factor;digital content;input technique;resizing;tangible interaction;interaction technique	We present a device concept and a prototype of a future mobile device. By featuring a rollable display, its display size and its form factor can be dynamically changed. Moreover, we investigate how physical resizing of the display can be used as an input technique for interacting with digital contents and present a set of novel interaction techniques. Evaluation results show that physical resizing of the display can improve the way we interact with digital contents on mobile devices.	display size;interaction technique;mobile device;prototype;rollable display;sensitivity and specificity	Mohammadreza Khalilbeigi;Roman Lissermann;Max Mühlhäuser;Jürgen Steimle	2011		10.1145/1978942.1979344	human–computer interaction;computer science;multimedia;interaction technique;computer graphics (images)	HCI	-44.55016151746291	-40.27436197350102	128904
908a2e45a0fa8ff90cd15d432be190c6773f533f	hacking nintendo wii to paint virtual graffiti	virtual tool pervasive computing gestural interface multimodal interface;virtual graffiti;sensor system;mice;multimodal interface;gestural interface;pervasive computing;virtual reality;remote consoles;pc screen;virtual reality gesture recognition remote consoles ubiquitous computing;tangible aerosol bomb;nintendo wii console gaming device;gesture recognition virtual reality virtual graffiti tangible aerosol bomb pc screen nintendo wii console gaming device nintendo wii console sensor system;ubiquitous computing;bluetooth;switches;nintendo wii console sensor system;gesture recognition;weapons;cameras;graphics;computer crime paints virtual reality feedback aerosols weapons sensor systems shape spraying graphics;virtual tool;aerosols	Any approach to the issues of virtual reality implies a general question about how to effectively plunge inside it. Despite a great variety of settings, the more feedbacks are the same as real ones, the more users feel involved on a likely situation. We can split such an assumption into the necessity of effective gestural recognition and likelihood of virtual tools to reality. In this paper we will present a system to draw virtual graffiti on the PC screen, as it were a wall. A virtual, but tangible aerosol bomb has been designed and built, by exploiting the Nintendo console gaming device and sensor system. Our object has the same shape and the same way of use as a common, real spray paint dispenser and it can be used on quite every available graphics application on a Desktop or notebook case. Besides the advantages due to the low costs and the good accuracy of gesture recognition, our system confirms the importance of similarity in simulated scenarios.	cognitive walkthrough;desktop computer;documentation;experiment;feedback;gesture recognition;graphics software;haptic technology;prototype;remote control;simulation;usability;virtual graffiti;virtual reality;wii remote plus	Paola Salomoni;Ludovico Antonio Muratori;Silvia Mirri;Francesco Pozzi	2009	2009 International Conference on Ultra Modern Telecommunications & Workshops	10.1109/ICUMT.2009.5345524	simulation;network switch;computer science;graphics;multimedia;bluetooth;ubiquitous computing;computer graphics (images)	Visualization	-45.10505832787789	-42.54225691493357	128988
666b1223620d3791bc24343dcd85893278414589	navigation assistive system for the blind using a portable depth sensor	robot sensing systems;microcontrollers;vibrations;vision defects assisted living biomedical measurement data gloves distance measurement handicapped aids medical computing notebook computers;computer architecture;navigation;navigation haptic interfaces robot sensing systems vibrations microcontrollers computer architecture;real time distance measurement navigation assistive system blind portable depth sensor 3 dimensional depth sensor computer vision robotics community haptic feedback system 3 d depth sensor portable system visual scene distance map tablet computer haptic cues vibration motors gloves;haptic interfaces;navigation assistance depth sensor haptic feedback blind	The lightweight and low-cost 3-dimensional depth sensors have gained much attention in the computer vision and robotics community. While its performance has been proven successful in the robotics community, these sensors have not been utilized successfully for many assistive devices. Leveraging on this gap, this paper presents the design, implementation, and preliminary evaluation of a haptic feedback system for the blind using 3-D depth sensors. The proposed portable system interprets the visual scene using the depth sensor, converts it into distance map, processes, and evaluates this information using a tablet computer. In addition, it provides haptic cues to the user through an array of vibration motors woven into the gloves. Throughout preliminary evaluation, this system has shown to successfully identify, track, and present closest objects, closest humans, multiple humans, and perform real-time distance measurements.	assistive technology;computer vision;distance transform;haptic technology;range imaging;real-time computing;real-time locating system;robotics;sensor;structured-light 3d scanner;tablet computer	Kumar Yelamarthi;Kevin Laubhan	2015	2015 IEEE International Conference on Electro/Information Technology (EIT)	10.1109/EIT.2015.7293328	microcontroller;embedded system;computer vision;navigation;simulation;computer science;engineering;vibration	Robotics	-42.6519502049799	-43.91165467023784	129003
14aba3d564fb56098ae9730db30db25e41065d3d	flipcloak - the window manager system for overlapping windows with vacuum metaphor of visualizations		Overlapping windows prevent us from finding the target window immediately. They reduce the operability and the visibility so that it is important issue how to visualize and manage the windows on a desktop. In this research we propose the new window management method that uses combinational operation with a personal computer and a smartphone. Then we prototype the application called the FlipCloak. The FlipCloak is the system that controls transfer of windows on the desktop between devices with simple gestural interaction. Using the FlipCloak we can manage windows on a desktop efficiently by putting the low-priority window in the other device temporarily as putting one’s stuff in the cloakroom.	combinational logic;desktop computer;gesture recognition;microsoft windows;operability;personal computer;prototype;smartphone;window manager	Ryutaro Motora;Toshiyuki Masui;Michiaki Yasumura	2012				HCI	-45.47860463998588	-41.20745942341842	129184
08f3d3108928ff9d40f90562a143cbfe710a426f	animation in a peripheral display: distraction, appeal, and information conveyance in varying display configurations	empirical study;peripheral display;information visualization;animation;evaluation	Peripheral displays provide secondary awareness of news and information to people. When such displays are static, the amount of information that can be presented is limited and the display may become boring or routine over time. Adding animation to peripheral displays can allow them to show more information and can potentially enhance visual interest and appeal, but it may also make the display very distracting. Is it possible to employ animation for visual benefit without increasing distraction? We have created a peripheral display system called BlueGoo that visualizes R.S.S. news feeds as animated photographic collages. We present an empirical study in which participants did not find the system to be distracting, and many found it to be appealing. The study also explored how different display sizes and positions affect information conveyance and distraction. Animations on an angled second monitor appeared to be more distracting than three other configurations.	multi-monitor;peripheral	Christopher Plaue;John T. Stasko	2007		10.1145/1268517.1268541	anime;computer vision;information visualization;computer science;evaluation;multimedia;empirical research;computer graphics (images)	HCI	-47.961809464644446	-43.13742855113182	129315
887b6726f541a4fc4a53f8e4301c7f547fb762e2	usability evaluation of the touch screen user interface design	touch screen;popover;performance measures;ipad;user interface elements	With the advancement of ICT technologies, touch-screen interface mobile devices become a standard feature. This study aims to evaluate the Popover interface design under different age groups. The UI elements being considered for evaluation include location, window length and font size of the popover in three visual search tasks. The results show that there were significant differences in reaction time and accuracy rate between age groups. The worst performance was found in the older group. The best button position was on the bottom screen. In addition, significant performance differences between popover window length and font size were also found. Generally speaking, it is recommended to use the popover window with long cell length, and bigger font size for better readability, especially for the older age group users.	usability;user interface design	Chih-Yu Hsiao;You-Jia Liu;Mao-Jiun J. Wang	2013		10.1007/978-3-642-39209-2_6	human–computer interaction;engineering;multimedia;world wide web	HCI	-47.585459486379605	-45.48735367620924	129334
2bf6291cce1039417ee348e6e5618c9bf3a10f11	a computer-aided usability testing tool for in-vehicle infotainment systems	usability testing;queuing network model human processor;human factors;computer aided engineering;human performance modeling;in vehicle infotainment system	This paper describes the development of a computer-aided engineering (CAE) software toolkit for designers of in-vehicle infotainment systems to predict and benchmark the system usability, such as task completion time, eye glance behaviors, and mental workload. A digital driver model was developed based on the task-independent cognitive architecture of QN-MHP (Queuing Network-Model Human Processor). At the front end of the software a graphical user interface (GUI) was developed that allows designers to create digital mockups of the designs and simulate drivers performing secondary tasks while steering a vehicle. To validate the software outputs, an experiment using human drivers was conducted on a fix-based driving simulator with a radio-tuning task as a test case. Three typical in-vehicle infotainment systems that have the function of radio tuning were investigated (a touch screen, physical buttons, and a knob). The results show that the software was able to generate task completion time, total eyes-off-road time, and mental workload estimates that were similar to the empirical data. The software toolkit has the potential to be a supplemental tool for designers to explore a larger design space and address usability issues at the early design stages with lower cost in time and manpower.	benchmark (computing);cognition;cognitive architecture;control knob;digital mockup;driving simulator;graphical user interface;human processor model;limited-memory bfgs;off road challenge;simulation;test automation;test case;touchscreen;usability testing	Fred Feng;Yili Liu;Yifan Chen	2017	Computers & Industrial Engineering	10.1016/j.cie.2017.05.019	workload;simulation;software;usability;component-based usability testing;front and back ends;computer-aided engineering;mockup;driving simulator;computer science	SE	-47.259147825985444	-47.65681886509032	129383
02c8de83c3bd2226a918c925400628902b6f175a	size matters! how thumbnail number, size, and motion influence mobile video retrieval	smart phone;video retrieval;interface design;universiteitsbibliotheek;user experience;handheld device;video retrieval interfaces;visual assessment tasks;video browsing;mobile video	Various interfaces for video browsing and retrieva l h ve been proposed that provide improved usability, better re t ieval performance, and richer user experience compared to simple result li sts that are just sorted by relevance. These browsing interfaces take advantage of the rather large screen estate on desktop and laptop PCs to visualize advanc ed configurations of thumbnails summarizing the video content. Naturally , the usefulness of such screen-intensive visual browsers can be called into question when applied on small mobile handheld devices, such as smart phones . I this paper, we address the usefulness of thumbnail images for mobile video r trieval interfaces. In particular, we investigate how thumbnail number, si ze, and motion influence the performance of humans in common recognition tas ks. Contrary to widespread believe that screens of handheld devices are unsuited for visualizing multiple (small) thumbnails simultaneously, our stu dy shows that users are quite able to handle and assess multiple small thum bnails at the same time, especially when they show moving images. Our result s give suggestions for appropriate video retrieval interface designs on ha d eld devices.	desktop computer;digital video;laptop;mobile device;relevance;smartphone;thumbnail;usability;user experience	Wolfgang Hürst;Cees Snoek;Willem-Jan Spoel;Mate Tomin	2011		10.1007/978-3-642-17829-0_22	user experience design;computer science;interface design;video tracking;mobile device;multimedia;world wide web;computer graphics (images)	Mobile	-47.39181357770476	-43.9014341878533	129455
83c7c26dc36cb38db73972639cad9509c132a20a	instant user interfaces: repurposing everyday objects as input devices	everyday objects;affordances;instant uis;input devices	Dedicated input devices are frequently used for system control. We present Instant User Interfaces, an interaction paradigm that loosens this dependency and allows operating a system even when its dedicated controller is unavailable. We implemented a reliable, marker-free object tracking system that enables users to assign semantic meaning to different poses or to touches in different areas. With this system, users can repurpose everyday objects and program them in an ad-hoc manner, using a GUI or by demonstration, as input devices. Users tested and ranked these methods alongside a Wizard-of-Oz speech interface. The testers did not show a clear preference as a group, but had individual preferences.	graphical user interface;hoc (programming language);input device;programming paradigm;tracking system;wizard (software)	Christian Corsten;Ignacio Avellino;Max Möllers;Jan O. Borchers	2013		10.1145/2512349.2512799	simulation;computer science;multimedia;communication	HCI	-46.479395770703796	-38.334210061698535	129610
fd8d68bbe728e6a4eb99b887a30938c0dcf1c63e	gesture identification based on zone entry and axis crossing	hierarchical menu;interface area;easy-to-use hierarchical menu selection;zone entry;detection axis;vision-based unicursal gesture interface;gesture identification;input vugi;dynamic detection zone;tv remotes;hand gesture interface;unicursal gesture interface	Hand gesture interfaces have been proposed as an alternative to the remote controller, and products with such interfaces have appeared in the market. We propose the vision-based unicursal gesture interface (VUGI) as an extension of our unicursal gesture interface (UGI) for TV remotes with touchpads. Since UGI allows users to select an item on a hierarchical menu comfortably, it is expected that VUGI will yield easy-to-use hierarchical menu selection. Moreover, gestures in the air such as VUGI offer an interface area that is larger than that provided by touchpads. Unfortunately, since the user loses track of his/her finger position, it is not easy to input commands continuously using VUGI. To solve this problem, we propose the dynamic detection zone and the detection axes. An experiment confirms that subjects can input VUGI commands continuously.	apache axis	Ryosuke Aoki;Yutaka Karatsu;Masayuki Ihara;Atsuhiko Maeda;Minoru Kobayashi;Shingo Kagami	2011		10.1007/978-3-642-21605-3_22	embedded system;simulation;artificial intelligence;operating system	HCI	-44.619865639102294	-43.77287043626229	129779
718b313887f8ab3923afb8786d4cd7895fa49ee4	posing and acting as input for personalizing furniture	human factors;design;embodied interaction	"""Digital fabrication is becoming increasingly practical for customizing products to users' specifications. However, the design interfaces for customizing items have focused more on 3D modelling and less on how people use the object or how it fits around their body. In this paper, we explore a user-centered approach: using posing and acting as input for personalizing furniture. Users specify dimensions by referring to their body parts and using simple speech commands such as """"this wide"""" or """"from here to here"""", while indicating a distance with their arms. A head-mounted display (HMD) provides instant feedback in real-size and allows users to experience and evaluate their virtual design as though it were a prototype. We report the formative and evaluative studies that indicate that the proposed approach engages casual users in the iterative design process of personalizing items in relation to their use, body, and environment."""	3d modeling;3d printing;coat of arms;digital modeling and fabrication;fits;head-mounted display;iteration;iterative design;prototype;speech synthesis;user-centered design	Bokyung Lee;Minjoo Cho;Joonhee Min;Daniel Saakes	2016		10.1145/2971485.2971487	design;simulation;human–computer interaction;computer science;human factors and ergonomics;multimedia	HCI	-44.32621111050989	-39.15983638754907	129867
3fa91596c79a0e931f56198e9a5f843095410f38	reaction times to constraint violation in haptics: comparing vibration, visual and audio stimuli	visualization haptic interfaces vibrations robot sensing systems surgery time factors;robot sensing systems;loud auditory tone;human computer interaction;visual stimuli;vibrations;h 5 2 information interfaces and presentation user interfaces haptic i o;weak vibrations;surgical robotics;auditory signal;medical robotics;visualization;vibrations haptic interfaces human computer interaction medical robotics surgery telerobotics;time factors;vibrotactile modality;sensory modality;alert signal;audio stimuli;surgery;visual signal;telerobotics;vibrotactile feedback reaction time constraint violation visual stimuli audio stimuli teleoperation surgical robotics virtual constraint sensory modality virtual tool haptic interface alert signal auditory signal vibrotactile signal visual signal vibrotactile modality weak vibrations loud auditory tone;vibrotactile feedback;vibrotactile signal;haptic interfaces;constraint violation;virtual constraint;reaction time;teleoperation;virtual tool;haptic interface	In teleoperation and in particular in surgical robotics, it is important to avoid getting closer to certain forbidden areas typically limited by virtual constraints. In this paper we compare different sensory modalities, vibratory, auditory and visual, to convey information about constraint violation to the operator. We focus on which of these modalities can elicit the fastest reaction time on the user. An experiment was devised in which subjects were asked to slowly insert a virtual tool by means of a haptic interface, and retract it as soon as they hit an obstacle; such event triggered an alert signal. We evaluated different signals: auditory, vibrotactile, and visual, with two amplitude levels for audio and vibration. Lower reaction times were observed on the strong vibrotactile modality, followed by the weak vibrations and the loud auditory tone, although the latter was described as uncomfortable by the subjects. The vibrotactile feedback was described as pleasant by most subjects and appears promising for future developments.	active set method;beep;experiment;fastest;feedback;haptic technology;interaction;modality (human–computer interaction);robotics	Adrian Ramos Peon;Domenico Prattichizzo	2013	2013 World Haptics Conference (WHC)	10.1109/WHC.2013.6548486	telerobotics;mental chronometry;computer vision;teleoperation;simulation;visualization;visual perception;computer science;artificial intelligence;vibration;haptic technology;stimulus modality	HCI	-46.0334254174049	-49.647517803040266	129933
bb89fd760bca73e9c8ca086121cf58afe9f8d8f4	user-centered robot head design: a sensing computing interaction platform for robotics research (sciprr)		We developed and evaluated a novel humanoid head, SCIPRR (Sensing, Computing, Interacting Platform for Robotics Research). SCIPRR is a head shell that was iteratively created with additive manufactur- ing. SCIPRR contains internal sca olding that allows sensors, small form computers, and a back-projection system to display an ani- mated face on a front-facing screen. SCIPRR was developed using User Centered Design principles and evaluated using three di erent methods. First, we created multiple, small-scale prototypes through additive manufacturing and performed polling and re nement of the overall head shape. Second, we performed usability evaluations of expert HRI mechanics as they swapped sensors and computers within the the SCIPRR head. Finally, we ran and analyzed an ex- periment to evaluate how much novices would like a robot with our head design to perform di erent social and traditional robot tasks. We made both major and minor changes a er each evalu- ation and iteration. Overall, expert users liked the SCIPRR head and novices wanted a robot with the SCIPRR head to perform more tasks (including social tasks) than a more traditional robot.	3d printing;computer;human–robot interaction;iteration;robot;robotics;sensor;usability;user-centered design;utility functions on indivisible goods	Anthony M. Harrison;Wendy M. Xu;J. Gregory Trafton	2018		10.1145/3171221.3171283	animation;simulation;human–computer interaction;computer science;robot;3d printing;usability;user-centered design;artificial intelligence;polling;robotics;human–robot interaction	Robotics	-37.97316456699899	-39.48729616372366	130130
50d0ef994eb4d8da6790c58a84cb0eb771fe8a78	coordinating turn-taking and talking in multi-party conversations by controlling robot's eye-gaze	robot sensing systems;glass;collaborative learning scenes robot eye gaze control multiparty conversations cooperative turn taking game nonverbal situation multiparty conversation environment;games;learning artificial intelligence control engineering computing gaze tracking groupware human robot interaction;robot kinematics games glass robot sensing systems timing containers;containers;robot kinematics;timing	In this study, we suggest a method to coordinate turn-taking and talking in multi-party conversations by the gaze of a robot that participates on the side. Also, we use the experimental paradigm named “Cooperative Turn-taking Game in Non-verbal Situation”, which is a simplified multi-party conversation environment. We investigated whether designing eye-gazes for such robots can coordinate turn-taking and talking in multi-party conversations, and we found the robot's gaze could coordinate turn-taking and talking in multi-party conversations. Our study is expected to effectively encourage desirable talking in such multi-party conversations as collaborative learning scenes.	programming paradigm;robot;tree accumulation	Ryo Sato;Yugo Takeuchi	2014	The 23rd IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2014.6926266	robot learning;games;computer vision;simulation;computer science;artificial intelligence;social robot;glass;robot kinematics	Robotics	-37.392462440503714	-39.17250758089714	130448
cc4222cbd7abd3c606b63ca0949c3e1d9570f01a	uniform vs. non-uniform scaling of shooter games on large displays	shooter games;computer games computer displays;display size scaling shooter games;scaling;games projectiles visualization marine vehicles software navigation mobile communication;display size;game environment shooter games large displays nonuniform scaling uniform scaling display sizes in ame elements	We present a study comparing player performance in a shooter game using two different types of scaling across four display sizes. The first scaling type used uniform scaling where increasing the display size also increased the size of all in-game elements by the same factor. The second employed non-uniform scaling where all in-game elements remained fixed in size, but the game environment increased (or decreased) in size. As expected, gameplay becomes much easier at larger scales with non-uniform scaling. Our results quantify this expectation: different difficulty attributes are very well modeled using either linear or power models. We discuss the implications this has on maintaining constant game difficulty and user experience.	display size;image scaling;mathematical model;user experience;virtual world	Robert J. Teather;Jacques Carette;Manivanna Thevathasan	2015	2015 IEEE Games Entertainment Media Conference (GEM)	10.1109/GEM.2015.7377210	simulation;computer science;multimedia;computer graphics (images)	Visualization	-43.47012522618395	-48.68655465571137	130467
72ae0705a059d926015d6238e04fb3acd5e5fd86	multimodal interaction in the car: combining speech and gestures on the steering wheel	gesture interaction;speech interaction;automotive user interfaces;multimodal interfaces	Implementing controls in the car becomes a major challenge: The use of simple physical buttons does not scale to the increased number of assistive, comfort, and infotainment functions. Current solutions include hierarchical menus and multi-functional control devices, which increase complexity and visual demand. Another option is speech control, which is not widely accepted, as it does not support visibility of actions, fine-grained feedback, and easy undo of actions. Our approach combines speech and gestures. By using speech for identification of functions, we exploit the visibility of objects in the car (e.g., mirror) and simple access to a wide range of functions equaling a very broad menu. Using gestures for manipulation (e.g., left/right), we provide fine-grained control with immediate feedback and easy undo of actions. In a user-centered process, we determined a set of user-defined gestures as well as common voice commands. For a prototype, we linked this to a car interior and driving simulator. In a study with 16 participants, we explored the impact of this form of multimodal interaction on the driving performance against a baseline using physical buttons. The results indicate that the use of speech and gesture is slower than using buttons but results in a similar driving performance. Users comment in a DALI questionnaire that the visual demand is lower when using speech and gestures.	baseline (configuration management);driving simulator;multimodal interaction;prototype;scalability;simulation;steering wheel;undo;user-centered design	Bastian Pfleging;Stefan Schneegaß;Albrecht Schmidt	2012		10.1145/2390256.2390282	simulation;speech recognition;engineering;communication	HCI	-47.22429161247118	-46.60639142828956	130490
d147db6ef73c9b3d41eb79d591a66e0cab6bc48a	user experience design of stroke patient communications using mobile finger (mofi) communication board with user center design approach		Stroke has become a phenomenon in Indonesia. From 2014 to mid-2015, the disease is the first cause of death in Indonesia. This is of particular concern to the Ministry of Health so that the prevention, treatment, and prevention of stroke is further enhanced. The condition of stroke patients whose movements are limited is exacerbated by the psychic condition of patients who are unable to communicate pressure will cause obstacles to the healing process of the patient. This research proposes a stroke patient communication media by applying the technology of Mobile Finger Communication Board and user center design approach (UCD). This communication board is operated using the patient's fingers so that it is comfortable for everyday activities. Mobile Finger Communication Board Design based on five planes of user experience. The result of Mobile Finger Communication Board implementation shows that application usability level seen from perception and ergonomic point of view shows satisfaction index of the user at the satisfactory level so that it can be said that the system works with the satisfactory result of the users. It is expected that with a convenient Mobile Finger Communication Board can support the healing process of stroke so that the healing rate of stroke is increasing. Keyword—User Experience, Stroke, Communication Board, Activity of Daily Living, User Center Design.		Aan Jelli Priana;Herman Tolle;Ismiarta Aknuranda;Eko Aristijono	2018	iJIM		human factors and ergonomics;activities of daily living;multimedia;christian ministry;computer science;user experience design;usability;stroke;cause of death;phenomenon	HCI	-48.03172537878492	-46.491612722008945	130556
ee810166a86a09ccea520ec0d20e16ff1a26f9a8	development of a power assist system of a walking chair (proposition of the speed-torque combination power assist system)	robotics;man machine system;welfare machine;power assist system;walking chair			Yunfeng Wu;Masaru Higuchi;Yukio Takeda;Koichi Sugimoto	2005	JRM	10.20965/jrm.2005.p0189	control engineering;simulation;engineering	HCI	-40.77067237364793	-45.70840618720581	130728
b614a26cca7698e7ce5a2d4fd995bb79a0196a05	dopgest: dual-frequency based ultrasonic gesture recognition		In-air gesture recognition is becoming a popular means of interacting with terminal devices, such as smart phones, tablets and laptops. To implement in-air gesture recognition, many methods have been researched, such as computer visionbased, RF-based, WiFi-based and ultrasonic-based. Compared to other approaches, the ultrasonic-based method shows superiorities in low-cost, robustness and so on. In this paper, we construct a dual-frequency based ultrasonic gesture recognition system called DopGest, which utilizes the Doppler Effect to recognize gestures. Our system is implemented completely on a laptop without any additional equipments, and we use two speakers and one microphone already embedded in laptop as sensors. Most of ultrasonic-based methods exploit single tone, which limit the type of gestures. For instance, they can not recognize horizontal swipe gestures. On the contrary, our system can recognize swipe left and swipe right gestures by using two high tones (18 kHz). In our system, DopGest generates two different high frequency tones from two speakers and detects Doppler Effect caused by hand movements, then it extracts features from Doppler shifts of these two high tones, since laptops generally contain at least two speakers. Compared to computervision based method, our system can work under any light conditions without extra hardware devices and with low power consumption. Experiments show that our system can achieve an average classification accuracy of 88.9%.		Qi Liu;Wei Yang;Yang Xu;Yahui Hu;Qijian He;Liusheng Huang	2018	2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)	10.1109/SmartWorld.2018.00084	laptop;distributed computing;swipe;gesture recognition;computer vision;robustness (computer science);computer science;doppler effect;ultrasonic sensor;microphone;artificial intelligence;gesture	Mobile	-38.94283959992748	-45.04720682196908	130774
f4ceb0e0748effc4c64ee06b38b6042d98a40fa1	fiberboard: compact multi-touch display using channeled light	light offer;compact multi-touch display;alternative technology;virtual camera software;multi-touch image processing tool;conventional camera-based multi-touch display;complex custom;ir multi-touch device;resulting display;ir light;compact ir-sensing multi-touch display;sensor array;image processing;optical fiber;fiber optic;infrared	Multi-touch displays based on infrared (IR) light offer many advantages over alternative technologies. Existing IR multi-touch devices either use complex custom electronic sensor arrays, or a camera that must be placed relatively distant from the display. FiberBoard is an easily constructed compact IR-sensing multi-touch display. Using an array of optical fibers, reflected IR light is channeled to a camera. As the fibers are flexible the camera is free to be positioned so as to minimize the depth of the device. The resulting display is around one tenth of the depth of a conventional camera-based multi-touch display. We present our prototype, its novel calibration process, and virtual camera software based on existing multi-touch image processing tools.	image processing;multi-touch;optical fiber;prototype;virtual camera system	Daniel Jackson;Tom Bartindale;Patrick Olivier	2009		10.1145/1731903.1731954	smart camera;telecommunications;engineering;optics;computer graphics (images)	HCI	-42.823156406207836	-41.07840503606891	130840
f9765759bb2ea82a5816ea6a212bee22ee90348a	dynamic augmented reality x-ray on google glass	google glass;x ray visualization;augmented reality	Over the recent years, research in sophisticated Augmented Reality (AR) X-Ray visualization techniques such as [Dey and Sandor 2014] that permit the user to see through real-world objects have created the possibility for mobile applications to show occluded information in an innovative and intuitive fashion. A popular application is navigation assistance in inner city environments. We think that Google Glass, a hands-free, wearable mobile device, which allows immediate access to web services, is a promising platform for such X-Ray systems.	augmented reality;glass;mobile app;mobile device;wearable computer;web service;x-ray (amazon kindle)	Damien Constantine Rompapas;Nicholas Sorokin;Arno in Wolde Lübke;Takafumi Taketomi;Goshiro Yamamoto;Christian Sandor;Hirokazu Kato	2014		10.1145/2669062.2669087	smartglasses;augmented reality;computer-mediated reality;simulation;computer science;multimedia;computer graphics (images)	HCI	-46.17009409281279	-40.42168389146661	130901
491ff14f99494be9f8ab83c651dbdd10f45ab8db	mobile gaze mapping: a python package for mapping mobile gaze data to a fixed target stimulus				Jeff MacInnes;Shariq Iqbal;John Pearson;Elizabeth N. Johnson	2018	J. Open Source Software	10.21105/joss.00984	computer vision;gaze;stimulus (physiology);python (programming language);artificial intelligence;computer science	SE	-40.79230409409891	-41.67091775326504	130966
3d76aed4c0949a14ae4498afc11c60f8c063cd59	beyond visual acuity: the perceptual scalability of information visualizations for large displays	high resolution;visual acuity;visual design;information visualization;scaling up;large displays	The scalability of information visualizations has typically been limited by the number of available display pixels. As displays become larger, the scalability limit may shift away from the number of pixels and toward human perceptual abilities. This work explores the effect of using large, high resolution displays to scale up information visualizations beyond potential visual acuity limitations. Displays that are beyond visual acuity require physical navigation to see all of the pixels. Participants performed various information visualization tasks using display sizes with a sufficient number of pixels to be within, equal to, or beyond visual acuity. Results showed that performance on most tasks was more efficient and sometimes more accurate because of the additional data that could be displayed, despite the physical navigation that was required. Visualization design issues on large displays are also discussed.	image resolution;information visualization;pixel;scalability	Beth Yost;Yonca Haciahmetoglu;Chris North	2007		10.1145/1240624.1240639	computer vision;information visualization;image resolution;computer science;multimedia;computer graphics (images)	HCI	-42.53238122276121	-48.64056543810419	131093
469b1092e6ffd97b45ad93563e2e8e2ccb576f8a	freehand sketching interfaces: early processing for sketch recognition	sketch recognition;segmentation;freehand sketching interface;recognition;human computer interface;stroke	Freehand sketching interfaces allow the user to directly interact with tasks without worrying about low-level commands. The paper presents a method for interpreting on-line freehand sketch and describes a human-computer interface prototype system of freehand sketch recognition (FSR) that is designed to infer designers' intention and interprets the input sketch into more exact 2D geometric primitives: straight lines, polylines, circles, circular arcs, ellipses, elliptical arcs, hyperbolas and parabolas. According to whether the stroke needs to be segmented or not, it is divided into single primitives and composite primitives correspondingly. Based on open/closed characteristic and semi-invariant, conic type and category of freehand sketch were defined for subdividing conic curve. Recognition approach for composite-primitive consists of three stages. The effectiveness of the algorithm is demonstrated preliminarily by experiments.	adobe freehand;sketch recognition	Shu-xia Wang;Man-Tun Gao;Le-hua Qi	2007		10.1007/978-3-540-73107-8_18	computer vision;stroke;human–computer interaction;computer science;segmentation;sketch recognition;computer graphics (images)	Vision	-42.358815374863184	-39.361797568708255	131129
805e07f5fccd144383ad001fa940eaeb20724b4d	design and implementation of virtual environments training of the visually impaire	application development;haptic device;training;virtual reality;virtual environments;haptics;feasibility study;design and implementation;visual impairment;visually impaired;virtual environment	This paper presents the virtual reality applications developed for the feasibility study tests of the EU funded IST project ENORASI. ENORASI aims at developing a highly interactive and extensible haptic VR training system that allows visually impaired people, especially those blind from birth, to study and interact with various virtual objects. A number of custom applications have been developed based on the interface provided by the CyberGrasp haptic device. Eight test categories were identified and corresponding tests were developed for each category. Twenty-six blind persons conducted the tests and the evaluation results have shown the degree of acceptance of the technology and the feasibility of the proposed approach.	haptic technology;virtual reality	Dimitrios Tzovaras;Georgios Nikolakis;George Fergadis;Sotiris Malassiotis;Modestos Stavrakis	2002		10.1145/638249.638259	feasibility study;simulation;human–computer interaction;computer science;virtual reality;multimedia;haptic technology	HCI	-47.82733226403732	-39.10483553977245	131211
bda0e5e8b7ed32de091f35cf5e789610029a05ea	visualization of user eye movements for search result pages		We propose new visualization techniques for the user behaviors when using search engine results pages. Our visualization method provides an overview of a user’s actual visual behavior using the logs for eye-movement data and browser link-clicking. We also report on the eye-movement data collected from user experiments.	experiment;search engine results page	Yuka Egusa;Masao Takaku;Hitoshi Terai;Hitomi Saito;Noriko Kando;Makiko Miwa	2008			visualization;creative visualization;information retrieval;search engine;eye movement;information visualization;visual analytics;computer science	HCI	-34.346033344587305	-50.249894261117625	131366
960c26d77393b66591a8297598d1077cb1084695	eog based control of mobile assistive platforms for the severely disabled	assistive robots;biocontrol;mobile robot;real time;mobile robot mobile assistive platforms severely disabled people assistive robots handicapped people eye motor coordination electrooculogram signals eye displacement;handicapped people;mobile robots;assistive device;electrooculogram signals;handicapped aids;motor coordination;electrooculography mobile robots robot kinematics wheelchairs signal processing vocabulary robot control muscles electrodes legged locomotion;mobile robots biocontrol electro oculography handicapped aids;eye displacement;eye motor coordination;severely disabled people;electro oculography;mobile assistive platforms	Assistive robots are increasingly being used to improve the quality of the life of disabled or handicapped people. In this paper a complete system is presented that can be used by people with extremely limited peripheral mobility but having the ability for eye motor coordination. The electrooculogram signals (EOG) that results from the eye displacement in the orbit of the subject are processed in real time to interpret intent and hence generate appropriate control signals to the assistive device. The effectiveness of the proposed methodology and the algorithms are demonstrated using a mobile robot for a limited vocabulary	algorithm;assistive technology;displacement mapping;electrooculography;mobile robot;peripheral;vocabulary	W. Sardha Wijesoma;Kang Say Wee;Ong Choon Wee;Arjuna P. Balasuriya;Tong San Koh;Kay Soon Low	2005	2005 IEEE International Conference on Robotics and Biomimetics - ROBIO	10.1109/ROBIO.2005.246316	mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence	Robotics	-40.55594354289472	-45.07159190234815	131371
803680ed53ed3662a22e458d1111a6c9f7c73aca	perceptual user interface in virtual shopping environment	e commerce;virtual reality;perceptual user interface;human computer interface	In this paper we present our effort towards the goal of perceptual user interface for main interaction tasks, such as navigation/travel, selection/picking and personal data access in a virtual shopping environment. A set of 3D navigation devices, vision-based pointing and personal access system are mainly discussed. The motivation and design principles behind these interfaces are also described. A prototype integration solution which brings these devices together in a virtual shopping environment, is given. These interfaces and interaction devices have been implemented and are tested for evaluation.	user interface	Weidong Geng;Wolfgang Strauss;Monika Fleischmann;Vladimir Elistratov;Marina Kolesnik	2003	Int. J. Image Graphics	10.1142/S0219467803001056	user interface design;simulation;human–computer interaction;computer science;virtual reality;multimedia;user interface	Graphics	-46.754817406271975	-42.073118695109336	131541
5077cd795e71c64a7e328bf2587a488855004f4b	force sensitive handles and capacitive touch sensor for driving a flexible haptic-based immersive system	hand;electric capacitance;transducers;flexible sensor;equipment failure analysis;transducers pressure;conformable sensor;equipment design;touch;conductometry;humans;user computer interface;tactile data processing;man machine systems;haptic strip;hand strength;haptic interface	In this article, we present an approach that uses both two force sensitive handles (FSH) and a flexible capacitive touch sensor (FCTS) to drive a haptic-based immersive system. The immersive system has been developed as part of a multimodal interface for product design. The haptic interface consists of a strip that can be used by product designers to evaluate the quality of a 3D virtual shape by using touch, vision and hearing and, also, to interactively change the shape of the virtual object. Specifically, the user interacts with the FSH to move the virtual object and to appropriately position the haptic interface for retrieving the six degrees of freedom required for both manipulation and modification modalities. The FCTS allows the system to track the movement and position of the user's fingers on the strip, which is used for rendering visual and sound feedback. Two evaluation experiments are described, which involve both the evaluation and the modification of a 3D shape. Results show that the use of the haptic strip for the evaluation of aesthetic shapes is effective and supports product designers in the appreciation of the aesthetic qualities of the shape.	experiment;fasttrack scripting host;fingers, unit of measurement;haptic device component;haptic technology;hereditary angioedema type iii;interactivity;interface device component;multimodal interaction;norm (social);six degrees of separation	Mario Covarrubias;Monica Bordegoni;Umberto Cugini	2013		10.3390/s131013487	embedded system;computer vision;simulation;transducer;computer science;engineering;haptic technology;physics;conductometry	HCI	-43.40954575312024	-41.155791359817854	131566
79334e0dd8f4f45440be21e5709a6fa87dbe09af	learning gestures for customizable human-computer interaction in the operating room		Interaction with computer-based medical devices in the operating room is often challenging for surgeons due to sterility requirements and the complexity of interventional procedures. Typical solutions, such as delegating the interaction task to an assistant, can be inefficient. We propose a method for gesture-based interaction in the operating room that surgeons can customize to personal requirements and interventional workflow. Given training examples for each desired gesture, our system learns low-dimensional manifold models that enable recognizing gestures and tracking particular poses for fine-grained control. By capturing the surgeon's movements with a few wireless body-worn inertial sensors, we avoid issues of camera-based systems, such as sensitivity to illumination and occlusions. Using a component-based framework implementation, our method can easily be connected to different medical devices. Our experiments show that the approach is able to robustly recognize learned gestures and to distinguish these from other movements.	component-based software engineering;experiment;human–computer interaction;medical devices;medical records systems, computerized;movement;obstruction;operating room;operating tables;requirement;solutions;manifold;sensor (device)	Loren Arthur Schwarz;Ali Bigdelou;Nassir Navab	2011	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-23623-5_17	computer vision;simulation	Robotics	-38.1158271332237	-45.90818083077694	131615
2cebc97b0907c8e4ceffe72dc745709555419edd	eye-wearable technology for machine maintenance: effects of display position and hands-free operation	machine maintenance;task guidance;smartglasses;action type;eye wearable technology;wearable computing	Exciting developments in eye-wearable technology and its potential industrial applications warrant a thorough understanding of its advantages and drawbacks through empirical evidence. We conducted an experiment to investigate what characteristics of eye-wearable technology impact user performance in machine maintenance, which included a representative set of car maintenance tasks involving Locate, Manipulate, and Compare actions. Participants were asked to follow instructions displayed on one of four technologies: a peripheral eye-wearable display, a central eye-wearable display, a tablet, or a paper manual. We found a significant effect of display position: the peripheral eye-wearable display resulted in longer completion time than the central display; but no effect for hands-free operation. The technology effects were also modulated by different Tasks and Action types. We discuss the human factors implications for designing more effective eye-wearable technology, including display position, issues of monocular display, and how the physical proximity of the technology affects users' reliance level.	action potential;binocular vision;design of experiments;display size;ecology;eye tracking;head-mounted display;human factors and ergonomics;interaction technique;modality (human–computer interaction);modulation;peripheral;tablet computer;taxonomy (general);wearable computer;wearable technology	Xianjun Sam Zheng;Cédric Foucault;Patrik Matos da Silva;Siddharth Dasari;Tao Yang;Stuart Goose	2015		10.1145/2702123.2702305	smartglasses;computer vision;simulation;wearable computer;computer science	HCI	-45.96440539943431	-48.06681713606872	131740
e5da1487ba0ff5c67b8acec1f6b1f5851f7c0124	human-oriented approaches for assistive and rehabilitation robotics - engineering methods, technical implementation, and treatment			rehabilitation robotics	Philipp Beckerle;Gionata Salvietti;Ramazan Unal;Fulvio Mastrogiovanni	2017	Robotics and Autonomous Systems	10.1016/j.robot.2017.07.010	simulation;design methods;computer science;rehabilitation robotics;future of robotics	Robotics	-40.0166497339003	-46.10265498583049	131859
7c5a3115a02bbfaa28666a0812210efeccccb148	a plainly designed robot for the experiments regarding the psychological boundaries of robots	boundary;psychology;recognition;robot		experiment;robot	Chyon Hae Kim;Yumiko Yamazaki;Shunsuke Nagahama;Shigeki Sugano	2014	JRM	10.20965/jrm.2014.p0109	psychology;mobile robot;computer vision;simulation;artificial intelligence;social robot;robot control;personal robot	Robotics	-35.98956330281914	-39.446904559185306	131903
ac3464646d8f16a906f67060a79bbc5bdd12d657	virtual odometry from visual flow	realite virtuelle;realidad virtual;simulation;virtual reality;simulacion;ecran projection;stereoscopy;pantalla proyeccion;distance estimation;observador;visual motion;distance measurement;observateur;varying speed;optical arrays;medicion distancia;scale factor;velocidad variable;projection screen;stereoscopie;computing systems;optical flow;estereoscopia;vitesse variable;virtual environment;computer animation;discriminacion;observer;facteur echelle;factor escala;discrimination;mesure de distance	We investigate how visual motion registered during one’s own movement through a structured world can be used to gauge travel distance. Estimating absolute travel distance from the visual flow induced in the optic array of a moving observer is problematic because optic flow speeds co-vary with the dimensions of the environment and are thus subject to an environment specific scale factor. Discrimination of the distances of two simulated self-motions of different speed and duration is reliably possible from optic flow, however, if the visual environment is the same for both motions, because the scale factors cancel in this case. 2 Here, we ask whether a distance estimate obtained from optic flow can be transformed into a spatial interval in the same visual environment. Subjects viewed a simulated self-motion sequence on a large (90 by 90 deg) projection screen or in a computer animated virtual environment (CAVE) with completely immersive, stereographic, head-yoked projection, that extended 180deg horizontally and included the floor space in front of the observer. The sequence depicted selfmotion over a ground plane covered with random dots. Simulated distances ranged from 1.5 to 13 meters with variable speed and duration of the movement. After the movement stopped, the screen depicted a stationary view of the scene and two horizontal lines appeared on the ground in front of the observer. The subject had to adjust one of these lines such that the spatial interval between the lines matched the distance traveled during the movement simulation. Adjusted interval size was linearly related to simulated travel distance, suggesting that observers could obtain a measure of distance from the optic flow. The slope of the regression was 0.7. Thus, subjects underestimated distance by 30%. This result was similar for stereoscopic and monoscopic conditions. We conclude that optic flow can be used to derive an estimate of travel distance, but this estimate is subject to scaling when compared to static intervals in the environment, irrespective of steroscopic depth cues.	binocular disparity;computer animation;depth perception;image scaling;motion simulator;odometry;optical flow;projection screen;simulation;stationary process;stereoscopy;virtual reality	Markus Lappe;Harald Frenz;Thomas Bührmann;Marina Kolesnik	2005		10.1117/12.610863	computer vision;simulation;geography;computer graphics (images)	Visualization	-43.449512924895735	-49.86162601373105	132237
3014cc89c76ef76768223750419fffcce9d3e5e5	design method of gui using genetic algorithm	remote control;touch sensitive screens;prototypes;digital tv;graphical user interfaces;design method;telecontrol;gui;genetic algorithm;genetic algorithms;switches;ga gui genetic algorithm av remote control touch panel display;graphical user interfaces gallium prototypes switches sdram digital tv;touch sensitive screens genetic algorithms graphical user interfaces telecontrol;gui genetic algorithm remote control;sdram;gallium	This paper proposes an AV remote control that can automatically arrange icons using a genetic algorithm (GA). Our study intends to ensure the simpler and more efficient GUI design of multi-remote controls that can operate two or more AV devices. These multi-remote controls require a touch panel display, with a GUI arranged with on-screen buttons that resemble the physical buttons of ordinary remote controls. Accordingly, we are working on a GUI whose icons are effectively arranged based on a GA. In the GA, various icons of different sizes and shapes are regarded as genes, and the order of arranging icons on the screen is regarded as an array of genes, or an individual. First, genes in the GA are taken as the buttons, and an individual is taken as the order in which the buttons are arranged. Next, PC simulation was used to determine the GA evaluation, the technique of a selection, a crossover and a mutation, the terminate check, and parameters so that the processing time would be shorter. Finally, a prototype was generated and utilized to verification experiments. However, it was found that the processing of the GA needed a maximum of five minutes and we tried to improve the system. This effort was effective and we could end the process within a minute. As a result, we could achieve the automatic arrangement of buttons that minimized the number of pages and gaps.	crossover (genetic algorithm);experiment;formal verification;genetic algorithm;graphical user interface;prototype;remote control;simulation;software release life cycle;terminate (software);touchscreen	Takanori Mori;Takako Nonaka;Tomohiro Hase	2010	2010 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2010.5642282	embedded system;genetic algorithm;computer hardware;computer science;artificial intelligence;graphical user interface;computer graphics (images)	Robotics	-44.69532973090585	-42.83626983466385	132391
0b53e07b2c349755938ce9926a341f974a7f1945	design and implementation of home automation system using facial expressions		Home automation system means centralized control of lighting, electrical appliances; locking of doors, alarm systems etc. for improving energy efficiency, security, and comfort. Assistive domotics is a home automation system specially developed for elderly and physically challenged persons for their convenience and comport who would otherwise require additional care. In this paper home automation is done using facial expressions which include angry, sad, neutral, fear and smiling. This system is designed for situations were speech control systems and gesture control systems fail.		P. C. Soumya	2014		10.1007/978-3-319-11933-5_69	embedded system;computer vision;computer hardware	Robotics	-40.7536768249857	-44.027937172957536	132489
cb498b2f9c58bec2065fc6a202e89a92b8f9eb93	teaching touch sensing technologies through project-based learning	libraries;computers;microcontrollers;sensors;capacitance;programming	Compared with conventional keyboard and mouse systems, touch sensing input devices provide more accurate and direct interaction with the user. More and more touch devices appear in various domains. To keep up with this technology change, we have introduced touch sensing technologies to electrical and computer engineering technology students by using the project-based learning (PBL) approach. Our experiences indicate that the PBL approach is efficient and practical for teaching touch sensing techniques. Two different sets of courseware including hardware kits and software packages have been utilized in design projects to teach touch in two existing courses respectively. Students have shown the great interest and the capability in adopting touch devices into their senior embedded systems design projects to improve user interactions with the computing systems.	computer engineering;embedded system;game controller;input device;interaction;mouse systems;systems design	Nannan He;Han-way Huang;Ying Qian	2016	2016 IEEE Frontiers in Education Conference (FIE)	10.1109/FIE.2016.7757625	human–computer interaction;engineering;multimedia;computer engineering	HCI	-45.5554714696035	-38.460744938767206	133008
a4271cced627bfd00aa68ffccdf26bfd2a86de91	remote drawing on vertical surfaces with a self-actuated display	vertical surface;mobile;display;drawing;sysdemo;self actuated			Patrick Bader;Norman Pohl;Valentin Schwind;Niels Henze;Katrin Wolf;Stefan Schneegaß;Albrecht Schmidt	2015			computer vision;engineering drawing;computer graphics (images)	HCI	-42.57437498270888	-40.40185584140017	133147
424f917806b068f249164e998d3fafd870a583ca	towards a low-cost framework for intelligent robots	databases;robot sensing systems;bluetooth three dimensional displays databases ieee 802 11 standards europe robot sensing systems;wireless lan cloud computing control engineering computing inference mechanisms intelligent robots mobile robots telerobotics;human computer interaction intelligent robots cloud robotics knowledge technologies intelligent systems;three dimensional displays;ieee 802 11 standards;bluetooth;europe;reasoning engine low cost framework intelligent robots cloud platform android based robot control framework mobile robots teleoperation mode wifi networks	Intelligent Robots can take advantage of a distributed, web-based information system deployed in the cloud to perform high level tasks. This paper proposes a robotic control framework suited to be used by low-cost robots, performing teloperated and/or autonomous tasks. The first part is dedicated to the development of an Android based robot control framework. This framework connects to specific low-level controllers that were developed for multicopters, wheeled/tracked mobile robots. The second part is dedicated to “place” the Android based robot in the cloud. There, the robot can perform Cloud based highly automated cognitive tasks in order to optimize their use and take best advantage of previous knowledge models, e.g., objects databases or 3D world models. Also, the robot can be controlled remotely using a classical teleoperation mode, using wifi networks. First experiments are presented when a tracked robot is performing surveillance tasks, while its state can be changed to teleoperation/videoconferencing mode, while interacting with a reasoning engine in the Cloud.	algorithm;android;arduino;autonomous robot;cloud computing;cognition;database;experiment;high- and low-level;high-level programming language;human–robot interaction;information system;input/output;interaction;internet;knowledge representation and reasoning;mobile robot;online and offline;robot control;semantic reasoner;server (computing);web application	Paulo Jorge Sequeira Gonçalves;F. M. S. Santos;Pedro M. B. Torres	2014	2014 Sixth World Congress on Nature and Biologically Inspired Computing (NaBIC 2014)	10.1109/NaBIC.2014.6921886	mobile robot;embedded system;real-time computing;simulation;computer science;social robot;robot control;bluetooth;personal robot	Robotics	-34.1425308764152	-39.28886245198623	133201
284b3e2cbee5b49b3dfc9c56d0f6ccf55105a917	two-phase calibration for a mirror metaphor augmented reality system	mirrors;video signal processing;display instrumentation;video signal processing augmented reality calibration display instrumentation error analysis mirrors video cameras;error analysis;mirrors cameras calibration augmented reality rendering computer graphics prototypes classification;video cameras;error analysis mirror metaphor augmented reality systems video see through virtual mirror displays reflective half mirror displays system configuration reflective half mirror display based augmented reality system two phase calibration method camera error sources;registration error augmented reality error analysis mirror metaphor augmented reality reflective half mirror displays;augmented reality;calibration	According to the ways to see the real environments, mirror metaphor augmented reality systems can be classified into video see-through virtual mirror displays and reflective half-mirror displays. The two systems have distinctive characteristics and application fields with different types of complexity. In this paper, we introduce a system configuration to implement a prototype of a reflective half-mirror display-based augmented reality system. We also present a two-phase calibration method using an extra camera for the system. Finally, we describe three error sources in the proposed system and show the result of analysis of these errors with several experiments.	augmented reality;complexity;experiment;prototype;system configuration;two-phase locking	Jae Seok Jang;Gi Sook Jung;Tae Hwan Lee;Soon Ki Jung	2014	Proceedings of the IEEE	10.1109/JPROC.2013.2294253	computer vision;augmented reality;calibration;simulation;computer science;computer graphics (images)	Visualization	-41.21838354066681	-39.498729095883306	133358
f5fbc999e8671edb3e0f5e85e98ae9a255eaa99c	personalized avatars for mobile entertainment	mpeg-4 fba;face animation;visual text-to-speech;virtual characters;3d modelling;3d graphics	With evolution in computer and mobile networking technologies comes the challenge of offering novel and complex multimedia applications and end-user services in heterogeneous environments for both developers and service providers. This paper describes one novel service, called LiveMail that explores the potential of existing face animation technologies for innovative and attractive services intended for the mobile market. This prototype service allows mobile subscribers to communicate using personalized 3D face models created from images taken by their phone cameras. The user can take a snapshot of someone's face  a friend, famous person, themselves, even a pet  using the mobile phone's camera. After a quick manipulation on the phone, a 3D model of that face is created and can be animated simply by typing in some text. Speech and appropriate animation of the face are created automatically by speech synthesis. Furthermore, these highly personalized animations can be sent to others as real 3D animated messages or as short videos in MMS. The clients were implemented on different platforms, and different network and face animation techniques, and connected into one complex system. This paper presents the architecture and experience gained in building such a system.	avatar (computing)	Tomislav Kosutic;Miran Mosmondor;Ivan Andrisek;Mario Weber;Maja Matijasevic;Igor S. Pandzic	2006	Mobile Information Systems		embedded system;simulation;computer science;computer animation;multimedia;3d computer graphics;computer graphics (images)	HCI	-47.917487853265186	-38.033955161043714	133511
49ba3dab5a88307ef70405f006b93664383a476d	haptic effects for virtual reality-based post-stroke rehabilitation	medical computing patient rehabilitation handicapped aids haptic interfaces virtual reality human factors force feedback;rutgers ankle;haptic effects;post stroke rehabilitation;legged locomotion;stroke rehabilitation;prototypes;patient rehabilitation;virtual reality;interaction realism;hand based interaction;foot;testing;virtual environments;medical computing;haptic interfaces foot virtual reality virtual environment medical treatment force feedback legged locomotion prototypes patient rehabilitation testing;force feedback;handicapped aids;human factors;walking simulation;patient rehabilitation haptic effects virtual reality post stroke rehabilitation haptic interfaces hand based interaction virtual environments walking simulation rutgers ankle interaction realism haptic feedback;haptic feedback;virtual environment;haptic interfaces;medical treatment	The majority of today’s haptic interfaces are designed for hand-based interaction with virtual environments. However, there are several reallife tasks that require a person to interact with the environment using one’s foot. Researchers have developed systems for simulating walking in a virtual environment. This paper describes a different approach to foot based interactions, intended for users in sitting position. A VRbased rehabilitation system using a prototype Rutgers Ankle device is presented, along with the methods of enhancing interaction realism through haptic feedback. Two application examples used for post-stroke patient rehabilitation are presented. Initial results from pilot clinical testing are briefly described.	feedback;haptic technology;high- and low-level;interaction;pneumatic artificial muscles;prototype;simulation;user interface;virtual reality	Rares F. Boian;Judith E. Deutsch;Chan Su Lee;Grigore C. Burdea;Jeffrey A. Lewis	2003		10.1109/HAPTIC.2003.1191289	simulation;human–computer interaction;engineering;multimedia	HCI	-41.59551232814494	-46.27857153633297	133587
661ce19f315aafbf5a3916684e0e7c10e642d5f1	shoesolesense: proof of concept for a wearable foot interface for virtual and real environments	mobile;virtual reality;wearable;foot;hands free;eyes free;tactile feedback;physical interface;shoe;insole	ShoeSoleSense is a proof of concept, novel body worn interface - an insole that enables location independent hands-free interaction through the feet. Forgoing hand or finger interaction is especially beneficial when the user is engaged in real world tasks. In virtual environments as moving through safety training applications is often conducted via finger input, which is not very suitable. To enable a more intuitive interaction, alternative control concepts utilize gesture control, which is usually tracked by statically installed cameras in CAVE-like-installations. Since tracking coverage is limited, problems may also occur. The introduced prototype provides a novel control concept for virtual reality as well as real life applications. Demonstrated functions include movement control in a virtual reality installation such as moving straight, turning and jumping. Furthermore the prototype provides additional feedback by heating up the feet and vibrating in dedicated areas on the surface of the insole.	natural user interface;prototype;real life;virtual reality;wearable computer	Denys J. C. Matthies;Franz Müller;Christoph Anthes;Dieter Kranzlmüller	2013		10.1145/2503713.2503740	embedded system;simulation;wearable computer;human–computer interaction;computer science;operating system;mobile technology;virtual reality;foot	Visualization	-43.72041248184262	-43.159406314531225	133590
2242c49a854ae1d38474ba998d5476328a26941a	towards implicit interaction by using wearable interaction device sensors for more than one task	context aware;human computer interaction;sensors;implicit interaction;wearable computer;wearable computing;user interaction;context recognition;activity recognition	User interaction and context awareness are key issues in today's wearable computing research. Context-aware wearable devices typically use a multitude of special purpose sensors for this. In this paper, we show for a simple test case that even simple sensors in a wearable human computer interaction device can be used for robust context detection. Furthermore, we describe in detail the recognition of different activities and discuss the results.	context awareness;human computer;human–computer interaction;sensor;test case;wearable computer;wearable technology	Hendrik Witt;Holger Kenn	2006		10.1145/1292331.1292354	computer vision;simulation;wearable computer;human–computer interaction;computer science;activity recognition	HCI	-43.548597837413695	-42.91485743891593	133705
296445019ae6bd6a5d46354a1b7571cc94fc641c	immersive telepresence system with a lovomotion interface using high-resolution omnidirectional videos	high resolution;virtual environment;motion estimation	This paper describes a novel telepresence system which enables users to walk through a photorealistic virtualized environment by actual walking. In this system, a wide-angle high-resolution video is projected on an immersive multiscreen display and a treadmill is controlled according to user’s locomotion. Calibration and motion estimation of a camera system are performed to enhance quality of presented images. The proposed system provides users with rich sense of walking in a remote site.	image resolution;interactivity;motion estimation;regular language description for xml;view synthesis	Sei Ikeda;Tomokazu Sato;Masayuki Kanbara;Naokazu Yokoya	2005			mathematics;artificial intelligence;hypophosphorous acid;computer vision;computer graphics (images);immersion (virtual reality);clostridium botulinum;corned beef;omnidirectional antenna	HCI	-41.33812306734492	-38.7194748125407	133722
c6475cfe1fc9f72f975ae1d01ecd68efacde2171	non-contact human-computer interaction system based on gesture recognition	image recognition;human computer interaction;video signal processing;interactive teaching entertainment noncontact human computer interaction system gesture recognition video cameras image recognition algorithms mouse movements computer screen trigger detection operations intelligent scheduling monitoring;human computer interaction non contact gesture recognition;high definition;cameras image color analysis skin human computer interaction image segmentation field programmable gate arrays image sensors;gesture recognition;video signal processing gesture recognition human computer interaction	A non-contact human-computer interaction system based on gesture recognition is presented in this paper. It tracks and collects the operator's gestures image via two digital high-definition video cameras at a specific location, uses Image Recognition Algorithms to detect gestures change information, and thus controls mouse movements on the computer screen. The experimental results show that this system can support the operator's fingers non-contact accurately to simulate actions of mouse clicks, synchronous positioning and trigger detection operations, etc. This system can be widely used in large screen media show, intelligent scheduling monitoring, interactive teaching entertainment and other many human-computer interaction occasions.	algorithm;computer monitor;computer vision;digital image processing;gesture recognition;hdmi;human–computer interaction;scheduling (computing);simulation;systems theory	Xiaoqiong Wang;Zhuyan Li;Jiangfeng Bai	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6022878	computer vision;intelligent character recognition;computer science;gesture recognition;sketch recognition;interaction technique;computer graphics (images)	Robotics	-39.92131559032818	-38.55860218681188	133886
76648d04563985f66dcafc8abf8524ff3730ee16	twirled affordances, self-conscious avatars, & inspection gestures	serious games;facial expression recognition;emotional development	"""Contemporary smartphones and tablets have magnetometers that can be used to detect yaw, which data can be distributed to adjust ambient media. We have built haptic interfaces featuring smartphones and tablets that use compass-derived orientation sensing to modulate virtual displays. Embedding mobile devices into pointing, swinging, and flailing affordances allows """"padiddle""""-style interfaces, finger spinning, and """"poi""""-style interfaces, whirling tethered devices, for novel interaction techniques [Cohen et al. 2013]."""	avatar (computing);haptic technology;interaction technique;mobile device;self-consciousness;smartphone;social affordance;tablet computer;yaws	Michael Cohen;Rasika Ranaweera;Kensuke Nishimura;Yuya Sasamoto;Tomohiro Oyama;Tetsunobu Ohashi;Anzu Nakada;Julián Villegas;Yong Ping Chen;Sascha Holesch;Jun Yamadera;Hayato Ito;Yasuhiko Saito;Akira Sasaki	2013		10.1145/2543651.2543691	computer vision;simulation;computer graphics (images)	HCI	-43.90065011808359	-41.90197444306787	133896
d3317e51ab1e105f767a718531cfb36b693fff90	"""the """"everywhere switch"""" using a projector and camera"""		We propose a virtual remote control interface called the “Everywhere Switch” as an alternative to multiple remote controllers for many computerized home appliances. The interface consists of a group of virtual touch buttons projected near the user from a projector affixed to a pan-tilt mount just below the living-room ceiling. Methods to implement our system, including methods to search for a place to project the virtual touch buttons, to extract finger and shadow regions on the virtual button area and determine their ratio, and to detect touch operations, are described. We evaluated the precisions of the foreground extraction (finger or its shadow) and the segmentation of the finger and its shadow under three different brightness conditions (dim, semi-bright, and bright). The foreground extraction showed an F value of more than 0.97, and the finger/shadow segmentation showed an F value of about 0.8 in all tested brightness conditions.	color;control function (econometrics);remote control;semiconductor industry;sensor;video projector	Akira Nozaki;Katsuto Nakajima	2015		10.5220/0005307601150122		HCI	-41.66973742310817	-41.01177084259195	134085
a477b9c84aa43afcdb36dcb35b000753a5711e14	evaluating perceptually complementary views for network exploration tasks		We explore the relative merits of matrix, node-link and combined side-by-side views for the visualisation of weighted networks with three controlled studies: (1) finding the most effective visual encoding for weighted edges in matrix representations; (2) comparing matrix, node-link and combined views for static weighted networks; and (3) comparing MatrixWave, Sankey and combined views of both for event-sequence data. Our studies underline that node-link and matrix views are suited to different analysis tasks. For the combined view, our studies show that there is a perceptually complementary effect in terms of improved accuracy for some tasks, but that there is a cost in terms of longer completion time than the faster of the two techniques alone. Eye-movement data shows that for many tasks participants strongly favour one of the two views, after trying both in the training phase.	eb-eye;sankey diagram;weighted network	Chunlei Chang;Benjamin Bach;Tim Dwyer;Kim Marriott	2017		10.1145/3025453.3026024	visualization;underline;encoding (memory);eye tracking;matrix (mathematics);sankey diagram;machine learning;artificial intelligence;graph drawing;computer vision;computer science	HCI	-45.547532136397955	-47.67025217388466	134121
bd362fbabc05dc9485d3e1fedfada8802e0ce5ad	metalogue: a multiperspective multimodal dialogue system with metacognitive abilities for highly adaptive and flexible dialogue management	software architecture cognition interactive systems;qa75 electronic computers computer science;educational institutions computer architecture speech artificial intelligence natural languages context training;human computer interface multi modal dialogue meta cognition systems architecture natural language;natural language;systems architecture;meta cognition;application domains multiperspective multimodal dialogue system metacognitive abilities adaptive dialogue management flexible dialogue management high level description metalogue project interactive behaviors multimodal interaction open architecture integrated metalogue system processing stages;multi modal dialogue;human computer interface	This poster paper presents a high-level description of the Metalogue project that is developing a multi-modal dialogue system that is able to implement interactive behaviors that seem natural to users and is flexible enough to exploit the full potential of multimodal interaction. We provide an outline of the initial work undertaken to define a an open architecture for the integrated Metalogue system. This system includes components that are necessary for the implementation of the processing stages for a variety of application domains: initialization, training, information gathering, orchestration, multimodality, dialogue management, speech recognition, speech synthesis and user modelling.	dialog system;dialog tree;high- and low-level;intelligent environment;modal logic;multimodal interaction;open architecture;speech recognition;speech synthesis;systems architecture	Jan Alexandersson;Maria Aretoulaki;Nick Campbell;Michael Gardner;Andrey Girenko;Dietrich Klakow;Dimitris Koryzis;Volha Petukhova;Marcus Specht;Dimitris Spiliotopoulos;Alexander Stricker;Niels Taatgen	2014	2014 International Conference on Intelligent Environments	10.1109/IE.2014.67	natural language processing;computer science;multimedia;communication	Robotics	-34.66290227207617	-40.680501179190394	134339
129e7699080c9be56d9f7158759fab04f860e795	multimodal laser-vision approach for the deictic control of a smart wheelchair	experimental tests;deictic control;user interface;laser range finder;vision;path following;smart wheelchair	This paper presents the design of the deictic functionalities for the navigation of a semi-autonomous powered wheelchair driven by a person with disability. Such functionalities, primarily based on a command by vision and a control by laser, offer an ergonomic mode of control to the user. The first functionality implemented is an automatic passing through narrow passages. The user must point the objective to be passed through, on an interface presenting an image of the environment. Then, the wheelchair moves in autonomous mode. Firstly, we describe the controlling mode for the wheelchair, the perception of the environment, the user interface and the means of path following. Then, we present and comment the results obtained during the experimental tests.	autonomous robot;human factors and ergonomics;multimodal interaction;semiconductor industry;user interface	Frédéric Leishman;Odile Horn;Guy Bourhis	2009		10.1007/978-3-642-02868-7_13	embedded system;vision;computer vision;simulation;computer science;operating system;user interface	Robotics	-40.27861282927675	-44.743436417799046	134364
1fba61cd67126f1d8e87be9847bb77703a087e0d	interactions in the air: adding further depth to interactive tabletops	3d;three dimensions;depth sensing cameras;tabletop;user feedback;interactive surfaces;switchable diffusers;three dimensional;computer vision;digital content;holoscreen;surfaces;3d graphics;tabletop display	Although interactive surfaces have many unique and compelling qualities, the interactions they support are by their very nature bound to the display surface. In this paper we present a technique for users to seamlessly switch between interacting on the tabletop surface to above it. Our aim is to leverage the space above the surface in combination with the regular tabletop display to allow more intuitive manipulation of digital content in three-dimensions. Our goal is to design a technique that closely resembles the ways we manipulate physical objects in the real-world; conceptually, allowing virtual objects to be 'picked up' off the tabletop surface in order to manipulate their three dimensional position or orientation. We chart the evolution of this technique, implemented on two rear projection-vision tabletops. Both use special projection screen materials to allow sensing at significant depths beyond the display. Existing and new computer vision techniques are used to sense hand gestures and postures above the tabletop, which can be used alongside more familiar multi-touch interactions. Interacting above the surface in this way opens up many interesting challenges. In particular it breaks the direct interaction metaphor that most tabletops afford. We present a novel shadow-based technique to help alleviate this issue. We discuss the strengths and limitations of our technique based on our own observations and initial user feedback, and provide various insights from comparing, and contrasting, our tabletop implementations	computer vision;digital recording;feedback;interaction;interactive media;multi-touch;projection screen	Otmar Hilliges;Shahram Izadi;Andrew D. Wilson;Steve Hodges;Armando Garcia-Mendoza;Andreas Butz	2009		10.1145/1622176.1622203	three-dimensional space;computer vision;human–computer interaction;multimedia;3d computer graphics;computer graphics (images)	HCI	-44.781868372676506	-39.6854834307575	134407
643b41eeb01d07f6aa1d950b79116de1b051c09b	hand gesture recognition using contour based method for tabletop surfaces	virtual reality;economic way contour based method tabletop surfaces hand gesture recognition based interactive tabletop hand detection hand extraction rotation invariant method scale invariant method chain code algorithm modified approximate string matching method virtual keyboard;surface topography;computer vision;string matching computer vision interactive table hand gesture recognition chain code;approximation theory;virtual reality approximation theory computer vision feature extraction gesture recognition surface topography;feature extraction;histograms gesture recognition augmented reality thumb cameras image color analysis;gesture recognition	In this paper, we present an interactive tabletop based on hand gesture recognition, In this case, we propose a method based histogram for hand detection and extraction. For gesture recognition we propose a scale and rotation invariant method merging Chain Code algorithm and modified approximate string matching method. With this developed system, the user can perform different gestures as zoom, move, draw, and write on a virtual keyboard. This implemented system provides more flexible, natural and intuitive interaction possibilities, and also offers an economic and practical way of interaction.	approximate string matching;chain code;gesture recognition;string searching algorithm;virtual keyboard	Abdelkader Bellarbi;Hayet Belghit;Samir Benbelkacem;Nadia Zenati-Henda;Mahmoud Belhocine	2013	2013 10th IEEE INTERNATIONAL CONFERENCE ON NETWORKING, SENSING AND CONTROL (ICNSC)	10.1109/ICNSC.2013.6548846	computer vision;speech recognition;feature extraction;computer science;gesture recognition;virtual reality;approximation theory;computer graphics (images)	Robotics	-37.594551944501454	-43.834565809756214	134442
ab185fdab6055d1487476b3cc7c942b60ccd4c07	behind the glass: driver challenges and opportunities for ar automotive applications	intelligent transportation systems augmented reality ar displays human factors;augmented reality automotive engineering visualization automotive components road traffic intelligent transportation systems;intelligent transportation systems;driver attention challenges automotive industry car driving safety video based augmented reality optical see through ar windshield heads up display center mounted display optical see through displays screen fixed ar graphics driver perception cognition challenges automotive ar systems human factors intelligent transportation system visual perceptual challenges;human factors;automotive components;cognition;visual perception;augmented reality;visual perception augmented reality automobile industry automotive components cognition helmet mounted displays human factors intelligent transportation systems road safety;road safety;automobile industry;helmet mounted displays	As the automotive industry moves toward the car of the future, technology companies are developing cutting-edge systems, in vehicle and out, that aim to make driving safer, more pleasant, and more convenient. While we are already seeing some successful video-based augmented reality (AR) auxiliary displays (e.g., center-mounted backup aid systems), the application opportunities of optical see-through AR as presented on a drivers' windshield are yet to be fully tapped; nor are the visual perceptual and attention challenges fully understood. As we race to field AR applications in transportation, we should first consider the perceptual and distraction issues that are known in both the AR and transportation communities, with a focus on the unique and intersecting aspects for driving applications. This paper describes the some opportunities and driver challenges associated with AR applications in the automotive domain. We first present a basic research space to assist in these inquiries, which delineates head-mounted from heads-up and center-mounted displays; video from optical see-through displays; and world-fixed from screen-fixed AR graphics. We then address benefits of AR related to primary, secondary, and tertiary driver tasks as well as driver perception and cognition challenges inherent in automotive AR systems.	augmented reality;backup;cognition;glass;graphics	Joseph L. Gabbard;Gregory M. Fitch;Hyungil Kim	2014	Proceedings of the IEEE	10.1109/JPROC.2013.2294642	embedded system;augmented reality;intelligent transportation system;simulation;cognition;visual perception;computer science;engineering;automotive engineering	Visualization	-40.231652216945406	-41.044555995150105	134484
18a71fcaa7c59ea9b166605558d9a963debdce21	emovoice - a framework for online recognition of emotions from voice	real time;emotion recognition	We present EmoVoice, a framework for emotional speech corpus and classifier creation and for offline as well as real-time online speech emotion recognition. The framework is intended to be used by non-experts and therefore comes with an interface to create an own personal or application specific emotion recogniser. Furthermore, we describe some applications and prototypes that already use our framework to track online emotional user states from voice information.	british informatics olympiad;emotion markup language;emotion recognition;feature extraction;online and offline;real-time locating system;speech corpus;usability testing	Thurid Vogt;Elisabeth André;Nikolaus Bee	2008		10.1007/978-3-540-69369-7_21	psychology;speech recognition;multimedia;communication	NLP	-36.170713530828685	-44.6042215626857	134632
f8fca5575ae4a216dca29d3a815d607b54542c55	a motion recognition method for a wearable dancing musical instrument	sensor equipped shoes motion recognition method wearable dancing musical instrument back ground music 3 axis wireless accelerometer;pilot study;wearable dancing musical instrument;instruments;pattern recognition accelerometers interactive devices music;3 axis wireless accelerometer;sensors;prototypes;back ground music;data mining;musical instruments;footwear;sensor equipped shoes;human interface;human interfaces;pattern recognition;motion recognition method;accelerometers;music;high speed;pattern recognition human interfaces;interactive devices	In this paper, we constructed a system for realizing a new style of dance performance that dancers play music by dancing. From pilot study, we have found that the motion recognition for dance performance needed the synchronism to back ground music (BGM). Therefore, we propose a new motion recognition method specialized to dance performances. The key techniques of the proposed method are (1) adaptive decision of the size of recognition window to recognize a motion in sync with BGM, and (2) motion recognition in two-phase (rough and detailed) to fulfill the accuracy in high speed recognition. Data was recorded using a 3-axis wireless accelerometers mounted on both shoes. We evaluated the method on a dataset of 5 different dance steps (each repeated 100 times).The results show that this method is capable of improving recognition for all steps (in one case improving recognition from 62% to 99%) while retaining a feeling of seamless connection between movement and sound.	bayesian network;performance;seamless3d;shoes;two-phase locking	Minoru Fujimoto;Naotaka Fujita;Yoshinari Takegawa;Tsutomu Terada;Masahiko Tsukamoto	2009	2009 International Symposium on Wearable Computers	10.1109/ISWC.2009.22	simulation;computer science;operating system;music;multimedia	Robotics	-42.63430965296485	-43.449758601751654	134662
4f28e16f4e2b6e67a9ee5c98f536672650f4f19b	real-time three-dimensional video image composition by depth information	real time;three dimensional	We have developed a system of compositing three-dimensional (3D) video images based on the depth information of the objects. The system consists of the Axi-Vision Camera that can measure object distance in real time and an arithmetic image processor that can synthesize video images according to the depth information. The paper demonstrates how to three-dimensionally synthesize such a scene as an array of computer-generated characters moving around a standing person in real time. The feasibility of using such a signal processor to create realistic TV programs in a broadcasting station has been studied.	real-time transcription	Masahiro Kawakita;Keigo Iizuka;Tahito Aida;Taiichirou Kurita;Hiroshi Kikuchi	2004	IEICE Electronic Express		three-dimensional space;computer vision;image processing;computer science;multimedia;computer graphics (images)	HCI	-40.142459949438155	-38.67007905891562	134715
be27daade3dd793227b8e5f15b0b5c3811d338a5	a multi-party multi-modal dataset for focus of visual attention in human-human and human-robot interaction		This papers describes a data collection setup and a newly recorded dataset. The main purpose of this dataset is to explore patterns in the focus of visual attention of humans under three different conditions two humans involved in task-based interaction with a robot; same two humans involved in task-based interaction where the robot is replaced by a third human, and a free three-party human interaction. The dataset contains two parts 6 sessions with duration of approximately 3 hours and 9 sessions with duration of approximately 4.5 hours. Both parts of the dataset are rich in modalities and recorded data streams they include the streams of three Kinect v2 devices (color, depth, infrared, body and face data), three high quality audio streams, three high resolution GoPro video streams, touch data for the task-based interactions and the system state of the robot. In addition, the second part of the dataset introduces the data streams from three Tobii Pro Glasses 2 eye trackers. The language of all interactions is English and all data streams are spatially and temporally aligned.	color depth;data structure alignment;display resolution;eye tracking;human–robot interaction;image resolution;kinect;modal logic;robot;streaming media;temporal logic	Kalin Stefanov;Jonas Beskow	2016			natural language processing;artificial intelligence;human–computer interaction;computer science;human–robot interaction	Vision	-34.7997549717504	-42.741327167177545	134737
d3475a1e6b32bc5c9ff804b572e3c9249d2d8475	using motion capture for interactive motion editing	user interface;motion capture;motion editing;computer animation	Motion capture technology has been widely used for creating character motions. Motion editing is usually also required to adjust captured motions. Because character poses which include joint rotations, body positions, and orientations are high-dimensional data, it is difficult to manipulate character poses through a conventional mouse-based interface. We propose a motion editing system that uses a motion capture device. Our system can capture a motion and edit the captured motion using the same motion capture device. Our motion-capture-based interface can specify motion editing parameters such as time period, body part selection, end-effector position, pose, and motion segment. We conducted a user study to compare our system and conventional mouse-based motion editing system. The results showed that our interface is more efficient than the conventional interface.	motion capture;robot end effector;usability testing	Masaki Oshita;Hiroyuki Muranaka	2014		10.1145/2670473.2670481	computer vision;facial motion capture;match moving;structure from motion;motion capture;simulation;computer science;motion interpolation;operating system;motion estimation;computer animation;motion field;user interface;computer graphics (images)	HCI	-39.92680492862813	-38.267251689932806	134951
164d02a390d88dcbf7d74043adb85eb572b6c56a	data collection system for the navigation of wheelchair users: a preliminary report	wheelchair;data collection;navigation;gps;qr code;system development;positional information;navigation system	In Japan, population of aged people is becoming greater. According to it, number of the wheelchair user would become larger. The government has being improved to lessen the barriers for these people. But depending on the physical power of wheel chair users, it was pointed out that there could be the places where they could not move by their physical power. This is a reason that many wheelchair users do not move by their alone. Therefore we tried to develop the navigation system to eliminate the problem. At the beginning we have developed the system to measure the required force to move the wheelchair, the width of the space of aisle where they could turn back the direction, and the position information. By the data base, we planned to develop the navigation system for wheelchair users who could move alone to working places or other places where they want to go. In this paper, we report the system developed to collect data for navigation of wheelchair users.		Yasuaki Sumida;Kazuaki Goshi;Katsuya Matsunaga	2010		10.1007/978-3-642-12179-1_45	turn-by-turn navigation;embedded system;navigation;simulation;global positioning system;computer security;statistics;data collection	Theory	-40.35693536738431	-46.50351079969636	134971
3d69f237d80d72f4439db590c7ae0c33d538b97d	enhancing fish tank vr	input device;perspective projection;computer displays virtual reality;marine animals virtual reality head computer displays navigation monitoring electrical capacitance tomography costs mathematics computer science;user study;virtual reality;stereo computer graphics;navigation;virtual world display fish tank vr systems head coupled perspective projected stereo images display device virtual workspace cadre viewing amplified head rotations depth perception negative parallax objects transparent navigation technique;fish tank vr;depth perception;computer displays;virtual worlds	Fish tank VR systems provide head coupled perspective projected stereo images on a display device of limited dimensions that resides at a fixed location. Therefore, fish tank VR systems provide only a limited virtual workspace. As a result, such systems are less suited for displaying virtual worlds that extend beyond the available workspace and depth perception problems arise when displaying objects (virtually) located on the edge of the workspace in between the viewer and the display screen. In this paper we present two techniques to reduce this disadvantage: cadre viewing andamplified head rotations . The first aims to eliminate the problems in depth perception for objects with negative parallax touching the screen surround. Subjective observations from an informal user study indicate a reduction of confusion in depth perception. The second provides a transparent navigation technique to allow users to view larger portions of the virtual world without the need for an additional input device to navigate. A user study shows it performs equally well when compared to a technique based on the use of an additional spatial input device.	depth perception;display device;head-coupled perspective;head-mounted display;input device;interaction technique;parallax;usability testing;viewing frustum;virtual world;workspace	Jurriaan D. Mulder;Robert van Liere	2000		10.1109/VR.2000.840486	computer vision;navigation;perspective;simulation;depth perception;computer science;operating system;virtual reality;input device;computer graphics (images)	Visualization	-43.794677309744465	-47.1990105141885	134975
832b3e3cf4a347c32cab537a0e513666b71802f5	unravelling seams: improvoing mobile gesture recognition with visual feedback techniques	user evaluation;seamful design;design space;mobile phone;visual feedback;mobile phones;gesture recognition;user acceptance;interaction technique	Gesture recognition is emerging as an engaging interaction technique in mobile scenarios, and high recognition rates promote user acceptance. Several factors influence recognition rates including the nature of the gesture set and the suitability of the gesture recognition algorithm. This work explores how seamfulness in gesture stroke visualization affects recognition rates. We present the results of a user evaluation of a gesture recognition system that shows that raw (seamful) visualization of low-delity gesture stroke data has recognition rates comparable to no feedback. Providing filtered (seamless) stroke visualization to the user, while retaining the un-filtered input data for recognition, resulted in a 34.9% improvement in gesture recognition rate over raw stroke data. The results provide insights into the broader design space of seamful design, and identifies areas where seamlessness is advantageous.	algorithm;gesture recognition;interaction technique;kripke semantics;seamless3d	Sven G. Kratz;Rafael Ballagas	2009		10.1145/1518701.1518844	computer vision;human–computer interaction;computer science;gesture recognition;multimedia;interaction technique	HCI	-46.98066261885919	-42.6028444893597	134996
3e2931ece575705391b1ffbfb24123f87129e5a2	contours and borders in animated mimic displays	apparent motion	Previous research has indicated that ambiguities in apparent motion (e.g., direction, rate) can result when color table techniques are used to produce animation in mimic displays. Two experiments were conducted to investigate alternative display designs in which contours (angled vs. straight) and borders (explicit vs. implicit) were varied. In Experiment 1, contours, borders, and temporal frequency interacted. At 5 Hz angled contours improved accuracy significantly. At 10 Hz explicit borders improved accuracy significantly with angled contours but degraded accuracy significantly with straight contours. In Experiment 2, the design of the angled contours was changed to convey less information and an additional border condition was added. Once again, borders and contours interacted: The widest explicit border degraded accuracy with straight contours. Angled contours also improved latency performance. The results suggest that angled contours can reduce ambiguity and improve the effectiveness of animated mimic...		Kevin B. Bennett;Ed Madigan	1994	Int. J. Hum. Comput. Interaction	10.1080/10447319409526083	computer vision;computer science;computer graphics (images)	HCI	-44.32894719383822	-48.68552669130283	135044
fafbcd9532f75ca35c965e893740971024eb2a9c	bci - tactile cursor task		1 Abstract This demonstration presents a tactile cursor task using a BCI (Brain-Computer Interface). The task will consist of mentally moving a cursor on the screen. The visitors will be invited to wear an EEG net (EGI system) and will be asked to put their finger on two Braille cells. They will be then asked to move the cursor based on the tactile stimulation. If the left cell is activated, the participants should think about moving the cursor to the left. If the right cell is activated, the participant would think about moving the cursor to the right. The BCI can predict the users’ intentions and move the cursor into the desired direction.	brain–computer interface;cursor (databases);egi;electroencephalography	Patrick Brewster;Cody Hock;Larry Flint;Alex Lanthier;Mounia Ziat;Darren Dow;Mike Hartman	2014		10.1109/HAPTICS.2014.6775563	footmouse;computer vision;computer hardware;computer science;communication	HCI	-45.38211043250982	-44.32339781964133	135135
873cdaf4f1d213a97b5d9d5343b8bb2009fd13e4	a new writing experience: finger writing in the air using a kinect sensor	image recognition;writing in the air multimedia multimedia applications microsoft kinect human computer interaction depth skin background mixture model for hand segmentation fingertip detection;human computer interaction;handwriting recognition;image segmentation;multimedia;microsoft kinect;multimedia applications;kinetics kinetic sensors handwriting recognition image color analysis fingerprint recognition;kinetic sensors;image sensors;image sensors image colour analysis image recognition image reconstruction image segmentation;writing experience numbers english characters chinese characters recognition accuracy dual mode switching method depth color nonsynchronization hand face overlapping problems hand segmentation background model skin model depth model 3d full body human pose reconstruction color information depth information microsoft kinect sensor finger writing system;fingerprint recognition;image color analysis;image colour analysis;image reconstruction;writing in the air;depth skin background mixture model for hand segmentation;kinetics;fingertip detection	With the introduction of Microsoft Kinect, there has been considerable interest in creating various attractive and feasible applications in related research fields. Kinect simultaneously captures the depth and color information and provides real-time reliable 3D full-body human-pose reconstruction that essentially turns the human body into a controller. This article presents a finger-writing system that recognizes characters written in the air without the need for an extra handheld device. This application adaptively merges depth, skin, and background models for the hand segmentation to overcome the limitations of the individual models, such as hand-face overlapping problems and the depth-color nonsynchronization. The writing fingertip is detected by a new real-time dual-mode switching method. The recognition accuracy rate is greater than 90 percent for the first five candidates of Chinese characters, English characters, and numbers.	algorithm;experiment;kinect;mobile device;optical character recognition;real-time clock;real-time locating system;real-time transcription	Xin Zhang;Zhichao Ye;Lianwen Jin;Ziyong Feng;Shaojie Xu	2013	IEEE MultiMedia	10.1109/MMUL.2013.50	iterative reconstruction;computer vision;speech recognition;computer science;image sensor;multimedia;handwriting recognition;image segmentation;fingerprint recognition;kinetics;computer graphics (images)	Robotics	-37.55113959931895	-43.89991085219947	135177
4e4670a1e63bbc95ca1e0792bc1810c59d102a86	touch projector: mobile interaction through video	input device;mobile device;multi display environments;user study;interaction techniques;multi touch;handheld device;augmented reality;interaction technique;mobile interaction	"""In 1992, Tani et al. proposed remotely operating machines in a factory by manipulating a live video image on a computer screen. In this paper we revisit this metaphor and investigate its suitability for mobile use. We present Touch Projector, a system that enables users to interact with remote screens through a live video image on their mobile device. The handheld device tracks itself with respect to the surrounding displays. Touch on the video image is """"projected"""" onto the target display in view, as if it had occurred there. This literal adaptation of Tani's idea, however, fails because handheld video does not offer enough stability and control to enable precise manipulation. We address this with a series of improvements, including zooming and freezing the video image. In a user study, participants selected targets and dragged targets between displays using the literal and three improved versions. We found that participants achieved highest performance with automatic zooming and temporary image freezing."""	computer monitor;handheld game console;literal (mathematical logic);mobile device;mobile interaction;usability testing;video projector;zooming user interface	Sebastian Boring;Dominikus Baur;Andreas Butz;Sean Gustafson;Patrick Baudisch	2010		10.1145/1753326.1753671	computer vision;augmented reality;human–computer interaction;computer science;operating system;mobile device;multimedia;computer graphics (images)	HCI	-44.10136525363396	-41.22288653125918	135369
bd744b4c44fd7a4c2d2dc766eeb385e630e6b88f	smart wheelchair control through a deictic approach	deictic;handicap;smart wheelchair;human machine interaction	In this paper, the implementation of assistance to the driving of a smart wheelchair through a deictic approach is described. Initially, a state of the art of mobility assistance, interfaces and types of commands for smart wheelchairs is presented. The deictic concept, and more particularly, the approach used for the design of our interface is examined. Then the two functionalities carried out to implement this type of interface, as well as methodology used to control our wheelchair are illustrated. Finally, the usability of this deictic approach for the assistance to the driving of a smart wheelchair is discussed.		Frédéric Leishman;Odile Horn;Guy Bourhis	2010	Robotics and Autonomous Systems	10.1016/j.robot.2010.06.007	embedded system;simulation;deixis	Robotics	-40.294092202526215	-44.858490368067635	135421
93a0687bc33239bb5da18613a41d49a849619c25	gaussbits: magnetic tangible bits for portable and occlusion-free near-surface interactions	near surface tracking;magnetism;tangible interactions;occlusion free;portable	We present GaussBits, which is a system of the passive magnetic tangible designs that enables 3D tangible interactions in the near-surface space of portable displays. When a thin magnetic sensor grid is attached to the back of the display, the 3D position and partial 3D orientation of the GaussBits can be resolved by the proposed bi-polar magnetic field tracking technique. This portable platform can therefore enrich tangible interactions by extending the design space to the near-surface space. Since non-ferrous materials, such as the user's hand, do not occlude the magnetic field, interaction designers can freely incorporate a magnetic unit into an appropriately shaped non-ferrous object to exploit the metaphors of the real-world tasks, and users can freely manipulate the GaussBits by hands or using other non-ferrous tools without causing interference. The presented example applications and the collected feedback from an explorative workshop revealed that this new approach is widely applicable.	interaction design;interference (communication);norm (social)	Rong-Hao Liang;Kai-Yin Cheng;Li-Wei Chan;Chuan-Xhyuan Peng;Mike Y. Chen;Rung-Huei Liang;De-Nian Yang;Bing-Yu Chen	2013		10.1145/2468356.2479537	magnetism;simulation;human–computer interaction;computer hardware	HCI	-44.52843388963568	-39.85465999845759	135513
a622f7b72e35fc1c8d12099ade9649288384b11f	robot-human hand-overs in non-anthropomorphic robots	home automation;human-robot interaction;service robots;turtlebot;home environment;human robot interaction community;moving tables;nonanthropomorphic robots;object hand-overs;robot-human hand-overs;teleconferencing robots;vacuum cleaners;robot-human hand-over;humar-robot interaction;spatial contrast;temporal contrast	Robots that assist and interact with humans will inevitably require to successfully achieve the task of handing over objects. Whether it is to deliver desired objects for the elderly living in their homes or hand tools to a worker in a factory, the process of robot hand-overs is one worthy study within the human robot interaction community. While the study of object hand-overs have been studied in previous works, these works have mainly considered anthropomorphic robots, that is, robots that appear and move similar to humans. However, recent trends within robotics, and in particular domestic robotics have witnessed an increase in non-anthropomorphic robotic platforms such as moving tables, teleconferencing robots and vacuum cleaners. The study of robot hand-over for non-anthropomorphic robots and in particular the study of what constitute a successful hand-over is at focus in this paper. For the purpose of investigation, the TurtleBot, which is a moving table like device is used in a home environment.	humans;human–robot interaction;robot;turtle (robot);vacuum cleaner	Prasanna Kumar Sivakumar;Chittaranjan S. Srinivas;Andrey Kiselev;Amy Loutfi	2013	2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)		human–robot interaction;robot;mobile robot;home automation;computer vision;simulation;ant robotics;computer science;artificial intelligence;trajectory;self-reconfiguring modular robot;robotics;aisoy1	Robotics	-36.86123100587765	-40.099572730590474	135566
172bf6b0ac9f966cd5aa177c233f4cbd1d27d0a9	writing with your eye: a dwell time free writing system adapted to the nature of human eye gaze	text input;dwell time;eye gaze	We investigate the usability of an eye controlled writing interface that matches the nature of human eye gaze, which always moves and is not immediately able to trigger the selection of a button. Such an interface allows the eye continuously to move and it is not necessary to dwell upon a specific position to trigger a command. We classify writing into three categories (typing, gesturing, and continuous writing) and explain why continuous writing comes closest to the nature of human eye gaze. We propose Quikwriting, which was originally designed for handhelds, as a method for text input that meets the requirements of eye gaze controlled input best. We adapt its design for the usage with eye gaze. Based on the results of a first study, we formulate some guidelines for the design of future Quikwriting-based eye gaze controlled applications.	human–computer interaction;input device;mobile device;prototype;quikwriting;requirement;stylus (computing);usability;usability testing	Nikolaus Bee;Elisabeth André	2008		10.1007/978-3-540-69369-7_13	computer vision;simulation;eye tracking;computer science;communication	HCI	-46.68350882186401	-43.89084403831113	135632
950de0d355f770b7f36511ee5dfde59602c21b0b	implementation of human-robot vqa interaction system with dynamic memory networks		One of the major functions of intelligent robots such as social or home service robots is to interact with users in natural language. Moving on from simple conversation or retrieval of data stored in computer memory, we present a new Human-Robot Interaction (HRI) system which can understand and reason over environment around the user and provide information about it in a natural language. For its intelligent interaction, we integrated Dynamic Memory Networks (DMN), a deep learning network for Visual Question Answering (VQA). For its hardware, we built a robotic head platform with a tablet PC and a 3 DOF neck. Through an experiment where the user and the robot had question answering interaction in our customized environment and in real time, the feasibility our proposed system was validated, and the effectiveness of deep learning application in real world as well as a new insight on human robot interaction was demonstrated.	computer memory;decision model and notation;deep learning;human–robot interaction;memory management;natural language;question answering;robot;tablet computer	Sanghyun Cho;Won-Hyong Lee;Jong-Hwan Kim	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122654	dynamic random-access memory;machine learning;deep learning;conversation;natural language;computer science;question answering;human–robot interaction;computer memory;artificial intelligence;server	Robotics	-33.95921551271233	-39.18843380800506	135894
9a7f46cf6cf2ef494d698989f6b8b4e3856ebc9c	league of lasers: a superhuman sport using motion tracking		League of Lasers is a motion-based game where two teams compete in a mix between football and Pong [2]. Players use `virtual mirrors' to try to guide a laser pulse towards the opponent team's target. The game aims at stimulating interaction between players by making cooperation a vital part of the gameplay, while having them physically move near each other. The game tracks the physical movement of players as the main control actions. A demonstration trailer can be found here: https://goo.gl/v7mu6k	game mechanics	Jop Vermeer;Shaad Alaka;Niels de Bruin;Nico Arjen Miedema;Nick Winnubst;Cyril Trap;Rafael Bidarra	2018		10.1145/3210299.3210307	simulation;trailer;league;social relation;football;computer science;match moving	HCI	-37.47335445168524	-39.200676277468574	135910
1ee93e4b4a49af166ec682ef07d6561577eb5642	customizable keyboard	onscreen keyboard;assistive technology	Customizable Keyboard is an on-screen keyboard designed to be flexible and expandable. Instead of giving the user a keyboard layout Customizable Keyboard allows the user to create a layout that is accommodating to the user's needs. Customizable Keyboard also allows the user to select from a variety of ways to interact with the keyboard including but not limited to using the mouse pointer to select keys and different types of scan based systems. Customizable Keyboard provides more functionality than a typical onscreen keyboard including the ability to control infrared devices such as TVs and send Twitter® Tweets.	computer keyboard;ibm pc keyboard;plover;pointer (computer programming);pointer (user interface);virtual keyboard	Eric S. Missimer;Samuel Epstein;John J. Magee;Margrit Betke	2010		10.1145/1878803.1878855	footmouse;embedded system;computer hardware;computer science;operating system	HCI	-44.67730326735813	-41.43581233964766	135987
8f4709f68b2b669d03cb7c05666bac3a98adb13c	controlling an entertainment robot through intuitive gestures	self adjusting systems gesture recognition image classification intelligent robots learning artificial intelligence neural nets;waving hand gesture;neural nets;intelligent robots;q learning;q learning entertainment robot control user intuitive gesture user control command gesture classification user intention interpretation waving hand gesture shutdown command aibo self creating neural network organizing neural network;self adjusting systems;image classification;robot control control systems home appliances communication system control turning organizing neural networks cybernetics dvd digital tv;gesture classification;entertainment robot control;user intention interpretation;aibo;organizing neural network;self creating neural network;learning artificial intelligence;user intuitive gesture;gesture recognition;shutdown command;neural network;user control command	This paper presents a novel system which learns how the users want to control the entertainment robot with intuitive gestures. Two major problems are focused on this paper. One is how to distinct the user intuitive gestures, another is how to make the correspondence between the gestures and the control commands. Users want to use their own gestures which are not known a priori. These gestures must be classified by the systems by themselves. After the classification, the system must interpret the user intention to the control command which the user want to input. For instance, one may use waving hand gesture as the shutdown command, and another may assign the same gesture to the different command such as turning around. The primitive systems are implemented on the entertainment robot named AIBO. AIBO recognizes the gestures which are classified using Self-creating and organizing neural network. AIBO also learns how to interpret them into its command using Q-Learning technique. Some experimental results show the feasibility of the proposed approach.	aibo;artificial neural network;entertainment robot;organizing (structure);q-learning;shutdown (computing)	Tomonori Hashiyama;Keiichiro Sada;Mitsuru Iwata;Shun'ichi Tano	2006	2006 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2006.385009	computer vision;contextual image classification;computer science;artificial intelligence;aibo;machine learning;artificial neural network;q-learning	Robotics	-38.31544144733762	-43.907908099568445	136185
3a8faf4530312cb5d053485c48af7ad25a67e6db	voicelabel: using speech to label mobile sensor data	multimodal interface;mobile device;mobile sensors;sensors;data collection;machine learning;speech recognition;mobile devices	Many mobile machine learning applications require collecting and labeling data, and a traditional GUI on a mobile device may not be an appropriate or viable method for this task. This paper presents an alternative approach to mobile labeling of sensor data called VoiceLabel. VoiceLabel consists of two components: (1) a speech-based data collection tool for mobile devices, and (2) a desktop tool for offline segmentation of recorded data and recognition of spoken labels. The desktop tool automatically analyzes the audio stream to find and recognize spoken labels, and then presents a multimodal interface for reviewing and correcting data labels using a combination of the audio stream, the system's analysis of that audio, and the corresponding mobile sensor data. A study with ten participants showed that VoiceLabel is a viable method for labeling mobile sensor data. VoiceLabel also illustrates several key features that inform the design of other data labeling tools.	activity recognition;desktop computer;effective method;graphical user interface;ground truth;machine learning;mobile computing;mobile device;multimodal interaction;online and offline;prototype;sensor;streaming media;taint checking	Susumu Harada;Jonathan Lester;Kayur Patel;T. Scott Saponas;James Fogarty;James A. Landay;Jacob O. Wobbrock	2008		10.1145/1452392.1452407	embedded system;speech recognition;mobile database;computer science;operating system;mobile technology;mobile device;world wide web	HCI	-36.813080943489844	-45.16090124817409	136311
0854ebace1fb36eb23cf06cf491954b6235eb8bd	long term goal oriented recommender systems		The main goal of recommender systems is to assist users in finding items of their interest in very large collections. The use of good automatic recommendation promotes customer loyalty and user satisfaction because it helps users to attain their goals. Current methods focus on the immediate value of recommendations and are evaluated as such. This is insufficient for long term goals, either defined by users or by platform managers. This is of interest in recommending learning resources to learn a target concept, and also when a company is organizing a campaign to lead users to buy certain products or moving to a different customer segment. Therefore, we believe that it would be useful to develop recommendation algorithms that promote the goals of users and platform managers (e.g. e-shop manager, e-learning tutor, ministry of culture promotor). Accordingly, we must define appropriate evaluation methodologies and demonstrate the concept on practical cases.	algorithm;computer user satisfaction;international symposium on fundamentals of computation theory;online and offline;online shopping;organizing (structure);persuasive technology;precision and recall;recommender system;scalability;sparse matrix;trusted computer system evaluation criteria;usability testing;value (computer science)	Amir Hossein Nabizadeh;Alípio Mário Jorge;José Paulo Leal	2015		10.5220/0005493505520557	computer science;data mining;recommender system;goal orientation	ML	-38.211742759883926	-52.07187230583373	136321
e2eebec8b948ca2067ffff590a93bbbf1da2951a	team monitoring and reporting for robot-assisted usar missions		This paper describes a monitoring and reporting system for robot-assisted long-term USAR missions. It is able to monitor robot activities and human verbal communication persistently and to make the information available to users in a structured multimodal interface integrating textual event descriptions, visualizations and audio playback as reports for briefing and debriefing activities as well as for creating situation awareness for new or outside participants. It is realized as a web application allowing it to be used anytime anywhere on any web enabled device. We also present results from an end-user evaluation of the system.	anytime algorithm;autonomous robot;multimodal interaction;software documentation;stationary process;vii;web application	Walter Kasper	2016	2016 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)	10.1109/SSRR.2016.7784306	simulation;engineering;knowledge management;multimedia	Robotics	-35.06351942503171	-40.82916750141827	136492
4b3f0fa6153b5b0284a9ed44c35653440a683377	comparative performance analysis of m-imu/emg and voice user interfaces for assistive robots		People with a high level of disability experience great difficulties to perform activities of daily living and resort to their residual motor functions in order to operate assistive devices. The commercially available interfaces used to control assistive manipulators are typically based on joysticks and can be used only by subjects with upper-limb residual mobilities. Many other solutions can be found in the literature, based on the use of multiple sensory systems for detecting the human motion intention and state. Some of them require a high cognitive workload for the user. Some others are more intuitive and easy to use but have not been widely investigated in terms of usability and user acceptance. The objective of this work is to propose an intuitive and robust user interface for assistive robots, not obtrusive for the user and easily adaptable for subjects with different levels of disability. The proposed user interface is based on the combination of M-IMU and EMG for the continuous control of an arm-hand robotic system by means of M-IMUs. The system has been experimentally validated and compared to a standard voice interface. Sixteen healthy subjects volunteered to participate in the study: 8 subjects used the combined M-IMU/EMG robot control, and 8 subjects used the voice control. The arm-hand robotic system made of the KUKA LWR 4+ and the IH2 Azzurra hand was controlled to accomplish the daily living task of drinking. Performance indices and evaluation scales were adopted to assess performance of the two interfaces.	assistive technology;electromyography;entity name part qualifier - adopted;evaluation function;experiment;extraction;finite-state machine;high-level programming language;joystick;kinesiology;patients;profiling (computer programming);randomized algorithm;robot (device);robot control;self-help devices;sensor;speaker recognition;usability;user interface device component;voice command device;voice user interface	Clemente Lauretti;Francesca Cordella;Francesco Scotto di Luzio;Stefano Saccucci;Angelo Davalli;Rinaldo Sacchetti;Loredana Zollo	2017	2017 International Conference on Rehabilitation Robotics (ICORR)	10.1109/ICORR.2017.8009380	activities of daily living;workload;inertial measurement unit;usability;joystick;robot control;simulation;speaker recognition;user interface;engineering	Robotics	-40.9308477962348	-45.55608694381968	136498
ab7bce392d180fa8c65a746b7e85a797d743af48	supporting responsive cohabitation between virtual interfaces and physical objects on everyday surfaces		Systems for providing mixed physical-virtual interaction on desktop surfaces have been proposed for decades, though no such systems have achieved widespread use. One major factor contributing to this lack of acceptance may be that these systems are not designed for the variety and complexity of actual work surfaces, which are often in flux and cluttered with physical objects. In this paper, we use an elicitation study and interviews to synthesize a list of ten interactive behaviors that desk-bound, digital interfaces should implement to support responsive cohabitation with physical objects. As a proof of concept, we implemented these interactive behaviors in a working augmented desk system, demonstrating their imminent feasibility.	desktop computer	Robert Xiao;Scott E. Hudson;Chris Harrison	2017	PACMHCI	10.1145/3095814	simulation;proof of concept;human–computer interaction;desk;engineering	HCI	-47.38547764881002	-41.074124255455516	136573
8fe41a140bb36a96a42d98cc1552e48530a9f4bc	concept and architecture for programming industrial robots using augmented reality with mobile devices like microsoft hololens		This paper proposes a concept for human-robot interaction using techniques of virtual and augmented reality on mobile devices like cell phones and tablets or mixed reality devices like the HoloLens. By combining data received from real robots together with the perception and abilities of a human operator innovative applications are imaginable. Visualizing not only the current robot state but also the robots environment captured with different sensors and processed with both machine and human vision can lead to a rising percentage of robot assisted workplaces or robot installations. Since the visualization of and the interaction with the robotic application is not locally restricted new or improved use cases like remote maintenance or faster startup of industrial robots can be realized. Therefore, an architecture split into three functional units and an overview of our implementation is presented.	augmented reality;human–robot interaction;industrial robot;microsoft hololens;mixed reality;mobile device;mobile phone;sensor;tablet computer	Jan Guhl;Son Tung;Jörg Krüger	2017	2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)	10.1109/ETFA.2017.8247749	real-time computing;visualization;mixed reality;robot;architecture;engineering;human–computer interaction;augmented reality;mobile device;use case;human–robot interaction	Robotics	-43.94118760772857	-41.817824354615446	136742
1dea2e0c04d43a68378212e725fd45b1a4af8548	evaluation of user sensibility experience by comparing the product use		This study drew user sensibility experience causes in the three dimensions of usability, perception and stimulation from smart-phone users who do not have use experience of I-Phone 4 or Galaxy S2. There were 10 cause factors that have impact on the sensibility experience of smart-phone users. They were; usefulness, ease of use, shortening capability, intuitive, orderly, similarity, aesthetic, difference, sensibility and fun. Among them, six causes had significant differences between the users of I-Phone 4 and Galaxy S2. These six causes were; usefulness, ease of use, intuitive, orderly, aesthetic and sensibility. Galaxy S2 received higher evaluation in usefulness, ease of use and sensibility. On the other hand, I-Phone received higher evaluation in intuitive, orderly and aesthetic.		Young Ju Lee	2012		10.1007/978-3-642-35521-9_11	human–computer interaction;systems engineering;multimedia	HCI	-48.20154921875328	-45.66529979535158	136819
148707729c2424b3b1b06601b0d874fe116946a0	haptic display for human interaction with virtual dynamic environments	haptic display;human interaction;dynamic environment	Haptics is an emerging technology that permits direct “hands-on” interaction with a virtual environment. A haptic device uses mechanical actuators to physically push a user’s finger or hand to give the sensation that he or she would have when interacting with a real physical environment. These force feedback systems have many applications, from training a surgeon to perform an operation, to assisting a child in understanding the behavior of a lever or pulley. In this paper we discuss methods and techniques to allow realistic and robust haptic interactions between a human and a complex dynamic virtual environment. Beyond modeling object penetration constraints, this work demonstrates how shading, friction, texture, and dynamics can be generated to create compelling and realistic virtual worlds. © 2001 John Wiley & Sons, Inc.	complex systems;hands-on computing;haptic technology;interaction;john d. wiley;shading;virtual reality;virtual world	Diego C. Ruspini;Oussama Khatib	2001	J. Field Robotics	10.1002/rob.8115	interpersonal relationship;simulation;human–computer interaction;engineering;multimedia	HCI	-40.989063733452184	-38.12123882157161	136868
f7ca84a2d44e6072c133d8c701d6f7618005380e	effects of interface and spatial ability on manipulation of virtual models in a stem domain	virtual environments;interface design;spatial ability;stereo;chemistry education;molecular models	Virtual models are increasingly employed in STEM education to foster learning about spatial phenomena. However, the roles of the computer interface and students’ cognitive abilities in moderating learning and performance with virtual models are not yet well understood. In two experiments students solved spatial organic chemistry problems using a virtual model system. Two aspects of the virtual model interface were manipulated: display dimensionality (stereoscopic vs. monoscopic displays) and the location of the hand-held device used to manipulate the virtual molecules (co-located with the visual display vs. displaced). The experimental task required participants to interpret the spatial structure of organic molecules and to manipulate the models to align them with orientations and configurations depicted by diagrams in Experiment 1 and three-dimensional models in Experiment 2. Co-locating the interaction device with the virtual image led to better performance in both experiments and stereoscopic viewing led to better performance in Experiment 2. The effect of co-location on performance was moderated by spatial ability in Experiment 1, and the effect of providing stereo viewing was moderated by spatial ability in Experiment 2. The results are in line with the ability-as-compensator hypothesis: participants with lower ability uniquely benefited from the treatment, while those with higher ability were not affected by stereo or co-location. The findings suggest that increased fidelity in a virtual model system may be one way of alleviating difficulties of low-spatial participants in learning spatially demanding content in STEM domains.		Trevor J. Barrett;Mary Hegarty	2016	Computers in Human Behavior	10.1016/j.chb.2016.06.026	computer vision;simulation;computer science;interface design;chemistry education;molecular model;communication;stereophonic sound	HCI	-44.337271892623555	-48.664489803333005	136894
5df0f6aa74fecf9b891b67744f552a0139f02dda	optimal 3d viewing with adaptive stereo displays for advanced telemanipulation	focusing;optimal 3d viewing;optimisation;manipulators;stereo images;adaptive stereo displays;display devices;real time;local scene;selection;virtual reality;teleoperators;layout;virtual 3d object;size control;imaging techniques;distortion;eyes;three dimensional displays;image projection;focus of attention;stereo imaging;focus of attention optimal 3d viewing adaptive stereo displays advanced telemanipulation stereo images virtual 3d object local scene stereo imaging image projection;stereo image processing;telerobotics;three dimensional displays layout cameras focusing humans eyes propulsion laboratories head size control;propulsion;humans;viewing;head;projection optimal parameter values;advanced telemanipulation;optimisation three dimensional displays stereo image processing telerobotics manipulators virtual reality;3d viewing advanced telemanipulation telemanipulation stereo imaging image;cameras	A method of optimal 3D viewing based on adaptive displays of stereo images is presented for advanced telemanipulation. The method provides the viewer with the capability of accurately observing a virtual 3D object or local scene of his/her choice with minimum distortion. The viewer is allowed to define a virtual 3D object or local scene at a desired depth as a scaled version of a real 3D object or local scene which he/she wants to focus on.The key result is an algorithm which determines in real-time the optimal parameter values associated with stereo imaging and image projection on a video screen. The selected parameter values implements the optimal stereo viewing adaptive to the viewer's focus of attention. Simulation results are shown.	algorithm;computer monitor;distortion;real-time clock;remote manipulator;simulation	Sukhan Lee;Srinivasan Lakshmanan;Sookwang Ro;Jong-Oh Park;Chong-Won Lee	1996		10.1109/IROS.1996.568944	telerobotics;layout;selection;computer vision;simulation;propulsion;distortion;computer science;artificial intelligence;virtual reality;head;display device;computer graphics (images)	Vision	-40.61673925835207	-38.59671362849843	136984
061fb815349a0695088270379c7ee68d428d64ba	computer vision based mouse	computers;computers transform coding;transform coding;simulation experiment;transport stream	We describe a computer vision based mouse, which can control and command the cursor of a computer or a computerized system using a camera. In order to move the cursor on the computer screen the user simply moves the mouse shaped passive device placed on a surface within the viewing area of the camera. The video generated by the camera is analyzed using computer vision techniques and the computer moves the cursor according to mouse movements. The computer vision based mouse has regions corresponding to buttons for clicking. To click a button the user simply covers one of these regions with his/her finger.	computer monitor;computer vision;cursor (databases)	Takeshi Takahashi;Hiroyuki Kasai;Tsuyoshi Hanamura;Hideyoshi Tominaga	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745636	embedded system;real-time computing;transform coding;telecommunications;computer science;electrical engineering;multiple description coding;mathematics;statistics	Vision	-41.69061213034504	-40.44478282868919	137129
f924a0a6eaf906ab898cd01015865c315a80f643	navbelt and the guide-cane [obstacle-avoidance systems for the blind and visually impaired]	mobile robot;mobile robots;medical robotics;handicapped aids;obstacle avoidance;software component;user machine interface navbelt guidecane mobile robotics obstacle avoidance technologies blind persons visually impaired persons ultrasonic sensor array acoustic signals stereo earphones virtual acoustic panoramic image wheeled device steering action mechanical components electronic components software components;visual impairment;panoramic image;collision avoidance;mobile robots handicapped aids medical robotics collision avoidance ultrasonic transducer arrays;sensor arrays mobile robots acoustic devices mobile computing robot sensing systems belts wearable sensors acoustic sensors headphones computer displays;ultrasonic transducer arrays;ultrasonic sensor	"""NavBelt and GuideCane are computerized devices based on advanced mobile robotics obstacle-avoidance technologies. NavBelt is worn by the user like a belt and is equipped with an array of ultrasonic sensors. It provides acoustic signals via a set of stereo earphones that guides the user around obstacles or """"displays"""" a virtual acoustic panoramic image of the traveler's surroundings. One limitation of the NavBelt is that it is exceedingly difficult for the user to comprehend the guidance signals in time to allow fast walking. A newer device, called GuideCane, effectively overcomes this problem. The GuideCane uses the same mobile robotics technology as the NavBelt but is a wheeled device pushed ahead of the user via an attached cane. When the GuideCane detects an obstacle, it steers around it. The user immediately feels this steering action and can follow the GuideCane's new path easily without any conscious effort. This article describes the two devices, including the mechanical, electronic, and software components, user-machine interface, and some experimental results."""		Shraga Shoval;Iwan Ulrich;Johann Borenstein	2003	IEEE Robot. Automat. Mag.	10.1109/MRA.2003.1191706	mobile robot;embedded system;computer vision;simulation;computer science;engineering;artificial intelligence	Robotics	-40.70329722655938	-44.27253558998153	137262
14ac3da7e083800eab410b9e0d879bceb4bd561f	construction of an intelligent room based on gesture recognition: operation of electric appliances with hand gestures	television set intelligent room gesture recognition electric appliances hand gesture pan tilt camera color information;domestic appliances;intelligent control;home appliances fingers intelligent robots data mining tv motion detection cameras skin intelligent networks man machine systems;telecontrol;telecontrol home automation intelligent control gesture recognition domestic appliances;gesture recognition;home automation	This paper proposes an intelligent room that is free of operator's position based on gesture recognition technologies. Intention and position of an operator are recognized by detecting hand waving, and pan-tilt cameras are zoomed and focused on the operator. The hand region is extracted using color information, and direction or number of fingers and motion of the hand region are detected. Home appliances such as a television set are controlled by using the direction or number of fingers and hand motions.	gesture recognition;sensor;television set	Kota Irie;Naohiro Wakamura;Kazunori Umeda	2004	2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)	10.1109/IROS.2004.1389351	embedded system;home automation;computer vision;computer science;engineering;artificial intelligence;gesture recognition;intelligent control	Robotics	-38.68730937573922	-43.84560974348858	137401
00022da3f9ebf5ca3d69da31019216d7fe5fdfbe	first international harting open source prize winner: the igus humanoid open platform		The use of standard platforms in the field of humanoid robotics can lower the entry barrier for new research groups, and accelerate research by the facilitation of code sharing. Numerous humanoid standard platforms exist in the lower size ranges of up to 60 cm, but beyond that humanoid robots scale up quickly in weight and price, becoming less affordable and more difficult to operate, maintain and modify. The igus Humanoid Open Platform is an affordable, fully open-source platform for humanoid research. At 92 cm, the robot is capable of acting in an environment meant for humans, and is equipped with enough sensors, actuators and computing power to support researchers in many fields. The structure of the robot is entirely 3D printed, leading to a lightweight and visually appealing design. This paper covers the mechanical and electrical aspects of the robot, as well as the main features of the corresponding open-source ROS software. At RoboCup 2016, the platform was awarded the first International HARTING Open Source Prize.	computer-aided design;humanoid robot;martin kay;open platform;open-source software;personalization;printing;robot operating system;robotics;sensor	Philipp Allgeuer;Grzegorz Ficht;Hafez Farazi;Michael Schreiber;Sven Behnke	2016		10.1007/978-3-319-68792-6_52	simulation;humanoid robot;scale-up;open platform;robot;software;computer science	Robotics	-37.87433506446216	-40.08219499491215	137646
565e48b824df72ffb6742a26e3f9739f6f6f44fc	communication based on frankl's psychology for humanoid robot partners using emotional model	robot sensing systems neurons educational institutions mood computational modeling;psychology humanoid robots human robot interaction;psychology;human robot interaction;humanoid robots;utterance systems frankl psychology humanoid robot partner sytem emotional model human robot interaction computational intelligence social interaction communication system;neural network robot partners emotional models computational intelligence human robot interaction	This paper discusses a robot partner system for natural communication using emotional models. In our daily life, robot partners should have an emotional model in order to co-exist and to realize natural communication with people. In this paper, we propose several emotional models for human-robot interaction based on computational intelligence. First we discuss the importance of emotion and its functions in the social interaction. Next, we propose an emotional model based on emotion, feeling, and mood. Furthermore, we use the emotional model as a method for communication system, and also, we discuss Frankl's psychology as the basis of communication. Finally, we show several experimental results of the proposed method, and discuss the utterance systems for a robot partner.	biological neuron model;computational intelligence;humanoid robot;human–computer interaction;human–robot interaction;information model;kinect;r. daneel olivaw;spontaneous order	Jinseok Woo;Naoyuki Kubota;Jun Shimazaki;Hiroyuki Masuta;Yusei Matsuo;Hun-ok Lim	2013	2013 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2013.6622536	human–robot interaction;computer science;humanoid robot;artificial intelligence;social robot	Robotics	-34.828481040575376	-39.46511169353993	137843
4cd46de77f08694b40f60a30b53d0f46b605453e	tablemouse: a novel multiuser tabletop pointing device	pointing device;interaction;collaboration;light emitting diode;collocation;device;infrared;visual tracking	This paper introduces the TableMouse, a new cursor manipulation interaction technology for tabletop computing, specifically designed to support multiple users operating on large horizontal displays. The TableMouse is a low-cost absolute positioning device utilising visually-tracked infrared light emitting diodes for button state, 3D position, 1D orientation, and unique identification information. The supporting software infrastructure is designed to support up to 16 TableMouse devices simultaneously, each with an individual system cursor. This paper introduces the device and software infrastructure and presents two applications exposing its functionality. A formal benchmarking was performed against the traditional mouse for its performance and accuracy.	cursor (databases);diode;multi-user;pointing device	Andrew Cunningham;Benjamin Close;Bruce H. Thomas;Peter Hutterer	2009		10.1145/1738826.1738854	embedded system;interaction;simulation;infrared;human–computer interaction;eye tracking;collocation;computer science;management;collaboration;light-emitting diode;pointing device	HCI	-44.66274403556597	-42.612406169564224	137868
a64985c9b8440d59b96a533d2ff7d7937a6e3a3a	whole-hand kinesthetic feedback and haptic perception in dextrous virtual manipulation	virtual reality dextrous virtual manipulation human operator kinesthetic feedback haptic feedback human haptic perception virtual physical properties exoskeleton glove grasping force distribution psychophysics;haptic device;degree of freedom;real time;virtual reality;indexing terms;device independence;dexterous manipulators;dexterous manipulators virtual reality haptic interfaces;force feedback;haptic interfaces humans force feedback virtual reality real time systems computational modeling computer simulation animation virtual environment application software;tactile display;haptic feedback;active sensing;experimental evaluation;virtual environment;haptic interfaces;sensory substitution;haptic perception;computer simulation;physical properties;haptic interaction;virtual worlds	One of the key requirements for a Virtual Reality system is the multimodal, real-time interaction between the human operator and a computer simulated and animated environment. This paper investigates problems related particularly to the haptic interaction between the human operator and a virtual environment. The work presented here focuses on two issues: 1) the synthesis of whole-hand kinesthetic feedback, based on the application of forces (torques) on individual phalanges (joints) of the human hand, and 2) the experimental evaluation of this haptic feedback system, in terms of human haptic perception of virtual physical properties (such as the weight of a virtual manipulated object), using psychophysical methods. The proposed kinesthetic feedback methodology is based on the solution of a generalized force distribution problem for the human hand during virtual manipulation tasks. The solution is computationally efficient and has been experimentally implemented using an exoskeleton force-feedback glove. A series of experiments is reported concerning the perception of weight of manipulated virtual objects and the obtained results demonstrate the feasibility of the concept. Issues related to the use of sensory substitution techniques for the application of haptic feedback on the human hand are also discussed.	algorithmic efficiency;computer simulation;experiment;haptic technology;multimodal interaction;real-time clock;requirement;sensory substitution;virtual reality	Costas S. Tzafestas	2003	IEEE Trans. Systems, Man, and Cybernetics, Part A	10.1109/TSMCA.2003.812600	computer simulation;computer vision;simulation;computer science;artificial intelligence;virtual reality;haptic technology	Robotics	-42.397981065743025	-47.881294627594485	137969
0c7a14ee933d969af5bb58cf9ca5b48a008f9630	ontology-based human-robot interaction: an approach and case study on adaptive remote control interface		The paper presents an approach to human-robot interaction in socio-cyberphysical systems. The paper propose a socio-cyberphysical system ontology that is aimed at describing the knowledge of the system resources. Humans are interacted with robots using personal smartphones. Every robot and mobile application for smartphone are designed based on the ontology that allows to support their semantic interoperability in socio-cyberphysical system. A case study considered in the paper is aimed at controlling the robots by group of humans. Group of robot consists of several robot types. Every type of robot has competencies that are described in the robot competency profile. Human experts also have own competencies that are described in the human profile. To control a robot a human should be available and have an appropriate competency for such type of robot. The paper describes the developed prototype for Android-based smartphone. The prototype implements the proposed approach and based on the developed ontology and Smart-M3 information sharing platform.	human–robot interaction;remote control	Alexey Kashevnik;Darya Kalyazina;Vladimir Parfenov;Anton I. Shabaev;Olesya Baraniuc;Igor Lashkov;Maksim Khegai	2018		10.1007/978-3-319-99582-3_13	semantic interoperability;android (operating system);robot;ontology;information sharing;human–computer interaction;remote control;competence (human resources);computer science;human–robot interaction	Robotics	-34.51483462407868	-39.28803648724374	137986
6541c1f1059446f067000ab2a251a494aea6397c	four-key text entry augmented with color blinking feedback for print-handicapped people with ocular pathology	learning process;keyboards;feedback pathology keyboards fingers light emitting diodes optical coupling retinopathy color impedance pressing;computer aided instruction;force feedback handicapped aids vision defects computer aided instruction user interfaces keyboards;text entry;visual feedback four key text entry technique color blinking feedback print handicapped people ocular pathology 2 color light emitting diode eyeglasses visually handicapped user;light emitting diode;force feedback;handicapped aids;vision defects;visual feedback;user interfaces	The print-handicapped users having ocular pathology may be under constraint of typing when they re-learn to manipulate new text entry technique. In our previous research, the 4-key text entry technique with 3 keystrokes per character was designed adhering to the principle of avoiding lateral hand or/and finger movements. In the present study, the 4-key text entry was augmented with the color blinking feedback provided through a single 2-color light emitting diode coupled with eyeglasses to facilitate learning visually handicapped users to type. The implemented approach resulted in significantly greater text entry rate of about 18 wpm under 2.5 hours of practice vs. 14 wpm and 4 hours of practice without the use of the visual feedback. The color blinking feedback could improve the learning process of text entry techniques if this kind of feedback is still possible to apply.	diode;event (computing);feedback;lateral thinking;words per minute	Tatiana Evreinova;Grigori E. Evreinov	2005	16th International Workshop on Database and Expert Systems Applications (DEXA'05)	10.1109/DEXA.2005.96	computer vision;computer science;artificial intelligence;multimedia;haptic technology;user interface;light-emitting diode	HCI	-43.45860509609461	-44.92085163293882	138047
727cb39480a2666b1c16b1a58e00cd9cde6d4f20	tactile interfaces for small touch screens	mobile device;touch screen;mobile computer;interface design;tactile feedback;tactile interface;mobile computers	We present the design, implementation, and informal evaluation of tactile interfaces for small touch screens used in mobile devices. We embedded a tactile apparatus in a Sony PDA touch screen and enhanced its basic GUI elements with tactile feedback. Instead of observing the response of interface controls, users can feel it with their fingers as they press the screen. In informal evaluations, tactile feedback was greeted with enthusiasm. We believe that tactile feedback will become the next step in touch screen interface design and a standard feature of future mobile devices.	embedded system;graphical user interface;haptic technology;mobile computing;mobile device;personal digital assistant;touchscreen;user interface design	Ivan Poupyrev;Shigeaki Maruyama	2003		10.1145/964696.964721	embedded system;human–computer interaction;computer science;interface design;operating system;mobile device;multimedia;mobile computing;tactile sensor	HCI	-46.333890622611904	-42.72218110307431	138575
075ea9a8a1978216dfcf842fb7cd2f7191715f52	supporting free throw situations of basketball players with augmented reality		This article presents our system for supporting basketball players with enhanced training information using augmented reality. A calibrated head mounted display guaranties the precise overlay of real environment and virtual objects. Visual information of the ideal and actual parabola and their deviation can be displayed. The setup is limited momentarily on free throw situations.	augmented reality;head-mounted display	Lüder A. Kahrs;Jörg Raczkowsky;Jürgen Manner;Andreas Fischer;Heinz Wörn	2006	Int. J. Comp. Sci. Sport		simulation;multimedia;advertising	HCI	-41.95216080994817	-38.739222695302836	138800
8a57a213eade081a00aee86bc16d96515c5aebd1	clog-integrated plantar visualization system for evaluating activity during walking		This study involves the development of a novel sensing system for the planta while walking on any surface and in any condition. In the study, the main aim is the visualization of the planta. A plantar image is reflected at the reflecting surface and captured by a camera embedded in a clog. The obtained visual information differs from the pressure and IMU data obtained from conventional sensors as it provides detailed information on the plantar surface. This is used to investigate the relationship between the activity in the toe area and stumbling experiences. The results indicate that high toe activity is associated with the group with no stumbling experiences and low toe activity is associated with the group with stumbling experiences.	embedded system;sensor;stumbleupon	Yingjie Jin;Miho Shogenji;Tetsuyou Watanabe	2017	2017 IEEE International Conference on Advanced Intelligent Mechatronics (AIM)	10.1109/AIM.2017.8014126	visualization;simulation;plantar surface;pressure sensor;inertial measurement unit;computer graphics (images);computer science	Robotics	-45.53253678066227	-48.76828418633818	138822
3662b5b2e3d75dc939576c3fd086f4d3c1a87d32	an optical see-through display for mutual occlusion of real and virtual environments	optical see through display;display design;color graphics display;computer graphic equipment augmented reality computer displays colour graphics hidden feature removal;degradation;virtual images colour;virtual objects;hidden feature removal;prototypes;computer graphic equipment;virtual reality;liquid crystal displays;layout;psychology;virtual environments;embedded light blocking mechanism;computer displays;layout virtual reality virtual environment prototypes liquid crystal displays cameras graphics psychology degradation spatial resolution;mutual occlusion;virtual environment;color graphics display optical see through display mutual occlusion virtual environments mixed reality system virtual objects semi transparent synthetic objects display design embedded light blocking mechanism incoming light virtual images colour;augmented reality;colour graphics;mixed reality;incoming light;semi transparent synthetic objects;mixed reality system;cameras;graphics;spatial resolution	For realizing mixed reality, which seamlessly integrates both real and virtual worlds, video seethrough displays degrade real images with system latency. Traditional optical see-through displays convey real images as they are, however, they cannot display real images covered with virtual images because they use half mirrors for superimposition. We developed a novel optical see-through display that can represent mutual occlusion of real and virtual worlds by using a transparent LCD panel.	apple ii graphics;blocking (computing);color;embedded system;mixed reality;prototype;see-through display;synthetic intelligence;virtual reality	Kiyoshi Kiyokawa;Yoshinori Kurata;Hiroyuki Ohno	2000		10.1109/ISAR.2000.880924	layout;computer vision;augmented reality;degradation;image resolution;computer science;virtual machine;graphics;operating system;liquid-crystal display;virtual reality;prototype;mixed reality;multimedia;computer graphics (images)	Visualization	-42.178260540227406	-38.829495541313925	138911
5ed6c032a4db5f026ab88534599aba5885b0c0f2	the hring: a wearable haptic device to avoid occlusions in hand tracking	cutaneous feedback hring wearable haptic device hand tracking wearable electronics business haptic feedback vibrotactile sensations wearable cutaneous device proximal finger phalanx servo motors shear force unobtrusive hand tracking systems leapmotion controller kinect sensor pick and place experiment human subjects;belts haptic interfaces skin servomotors force dc motors presses;wearable computers haptic interfaces image sensors object tracking servomotors	"""The wearable electronics business has powered over $14 billion in 2014 and it is estimated to power over $70 billion by 2024. However, commercially-available wearable devices still provide very limited haptic feedback, mainly focusing on vibrotactile sensations. Towards a more realistic feeling of interacting with virtual and remote objects, we propose a novel wearable cutaneous device for the proximal finger phalanx, called """"hRing"""". It consists of two servo motors that move a belt placed in contact with the user's finger skin. When the motors spin in opposite directions, the belt presses into the user's finger, while when the motors spin in the same direction, the belt applies a shear force to the skin. Its positioning on the proximal finger phalanx improves the capability of this device to be used together with unobtrusive hand tracking systems, such as the LeapMotion controller and the Kinect sensor. The viability of the proposed approach is demonstrated through a pick-and-place experiment involving seven human subjects. Providing cutaneous feedback through the proposed device improved the performance and perceived effectiveness of the considered task of 20% and 47% with respect to not providing any force feedback, respectively. All subjects found no difference in the quality of the tracking when carrying out the task wearing the device versus barehanded."""	haptic technology;human factors and ergonomics;information;interaction;kinect;motion controller;smt placement equipment;servo;tracking system;wearable computer;wearable technology	Claudio Pacchierotti;Gionata Salvietti;Irfan Hussain;Leonardo Meli;Domenico Prattichizzo	2016	2016 IEEE Haptics Symposium (HAPTICS)	10.1109/HAPTICS.2016.7463167	control engineering;embedded system;simulation;engineering	HCI	-41.40861330560361	-44.39295496440872	139175
2d8fd36f04d974f46e69cc916e3c68120857e66c	error analysis for tablet user interface transfers based on operational knowledge interference	gesture;error analysis;tablet pc;knowledge transfer;mental model	Operational errors were collected and analyzed with regard to the use of different tablet UIs. The effects of previous operational knowledge upon the use of new devices were clarified through user experiments in which forty subjects participated. A comparison was made of three different types of tablet UIs that were equipped with three different operating systems: iOS 5, Windows 8 (release preview), and Windows 7. The results showed the user’s dependence upon previous operational knowledge when using a new tablet PC. This dependency was demonstrated both in the ratio of the users’ accurate operation, and in their process of exploring an unknown operation.		Kazutoyo Takata;Koji Morikawa;Tsukasa Hirashima	2013		10.1007/978-3-642-39360-0_10	embedded system;simulation;computer science;multimedia	Metrics	-47.35746055338667	-46.722871184222434	139223
466a62477c5d9392a6e5d8697093358feb9da5a6	comparing fatigue when using large horizontal and vertical multi-touch interaction displays	professor patrick olivier;selina sutton;amey holden;dr ahmed kharrufa;eprints newcastle university;open access;dr jonathan hook	We report on a user study that compared muscle fatigue experienced when using a large multi-touch display in horizontal and vertical configurations over a one-hour period. Muscle fatigue is recognized as the reduction in a muscle’s capacity to generate force or power output and was measured objectively and subjectively before and after a puzzle-solving task. While subjective measures showed a significant level of overall arm muscle fatigue after the task for both configurations, objective measures showed a significant level of muscle fatigue on the middle deltoids and the non-dominant extensor digitorum for the vertical configuration only. We discuss the design implications of these findings and suggest relevant future areas of investigation.	multi-touch;usability testing	Shiroq Al-Megren;Ahmed Kharrufa;Jonathan Hook;Amey Holden;Selina Sutton;Patrick Olivier	2015		10.1007/978-3-319-22723-8_13	psychology;artificial intelligence;operations research	HCI	-46.275091480265374	-47.7669506074026	139607
5a7e308b924d7f4a5384a8bd1f45f76856f1f22a	too many pixels to perceive: subpixel shutoff for display energy reduction on oled smartphones		Organic light-emitting diode (OLED) has been widely recognized as the next-generation mobile display. Recently, smartphone manufacturers have been pushing up the pixel density of OLED display. Unfortunately, such an effort does not necessarily improve the everyday viewing because of the limitation in human visual acuity. Instead, high pixel density OLED can drain the battery power even more quickly since the power dissipation of OLED is determined by the number of displayed pixels and their RGB values, or subpixels. This paper presents a new design dimension to remedy this prevailing issue by leveraging the intuition that shutting off redundant subpixels of the display content on OLED can reduce power consumption without impacting viewing perception. We introduce ShutPix, a power-saving display system for OLED smartphones that can optimally shut off the redundant subpixels before the content is displayed. Inspired by the motivational studies, ShutPix is empowered by a suite of designs based on visual acuity, human perception, and content redundancy. Experimental results show that ShutPix can, on average, reduce 21% of display power and 15% of system power without degrading user viewing experience.	color management;diode;experiment;ibm notes;mobile phone;oled;pixel density;shutdown (computing);smartphone;subpixel rendering	Zhisheng Yan;Chang Wen Chen	2017		10.1145/3123266.3123344	pixel;artificial intelligence;computer vision;redundancy (engineering);rgb color model;subpixel rendering;computer science;pixel density;oled;intuition	HCI	-37.8122638158581	-48.36466072183378	139715
01b599031b497b95e087203a13c6e8cdaa5bf395	object schemas for responsive robotic language use	affordances robot object schema behavior based language grounding;language use;human robot interaction;affordances;natural language processing human robot interaction;collision detection;natural language;robot sensing systems robot kinematics planning visualization history context;robot;behavior based;natural language processing;planning object schemas responsive robotic language use natural language robot system verbal interaction reactivity concurrent interaction process visual trackers collision detection process physical object;language grounding;object schema	The use of natural language should be added to a robot system without sacrificing responsiveness to the environment. In this paper, we present a robot that manipulates objects on a tabletop in response to verbal interaction. Reactivity is maintained by using concurrent interaction processes, such as visual trackers and collision detection processes. The interaction processes and their associated data are organized into object schemas, each representing a physical object in the environment, based on the target of each process. The object schemas then serve as discrete structures of coordination between reactivity, planning, and language use, permitting rapid integration of information from multiple sources.	collision detection;discrete mathematics;natural language;responsiveness;robot	Kai-yuh Hsiao;Soroush Vosoughi;Stefanie Tellex;Rony Kubat;Deb Roy	2008	2008 3rd ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/1349822.1349853	natural language processing;human–robot interaction;robot;computer vision;computer science;artificial intelligence;social robot;affordance;natural language;collision detection	Robotics	-34.41958908131583	-41.09280192856313	139724
37c056593c79706446d4bb27be02c2852416e280	robust gesture processing for multimodal interaction	mobile;bottom up;multimodal interface;mobile device;finite state methods;mobile computer;speech gesture integration;language processing;multimodal interaction;robustness;spoken language processing;multimodal interfaces;local search	With the explosive growth in mobile computing and communication over the past few years, it is possible to access almost any information from virtually anywhere. However, the efficiency and effectiveness of this interaction is severely limited by the inherent characteristics of mobile devices, including small screen size and the lack of a viable keyboard or mouse. This paper concerns the use of multimodal language processing techniques to enable interfaces combining speech and gesture input that overcome these limitations. Specifically we focus on robust processing of pen gesture inputs in a local search application and demonstrate that edit-based techniques that have proven effective in spoken language processing can also be used to overcome unexpected or errorful gesture input. We also examine the use of a bottom-up gesture aggregation technique to improve the coverage of multimodal understanding.	display size;gesture recognition;local search (optimization);mobile computing;mobile device;multimodal interaction;television	Srinivas Bangalore;Michael Johnston	2008		10.1145/1452392.1452439	natural language processing;speech recognition;computer science;local search;operating system;mobile technology;multimodal interaction;top-down and bottom-up design;gesture recognition;mobile device;mobile computing;robustness	HCI	-47.787423910262405	-43.997295605312736	139808
85947a6a6179c8410e7f0966b8ba6e70ac299e8f	an exploration of search session patterns in an image-based digital library		Three months of server transaction logs containing complete clickstream data for an image collection digital library were analysed for usage patterns to better understand user searching and browsing behaviour in this environment. Eleven types of user actions were identified from the log content. The study is novel in its combined analytical techniques and use of clickstream data from an image-based digital library. Three analytical techniques were used to analyse the data: a network analysis to better understand the relationship between sequential actions; b sequential pattern mining to identify frequent action sequences; and c k-means cluster analysis to identify groups of session patterns. The analysis revealed strong ties between several pairs of actions, relatively short pattern sequences that frequently duplicate previous actions and largely uniform session behaviour with little individual item browsing within sessions, indicating users are primarily engaged in purposeful and directed searching. Developers of image-based digital libraries should consider design features that support rapid browsing.	digital library;session (web analytics)	Hye Jung Han;Dietmar Wolfram	2016	J. Information Science	10.1177/0165551515598952	multimedia;world wide web;information retrieval	HPC	-34.923113428154025	-50.79204343220588	139852
dff5dea784749b92767428cef40afed161c1f2de	visual fixation for 3d video stabilization		Visual fixation is employed by humans and some animals to keep a specific 3D location at the center of the visual gaze. Inspired by this phenomenon in nature, this paper explores the idea to transfer this mechanism to the context of video stabilization for a handheld video camera. A novel approach is presented that stabilizes a video by fixating on automatically extracted 3D target points. This approach is different from existing automatic solutions that stabilize the video by smoothing. To determine the 3D target points, the recorded scene is analyzed with a stateof-the-art structure-from-motion algorithm, which estimates camera motion and reconstructs a 3D point cloud of the static scene objects. Special algorithms are presented that search either virtual or real 3D target points, which back-project close to the center of the image for as long a period of time as possible. The staDigital Peer Publishing Licence Any party may pass on this Work by electronic means and make it available for download under the terms and conditions of the current version of the Digital Peer Publishing Licence (DPPL). The text of the licence may be accessed and retrieved via Internet at http://www.dipp.nrw.de/. First presented at the 6th European Conference on Visual Media Production (CVMP 2009) under the title: ”Sceneaware Video Stabilization by Visual Fixation”, extended and revised for JVRB bilization algorithm then transforms the original images of the sequence so that these 3D target points are kept exactly in the center of the image, which, in case of real 3D target points, produces a perfectly stable result at the image center. Furthermore, different methods of additional user interaction are investigated. It is shown that the stabilization process can easily be controlled and that it can be combined with state-of-theart tracking techniques in order to obtain a powerful image stabilization tool. The approach is evaluated on a variety of videos taken with a hand-held camera in natural scenes. keywords : video stabilization, visual fixation, camera shake, camera motion estimation, structure-frommotion.	algorithm;download;handheld game console;internet;mobile device;motion estimation;point cloud;smoothing;structure from motion	Christian Kurz;Thorsten Thormählen;Hans-Peter Seidel	2011	JVRB		image stabilization;structure from motion;camera auto-calibration;video tracking;smoothing;motion field;computer vision;artificial intelligence;computer science;phenomenon;video camera	Vision	-34.17333072431448	-43.64184369308013	139872
c9c5eac0cd24c545c00072a5c3bf80c3e10150c4	understanding, evaluating and analyzing touch screen gestures for visually impaired users in mobile environment		Smartphones usage among visually impaired users is growing in prominence and mobile phone providers are continuously looking for solutions to make touch screen interfaces more accessible to them. Key accessibility features for vision related impairment includes assistive screen reading applications like Voiceover in iOS or Talkback in Android which supports a variety of touch gestures for performing basic functions and commands. Our preliminary interactions with users from this community revealed that some of these existing gestures are ambiguous, difficult to perform, non-intuitive and have accuracy and detection issues. Moreover there is lack of understanding regarding usage of these accessibility features and existing gestures. In this paper, we address these challenges through set of three experimental exercises-task based comparative evaluation, gesture elicitation and gesture performance done with a group of 12 visually impaired users. Based on experimental evidences we pinpoint the exact problems with few existing gestures. Additionally, this work contributes in identifying some characteristics of effective and easy gestures for the target segment. We also propose design solutions to resolve users pain points and discuss some touch screen accessibility design guidelines keeping in mind different type of visually impaired users fully blind, extremely low vision and low vision.	accessibility;ambiguous grammar;android;experiment;feedback;gesture recognition;interaction;mobile phone;screen reading;smartphone;touchscreen;user interface;ios	Vikas Luthra;Sanjay Ghosh	2015		10.1007/978-3-319-20681-3_3	human–computer interaction;engineering;multimedia;communication	HCI	-47.262693327537136	-43.88754085847291	140037
4524a8f77cf4ab7f39fd9b2ef82a2b56acaa90ec	drishti: an integrated indoor/outdoor blind navigation system and service	blind navigation system;step by step walking guidance;wireless connection;vocal prompts;legged locomotion;global positioning system mobile computing handicapped aids navigation user interfaces wearable computers position measurement;routing;ultrasound;vocal communication interface;ultrasonic imaging;indoor location measurements;wearable computers;ultrasonic variables measurement;measurement system;navigation position measurement switches indoor environments wearable computers global positioning system routing ultrasonic imaging ultrasonic variables measurement legged locomotion;dynamic routing;blind users;navigation;visually impaired people;location system;handicapped aids;global positioning system;lessons learned;optimal routing;drishti system;indoor environment;outdoor system;oem ultrasound positioning system;position measurement;indoor environments;vocal communication;positioning system;visual impairment;dgps;outdoor system blind navigation system visually impaired people precise position measurement system wireless connection wearable computer vocal communication interface blind users dgps location system optimal route dynamic routing indoor environment vocal command oem ultrasound positioning system indoor location measurements vocal prompts step by step walking guidance drishti system indoor navigation design;wearable computer;navigation system;vocal command;switches;mobile computing;indoor navigation design;user interfaces;optimal route;precise position measurement system	There are many navigation systems for visually impaired people but few can provide dynamic interactions and adaptability to changes. None of these systems work seamlessly both indoors and outdoors. Drishti uses a precise position measurement system, a wireless connection, a wearable computer, and a vocal communication interface to guide blind users and help them travel in familiar and unfamiliar environments independently and safely. Outdoors, it uses DGPS as its location system to keep the user as close as possible to the central line of sidewalks of campus and downtown areas; it provides the user with an optimal route by means of its dynamic routing and rerouting ability. The user can switch the system from an outdoor to an indoor environment with a simple vocal command. An OEM ultrasound positioning system is used to provide precise indoor location measurements. Experiments show an in-door accuracy of 22 cm. The user can get vocal prompts to avoid possible obstacles and step-by-step walking guidance to move about in an indoor environment. This paper describes the Drishti system and focuses on the indoor navigation design and lessons learned in integrating the indoor with the outdoor system.	differential gps;interaction;positioning system;routing;system of measurement;wearable computer	Lisa Ran;Abdelsalam Helal;Steve Moore	2004	Second IEEE Annual Conference on Pervasive Computing and Communications, 2004. Proceedings of the	10.1109/PERCOM.2004.1276842	embedded system;computer vision;simulation;wearable computer;computer science;operating system;mobile computing;computer network	HCI	-39.05876688717371	-44.51199359123703	140055
28820a3f14a1b2621c269160278f745b00b8ca56	unconscious guidance of pedestrians using vection and body sway		In daily life our behavior is guided by various visual stimuli, such as the information on direction signs. However, our environmentally-based perceptual capacity is often challenged under crowded conditions, even more so in critical circumstances like emergency evacuations. In those situations, we often fail to pay attention to important signs. In order to achieve more effective direction guidance, we considered the use of unconscious reflexes in human walking. In this study, we experimented with vision-guided walking direction control by inducing subjects to shift their gaze direction using a vection stimulus combined with body sway. We confirmed that a shift in a subject’s walking direction and body sway could be induced by a combination of vection and vibratory stimulus. We propose a possible mechanism for this effect.		Norifumi Watanabe;Takashi Omori	2012		10.1007/978-3-642-34274-5_59	reflex;cognitive psychology;unconscious mind;perception;stimulus (physiology);psychology;visual perception	HCI	-46.053368909224	-50.660622287281534	140269
876cd20f86d205fe11306e38638717ade4fa232d	evaluation of mono/binocular depth perception using virtual image display		Augmented reality (AR) is a very popular technology in various applications. It allows the user to see the real world, with virtual objects composited with or superimposed upon the real world. The usability of interactive user interface based on AR relies heavily on visibility and depth perception of content, virtual image display particularly. In this paper, we performed several basic evaluations for a commercial see-through head mounted display based on those factors that can change depth perception: binocular or monocular, viewing distance, eye dominance, content changed in shape or size, indicated by hand or reference object. The experiment results reveal many interesting and fascinating features. The features will be user interface design guidelines for every similar see-through near-eye display systems.		Shys-Fan Yang-Mao;Yu-Ting Lin;Ming-Hui Lin;Wen-Jun Zeng;Yao-lien Wang	2013		10.1007/978-3-642-39342-6_53	stereoscopy;computer vision;stereo display;autostereogram	HCI	-41.82617038736463	-39.17714541464262	140403
d83598e16c7a3830b1bc603dc159db234a50bde7	positional prediction: consonant cluster prediction text entry method for burmese (myanmar language)	mobile device;user study;text entry;text input;syllabic languages;predictive text entry;myanmar language;positional prediction	I am investigating a consonant cluster prediction text entry method for Myanmar language based on positional vowel information. The concept of this method is adapted by inspecting and carefully considering the nature of Myanmar language word formations or hand writing orders. According to the initial user study with our software keyboard prototype, users were able to type at ~41.57 characters per minute with mouse, ~41.84 characters per minute with stylus pen, ~14.76 characters per minute with Logitech dual thumbstick game pad and ~20.52 characters per minute with Nintendo Wii remote controller. These numbers are sure to improve as further refinements on program interface are made. The merits of the proposed predictive text input method is that even first time users can type Myanmar sentences with appropriate typing speed, and that this approach is applicable for various kinds of mobile devices (see Figure.1) and extendable for other similar syllabic languages such as Khmer (language of Cambodia), Nepali (language of Nepal), Thai (language of Thailand) and Hindi (one of the languages of India) etc.	analog stick;extensibility;gamepad;input method;mobile device;predictive text;prototype;remote control;stylus (computing);usability testing;virtual keyboard;wii remote plus;words per minute	Ye Kyaw Thu	2008		10.1145/1358628.1358930	natural language processing;speech recognition;computer science;operating system;mobile device	NLP	-47.46936406860156	-44.731552003013796	141055
e79ced11805937c858db166fe00fd5e7afe0b69f	smart walker development based on experts' feedback for elderly and disabled	motorcycles electric vehicles handicapped aids medical robotics mobile robots;expert feedback smart mobile walker sit to stand;mobile robots;medical robotics;motorcycles;handicapped aids;smw experts feedback smart mobile walker elderly aid disabled aid walking aid sit to stand aid electric scooter rehabilitation hospitals robotic support system;legged locomotion motorcycles mobile communication force senior citizens hospitals vectors;electric vehicles	This paper introduces a smart mobile walker which was developed for elderly and disabled. It supports three features, which are walking aid, sit-to-stand aid, and electric scooter. Implementation of each function is briefly described. The overall performance of the smart mobile walker is evaluated by experts in rehabilitation hospitals and continuously being improved.	amiga walker	Gyung-Hwan Yuk;Yoon Young Chang;HongSoo Park;Byung Ju Dan;Woo Sok Chang	2013	2013 10th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2013.6677337	mobile robot;embedded system;simulation;computer science;artificial intelligence	Robotics	-40.287586260193045	-45.76837563308602	141104
2633d3d1f09ebc8714f65788261af40439a9e1a0	button component encasing for wearable technology applications	user interfaces clothing temperature sensors ubiquitous computing textile fibres yarn;yarn;textile technology galvanizing contacts clothing prototypes optical fiber testing usability temperature sensors optical fiber communication optical fiber sensors;temperature sensors;null;temperature sensor;electrically conductive fiber yarn button component encasing wearable technology application galvanic contact hard electronics textiles clothing temperature sensor sewing;ubiquitous computing;clothing;electric conductivity;textile fibres;user interfaces	This paper describes a novel button component encasing for wearable technology applications. The button covers the electronics inside and forms a galvanic contact for communications between the hard electronics and textiles. However, the appearance and usability of the clothing are maintained. A prototype for the encasing has been implemented, in which a temperature sensor is embedded inside the button. The contact between the button and clothing is implemented by sewing with electrically conductive fiber yarn. Prototype tests have shown that the encasing is directly suitable to be used in a range of applications and galvanic connections are possible to implement by sewing with electrically conductive fibers.	embedded system;galvanic isolation;optical fiber;prototype;usability;wearable technology	Jaana Htinnikainen;Jussi Mikkonen;Jukka Vanhala	2005	Ninth IEEE International Symposium on Wearable Computers (ISWC'05)	10.1109/ISWC.2005.14	human–computer interaction;computer science;clothing;user interface;electrical resistivity and conductivity;ubiquitous computing	Embedded	-42.20010709968934	-41.755018782985296	141204
df486e221fb759e90f1e087964db6ac2f5cff9b8	dancing game by digital textile sensor, accelerometer and gyroscope	microcontrollers;gyroscopes;games accelerometers gyroscopes microprogramming textiles wires virtual reality;dance game;gyroscope;wearable computers;mixed reality game;virtual reality;wires;firmware;pressure sensor;mixed reality game dancing game digital textile sensor accelerometer gyroscope pressure sensors socks firmware microcontroller wearable entertainment system;sensor in textile;games;wearable computers accelerometers computer games firmware gyroscopes microcontrollers pressure sensors textiles;accelerometer;pressure sensors;textiles;microprogramming;computer games;accelerometers;mixed reality game sensor in textile accelerometer gyroscope body movement dance game;body movement	A novel dancing game, comprised of pressure sensors on socks with accelerometer and gyroscope to detect the movement of the player, is presented. The firmware in microcontroller can judge the movement of the player with enough accuracy such that the player would not be limited by wires and resident equipments. We designed a novel wearable entertainment system to provide a mixed reality game in which users can play dancing game.	firmware;gyroscope;microcontroller;mixed reality;socks;sensor;wearable computer	Chang-Ming Yang;Jwu-Sheng Hu;Ching-Wen Yang;Chih-Chung Wu;Narisa N. Y. Chu	2011	2011 IEEE International Games Innovation Conference (IGIC)	10.1109/IGIC.2011.6115112	embedded system;simulation;computer hardware;engineering	Visualization	-41.97542838130401	-43.17067861751032	141233
32a129f0c66f8ee89ab5980dcf3a1a39ee8ee213	human robot interaction from visual perception	human intention human robot interaction visual perception gesture based guidance robot control;human computer interaction;real time;service robots;human robot interaction;control engineering computing human computer interaction user interfaces visual perception service robots robot programming;visual perception;control engineering computing;guidance and control;human robot interaction visual perception robot programming robot sensing systems robot vision systems force measurement force control sensor systems biomedical computing force sensors;user interfaces;robot programming	As robotics evolves towards application fields in which humans cooperate with robots, working closer and closer, the requirements for human robot interactions increase. This paper presents new advances in gesture based guidance and control of a robot in assistant tasks. The work described includes the integration of force and vision for the quantitative and qualitative appreciation of human intention and the robot behaviors to respond in real time to these human orders.	humans;human–robot interaction;requirement;robot;robotics	Josep Amat;Manel Frigola;Alicia Casals	2004	2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)	10.1109/IROS.2004.1389691	human–robot interaction;mobile robot;robot learning;computer vision;tico robot;simulation;human–computer interaction;visual perception;computer science;engineering;artificial intelligence;social robot;arm solution;robot control;ubiquitous robot;visual servoing;user interface;mobile robot navigation;personal robot	Robotics	-36.07933636204164	-40.91001190101046	141254
fae8df30b12fec94a2258480de9388d75c3ab011	lower-limb movement assistance through wearable robots: state of the art and challenges	human interaction;ambient intelligence;lower limb exoskeletons;ambient assisted living;rehabilitation robotics;elderly person;focus of attention;wearable robots;lower limb;industrial application;functional electrical stimulation	Recent technological advances made necessary the use of robots in various types of applications. Unlike traditional robot-like scenarios dedicated to industrial applications with repetitive tasks, the current focus of attention is on applications that require close human interactions. One of the main fields of such applications concerns assisting and rehabilitating of dependent/elderly persons. In this study, the state-of-the-art of the main research advances in lower-limbs human assistance is presented. This includes a review on research covering mainly lower-limb actuated exoskeletons. Some case studies related to full-limb exoskeletons are presented as well. Lower-limb movement restoration using functional electrical stimulation and treadmillbased rehabilitation devices is also investigated. In addition, the seamless integration of wearable robots in ambient intelligence spaces appears as one of the major challenges for ubiquitous environments and ambient assisted living. Some open issues related to this integration are discussed in this paper. © Koninklijke Brill NV, Leiden and The Robotics Society of Japan, 2012	ambient intelligence;circuit restoration;functional electrical stimulation;interaction;nv network;powered exoskeleton;robot;robotics;seamless3d;sensor;shin megami tensei: persona 3;wearable computer	Hala Rifai	2012	Advanced Robotics	10.1163/016918611X607356	interpersonal relationship;simulation;ambient intelligence;computer science;engineering;biological engineering	Robotics	-39.346423477795334	-46.25749344971024	141321
5eec6d699dc3639a190975649c184785ec013f31	evaluation of gesture-command input method for pen-based group kj system		The processes involved in carrying out collaborative creative work and the ability to record and circulate the results are important. Previously, we proposed a label-capturing system for the group KJ method that uses anoto-based pens. The system instantly digitizes the positions and contents of labels containing ideas and thoughts, and then generates KJ diagrams for distribution. We also proposed a method for editing the visual properties of KJ diagrams using pen-writing gestures with physical transparent sheets on target labels. However, when the number of functions increases, the management of the physical transparent sheet becomes difficult. In this study, we verified the suitability of the system by introducing a user-defined gesture command framework. This framework increases the flexibility of user operation as well as the number of functions acceptable by the system. We confirmed the suitability through preliminary evaluation of the system with respect to recognition.	diagram;handwriting recognition;input method	Takahiro Nyu;Motoki Miura	2012			creative work;input method;gesture;computer science;computer hardware	HCI	-39.26787823388713	-41.27512526210058	141457
f22f832bab87f8b3a0c140b635cd73ca0f9efd9c	performance evaluation of the wheel navigation key used for mobile phone and mp3	performance evaluation;performance test;mobile phone;wheel navigation key;button key;usability;mp3	The aim of the study was to investigate the usability of wheel-type navigation key of cell phone and MP3 product. An experiment was designed to evaluate the functional benefit of wheel navigation key by using performance test. A questionnaire was also used to examine the personal preference. Eighteen subjects were recruited. In results, a significant difference was found in performance time between wheel-type and button-type product. In general, the difference was more significant as subject's skill level was higher. In questionnaire, different preference depending on the skill level and key type was reported. In conclusion, it was shown that the wheel-type navigation key improved the performance better as the skill level and search requirement became higher. Therefore, the wheel navigation key would be helpful device for users if we could use them selectively in order to speed up the searching task and replacing simple push button task.		Hyun-Wook Jung;Jung-Yong Kim	2007		10.1007/978-3-540-73287-7_61	embedded system;simulation;engineering;multimedia	HCI	-47.30680683793904	-45.66845817078914	141511
7a8cd91b0675d6ab279248cba11d9c8052ebe2e4	stick and roll: a physical interactive system using curved displays and rolling batons	curved display;rolling;baton;picture book	We present a physical interactive system in which a player manipulates batons as interface on curved displays. Touching of batons on the displays are detected by sensor-sheets based on electro-magnetic induction. To show the feasibility and effectiveness of the system, we developed digital picture book contents in which a player uses batons in various ways such as rolling, pointing, and sweeping to interact with the sceneries.	digital camera;interactivity	Akihiro Matsuura;Masato Ohno;Hiroki Tone	2016		10.1145/3005274.3005319	simulation;rolling;computer graphics (images)	HCI	-42.83277991210054	-39.48677702598931	141758
ac8d2de567aea1fe228b88f3bd43137cf3354a24	3d-hudd - developing a prototyping tool for 3d head-up displays		The ability of head-up displays (HUDs) to present information within the usual viewpoint of the user has led to a quick adoption in domains where attention is crucial, such as in the car. As HUDs employ 3D technology, further opportunities emerge: information can be structured and positioned in 3D space thus allowing important information to be perceived more easily and information can be registered with objects in the visual scene to communicate a relationship. This allows novel user interfaces to be built. As of today, however, no prototyping tools exist, that allow 3D UIs for HUDs to be sketched and tested prior to development. To close this gap, we report on the design and development of the 3D Head-Up Display Designer (3D-HUDD). In addition, we present an evaluation of the tool with 24 participants, comparing different input modalities and depth management modes.	head-up display;prototype;usability	Nora Broy;Matthias Nefzger;Florian Alt;Mariam Hassib;Albrecht Schmidt	2015		10.1007/978-3-319-22723-8_24	simulation;human–computer interaction;multimedia	HCI	-47.181647586600874	-41.24549692423178	142340
5e74accdbab047fb24272dd7df0f776ac35cea94	designing an encountered-type haptic display for multiple fingertip contacts based on the observation of human grasping behavior	haptic display;fingertip contact;grasping;encountered type haptic display;haptic device;three fingered grasping encountered type haptic display multiple fingertip contact human grasping behavior haptic devices virtual object;virtual reality;teleoperators;dexterous manipulators;virtual object;human factors;human grasping behavior;displays;haptic interfaces displays humans grasping virtual environment teleoperators;grasping encountered type;humans;virtual environment;haptic interfaces;observation of human motion;haptic devices;multiple fingertip contact;three fingered grasping;dexterous manipulators haptic interfaces human factors virtual reality;haptic interface	Unlike conventional haptic devices, an encountered-type device is not held by a user all the time. Instead, the device stays at the location of a virtual object and waits for the user to encounter it. In this paper, we extend this concept to fingertip contacts and design an encountered-type haptic display for multiple fingertip contacts to simulate tasks of grasping an object with any shape and size. Before designing the device, we intensively observed human grasping behaviors. This observation was very helpful to determine the mechanism of the device. An encountered-type device for three-fingered grasping was actually prototyped based on our design.	haptic technology;simulation;waits	Yasuyoshi Yokokohji;Nobuhiko Muramori;Yuji Sato;Tsuneo Yoshikawa	2003	12th International Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, 2004. HAPTICS '04. Proceedings.	10.1109/HAPTIC.2004.1287179	computer vision;simulation;communication	Robotics	-45.161856564497455	-49.40704994532387	142390
1f736b39ef43e3dcc82e6c62a38b94a630b7d830	human-based video browsing - investigating interface design for fast video browsing	interaction design video browsing video search interfaces;visualization query processing layout navigation multimedia communication streaming media browsers;video browser showdown interface design query processing video indexing interactive aspects human based browsing indexed archive prefiltering video search systems vbs;video search interfaces;video signal processing information retrieval online front ends;video browsing;interaction design	The Video Browser Showdown (VBS) is an annual event where researchers evaluate their video search systems in a competitive setting. Searching in videos is often a two-step process: first some sort of pre-filtering is done, where, for example, users query an indexed archive of files, followed by a human-based browsing, where users skim the returned result set in search for the relevant file or portion of it. The VBS aims at this whole search process, focusing in particular on its interactive aspects. Encouraged by previous years' results, we created a system that purely addresses the latter issue, i.e., interface and interaction design. By eliminating all kind of video indexing and query processing, we were aiming to demonstrate the importance of good interface design for video search and that its relevance is often underestimated by today's systems. This claim is clearly proven by the results our system achieved in the VBS 2015 competition, where our approach was on a par with the top performing ones. In this paper, we will describe our system along with related design decisions, present our results from the VBS event, and discuss them in further detail.	archive;browsing;database;interaction design;relevance;result set;smart common input method;vbscript	Wolfgang Hürst;Rob van de Werken	2015	2015 IEEE International Symposium on Multimedia (ISM)	10.1109/ISM.2015.104	computer science;interaction design;video tracking;database;multimedia;smacker video;world wide web;information retrieval	Visualization	-35.65464965698499	-50.41871627180349	142442
6ea9e161637ee59498ad36e5ceb2ed8ff591bddc	using locomotion models for estimating walking targets in immersive virtual environments	virtual reality gait analysis human computer interaction;legged locomotion virtual environments tracking cost function planning solid modeling trajectory;prediction virtual reality redirection human locomotion;virtual reality;human locomotion;prediction;human locomotion locomotion models walking targets estimation immersive virtual environments redirected walking physical tracking space redirection technique steering algorithms;redirection	Redirected Walking allows a person to explore unlimited virtual environments in a limited physical tracking space. To prevent the user from colliding with the physical boundaries of the tracking space, so-called redirection techniques are used. These techniques introduce a subtle mismatch between the user's real and virtual movement and therefore keep him inside the tracking space while at the same time they allow him to explore an unlimited virtual environment. In most cases, there is more than one redirection technique available, and steering algorithms are used to select the best one at any given time. These algorithms use an optimal control scheme to select the optimal redirection action based on a prediction of the user's future path. In this paper, we present a novel approach for predicting a person's locomotion target. Using a set of known possible targets and models of human locomotion, this approach creates a set of expected paths and compares them to the path already traveled by the user in order to estimate the probability of the user heading for a certain target. We present a new approach for comparing two paths and evaluate its performance against three other approaches. We also compare four different ways of modeling a human's path to a target. To gather data for the comparison, a user study is conducted and the prediction performance of the different proposed approaches is discussed.	algorithm;course (navigation);dynamic time warping;eye tracking;graph (discrete mathematics);loss function;optimal control;redirected walking;redirection (computing);usability testing;virtual reality;waypoint	Markus Zank;Andreas M. Kunz	2015	2015 International Conference on Cyberworlds (CW)	10.1109/CW.2015.20	computer vision;simulation;prediction;computer science;virtual reality;statistics	Visualization	-38.61245800488507	-39.445134727121236	142519
816a86e047d4115e49301fa4a67ed1a406919da2	hovr-type: smartphone as a typing interface in vr using hovering		We propose a text entry method for VR, using the smartphone and its hovering function, called the HoVR-Type. The hovering function effectively acts as the finger tracking sensor thereby allowing the user to type in the virtual space. When added with the additional phase to correct the initial touch input and having the final key entered upon the finger release, the proposed method showed competitive performance to that of the conventional “aim-and-shoot” method and also exhibited much higher usability. HoVR-Type also showed a significantly faster speed of input for the individual character. However, it remains to improve the interface with regards to reducing the error. Overall, the use of the smartphone leverages on the already established mobile user experience and can be further extended to other VR interaction techniques so that one can use the common smartphone as an all-purpose VR interaction device.	smartphone	Youngwon R. Kim;Gerard Jounghyun Kim	2017		10.1109/ICCE.2017.7889285	embedded system;simulation;computer hardware;engineering;operating system	HCI	-46.48666226557304	-44.55218820298086	142534
23f54af98944a47fecb918f847cbd3f081330ab0	papertab: tablets as thin and flexible as paper	e ink;organic user interfaces;flexible displays;manipulation;physical	We present PaperTab, a paper tablet computer that allows physical manipulation of windows embodied in multiple flexible displays. PaperTab offers the benefits of updating electronic information on the fly, while maintaining the haptic/kinesthetic feedback of tangible documents, as each document is a fully functional, paper-like E Ink display. We present windowing techniques for a paper computer that relies on multiple physical windows. Our between-display interactions are based on the proximity of a display to the user. They are categorized into hot zones, for active editing, warm zones for temporary storage, and cold zones for long-term storage. Our within-display interactions use pointing with a display as a focus+context tool.	active galactic nucleus;categorization;document;e ink;flexible display;haptic technology;interaction;microsoft windows;on the fly;tablet computer	Aneesh P. Tarun;Peng Wang;Paul Strohmeier;Audrey Girouard;Derek F. Reilly;Roel Vertegaal	2013		10.1145/2468356.2479559	simulation;multimedia;flexible display;computer graphics (images)	HCI	-44.129973116680766	-39.88829159218744	142582
5b944eed62a0f0b7d35b0dfa4e22674a6596958f	principled hybrid systems: theory and applications (physta)	emotion analysis symbolic subsymbolic hybrid system signal processing neural networks rule based systems high level knowledge multimedia application human computer interaction recognition of emotion;human computer interaction;multimedia;neural networks;rule based systems;neural nets;application software;user interfaces neural nets knowledge based systems feature extraction;rule based system;hci;rule extraction;subsymbolic;emotion recognition;multimedia application;testing;conceptual framework;feature extraction;signal processing;visual cues;recognition of emotion;signal processing algorithms neural networks robustness knowledge based systems testing application software human computer interaction emotion recognition speech recognition feature extraction;symbolic;hybrid system;high level knowledge;speech recognition;robustness;emotion analysis;signal processing algorithms;user interfaces;knowledge based systems;hybrid systems;neural network	Systematic principles for integrating symbolic and subsymbolic processing will be developed in the project. Key aims are to ensure that the resulting total hybrid system retains desirable properties of both processing levels. On the one side the signal processing abilities, robustness and learning capability of neural networks should be preserved. On the other side advantage should be taken of the ability of rule-based systems to exploit high level knowledge and existing algorithms and to explain (to a user) why conclusions were reached in particular cuses. The methodologies developed in the project are tested in a challenging rrzultimedia application related to human computer interaction, which is recognition of emotion based on both voice and visual cues. Low level features are extracted from signals using neural networks and subsequent formulation of rules provides a conceptual framework, substantial for emotion	algorithm;artificial intelligence;artificial neural network;high-level programming language;human computer;human–computer interaction;hybrid system;rule-based system;signal processing	Stefanos D. Kollias;Frédéric Piat	1999		10.1109/MMCS.1999.778667	natural language processing;computer vision;computer science;artificial intelligence;machine learning;artificial neural network;hybrid system	AI	-35.90886429178615	-45.59030291153267	142744
9b1efdfb048bcb66ffce15de580e5051de76b82c	pursuits: spontaneous interaction with displays based on smooth pursuit eye movement and moving targets	smooth pursuits;eye based interfaces;eye movement;correlation;spontaneous interaction	Although gaze is an attractive modality for pervasive interactions, the real-world implementation of eye-based interfaces poses significant challenges, such as calibration. We present Pursuits, an innovative interaction technique that enables truly spontaneous interaction with eye-based interfaces. A user can simply walk up to the screen and readily interact with moving targets. Instead of being based on gaze location, Pursuits correlates eye pursuit movements with objects dynamically moving on the interface. We evaluate the influence of target speed, number and trajectory and develop guidelines for designing Pursuits-based interfaces. We then describe six realistic usage scenarios and implement three of them to evaluate the method in a usability study and a field study. Our results show that Pursuits is a versatile and robust technique and that users can interact with Pursuits-based interfaces without prior knowledge or preparation phase.	field research;interaction technique;modality (human–computer interaction);pervasive informatics;spontaneous order;usability testing	Mélodie Vidal;Andreas Bulling;Hans-Werner Gellersen	2013		10.1145/2493432.2493477	computer vision;simulation;smooth pursuit;correlation;eye movement	HCI	-46.024432144382764	-43.42509503067071	142840
c151d6264c1ba14e06847ad5b7eefbc1c45440ea	a vibration signaling system for support of the hearing-impaired	touch physiological;vibrations;frequency 125 hz vibration signaling system speech information presentation tactile senses vibration perception threshold measurement fingertip position proximal phalange position wrist position vibration presentation position frequency band vibration stimuli hearing impaired support device;speech;support system vibration perception threshold tactile sence;vibrations speech wrist speech recognition educational institutions frequency measurement position measurement;medical disorders;ear;vibrations biomedical measurement ear hearing medical disorders speech touch physiological;biomedical measurement;hearing	An alternative for speech information is highly desirable for the hearing-impaired. The presentation of speech information using tactile senses is proposed in this study. Initially, vibration perception thresholds were measured to determine the optimum position (fingertips, proximal phalanges, or wrist) and frequency for presenting vibration in the proposed system. It was established that all three places could be candidates for vibration presentation positions. In addition, we showed that the frequency band of 125 Hz was suitable for vibration stimuli.	cell signaling;frequency band;signalling system no. 7	Daisuke Iwase;Shunsuke Ishimitsu	2013	2013 IEEE 2nd Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2013.6664795	speech recognition;acoustics;engineering;audiology	Visualization	-42.97435372531754	-44.89011952339826	142892
89367c06f3c11f54a26c7a37bc018a10bd485b5d	developing a depth-based tracking system for interactive playful environments with animals	tracking system;play;comunicacion en congreso;depth based tracking;interaction design;smart environment;animal computer interaction	Digital games for animals within Animal Computer Interaction are usually single-device oriented, however richer interactions could be delivered by considering multimodal environments and expanding the number of technological elements involved. In these playful ecosystems, animals could be either alone or accompanied by human beings, but in both cases the system should react properly to the interactions of all the players, creating more engaging and natural games. Technologically-mediated playful scenarios for animals will therefore require contextual information about the game participants, such as their location or body posture, in order to suitably adapt the system reactions. This paper presents a depth-based tracking system for cats capable of detecting their location, body posture and field of view. The proposed system could also be extended to locate and detect human gestures and track small robots, becoming a promising component in the creation of intelligent interspecies playful environments.	algorithm;bittorrent tracker;computer vision;ecosystem;experience;humans;kinect;multimodal interaction;point of view (computer hardware company);poor posture;robot;sensor;top-down and bottom-up design;tracking system;unmanned aerial vehicle	Patricia Pons;Javier Jaén Martínez;Alejandro Catalá	2015		10.1145/2832932.2837007	simulation;engineering;multimedia;communication	HCI	-38.078521880068436	-39.16246126735752	142933
edab370639da020ced01326606ca891bb44c2ba0	a data-driven approach to developing iot privacy-setting interfaces		"""User testing is often used to inform the development of user interfaces (UIs). But what if an interface needs to be developed for a system that does not yet exist? In that case, existing datasets can provide valuable input for UI development. We apply a data-driven approach to the development of a privacy-setting interface for Internet-of-Things (IoT) devices. Applying machine learning techniques to an existing dataset of users' sharing preferences in IoT scenarios, we develop a set of """"smart"""" default profiles. Our resulting interface asks users to choose among these profiles, which capture their preferences with an accuracy of 82%---a 14% improvement over a naive default setting and a 12% improvement over a single smart default setting for all users."""	machine learning;user interface;what if	Paritosh Bahirat;Yangyang He;Abhilash Menon;Bart P. Knijnenburg	2018		10.1145/3172944.3172982	human–computer interaction;computer science;internet of things;data-driven;user interface	HCI	-36.476703226705844	-46.9401343803241	142996
691380481fba535fc45de1815962a742c746bbbb	metamaterial textures		"""We present metamaterial textures---3D printed surface geometries that can perform a controlled transition between two or more textures. Metamaterial textures are integrated into 3D printed objects and allow designing how the object interacts with the environment and the user's tactile sense. Inspired by foldable paper sheets (""""origami"""") and surface wrinkling, our 3D printed metamaterial textures consist of a grid of cells that fold when compressed by an external global force. Unlike origami, however, metamaterial textures offer full control over the transformation, such as in between states and sequence of actuation. This allows for integrating multiple textures and makes them useful, e.g., for exploring parameters in the rapid prototyping of textures. Metamaterial textures are also robust enough to allow the resulting objects to be grasped, pushed, or stood on. This allows us to make objects, such as a shoe sole that transforms from flat to treaded, a textured door handle that provides tactile feedback to visually impaired users, and a configurable bicycle grip. We present an editor assists users in creating metamaterial textures interactively by arranging cells, applying forces, and previewing their deformation."""	3d printing;interactivity;rapid prototyping;texture mapping	Alexandra Ion;Robert Kovacs;Oliver S. Schneider;Pedro Lopes;Patrick Baudisch	2018		10.1145/3173574.3173910	human–computer interaction;metamaterial;computer graphics (images);grid;door handle;programmable matter;3d printing;rapid prototyping;computer science;tactile sense	HCI	-43.04961850460654	-39.926705489155516	143120
2b1c550b4f37551c94d559bc0d015defc43a1270	touch interaction with google glass - is it suitable for older adults?		Qualitative and quantitative measurements of feasibility, usability and acceptance were applied.High acceptance of Google Glass but performance difficulties and moderate usability issues.Older adults with cognitive impairment have more difficulties using Google Glasses.Need for a combination of initial training and long-term support.An integration in their own glasses is absolutely essential for long-term usage. Cognitive, perceptual and motor skill changes may prevent older adults from successfully using modern technology. Google Glass possesses only a few buttons and seems to be overall intuitive. The present study aimed to assess the usability and acceptance of Google Glass with 30 older adults aged 65 years and over. The participants were asked to perform several standardized tasks and rate the usability. They experienced severe problems solving the tasks. Although Google Glass received only a marginal usability rating with an SUS score of 64.2 points, 60% of the participants rated the design as attractive, the handling as rather good and the complexity of the system as low. With a short training, even older adults with sensory and cognitive disabilities are able to use Google Glass. Recommendations for developers were derived from the study and are presented at the end of the article. They should be taken into account for the development of mobile applications for older adults using Google Glass.	glass	Marten Haesner;Sebastian Wolf;Anika Steinert;Elisabeth Steinhagen-Thiessen	2018	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2017.09.006	human–computer interaction;motor skill;sensory impairment;perception;usability;cognition;computer science	HCI	-48.0782207422646	-46.11363390535269	143172
b059858feb39dbdf44c2d02150e27453330c88db	moca: a low-power, low-cost motion capture system based on integrated accelerometers	motion capture;low power	Human-computer interaction (HCI) and virtual reality applications pose the challenge of enabling real-time interfaces for natural interaction. Gesture recognition based on body-mounted accelerometers has been proposed as a viable solution to translate patterns of movements that are associated with user commands, thus substituting point-and-click methods or other cumbersome input devices. On the other hand, cost and power constraints make the implementation of a natural and efficient interface suitable for consumer applications a critical task. Even though several gesture recognition solutions exist, their use in HCI context has been poorly characterized. For this reason, in this paper, we consider a low-cost/low-power wearable motion tracking system based on integrated accelerometers called motion capture with accelerometers (MOCA) that we evaluated for navigation in virtual spaces. Recognition is based on a geometric algorithm that enables efficient and robust detection of rotational movements. Our objective is to demonstrate that such a low-cost and a low-power implementation is suitable for HCI applications. To this purpose, we characterized the system from both a quantitative point of view and a qualitative point of view. First, we performed static and dynamic assessment of movement recognition accuracy. Second, we evaluated the effectiveness of user experience using a 3D game application as a test bed.	algorithm;cpu power dissipation;embedded system;foremost;gesture recognition;human–computer interaction;input device;low-power broadcasting;motion capture;point and click;point of view (computer hardware company);real-time locating system;testbed;tracking system;user experience;virtual reality;wearable computer	Elisabetta Farella;Luca Benini;Bruno Riccò;Andrea Acquaviva	2007	Adv. in MM	10.1155/2007/82638	embedded system;computer vision;motion capture;simulation;computer science;operating system	HCI	-43.64147215716917	-43.38263703819234	143287
a70bbb48b8917dcb288e9625655a3f6a1c594f82	wristque: a personal sensor wristband	sensors;wrist;wrist lighting sensors aerospace electronics servers buildings watches;servers;aerospace electronics;watches;lighting;buildings	WristQue combines environmental and inertial sensing with precise indoor localization into a wristband wearable device that serves as the user's personal control interface to networked infrastructure. WristQue enables users to take control of devices around them by pointing to select and gesturing to control. At the same time, it uniquely identifies and locates users to deliver personalized automatic control of the user's environment. In this paper, the hardware and software components of the WristQue system are introduced, and a number of applications for lighting and HVAC control are presented, using pointing and gesturing as a new human interface to these networked systems.	automatic control;component-based software engineering;experiment;personalization;user interface;wearable computer;wearable technology	Brian D. Mayton;Nan Zhao;Matthew Aldrich;Nicholas Edward Gillian;Joseph A. Paradiso	2013	2013 IEEE International Conference on Body Sensor Networks	10.1109/BSN.2013.6575483	embedded system;simulation;computer science;sensor;electrical engineering;operating system;lighting;server	Robotics	-43.40644265811934	-42.20630276229105	143346
553ef461b37dc236b23ce587f49ef84bf727f59a	improvements of the sound perception processing of the anthropomorphic flutist robot (wf-4r) to effectively interact with humans	humanoid robot;human performance;robotics;human robot interaction;indexing terms;anthropomorphism human robot interaction humanoid robots performance analysis robot sensing systems educational robots education mechanical engineering computer science shape;humanoid robots;user interfaces humanoid robots music man machine systems;robotteknik och automation;human robot interaction sound perception processing anthropomorphic flutist robot wf 4r musical scores humanoid robot;effective interaction;user interfaces;music;man machine systems	The development of the anthropomorphic flutist robot, at Waseda University, has demonstrated how the robot can communicate with humans at emotional level by performing musical scores with expressiveness and by transferring basic skills from robot to beginners. However, the interaction among humans is characterized by a highly interactive process of analyzing and responding to incoming stimuli from the partner. Even that flutist robot has successfully imitated the flute playing quite similar to human performance; the way of processing and analyzing the music as human does still requires further improvements. In this paper, we would describe how we implemented a human like sound processing system to enable robot to interact with humans at the same level of perception. An experimental setup was done to verify the validity of the developed system.	audio signal processing;experiment;fast fourier transform;human reliability;humans;human–robot interaction;psychoacoustics;robot;sound quality	Jorge Solis;Keisuke Chida;Kei Suefuji;Atsuo Takanishi	2005	ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.	10.1109/ROMAN.2005.1513820	human–robot interaction;mobile robot;robot learning;computer vision;cog;simulation;computer science;humanoid robot;artificial intelligence;social robot;robot control;robotics;ubiquitous robot;personal robot	Robotics	-37.74922994351807	-42.30133070584444	143358
8c804565bd36c766cfab5b710aedd750acfce8c0	visualizing missing data: graph interpretation user study	interfase usuario;visualizacion;dato que falta;user interface;informacion incompleta;user study;relacion hombre maquina;man machine relation;sistema n niveles;donnee manquante;incomplete information;visualization;visualisation;systeme n niveaux;information incomplete;multilevel system;interface utilisateur;relation homme machine;missing data	Most visualization tools fail to provide support for missing data. In this paper, we identify sources of missing data and describe three levels of impact missing data can have on the visualization: perceivable, invisible or propagating. We then report on a user study with 30 participants that compared three design variants. A between-subject graph interpretation study provides strong evidence for the need of indicating the presence of missing information, and some direction for addressing the problem.	missing data;usability testing	Cyntrica Eaton;Catherine Plaisant;Terence Drizd	2005		10.1007/11555261_68	visualization;computer science;data science;data mining;world wide web;statistics	HCI	-38.72608322763115	-50.782777399743416	143496
0a2e923b324de238d359d7d1019f167342698b54	online real-time presentation of virtual experiences forexternal viewers	health research;uk clinical guidelines;biological patents;viewpoint similarity;europe pubmed central;stabilization;citation search;virtual reality;camera motion;uk phd theses thesis;life sciences;observation;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	Externally observing the experience of a participant in a virtual environment is generally accomplished by viewing an egocentric perspective. Monitoring this view can often be difficult for others to watch due to unwanted camera motions that appear unnatural and unmotivated. We present a novel method for reducing the unnaturalness of these camera motions by minimizing camera movement while maintaining the context of the participant's observations. For each time-step, we compare the parts of the scene viewed by the virtual participant to the parts of the scene viewed by the camera. Based on the similarity of these two viewpoints we next determine how the camera should be adjusted. We present two means of adjustment, one which continuously adjusts the camera and a second which attempts to stop camera movement when possible. Empirical evaluation shows that our method can produce paths that have substantially shorter travel distances, are easier to watch and maintain the original observations of the participant's virtual experience.	distance;experience;image viewer;motion;published comment;real-time transcription;virtual reality	Kevin Ponto;Hyun Joon Shin;Joe Kohlmann;Michael Gleicher	2012	Proceedings of the ACM Symposium on Virtual Reality Software and Technology. ACM Symposium on Virtual Reality Software and Technology	10.1145/2407336.2407346	computer vision;simulation;computer science;artificial intelligence;virtual reality;multimedia;observation	Visualization	-42.593478401523	-50.70315650118288	143847
1b5a79453ed604ea2ed1e02eaaf53f0de588c71d	glance awareness and gaze interaction in smartwatches	smartwatch;gaze input;haptic feedback;wearable computing	"""Smartwatches are widely available and increasingly adopted by consumers. The most common way of interacting with smartwatches is either touching a screen or pressing buttons on the sides. However, such techniques require using both hands. We propose glance awareness and active gaze interaction as alternative techniques to interact with smartwatches. We will describe an experiment conducted to understand the user preferences for visual and haptic feedback on a """"glance"""" at the wristwatch. Following the glance, the users interacted with the watch using gaze gestures. Our results showed that user preferences differed depending on the complexity of the interaction. No clear preference emerged for complex interaction. For simple interaction, haptics was the preferred glance feedback modality."""	haptic technology;interaction;modality (human–computer interaction);smartwatch;user (computing);watch	Deepak Akkil;Jari Kangas;Jussi Rantala;Poika Isokoski;Oleg Spakov;Roope Raisamo	2015		10.1145/2702613.2732816	computer vision;wearable computer;computer science;smartwatch;multimedia;haptic technology	HCI	-46.85872771541808	-44.05209507910885	144001
c5c7e05af3bb42c7ac505fd39568080a2a58a79a	etao keyboard: text input technique on smartwatches		Abstract   In the present day context of wearable computing, smartwatches augment our mobile experience even further by providing in- formation at our wrists. What they fail to do is to provide a comprehensive text entry solution for interacting with the various app notifications they display. In this paper we present ETAO keyboard, a full-fledged keyboard for smartwatches, where a user can input the most frequent English alphabets with a single tap. Other keys which include numbers and symbols are entered by a double tap. We conducted a user study that involved sitting and walking scenarios for our experiments and after a very short training session, we achieved an average words per minute (WPM) of 12.46 and 9.36 respectively. We expect that our proposed keyboard will be a viable option for text entry on smartwatches.	smartwatch	Rajkumar Darbar;Punyashlok Dash;Debasis Samanta	2015		10.1016/j.procs.2016.04.078	simulation;operating system;multimedia;world wide web	HCI	-47.152317466955374	-45.15307274482201	144074
775f765a61a1b2288479645494bfc1e8aa957dc2	squeezy: extending a multi-touch screen with force sensing objects for controlling articulatory synthesis	touch screen	This paper describes Squeezy: a low-cost, tangible input device that adds multi-dimensional input to capacitive multi-touch tablet devices. Force input is implemented through force sensing resistors mounted on a rubber ball, which also provides passive haptic feedback. A microcontroller samples and transmits the measured pressure information. Conductive fabric attached to the finger contact area translates the touch to the bottom of the ball which allows the touchscreen to detect the position and orientation. The addition of a tangible, pressuresensitive input to a portable multimedia device opens up new possibilities for expressive musical interfaces and Squeezy is used as a controller for real-time gesture controlled voice synthesis research.	articulatory synthesis;haptic technology;input device;microcontroller;multi-touch;real-time transcription;speech synthesis;tablet computer;touchscreen	Johnty Wang;Nicolas D'Alessandro;Sidney S. Fels;Bob Pritchard	2011			computer vision;acoustics;computer science	HCI	-42.62952623488719	-41.80410891409923	144129
f736871948a38423b8b1f8fd227d57d30ee94d2c	a cross-platform, remotely-controlled mobile avatar simulation framework for ami environments	ambient intelligence;performance;biped skeletal animation;text to speech;design;lip synchronization;presence;human factors virtual assistant;experimentation;virtual character simulation	Nowadays, users are able to interact with digital content using their mobile devices almost everywhere and anytime due to the increased power, portability, and ubiquitous connectivity of mobile devices. This paper presents the design and implementation of a novel, remotely controlled for the purposes of edutainment and instructor-student interaction, three-dimensional full body avatar gamification framework. The main innovation introduced focuses on multi-presence gamified educational scenarios in multiple desktop computers and mobile devices. Thus the remotely-controlled avatar can act as a guide, assistant or information presenter for novel, cross-platform Ambient Intelligence (AmI) edutainment scenarios. In detail, the avatar's role depends on the requirements of the AmI client applications as these are propagated remotely (using remote procedure calls). Examples of remote function invocations include real-time 3D biped skinned animations, text-to-speech, producing facial expressions and presenting multimedia content.	ambient intelligence;anytime algorithm;desktop computer;digital recording;educational entertainment;gamification;mobile device;real-time transcription;remote control;remote procedure call;requirement;simulation;speech synthesis;virtual desktop	Emmanouil Zidianakis;George Papagiannakis;Constantine Stephanidis	2014		10.1145/2669062.2669083	computer vision;design;real-time computing;simulation;ambient intelligence;performance;computer science;artificial intelligence;operating system;multimedia;speech synthesis;computer graphics (images)	HCI	-47.52159429395658	-38.27179882741855	144194
6704a5e453eed3416f64052c719b688ae6aa1192	color sensing and illumination with led lamps	perceivable flicker color sensing light emitting diode object illumination wavelength sensitive light sensor rgb led lamp red green blue led lamp associated driver protocol;sensors;color;sensors led lamps color regulators image color analysis;image color analysis;led lamps;protocols led lamps lighting optical sensors;illumination and color sensing driver protocol led sensing color picking;regulators	With the advent of light emitting diodes (LEDs) in lamps, new ways of user interaction with LED lamps are becoming possible. We present an LED lamp that can sense and pick up the color from an illuminating object. The principle of employing an LED as a wavelength-sensitive light sensor is employed in designing an RGB (red green blue) LED lamp. A design prototype of such an LED lamp is presented, along with an associated driver protocol, such that it can sense color and illuminate at the picked up color without perceivable flicker. Experimental results are shown to demonstrate the dual function of the RGB lamp in color sensing and illumination at primary RGB colors.	color;diode;duality (optimization);flicker (screen);illumination (image);led lamp;oled;prototype	Shuai Li;Ashish Pandharipande	2014	2014 IEEE Fourth International Conference on Consumer Electronics Berlin (ICCE-Berlin)	10.1109/ICCE-Berlin.2014.7034294	color temperature;optoelectronics;optics;seachanger color engine	Robotics	-40.709372224233704	-41.55273753613897	144283
e74aae5fb73f6ed039fc45a669a8319abd8a1ed5	towards an objective comparison of scanning-based interaction techniques	mobile navigation;field study;scanning-based interaction technique;different scanning-based interaction concept;scan-based interaction technique;objective comparison;mobile phone;interaction technique;varying direction;standard set;proper scan-based interaction technique;audio feedback	The direction where a user points a mobile phone to can be measured with the phone's integrated compass. Pointing over time and with varying direction is often referred to as scanning , which is an emerging interaction technique and increasingly applied in the eld of mobile navigation and orientation. Because there is no need to look at the screen while scanning, often haptic or audio feedback is used. In fact there exist several di erent scanning-based interaction concepts. However, until now it is impossible to analyse and compare these techniques systematically to identify the best concept for a certain scenario. In this paper we investigated how our own Tactile Compass scanning technique has been used in a eld study. Based on our observations we identi ed a set of measures, which we propose to become a standard set for the analysis and comparison of scan-based interaction techniques. We further argue that our contribution may be bene cial for the creation of guidelines and support designers in selecting a proper scan-based interaction technique.	audio feedback;existential quantification;haptic technology;interaction technique;mobile phone	Benjamin Poppinga;Martin Pielot;Wilko Heuten;Susanne Boll	2012		10.1007/978-3-642-32796-4_12	computer vision;simulation;engineering;multimedia	HCI	-46.775529202769164	-43.48732960914588	144493
5bf165e450ad82b65376a496aa055877b91ab19c	asymmetry in deviation of rapid-targeting eye movement under the sensation of visually induced illusory self-motion	eye movement			Masahiko Fujita;Kikumi Hoshi;Tomoari Seita	1998			pattern recognition;artificial intelligence;computer vision;asymmetry;sensation;artificial neural network;computer science;eye movement	Arch	-44.48792725207611	-50.8857484979447	144560
593aa75f4434ca9800f12d6e4e58f57beaff36e0	dynamic context switching for gaze based interaction	gaze based interaction;selection by gaze;user experience;dynamic context switching	This paper introduces Dynamic Context Switching (DCS) as an extension of the Context Switching (CS) paradigm for gaze-based interaction. CS replicates information in each context. The user can freely explore one context without worrying about the Midas touch problem, and a saccade to the other context triggers the selection of the item under focus. Because CS has to display two contexts simultaneously, the amount of useful screen space is limited. DCS dynamically adjusts the context sizes, where the context that has the focus is displayed in full size, while the other is minimized, thus improving useful screen space. A saccade to the minimized context triggers selection, and properly readjusts the sizes of the contexts. Results from a pilot user experiment show that DCS improves user performance and do not cause disorientation due to the dynamic context resizing.	context switch;glossary of computer graphics;programming paradigm	Antonio Diaz Tula;Filipe Morgado Simoes de Campos;Carlos Hitoshi Morimoto	2012		10.1145/2168556.2168635	simulation;computer science;multimedia;communication	HCI	-45.6481718644633	-46.22270958975786	144603
96e527d3353c46d6cda573d8b7ed054e9fbbe8fc	design of an interface on pda for korean	user interface;character recognition hidden markov models personal digital assistants speech recognition shape natural languages user interfaces mobile computing databases pattern recognition;hidden markov models;storage capacity;notebook computers;mobile computing on line recognition system cursive korean character interface design embedded user interface primitive strokes connection path discriminant hmm pdhmm hand printed characters recognition rate;user interfaces notebook computers handwritten character recognition hidden markov models;user interfaces;character recognition;handwritten character recognition	We present an on-line recognition system of cursive Korean character on a PDA. Because of the limited storage capacity and small size of the PDA, a more compact and more convenient design is needed in an embedded user interface, like the on-line character recognition system. To make a convenient interface, the character recognition system has to recognize the cursive type of Korean characters. We define a cursive type of Korean characters as the connection of primitive strokes. We use the path discriminant HMM (PDHMM). The initial structure of the PDHMM is designed from the hand-printed characters and then it is expanded to the cursive case. When new strokes for the cursive type are defined, we can easily extend the PDHMM by inserting proper edges. To save the storage space of the PDA, character recognition is done simply with traversing a single PDHMM. We make smaller states for the PDHMM than the ones for other HMMs. We implemented it on a handheld PC (HPC) and got a good recognition rate.	personal digital assistant	Hyun Uk Kang;Hang Joon Kim	2000	IEEE Trans. Consumer Electronics	10.1109/30.883457	speech recognition;document processing;intelligent character recognition;computer science;intelligent word recognition;operating system;user interface;hidden markov model;computer graphics (images)	Visualization	-44.08776221029074	-43.62289929195691	144645
07c763cd86b59b82761029158b3ef7e3dd8b1287	this study of hand anthropometry and touchscreen size of smartphones		With the development of communication technology, smartphone becomes an important personnel device that everyone must have. As the introduction of 3 rd Generation mobile telecommunication, the technology of touch screen was started to be applied on the mobile phones and an indis- pensable component on a 3G smartphone that is built in advanced computing capability like digital cameras, GPS and web-browser. Most of modern smart- phones include high-resolution touchscreens for display and control and the size of touchscreen becomes larger and larger. However, the question is that a larger screen is convenient to every user? The aim of this study is to realize the relationship between touchscreen sizes of smartphones and user's relative hand dimensions based on the operation time, operation error rate and subjective thumb fatigue. One hundred subjects, including 50 males and 50 females were invited to attend the experiment. The experiment design includes 4 touchscreen sizes. The hand length and hand width were both divided into 4 categories respectively. The results indicated there is a rapid increasing trend from 3.5 in. screen to 5.7 in. screen for smaller hand length/width and the trend decreases firstly slight and then increases for large hand length/width. Touchscreen size from 4.6 in. to 5.0 in. is suggested for most people because this interval of screen size is relatively suitable based on the analysis of hand dimensions and 3 performance evaluation indices. Too large touchscreen (5.7 in.) is hard to use for any hand length or width.	anthropometry;smartphone;touchscreen	Yu-Cheng Lin;Ming-Hung Lin	2015		10.1007/978-3-319-20612-7_58	embedded system;simulation;computer hardware;engineering	HCI	-47.58687960505555	-45.59170121054287	144681
13be2bdff33469f510d0bc4ef1127f614339800d	coming to grips with the objects we grasp: detecting interactions with efficient wrist-worn sensors	wrist worn rfid;wearable interaction;rfid tag;gesture detection	The use of a wrist-worn sensor that is able to read nearby RFID tags and the wearer's gestures has been suggested frequently as a way to both detect the objects we interact with and to identify the interaction. Making such a prototype feasible for longer-term deployments is far from solved however, as plenty of challenges remain in the hardware, embedded algorithms, and the overall design of such a bracelet-like device. This paper presents several of the challenges that emerged during the development of a functioning prototype that is able to sense interaction data for several days. We focus in particular on RFID tag reading range optimization, efficient data logging methods, meaningful evaluation techniques, and long-term deployments.	algorithm;data logger;embedded system;interaction;mathematical optimization;prototype;radio-frequency identification;sensor	Eugen Berlin;Jun Liu;Kristof Van Laerhoven;Bernt Schiele	2010		10.1145/1709886.1709898	embedded system;real-time computing;engineering;communication	Mobile	-43.42604324370315	-43.01617747949406	144752
85c72249963b8d8f89a4c90b6a811c0793f8c3c6	whadget: interactive animation using personification gesture expression of hand	personified hand movement;hand gesture;interactive animation;interaction technique	By pointing your index and middle fingers downward and make strides with them, you have what looks like a walking hand. Whadget is an interactive animation which uses a system that distinguishes various personified gesture motions of a hand such as walking on fingertips. By assigning a bare hand as an independent character by itself, users are able to interact with a digital creature which reacts to the users' personified hand movements.		Yu Nagao;Haruka Yamaguchi;Kazuhiro Harada;Kaori Omura;Masa Inakage	2008		10.1145/1400885.1400993	computer vision;computer science;interaction technique;computer graphics (images)	HCI	-44.438376718701825	-44.57498443623995	145026
2d7a00a9df9085ab220ea0e46cadfcc05e4e466d	gazetry: swipe text typing using gaze	assistive technologies;dwell free;eye typing	Over the last decades, eye gaze has become an alternative form of text entry by some physically challenged people. Recently a dwell-free system has been proposed, which has been proven to be much faster compared to other existing dwell-free systems. However, it is vulnerable to some common text entry problems. In the paper, we propose GazeTry, a dwell-free gaze-based text entry system which allows people to type a word by gazing sequentially at the letters of the word. Simulation and experiments results show that our proposed new dwell-free system, GazeTry with the Moving Window String Matching (MoWing) algorithm has better accuracy and more resilience to text entry errors.	experiment;simulation;string searching algorithm;typing	Yi Liu;Chi Zhang;Chonho Lee;Bu-Sung Lee;Alex Qiang Chen	2015		10.1145/2838739.2838804	simulation;speech recognition;computer science	HCI	-47.13414800897234	-45.188893077527595	145053
34d0e9ee01e0dd898d72ed7d095148bfde30f31d	high precision touchscreens: design strategies and comparisons with a mouse	interfase usuario;user interface;taux erreur;reaccion utilizador;estudio comparativo;labour productivity;performance;hombre;mouse computer;user preferences;user reaction;man machine system;productivite travail;etude comparative;touchscreen;reaction utilisateur;souris informatique;ratita;ecran tactile;human;comparative study;productividad trabajo;preferencia;error rate;sistema hombre maquina;interface utilisateur;preference;actitud;technical report;pantalla tactil;rendimiento;indice error;materiel informatique;attitude;material informatica;homme;hardware;systeme homme machine	Three studies were conducted comparing speed of performance, error rates, and user preference ratings for three selection devices. The devices tested were a touchscreen, a touchscreen with stabilization (stabilization software filters and smooths raw data from hardware), and a mouse. The task was the selection of rectangular targets 1, 4, 16, and 32 pixels per side (0.4x0.6, 1.7x2.2, 6.9x9.0, 13.8x17.9 mm respectively). T ouchscreen users were able to point at single pixel targets, thereby countering widespread expectations of poor touchscreen resolution. The results show no difference in performance between the mouse and touchscreen for targets ranging from 32 to 4 pixels per side. In addition, stabilization significantly reduced the error rates for the touchscreen when selecting small targets. These results imply that touchscreens, when properly used, have attractive advantages in selecting targets as small as 4 pixels per size (approximately onequarter of the size of a single character). A variant of Fitts' Law is proposed to predict touchscreen pointing times. Ideas for future research are also presented.	fitts's law;pixel;touchscreen	Andrew Sears;Ben Shneiderman	1991	International Journal of Man-Machine Studies	10.1016/0020-7373(91)90037-8	attitude;simulation;performance;computer science;artificial intelligence;technical report;user interface	HCI	-46.96895813895514	-46.24639301590346	145299
48f1f67e8051080404143dddf3bbed212893f5e3	vibration enhances geometry perception with tactile shape displays	geometry shape displays frequency fingers skin actuators virtual environment vibration measurement bars;fishbone tactile illusion;touch sensitive screens;vibrations;vibrations computer displays geometry haptic interfaces psychology touch sensitive screens;skin;geometry;vibrational stimulus;actuators;psychology;tactile shape displays;shape;5 hz geometry perception tactile shape displays passive touch perception fishbone tactile illusion vibrational stimulus;5 hz;tactile display;displays;computer displays;fingers;vibration measurement;passive touch perception;virtual environment;haptic interfaces;geometry perception;frequency;bars;spatial information	"""Tactile displays can provide detailed spatial information to the skin, but little is known about the effects of vibrating displayed shapes. This study examines passive touch perception of flat and indented surfaces displayed on a 36 pin tactile display with 2 mm pin pitch. Subjects could not perceive a 0.1 mm deep central indentation when it was presented statically, but it was readily detected when the pattern was vibrated at 5 Hz. A central raised bar was incorrectly perceived as indented when the adjacent pins were vibrated, which is consistent with the """"fishbone tactile illusion"""" (Nakatani et al., Proc. EuroHaptics 2006). These results suggest that tactile display devices can use vibrational stimulus to enhance perception of small differences in height"""		María Oyarzábal;Masashi Nakatani;Robert D. Howe	2007	Second Joint EuroHaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems (WHC'07)	10.1109/WHC.2007.124	computer vision;shape;computer science;engineering;virtual machine;vibration;frequency;spatial analysis;skin;engineering drawing;actuator	HCI	-43.6173810562665	-49.18502758955689	145493
43ef6bfb5bbffdf0c2a9141f0cff132c39656b0c	wearable rgbd indoor navigation system for the blind		In this paper, we present a novel wearable RGBD camera based navigation system for the visually impaired. The system is composed of a smartphone user interface, a glass-mounted RGBD camera device, a real-time navigation algorithm, and haptic feedback system. A smartphone interface provides an effective way to communicate to the system using audio and haptic feedback. In order to extract orientational information of the blind users, the navigation algorithm performs real-time 6-DOF feature based visual odometry using a glass-mounted RGBD camera as an input device. The navigation algorithm also builds a 3D voxel map of the environment and analyzes 3D traversability. A path planner of the navigation algorithm integrates information from the egomotion estimation and mapping and generates a safe and an efficient path to a waypoint delivered to the haptic feedback system. The haptic feedback system consisting of four micro-vibration motors is designed to guide the visually impaired user along the computed path and to minimize cognitive loads. The proposed system achieves real-time performance faster than 30Hz in average on a laptop, and helps the visually impaired extends the range of their activities and improve the mobility performance in a cluttered environment. The experiment results show that navigation in indoor environments with the proposed system avoids collisions successfully and improves mobility performance of the user compared to conventional and state-of-the-art mobility aid devices.	algorithm;cinii;computer performance;experiment;haptic technology;in the beginning... was the command line;input device;laptop;mobile app;real-time clock;real-time locating system;ric weiland;sensor web;smartphone;tree accumulation;user interface;visual odometry;voxel;waypoint	Young Hoon Lee;Gérard G. Medioni	2014		10.1007/978-3-319-16199-0_35	artificial intelligence;input device;computer science;computer vision;wearable computer;navigation system;haptic technology;visual odometry;mobility aid;user interface;waypoint	HCI	-39.0246228065946	-44.51876874448038	145604
9c0a22cefb9125c5b1c29be2682d71d342061565	touchable wall: easy-to-install touch-operated large-screen projection system		"""Recently, small and inexpensive portable projectors are commonly being used for presentations. For convenience, it is desired to perform a direct pointing operation on the screen without requiring to grip or mount any device and control the pointer intuitively. Therefore, this study proposes a new touch-operated large-screen projection system using acoustic vibration sensing and a projector-camera system. We apply a continuous signal in the inaudible range of an actuator to a surface and acquire its elastic compliance change as a resonance of sounds. We perform touch detection, position estimation, and pressure estimation by using machine learning techniques. Our actuator and sensor units are small and easy to install on a surface. Furthermore, our system can detect """"true"""" touch without requiring any devices such as pens or pointers. We demonstrate a series of novel interactions for a large-screen projection system with our proposed technology through three applications."""	acoustic cryptanalysis;continuous signal;interaction;machine learning;movie projector;pointer (computer programming);resonance;video projector	Takefumi Hiraki;Masaaki Fukumoto;Yoshihiro Kawahara	2018		10.1145/3279778.3279916	pointer (computer programming);computer hardware;continuous signal;mount;actuator;user interface;computer science;vibration	HCI	-42.90102285174249	-42.74209108995604	145734
aa56667ce8df64c096dfe3b69f879bce27f6ecdc	anticipatory vibrotactile cueing facilitates grip force adjustment during perturbative loading	grip force adjustment;force vibrations loading indexes instruments dc motors thumb;instruments;vibrations;vibrotactile stimulus;loading;force;thumb;vibrations force feedback haptic interfaces human computer interaction;indexes;human behavior anticipatory vibrotactile cueing grip force adjustment perturbative loading nervous system computer control gripping finger vibration vibrotactile stimulation human machine interface;dc motors;asynchronous cues grip force adjustment vibrotactile stimulus;asynchronous cues	Grip force applied to an object held between the thumb and index finger is automatically and unconsciously adjusted upon perception of an external disturbance to the object. Typically, this adjustment occurs within approximately 100 ms. Here, we investigated the effect of anticipatory vibrotactile cues prior to a perturbative force, which the central nervous system may use for rapid grip re-stabilization. We asked participants to grip and hold an instrumented, actuated handle between the thumb and index finger. Under computer control, the handle could suddenly be pulled away from a static grip and could independently provide vibration to the gripping fingers. The mean latency of corrective motor action was 139 ms. When vibrotactile stimulation was applied 50 ms before application of tractive force, the latency was reduced to 117 ms, whereas the mean latency of the conscious response to vibrotactile stimuli alone was 229 ms. This suggests that vibrotactile stimulation can influence reflex-like actions. We also examined the effects of anticipatory cues using a set of perturbative loads with different rising rates. As expected, facilitation of grip force adjustment was observed for moderate loads. In contrast, anticipatory cues had an insignificant effect on rapid loads that evoked an adjustment within 60-80 ms, which approaches the minimum latency of human grip adjustment. Understanding the facilitative effects of anticipatory cues on human reactive grip can aid the development of human-machine interfaces to enhance human behavior.	computer control company;eighty;emdebian grip;fingers, unit of measurement;index finger;influenza;nervous system structure;perturbation theory (quantum mechanics);reflex action;sacral nerve stimulation;user interface;facilitation	Shogo Okamoto;Michael Wiertlewski;Vincent Hayward	2016	IEEE Transactions on Haptics	10.1109/TOH.2016.2526613	control engineering;database index;simulation;engineering;vibration;dc motor;force;physics;quantum mechanics	HCI	-45.02933859240059	-50.78044374434245	145801
438dde4d06c6a5aff9d4a43145240510ec04b9d2	tele-robots with shared autonomy: tele-presence for high level operability		The aim is to improve the operability of an advanced demonstration platform incorporating reflexive teleoperated control concepts developed on a mobile robot system. The robot is capable of autonomously navigating in semi-structured environments. Reflexive tele-operation mode employs the robot extensive onboard sensor suite to prevent collisions with obstacles when the human operator assumes control and remotely drives the robot to investigate a situation of interest. For the shared autonomy aspect, four levels of autonomy have been implemented: tele-operation, safe mode, shared autonomy and autonomous mode. The operability level is enhanced by improving significantly the situational awareness of the operator by using an inertial tracker in combination with a head mounted display creating a certain feeling of presence. As such, the system permits precision observation and pinpoint data collection without subjecting the user to a possibly hazardous remote environment.	autonomous robot;autonomy;head-mounted display;high-level programming language;mobile robot;operability;semiconductor industry;television	Thomas Geerinck;Valentin Enescu;Ioan Alexandru Salomie;Sid Ahmed Berrabah;Kenny Cauwerts;Hichem Sahli	2005			operability;systems engineering;control engineering;robot;engineering;autonomy	Robotics	-36.9306622768085	-41.57991830740173	145909
65555a852eff770439b0ecec7b265d137c8afc2a	page detection using embedded tags	embedded tags;human interaction;page detection;smart documents;electronic book;rfid;simultaneous id;electronic books;page id	We describe a robust working prototype of a system for accurate page-ID detection from bound paper books. Our method uses a new RFID technology to recognize book page location. A thin flexible transponder tag with a unique ID is embedded in the paper of each page, and a tag reader is affixed to the binding of the back of the book. As the pages turn, the tag reader notices which tags are within its read range and which have moved out of its range (which is about four inches). The human interacts with the book naturally, and is not required to perform any actions for page detection that are not usual in book interaction. The page-detection data can be used to enhance the experience of the book, or to enable the book as a controller for another system. One such system, an interactive museum exhibit, is briefly described.	book;embedded system;html element;prototype;tag (metadata);transponder	Maribeth Back;Jonathan Cohen	2000		10.1145/354401.354441	radio-frequency identification;interpersonal relationship;operating system;internet privacy;world wide web	OS	-44.410122069824624	-41.29553674520881	145937
1b6133b2036c57f4c6388cc3e684fe0d5763297e	blind path obstacle detector using smartphone camera and line laser emitter		Visually impaired people find navigating within unfamiliar environments challenging. Many smart systems have been proposed to help blind people in these difficult, often dangerous, situations. However, some of them are uncomfortable, difficult to obtain or simply too expensive. In this paper, a low-cost wearable system for visually impaired people was implemented which allows them to detect and locate obstacles in their locality. The proposed system consists of two main hardware components, a laser pointer ($12) and an android smart phone, making our system relatively cheap and accessible. The collision avoidance algorithm uses image processing to measure distances to objects in the environment. This is based on laser light triangulation. This obstacle detection is enhanced by edge detection within the captured image. An additional feature of the system is to recognize and warn the user when stairs are present in the camera's field of view. Obstacles are brought to the user's attention using an acoustic signal. Our system was shown to be robust, with only 5 % false alarm rate and a sensitivity of 90 % for 1 cm wide obstacles.	acoustic cryptanalysis;algorithm;android;ccir system a;depth map;edge detection;global positioning system;image processing;lambert's cosine law;locality of reference;microsoft outlook for mac;pointer (computer programming);robustness (computer science);smart system;smartphone;template matching;wearable computer	Rimon Saffoury;Peter Blank;Julian Sessner;Benjamin H. Groh;Christine F. Martindale;Eva Dorschky;Jörg Franke;Bjoern M. Eskofier	2016	2016 1st International Conference on Technology and Innovation in Sports, Health and Wellbeing (TISHW)	10.1109/TISHW.2016.7847770	computer vision;simulation;engineering;communication	Robotics	-39.87507573454864	-43.617769138321094	146001
4eba292f1a71294571f3881c62e5d05be020f44d	human factors of stereoscopic 3d displays		Human Factors of Stereoscopic Displays provides an overview of all vision-relevant topics and issues that inform stereo display design from a user-centric or human factor, perspective. Although both the basic vision science literature and the applied literature will be reviewed, the strength and originality of this book comes from the emphasis on the basic science literature on human stereo vision and its implications for stereo display design. The reader will learn how to design stereo displays from a human vision/human factors perspective. Over the past several years, there has been a growing interest in the development of high-quality displays that present binocular parallax information to the human visual system for inducing the perception of three-dimensional depth. The methods for presenting binocular parallax to an observer vary widely and include three broad categories of display: stereoscopic, holographic and volumetric displays. Because the technology for stereoscopic displays is more developed and more widely used, than those based on holography or volumetric methods, the proposed book addresses those human factors issues involved in the viewing of stereoscopic displays. Despite the diverse methods for creating stereoscopic displays, which includes stereo spatial multiplexing as well as temporal multiplexing (i.e., field sequential) techniques, there remain common human factor issues that arise when viewing such displays. Human Factors of Stereoscopic Displays will provide a detailed review of these important issues so that they can be considered when designing and using 3D displays. In doing so, the following topics will be covered: interocular cross talk; interocular differences in luminance and contrast; accommodation-vergence mismatch; stereoanomaly; spatio-temporal frequency effects; distance scaling of disparity and high-level cue conflict.body	human factors and ergonomics;stereo display;stereoscopic video game;stereoscopy	Robert Earl Patterson	2015		10.1007/978-1-4471-6651-1	stereoscopy;holography;computer vision;multimedia;stereo display;vision science;human visual system model;parallax;active shutter 3d system;artificial intelligence;geography;stereopsis	HCI	-40.15369589341986	-40.73300217260641	146150
7eaa2e29193f9265f0aecc3e2bc029090d4f9e21	open device control (opendc): human interface device framework for interactive applications including educational contents in ubiquitous environments	development framework;ubiquitous environment development framework toolkit systems human interface device educational contents web service digital broadcast service;3d educational content open device control human interface device framework interactive application ubiquitous environment mobile device portable device ipad iphone android tablet pc intuitive interface webib software development system web 3d content attractive educational content 3d graphics world wide web digital tv;human interface device;digital tv;web service;three dimensional;artificial intelligent;three dimensional displays digital tv graphics artificial intelligence humans markup languages educational institutions;tablet pc;human interface;computer networks and communications;interactive application;educational aids;software development;web services;ubiquitous computing;toolkit systems;educational computing;web services educational aids educational computing interactive systems ubiquitous computing user interfaces;interactive systems;markup language;user interfaces;3d graphics;educational contents;digital broadcast service;ubiquitous environment	This paper proposes a human interface device framework for interactive applications including educational contents in ubiquitous environments. Recently, mobile and portable devices such as iPad, iPhone and Android tablet PC have become common and popular. We should use these devices for providing learners with more attractive educational contents supporting various intuitive interfaces. Using the proposed framework, it becomes possible to easily develop such educational contents. Furthermore, the combined use of the framework with WebIB, which is a software development system for Web 3D contents, it also becomes possible to develop more attractive educational contents using 3D graphics that run on Web and Digital TV. To clarify the usefulness of the framework, this paper introduces interactive application examples including 3D educational contents available on Web and Digital TV actually developed using the proposed framework and WebIB.	3d computer graphics;android;human interface device;interactivity;personal digital assistant;software development;tablet computer;user interface;ipad	Kosuke Kaneko;Tomoyuki Nakamura;Yoshihiro Okada;Hiroyuki Matsuguma	2012	2012 IEEE Seventh International Conference on Wireless, Mobile and Ubiquitous Technology in Education	10.1109/WMUTE.2012.29	web service;human–computer interaction;computer science;operating system;multimedia;world wide web;ubiquitous computing;human interface device	HCI	-47.986744009992435	-38.59688534043559	146229
623a78e444e17d6e7dca7ee5db996774886b1bcd	can user-paced, menu-free spoken language interfaces improve dual task handling while driving?	tl motor vehicles aeronautics astronautics;human computer interaction;manniska datorinteraktion interaktionsdesign;computer and information science;qa75 electronic computers computer science;data och informationsvetenskap	The use of speech-based interaction over traditional means of interaction in secondary tasks may increase safety in demanding environments with high requirements on operator attention. Speech interfaces have suffered from issues similar to those of visual displays, as they often rely on a complex menu structure that corresponds to that of visual systems. Recent advances in speech technology allow the use of natural language, eliminating the need for menu structures and offering a tighter coupling between the intention to act and the completion of the action. Modern speech technology may not only make already existing types of interaction safer, but also opens up for new applications, which may enhance safety. One such application is a speech-based hazard reporting system. A small fixed-base simulator study showed that drivers adapt the timing of the hazard reports to the situation at hand, such that an increase in reported workload was avoided.	natural language;requirement;simulation;speech technology	Alexander Eriksson;Anders Lindström;Albert Seward;Alexander Seward;Katja Kircher	2014		10.1007/978-3-319-07230-2_38	simulation;speech recognition;human–computer interaction;computer science;artificial intelligence;operating system;information and computer science	HCI	-48.093497129600095	-46.73655689850088	146423
708c72f649833875b339a525ec722a34078d5b28	evaluation of pointing navigation interface for mobile robot with spherical vision system	vision system;mobile robot;spherical vision system;path planning;omni directional mobile robot;mobile robots;human robot interaction;navigation;pointing navigation interface;robot vision;humans;robot vision cameras gesture recognition human robot interaction mobile robots path planning;intuitive interface pointing navigation interface mobile robot spherical vision system human robot interaction joystick teaching pendant gesture interaction human pointing recognition system directed camera visual feedback control user interface;omni directional mobile robot pointing navigation interface spherical vision system;robot vision systems;gesture recognition;cameras;humans navigation cameras robot vision systems robot kinematics mobile robots;robot kinematics	In human robot interaction, intuitive interface is necessary. A specific interaction device, for instance, a joystick or a teaching pendant, is not usually intuitive and needs trainings for a general user. Instruction by gesture is one of the intuitive interfaces and a potential user does not need any training for showing a gesture. Pointing is one of the simplest gestures. Hibino et. al.[1] proposed a simple human pointing recognition system for a mobile robot that has an upward directed camera and recognizes human pointing and navigate itself to the place a user is pointing by simple visual feedback control. This paper shows improvement of the method and investigates the validity and usefulness of the proposed method with questionnaire investigations with the proposed and conventional user interfaces.	feedback;human–robot interaction;joystick;mobile robot;user interface	Kyohei Yoshida;Fuminori Hibino;Yasutake Takahashi;Yoichiro Maeda	2011	2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)	10.1109/FUZZY.2011.6007673	mobile robot;computer vision;simulation;computer science;artificial intelligence;gesture recognition;pointing device	Robotics	-42.908358358359514	-44.902424605179824	146667
0886617507eb2133c27fc1351cbcb75aff4b34e8	game development framework based upon sensors and actuators	wireless sensor;human interaction;game development;pervasive game;sensors and actuators;game playing	Urge for comfort and excitement have made gadgets indispensable part of our life. The technology-enabled gadgets not only facilitate and enrich our daily lives but also are interesting tools to challenge human imagination to design and implement new ubiquitous applications. Pervasive gaming, in which human interaction and game/scenario-dependent designs are often common practices, has proved to be one of the areas to successfully combine technology and the human fantasy. By moving away from games being played by humans and by focusing instead on games played by robots and giving humans the leading role of defining game strategies and players’ roles, this paper aims at bridging the two fields of robotics and wireless sensor/actuator networks and exploring their potentials in the field of pervasive gaming. A generic game development framework is introduced that accommodates different types of robots and various kinds of sensors and actuators. Being extensible and modular, the proposed framework can be used for a wide range of pervasive applications built upon sensors and actuators. To enable game development, a Wiimotebased robot identification and localization technique is presented. The proposed framework and robot identification, localization, control and communication mechanisms are evaluated by implementing a game example.	as-interface;bridging (networking);control theory;extensibility;humans;internationalization and localization;modular programming;modularity (networks);pervasive informatics;robot;robotics;sensor;spline (mathematics);video game development;wii	Ray van Brandenburg;Arie Horst;Bas Burgers;Nirvana Meratnia	2008		10.1007/978-3-540-88875-8_111	simulation;engineering;multimedia;communication	Robotics	-47.16053747751691	-38.69183850675972	146739
35bca39cce4ec433ca41337e2c3bf84a66e5cfe7	precise pointing techniques for handheld augmented reality		We propose two techniques that improve accuracy of pointing at physical objects for handheld Augmented Reality (AR). In handheld AR, pointing accuracy is limited by both touch input and camera viewpoint instability due to hand jitter. The design of our techniques is based on the relationship between the touch input space and two visual reference frames for on-screen content, namely the screen and the physical object that one is pointing at. The first technique is based on Shift, a touch-based pointing technique, and video freeze, in order to combine the two reference frames for precise pointing. Contrastingly -without freezing the video-, the second technique offers a precise mode with a cursor that is stabilized on the physical object and controlled with relative touch inputs on the screen. Our experimental results show that our techniques are more accurate than the baseline techniques, namely direct touch on the video and screen-centered crosshair pointing.	ambiguous name resolution;augmented reality;baseline (configuration management);cursor (databases);experiment;handheld game console;instability;interaction technique;tablet computer;tracking system;transfer function	Thomas Vincent;Laurence Nigay;Takeshi Kurata	2013		10.1007/978-3-642-40483-2_9	embedded system;computer vision;computer science;computer graphics (images)	HCI	-44.14508197611283	-45.90350210020873	146886
99acfb2adf74d1c2735e4f8af82e827be91fefd9	real stiffness augmentation for haptic augmented reality	perceptual quality;module system;satisfiability;contact dynamics;augmented reality;physical performance;force control	Haptic augmented reality (AR) mixes a real environment with computer-generated virtual haptic stimuli, enabling the system to modulate the haptic attributes of a real object to desired values. This paper reports our second study on this functionality, with stiffness as a goal modulation property. Our first study explored the potential of haptic AR by presenting an effective stiffness modulation system for simple 1D interaction. This paper extends the system so that a user can interact with a real object in any 3D exploratory pattern while perceiving its augmented stiffness. We develop a complete set of algorithms for contact detection, deformation estimation, force rendering, and force control. The core part is the deformation estimation where the magnitude and direction of real object deformation are estimated using a contact dynamics model identified in a preprocessing step. All algorithms are designed in a way that maximizes the efficiency and usability of the system while maintaining convincing perceptual quality. In particular, the need for a large amount of preprocessing such as geometry modeling is avoided to improve the usability. The physical performance of each algorithm is thoroughly evaluated with real samples. Each algorithm is experimentally verified to satisfy the physical performance requirements that need to be satisfied to achieve convincing rendering quality. The final perceptual quality of stiffness rendering is assessed in a psychophysical experiment where the difference in the perceived stiffness between augmented and virtual objects is measured. The error is less than the human discriminability of stiffness, demonstrating that our system can provide accurate stiffness modulation with perceptually insignificant errors. The limitations of our AR system are also discussed along with a plan for future work.	algorithm;augmented reality;bmc remedy action request system;computer-generated holography;direct stiffness method;embedded system;experiment;geometric modeling;haptic technology;interaction;multipoint ground;preprocessor;pulse-width modulation;real-time clock;requirement;sensor;simulation;usability testing;virtual reality	Seokhee Jeon;Seungmoon Choi	2011	PRESENCE: Teleoperators and Virtual Environments	10.1162/PRES_a_00051	computer vision;augmented reality;simulation;computer science;satisfiability	Visualization	-43.61604602521635	-46.4877110358503	146924
e5cd9aaaa889f7f13a7d6f30efa93b6906293686	exploring sensor gloves for teaching children sign language	different child;children sign language;current literature;sensor noise;sign language;sensor glove;teaching children sign language;alternative input device	This research investigates if a computer and an alternative input device in the form of sensor gloves can be used in the process of teaching children sign language. The presented work is important, because no current literature investigates how sensor gloves can be used to assist children in the process of learning sign language. The research presented in this paper has been conducted by assembling hardware into sensor gloves, and by designing software capable of (i) filtering out sensor noise, (ii) detecting intentionally posed signs, and (iii) correctly evaluating signals in signs posed by different children. Findings show that the devised technology can form the basis of a tool that teaches children sign language, and that there is a potential for further research in this area.		Kirsten Ellis;Jan Carlo Barca	2012	Adv. Human-Computer Interaction	10.1155/2012/210507	simulation;engineering;artificial intelligence;communication	HCI	-42.32460354079842	-44.49848885773849	147033
0bd9f3fb210ad37a5c68fddcc3bcd86be3e806f5	kansei estimation models for the sense of presence in audio-visual content with different audio reproduction methods	audio visual systems;estimation theory;standards;neural networks;sound spatial impression audio visual content system audio reproduction method content presence system presence neural network based kansei estimation model subjective evaluation experiment audio only condition;sense of presence;sound reproduction;testing;joints;system presence;visualization;system presence sense of presence audio visual content neural network content presence;content presence;audio visual content;calibration visualization neural networks standards testing headphones joints;sound reproduction audio visual systems estimation theory;headphones;calibration;neural network	The performances of both audio-visual content and systems are often evaluated by the sense of presence, which can be divided into two aspects: content presence and system presence. In our previous study, we constructed neural-network-based Kansei estimation models to evaluate content presence. Herein we aim to incorporate system presence into the Kansei models. We initially examined five audio reproduction methods, which simulate different systems, and conducted subjective evaluation experiments using 12 audio-visual content items. The experiments indicate the audio reproduction method influences both the audio-only and audio-visual conditions, but the effect is larger in the audio-only condition. Thus, we introduced four features related to the spatial impression of sound as new inputs into the previous Kansei estimation models. The expanded models successfully estimated both the content presence and system presence regardless of the condition. Hence, these models can quantitatively estimate the sense of presence in both audio-visual content and systems.	anomalous experiences;artificial neural network;audio signal processing;experiment;performance;presence information;simulation	Kenji Ozawa;Masashi Obinata;Yuichiro Kinoshita	2012	2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing	10.1109/SNPD.2012.106	computer vision;calibration;visualization;computer science;machine learning;multimedia;software testing;estimation theory;artificial neural network	SE	-39.568289246485115	-50.56268982863363	147140
06e24c7c9b8fe66b0222463f93cf0af24466ee68	power assist system hal-3 for gait disorder person	aide handicape;handicapped aid;contraste;ayuda minusvalido;interfase usuario;walking;electromyographie;caminata;user interface;deficiencia motora;motor handicap;marche a pied;interface utilisateur;electromyography;etalonnage;handicap moteur;electromiografia;calibration	We have developed the power assistive suit, HAL (Hybrid Assistive Leg) which provide the self-walking aid for gait disorder persons or aged persons. In this paper, We introduce HAL-3 system, improving HAL-1,2 systems which had developed previously. EMG signal was used as the input information of power assist controller. We propose a calibration method to identify parameters which relates the EMG to joint torque by using HAL-3. We could obtain suitable torque estimated by EMG and realize an apparatus that enables power to be used for walking and standing up according to the intention of the operator.	assistive technology;electromyography;hal	Hiroaki Kawamoto;Yoshiyuki Sankai	2002		10.1007/3-540-45491-8_43	calibration;simulation;computer science;artificial intelligence;operating system;user interface	Robotics	-40.25053491740142	-45.50054328403618	147153
505cf72a2d37c804176dea62cdfed21555255d18	a novel online sketch graphics recognition method for the scene of road traffic accident	human machine interaction online sketchy graphics recognition scene of road traffic accident corresponding feedback;road traffic engineering graphics human computer interaction image recognition natural scenes road accidents;scene of road traffic accident;operator drawing online sketch graphics recognition road traffic accident scene human machine interaction feedback graphics;image recognition;human computer interaction;traffic accident;road accidents;road traffic accident scene;engineering graphics;feedback graphics;road traffic;road traffic accident;corresponding feedback;man machine system;online sketch graphics recognition;accuracy;online sketchy graphics recognition;shape;accidents;roads;mean square error;mean square error methods;accidents shape roads man machine systems mean square error methods accuracy;operator drawing;graphics recognition;man machine systems;natural scenes;human machine interaction	A novel online sketch graphics recognition for the scene of road traffic accident is proposed to make the traffic accident processing more conveniently and efficiently. The practical human-machine interaction interface is designed to support showing and updating the corresponding feedback graphics on the screen as the operator drawing. The graphics recognition process is the crucial part of human-machine system. This paper proposes a novel way for sketch graphics recognition of traffic accident scene, the experimental results show the effectiveness of the proposed method.	graphics;human–computer interaction;human–machine system;user interface	Zhiquan Zhou;Tingting Zhang;Zhanfeng Zhao	2011	2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2011.6019853	computer vision;simulation;sketch-based modeling;shape;real-time computer graphics;mean squared error;accuracy and precision;sketch recognition;statistics;3d computer graphics;computer graphics (images)	Robotics	-38.456011197329325	-43.113528739098165	147513
abc93265b1680852551cb63fa1b748a855f88c5f	live-feedback from the imus: animated 3d visualization for everyday-exercising	wearables;imu;live feedback;kinematic model;sports	The use of external motion sensors with the athletes' wearables will enable new advanced applications for providing feedback on movement. We show how visual 3D-feedback based on Inertial Measurement Units (IMU) is technically and conceptually feasible. Our wearable-app connects to our custom designed IMU-pods for measuring the legs' orientation. The App then shows an animated kinematic model in 3D. Our study with ten runners further contributes by providing first insights into how to apply natural, live-feedback with wearables for supporting run-training. Our prototype for the smartphone and head-up display demonstrates great potential for personal- and everyday use.	feedback;head-up display;prototype;sensor;smartphone;symposium on principles of database systems;visualization (graphics);wearable computer	Matthias Seuter;Lucien Opitz;Gernot Bauer;David Hochmann	2016		10.1145/2968219.2968576	embedded system;inertial measurement unit;simulation;wearable computer;computer science;multimedia;computer graphics (images)	HCI	-44.93138591827537	-41.83046830134045	147740
05aaeafd520ff2d8fd785e68874df90f0cd9c758	consistent left-right reversals for visual path integration in virtual reality: more than a failure to update one's heading?	path integral;virtual reality;left right	Even in state-of-the-art virtual reality (VR) setups, participants often feel lost when navigating through virtual environments. In VR applications and psychological experiments, such disorientation is often compensated for by extensive training. Here, two experimental series investigated participants' sense of direction by means of a rapid point-to-origin paradigm without any performance feedback or training. This paradigm allowed us to study participants' intuitive spatial orientation in VR while minimizing the influence of higher cognitive abilities and compensatory strategies. After visually displayed passive excursions along one- or two-segment trajectories, participants were asked to point back to the origin of locomotion as accurately and quickly as possible. Despite using an immersive, high-quality video projection with a 84 63 field of view, participants' overall performance was rather poor. Moreover, about 40 of the participants exhibited striking qualitative errors, namely left-right reversalsdespite not misinterpreting the visually simulated turning direction. Even when turning angles were announced in advance to obviate encoding errors due to misperceived turning angles, many participants still produced surprisingly large systematic and random errors, and perceived task difficulty and response times were unexpectedly high. Careful analysis suggests that some, but not all, of the left-right inversions can be explained by a failure to update visually displayed heading changes. Taken together, this study shows that even an immersive, high-quality video projection system is not necessarily sufficient for enabling natural and intuitive spatial orientation or automatic spatial updating in VR, even when advance information about turning angles was provided. We posit that investigating qualitative errors for basic spatial orientation tasks using, for example, rapid point-to-origin paradigms can be a powerful tool for evaluating and improving the effectiveness of VR setups in terms of enabling natural and unencumbered spatial orientation and performance. We provide some guidelines for VR system designers.	cognition;course (navigation);experiment;immersion (virtual reality);inversion (discrete mathematics);programming paradigm;video projector;virtual reality	Bernhard E. Riecke	2008	PRESENCE: Teleoperators and Virtual Environments	10.1162/pres.17.2.143	computer vision;path integral formulation;simulation;computer science;virtual reality	Visualization	-44.50083939886373	-48.87243054729447	147744
22dbb5fc0cd932e9d51e64f9ec8f062f32f9cccc	smart privacy-preserving screen based on multiple sensor fusion	screen privacy protection smart privacy preserving screen sensor fusion display screen laptop smart phone pad screen peeping detection video camera module ultrasonic distance module light sensor module user distance environmental lightness screen lightness eye detection algorithm eye pair decision algorithm person counting algorithm;privacy cameras mobile communication smart phones acoustics films sensor fusion;data privacy;sensor fusion;sensor fusion data privacy object detection screens display;screens display;object detection;privacy preserving smart screen sensor fusion eye detection	Privacy problems arise with the popular usage of personal devices with display screen, e.g., laptop, smart phone or pad, in public areas, one of which is the screen peeping. This paper proposes a smart privacy-preserving screen system that can detect someone else see the consumer's screen and then protect the screen automatically and adaptively. It depends on multiple sensors, i.e., video camera module, ultrasonic distance module, light sensor module, to detect screen peeping, user distance and environmental lightness, and decide whether to adjust the screen's lightness. With limited lightness or contrast, the screen can only be seen by the consumer, while others cannot watch the screen clearly. Especially, the screen peeping detection scheme is composed of several algorithms, i.e., eye detection, eye pair decision and person counting. Based on the video frames captured by the video camera module, the scheme will detect eyes in the video frame, decide the number of persons peeping at the private screen, and inform the smart screen to protect the screen by adjusting the light or contrast. With the smart phone as an example, various experiments are done and comparative results show that the proposed scheme obtains better performance than existing works and is a good solution for automatic screen privacy protection.	algorithm;camera module;computational complexity theory;experiment;laptop;mobile phone;performance;privacy;sensor;smartphone	Shiguo Lian;Wei Hu;Xingguang Song;Zhaoxiang Liu	2013	IEEE Transactions on Consumer Electronics	10.1109/TCE.2013.6490252	computer vision;information privacy;computer science;engineering;sensor fusion;internet privacy;computer security	Mobile	-34.71424131873605	-44.76870146717758	148065
6ad75828807ab1cc830bb6bee66589e9bf4c29ba	collision avoidance affected by walker's head direction in a virtual environment			amiga walker;virtual reality	Shunya Ueda;Michiteru Kitazaki	2013		10.1007/978-3-642-39476-8_146		HPC	-36.36709702582627	-39.2361039043852	148175
1efa26759f5154b6839052c274d1eb846c5f7be5	enhanced auditory menu cues improve dual task performance and are preferred with in-vehicle technologies	multiple resources;ivts in vehicle technologies;dual task;theory and practice;user preferences;information presentation;spearcon;spindex;infotainment;indexation;auditory menus;success rate;text to speech;auditory display;tts text to speech	Auditory display research for driving has mainly focused on collision warning signals, and recent studies on auditory in-vehicle information presentation have examined only a limited range of tasks (e.g., cell phone operation tasks or verbal tasks such as reading digit strings). The present study used a dual task paradigm to evaluate a plausible scenario in which users navigated a song list. We applied enhanced auditory menu navigation cues, including spearcons (i.e., compressed speech) and a spindex (i.e., a speech index that used brief audio cues to communicate the user's position in a long menu list). Twenty-four undergraduates navigated through an alphabetized song list of 150 song titles---rendered as an auditory menu---while they concurrently played a simple, perceptual-motor, ball-catching game. The menu was presented with text-to-speech (TTS) alone, TTS plus one of three types of enhanced auditory cues, or no sound at all. Both performance of the primary task (success rate of the game) and the secondary task (menu search time) were better with the auditory menus than with no sound. Subjective workload scores (NASA TLX) and user preferences favored the enhanced auditory cue types. Results are discussed in terms of multiple resources theory and practical IVT design applications.	auditory display;mobile phone;netware file system;programming paradigm;speech coding;speech synthesis;time-compressed speech;user (computing)	Myounghoon Jeon;Benjamin K. Davison;Michael A. Nees;Jeff Wilson;Bruce N. Walker	2009		10.1145/1620509.1620528	speech recognition;computer science;auditory display;multimedia;communication	HCI	-47.981505577502716	-46.440055735631624	148381
e5ccdb897244254d73cf7ad3ac1efafc019031c8	design of a lower extremity exoskeleton for motion assistance in paralyzed individuals	smart phones handicapped aids human computer interaction;human machine interface lower extremity exoskeleton motion assistance paralyzed individuals chinese university of hong kong cuhk exo stand up sit down sts ergonomic design user friendly interface hardware design mechanical structure electrical system smart crutches smart phone app;robots;conferences robots biomimetics;conferences;biomimetics	In this paper, we reported innovative design of a lower extremity exoskeleton developed by The Chinese University of Hong Kong (CUHK-EXO) that can help the paralyzed individuals to regain the ability to stand up/sit down (STS) and walk. CUHK-EXO is developed with the features of ergonomic design, user-friendly interface and high safety. The CUHK-EXO hardware design including mechanical structure, electrical system, and multiple sensors is introduced first. A pair of smart crutches and smart phone App are designed as part of the human-machine interface, which can improve the intelligence of the system and make the exoskeleton easier to be used by physical therapists and paralyzed patients. Then, the CUHK-EXO control is developed with STS motion assistance. Finally, pilot trial study is conducted for a healthy subject, and testing results show that the developed CUHK-EXO can help the subject perform desired STS motions.	experiment;human factors and ergonomics;lopes (exoskeleton);poor posture;sensor;smartphone;usability;user interface	Bing Chen;Hao Ma;Lai-Yin Qin;Xiao Guan;Kai-Ming Chan;Sheung-Wai Law;Ling Qin;Wei-Hsin Liao	2015	2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2015.7418758	biomimetics;robot;simulation;computer science;engineering;artificial intelligence;mechanical engineering	Robotics	-40.88740931349391	-45.52410558883066	148551
17a0c6e4103a5a4e1dee27e39a9b4dea1378b15e	augmented media for traditional magazines	object recognition;mobile device;real time;magazine;planar object recognition;augmented reality;new products	Reading traditional newspapers or magazines is a common way to get latest information about events or new products. However these printed materials only provide readers with static information. Readers may want to know more detail information about some product in an article or to watch video clips related to an event mentioned in a news right at the moment when they read that article or news. The authors propose a system with mobile devices that can provide extra information and multimedia for readers by applying augmented reality to traditional magazines. A user can enjoy extra rich multimedia information on a product or news on his/her mobile device just by looking at an article in a traditional magazine through his/her mobile device. The system detects which article in which page of a magazine that is being displayed in a mobile device and provides a reader with related information and multimedia objects. The important feature of our proposed system is using lightweight filter to efficiently filter out candidate covers or articles that do not visually match an image captured by a mobile device. The experiment shows that our proposed system achieves the average accuracy of more than 90% and can process in the real-time manner.	augmented reality;chi;mobile device;parallel computing;printing;real-time clock;real-time computing;real-time transcription;social media;social network;video clip	Vinh-Tiep Nguyen;Minh-Triet Tran;Trung-Nghia Le;Quoc-Minh Bui;Anh Duc Duong	2012		10.1145/2350716.2350733	mobile search;computer science;multimedia;advertising;world wide web	HCI	-40.0548876831641	-40.07400075672283	148749
a88507f2bfb3ff27c4dfb4057ef88e89a5bca8b2	evaluating fitts' law performance with a non-iso task		We used a target-selection task to evaluate head-tracking as an input method on a mobile device. The procedure used a non-ISO Fitts’ law task since targets were randomly positioned from trial to trial. Due to a non-constant amplitude within each sequence of trials, throughput was calculated using two methods of data aggregation: by sequence of trials using the mean amplitude and by common A-W conditions. For each data set, we used four methods for calculating throughput. The grand mean for throughput calculated using the division of means and the adjustment for accuracy was 0.74 bps, which is 45% lower than the value obtained using an ISO task. We recommend calculating throughput using the division of means (and not the slope reciprocal from the regression model) and with the adjustment for accuracy. We present design recommendation for non-ISO tasks: Keep amplitude and target width constant within each sequence of trials and use strategies to avoid or remove reaction time.	data aggregation;fitts's law;ibm basic programming support;input method;mobile device;randomness;spatial variability;throughput	Maria Francesca Roig-Maimó;I. Scott MacKenzie;Cristina Manresa-Yee;Javier Varona	2017		10.1145/3123818.3123827	computer science;regression analysis;fitts's law;throughput;grand mean;real-time computing;amplitude;input method	Robotics	-47.02669488792864	-45.91175067327575	148815
5fc4d819fb495d4086c2820e75ac31f041440091	learning users' interest to assist image browsing and searching	attention object;browsing image;browsing experience;image attention model;interactive browsing;mobile device;image browsing;automatic browsing;large photo;manual browsing;browsing efficiency	In this thesis, we first investigate how to improve user experience of browsing large photos on mobile devices. Currently, the predominant methods for accessing large photos on small devices are down-sampling or manual browsing by zooming and scrolling. Image down-sampling or thumbnail view results in significant information loss due to the excessive resolution reduction. Manual browsing can avoid information loss but it is often time-consuming for users to catch the most crucial information of an image. Employing the concept of information asymmetry, the thesis investigates an image attention model and its extensions under different application scenarios. The image attention model can successfully solve the small screen browsing problem. First of all, different parts of an image contain different information, computer vision technology can analysis image and extract the attention objects to build the image attention model. Secondly, browsing large photo on small display can be formulated as a problem of optimized manipulating of attention objects to improve viewer's browsing experience, which brings about new functions such as automatic browsing and interactive browsing. Thirdly, the screen limitations of the mobile devices will force users to scroll and zoom while browsing images, which can be regarded as direct indications for users' attention, therefore new attention objects can be discovered by analyzing the past browsing log. Lastly, since psycho-physiological research tells us that the importance of an attention object may vary with the users' preferences and tasks, so attention model should also be adaptive to the users' interest. #R##N#The thesis also investigates the problem on how to improve the usability of the current image search engines, that is, how to explore the image search results on desktop PCs and mobile devices in a more effective way. The research results show that unsupervised learning technology can help improve the browsing efficiency. The similarity-based presentation method and clustering-base method are better for desktop PC platform and mobile platform respectively. The navigation operations are re-designed to make the new interface easy to use. Based on these results, the thesis also tries to investigate the problem on how to incorporate users' interaction in content-based image retrieval relevance feedback process. The new approach employs the parameter embedding visualization, multi-classifier learning and fusion methods to provide a seamless integrated browsing, user feedback and classifier learning process. (Abstract shortened by UMI.)	image viewer	Hao Liu	2005			computer vision;computer science;multimedia;world wide web	Mobile	-34.85495070966042	-48.93797149672197	149240
6b3b7fbfee8a87d9cece53190830eef7d81f21e0	interaction techniques for hmd-hhd hybrid ar systems	human computer interaction;mobile computing augmented reality data visualisation helmet mounted displays human computer interaction;conference contribution paper in published proceedings;data visualisation;multilayered visualization interaction techniques hmd hhd hybrid ar system mobile augmented reality systems head mounted display handheld display mobile devices ar visualization ar interaction cross device information sharing situation adaptive visualization management;handheld display mobile augmented reality wearable computer headmounted display;handheld display;wearable computer;augmented reality;mobile computing;mobile augmented reality;helmet mounted displays;visualization augmented reality mobile communication three dimensional displays mobile handsets wearable computers hardware;head mounted display	Most mobile Augmented Reality (AR) systems use either a head mounted display (HMD) or a handheld display (HHD) as a hardware platform. As mobile devices become more affordable, it becomes more common that users own more than one mobile device and use them together. In this research, we investigate Hybrid AR systems that use both HMD and HHD for AR visualization and interaction. In addition to a simple approach of using HMD as a display and HHD as an input device (e.g. a touch pad or a pointer), we further explore novel interaction techniques that can take advantage of having both HMD and HHD closely integrated into one AR system, such as cross-device information sharing, situation adaptive visualization management, and multi-layered visualization.	augmented reality;bmc remedy action request system;handheld game console;head-mounted display;input device;interaction technique;mobile device;pointer (computer programming);touchpad	Rahul Budhiraja;Gun A. Lee;Mark Billinghurst	2013	2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)	10.1109/ISMAR.2013.6671786	augmented reality;computer-mediated reality;wearable computer;human–computer interaction;computer science;optical head-mounted display;multimedia;mobile computing;computer graphics (images)	Visualization	-43.50420954767786	-39.20914685520277	149267
f269c9b9ce8a42fc4990941d21b457ffdb4fc514	predictive, augmented and transformed information display for time delay compensation in tele-handling/ machining	displays delay effects humans force feedback machining control systems appropriate technology prototypes master slave manipulators;machining;tele operated machine time delay compensation telehandling machining system predictive information display visual information audio information machine tool;time delay;multimedia computing;process control;telecontrol;machine tool;machine tools;delays machining process control machine tools telecontrol multimedia computing user interfaces;user interfaces;delays	This paper describes the construction of a telehandling/machining system with predictive and augmented information display. In particular, we propose a method for predictive information display and methods for information transformation of force to sound and visual information and illustrate their application through the use of a machine tool as an example of a tele-operated machine.	television	Mamoru Mitsuishi;Toshio Hori;Takaaki Nagao	1995		10.1109/ROBOT.1995.525262	control engineering;embedded system;simulation;engineering;machine tool;process control	HCI	-39.57437692937565	-45.698272606942595	149584
9f64c94dab46441cc6f709c4572398fef416118f	intersectionexplorer, a multi-perspective approach for exploring recommendations		Abstract In this paper, we advent a novel approach to foster exploration of recommendations: IntersectionExplorer, a scalable visualization that interleaves the output of several recommender engines with human-generated data, such as user bookmarks and tags, as a basis to increase exploration and thereby enhance the potential to find relevant items. We evaluated the viability of IntersectionExplorer in the context of conference paper recommendation, through three user studies performed in different settings to understand the usefulness of the tool for diverse audiences and scenarios. We analyzed several dimensions of user experience and other, more objective, measures of performance. Results indicate that users found IntersectionExplorer to be a relatively fast and effortless tool to navigate through conference papers. Objective measures of performance linked to interaction showed that users were not only interested in exploring combinations of machine-produced recommendations with bookmarks of users and tags, but also that this “augmentation” actually resulted in increased likelihood of finding relevant papers in explorations. Overall, the findings suggest the viability of IntersectionExplorer as an effective tool, and indicate that its multi-perspective approach to exploring recommendations has great promise as a way of addressing the complex human-recommender system interaction problem.		Bruno De Lemos Ribeiro Pinto Cardoso;Gayane Sedrakyan;Francisco Gutiérrez;Denis Parra;Peter Brusilovsky;Katrien Verbert	2019	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2018.04.008	visualization;scalability;user experience design;human–computer interaction;computer science	HCI	-36.01611004796031	-51.20722333966071	149639
bc69f00156d4e8357739c2f280bfd9b5275e3bdb	palmrc: imaginary palm-based remote control for eyes-free television interaction	direct touch;remote control;touch screen;user feedback;controlled experiment;non visual;input;conceptual design;omnipresent;eyes free;alternative remote control;handheld device;device less;tv;visual attention;exploratory study;memory	User input on television (TV) typically requires a mediator device, such as a handheld remote control. While being a well-established interaction paradigm, a handheld device has serious drawbacks: it can be easily misplaced due to its mobility and in case of a touch screen interface, it also requires additional visual attention. Emerging interaction paradigms like 3D mid-air gestures using novel depth sensors, such as Microsoft's Kinect, aim at overcoming these limitations, but are known to be e.g. tiring. In this paper, we propose to leverage the palm as an interactive surface for TV remote control. Our contribution is three-fold: (1) we explore the conceptual design space in an exploratory study. (2) Based upon these results, we investigate the effectiveness and accuracy of such an interface in a controlled experiment. And (3), we contribute PalmRC: an eyes-free, palm-surface-based TV remote control, which in turn is evaluated in an early user feedback session. Our results show that the palm has the potential to be leveraged for device-less and eyes-free TV remote interaction without any third-party mediator device.	handheld game console;imaginary time;interaction technique;kinect;mobile device;programming paradigm;remote control;sensor;television;touchscreen	Niloofar Dezfuli;Mohammadreza Khalilbeigi;Jochen Huber;Florian Müller;Max Mühlhäuser	2012		10.1145/2325616.2325623	simulation;human–computer interaction;telecommunications;computer science;conceptual design;mobile device;multimedia;memory;world wide web;exploratory research;remote control;computer graphics (images)	HCI	-46.587420631084875	-43.0589642954168	149797
6b2ae692e6cddd9808cd22c241e65555dd8e7768	development and evaluation of a system for ar enabling realistic display of gripping motions using leap motion controller		Abstract   Augmented Reality (AR) in the traditional systems have a problem that the drawn objects are always displayed in the foreground because 3D models by AR are superimposed later than the picture of the actual world. This paper proposed a system to produce a realistic picture of AR in accordance with every depth. We developed a prototype system to verify the effect of the method. The prototype system was developed by focusing on a human hand. This paper utilized a Leap Motion Controller as a motion capture device to acquire the depth data of the hand and fingers.	motion controller	Reiji Katahira;Masato Soga	2015		10.1016/j.procs.2015.08.269	computer vision;simulation;computer graphics (images)	HCI	-41.832200753680475	-38.59173735665929	149996
2700fb71a2938df26d41b64ef3149d6a38c807f1	combining 2d and 3d views for orientation and relative position tasks	display design;relative position;empirical study;3d visualization;orientation and relative position tasks;2d and 3d visualization;experiment;3d display	We compare 2D/3D combination displays to displays with 2D and 3D views alone. Combination displays we consider are: orientation icon (i.e., side-by-side), in-place methods (e.g., clip planes), and a new method called ExoVis. We specifically analyze performance differences (i.e., time and accuracy) for 3D orientation and relative position tasks. Empirical results show that 3D displays are effective for approximate navigation and relative positioning whereas 2D/3D combination displays (orientation icon and ExoVis) are useful for precise orientation and position tasks. Combination 2D/3D displays had as good or better performance as 2D displays. Clip planes were not effective for a 3D orientation task, but may be useful when only one slice is needed.	approximation algorithm;in-place algorithm	Melanie Tory;Torsten Möller;M. Stella Atkins;Arthur E. Kirkpatrick	2004		10.1145/985692.985702	experiment;computer vision;visualization;stereo display;computer science;empirical research;computer graphics (images)	HCI	-44.495163989841544	-47.170539011228726	150014
e9c00d73571f61afe2abeff375da6b4a054266e6	semanticpaint: interactive segmentation and learning of 3d worlds		We present a real-time, interactive system for the geometric reconstruction, object-class segmentation and learning of 3D scenes [Valentin et al. 2015]. Using our system, a user can walk into a room wearing a depth camera and a virtual reality headset, and both densely reconstruct the 3D scene [Newcombe et al. 2011; Nießner et al. 2013; Prisacariu et al. 2014]) and interactively segment the environment into object classes such as 'chair', 'floor' and 'table'. The user interacts physically with the real-world scene, touching objects and using voice commands to assign them appropriate labels. These user-generated labels are leveraged by an online random forest-based machine learning algorithm, which is used to predict labels for previously unseen parts of the scene. The predicted labels, together with those provided directly by the user, are incorporated into a dense 3D conditional random field model, over which we perform mean-field inference to filter out label inconsistencies. The entire pipeline runs in real time, and the user stays 'in the loop' throughout the process, receiving immediate feedback about the progress of the labelling and interacting with the scene as necessary to refine the predicted segmentation.	algorithm;conditional random field;headset (audio);interaction;interactivity;machine learning;random forest;real-time clock;real-time computing;user-generated content;virtual reality headset;word lists by frequency	Stuart Golodetz;Michael Sapienza;Julien P. C. Valentin;Vibhav Vineet;Ming-Ming Cheng;Victor Adrian Prisacariu;Olaf Kähler;Carl Yuheng Ren;Anurag Arnab;Stephen L. Hicks;David W. Murray;Shahram Izadi;Philip H. S. Torr	2015		10.1145/2782782.2792488	computer vision;simulation;computer science;artificial intelligence;operating system;machine learning;computer graphics (images)	Vision	-34.196981797636035	-42.956274015158925	150047
29466a4096f64dc6265eedf1973c710c5f9ebba7	skinput: appropriating the skin as an interactive canvas	minimally invasive;sensor array;interactive graphics	Skinput is a technology that appropriates the skin as an input surface by analyzing mechanical vibrations that propagate through the body. Specifically, we resolve the location of finger taps on the arm and hand using a novel sensor array, worn as an armband. This approach provides an on-body finger input system that is always available, naturally portable, and minimally invasive. When coupled with a pico-projector, a fully interactive graphical interface can be rendered directly on the body. To view video of Skinput, visit http://cacm.acm.org.	graphical user interface;handheld projector;skin (computing);skinput;video projector	Chris Harrison;Desney S. Tan;Dan Morris	2011	Commun. ACM	10.1145/1978542.1978564	embedded system;computer hardware;computer science;sensor array;computer graphics (images)	HCI	-43.3023582358458	-40.870266398533005	150105
8548deaa3a803679d80d72d422ebbc37572b00ec	bimanual natural user interaction for 3d modelling application using stereo computer vision	hand-posture-based control;usability test;hand posture;stereo computer vision;bimanual control;right hand;control mode command;modelling application;tasks multiple time;modelling software;test task;computer vision technique;hand tracking system;natural user interface;bimanual natural user interaction;stereo vision;computer vision	We demonstrate a system that allows the user to perform 3D modelling and sculpting using postures and 3D movements of their hands. The system utilises the concept of a Natural User Interface using computer vision techniques. This enables the user to operate 3D modelling software. The system's bimanual control allows left hand postures to select control mode commands, while the right hand controls movements.	3d computer graphics;computer stereo vision;computer vision;list of 3d modeling software;natural user interface	Roy Sirui Yang;Anthony Lau;Yuk Hin Chan;Alfonso Gastelum Strozzi;Christof Lutteroth;Patrice Delmas	2012		10.1145/2379256.2379279	computer vision;simulation;computer science;stereopsis;natural user interface	Vision	-40.016472264611316	-38.48314692120978	150125
d5418c51334c6dc007f12300ddb36302bb0decd4	music staging ai		"""Through smartphones, user enables to download/listen music anytime and anywhere. As a concept of a future audio player, we propose a framework of """"music staging artificial intelligence (AI)"""". In that framework, audio object signals, e.g. vocal, guitar, bass, drums and keyboards, are assumed to be extracted from stereo music signals. To visualize music as if live performance is virtually conducted, playing motion sequence is estimated by using separated signals. After adjusting the spatial arrangement of audio objects so as to each user prefers it, audio/visual rendering is conducted. We constructed two types of demonstration systems for music staging AI. In the smartphone-based implementation, each user enables to change the spatial arrangement through sliderbar dragging. Since information of user preferable spatial arrangement can be sent from each smartphone to server, it would enable to predict/recommend the user preferable spatial arrangement. In another implementation, head mount display (HMD) was utilized to dive into virtual music live performance. Each user enables to walk/teleport anywhere and audio is then changing corresponding to the user view."""	anytime algorithm;artificial intelligence;beneath a steel sky;computer keyboard;disk staging;download;drag and drop;head-mounted display;quantum teleportation;server (computing);smartphone	Yusuke Hioka;Kento Ohtani;Kazuya Takeda	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.8005294	visualization;rendering (computer graphics);computer science;multiple signal classification;mount;guitar;download;stage (cooking);multimedia	AI	-43.88946895818767	-40.310247124956796	150347
caee160eb2d1f2779de20f31b9cdc57797e81f68	utilizing sequences of touch gestures for user verification on mobile devices		Smartphones have become ubiquitous in our daily lives; they are used for a wide range of tasks and store increasing amounts of personal data. To minimize risk and prevent misuse of this data by unauthorized users, access must be restricted to verified users. Current classification-based methods for gesture-based user verification only consider single gestures, and not sequences. In this paper, we present a method which utilizes information from sequences of touchscreen gestures, and the context in which the gestures were made using only basic touch features. To evaluate our approach, we built an application which records all the necessary data from the device (touch and contextual sensors which do not consume significant battery life). Using XGBoost on the collected data, we were able to classify between a legitimate user and the population of illegitimate users (imposters) with an average equal error rate (EER) of 4.78% and an average area under the curve (AUC) of 98.15%. Our method demonstrates that by considering only basic touch features and utilizing sequences of gestures, as opposed to individual gestures, the accuracy of the verification process improves significantly.		Liron Ben Kimon;Yisroel Mirsky;Lior Rokach;Bracha Shapira	2018		10.1007/978-3-319-93040-4_64	artificial intelligence;computer science;machine learning;human–computer interaction;population;mobile device;word error rate;touchscreen;gesture	Mobile	-37.36120197244006	-46.298362924257596	150358
0e908340109f2a54aae66ac2ed1e8b26257cd9a9	dynamic image stacks	cameras hardware tracking image color analysis photography animation portable document format;exposure stacks;focal stacks;photography computer displays interactive systems;dynamic image stacks photograph viewing image parameters touch gestures photography interactive image viewer;computational photography;computational photography focal stacks exposure stacks	Traditionally, photography has been driven by a relatively fixed paradigm: capture, develop, and print. Even with the advent of digital photography, the photographic process still continues to focus on creating a single, final still image suitable for printing. This implicit association between a display pixel and a static RGB value can constrain a photographer's creative agency. We present dynamic image stacks, an interactive image viewer exploring what photography can become when this constraint is relaxed. Our system first captures a burst of images with varying capture parameters, then, in response to simple touch gestures on the image, our interactive viewer displays the best available image at the user's focus of attention. Exposure, focus, or white balance may be slightly compromised in the periphery, but the image parameters are optimal at the selected location. Dynamic image stacks turn photograph viewing into an interactive, exploratory experience that is engaging, evocative, and fun.	burst transmission;color balance;digital photography;ecosystem;image quality;image viewer;pixel;printing;programming paradigm	David E. Jacobs;Orazio Gallo;Kari Pulli	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2014.27	computer vision;computational photography;computer science;computational photography;multimedia;computer graphics (images)	Vision	-44.76850233813304	-40.084899496154385	150645
db9293d03caaefa1f81495f4e15cf46c72d922bf	multi-microphone adaptive array augmented with visual cueing	speech intelligibility acoustic noise audio signals biomedical equipment cameras consumer electronics ear hearing aids medical computing microphone arrays smart phones;hearing aids;speech intelligibility;smart phones;consumer electronics;microphones arrays auditory system prototypes noise speech face;adult algorithms cellular phone female hearing aids humans male middle aged software;medical computing;audiovisual multimicrophone array multimicrophone adaptive array visual cueing audiovisual array reverberant environments speech babble noise hearing aids smartphone software application smartphone accessory concept lower power radio audio signals camera real time face detection built in face detection support audiovisual beamforming algorithm signal noise ratio speech intelligibility handheld consumer electronic device;ear;acoustic noise;audio signals;microphone arrays;biomedical equipment;cameras	We present the development of an audiovisual array that enables hearing aid users to converse with multiple speakers in reverberant environments with significant speech babble noise where their hearing aids do not function well. The system concept consists of a smartphone, a smartphone accessory, and a smartphone software application. The smartphone accessory concept is a multi-microphone audiovisual array in a form factor that allows attachment to the back of the smartphone. The accessory will also contain a lower power radio by which it can transmit audio signals to compatible hearing aids. The smartphone software application concept will use the smartphone's built in camera to acquire images and perform real-time face detection using the built-in face detection support of the smartphone. The audiovisual beamforming algorithm uses the location of talking targets to improve the signal to noise ratio and consequently improve the user's speech intelligibility. Since the proposed array system leverages a handheld consumer electronic device, it will be portable and low cost. A PC based experimental system was developed to demonstrate the feasibility of an audiovisual multi-microphone array and these results are presented.	acquired immunodeficiency syndrome;algorithm;attachments;audio media;beamforming;canonical account;experimental system;face detection;handheld game console;hearing aids;intelligibility (philosophy);microphone device component;noise-induced hearing loss;numerous;optic nerve glioma, childhood;personal computer;real-time locating system;signal-to-noise ratio;smartphone;speech	Paul L. Gibson;Daniel S. Hedin;Evelyn E. Davies-Venn;Peggy Nelson;Kevin Kramer	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6346102	speech recognition;acoustics;computer science;engineering;audio signal;noise;communication;intelligibility;physics	HCI	-41.10016792554853	-43.625276380395334	150909
8a7697a9687e5d3dcbe06789c2b76013a478cabf	how do adults solve digital tangram problems? analyzing cognitive strategies through eye tracking approach	tangram;eye movement;cognitive strategies;spatial ability;eye tracking;problem solving	Purpose of the study is to investigate how adults solve tangram based geometry problems on computer screen. Two problems with different difficulty levels were presented to 20 participants. The participants tried to solve problems by placing seven geometric objects into correct locations. In order to analyze the process, the participants and their eye movements were recorded by an Tobii Eye Tracking device while solving the problems. The results showed that the participants employed different strategies while solving problems with different difficulty levels	computer monitor;eye tracking;regular expression	Bahar Baran;Berrin Dogusoy-Taylan;Kursat Cagiltay	2007		10.1007/978-3-540-73110-8_60	computer vision;simulation;eye tracking;eye movement	HCI	-46.00138599118017	-46.882203482607366	151462
b29900a333fffbcf506c46535234a8766e72dd35	building a driving simulator with parallax barrier displays		In this paper, we present an optimized 3D stereoscopic display based on parallax barriers for a driving simulator. The overall purpose of the simulator is to enable user studies in a reproducible environment under controlled conditions to test and evaluate advanced driver assistance systems. Our contribution and the focus of this article is a visualization based on parallax barriers with (I) a-priori optimized barrier patterns and (II) an iterative calibration algorithm to further reduce visualization errors introduced by production inaccuracies. The result is an optimized 3D stereoscopic display perfectly integrated into its environment such that a single user in the simulator environment sees a stereoscopic image without having to wear specialized eye-wear.	algorithm;driving simulator;iterative method;parallax barrier;simulation;stereoscopy;usability testing	Christoph Schinko;Markus Peer;Daniel Hammer;Matthias Pirstinger;Cornelia Lex;Ioana Koglbauer;Arno Eichberger;Jürgen Holzinger;Eva Eggeling;Dieter W. Fellner;Torsten Ullrich	2016		10.5220/0005711302810289	computer vision;computer graphics (images)	Visualization	-44.39598682299082	-46.54702369267404	151476
e227b74f43bbbf8428a8d57665b12c24e301d788	a pointer device for tft display screens that determines position by detecting colours on the display using a colour sensor and an artificial neural network	interfase usuario;transistor couche mince;touch sensitive screens;neural networks;fotodiodo;touch screen;display equipment;user interface;sistema informatico;color display;cathode tube;cathode ray tube;photodiode;teclado;liquid crystal displays;computer system;computer screen;affichage ecran plat;transistor capa delgada;computing;ecran visualisation;captador medida;prototipo;colour sensing;pantalla visualizacion;optoelectronic device;measurement sensor;capteur mesure;pointer;flat panel displays;affichage polychrome;ecran tactile;equipement affichage;dispositif pointage;keyboard;interface utilisateur;systeme informatique;dispositif optoelectronique;0707d;display screen;reseau neuronal;equipo visualizacion;pointing systems;tubo catodico;thin film transistor;prototype;tube cathodique;red neuronal;affichage cristaux liquides;dispositivo optoelectronico;clavier;artificial neural network;visualizacion policromo;neural network	Thin Film Transistor (TFT) screens are replacing traditional Cathode Ray Tube (CRT) screens and are becoming more common in the workplace. Traditional light sensing pens do not work effectively on a TFT screen, as the TFT screen does not generate a scan-line. A novel prototype computer system has been created that depicts where a user is pointing. The new sensor may replace the need for a touch-screen, mouse or keyboard. The system flashes a kaleidoscope of colours onto a TFT screen. A tri-colour photodiode detects these colours. A problem with this solution was the limited number of colours that could reliably be detected by the photodiodes and the time that the colours needed to remain on the screen. This paper describes a new system that includes an Artificial Neural Network (ANN) to predict the future position of the colour sensor on a computer screen. This information was used to place the optimum colours closest to the sensor in order to increase the accuracy of the reported position.	artificial neural network;color;pointer (computer programming);sensor;thin-film-transistor liquid-crystal display	David A. Sanders;Giles Tewkesbury	2009	Displays	10.1016/j.displa.2009.01.001	cathode ray tube;electronic engineering;computer science;engineering;electrical engineering;artificial neural network;computer graphics (images)	HCI	-40.778938607104884	-42.83730528866157	151480
f8b36d66b351912930b9cafed6f86c2320dd823a	fingerprint authentication device based on optical characteristics inside a finger	degradation;light scattering;authentication;ease of use;optical scattering;fingerprint recognition;fingers;access control;optical sensors;optical devices;optical devices fingerprint recognition authentication fingers optical scattering optical sensors access control costs degradation light scattering	Fingerprint is the most popular modality that is widely used in various authentication applications; PC logon, gate access control systems, and so on. The reason can be considered that fingerprint can achieve the best balance among authentication performance, cost, size of device, and ease of use. However, most of fingerprint authentication devices have some problems to be solved. One is that captured images are easily affected by the condition of finger surface and it can degrade authentication performance. The other is that the problem of impersonation by artificial gummy fingers has been pointed out. To solve those problems, we developed a new fingerprint authentication device that has a novel sensing principle. This device forms a image of fingerprint pattern based on optical characteristics of a finger’s interior by scattered transmission light. The images so obtained are unaffected by the condition of finger surface such as dry or moist fingers or operating environment, and enable stable authentication processes. And it can differentiate between real fingers and fake gummy fingers made from gelatin or other material using optical characteristics. Because this device utilizes the optical characteristics inside a finger, it has possibility to achieve higher authentication performance by combining multiple characteristics of a finger’s interior as a modality. In this paper, we describe the sensing principle and process algorithm of this device.	algorithm;authentication;control system;fingerprint recognition;modal logic;modality (human–computer interaction);operating environment;usability	Emiko Sano;Takuji Maeda;Takahiro Nakamura;Masahiro Shikai;Koji Sakata;Masahito Matsushita;Koichi Sasakawa	2006	2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)	10.1109/CVPRW.2006.83	embedded system;telecommunications;computer science;light scattering;computer security;fingerprint recognition	Vision	-40.717992473325445	-41.6967431365199	151494
33b879a1fd9ad3880b8a005dc946cdbab5b63088	impact de la représentation graphique de points d'intérêt sur la perception des distances en réalité augmentée	egocentric depth perception;depth cues;distance estimation;visual perception;augmented reality	This paper presents preliminary results of a study aimed at evaluating the effect of different ways of presenting points of interests (POI) in an augmented reality system on distance estimation. Each presentation applies specific combination of depth cues (relative size, occlusion, height, partial occlusion). Preliminary results from seven participants indicate that depth cues included in the design of the POI reduces the underestimation of the distances of the POI. The interfaces that facilitate distance estimation are those that rely both on the use of height in the visual field and relative size but not on occlusion.	augmented reality;depth perception;hidden surface determination;linear algebra	Thomas Dazeniere;J. M. Christian Bastien;Pascal Gaden;Jordane G. Grenier	2012		10.1145/2652574.2653437	computer vision;simulation;geography;communication	HCI	-45.08083548530166	-48.37553070078957	151913
7b377eb21e97448c1bfec0c279893f3eb7d6dd33	utilizing the android robot controller for robots, wearable apps, and the hotel room of the future		The Android Robot Controller (ARC) is designed to be a simple, standardized system that is able to be applied to robot or IoT devices using their native APIs. A description of how the different parts of ARC were chosen is discussed. The usability of ARC is shown through the development of applications running on multiple platforms. Application examples include: Controlling robots, a wearable metronome, and various human-in-the-loop applications for the IEEE RAS Winter School on Consumer Robotics' Hotel Room of the Future, Examples are given on mobile and wearable devices.	android (robot);control system;open-source software;real-time clock;robot control;usability;wearable computer;wearable technology	Daniel M. Lofaro	2017	2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2017.7992671	control theory;humanoid robot;wearable technology;wearable computer;usability;embedded system;robot control;robot;artificial intelligence;engineering;robotics	Robotics	-38.200721552139385	-44.983454461172045	151919
1f71d562ff3641e1e8b3e210caf38e9e6ad60bfa	usability analysis in gesture operation of interactive e-books on mobile devices		This paper analyses usability of interface including gesture operation and interaction in HCI (human-computer interaction). We explore the common gestures which are used to read e-books, and summarize the mode of operation of these gestures. This analysis bases on the five indicators that are proposed by Nielsen in 1993 to assess user interface of the interaction e-book. It discusses the interface of e-books that experts read and places emphasis on key points for visibility, button design and operation mode of multimedia button. Heuristic evaluation was adopted to study the gestures operation in reading e-book modes based on mobile devices. The study concludes that operation mode, perception & feedback design, and user's behavior are important interactions mode for interface design on mobile devices. The paper offers interaction design principles for development and improvement of the interactive e-book.		Ching-Hua Huang;Chao-Ming Wang	2011		10.1007/978-3-642-21675-6_66	user interface design;simulation;interactive systems engineering;human–computer interaction;computer science;multimedia;heuristic evaluation;interaction technique	HCI	-47.65479147611138	-45.05278709210784	151931
4a855fbfb55434267b1c7495352e0eb60f9d8699	tangible menus and interaction trays: core tangibles for common physical/digital activities	core tangibles;interaction trays;dynamic binding;blades tiles;tangible interaction;tangible menus;physical interaction	We introduce core tangibles: physical interaction elements which serve common roles across a variety of tangible and embedded interfaces. We describe two such tangibles: tangible menus and interaction trays. These may be composed together to dynamically bind discrete and continuous interactors to various digital behaviors. We discuss our approach, implementation, and early usage experiences.	embedded system;fundamental interaction	Brygg Ullmer;Rajesh Sankaran;Srikanth Jandhyala;Blake Tregre;Cornelius Toole;Karun Kallakuri;Christopher Laan;Matthew Hess;Farid Harhad;Urban Wiggins;Shining Sun	2008		10.1145/1347390.1347436	simulation;human–computer interaction;multimedia	HCI	-45.56526605148448	-38.6711686969126	151979
34df421debb47203c2c81b07eb98c696eb57e990	software-based adjustment of mobile autostereoscopic graphics using static parallax barriers	autostereoscopic display;context aware;mobile device;gpu;adaptive interface;display;context aware interface;ghosting;user testing;autostereoscopy;parallax barriers	We show that the autostereoscopic display of stereoscopic images using a static parallax barrier can be improved by adapting the rendering to the angle under which the user is looking at a mobile display; thus, ghosting artifacts and depth reversals can often be avoided even if the user tilts the mobile device. Instead of moving the barrier itself to compensate for a misplacement of the viewing zones in relation to the user, we employ dynamic pixel column shifts to provide a similar compensation in software. This requires a parallax barrier where each section covers two pixel columns at a time instead of one. The proposed method has been implemented using OpenGL shaders and a parallax barrier that was designed for a display of exactly half the resolution of the employed display. Technical tests showed a good separation of the left and right images for viewing angles of up to ± 30°. Preliminary user tests indicate that an improvement in the stereoscopic experience can be achieved.	autostereoscopy;column (database);graphics;mobile device;mobile phone;opengl;parallax barrier;pixel;rollover (key);shader;stereoscopy	Martin Paprocki;Kim Krog;Morten Bak Kristoffersen;Martin Kraus	2012		10.1145/2254556.2254612	autostereoscopy;parallax mapping;parallax barrier;computer vision;simulation;parallax occlusion mapping;computer science;operating system;mobile device;computer graphics (images)	Graphics	-43.927195317223415	-46.83075417709811	151986
86f0a9b8ad024b16e63344659af84e33812256a9	blind and visually impaired people: mobility and orientation-cyarm: interactive device for environment recognition and joint haptic attention using non-visual modality	aide handicape;handicapped aid;gaze;ayuda minusvalido;interfase usuario;vision disorder;movilidad;user interface;mobility;mobilite;mirada;atencion visual;user assistance;detection objet;regard;detector proximidad;assistance utilisateur;sensibilidad tactil;asistencia usuario;visual impairment;interface utilisateur;attention visuelle;intencion;trouble vision;trastorno vision;visual attention;sensibilite tactile;eye gaze;intention;tactile sensitivity;proximity detector;object detection;detecteur proximite	We have developed CyARM, a new kind of sensing device especially for visually impaired persons, to assist with mobility and detection of nearby objects. This user interface has unique characteristics of giving visually impaired persons the impression of an “imaginary arm” that extends to existing obstacles. CyARM is also a communication device for constructing “joint haptic attention” between impaired and unimpaired persons watching or feeling the same objects. In other words, this device offers a new methodology for judging the others' attentions or intentions without using their eye gaze direction. We verified the efficiency and ability of CyARM for environment recognition through experiments and discuss its possibility as a communication device for realizing joint haptic attention	haptic technology	Tetsuo Ono;Takanori Komatsu;Junichi Akita;Kiyohide Ito;Makoto Okamoto	2006		10.1007/11788713_180	computer vision;simulation;eye tracking;computer science;operating system;user interface;mobile computing	HCI	-43.17558592840212	-44.14240445153708	151990
4b11146413db056a2d9fbae81b465a7a0c3ebbda	delegation impossible?: towards novel interfaces for camera motion	motion control;interactive automation;cinematography;take over;human centered automation;user interfaces	When watching a movie, the viewer perceives camera motion as an integral movement of a viewport in a scene. Behind the scenes, however, there is a complex and error-prone choreography of multiple people controlling separate motion axes and camera attributes. This strict separation of tasks has mostly historical reasons, which we believe could be overcome with today's technology. We revisit interface design for camera motion starting with ethnographic observations and interviews with nine camera operators. We identified seven influencing factors for camera work and found that automation needs to be combined with human interaction: Operators want to be able to spontaneously take over in unforeseen situations. We characterize a class of user interfaces supporting (semi-)automated camera motion that take both human and machine capabilities into account by offering seamless transitions between automation and control.	cognitive dimensions of notations;integral theory (ken wilber);seamless3d;user interface;viewport	Axel Hoesl;Julie Wagner;Andreas Butz	2015		10.1145/2702613.2732904	motion control;smart camera;computer vision;match moving;simulation;human–computer interaction;computer science;multimedia;cinematography;user interface	HCI	-44.308290569801514	-38.733313679397156	152041
dd73c349898ff3dae293b9db93b1376cfc837145	frame untangling for unobtrusive display-camera visible light communication	information display;visible light communication	"""Pairing displays and cameras can open up convenient and """"free"""" visible light communication channels. But in realistic settings, the synchronization between displays (transmitters) and cameras (receivers) can be far more involved than assumed in the literature. This study aims to analyze and model the temporal behaviors of displays and cameras to make the visible light communication channel between the two more robust, while maintaining perceptual transparency of the transmitted data."""	channel (communications);download;transmitter	Xiao Shu;Xiaolin Wu	2016		10.1145/2964284.2967302	computer vision;computer science;visible light communication	HCI	-43.62488783394448	-40.470235244594775	152091
37ab4affdbd2c446d9a248bfa158dbf3186abc96	pop-up windows and information retrieval	information retrieval	Paralinguistics organizers such as parenthesis, footnotes, and pop-up fields on screen, should play an important role signalling the writer’s “minimization” intention when secondary points are concerned. This experiment investigated the role of pop-up fields compared to brackets in an information retrieval task. The results showed that, in this kind of task, putting secondary pieces of information into pop-up fields significantly speeded the search process compared to a condition in which the same information was displayed in brackets.	information retrieval;microsoft windows	Stéphane Caro	1997		10.1007/978-0-387-35175-9_88	document retrieval;query expansion;visual word;computer science;concept search;adversarial information retrieval;retrievability;information retrieval;human–computer information retrieval	AI	-35.919527393242305	-50.1058978405229	152115
01620532cca7bbd005f8d26ebb3d01a505fd8dd9	error augmented robotic rehabilitation of the upper limb - a review		Objective: To collect and assess the available evidence for the efficacy of error augmentation in upper limb robotic rehabilitation. Methods: A systematic literature search up to May 2013 was conducted in one citation index, the Web of Knowledge, and in two individual databases: PubMed and Scopus, for publications that utilized error augmented feedback as practice modality in robotic rehabilitation of the upper limb. Results: The systematic search returned 12 studies that utilized error augmented feedback in trials to unimpaired and impaired individuals suffering from stroke, multiple sclerosis and primary dystonia. One additional study utilizing viscous force fields was included as the authors paid special merit to the effects of the field in directions where the error was amplified. In the studies that met the inclusion criteria two different types of error augmented feedback was used that is, haptic and visual feedback which were used either separately as rehabilitation modalities or in conjunction with each other. All studies but one report positive outcome regardless of the type(s) of feedback utilized. Conclusions: Error augmentation in upper limb robotic rehabilitation is a relatively new area of study, counting almost nine years after the first relevant publication and rather understudied. Error augmentation in upper limb robotic rehabilitation should be further researched in more practice-intensive studies and with larger trial groups. The potential of error augmented upper limb rehabilitation should also be explored with conditions other than the ones described in this review.	database;feedback;force field (chemistry);haptic technology;modality (human–computer interaction);pubmed;rehabilitation robotics;scopus;web of science;world wide web	Aris C. Alexoulis-Chrysovergis;Andrew Weightman;Emma F. Hodson-Tole;Frederik J. A. Deconinck	2013		10.5220/0004654101670178	upper limb;physical medicine and rehabilitation;rehabilitation;medicine	HCI	-46.76985930962695	-48.1550994012026	152417
0d5a251b17d1a6dbdda5c52ee7626a0571875283	web-adapted supervised segmentation to improve a new tactile vision sensory substitution (tvss) technology		Nowadays, hand-held devices are being used more and more, especially for web navigation. But the small screen size of these devices requires adapting web page contents to be browsed more conveniently. A fundamental step in a successful automatic adaptation process of a web page is perception its visual layout and mining its Document Object Model (DOM) structure. In this paper, we present a new web-adapted supervised segmentation algorithm dedicated to vibro-tactile access on touch-screen devices. This suggested algorithm is fundamental in our framework whose aim is enhancing the ability of Visually Impaired Persons (VIP) to understand the 2-dimension web page layout by converting web pages into vibrating pages using a graphical vibro-tactile language. A comparison between automatic and manual segmented pages is presented. The objectives of this comparison are, on the one hand, to know how users understand web page layout structure based on their visual perception, and on the other hand, to explore the main differences between automatic and manual segmentation. © 2015 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Conference Program Chairs.	algorithm;automatic summarization;display size;document object model;experiment;graphical user interface;image processing;mobile device;sensory substitution;surge protector;television;touchscreen;web design;web navigation;web page	Waseem Safi;Fabrice Maurel;Jean-Marc Routoure;Pierre Beust;Gaël Dias	2015		10.1016/j.procs.2015.05.014	computer vision;computer science;artificial intelligence;machine learning;data mining;multimedia;world wide web	HCI	-36.4110350492403	-48.26592214599342	152638
8b7700ae7e4c9219b0d793d3544dafa1f486fdf5	food simulator: a haptic interface for biting	mouth;chemicals;bite force;auditory display food simulator haptic interface biting force food taste chemical sensation auditory sensation olfactory sensation haptic sensation taste display food texture force sensor chemical display multimodal sensations chemical taste;force sensors;virtual reality;testing;auditory displays;biting force;taste;chemical sensation;food taste;haptic interfaces force sensors chemicals auditory displays mouth olfactory teeth couplings end effectors testing;food simulator;food texture;chemioception gesture recognition digital simulation haptic interfaces virtual reality force sensors;teeth;auditory display;multimodal sensations;chemical taste;couplings;olfactory;haptic interfaces;haptic;chemical display;olfactory sensation;gesture recognition;force sensor;taste display;digital simulation;chemioception;end effectors;haptic sensation;auditory sensation;haptic interface	The food simulator is a haptic interface that presents biting force. The taste of food arises from a combination of chemical, auditory, olfactory and haptic sensation. Haptic sensation while eating has been an ongoing problem in taste display. The food simulator generates a force on the user's teeth as an indication of food texture. The device is composed of four linkages. The mechanical configuration of the device is designed such that it will fit into the mouth, with a force sensor attached to the end effector. The food simulator generates a force representing the force profile captured from the mouth of a person biting real food. The device has been integrated with auditory and chemical display for multi-modal sensations in a taste the food simulator has been tested on a large number of participants. The results indicate that the device has succeeded in presenting food texture as well as chemical taste.	haptic technology;modal logic;robot end effector;simulation	Hiroo Iwata;Hiroaki Yano;Takahiro Uemura;Tetsuro Moriya	2004	IEEE Virtual Reality 2004	10.1109/VR.2004.40	computer vision;simulation;computer science;artificial intelligence;gesture recognition;virtual reality;haptic technology	HCI	-41.29568301064585	-42.2196529374853	152891
7bc88b606bf3136574a72cf11c69a31b5d4e2ea0	collaborative search trails for video search	user study;information space;qa75 electronic computers computer science;z665 library science information science;video search	In this paper we present an approach for supporting users in the difficult task of searching for video. We use colla borative feedback mined from the interactions of earlier use rs of a video search system to help users in their current search tasks. Our objective is to improve the quality of the results that users find, and in doing so also assist users to explore a larg e nd complex information space. It is hoped that this will lead to them considering search options that they may not have c onsidered otherwise. We performed a user centred evaluation. The results of our evaluation indicate that we achieved our goals, the performance of the users in finding relevant video clips was enhanced with our system; users were able to explor e the collection of video clips more and users demonstrat ed a preference for our system that provided recommendat ions.	baseline (configuration management);bridging (networking);feedback;interaction;mined;overhead (computing);recommender system;video clip	Frank Hopfgartner;David Vallet;Martin Halvey;Joemon M. Jose	2008	CoRR		human–computer interaction;computer science;multimedia;world wide web;information retrieval	HCI	-34.10817821395338	-51.22722277435183	153243
b4ffe1ade4c50d27d2d4cd4c90fcb1d9108f1ed6	engagement-based user attention distribution on web article pages	ads positioning;user attention	The main monetization vehicle of many Web media sites are display ads located on article pages. Those ads are typically displayed either as banners on top of the page, or on the page's side bar. Advertiser ROI depends on the quality of ad targeting, as well as on how noticeable those ads are to users reading the article. Focusing on the latter issue, previous work has studied which ad positions are, on aggregate, more noticed by users.  This work takes the first step toward the personalized positioning of ads on article pages. We demonstrate a correlation between the level of attention that users devote to a story, and the position of the most noticeable graphic element on the side bar. In particular, we find that the graphic element most noticed by a user is roughly to the side of the point in the article where the user's attention waned. We argue that this finding lays the foundation for increasing display advertising effectiveness by tailoring ad positions on each article page impression to the user viewing it.	aggregate data;display advertising;monetization;page view;personalization;region of interest;widget (gui)	Oleg Rokhlenko;Nadav Golbandi;Ronny Lempel;Limor Leibovich	2013		10.1145/2481492.2481516	computer science;multimedia;world wide web	Web+IR	-35.72241866750059	-49.43263081472714	153345
affabb9329de8b1105545ca0da17348e5ac247f4	improving multimodal web accessibility for deaf people: sign language interpreter module	transparent video;human computer interaction;web pages;design and development;sign language;web accessibility;information presentation;accessibility;world wide web;video;deaf and hard of hearing;usability;web development;qualitative evaluation	The World Wide Web is becoming increasingly necessary for everybody regardless of age, gender, culture, health and individual disabilities. Unfortunately, there are evidently still problems for some deaf and hard of hearing people trying to use certain web pages. These people require the translation of existing written information into their first language, which can be one of many sign languages. In previous technological solutions, the video window dominates the screen, interfering with the presentation and thereby distracting the general public, who have no need of a bilingual web site. One solution to this problem is the development of transparent sign language videos which appear on the screen on request. Therefore, we have designed and developed a system to enable the embedding of selective interactive elements into the original text in appropriate locations, which act as triggers for the video translation into sign language. When the short video clip terminates, the video window is automatically closed and the original web page is shown. In this way, the system significantly simplifies the expansion and availability of additional accessibility functions to web developers, as it preserves the original web page with the addition of a web layer of sign language video. Quantitative and qualitative evaluation has demonstrated that information presented through a transparent sign language video increases the users’ interest in the content of the material by interpreting terms, phrases or sentences, and therefore facilitates the understanding of the material and increases its usefulness for deaf people.	database trigger;e-material;goto;javascript;maxima and minima;multimodal interaction;petra mutzel;prototype;quality of service;requirement;scalable link interface;video clip;web content accessibility guidelines;web accessibility;web developer;web page;world wide web	Matjaz Debevc;Primoz Kosec;Andreas Holzinger	2010	Multimedia Tools and Applications	10.1007/s11042-010-0529-8	web service;web development;framing;video;html;usability;web design;sign language;web accessibility initiative;web standards;computer science;accessibility;web navigation;web accessibility;web page;multimedia;programming language;web 2.0;world wide web	Web+IR	-35.177744993716516	-47.63558428786286	153386
c25665e1b31e87ecfcfa3f415d6c3c4d07ede60e	building a relational robot to be student&#146;s private learning secretary	structure dialogue;secretary role;internet interface;verbal communication unit;process unit;interaction module;intelligent conversational agent;relational robot;private learning secretary;relational robot;nonverbal communication unit;communication interface	Relational robot is an intelligent conversational agent designed to be a learning secretary for student to notify student of using our learning Web site on time. In this paper we discuss some modules of relational robot to let it play a learning secretary role; it contains interaction module (verbal communication unit, nonverbal communication unit and process unit), internet interface, and communication interface with structure dialogue	dialog system;robot	Len-Yan Sun;Chih-Wei Chang;Gwo-Dong Chen;Jorng-Tzong Horng	2006	Sixth IEEE International Conference on Advanced Learning Technologies (ICALT'06)	10.1109/ICALT.2006.96	nonverbal communication;the internet;simulation;computer science;humanoid robot;artificial intelligence;software agent;dialog system;multimedia	Robotics	-33.710759998384475	-38.49132354141832	153435
071b45b7b0baf114cce3123a1ac9156f9c1e8ef6	rotational dynamics for design of bidirectional feedback during manual interaction	mobile device;system modeling;spatial interaction;dynamic system;qa75 electronic computers computer science;computer science;hamilton institute;drag and drop;rotational dynamics	Rotational dynamic system models can be used to enrich tightlycoupled embodied control of movement-sensitive mobile devices, and support a more bidirectional, negotiated style of interaction. This can provide a constructive, as well as informative, approach to the design of engaging, playful elements in interaction mechanisms. A simulated rotational spring system is used for natural eyes-free feedback in both the audio and haptic channels, and in a Mobile Spatial Interaction application, using twisting and tilting motions to drag and drop content, where users perceived the effect of varying the parameters of the simulated dynamic system.	drag and drop;duplex (telecommunications);dynamical system;haptic technology;information;interaction;mobile device	Roderick Murray-Smith;Steven Strachan	2008		10.1007/978-3-540-88322-7_1	control engineering;simulation;human–computer interaction;engineering	HCI	-46.39005240236209	-39.07856621996093	153480
26354f218cf59fd2e4f197ee3139326c2430b388	look where you're going: visual interfaces for robot teleoperation	navigation;robot vision systems;context;cameras;robot kinematics	Two related challenges with current teleoperated robotic systems are lack of peripheral vision and awareness, and difficulty or tedium of navigating through remote spaces. We address these challenges by providing an interface with a focus plus context (F+C) view of the robot location, and where the user can navigate simply by looking where they want to go, and clicking or drawing a path on the view to indicate the desired trajectory or destination. The F+C view provides an undistorted, perspectively correct central region surrounded by a wide field of view peripheral portion, and avoids the need for separate views. The navigation method is direct and intuitive in comparison to keyboard or joystick based navigation, which require the user to be in a control loop as the robot moves. Both the F+C views and the direct click navigation were evaluated in a preliminary user study.	colour banding;computer keyboard;control system;fisheye;futures studies;item unique identification;joystick;mobile robot;motion planning;netware file system;obstacle avoidance;peripheral vision;robot;spatial–temporal reasoning;uniform resource identifier;usability testing;user interface	Jim Vaughan;Sven G. Kratz;Don Kimber	2016	2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)	10.1109/ROMAN.2016.7745142	turn-by-turn navigation;computer vision;navigation;simulation;computer science;artificial intelligence;social robot;multimedia;mobile robot navigation;robot kinematics	Robotics	-43.355215918088355	-45.01902987391497	153882
9e9d098ac02e7dc2eea9742c06a05401e89d1147	the role of static and dynamic shadows in a three-dimensional computer environment	pointing task.;3d input devices;. 3d interfaces;contextual hints;depth;evaluation;input device;three dimensional	The aim of this study was to analyse the effects of manipulating the contextual cues of depth on the accuracy of an aiming task in three-dimensional computer environment. Subjects performed a pointing task. For this task, we used two kind of input devices : a 3D mouse and an isotonic device. The target was a cube presented in a 3D room. The 3D context is configured by three factors which are texture, shadow or no shadow to the pointer and shadow or no shadow to the target. The results have shown the superiority of using isotonic input device and the usefulness of the shadow for a guidance in a 3D computer environment.		Patricia Plénacoste;Catherine Demarey;Cédric Dumas	1998			computer hardware;pointer (computer programming);input device;isotonic;computer science;shadow	HCI	-44.791071300358645	-45.20241615174814	154051
a31cb5f77f45f3e391ae6deda5875c072bf4879f	investigation of the user's text reading speed on mobile devices	human computer interaction;mobile computing	The paper presents an investigation of the user's text reading speed on mobile devices. For this purpose specialised software was developed and used. A comparison with the text reading speeds when is used a paper, personal computer and mobile device screen is made. The obtained results are compared with those of similar investigations. The appropriate conclusions are made.	mobile device;personal computer	Tsvetozar Georgiev	2012		10.1145/2383276.2383324	mobile search;human–computer interaction;computer science;operating system;mobile technology;multimedia;mobile computing;world wide web	HCI	-47.785258578394156	-44.068345959635444	154115
2d0ab445dd4e8f36ba1b069c35e156fc732c4d2f	is tilt interaction better than keypad interaction for mobile map-based applications?	mobile mapping;touch screen;smart phone;mobile map based applications;ease of use;mobile phone;sensor based interaction;tilt interaction;interaction technique	Map-based applications are becoming standard features of most smart phones. Keypad and touch-screen interaction have traditionally been used to interact with maps on mobile phones, but both these interaction techniques have several shortcomings. Tilt interaction offers an alternative approach with several advantages. Tilt interaction is intuitive and can be performed one-handed. Previous research has shown that tilt interaction offers equivalent or inferior results in terms of effectiveness, efficiency and preference. Similar studies have offered limited insight into the comparative performance of tilt and keypad interaction. This paper discusses the development and comparative evaluation of a prototype mobile map-based application supporting both keypad and tilt interaction. The results of this evaluation showed that keypad interaction was more efficient for tasks requiring precise selection, but that tilt interaction offered greater perceived controllability, efficiency and ease of use for navigation tasks.	interaction technique;mobile phone;prototype;smartphone;touchscreen;usability	Bradley van Tonder;Janet Wesson	2010		10.1145/1899503.1899539	embedded system;computer vision;simulation;engineering	HCI	-46.666626846674	-44.02285518614836	154285
87ec7aa65274e42ee61aac1d8991aabc9a22f017	shared control for assistive mobile robots based on vector fields	path planning;mobile robots;human robot interaction;path planning assisted living control engineering computing handicapped aids human robot interaction medical robotics mobile robots;human robot interaction medical assistive mobile robotics shared control vector fields;medical robotics;assisted living;handicapped aids;vectors navigation turning mobile robots electromyography collision avoidance;navigation route shared control assistive mobile robots vector fields brain computer interface electromyography emg limited mobility low cost bci signal to noise ratio classification accuracy loop control misinterpreted commands manual navigation;control engineering computing	Technologies such as Brain-Computer Interface (BCI) and Electromyography (EMG) allow people with limited mobility to interact with devices such as computers, home appliances and mobile robots. However, low cost BCI and EMG have not matured yet. Moreover, these technologies present relative low signal-to-noise ratio and classification accuracy. In case of employing BCI or EMG to manually control a mobile robot, a shared control must be inserted in the loop control to compensate misinterpreted commands. This paper presents a novel shared control approach based on vector fields for the manual navigation of assistive mobile robots. Unlike other approaches which take full control of the robot in certain situations, this technique allows full control to the user. Also, this approach reduces interventions to correct the navigation route caused by wrong classification of commands issued by the user. Results show that it is a simple, fast and effective technique.	algorithm;autonomous robot;brain–computer interface;computer;control theory;course (navigation);electromyography;mathematical optimization;mobile robot;signal-to-noise ratio	Leonardo Olivi;Ricardo Silva Souza;Eric Rohmer;Eleri Cardozo	2013	2013 10th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2013.6677482	human–robot interaction;mobile robot;computer vision;simulation;computer science;artificial intelligence;motion planning;robot control;mobile robot navigation	Robotics	-40.378488932421355	-44.9437310731835	154305
1a8a30340516063f8429a7e208f2187bd8791f36	modeling intuitive behavior for safe human/robot coexistence cooperation	robots man machine systems path planning;behavior modeling;path planning;interaction context;interaction context safe human robot coexistence human robot cooperation intuitive behavior model robot behavior motion context;robot behavior;human robot cooperation;safe human robot coexistence;intuitive behavior model;robots;motion context;humans robots robotics and automation;man machine systems	An intuitive behavior model for safe human/robot coexistence and cooperation is presented. For this purpose, we classify the robot behavior into four cooperation states, which are characterized by two criteria: motion context and interaction context. The motion context can be gross or fine and the interaction context can be free or guided. We describe basic principles relevant to modeling safe and intuitive behavior for robots. The desired robot behavior is detailed for each of the four states along with the necessary transitions between those states	behavior model;coexist (image);collision detection;image impedance;robot;state diagram;state transition table;velocity (software development)	Dominik Henrich;Stefan Kuhn	2006	Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.	10.1109/ROBOT.2006.1642304	behavioral modeling;behavior-based robotics;robot;mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence;social robot;motion planning;robot control;personal robot	Robotics	-34.298843103674386	-40.2932363872524	154409
860594aae608f50b9c3ed354d57b08123fa23d33	thermal sweet taste machine for multisensory internet	digital taste;hci;thermal sweet taste machine;multisensory communication;trpm5	"""This paper presents a new taste interface for multisensory communication called """"Thermal Sweet Taste Machine"""". We developed this interface in order to create sweet sensations, by manipulating the temperature on the tongue, without using chemicals. This device device changes the temperature on the surface of the tongue (from 20°C to 40°C) within a short period of time using a computer controlled circuit. Our preliminary user studies suggested that this device would be effective in two ways; producing the sweet sensations without the aid of chemicals, and enhancing the sweetness of the food and drinks. Here we discuss our concept, development of the interface, and some preliminary studies that has been carried out. We believe our technology would enhance the experiences and capabilities in future multisensory communication in different disciplines such as Human-Computer Interaction, human robot interactions, gaming and interacting with artificial agents."""	human–computer interaction;intelligent agent;thermal grease;usability testing	Nur Amira Samshir;Nurafiqah Johari;Kasun Karunanayaka;Adrian David Cheok	2016		10.1145/2974804.2980503	simulation;engineering;artificial intelligence;communication	HCI	-46.79877886878586	-40.90690772021083	154494
b2c392719c015879bec8c655e64a3558ff8b3f99	presentation of virtual liquid by modeling vibration of a japanese sake bottle	liquids vibrations solid modeling viscosity haptic interfaces virtual reality visualization;viscosity virtual liquid modeling vibration japanese sake bottle visual modality auditory modality tactile modality glug sound sinusoidal wave;viscosity;tactile display haptic rendering pouring water;vibrations;virtual reality;viscosity haptic interfaces rendering computer graphics vibrations virtual reality;liquids;visualization;haptic rendering;tactile display;solid modeling;haptic interfaces;pouring water	It is known that visual, auditory, and tactile modalities affect the experiences of eating and drinking. One such example is the “glug” sound and vibration from a Japanese sake bottle when pouring liquid. Our previous studies have modeled the wave of the vibration by summation of two decaying sinusoidal waves with different frequencies. In this paper, to enrich expression of various types of liquid, we included two new properties of liquid: the viscosity and the residual amount of liquid, both based on recorded data.	experience	Sakiko Ikeno;Ryuta Okazaki;Taku Hachisu;Hiroyuki Kajimoto	2015	2015 IEEE Virtual Reality (VR)	10.1109/VR.2015.7223430	computer vision;visualization;viscosity;computer science;artificial intelligence;vibration;virtual reality;solid modeling	Visualization	-44.43031660473878	-50.82932576357941	154497
6701e82b049f541c0a114506708ce31a73b6945f	utilizing sound effects in mobile user interface design	sound effects;auditory interface;mobile user interfaces;mobile device;sonification;auditory interfaces;multi modal interface;real time;field trial;non speech audio;multimodal user interface;mobile phone;navigation;reverb;signal processing;calendar;multi modal interfaces;mobile phones;data auralization	The current generation of mobile devices is capable of producing polyphonic sounds, has enough processing power for real-time signal processing, and much better sound quality than their predecessors. The importance of audio is increasing as we are moving towards multimodal user interfaces where audio is one of the major components. In this paper, we present new ways of using audio feedback more efficiently and intelligently in mobile user interfaces by utilizing real-time signal processing. To test the ideas in practice, a prototype calendar application was implemented. We arranged a one week field trial to validate the design ideas. The results indicate that sound effects are capable of passing information to the user in some extent, but they are more useful in impressing the user and making existing audio feedback	audio feedback;calendaring software;jukka tapanimäki;jussi karlgren;mobile device;multimodal interaction;prototype;real-time locating system;real-time transcription;signal processing;sound quality;transmitter;unix signal;usability;user interface design	Hannu Korhonen;Jukka Holm;Mikko Heikkinen	2007		10.1007/978-3-540-74796-3_27	user interface design;navigation;sonification;human–computer interaction;audio signal processing;reverberation;computer science;operating system;signal processing;mobile device;multimedia	HCI	-47.42288882389968	-42.81238211442258	154737
79bf8920c30fcb76d6f314e73c57e4b74627afec	exploring the interactivity issues of the stereoscopic 3d systems for design education	design education;vdp teknologi 500 informasjons og kommunikasjonsteknologi 550 annen informasjonsteknologi 559;vdp teknologi 500 informasjons og kommunikasjonsteknologi 550 datateknologi 551;interactivity issues;stereoscopic 3d displays	Stereoscopic 3D displays have been used by some research groups to present learning contents for education. However, in the highly interactive situations, the intertwined depth cues may result in symptoms that hamper the usability of such systems. In this research, an experiment was conducted to explore the interactivity issues. Thirty students were invited to participate in the experiment. The first task was to identify the differences between printed pictures and 3D virtual models. The second task was to point out ergonomic or design problems in a single piece of furniture or pairs of chairs and tables. Based on the analysis, discomfort caused by model rotation did contribute to the degree of overall discomfort. Even all participants had the background of using 3D modeling systems, some still experienced different levels of symptoms. Their comments indicated that adaptive adjustments of disparity and control response ratio were necessary in the highly interactive situations.	interactivity;stereoscopic video game;stereoscopy	Li-Chieh Chen;Yun-Maw Cheng;Po-Ying Chu;Frode Eika Sandnes	2015		10.1007/978-3-319-20684-4_3	human–computer interaction;engineering;multimedia;computer graphics (images)	HCI	-44.36431861924921	-47.787983526529224	154798
838c22bb499273d2ff79ad6aa52de492715e5101	"""multimodal user interface system for blind and """"visually occupied"""" users: ergonomic evaluation of the haptic and auditive dimensions"""	multimodal interfaces;physical model;force-feedback;blind people.;user interface development system;haptic and auditory interfaces;graphic user interface;force feedback;error rate	"""An ergonomical evaluation of a multimodal windows-oriented interface that gives blind and """"visually occupied"""" persons access to graphical user interfaces. Physical models are used to represent interface features providing haptic informations, thanks to a force-feedback device, and nonspeech audio. In the absence of vision, both auditory and haptic modalities are used, to perform manipulation tasks. Three conditions were compared: sound feedback alone, force feedback alone and bimodal feedback. Measures of usability were timing, error rates with subjective satisfaction. Results from the experiment show that multimodality was associated with better performance for blind and sighted subjects and that it is ranked as the best interface."""	graphical user interface;haptic technology;human factors and ergonomics;microsoft windows;multimodal interaction;usability	Aude Dufresne;Odile Martial;Christophe Ramstein	1995				HCI	-48.24378289809009	-45.75448606252652	155014
69e95b1ea19cbf9c9bdbb57bb0b1a92a9faefb75	design and evaluation of haptic effects for use in a computer desktop for the physically disabled	input device;haptic mouse;physical disability;target prediction;force feedback;target selection;targeting;algorithm design;haptic interaction;human computer interface;haptic interface	The human–computer interface remains a mostly visual environment with little or no haptic interaction. While haptics is finding inroads in specialized areas such as surgery, gaming, and robotics, there has been little work to bring haptics to the computer desktop, which is largely dominated today by the GUI/mouse relationship. The mouse as an input device, however, poses many challenges for users with physical disabilities, and it is believed that a haptically enhanced interface could have significant impact assisting in target selection. This paper presents a study intended to evaluate haptic effects used with a force feedback mouse on a computer desktop and a prediction algorithm designed to focus those effects on the desired target. Results of the experiment were partially successful and indicated future directions for improvement. The paper introduces the proposed framework and presents experimental results from targeting tasks using differing haptic effects with a group of physically disabled users.	algorithm;channel capacity;desktop computer;experiment;fitts's law;graphical user interface;haptic technology;human–computer interaction;input device;mouse button;robotics;surround sound	Brian Holbert;Manfred Huber	2010	Universal Access in the Information Society	10.1007/s10209-010-0192-x	stereotaxy;simulation;human–computer interaction;computer science;multimedia;haptic technology	HCI	-45.705715313405605	-47.90512247303846	155060
a50ab064244f5cdcafcc4d2d6f9c3636ee081628	multimodal desktop interaction: the face - object - gesture - voice example	object recognition;user interface management systems face recognition gesture recognition human computer interaction object recognition sensor fusion speech recognition;human computer interaction;face face recognition speech recognition databases joints speech;face recognition;multimodal input multimodal interaction natural interaction ms kinect;natural user interface system human behaviors microsoft kinect multisensor input qualifiers desktop applications interaction intermediate module multimodal human computer interaction;speech recognition;user interface management systems;sensor fusion;gesture recognition	This paper presents a natural user interface system based on multimodal human computer interaction, which operates as an intermediate module between the user and the operating system. The aim of this work is to demonstrate a multimodal system which gives users the ability to interact with desktop applications using face, objects, voice and gestures. These human behaviors constitute the input qualifiers to the system. Microsoft Kinect multi-sensor was utilized as input device in order to succeed the natural user interaction, mainly due to the multimodal capabilities offered by this device. We demonstrate scenarios which contain all the functions and capabilities of our system from the perspective of natural user interaction.	algorithm;database;desktop computer;emoticon;face detection;human computer;human–computer interaction;input device;kinect;login;modality (human–computer interaction);multimodal interaction;natural user interface;object detection;operating system;robotic mapping;robotics;sensor	Nikolas Vidakis;Anastasios Vlasopoulos;Tsampikos Kounalakis;Petros Varchalamas;Michalis Dimitriou;Grigorios Kalliatakis;Efthimios Syntychakis;John Christofakis;George A. Triantafyllidis	2013	2013 18th International Conference on Digital Signal Processing (DSP)	10.1109/ICDSP.2013.6622782	computer vision;speech recognition;computer science;multimodal interaction;gesture recognition;natural user interface;communication;sketch recognition;interaction technique	Robotics	-36.74875279768346	-42.90662709329471	155106
16ff942c6bc6d16cc7b2eea768e55462c3f88d65	virtual locomotion: walking in place through virtual environments	virtual environment	This paper presents both an analysis of requirements for user control over simulated locomotion and a new control technique designed to meet these requirements. The goal is to allow the user to move through virtual environments in as similar a manner as possible to walking through the real world. We approach this problem by examining the interrelationships between motion control and the other actions people use to act, sense, and react to their environment. If the interactions between control actions and sensory feedback can be made comparable to those of actions in the real world, then there is hope for constructing an effective new technique. Candidate solutions are reviewed once the analysis is developed. This analysis leads to a promising new design for a sensor-based virtual locomotion called Gaiter. The new control allows users to direct their movement through virtual environments by stepping in place. The movement of a person's legs is sensed, and in-place walking is treated as a gesture indicating the user intends to take a virtual step. More specifically, the movement of the user's legs determines the direction, extent, and timing of their movement through virtual environments. Tying virtual locomotion to leg motion allows a person to step in any direction and control the stride length and cadence of his virtual steps. The user can walk straight, turn in place, and turn while advancing. Motion is expressed in a body-centric coordinate system similar to that of actual stepping. The system can discriminate between gestural and actual steps, so both types of steps can be intermixed.	control theory;feedback;gesture recognition;in-place algorithm;interaction;pointing device gesture;requirement;sensor;simulation;stepping level;user interface;virtual reality	James N. Templeman;Patricia S. Denbrook;Linda E. Sibert	1999	Presence	10.1162/105474699566512	computer vision;simulation;computer science;virtual machine;operating system	Graphics	-45.79042123955465	-48.08669176737048	155222
1820f10245d1cf53f9ec02879fa2ada0e0f893b4	kinematic analysis of remote target pointing hand movements in a 3d environment		The study investigates kinematics of target pointing hand movements in a free-hand, touchless, 3D environment. Consistent with Fitts’ original information processing model, target pointing hand movement time in a 3D space increased with an increase of distance, and increased with a decrease of target size. The relative primary submovement movement time (% of total movement time) for target with depth (67.24%) was higher than that for targets without depth (64.49%). Pointing targets in the upper half of the reference framework required a lifting up of the arm to work against the gravity, and thus cost more than those in the lower half. For targets with depth, they required an even elevated upper-arm, thus result a longer “primary submovement time”, a decreased “peak velocity”, and a decreased relative time to peak velocity.		Yung-Hui Lee;Shu-Kai Wu	2011		10.1007/978-3-642-22095-1_35	computer vision;human–computer interaction;computer science;artificial intelligence;information processing theory;depth perception;kinematics	HCI	-45.048689388431	-48.82064860861602	155291
fb69aae72b9f1379cabd725d35e0c425c60c7d67	using eye movement parameters for evaluating human-machine interface frameworks under normal control operation and fault detection situations	performance measure;ecological interface design;mental workload;information content;performance improvement;human machine interface;eye movement;fault detection;normal control;pupil diameter;mental model	A human-machine interface framework provides general guidelines for what information should be put on an interface display screen. The framework is thus a first step towards the design of an effective and efficient interface. This paper reports on an experimental study of two proposed frameworks: the ecological interface design framework and the function behaviour-state framework. In order to provide an unbiased comparative evaluation for both interfaces, the same application problem is used. The interfaces, based on each of the two frameworks, are implemented with as similar look-and-feel forms as possible in the presentation of information contents. Only the normal control operation and fault detection situations are considered at this stage of the study. In addition, in this study three categories of measures are used, namely: the performance measure; the physiological measure (the eye movement measure: the eye fixation and the pupil diameter change, in particular); and the subjective (or the user-rated) measure. The major results obtained from the study includes the following: (1) the information called the abstract function in the ecological interface design framework may not positively correlate to the performance improvement yet may increase the mental workload, (2) the function-behaviour-state framework seems more agreeable with the operator's mental model, and (3) operators may perform equally well with a function-behaviour-state interface but with a reduced mental workload. It is also found that the eye fixation measure is highly consistent with the performance measure and the subjective measure. The pupil diameter measure is found not to be significantly sensitive to the mental workload information; however, it appears sensitive to the mental workload information among individual participants and shows a consistent result with the other measures used.	fault detection and isolation;user interface	Yaping Lin;W. J. Zhang;L. G. Watson	2003	Int. J. Hum.-Comput. Stud.	10.1016/S1071-5819(03)00122-8	human–machine interface;computer vision;real-time computing;simulation;self-information;ecological interface design;computer science;fault detection and isolation;eye movement	HCI	-48.1660225388647	-47.32670612291056	155342
058d94646429f384b12dfbeda5a7797a77a514fe	gaze typing through foot-operated wearable device	foot operated devices;gaze typing;wearable devices	Gaze Typing, a gaze-assisted text entry method, allows individuals with motor (arm, spine) impairments to enter text on a computer using a virtual keyboard and their gaze. Though gaze typing is widely accepted, this method is limited by its lower typing speed, higher error rate, and the resulting visual fatigue, since dwell-based key selection is used. In this research, we present a gaze-assisted, wearable-supplemented, foot interaction framework for dwell-free gaze typing. The framework consists of a custom-built virtual keyboard, an eye tracker, and a wearable device attached to the user's foot. To enter a character, the user looks at the character and selects it by pressing the pressure pad, attached to the wearable device, with the foot. Results from a preliminary user study involving two participants with motor impairments show that the participants achieved a mean gaze typing speed of 6.23 Words Per Minute (WPM). In addition, the mean value of Key Strokes Per Character (KPSC) was 1.07 (ideal 1.0), and the mean value of Rate of Backspace Activation (RBA) was 0.07 (ideal 0.0). Furthermore, we present our findings from multiple usability studies and design iterations, through which we created appropriate affordances and experience design of our gaze typing system.	entry sequenced data set;experience design;eye tracking;iteration;typing;usability testing;virtual keyboard;wearable computer;wearable technology;words per minute	Vijay Rajanna	2016		10.1145/2982142.2982145	computer vision;simulation;computer science;wearable technology	HCI	-46.89878779204973	-45.29947844389519	155350
a022c9e73a331833efcb4641a6f15447ed052425	comp2watch: enhancing the mobile video browsing experience	mobile;video summarization;mobile device;cost function;layout;summarization;adaptation;video;mobile video;aspect ratio	"""The mobile devices have been widely spread and become frequently used equipment in daily life. Besides, watching videos on these devices has become a more and more popular activity. However, there are several challenges (e.g., small mobile screen size, low bandwidth, fragmented watching time) hindering mobile video watching: they either interrupt the watching process or limit users to browse many contents at the same time. Traditional video summarization techniques are suffering the small screen issue. Therefore, we propose a system, Comp2Watch which is pronounced like """"come to watch"""". It implies the meaning of """"composing the frames into a collage"""" and """"compressing the watching time"""". It puts ROI factors into consideration in order to help users take a quick glance at videos. Also, we modify the cost function to incorporate the templates with variable aspect ratios. We also address the monotone layout problem caused by the limited space. The experimental results show that users can obtain clearer subject without losing many contexts."""	browsing;display size;interrupt;loss function;mobile device;mobile phone;region of interest;retargeting;seam carving;television;usability testing;user interface;monotone	Yu-Ming Hsu;Ming-Kuang Tsai;Yen-Liang Lin;Winston H. Hsu	2011		10.1145/2072561.2072565	mobile search;simulation;computer science;multimedia;world wide web	HCI	-35.53642188188922	-47.83598559395147	155454
819794b33a2076b0b7c492201de6286b15634924	utilization of visual information perception characteristics to improve classification accuracy of driver's visual search intention for intelligent vehicle		To cite this article: Ji Hoon Kim, Ji Hyoun Lim, Chun Ik Jo & Kyungdoh Kim (2015) Utilization of Visual Information Perception Characteristics to Improve Classification Accuracy of Driver’s Visual Search Intention for Intelligent Vehicle, International Journal of Human-Computer Interaction, 31:10, 717-729, DOI: 10.1080/10447318.2015.1070561 To link to this article: http://dx.doi.org/10.1080/10447318.2015.1070561	expanded memory;human–computer interaction	Ji Hoon Kim;Ji Hyoun Lim;Chunik Jo;Kyungdoh Kim	2015	Int. J. Hum. Comput. Interaction	10.1080/10447318.2015.1070561	computer vision;simulation	Robotics	-40.35927292270814	-49.9429639695415	155502
4270e94f812259ffce728c77f28d6faf3a27deb8	visualize your robot with your eyes closed: a multi-modal interactive approach using environmental feedback	robot sensing systems;user interaction multimodal interactive approach environmental feedback visual impairment program sequence robotic system human robot interaction framework multimodal sensory feedback environmental perception haptic feedback auditory feedback;eyes closed;proceedings;sensory feedback;robotics;human robot interaction;force;visualization;feedback;handicapped aids;human robot interaction feedback handicapped aids haptic interfaces;robot sensing systems haptic interfaces collision avoidance force visualization;visual impairment;haptic feedback;collision avoidance;auditory feedback;haptic interfaces;user interaction;haptic interface;multi modal interaction	In this paper, we discuss an approach for enabling students with a visual impairment (VI) to validate the program sequence of a robotic system operating in the real world. We introduce a method that enables the person with VI to feel their robot's movement as well as the environment in which the robot is traveling. The design includes a human-robot interaction framework that utilizes multi-modal feedback to transfer the environmental perception to a human user with VI. Haptic feedback and auditory feedback are selected as primary methods for user interaction. Using this multi-modal sensory feedback approach, participants are taught to program their own robot to accomplish varying navigation tasks. We discuss and analyze the implementation of the method as deployed during two summer camps for middle-school students with visual impairment.	feedback;haptic technology;human–robot interaction;modal logic;robot	Chung Hyuk Park;Sekou Remy;Ayanna M. Howard	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5980381	computer vision;simulation;computer science;artificial intelligence;haptic technology;robotics	Robotics	-43.01127263862729	-45.493973848132704	155815
8efd21104900e9114244270ee91254f33a50a53c	assessment of a user centered interface for teleoperation and 3d environments	human computer interaction;virtual reality;teleoperation	The efficient, natural and precise selection and manipulation of nearby objects in 3D environments is yet a challenge in the area of 3D interaction. Robot teleoperation is a potential application field for this kind of interaction and a difficult task. In part due to the low quality of the information delivered to the operator, but also because of the non-natural interfaces used to operate the robot. This paper presents a direct 3D user interface based on visual and haptic feedback that can be correctly operated by untrained users. The interface offers a higher level of presence than the average existing solutions, and involves the use of the operator's primary hand to control a robotic arm using a Phantom device. Spatial awareness, in turn, is obtained from stereoscopic vision and motion parallax effect. The main contribution though is an assessment study to investigate which interface elements maximize user performance and if any interface elements is counter-effective. Surprising results show that parallax is the most effective feature while stereoscopy is often detrimental and force feedback requires training.	3d interaction;3d user interaction;haptic technology;parallax;robot;robotic arm;stereopsis;stereoscopy;user interface;user-centered design	Juliano Franz;Anderson Maciel;Luciana Porcher Nedel	2013		10.1145/2480362.2480546	computer vision;teleoperation;simulation;human–computer interaction;computer science;artificial intelligence;virtual reality;natural user interface;user interface	HCI	-44.86333021227978	-47.43896007280635	155880
1aa95da4e3603d3df05c2ae7ae74234180caaf72	leonardo: a first step towards an interactive decision aid for port-placement in robotic surgery	human computer interaction;da vinci surgical system;laparoscopy surgery;planning phase leonardo interactive decision aid port placement prototype interactive augmented reality system surgeon robot assisted laparoscopic surgery computer projector color tracking device motion tracking device human computer interface patient abdomen patient body surgical procedure port locations gloved hand;user interfaces augmented reality control engineering computing decision making image colour analysis interactive systems medical computing medical robotics object tracking surgery;medical robotics;medical computing;minimally invasive surgery ports computers cameras robot kinematics laparoscopes;image colour analysis;object tracking;surgery;control engineering computing;surface augmented reality;augmented reality;interactive systems;user interfaces;human computer interaction da vinci surgical system surface augmented reality laparoscopy surgery	Leonardo is a prototype interactive augmented reality system designed to aid a surgeon in port-placement prior to performing robot-assisted laparoscopic surgery. The system components consist of a computer, a projector, and a motion and color tracking device. The human-computer interface through which the surgeon interacts with the system is the patient's abdomen itself. Based on the parameters of the patient body and surgical procedure, an optimized set of port locations is provided to the surgeon as colored icons projected onto the abdomen. The surgeon can adjust the suggested locations by direct manipulation with the gloved hand and fingers in real-time. This interactive decision-aid is expected to shorten the planning phase, and enhance performance of robot-assisted laparoscopic surgery.	algorithm;augmented reality;computer;direct manipulation interface;human–computer interaction;interaction technique;leonardo (robot);mathematical optimization;prototype;real-time computing;real-time locating system;robot;tracking system;video projector	Manuel Simões;Caroline G. L. Cao	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.90	computer vision;augmented reality;simulation;computer science;video tracking;user interface	Robotics	-42.40608946977794	-46.042443578306106	155885
3256951b113a266787a22c25e96bac3f50b04817	smarttouch - augmentation of skin sensation with electrocutaneous display	thin film;electric stimulation;tactile information smarttouch electrocutaneous display system optical sensors skin sensation electrical stimulation;augmented reality haptic interfaces;skin displays tactile sensors electrodes optical sensors optical films thin film sensors force sensors image sensors electrical stimulation;augmented reality;haptic interfaces;optical sensor;force sensor	An electrocutaneous display system composed of three layers is implemented for augmentation of skin sensation. The first layer has electrodes on the front side of a thin plate, the second has optical sensors on the reverse side of the plate, and the third is a thin film force sensor between the other two layers. Visual images captured by the sensor are translated into tactile information, and displayed through electrical stimulation. Thus, visual surface information can be perceived through the skin while natural tactile sensation is unhindered. Based on the sensor, the user can ”touch” other modalities of surface information as well.	functional electrical stimulation;sensor	Hiroyuki Kajimoto;Masahiko Inami;Naoki Kawakami;Susumu Tachi	2003		10.1109/HAPTIC.2003.1191225	computer vision;electronic engineering;engineering;communication;tactile sensor	HCI	-41.237860267580615	-41.75571192751833	156197
9b8b0bdbc249ad7213361c711411d18adedbdb71	the effects of fast disparity adjustment in gaze-controlled stereoscopic applications	comfort models;fusion time;eye tracking;stereoscopic rendering	With the emergence of affordable 3D displays, stereoscopy is becoming a commodity. However, often users report discomfort even after brief exposures to stereo content. One of the main reasons is the conflict between vergence and accommodation that is caused by 3D displays. We investigate dynamic adjustment of stereo parameters in a scene using gaze data in order to reduce discomfort. In a user study, we measured stereo fusion times after abrupt manipulation of disparities using gaze data. We found that gaze-controlled manipulation of disparities can lower fusion times for large disparities. In addition we found that gaze-controlled disparity adjustment should be applied in a personalized manner and ideally performed only at the extremities or outside the comfort zone of subjects. These results provide important insight on the problems associated with fast disparity manipulation and are essential for developing appealing gaze-contingent and gaze-controlled applications.	binocular disparity;contingency (philosophy);emergence;personalization;stereoscopy;usability testing;vergence	Matthias Bernhard;Camillo Dell'mour;Michael Hecher;Efstathios Stavrakis;Michael Wimmer	2014		10.1145/2578153.2578169	computer vision;simulation;geography;computer graphics (images)	HCI	-42.48592059761017	-48.91832208565744	156776
8a03870e2b4e75474a2255b8a7c3b1304ad712d5	towards imitation learning of grasping movements by an autonomous robot	imitation learning;autonomous robot	r2sutCvnw xzy|{ tC}gy ~  ~S{ vVVy|tOx[CtC}nOt  }ntOx|zy%~ ~gintOy|tOx n~gintCy|tOx- L  ? S?*[ ¡[¢[ Q£¤[¥g p¡[g gS £¦S§S ̈ ©«a }ny|y|Vy |¬ Vx­tC?x|~S}VM~*x|{4wpy| ̄®AVVVxz°  }VtOx|zyp¬ w y ±J~gL[n{ s­°¤2*2g3* ́*μb± ~gi[n{2n¶­tOx|{4w*}g ·C ̧ *S1*¡?o*§1⁄4»1⁄2^oSg3⁄4T»1⁄2^o3⁄4g ¿ ̈g1?Àp¢*Á[S ̈gg¥g ÁgÂ[¥* 1⁄2no*? Ã £¦* ̈¡[[Ä ̈Á?Ä ¿?¥g ¡ ̈p1⁄2 £§g Å9Æ w*ÇZÈVM~*xu ~S{ vVVyiw y|~*}^w*ÉTw*}nÊ.± ~*É ̄~*ËS w ÉÌ­z~S}  }VtOx|zy%~* ?~S?y|ntOx|}2 w*Éf~ x|}n ̄w? Æ ~S}VËStCÉtC ¤ uL  ?	autonomous robot	Jochen Triesch;Jan Wieghardt;Eric Maël;Christoph von der Malsburg	1999		10.1007/3-540-46616-9_7	psychology;robot learning;computer vision;simulation;social robot;communication	Robotics	-35.664253780837726	-39.38457768675186	156831
066c85b26494ba7867ec1a2f42eda5917502667f	an open-source tele-operated mobile manipulator: chap v1		Teleoperated mobile manipulators are of use for disabled people and for the wider public interested in acting at distance. The high price of existing devices is a barrier to their diffusion. The paper reports on the first design produced in the Cheap Arm Project (CHAP). It costs less than £2000, uses easily available parts and can be assembled by anybody with basic technical skills. The manipulator can reach objects from floor-level up to shelves at a height of 170 cm using a new low-cost arm design. Teleoperation is be done using a tablet, smartphone or browser. The cost could be further reduced by using different servo motors. The design and assembly instructions are made available on the open-source repository GitHub, with the hope that the community will build and improve the design. The first version has been tested in a college for disabled young people who provided initial recommendations for improvement.	mobile manipulator;open-source software;servo;smartphone;tablet computer;television	Guido Bugmann;Dominic Cassidy;Paul Doyle;Khushdeep Singh Mann	2017		10.1007/978-3-319-64107-2_16	embedded system;robot;teleoperation;manipulator;mobile manipulator;servomotor;computer science	HCI	-37.92910328311586	-40.110897685333235	156880
02a7a7e3fc6cd9fca2717c31ae724c31d33f2bd5	predicting the cost of error correction in character-based text entry technologies	human performance;virtual keyboard;data collection;user study;text entry;text input;performance metric;mobile phone;human factors;error correction;error rate;experience base;cognitive model;prediction;hand held devices	Researchers have developed many models to predict and understand human performance in text entry. Most of the models are specific to a technology or fail to account for human factors and variations in system parameters, and the relationship between them. Moreover, the process of fixing errors and its effects on text entry performance has not been studied. Here, we first analyze real-life text entry error correction behaviors. We then use our findings to develop a new model to predict the cost of error correction for character-based text entry technologies. We validate our model against quantities derived from the literature, as well as with a user study. Our study shows that the predicted and observed cost of error correction correspond well. At the end, we discuss potential applications of our new model.	error detection and correction;human factors and ergonomics;human reliability;real life;text-based (computing);usability testing	Ahmed Sabbir Arif;Wolfgang Stuerzlinger	2010		10.1145/1753326.1753329	cognitive model;human performance technology;simulation;error detection and correction;speech recognition;prediction;word error rate;computer science;human factors and ergonomics;data mining;world wide web;data collection	HCI	-47.488303070514625	-45.90929750496779	156903
f79580c213b2d4c2c3c4d9eea3f89c42a74ab190	handsfree omnidirectional vr navigation using head tilt		Navigating mobile virtual reality (VR) is a challenge due to limited input options and/or a requirement for handsfree interaction. Walking-in-place (WIP) is considered to offer a higher presence than controller input but only allows unidirectional navigation in the direction of the user's gaze--which impedes navigation efficiency. Leaning input enables omnidirectional navigation but currently relies on bulky controllers, which aren't feasible in mobile VR contexts. This note evaluates the use of head-tilt - implemented using inertial sensing - to allow for handsfree omnidirectional VR navigation on mobile VR platforms. A user study with 24 subjects compared three input methods using an obstacle avoidance navigation task: (1) head-tilt alone (TILT) (2) a hybrid method (WIP-TILT) that uses head tilting for direction and WIP to control speed; and (3) traditional controller input. TILT was significantly faster than WIP-TILT and joystick input, while WIP-TILT and TILT offered the highest presence. There was no difference in cybersickness between input methods.	gps navigation device;in-place algorithm;input method;joystick;obstacle avoidance;usability testing;virtual reality sickness	Sam Tregillus;Majed Al Zayer;Eelke Folmer	2017		10.1145/3025453.3025521	handsfree;simulator sickness;control theory;joystick;simulation;omnidirectional antenna;inertial frame of reference;computer science;obstacle avoidance;computer vision;virtual reality;artificial intelligence	HCI	-45.27254533425034	-46.48518440271386	157069
a72836a25eeaf0d4ea5144246abc330fb2676290	the one-key challenge: searching for a fast one-key text entry method	keyboards;assistive technologies;text entry;mobile computer;scanning keyboards;ambiguous keyboards;error correction;assistive technology;mobile computing	"""A new one-key text entry method is presented. SAK, for """"scanning ambiguous keyboard"""", combines one-key physical input (including error correction) with three virtual letter keys and a SPACE key. The virtual letter keys are highlighted in sequence (""""scanned"""") and selected when the key bearing the desired letter receives focus. There is only one selection per letter. Selecting SPACE transfers scanning to a word-selection region, which presents a list of candidate words. A novel feature of SAK is multiple-letter-selection in a single scanning interval. In an evaluation with 12 participants, average entry speeds reached 5.11 wpm (all trials, 99% accuracy) or 7.03 wpm (error-free trials). A modification using """"timer restart on selection"""" allowed for more time and more selections per scanning interval. One participant performed extended trials (5 blocks x 5 phrases/block) with the modification and reached an average entry speed of 9.28 wpm."""	error detection and correction;secure attention key;sticky keys;timer;words per minute	I. Scott MacKenzie	2009		10.1145/1639642.1639660	simulation;error detection and correction;speech recognition;human–computer interaction;computer science;operating system;mobile computing;world wide web	HCI	-46.95918353186569	-45.28230166485279	157108
1fe8fc424e54e63e7c04f26efb6d837010f073ef	spacetop: integrating 2d and spatial 3d interactions in a see-through desktop environment	desktop management;3d ui;augmented reality	SpaceTop is a concept that fuses spatial 2D and 3D interactions in a single workspace. It extends the traditional desktop interface with interaction technology and visualization techniques that enable seamless transitions between 2D and 3D manipulations. SpaceTop allows users to type, click, draw in 2D, and directly manipulate interface elements that float in the 3D space above the keyboard. It makes it possible to easily switch from one modality to another, or to simultaneously use two modalities with different hands. We introduce hardware and software configurations for co-locating these various interaction modalities in a unified workspace using depth cameras and a transparent display. We describe new interaction and visualization techniques that allow users to interact with 2D elements floating in 3D space. We present the results from a preliminary user study that indicates the benefit of such hybrid workspaces.	desktop computer;interaction;modality (human–computer interaction);oled;seamless3d;usability testing;workspace	Jinha Lee;Alex Olwal;Hiroshi Ishii;Cati N. Boulanger	2013		10.1145/2470654.2470680	augmented reality;human–computer interaction;computer science;operating system;multimedia;world wide web;computer graphics (images)	HCI	-44.81122595599892	-39.65560619815853	157382
da5860435fd6d3461217f44311965a3882d04289	the scaling of the haptic perception on the fingertip using an interface of anthropomorphic finger motions		The demonstration described in this paper attempts to give users tactile feedback on the sole of the virtual avatar using the locomotion interface of anthropomorphic finger motions. We believe that the illusion in the contact area can be caused by the sense of ownership derived from the close relationship of the motion between the fingers and the avatar’s legs. The objective of this study was to prove the possibility that fingers can be substituted in the place of legs in locomotion interfaces in terms of tactile sensation.		Yusuke Ujitoko;Koichi Hirota	2014		10.1007/978-4-431-55690-9_1	computer vision;scaling;haptic perception;contact area;sensation;illusion;artificial intelligence;computer science	Robotics	-45.344062875978665	-49.20284240991809	157391
3e033d941be6628ececf0e69c967497ce0146273	tracking contact and free gesture across large interactive surfaces	tecnologia electronica telecomunicaciones;computacion informatica;grupo de excelencia;ciencias basicas y experimentales;tecnologias	Built into store windows, museum exhibits, and other communal spaces, these surfaces entice even casual passersby to playfully interact with information---and each other---by knocking on the glass.	microsoft windows;port knocking	Joseph A. Paradiso	2003	Commun. ACM	10.1145/792704.792731	theoretical computer science;human–computer interaction;computer science;gesture;casual	Graphics	-44.58698201031417	-38.32896712360713	157416
0333ad2a4cac3d343259c3d47f1097fffbc7d1a4	synchrowatch: one-handed synchronous smartwatch gestures using correlation and magnetic sensing		SynchroWatch is a one-handed interaction technique for smartwatches that uses rhythmic correlation between a user's thumb movement and on-screen blinking controls. Our technique uses magnetic sensing to track the synchronous extension and reposition of the thumb, augmented with a passive magnetic ring. The system measures the relative changes in the magnetic field induced by the required thumb movement and uses a time-shifted correlation approach with a reference waveform for detection of synchrony. We evaluated the technique during three distraction tasks with varying degrees of hand and finger movement: active walking, browsing on a computer, and relaxing while watching online videos. Our initial offline results suggest that intentional synchronous gestures can be distinguished from other movement. A second evaluation using a live implementation of the system running on a smartwatch suggests that this technique is viable for gestures used to respond to notifications or issue commands. Finally, we present three demonstration applications that highlight the technique running in real-time on the smartwatch.	computer;interaction technique;online and offline;real-time clock;smartwatch;synchronization (computer science);video clip;waveform	Gabriel Reyes;Jason Wu;Nikita Juneja;Maxim Goldshtein;W. Keith Edwards;Gregory D. Abowd;Thad Starner	2017	IMWUT	10.1145/3161162	computer vision;distraction;smartwatch;interaction technique;wearable computer;waveform;gesture;correlation;computer science;artificial intelligence	HCI	-48.254341123325524	-47.44840526653957	157420
16d30c7efb7b4f0a6aaf31e4a8eb9ed590301ed9	formal verification of human-automation interaction	automatic control;model design;systeme commande;interfase usuario;sistema control;metodo analisis;commercial aircraft;human performance;human computer interaction;operator;metodologia;ergonomia;operador;user interface;program verification computers;relacion hombre maquina;commande automatique;hombre;man machine relation;flight;ergonomie;methodologie;formal method;operator interaction;vol;methode analyse;control system;formal verification;support u s gov t non p h s;analysis method;human;operateur;aviation;interface utilisateur;control automatico;machines;relation homme machine;methodology;aeronautics;vuelo;man machine systems;ergonomics;human machine interaction;task performance and analysis;homme;automated control systems;human automation	This paper discusses a formal and rigorous approach to the analysis of operator interaction with machines. It addresses the acute problem of detecting design errors in human-machine interaction and focuses on verifying the correctness of the interaction in complex and automated control systems. The paper describes a systematic methodology for evaluating whether the interface provides the necessary information about the machine to enable the operator to perform a specified task successfully and unambiguously. It also addresses the adequacy of information provided to the user via training material (e.g., user manual) about the machine's behavior. The essentials of the methodology, which can be automated and applied to the verification of large systems, are illustrated by several examples and through a case study of pilot interaction with an autopilot aboard a modern commercial aircraft. The expected application of this methodology is an augmentation and enhancement, by formal verification, of human-automation interfaces.	addresses (publication format);automatic control;automation;autopilot;control system;correctness (computer science);formal verification;human–computer interaction;laboratory sample manual;sensor;verification and validation;verification of theories;verifying specimen	Asaf Degani;Michael Heymann	2002	Human factors	10.1518/0018720024494838	human performance technology;machine;simulation;formal verification;computer science;engineering;artificial intelligence;operator;automatic control;methodology;flight;user interface;aviation	SE	-38.86380029915335	-48.10645144681155	157442
77051db62164edc2e51d0a0a9976970daa292e47	virtual, augmented and mixed reality. designing and developing virtual and augmented environments		3D Virtual Environments (3DVE) are more and more used in different applications such as CAD, games, or teleoperation. Due to the improvement of smartphones hardware performance, 3D applications were also introduced to mobile devices. In addition, smartphones provide new computing capabilities far beyond the traditional voice communication. They are permitted by the variety of built-in sensors and the internet connectivity. In consequence, interesting 3D applications can be designed by enabling the device capabilities to interact in a 3DVE. Due to the fact that smartphones have small and flat screens and that a 3DVE is wide and dense, mobile devices present some constraints: the environment density, the depth of targets and the occlusion. The pointing task faces these three problems to select a target. We propose a new classification of the existing interaction techniques, according to three axis of classification: a) the three discussed problems (density, depth and occlusion); b) the first two subtasks of the pointing task (navigation, selection); and c) the number of targets selected by the pointing technique (1 or N). In this paper we will begin by presenting a state of the art of the different pointing techniques in existing 3DVE, structured around three selection techniques: a) Ray casting, b) Curve and c) Point cursor. Then we will present our classification, and we will illustrate the classification of the main pointing techniques for 3DVE. From this classification, we will discuss the type of interaction that seems the most appropriate to perform this subtask optimally.	apache axis;augmented reality;computer-aided design;cursor (databases);emoticon;interaction technique;internet access;mixed reality;mobile device;ray casting;sensor;smartphone;virtual reality	Esubalew Bekele;Joshua W. Wade;Dayi Bian;Lian Zhang;Zhi Zheng;Amy Swanson;Medha Shukla Sarkar;Zachary Warren;Nilanjan Sarkar	2014		10.1007/978-3-319-07458-0	mixed reality	HCI	-45.60522750059656	-41.52221414226527	157575
f29b3c8f9fdfe3ad9aa3eb233a65bdf4dbcd353b	user customizable interaction in coexistence space	robot sensing systems;electronic mail;command transmission user customizable interaction coexistence space user centered interaction user customizable interface devices control object handling environmental interaction;user interfaces gesture recognition;user interfaces aerospace electronics robot sensing systems gesture recognition real time systems electronic mail;aerospace electronics;gesture recognition user customizable interface coexistent interaction;user interfaces;gesture recognition;real time systems	This paper proposes a user customizable interface for user-centered interaction in a space where the real, virtual and remote world is unified and indistinguishable through a combination of humans, artifacts and a virtual world. We have developed a new user customizable interface, which is a set according to the preferences of users and is intuitive and natural to minimize inconvenience of using the dedicated interface. We have executed several experiments using user gestural expression as user customizable interface such as: various devices control, object handling, environmental interaction, and command transmission in the space. The results of the demonstration have shown that effective interaction with a user customizable interface is possible among the real, virtual and remote worlds.	coexist (image);experiment;user-centered design;virtual world	KangGeon Kim;Myoung Soo Park;Jung-Min Park	2014	2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2014.7057419	user interface design;embedded system;user;10-foot user interface;user experience design;user modeling;human–computer interaction;computer science;gesture recognition;multimedia;natural user interface;user interface;interaction technique	Robotics	-46.18760974329967	-39.35030355662402	157687
7331a115837915a36c90a204ac1b7e390cb34efc	virtual camera scouting with 3dof motion trackers	virtual cinematography;degree of freedom;real time;motion tracking	The Screenplay Scout interface enables a filmmaker to scout camera placements and angles in a virtual 3D world using a lightweight and low-cost combination of a gamepad and a three-degree of freedom (3DOF) orientation sensor. He or she records the camera moves as the orientation sensor's real-time 3DOF data is mapped to the aim direction of the virtual camera and the gamepad controls camera position and the zoom lens. During recording the Screenplay Scout automatically saves to disk still frame images and a text log of camera setting data to support creation of storyboards and cinematic previsualization.	gamepad;previsualization;real-time clock;scout;storyboard;virtual camera system	William H. Bares;Nick Buckner;Daniel Castille	2010		10.1145/1900008.1900129	computer vision;camera auto-calibration;simulation;computer science;computer graphics (images)	HCI	-39.652441422846174	-39.507485767409314	157723
144de21ca089474ca12b9fecd0a16fdaaf78cbbc	automated capture and delivery of assistive task guidance with an eyewear computer: the glaciar system	assistive computing;task guidance;augmented reality;eyewear computing	In this paper we describe and evaluate an assistive mixed reality system that aims to augment users in tasks by combining automated and unsupervised information collection with minimally invasive video guides. The result is a fully self-contained system that we call GlaciAR (Glass-enabled Contextual Interactions for Augmented Reality). It operates by extracting contextual interactions from observing users performing actions. GlaciAR is able to i) automatically determine moments of relevance based on a head motion attention model, ii) automatically produce video guidance information, iii) trigger these guides based on an object detection method, iv) learn without supervision from observing multiple users and v) operate fully on-board a current eyewear computer (Google Glass). We describe the components of GlaciAR together with user evaluations on three tasks. We see this work as a first step toward scaling up the notoriously difficult authoring problem in guidance systems and an exploration of enhancing user natural abilities via minimally invasive visual cues.	augmented reality;emoticon;glass;guidance system;image scaling;interaction;mixed reality;multi-user;object detection;on-board data handling;relevance	Teesid Leelasawassuk;Dima Damen;Walterio W. Mayol-Cuevas	2017		10.1145/3041164.3041185	computer vision;augmented reality;simulation;human–computer interaction;computer science;multimedia	HCI	-38.362085732831396	-41.6273889829114	157972
24335fc30892a07f2d02c8685388ca026734b13d	an interactive design system for sphericon-based geometric toys using conical voxels	user interface;conical voxel;sphericon;geometric modeling	In this paper, we focus on a unique solid, named a “sphericon”, which has geometric properties that cause it to roll down a slope while swinging from side to side. We propose an interactive system for designing 3D objects with the same geometric characteristics as a sphericon. For high system efficiency, we used a conical voxel representation for defining these objects. The system allows the user to concentrate on the design itself while the system ensuring that the geometrical constraints of a sphericon are satisfied. The user can also preview the rolling motion of the object. To evaluate the effectiveness of the proposed system, we fabricated the designed models using a 3D printer, and confirmed that they rolled as smoothly as a standard sphericon.	3d printing;interactive design;interactivity;printer (computing);smoothing;toys;voxel	Masaki Hirose;Jun Mitani;Yoshihiro Kanamori;Yukio Fukui	2011		10.1007/978-3-642-22571-0_4	computer vision;simulation;computer science;geometric modeling;operating system;user interface;computer graphics (images)	HCI	-42.407332547151995	-39.61149282280115	157981
0ecb364d801f506cdb5210b6f9a6c5be7197e81e	leveraging contextual cues for generating basketball highlights	highlight ranking;content analysis;video highlights;basketball video	The massive growth of sports videos has resulted in a need for automatic generation of sports highlights that are comparable in quality to the hand-edited highlights produced by broadcasters such as ESPN. Unlike previous works that mostly use audio-visual cues derived from the video, we propose an approach that additionally leverages contextual cues derived from the environment that the game is being played in. The contextual cues provide information about the excitement levels in the game, which can be ranked and selected to automatically produce high-quality basketball highlights. We introduce a new dataset of 25 NCAA games along with their play-by-play stats and the ground-truth excitement data for each basket. We explore the informativeness of five different cues derived from the video and from the environment through user studies. Our experiments show that for our study participants, the highlights produced by our system are comparable to the ones produced by ESPN for the same games.	experiment;ground truth;inter-rater reliability;usability testing	Vinay Bettadapura;Caroline Pantofaru;Irfan A. Essa	2016		10.1145/2964284.2964286	simulation;content analysis;multimedia	Vision	-33.764289815405384	-48.29509473144126	158239
c39cbdf66d3fbad0273cc7683359dc9a68eddd8c	design and enhancement of painting interface for room lights	image based lighting;painting interface;interactive lighting design;article	We propose a novel painting interface that enables users to design an illumination distribution for a real room using an array of computer-controlled lights. Users specify which area of the room is to be well-lit and which is to be dark by painting a target illumination distribution on a tablet device displaying the image obtained by a camera mounted in the room. The painting result is overlaid on the camera image as contour lines of the target illumination intensity. The system then runs an optimization to calculate light parameters to deliver the requested illumination condition. We implemented a GPU-based parallel search to achieve real-time processing. In our system, we used actuated lights that can change the lighting direction to generate the requested illumination condition more faithfully than static lights. We built a miniature-scale experimental environment and ran a user study to compare our method with a standard direct manipulation method using sliders. The results showed that the users preferred our method for informal light control.	contour line;direct manipulation interface;graphics processing unit;mathematical optimization;natural language;prototype;real-time clock;real-time computing;robot;tablet computer;usability testing	Seung-tak Noh;Sunao Hashimoto;Daiki Yamanaka;Yoichi Kamiyama;Masahiko Inami;Takeo Igarashi	2013	The Visual Computer	10.1007/s00371-013-0872-7	computer vision;computer science;image-based lighting;computer graphics (images)	HCI	-43.00357092094085	-40.07334648216914	158258
845865a5df5fdb74fef24859288ab741b1ad8e35	evaluation of a visual interface for gesticulation recognition	mobile robot;hidden markov model;dynamic bayesian network;human machine interface;man machine interface;point of view;visual interfaces	The automatic recognition of gestures is a well known area, but on the other hand, the usability of these type of human-machine interfaces has not been deeply taking into account. This paper shows the results of evaluation of a man-machine interface, from two points of view: i) automatic recognition and; ii) usability from the point of view of the end-users. We choose 5 gestures focus to control a mobile robot. With 8 end-user, the interface gives a 59.46% of good results when using Dynamic Bayesian Networks, a better result than using Hidden Markov Models that obtained an average of 43.03%. This result contrasts with the 84.01% with only one end-user, showing the difficulties to extend the number of end-users to the system. The usability of the interface was tested using 6 of the 8 end-users, divided into 3 categories: expert, semi-expert and non-expert end-users, the result shows a closer perceptions between expert and not-expert end-users.	dynamic bayesian network;hidden markov model;markov chain;mobile robot;point of view (computer hardware company);semiconductor industry;usability;user interface	Victor Hugo Zárate Silva;Héctor Hugo Avilés-Arriaga	2003			computer vision;human–computer interaction;computer science;machine learning	Robotics	-37.73832244208873	-43.49331759385398	158479
d4ba9d207a1dd1345b2ca0e2d0feb856e37e47f3	haunted house: an interactive experience using a pico projector	interaction;gaming;mobile applications;augmented reality;experimentation;handheld projection systems	This paper presents Haunted House, a game where a mobile device projects the interior of a virtual haunted house over a room. The users explore the house discovering its secrets. Pico projectors are becoming popular and smartphones are starting to be designed with this functionality. When projecting we can overcome many problems of the mobile device small display, giving room to new and more interesting applications. Even when a mobile device does not have the projecting functionality, it is possible to attach a pico-projector to it. By having this technology available for everybody with a smartphone, new forms of interaction will emerge. We aim at exploring the use of pico-projectors to create interactive experiences by using an infrastructure, adaptable to a space that uses existing video camera networks to support these experiences. We intend to develop new forms of interaction and social experiences using mobile devices and pico-projectors.	mobile device;movie projector;pico;pico-itx;smartphone;video projector	Rossana Santos;Nuno Correia	2015		10.1145/2832932.2832980	simulation;engineering;multimedia;computer graphics (images)	HCI	-46.998013489328656	-39.2395281201271	158904
47608c664076c60f89a799e4e5bf3bdac6f13db8	research of autonomous active control for virtual human based on emotion-driven model	human computer interaction;smart home;emotional decision;informing science;virtual human;autonomous active control;obstacle avoidance;educational game;obstacle avoiding behavior;emotional memory;active control;finding exit mode	Information science has been widely used in many aspects, such as e-learning, educational games and smart home. In environment above, the study of virtual human can make human-computer interaction more natural. This paper associated emotional decision with virtual human's active control. It can make the virtual human in smart home generate some inner directed behaviors. Meanwhile, it came up with a mode called finding exit to solve the problem that there are a lot of ring-shaped obstacles. This mode improved the basic obstacle-avoiding behaviors. Then with the organic combination of emotion, memory and behavior, the autonomous active controlling system of virtual human in smart home is realized.	autonomous robot;virtual actor	Fenhua Wang;Xiaodan Huang;Zhiliang Wang	2009	Trans. Edutainment	10.1007/978-3-642-11245-4_16	simulation;computer science;artificial intelligence;obstacle avoidance	Graphics	-34.92615932733946	-38.860799212350344	158973
278d31b9fc7e85cda6a26c8004589da8d2cc9853	evaluation of hand gesture annotation in remote collaboration using augmented reality		In this research we have devised a system which can tell works to the worker by using the hand gestures of the helper. In this system, by using augmented reality, it is possible to display as if the helper's hand model actually exist in front of the worker. In order to evaluate the usefulness of the proposed system, we conducted comparative experiments on remote work support by instruction annotations using conventional method of drawn lines, and the proposed method of by using hand gesture instructions. As a result, no significant difference was found between two methods in terms of ease of understanding in the instructions. However, regarding working time, the hand gesture instructions were shorter by 20 seconds (shortened by 19%) on average than the other.	augmented reality;experiment;telecommuting	Shohei Yamada;Naiwala P. Chandrasiri	2018	2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)	10.1109/VR.2018.8446287	simulation;computer science;virtual reality;augmented reality;annotation;user interface;gesture	Visualization	-47.67470644394233	-44.53567558853376	159153
e29cf334cd7ffd4cfb33734c1855654357b06c7f	visualization support for managing information overload in the web environment	human computer interaction;information visualization;information overload;web search;visual system	This research focuses on the information overload problem on the Internet, and proposes a potential remedy to the overload encountered while searching the Web. We developed a system that makes use of clustering and visualization for browsing the results of a typical web search. We built two different (full and fisheye) zooming capabilities into our system, and empirically compared their success with each other as well as with the traditional non-visual presentation method through an experiment. We hypothesize that the visual systems will lead to higher success than the text-based system, and that the fisheye zooming system will lead to higher success than the full zoom system. The results of our exploratory test provide partial support for our hypotheses. This empirical support and the comments made by the participants in the experiments suggest that our design ideas are promising, and it is worthwhile to further investigate the use of clustering and visualization mechanisms for reducing information overload.	cluster analysis;experiment;exploratory testing;fisheye;information overload;preprocessor;text-based (computing);user interface design;web search engine;web search query;world wide web;zooming user interface	Ozgur Turetken;Ramesh Sharda	2001			visual analytics;information visualization;visual system;human–computer interaction;computer science;information overload;multimedia;world wide web	HCI	-34.85752516535563	-51.11060312051031	159321
0691284454bf4491ca61316cd892ad3aa6ea9e6a	design and implementation of a widget set for steerable projector-camera units	computer vision technique steerable projector camera unit graphical interaction widget visual cue ergonomics;optical projectors;computer vision;graphical user interfaces;design and implementation;visual cues;instruments cameras computer vision image resolution feedback computer science informatics ergonomics robustness ubiquitous computing;cameras interactive devices computer vision graphical user interfaces optical projectors;cameras;interactive devices	We describe the design and implementation of graphical interaction widgets for use with a steerable projectorcamera unit. The design of our widgets is adapted to provide the right visual cues when projected and they are controlled by the user’s hand. The widgets’ input regions are arranged in an ergonomic way and they use a simple but robust computer vision technique for interaction. The widget set has been implemented in our instrumented environment, and we demonstrate interactive buttons, checkboxes, sliders and selection boxes.	algorithm;computer vision;graphical user interface builder;human factors and ergonomics;pointer (computer programming);pointer (user interface);programmer;software widget;steerable filter;uncontrolled format string;video projector;wimp (computing);widget (gui);widget toolkit	Dennis Reiter;Andreas Butz	2005		10.1109/ISMAR.2005.25	computer vision;sensory cue;human–computer interaction;computer science;graphical user interface;computer graphics (images)	HCI	-43.23107598137526	-40.28196663595829	159328
3a2349664ff42214775934bfdd8e1906cdd7d0ed	bimodal speech recognition for robot applications		By the earliest motivation of building robots to take care of human being in the daily life, the researches on humanoid robots have been explored and devel- oped over the recent decades. One challenge of humanoid robots is its capability of communication with people via audio-visual speech. Certainly, the integration of audio module and visual module improve the robot performance. In this paper, we present a novel system combines these two modules in one environment to be utilized alone or in humanoid robots. The proposed system adapts the user facial movements that cannot be avoided in reality. The experimental results show how these two modules enhance each other and yield an effective speech recognizer.	robot;speech recognition	Alaa El. Sagheer;Saleh Aly;Samar Anter	2013		10.1007/978-3-319-02309-0_9	simulation;engineering;artificial intelligence;communication	Robotics	-37.37076887435518	-43.170713779653234	159356
2dfa3795eaf1b09024f60576c9de8f23d273a881	look and lean: accurate head-assisted eye pointing	pointing;head movements;gaze input;eye tracking	Compared to the mouse, eye pointing is inaccurate. As a consequence, small objects are difficult to point by gaze alone. We suggest using a combination of eye pointing and subtle head movements to achieve accurate hands-free pointing in a conventional desktop computing environment. For tracking the head movements, we exploited information of the eye position in the eye tracker's camera view. We conducted a series of three experiments to study the potential caveats and benefits of using head movements to adjust gaze cursor position. Results showed that head-assisted eye pointing significantly improves the pointing accuracy without a negative impact on the pointing time. In some cases participants were able to point almost 3 times closer to the target's center, compared to the eye pointing alone (7 vs. 19 pixels). We conclude that head assisted eye pointing is a comfortable and potentially very efficient alternative for other assisting methods in the eye pointing, such as zooming.	cursor (databases);desktop computer;experiment;eye tracking;pixel	Oleg Spakov;Poika Isokoski;Päivi Majaranta	2014		10.1145/2578153.2578157	computer vision;simulation;eye tracking;eye tracking on the iss;optics	HCI	-45.438003030663346	-46.4162664483049	159367
30036209d4007ce5b5a8f6de02533f156399e389	it's in your eyes: towards context-awareness and mobile hci using wearable eog goggles	context awareness;context aware;human computer interaction;eye gestures;data locality;real time;physical activity;signal processing;electrooculography eog;eye movement;human computer interaction hci;wearable computer;eye tracking;wearable computing;stream processing;adaptive filter;computer game	In this work we describe the design, implementation and evaluation of a novel eye tracker for context-awareness and mobile HCI applications. In contrast to common systems using video cameras, this compact device relies on Electrooculography (EOG). It consists of goggles with dry electrodes integrated into the frame and a small pocket-worn component with a DSP for real-time EOG signal processing. The device is intended for wearable and standalone use: It can store data locally for long-term recordings or stream processed EOG signals to a remote device over Bluetooth. We describe how eye gestures can be efficiently recognised from EOG signals for HCI purposes. In an experiment conducted with 11 subjects playing a computer game we show that 8 eye gestures of varying complexity can be continuously recognised with equal performance to a state-of-the-art video-based system. Physical activity leads to artefacts in the EOG signal. We describe how these artefacts can be removed using an adaptive filtering scheme and characterise this approach on a 5-subject dataset. In addition to explicit eye movements for HCI, we discuss how the analysis of unconscious eye movements may eventually allow to deduce information on user activity and context not available with current sensing modalities.	adaptive filter;bluetooth;context awareness;electrooculography;eye tracking;goggles;human–computer interaction;mobilehci;pc game;real-time clock;signal processing;wearable computer	Andreas Bulling;Daniel Roggen;Gerhard Tröster	2008		10.1145/1409635.1409647	computer vision;wearable computer;human–computer interaction;computer hardware;computer science;signal processing	HCI	-47.46297026504455	-41.719595883273215	159637
9d1f95219cc1e1e050c97ce4aaa419dd7dba8dfb	gecco: finger gesture-based command and control for touch interfaces	touch sensitive screens gesture recognition notebook computers pattern classification statistical analysis;touch sensitive screens;statistical analysis;gesture shortcuts gesture recognition touch gestures mode switching adaptation;pattern classification;notebook computers;fingers switches mice prototypes shape keyboards navigation;dtw gecco finger gesture based command and control touch based interfaces personal computing devices keyboard input mouse input touch surface media collections statistical pattern classification techniques dynamic time warping distance;gesture recognition	With touch-based interfaces becoming commonplace on personal computing devices ranging from phones and slates to notebook and desktop PCs, a number of common tasks that were once performed using mouse or keyboard input now need to be performed using fingers on the touch surface. Finger-drawn gestures offer a viable alternative to desktop and keyboard shortcuts as shortcuts for common tasks such as launching of applications and navigation of large media collections. In order to be truly effective, the interface for definition, management and invocation of gestures should be highly intuitive, and optimized for the device. In particular, the process of invoking gestures should be seamless and natural. Further, the recognition of gestures needs to be robust for the specific user. In this paper, we describe GeCCo (Gesture Command and Control), a system for personalized finger gesture shortcuts for touch-enabled desktops and trackpad-enabled notebook PCs. One of the key issues addressed in the design of GeCCo is that of mode switching in the context of notebook PCs. We describe a user study to decide between different interactions for mode switching. The interactions are designed such that mode switch and gesture can be simultaneously indicated. Since new gestures may be defined by the user at any time, statistical pattern classification techniques which require large numbers of training samples for each gesture are not useful. Instead we use nearest-neighbor classification with Dynamic Time Warping (DTW) distance, and a writer adaptation scheme for improving accuracy to desired levels. We conclude the paper with experimental results and some thoughts on next steps.	cursor (databases);desktop computer;dynamic time warping;genetic and evolutionary computation conference;gesture recognition;interaction;keyboard shortcut;mobile phone;personal computer;personalization;plover;point of sale;prototype;seamless3d;statistical classification;touchpad;touchscreen;usability testing	Sriganesh Madhvanath;Dinesh Mandalapu;Tarun Madan;Naznin Rao;Ramesh Kozhissery	2012	2012 4th International Conference on Intelligent Human Computer Interaction (IHCI)	10.1109/IHCI.2012.6481796	computer vision;speech recognition;computer science;gesture recognition;multimedia;statistics	HCI	-44.9847179843944	-43.843883117468124	159680
431a910653b877178246088fbfb530a920a6563a	a six-legged telerobot for nuclear applications development	application development;engineering;robot movil;eficacia sistema;control systems;description systeme;systeme commande;sistema control;national organizations;industrie nucleaire;system description;sistema experto;general and miscellaneous mathematics computing and information science;uses;robot robin;mobility;relacion hombre maquina;performance systeme;nuclear industry;man machine relation;technology assessment;locomotion;robotics;product model;system performance;remote operation;man machine system;mathematics computers;savannah river plant;mobility control;control system;industry;research and development;industria nuclear;robot mobile;us organizations 420203 engineering handling equipment procedures;system design;teleaccion;research programs;us aec;industrial robots;robots;us erda;robotica;design;evaluation;descripcion sistema;us doe;robotique;relation homme machine;electric power;systeme expert;locomocion;teleaction;man machine systems;moving robot;expert system	A six-legged telerobot was evaluated for nuclear applications at the Savannah River Laboratory. Enhancements were added to the man-machine control interface to improve the efficiency and productivity of operations. Although this system was a prototype for laboratory research and development and was not intended for operation in a nuclear environment, the work demonstrated the feasibility of sophisticated walking robots in nuclear service. The Savannah River system has served as a valuable prototype forerunner to a production model telerobot that is now under development by Odetics Incorporated for the Electric Power Research Institute.	telerobotics	Joseph S. Byrd;Kevin R. DeVries	1990	I. J. Robotics Res.	10.1177/027836499000900204	design;simulation;electric power;computer science;engineering;control system;artificial intelligence;evaluation;technology assessment	Robotics	-38.91116628147493	-47.866740651277624	159920
