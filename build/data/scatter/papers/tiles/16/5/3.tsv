id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
4f784527f33110c5d8a292e9e87c8c6beabe9567	evaluating geographic information retrieval	geographic information retrieval;evaluation model	In GIR, as a research field branching from IR, initial evaluation initiatives were unsurprisingly inspired from established IR evaluation models. The GeoCLEF evaluation [6], which I will look into in some detail below, follows such a model. While this model suits the goal of a generic evaluation for all types of GIR systems, other evaluation initiatives forked so that particularly geographic challenges of the task could be more thoroughly focused on. NTCIR [7], with its GeoTime task, looked to include time expressions and had a topic set that resembles more questions that stir up the focus from retrieval, and more on reasoning.	fork (software development);geotime;information retrieval;regular expression	Nuno Cardoso	2011	SIGSPATIAL Special	10.1145/2047296.2047307	computer science;knowledge management;data mining;database;world wide web;information retrieval	Web+IR	-41.22033054403251	-61.49870109648401	88271
58242423568bd4f902036fe590cd20928a000cb1	relevance in the eye of the search software	databases;busqueda informacion;clasificacion por relevanciaa;search engine;buscador;base donnee;search engines;information retrieval;new record;database;pertinencia;base dato;relevance ranking;large scale;recherche information;pertinence;computer software;relevance;moteur recherche;classement par pertinence;relevance ordering;design methodology	Purpose – The purpose of this article is to look into relevance ranking and its importance in trying to bring some order to the deluge of results in response to a query.Design/methodology/approach – A large‐scale analysis of detailed web logs of various search engines was performed. Sample tests were made on five to eight versions of MEDLINE, ERIC, and PsycINFO on hosts which have comparable versions of the databases and offer relevance ranking.Findings – It was found that, for fairness, it must be ensured that the implementations are identical, they have the same retrospective coverage, the same MEDLINE/PubMed subsets, and (quasi) identical update.Research limitations/implications – The tests were made early September 2005. As databases are updated at different times, perfect synchronicity is not easy to achieve. When new records are added to the database, they may change the ranking of the test result set. Similarly, a small change in the fine‐tuning of the algorithm may yield different rank order posit...	relevance	Péter Jacsó	2005	Online Information Review	10.1108/14684520510638106	computer science;data mining;database;world wide web;information retrieval;search engine	SE	-37.60678330104425	-59.77493424888158	88709
37849b01d9460be79b1585f3f792a60fe7bb14dc	validation of a tuning method for haptic shared control using neuromuscular system analysis	haptic support neuromuscular system analysis neuromuscular analysis based tuning procedure haptic shared control system operator workload heuristic tuning method human arm stiffness neuromuscular property of concern reflex strength simulated haptic collision avoidance system unmanned aircraft teleoperation relax task tuning;collision avoidance tuning haptic shared control neuromuscular admittance haptic human machine interface force feedback support system human centered design unmanned aerial vehicle uav;haptic interfaces tuning neuromuscular admittance automation human factors collision avoidance;telerobotics aircraft control autonomous aerial vehicles collision avoidance control engineering computing haptic interfaces neuromuscular stimulation	This research investigates a neuromuscular analysis based tuning procedure for haptic shared control systems that has been hypothesized to improve subjective operator workload when compared to heuristic tuning methods. Here, the tuning procedure takes into consideration the response of the neuromuscular system to haptic cues. Human arm stiffness, the neuromuscular property of concern, can be changed by modulating reflex strength. The `relax task' setting of the neuromuscular system, for which reflexes are minimized, is chosen as the design point for tuning haptic cues as it is hypothesized to lead to the lowest workload. A simulated haptic collision avoidance system for unmanned aircraft teleoperation is used as a platform to experimentally validate the tuning method. The results show that the novel tuning procedure, particularly for relax task tuning, substantially improves workload and situational awareness over conditions that ignores the neuromuscular system. Additionally, over-tuning, which frequently occurs for heuristic methods, leads to worse user acceptance than a condition without haptic support.	control system;disk controller;experience;experiment;haptic technology;heuristic;lateral thinking;memory controller;performance tuning;regular language description for xml;system analysis;unmanned aerial vehicle	Emmanuel Sunil;Jan Smisek;René van Paassen;Max Mulder	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974128	embedded system;simulation	Robotics	-45.54684816841616	-54.1517444212292	88777
1f5616b44651858e965c5452c7d1a08a1ec24f9f	a multiagent-based peer-to-peer network in java for distributed spam filtering	money;filtering;filtrage;fiabilidad;reliability;multiagent system;text;electronic mail;peer to peer network;sorting;filtrado;langage java;monnaie;correo electronico;texte;tria;classification;courrier electronique;moneda;internet;spam filtering;filter;fiabilite;triage;coste;filtre;lenguaje java;publicidad;sistema multiagente;publicite;texto;filtro;clasificacion;systeme multiagent;java language;advertising;cout	With the growing amount of internet users, a negative form of sending email spreads that affects more and more users of email accounts: Spamming. Spamming means that the electronic mailbox is congested with unwanted advertising or personal email. Sorting out this email costs the user time and money. This paper introduces a distributed spam filter, which combines an off-the-shelf text classification with multiagent systems. Both the text classification as well as the multiagent platform are implemented in Java. The content of the emails is analyzed by the classification algorithm 'support vector machines'. Information about spam is exchanged between the agents through the network. Identification numbers for emails which where identified as spam are generated and forwarded to all other agents connected to the network. These numbers allow agents to identify incoming spam email. In this way, the quality of the filter increases continuously.	agent-based model;anti-spam techniques;java	Jörg Metzger;Michael Schillo;Klaus Fischer	2003		10.1007/3-540-45023-8_59	filter;email bomb;the internet;html email;input/output;biological classification;filter;computer science;sorting;artificial intelligence;money;spamming;operating system;spam and open relay blocking system;feedback loop;reliability;email spoofing;database;distributed computing;email address harvesting;internet privacy;world wide web;computer security;email authentication	Theory	-35.802339944216484	-56.787928119476895	89197
53e496ebfb9248599e5dad6ce98da0be220d431c	the effect of user characteristics on search effectiveness in information retrieval	search effectiveness;information retrieval;user study;information retrieval evaluation;user characteristics;cognitive skills;test collection	This paper investigates the influence of user characteristics (e.g. search experience and cognitive skills) on user effectiveness. A user study was conducted to investigate this effect, 56 participants completed searches for 56 topics using the TREC test collection. Results indicated that participants with search experience and high cognitive skills were more effective than those with less experience and slower perceptual abilities. However, all users rated themselves with the same level of satisfaction with the search results despite the fact they varied substantially in their effectiveness. Therefore, information retrieval evaluators should take these factors into consideration when investigating the impact of system effectiveness on user effectiveness.	information retrieval;text retrieval conference;usability testing	Azzah Al-Maskari;Mark Sanderson	2011	Inf. Process. Manage.	10.1016/j.ipm.2011.03.002	cognitive skill;computer user satisfaction;cognitive models of information retrieval;computer science;multimedia;world wide web;information retrieval;human–computer information retrieval	Web+IR	-36.18131877685085	-52.69952124441332	89446
f52d96a0451c3d40fd5f92eaed4ab11c54481b56	evaluation of advanced multi-modal command and control communication management suite	mental workload;multi modal display design;command and control;operational research	Command and Control (C2) operators function in communication intensive environments that impose a high degree of workload on them, thus resulting in failures of detection or comprehension of messages. To combat these issues, researchers at the Air Force Research Laboratory have developed an advanced network-centric communication management suite that aids C2 operators in their mission called Multi-Modal Communication (MMC). This system provides operators with the tools to manage communication in a single, intuitive, dynamic display that reduces perceived mental workload and aids in decision making and situation awareness. This study set out to evaluate the MMC tool as a communication management suite, which affords participants the ability to detect as well as comprehend the presentation of multiple critical messages. The use of the MMC tool resulted in more detections of critical messages and greater message comprehension, while also lowering ratings of perceived mental workload as compared to traditional communication tools such as radio and chat.		Victor S. Finomore;Adam Sitz;Kelly Satterfield;Courtney Castle;Elizabeth Blair	2013		10.1007/978-3-642-39360-0_24	real-time computing;simulation;computer science;computer security	Robotics	-47.788507137348766	-53.70111136383347	89542
0ba6ccd70a8a0ae6f61e1015d33340c83b2a55f0	experiments on cross–linguality and question–type driven strategy selection for open–domain qa	lenguaje natural;linguistique;repondeur;validacion;langage naturel;analyse temporelle;analisis temporal;time analysis;linguistica;natural language;comportement utilisateur;responder;validation;user behavior;information system;multilinguisme;systeme information;multilingualism;comportamiento usuario;contestador;multilinguismo;sistema informacion;linguistics	We describe the extensions made to our 2004 QA@CLEF German/English QA-system, toward a fully German-English/EnglishGerman cross-language system with answer validation through web usage. Details concerning the processing of factoid, definition and temporal questions are given and the results obtained in the monolingual German, bilingual English-German and German-English tasks are briefly presented and discussed.	experiment;software quality assurance	Günter Neumann;Bogdan Sacaleanu	2005		10.1007/11878773_48	natural language processing;artificial intelligence;linguistics;natural language;information system	AI	-35.843942338885356	-63.1067522370143	89731
80f7380ceeda17cd5760fda5d906900311f90731	peer-review under review - a statistical study on proposal ranking at eso. part i: the pre-meeting phase		Peer review is the most common mechanism in place for assessing requests for resources in a large variety of scientific disciplines. One of the strongest criticisms to this paradigm is the limited reproducibility of the process, especially at largely oversubscribed facilities. In this and in a subsequent paper we address this specific aspect in a quantitative way, through a statistical study on proposal ranking at the European Southern Observatory. For this purpose we analysed a sample of about 15000 proposals, submitted by more than 3000 Principal Investigators over 8 years. The proposals were reviewed by more than 500 referees, who assigned over 140000 grades in about 200 panel sessions. After providing a detailed analysis of the statistical properties of the sample, the paper presents an heuristic model based on these findings, which is then used to provide quantitative estimates of the reproducibility of the pre-meeting process. On average, about one third of the proposals ranked in the top quartile by one referee are ranked in the same quartile by any other referee of the panel. A similar value is observed for the bottom quartile. In the central quartiles, the agreement fractions are very marginally above the value expected for a fully aleatory process (25%). The agreement fraction between two panels composed by 6 referees is 55±5% (50% confidence level) for the top and bottom quartiles. The corresponding fraction for the central quartiles is 33±5%. The model predictions are confirmed by the results obtained from boot-strapping the data for sub-panels composed by 3 referees, and fully consistent with the NIPS experiment. The post-meeting phase will be presented and discussed in a forthcoming paper.		Ferdinando Patat	2018	CoRR	10.1088/1538-3873/aac463		ML	-42.81697168300261	-64.54002150800058	89873
d9859e0871c343ab843e7469184184476af114c1	what is wrong with your gesture? an error-based assistance for gesture training in virtual environments	3d interaction;gesture;training;virtual environments;gesture 3d interaction immersion training;training three dimensional displays time series analysis virtual environments cleaning aluminum foundries;aluminum;immersion;time series analysis;three dimensional displays;aluminium foundry error based feedback gesture training 3d virtual environment;virtual reality aluminium manufacture force feedback foundries gesture recognition production engineering computing;foundries;cleaning	Virtual environments are commonly used to train people, especially for gesture training. A major challenge consists in error perception. In this paper, we propose to guide the user gesture with an error-based feedback in a 3D virtual environment. In a preliminary experiment, trainees reproduce an expert gesture from an aluminium foundry. Their gesture performance is evaluated according to two critical error criteria to improve the interaction for training.	fatal exception error;virtual reality	Florian Jeanne;Yann Soullard;Indira Thouvenin	2016	2016 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2016.7460063	simulation;engineering;gesture recognition;multimedia;communication	Visualization	-48.153027591988746	-54.78796358435259	90497
8f2467ae8d16d47c91d0ae25799cfb61c20b6840	information retrieval based on statistical language models	extraction information;dato;extraction connaissance;information extraction;reconocimiento palabra;information retrieval;language technology;data;knowledge extraction;data mining;statistical model;topic detection and tracking;donnee;recherche information;excavacion;modele statistique;speech recognition;modelo estadistico;recuperacion informacion;reconnaissance parole;information system;statistical language model;systeme information;language model;machine translation;excavation;sistema informacion;extraction informacion;fouille	The amount of on-line information is growing exponentially. Much of this information is unstructured and language-based. To deal with this flood of information, a number of tools and language technologies have been developed. Progress has been made in areas such as information retrieval, information extraction, filtering, speech recognition, machine translation, and data mining. Other more specific areas such as cross-lingual retrieval, summarization, categorization, distributed retrieval, and topic detection and tracking are also contributing to the proliferation of technologies for managing information. Currently these tools are based on many different approaches, both formal and ad hoc. Integrating them is very diffcult, yet this will be a critical part of building effective information systems in the future. In this paper, we discuss an approach to providing a framework for integration based on language models.	information retrieval;language model	W. Bruce Croft	2000		10.1007/3-540-39963-1_1	natural language processing;statistical model;excavation;question answering;relevance;cognitive models of information retrieval;computer science;artificial intelligence;information integration;information filtering system;concept search;data mining;database;adversarial information retrieval;machine translation;knowledge extraction;language technology;information extraction;information retrieval;information system;data;language model;human–computer information retrieval	NLP	-35.826611835134706	-63.54444797069876	90498
c289837593c4e03d57edca086ea9be913683650a	experimental induction and measurement of negative affect induced by interacting with in-vehicle information systems		The goal of this study was to investigate whether it is possible to induce negative affects in the interaction with IVIS in a driving simulator environment and whether this would be reflected in the driver’s facial expressions. N = 29 participants completed a 30-min-drive in a high-fidelity driving simulator performing several IVIS tasks using both a visual multi-level menu and a simulated speech recognition navigation system. During the drive, negative affects were induced by increasing the complexity of the menu tasks and decreasing the recognition rate of the speech system. The results show that experiencing difficulties in solving IVIS tasks went along with self-reported negative affect such as frustration and anger. A newly developed observation protocol adapted from the Facial Action Coding System (FACS) used for the ratings of negative emotions based on the video footage of the drivers’ face during the simulator trials was established. The method revealed significant correlations with the self-reported measures of negative emotions. However, there were no correlations between the observed facial expressions and the sub-scales of the State-Trait-Anger-Inventory (STAXI). The study shows the importance of a well-designed IVIS in order to prevent negative emotions that might result in non-acceptance of new technologies. The study also shows the potential of facial recognition technology to provide assistance or tutoring functions that could relieve the driver from emotional discomfort.	information system	Nadja Schömig;Frederik Naujoks;Thomas Hammer;Markus Tomzig;Bastian Hinterleitner;Stefan Mayer	2018		10.1007/978-3-319-91238-7_36	human–computer interaction;information system;navigation system;facial recognition system;affect (psychology);facial expression;computer science;facial action coding system;frustration;driving simulator	ECom	-47.10271865940698	-52.75867075339759	90840
42e84c4c1ec0bed9c1436ff1b5d6ecfe07981615	state-of-the-art in string similarity search and join	string join;comparison;string search;qa75 please use qa76 electronic computers computer science;scalability;computer science	String similarity search and its variants are fundamental problems with many applications in areas such as data integration, data quality, computational linguistics, or bioinformatics. A plethora of methods have been developed over the last decades. Obtaining an overview of the state-of-the-art in this field is difficult, as results are published in various domains without much cross-talk, papers use different data sets and often study subtle variations of the core problems, and the sheer number of proposed methods exceeds the capacity of a single research group. In this paper, we report on the results of the probably largest benchmark ever performed in this field. To overcome the resource bottleneck, we organized the benchmark as an international competition, a workshop at EDBT/ICDT 2013. Various teams from different fields and from all over the world developed or tuned programs for two crisply defined problems. All algorithms were evaluated by an external group on two machines. Altogether, we compared 14 different programs on two string matching problems (k-approximate search and k-approximate join) using data sets of increasing sizes and with different characteristics from two different domains. We compare programs primarily by wall clock time, but also provide results on memory usage, indexing time, batch query effects and scalability in terms of CPU cores. Results were averaged over several runs and confirmed on a second, different hardware platform. A particularly interesting observation is that disciplines can and should learn more from each other, with the three best teams rooting in computational linguistics, databases, and bioinformatics, respectively.	approximation algorithm;benchmark (computing);bioinformatics;central processing unit;computational linguistics;crosstalk;data quality;database;scalability;similarity search;string metric;string searching algorithm	Sebastian Wandelt;Dong Deng;Stefan Gerdjikov;Shashwat Mishra;Petar Mitankin;Manish Patil;Enrico Siragusa;Alexander Tiskin;Wei Wang;Jiaying Wang;Ulf Leser	2014	SIGMOD Record	10.1145/2627692.2627706	scalability;computer science;theoretical computer science;data mining;database;programming language;algorithm;string searching algorithm	DB	-41.072582780140216	-62.79444934000252	90977
2ba5b102da7e90549e6f419b4a0c8b7dcac25544	effect of strategic accommodation training by wide stereoscopic movie presentation on myopic young people of visual acuity and asthenopia	visual acuity;control group;visual communication;power 3d;visual analog scale;ciliary muscle;asthenopia;stereoscopic movie;accommodative function;accommodation training;young people	Please cite this article in press as: A. Sugiura et people of visual acuity and asthenopia, Display The abnormal contraction of ciliary muscles due to the performance of a near visual task for several hours causes various vision problems such as asthenopia and visual loss. However, these problems can be resolved by activating the muscles by alternately repeating negative and positive accommodation. In this study, we have verified the effect of accommodation training that uses the strategy of presenting a stereoscopic movie to myopic youth and measuring the uncorrected distant visual acuity, spherical diopter (SPH), a flicker test and subjective index of asthenopia obtained using a visual analog scale (VAS). Stereoscopic movies are prepared by using the POWER 3D method (Olympus Visual Communications Co., Ltd.), which reduces the inconsistency between the experienced and the actual senses. Thirty-two myopic students aged 20 ± 1 years (16 males and 16 females) were chosen as the subjects. One group performed the accommodation training for 6 min, and the other group underwent a near visual task during the same period as the control group. We concluded that the accommodation training using a stereoscopic movie had temporarily improved visual acuity. This training seemed to lead to a decrease in asthenopia. 2011 Elsevier B.V. All rights reserved.	3d film;binocular vision;emoticon;flicker (screen);maxima and minima;smoothed-particle hydrodynamics;spherical basis;stereoscopy;visual basic[.net]	Akihiro Sugiura;Masaru Miyao;Tetsuya Yamamoto;Hiroki Takada	2011	Displays	10.1016/j.displa.2011.04.001	computer vision;simulation;visual analogue scale;optics;scientific control;visual communication	Vision	-43.33203271222455	-53.25578929635187	91127
e24d09890a53bd8b90516b2659c93d15101fb7c1	a preference scoring technique for personalized advertisements on internet storefronts	commerce electronique;preference theory;filtering;filtrage;metodo analisis;electronic commerce;matematicas aplicadas;web pages;modele mathematique;mathematiques appliquees;learning;analisis datos;storage structure;rule based;filtrado;customization;personnalisation;stockage donnee;tabla dato;modelo matematico;personalization;aprendizaje;data analysis;data storage;methode analyse;apprentissage;internet;internet storefront;table donnee;collaborative filtering;analysis method;theorie preference;vitrine virtuelle;mathematical model;almacenamiento datos;analyse donnee;recommendation techniques;estructura memoria;teoria preferencia;structure memoire;data table;applied mathematics;techniques de recommandation	This paper describes a new personalized advertisement selection technique based on a customer's preference scores for product categories. This method performs well, despite having low data and analysis requirements, relative to other methods in use. Customer preference scores are updated based on a customer's initial profile, purchase history, and behavior in an Internet storefront, and are then used to select and display appropriate advertisements on Internet web pages when the customer visits the Internet storefront. Compared with currently available recommendation techniques such as collaborative filtering or rule-based methods, preference scoring techniques use only a single customer's data to select appropriate advertisements and do not require a learning data set, and yet have competitive performance and can reflect changes in a customers' preference. An experiment is performed to compare two alternative data storage structures, the preference table and the preference tree, with random selection and collaborative filtering.	personalized marketing	Jong Woo Kim;Kyung Mi Lee;Michael J. Shaw;Hsin-Lu Chang;Matthew L. Nelson;Robert F. Easley	2006	Mathematical and Computer Modelling	10.1016/j.mcm.2004.12.011	e-commerce;rule-based system;preference learning;computer science;data mining;personalization;world wide web;statistics	ECom	-36.49876443184093	-57.15478823891957	91150
9fff930edf1451f47e2e64cb6f660638b546c72b	field-weighted xml retrieval based on bm25	busqueda informacion;modelizacion;sistema interactivo;experimental method;first year;information retrieval;xml language;xml retrieval;indexing and retrieval;systeme conversationnel;modelisation;recherche documentaire;indexing;interactive system;recherche information;busqueda documental;indexation;indizacion;document retrieval;modeling;z665 library science information science;langage xml;lenguaje xml	This is the first year for the Centre for Interactive Systems Research participation of INEX. Based on a newly developed XML indexing and retrieval system on Okapi, we extend Robertson’s field-weighted BM25F for document retrieval to element level retrieval function BM25E. In this paper, we introduce this new function and our experimental method in detail, and then show how we tuned weights for our selected fields by using INEX 2004 topics and assessments. Based on the tuned models we submitted our runs for CO.Thorough, CO.FetchBrowse, the methods we propose show real promise. Existing problems and future work are also discussed.	algorithm;database index;document retrieval;experiment;information retrieval;interactivity;xml retrieval	Wei Lu;Stephen E. Robertson;Andrew MacFarlane	2005		10.1007/978-3-540-34963-1_12	computer science;database;okapi bm25;world wide web;information retrieval	Web+IR	-35.09781618390712	-62.15644916766021	91296
03ba837a170df377d350b9becab32a16a6a32557	a multi-layered summarization system for multi-media archives by understanding and structuring of chinese spoken documents	broadcast news;modelizacion;lenguaje natural;multidisciplinaire;linguistique;navegacion informacion;multimedia;generacion automatica;navigation information;speech processing;langage naturel;interrogation base donnee;information browsing;tratamiento palabra;interrogacion base datos;traitement parole;semantics;resumen;chino;probabilistic approach;spoken document retrieval;archive;semantica;semantique;automatic generation;user assistance;modelisation;mandarin chinese;linguistica;generation automatique;assistance utilisateur;archivo;resume;enfoque probabilista;approche probabiliste;natural language;asistencia usuario;multidisciplinary;multidisciplinar;analisis semantico;analyse semantique;chinois;chinese;abstract;modeling;database query;probabilistic latent semantic analysis;semantic analysis;linguistics	The multi-media archives are very difficult to be shown on the screen, and very difficult to retrieve and browse. It is therefore important to develop technologies to summarize the entire archives in the network content to help the user in browsing and retrieval. In a recent paper [1] we proposed a complete set of multi-layered technologies to handle at least some of the above issues: (1) Automatic Generation of Titles and Summaries for each of the spoken documents, such that the spoken documents become much more easier to browse, (2) Global Semantic Structuring of the entire spoken document archive, offering to the user a global picture of the semantic structure of the archive, and (3) Query-based Local Semantic Structuring for the subset of the spoken documents retrieved by the user’s query, providing the user the detailed semantic structure of the relevant spoken documents given the query he entered. The Probabilistic Latent Semantic Analysis (PLSA) is found to be helpful. This paper presents an initial prototype system for Chinese archives with the functions mentioned above, in which the broadcast news archive in Mandarin Chinese is taken as the example archive.	archive;browsing;probabilistic latent semantic analysis;prototype;super robot monkey team hyperforce go!	Lin-Shan Lee;Sheng-yi Kong;Yi-Cheng Pan;Yi-Sheng Fu;Yu-tsun Huang;Chien-Chih Wang	2006		10.1007/11939993_69	natural language processing;speech recognition;mandarin chinese;computer science;speech processing;database;semantics;linguistics;natural language;probabilistic latent semantic analysis;world wide web;chinese	Web+IR	-35.60347444782844	-63.39765408846246	91333
8fa336307bd4a4f80337469e5826c0b04161a125	relevance assessment: are judges exchangeable and does it matter	test collection relevance judgements;inter rater agreement;ucl;discovery;theses;conference proceedings;gold standard;system performance;digital web resources;ucl discovery;task analysis;open access;ucl library;book chapters;open access repository;information seeking;test collection;ucl research	"""We investigate to what extent people making relevance judgements for a reusable IR test collection are exchangeable. We consider three classes of judge: """"gold standard"""" judges, who are topic originators and are experts in a particular information seeking task; """"silver standard"""" judges, who are task experts but did not create topics; and """"bronze standard"""" judges, who are those who did not define topics and are not experts in the task.  Analysis shows low levels of agreement in relevance judgements between these three groups. We report on experiments to determine if this is sufficient to invalidate the use of a test collection for measuring system performance when relevance assessments have been created by silver standard or bronze standard judges. We find that both system scores and system rankings are subject to consistent but small differences across the three assessment sets. It appears that test collections are not completely robust to changes of judge when these judges vary widely in task and topic expertise. Bronze standard judges may not be able to substitute for topic and task experts, due to changes in the relative performance of assessed systems, and gold standard judges are preferred."""	experiment;information seeking;relevance;task manager;turing test	Peter Bailey;Nick Craswell;Ian Soboroff;Paul Thomas;Arjen P. de Vries;Emine Yılmaz	2008		10.1145/1390334.1390447	gold standard;computer science;data mining;task analysis;computer performance;inter-rater reliability;operations research;world wide web;information retrieval;statistics	Web+IR	-38.28048680762444	-61.57026639056085	91621
d6417b42322a36549bf4358b37b3ce4c7c210bf3	ariex: automated ranking of information extractors		Information extractors are used to transform the user-friendly information in a web document into structured information that can be used to feed a knowledge-based system. Researchers are interested in ranking them to find out which one performs the best. Unfortunately, many rankings in the literature are deficient. There are a number of formal methods to rank information extractors, but they also have many problems and have not reached widespread popularity. In this article, we present ARIEX, which is an automated method to rank web information extraction proposals. It does not have any of the problems that we have identified in the literature. Our proposal shall definitely help authors make sure that they have advanced the state of the art not only conceptually, but from an empirical point of view; it shall also help practitioners make informed decisions on which proposal is the most adequate for a particular problem.		Patricia Jiménez;Rafael Corchuelo;Hassan A. Sleiman	2016	Knowl.-Based Syst.	10.1016/j.knosys.2015.11.004	computer science;data science;machine learning;data mining;information retrieval	DB	-40.77547373085788	-64.2198068319008	91643
61cba457197d14f9fbf4e795b543a4ee6a1fb7fc	seeking and implementing automated assistance during the search process	busqueda informacion;interactive information retrieval;adaptive interfaces;interfase usuario;empirical study;systeme intelligent;analisis estadistico;intelligent information retrieval systems;information retrieval system;user interface;information retrieval;implicit feedback;implementation;analisis forma;sistema inteligente;systeme recherche;user assistance;search system;adaptive interface;retroaccion;assistance utilisateur;statistical analysis;retroaction;sistema investigacion;recherche information;system design;automated assistance;intelligent information retrieval;asistencia usuario;analyse statistique;intelligent system;feedback regulation;explanation systems;interface utilisateur;contextual help;pattern analysis;information system;implementacion;systeme information;analyse forme;sistema informacion	Searchers seldom make use of the advanced searching features that could improve the quality of the search process because they do not know these features exist, do not understand how to use them, or do not believe they are effective or efficient. Information retrieval systems offering automated assistance could greatly improve search effectiveness by suggesting or implementing assistance automatically. A critical issue in designing such systems is determining when the system should intervene in the search process. In this paper, we report the results of an empirical study analyzing when during the search process users seek automated searching assistance from the system and when they implement the assistance. We designed a fully functional, automated assistance application and conducted a study with 30 subjects interacting with the system. The study used a 2G TREC document collection and TREC topics. Approximately 50% of the subjects sought assistance, and over 80% of those implemented that assistance. Results from the evaluation indicate that users are willing to accept automated assistance during the search process, especially after viewing results and locating relevant documents. We discuss implications for interactive information retrieval system design and directions for future research. 2004 Elsevier Ltd. All rights reserved.	algorithm;application programming interface;archive;assistive technology;ibm pc compatible;information retrieval;interaction;internet explorer;internet information services;jones calculus;meadow;microsoft windows;operating system;sensor;system integration;systems design;text retrieval conference;world wide web;wrapper library	Bernard J. Jansen	2005	Inf. Process. Manage.	10.1016/j.ipm.2004.04.017	simulation;computer science;data mining;user interface;empirical research;implementation;world wide web;information retrieval;information system;systems design	Web+IR	-37.23753541223215	-59.28387827113116	92456
c7856e18142eaca08b474f73f100d9f122c0f731	age-related differences in the content of search queries when reformulating	reformulation strategies;aging;information search;task difficulty	This study investigated the change in the content of the queries when performing reformulations in relation to age and task difficulty. Results showed that both generalization and specialization strategies were applied significantly more often for difficult tasks compared to simple tasks. Young participants were found to use specialization strategy significantly more often than old participants. Generalization strategy was also used significantly more often by young participants, especially for difficult tasks. Young participants were found to reformulate much longer than old participants. The semantic relevance of queries with the target information was found to be significantly higher for difficult tasks compared to simple tasks. It showed a decreasing trend across reformulations for old participants and remained constant for young participants, indicating that as old participants reformulated, they produced queries that were further away from the target information. Implications of these findings for design of information search systems are discussed.	partial template specialization;relevance	Saraschandra Karanam;Herre van Oostendorp	2016		10.1145/2858036.2858444	ageing;computer science;machine learning;data mining	HCI	-35.15262667503697	-53.0809890636901	92464
1ae678e0b51a8a7706b47225bfc62abf4e2ef1f4	recognizing ontology-applicable multiple-record web documents	web documents;ontologie;decision tree;red www;heuristic method;reseau web;base connaissance;metodo heuristico;arbol decision;resolucion problema;expected value;internet;machine learning;document web;world wide web;base conocimiento;methode heuristique;ontology;arbre decision;problem solving;resolution probleme;knowledge base	Automatically recognizing which Web documents are “of interest” for some specified application is non-trivial. As a step toward solving this problem, we propose a technique for recognizing which multiple-record Web documents apply to an ontologically specified application. Given the values and kinds of values recognized by an ontological specification in an unstructured Web document, we apply three heuristics: (1) a density heuristic that measures the percent of the document that appears to apply to an application ontology, (2) an expected-value heuristic that compares the number and kind of values found in a document to the number and kind expected by the application ontology, and (3) a grouping heuristic that considers whether the values of the document appear to be grouped as application-ontology records. Then, based on machine-learned rules over these heuristic measurements, we determine whether a Web document is applicable for a given ontology. Our experimental results show that we have been able to achieve over 90% for both recall and precision, with an Fmeasure of about 95%.	heuristic (computer science);precision and recall;web page	David W. Embley;Yiu-Kai Ng;Li Xu	2001		10.1007/3-540-45581-7_41	knowledge base;the internet;computer science;artificial intelligence;machine learning;decision tree;ontology;data mining;database;world wide web;algorithm;expected value	Web+IR	-35.34846960205564	-59.951654833577585	92542
e2e5f0d77122edafa6851192ee1d03bf5497547e	summary in context: searching versus browsing	natural language processing;summarization;search engine	The use of text summaries in information-seeking research has focused on query-based summaries. Extracting content that resembles the query alone, however, ignores the greater context of the document. Such context may be central to the purpose and meaning of the document. We developed a generic, a query-based, and a hybrid summarizer, each with differing amounts of document context. The generic summarizer used a blend of discourse information and information obtained through traditional surface-level analysis. The query-based summarizer used only query-term information, and the hybrid summarizer used some discourse information along with query-term information. The validity of the generic summarizer was shown through an intrinsic evaluation using a well-established corpus of human-generated summaries. All three summarizers were then compared in an information-seeking experiment involving 297 subjects. Results from the information-seeking experiment showed that the generic summaries outperformed all others in the browse tasks, while the query-based and hybrid summaries outperformed the generic summary in the search tasks. Thus, the document context of generic summaries helped users browse, while such context was not helpful in search tasks. Such results are interesting given that generic summaries have not been studied in search tasks and the that majority of Internet search engines rely solely on query-based summaries.		Daniel McDonald;Hsinchun Chen	2006	ACM Trans. Inf. Syst.	10.1145/1125861	computer science;automatic summarization;data mining;database;world wide web;information retrieval;search engine	Security	-34.36173172869392	-54.69510210013618	92997
59ea1d8a7cc54be3b19b97845b25b563330d7c41	crowdsourcing the indexing of film and television media	film and television;moving images;media;visualization;indexing;video;crowdsourcing	In this paper we describe a project that explores how advances in information technology could be used to make film and television media more accessible to both scholarly and non-scholarly audiences. By indexing, at a detailed level, a range of time-synchronized and non-timesynchronized elements in a test collection of 12 films and 8 television programs, we demonstrate how structured data representing many aspects of media content can be produced in a streamlined manner, and discuss how this work could potentially be augmented with automated indexing to be more efficient. We present examples of how this data can be utilized to produce a variety of tools and artifacts that make film and television media more accessible, and suggest that crowdsourcing could be an effective strategy for accomplishing this work on a larger scale. This research contributes to the growing body of literature exploring how multimedia collections can be made more accessible and useful for a variety of purposes.		Gary Geisler;Geoff Willard;Eryn Whitworth	2010		10.1002/meet.14504701244	search engine indexing;media;video;visualization;computer science;multimedia;internet privacy;world wide web;crowdsourcing;information retrieval	HCI	-35.91233861442139	-55.22718681328289	93275
8446733178b7e7709105eddfd784bd9d8e524f1b	the eye as the window of the language ability: estimation of english skills by analyzing eye movement while reading documents	natural language processing behavioural sciences computing document handling gaze tracking;language ability reading life log english language skill english documents eye movement information skillful readers fixation duration english standardized test toeic	Reading-life log is a research field of analyzing our activities of reading documents to know more about readers and documents. In this paper we propose an implementation of reading-life log which is to estimate the English language skill by analyzing the activities of reading English documents. As input for the analysis, we employ eye movement information, because we consider the eye movement of skillful readers is far different from that of novices. From the experiments, we have found that the following two features are informative: (1) the sum of fixation duration, and (2) the sum of the velocity of saccades. By using these features the proposed method is to estimate the class of English skill from among low, middle and high, which are defined based on the scores of English standardized test called TOEIC. From the experimental results with 11 subjects and 10 documents, we have been successful to estimate the class with the accuracy of 90.9%.	experiment;information;velocity (software development)	Kazuyo Yoshimura;Koichi Kise;Kai Kunze	2015	2015 13th International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2015.7333762	natural language processing;speech recognition;computer science	Robotics	-44.49536790704436	-58.71364924036685	93369
da9e012fb3b631ae261bcb2c0c5a9e944e3f7741	hanzi grid	ontological knowledge resource;chinese characters	The long-term historical development and broad geographical variation of Chinese character (Hanzi/Kanji) has made it a crosscultural information sharing platform in East Asia. In this paper, we propose a theoretical framework for the knowledge representation of Hanzi in the cross-cultural context. Our proposal is mainly based on two resources: Hantology and Generative Lexicon Theory. Hantology is a comprehensive Chinese character-based knowledge resource created to provide a solid foundation both for philological surveys and language processing tasks, while Generative lexicon theory is extended to catch the abundant knowledge information of Chinese characters within its proposed qualia structure. We believe that the proposed theoretical framework will have great influence on the current research paradigm of Hanzi studies, and help to shape an emergent model of intercultural collaboration.	cjk characters;computational linguistics;conceptual system;conceptualization (information science);encode;embedded system;emergence;generative lexicon;knowledge representation and reasoning;knowledge-based systems;programming paradigm;shallow parsing;text-based (computing);wordnet	Ya-Min Chou;Shu-Kai Hsieh;Chu-Ren Huang	2007		10.1007/978-3-540-74000-1_10		AI	-43.71928338549332	-63.02681593508031	93885
c82efee8c869691407e7bbbb7bdc601e635212ec	a comparison of melodic segmentation techniques for music information retrieval	busqueda informacion;traitement signal;text;separador;information retrieval;musica;interrogation base donnee;acoustique musicale;interrogacion base datos;texte;segmentation;musical acoustics;biblioteca electronica;musique;recherche documentaire;acustica musical;recherche information;busqueda documental;signal processing;indexation;music information retrieval;electronic library;melodia;document retrieval;separator;texto;procesamiento senal;scientific research;separateur;music;database query;segmentacion;bibliotheque electronique;melody;melodie	The scientific research on accessing and retrieval of music documents is becoming increasingly active, including the analysis of suitable features for content description or the development of algorithms to match relevant documents with queries. One of the challenges in this area is the possibility to extend textual retrieval techniques to music language. Music lacks of explicit separators between its lexical units, thus they have to be automatically extracted. This paper presents an overview of different approaches to melody segmentation aimed at extracting music lexical units. A comparison of different approaches is presented, showing their impact on indexes size and on retrieval effectiveness.	information retrieval	Giovanna Neve;Nicola Orio	2005		10.1007/11551362_5	natural language processing;document retrieval;separator;melody;speech recognition;computer science;signal processing;musical acoustics;music;segmentation;information retrieval	Web+IR	-35.349882502411276	-63.47111314210692	93949
c1354028cf2aebaece10163c61b10abe099f78de	metadata visualization of scholarly search results: supporting exploration and discovery	information retrieval system;information retrieval;digital library;information visualization;information seeking;academic libraries;document similarity	Studies of online search behaviour have found that searchers often face difficulties formulating queries and exploring the search results sets. These shortcomings may be especially problematic in digital libraries since library searchers employ a wide variety of information seeking methods (with varying degrees of support), and the corpus to be searched is often more complex than simple textual information. This paper presents Bow Tie Academic Search, an interactive Web-based academic library search interface aimed at supporting the strategic retrieval behaviour of searchers. In this system, a histogram of the most frequently used keywords in the top search results is provided, along with a compact visual encoding that represents document similarities based on the co-use of keywords. In addition, the list-based representation of the search results is enhanced with visual representations of citation information for each search result. A detailed view of this citation information is provided when a particular search result is selected. These tools are designed to provide visual and interactive support for query refinement, search results exploration, and citation navigation, making extensive use of the metadata provided by the underlying academic information retrieval system.	academic search;digital library;information retrieval;information seeking;library (computing);online search;refinement (computing);semantic similarity	Taraneh Khazaei;Orland Hoeber	2012		10.1145/2362456.2362483	cognitive models of information retrieval;computer science;phrase search;concept search;data mining;search analytics;world wide web;information retrieval;search engine;human–computer information retrieval	Web+IR	-34.65253347858743	-54.3991311000224	94085
deb2aaaab59fbb97d97c437b5603951797609c74	a multi-dimensional analysis of deception		This study presents a multi-dimensional (MD) analysis which attempts to explore the linguistic differences between truthful and deceptive statements. Based on the analysis, three primary dimensions of linguistic features are identified, i.e. narrative concerns, interpersonal relationship, and perceptive expressions. These dimensions show significant differences in their distribution between truthful and deceptive statements, and could thus serve as fingerprints for the identification of deception.	fingerprint	Qi Su	2017	2017 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2017.8300569	natural language processing;deception;social psychology;artificial intelligence;narrative;interpersonal relationship;computer science;multidimensional analysis;expression (mathematics)	SE	-44.20881912249566	-60.05443097627131	94484
ab5dd5792d4f8e8e16b4faea8975bc59f832bed8	the comparison of classification-system-based normalization procedures with source normalization alternatives in waltman and van eck (2013)		Along with other co-authors, Ludo Waltman and Nees Jan van Eck have recently contributed to significantly increasing he fairness of bibliometric research assessments, in particular that of multidisciplinary assessments involving comparisons f citation impact between different fields of science (Van Eck, Waltman, Van Raan, Klautz, & Peul, 2013; Waltman & Van Eck, 013a; Waltman, Van Eck, Van Leeuwen, & Visser, 2013). This note addresses some pending issues in their latest contribution Waltman and Van Eck (2013b) for this journal, referred to as WVE hereafter – concerning a systematic large-scale empirical omparison of classification-system-based versus source normalization procedures.1 Among the former, they focus on what e will call classification system WoS, namely, the system where publications are classified into fields based on the journal ubject categories in the Web of Science bibliographic database. WVE study the normalization procedure based on this ystem that uses field mean citations as normalization factors.2 To differentiate this procedure from others of the same ype, we denote it by NWoS (rather than NSC as do WVE). On the other hand, according to WVE SNCS(3) exhibits the best erformance among the source normalization procedures. Therefore, for our purposes, the issue put forward by WVE is the omparison of NWoS versus SNCS(3). A key methodological feature of WVE’s contribution is the distinction between the use of a classification system in he implementation and the evaluation of a normalization procedure. Sirtes (2012) first suggested that using a certain lassification system for evaluation purposes would be generally biased in favor of normalization procedures based on that articular system. WVE concur with this idea, and provide further arguments about the possibility of this bias (see footnote and Appendix C). Therefore, they recommend that the comparison between NWoS and SNCS(3) should be done using a econd, independent classification system for evaluation purposes. Following this recommendation, WVE use three systems lgorithmically constructed according to the methods in Waltman and Van Eck (2012), systems A, B, and C, consisting of 21, 61, and 1334 scientific fields at different granularity or aggregation levels. Given a classification system, the degree to which differences in citation practices between fields have been corrected s indicated by the degree to which the field-normalized citation distributions coincide with each other. In particular, WVE se the measurement framework introduced in Crespo, Li, and Ruiz-Castillo (2013) where, given a classification system, he effect on citation inequality of differences in citation practices is captured by an IDCP between-group inequality term n a certain partition of the overall citation distribution by field and quantile, where IDCP stands for citation Inequality ttributable to Differences in Citation Practices. The evaluation of any set of normalization procedures in terms of a given lassification system can take a graphical or a numerical form. Following the graphical approach, WVE reach the following onclusion: The SNCS(3) procedure generally performs better than the NWoS procedure, specifically at higher levels of granularity. Li and Ruiz-Castillo (2013) establish that the graphical and the numerical approaches are logically independent. Therefore, hey can be used in a complementary fashion. To save space, in most of this note I will follow the numerical approach where, iven a classification system, each normalization procedure is assessed in terms of the reduction it generates in the IDCP erm. To understand the evaluation results obtained with this approach, some notation is needed. Recall that we have four lassification systems, which will be indexed by K = WoS, A, B, and C. Given system K, denote by IDCP(K) the IDCP term that aptures the effect on citation inequality of differences in citation practices across fields in K. Similarly, given system K, enote by NK the associated normalization procedure. Finally, given a classification system G for the evaluation of procedure	bibliographic database;bibliometrics;ccir system g;database normalization;fairness measure;graphical user interface;national supercomputer centre in sweden;numerical analysis;social inequality;tempest (codename);web of science;world wide web	Javier Ruiz-Castillo	2014	J. Informetrics	10.1016/j.joi.2013.10.002	speech recognition;computer science;data mining	Web+IR	-42.96672953103995	-65.04611513916055	94577
6f558b3a46402cf335370a86bb1600f48da00b4c	extending the rocchio relevance feedback algorithm to provide contextual retrieval	distributed system;systeme reparti;red www;on line;en linea;interrogation base donnee;reseau web;interrogacion base datos;satisfiability;feedback;sistema repartido;internet;retroaction pertinence;world wide web;en ligne;information need;boucle reaction;retroalimentacion;article;relevance feedback;database query	Contextual retrieval supports differences amongst users in their information seeking requests. The Web, which is very dynamic and nearly universally accessible, is an environment in which it is increasingly difficult for users to find documents that satisfy their specific information needs. This problem is amplified as users tend to use short queries. Contextual retrieval attempts to address this problem by incorporating knowledge about the user and past retrieval results in the search process. In this paper we explore a feedback technique based on the Rocchio algorithm that significantly reduces demands on the user while maintaining comparable performance on the Reuters-21578 corpus.	computer accessibility;hybrid system;information needs;information retrieval;information seeking;relevance feedback;rocchio algorithm;user (computing);world wide web	Christopher T. Jordan;Carolyn R. Watters	2004		10.1007/978-3-540-24681-7_16	information needs;the internet;computer science;artificial intelligence;feedback;database;distributed computing;multimedia;world wide web;information retrieval;satisfiability	Web+IR	-37.365156654428205	-57.39983996508284	94648
56bdcae540036d9fb29caff5b897c15ad6298953	network visualisations for exploring political concepts		This work presents a system for exploring the conceptual environment of words in a corpus with interactive network representations of corpus-derived grammatical relations and word associations. The representations consist of part-of-speech tagged words connected by typed, weighted, edges indicating the strength of relations between words, as measured by weighted pointwise mutual information of different types of co-occurrences. An interactive animated interface allows users to adjust the node degree directly, or to specify edge-weight thresholds, and observe the resulting effect on the network. The system can be searched by neighbourhood sub-graphs (‘ego graphs’) of particular query terms. The force-directed layout of the network highlights conceptual structure, as terms connected by many relations are drawn together, and the user can select which subsets of relations and sub-corpora to display. As an example of such a system for exploring the structure of political concepts, an implementation on comments from libertarian and socialist partisan online communities is presented.	force-directed graph drawing;neighbourhood (graph theory);online community;pointwise mutual information;text corpus	Paul Nulty	2017			social science;politics;political science	Web+IR	-41.31270937278842	-57.78350588114659	94657
f1b55b6bac83251e34b24cc74a746688b88c204e	majority based ranking approach in web image retrieval	busqueda informacion;analisis imagen;utilisation information;analisis contenido;contenu image;uso informacion;image content;text;keyword;red www;information use;recherche image;information retrieval;reseau web;palabra clave;texte;mot cle;content analysis;hierarchical classification;internet;recherche information;clustering method;classification hierarchique;world wide web;image analysis;information system;analyse contenu;contenido imagen;texto;analyse image;clasificacion jerarquizada;article;systeme information;sistema informacion;image retrieval	In this paper, we address a ranking problem in web image retrieval. Due to the growing availability of web images, comprehensive retrieval of web images has been expected. Conventional systems for web image retrieval are based on keywordbased retrieval. However, we often find undesirable retrieval results from the keyword based web image retrieval system since the system uses the limited and inaccurate text information of web images ; a typical system uses text information such as surrounding texts and/or image filenames, etc. To alleviate this situation, we propose a new ranking approach which is the integration of results of text and image content via analyzing the retrieved results. We define four ranking methods based on the image contents analysis of the retrieved images; (1) majority-first method, (2) centroid-of-all method, (3) centroid-of-top K method, and (4) centroid-of-largest-cluster method. We evaluate the retrieval performance of our methods and conventional one using precision and recall graphs. The experimental results show that the proposed methods are more effective than conventional keywordbased retrieval methods.	experiment;image retrieval;information retrieval;precision and recall	Gunhan Park;Yunju Baek;Heung-Kyu Lee	2003		10.1007/3-540-45113-7_12	computer vision;visual word;the internet;image analysis;content analysis;image retrieval;computer science;pattern recognition;data mining;adversarial information retrieval;automatic image annotation;world wide web;information retrieval;information system;human–computer information retrieval	Web+IR	-35.939765871007396	-58.919730171815175	94856
6b94adf360514061e59873ea37fc78f43acd6040	concept based retrieval in classical ir systems	information retrieval;document retrieval;concept space	This paper describes some aspects of a project with the aim of developing a user-friendly interface to a classical Information Retrieval (IR) System in order to improve the effectiveness of retrieval. The character by character approach to IR has been abandoned in favor of an approach based on the meaning of both the queries and the texts containing the information to be sought. The concept space, locally derived from a thesaurus, is used to represent a query as well as documents retrieved in atomic concept units. Dependencies between the search terms are taken into account. The meanings of the query and the retrieved documents (results of Elementary Logical Conjuncts (ELCs)) are compared. The ranking method on the semantical level is used in connection with existing data of a classical IR system. The user enters queries without using complex Boolean expressions.	boolean expression;information retrieval;thesaurus;usability	Hanspeter Giger	1988		10.1145/62437.62461	document retrieval;query expansion;ranking;standard boolean model;computer science;theoretical computer science;concept search;data mining;information retrieval;human–computer information retrieval	Web+IR	-33.95142942063727	-60.767862941443994	95547
9f74d5965a9ddb59ec56662cf5fb9830f815caa5	a simple, structure-sensitive approach for web document classification	busqueda informacion;document structure;graph theory;model based reasoning;web documents;raisonnement base sur modele;teoria grafo;metodo vectorial;classification algorithm;red www;graph method;estructura documental;information retrieval;structure document;reseau web;vector space;intelligence artificielle;document representation;metodo grafo;classification;theorie graphe;methode graphe;vecino mas cercano;hybrid approach;recherche documentaire;internet;recherche information;busqueda documental;vector method;plus proche voisin;artificial intelligence;world wide web;nearest neighbour;methode vectorielle;k nearest neighbor;document retrieval;inteligencia artificial;espace vectoriel;classification accuracy;espacio vectorial;clasificacion	In this paper we describe a new approach to classification of web documents. Most web classification methods are based on the vector space document representation of information retrieval. Recently the graph based web document representation model was shown to outperform the traditional vector representation using k-Nearest Neighbor (k-NN) classification algorithm. Here we suggest a new hybrid approach to web document classification built upon both, graph and vector representations. K-NN algorithm and three benchmark document collections were used to compare this method to graph and vector based methods separately. Results demonstrate that we succeed in most cases to outperform graph and vector approaches in terms of classification accuracy along with a significant reduction in classification time.	benchmark (computing);document classification;id3 algorithm;information retrieval;k-nearest neighbors algorithm;naive bayes classifier;run time (program lifecycle phase);web page	Alex Markov;Mark Last	2005		10.1007/11495772_46	document retrieval;web query classification;the internet;vector space;biological classification;computer science;artificial intelligence;graph theory;document structure description;model-based reasoning;machine learning;linear classifier;data mining;world wide web;k-nearest neighbors algorithm;library classification	Web+IR	-36.13282185136005	-58.55441686428427	95637
b4d1e40637098cf2cfe883033fe283aa9d4c2177	developing fitness functions for pleasant music: zipf's law and interactive evolution systems	sistema interactivo;assignment problem;probleme affectation;software maintenance;musica;hombre;automatisation;metric;automatizacion;systeme conversationnel;human subjects;aesthetics;maintenance logiciel;musique;zipf law;interactive system;visual art;human;loi zipf;problema asignacion;algorithme evolutionniste;metrico;ley zipf;algoritmo evolucionista;esthetique;evolutionary algorithm;music;metrique;estetica;fitness function;homme;automation	In domains such as music and visual art, where the quality of an individual often depends on subjective or hard to express concepts, the automating fitness assignment becomes a difficult problem. This paper discusses the application of Zipf’s Law in evaluation of music pleasantness. Preliminary results indicate that a set of Zipf-based metrics can be effectively used to classify music according to pleasantness as reported by human subjects. These studies suggest that metrics based on Zipf’s law may capture essential aspects of proportion in music as it relates to music aesthetics. We discuss the significance of these results for the automation of fitness assignment in evolutionary music	evolutionary music;fitness function;interactive evolutionary computation;interactivity;zipf's law	Bill Z. Manaris;Penousal Machado;Clayton McCauley;Juan Romero;Dwight Krehbiel	2005		10.1007/978-3-540-32003-6_50	evolutionary music;metric;computer science;artificial intelligence;automation;evolutionary algorithm;music;mathematics;assignment problem;software maintenance;fitness function;algorithm	ML	-39.32613913992063	-58.32809884842236	95722
1412d504a6fdc65f059071be339e5ef15a78c9c3	improving heuristic mini-max search by supervised learning	evaluation function;selective game tree search;supervised learning;l ogistello;feature space;general methods;glem;opening book learning;feature construction;least square;p rob c ut;game tree search;evaluation model	This article surveys three techniques for enhancing heuristic game-tree search pioneered in the author’s Othello program LOGISTELLO, which dominated the computer Othello scene for several years and won against the human World-champion 6–0 in 1997. First, a generalized linear evaluation model (GLEM) is described that combines conjunctions of Boolean features linearly. This approach allows an automatic, data driven exploration of the feature space. Combined with efficient least squares weight fitting, GLEM greatly eases the programmer’s task of finding significant features and assigning weights to them. Second, the selective search heuristic PROBCUT and its enhancements are discussed. Based on evaluation correlations PROBCUT can prune probably irrelevant sub-trees with a prescribed confidence. Tournament results indicate a considerable playing strength improvement compared to full-width α-β search. Third, an opening book framework is presented that enables programs to improve upon previous play and to explore new opening lines by constructing and searching a game-tree based on evaluations of played variations. These general methods represent the state-of-the-art in computer Othello programming and begin to attract researchers in related fields.  2002 Elsevier Science B.V. All rights reserved.	computer othello;feature vector;heuristic;least squares;logistello;programmer;relevance;reversi;supervised learning	Michael Buro	2002	Artif. Intell.	10.1016/S0004-3702(01)00093-5	feature vector;computer science;artificial intelligence;machine learning;evaluation function;mathematics;supervised learning;least squares;algorithm	AI	-42.14165136190722	-63.100325059182175	96020
07c2e545d19ddfca4b43d5ef78c2dfbb8aa73455	a comparison of a hierarchical tree to an associative map interface for the selection of classification terms	controlled vocabulary;digital libraries;user study;visual interfaces	This paper reports the results of a user study comparing the use of a traditional hierarchical tree interface with an associative graphical interface to select controlled vocabulary terms for document classification. The results suggest that users prefer to have both the hierarchical and associative map rather than the hierarchical tree alone and tend to ascribe more categories when using the experimental interface.		Jan W. Buzydlowski;Xia Lin;Mi Zhang;Lillian N. Cassel	2013		10.1002/meet.14505001143	natural language processing;controlled vocabulary;digital library;computer science;machine learning;world wide web	HCI	-33.833680940718104	-61.03303205380719	96234
8ef406e047af8030fee1926cf233d7b8e5d3a963	expansión fonética de la consulta para la recuperación de información en documentos hablados	expansion de la consulta;codificacion fonetica;computacion informatica;phonetic codes;information retrieval;filologias;recuperacion de informacion;info eu repo semantics article;informacion documentacion;linguistica;ciencias basicas y experimentales;grupo a;query expansion;ciencias sociales;spoken documents;grupo b;documentos hablados	The traditional approach for searching information in large collections of spoken documents consists of integrating automatic speech recognition (ASR) methods and traditional text retrieval (IR) techniques. One disadvantage of this approach is its dependence to the precision of the ASR system, since transcription errors strongly affect the IR machine. With the aim of reducing the impact of these errors, especially those concerning substitutions, in this paper we propose expanding the queries by means of phonetically similar words, and by this increasing the possibility of matching incorrectly transcribed words from the documents. Results on two very different spoken-document collections show the relevance of the proposed method, which outperformed the MAP from traditional expansion techniques by up to 3.68%. K eywords: Information Retrieval, Spoken Documents, Query Expansion, Phonetic Codes.	automated system recovery;code;document retrieval;information retrieval;linear algebra;query expansion;relevance;speech recognition;transcription (software)	M. Alejandro Reyes-Barragán;Luis Villaseñor Pineda;Manuel Montes-y-Gómez	2011	Procesamiento del Lenguaje Natural		query expansion;computer science	Web+IR	-35.576947461198856	-62.950849614017095	96457
ef210d43c0ce301c41e2a7a81ee724874a7e7181	cultural diversity of quality of information on wikipedias		This article explores the relationship between linguistic culture and the preferred standards of presenting information based on article representation in major Wikipedias. Using primary research analysis of the number of images, references, internal links, external links, words, and characters, as well as their proportions in Good and Featured articles on the eight largest Wikipedias, we discover a high diversity of approaches and format preferences, correlating with culture. We demonstrate that high-quality standards in information presentation are not globally shared and that in many aspects, the language cultureu0027s influence determines what is perceived to be proper, desirable, and exemplary for encyclopedic entries. As a result, we demonstrate that standards for encyclopedic knowledge are not globally agreed-upon and objective but local and very subjective.		Dariusz Jemielniak;Maciej Wilamowski	2017	JASIST	10.1002/asi.23901	data mining;computer science;cultural diversity;information retrieval;information quality;knowledge management;primary research	ECom	-39.45959502600129	-59.883910395017146	96546
8c5cea416a896d13fac3f3d9af16a964d6a7faf5	mental models of the bibliographic universe. part 1: mental models of descriptions	etude utilisateur;bibliographics;slovenia;catalogacion;user study;estudio usuario;library and information science;conceptual model;catalogage;bibliographic description;cataloguing;user studies;frbr functional requirements for bibliographic records;small samples;descripcion bibliografica;space use;modelo mental;cognition;concept map;user testing;modele mental;cataloging;cumulant;functional requirements for bibliographic records;description bibliographique;mental model;design methodology	Purpose – The paper aims to present the results of the first two tasks of a user study looking into mental models of the bibliographic universe and especially their comparison to the Functional Requirements for Bibliographic Records (FRBR) conceptual model, which has not yet been user tested.Design/methodology/approach – The paper employes a combination of techniques for eliciting mental models and consisted of three tasks, two of which, card sorting and concept mapping, are presented herein. Its participants were 30 individuals residing in the general area of Ljubljana, Slovenia.Findings – Cumulative results of concept mapping show a strong resemblance to FRBR. Card sorts did not produce conclusive results. In both tasks, participants paid special attention to the original expression, indicating that a special place for it should be considered.Research limitations/implications – The study was performed using a relatively small sample of participants living in a geographically limited space using relative...	mental model	Jan Pisanski;Maja Zumer	2010	Journal of Documentation	10.1108/00220411011066772	concept map;library science;social science;cognition;design methods;computer science;artificial intelligence;conceptual model;data mining;sociology;world wide web;information retrieval;statistics;cumulant	NLP	-40.973288938145856	-56.602304916855054	96582
d3366f5b105832bef31360241aafb5c83a7103ec	understanding and enhancing user acceptance of computer technology	automatic control;computer aided design;space technology automatic control humans health and safety computer aided manufacturing manufacturing automation personnel performance analysis occupational safety hardware;manufacturing automation;computer assisted instruction;personnel;health and safety;computer aided manufacturing;occupational safety;performance analysis;user requirements;computer techniques;humans;space technology;man machine systems;user acceptance;hardware;automation	Technology-driven efforts to implement computer technology often encounter problems due to lack of acceptance or begrudging acceptance of the personnel involved. It is argued that individuals' acceptance of automation, in terms of either computerization or computer aiding, is heavily influenced by their perceptions of the impact of the automation on their discretion in performing their jobs. It is suggested that desired levels of discretion reflect needs to feel in control and achieve self-satisfaction in task performance, as well as perceptions of inadequacies of computer technology. Discussion of these factors leads to a structured set of considerations for performing front-end analysis, deciding what to automate, and implementing the resulting changes.	computer;job stream	Willian Bill Rouse;Nancy M. Morris	1986	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1986.4309014	simulation;user requirements document;automation;occupational safety and health;space technology	Visualization	-46.6053369086087	-56.205921500546516	96598
bad6cefaff39e3eb4a5c0c3e2c0152027959a224	who benefits from clir in web retrieval?	busqueda informacion;etude utilisateur;etude utilisation;search engine;translating;information retrieval;user study;language skills;estudio utilizacion;estudio usuario;lenguaje;langage;laboratory tests;user studies;result;traduction;recherche information;fi artikkeli aikakauslehdessa en journal article;target language;source language;web retrieval;resultado;world wide web;traduccion;resultat;query translation;language;cross language information retrieval;use study;machine translation;design methodology	Purpose – The aim of the current paper is to test whether query translation is beneficial in web retrieval.Design/methodology/approach – The language pairs were Finnish‐Swedish, English‐German and Finnish‐French. A total of 12‐18 participants were recruited for each language pair. Each participant performed four retrieval tasks. The author's aim was to compare the performance of the translated queries with that of the target language queries. Thus, the author asked participants to formulate a source language query and a target language query for each task. The source language queries were translated into the target language utilizing a dictionary‐based system. In English‐German, also machine translation was utilized. The author used Google as the search engine.Findings – The results differed depending on the language pair. The author concluded that the dictionary coverage had an effect on the results. On average, the results of query‐translation were better than in the traditional laboratory tests.Origina...	cross-language information retrieval	Eija Airio	2008	Journal of Documentation	10.1108/00220410810899754	natural language processing;design methods;data control language;computer science;rdf query language;language;machine translation;world wide web;information retrieval;query language;search engine	Web+IR	-35.08399989289414	-62.10525022217349	97573
11559d6c6d85c2cd2135434a5540aaad88e09d30	which parts of scientific explanations are most important?		Given the depth and breadth of available information, determining which components of an explanation are most important is a crucial process for simplifying learning. Two experiments tested whether people believe that components of an explanation with more elaboration are more important. In Experiment 1, participants gave higher importance scores for components that they judged to be elaborated upon by many other components. In Experiment 2, the amount and type of elaboration was experimentally manipulated. Experiment 2 demonstrated that elaboration increases the importance of the elaborated information by providing insight into understanding the elaborated information; information that was too technical to provide insight into the elaborated component did not increase the importance of the elaborated component. While learning an explanation, people piece together the structure of elaboration relationships between components and use the insight provided by elaboration to identify important components.	experiment	Benjamin M. Rottman;Frank C. Keil	2011				ML	-41.34735698963445	-55.092123896014	97654
c23bc9051d63ccea0d20a22b55f9abf5d3684f5a	summarization of dynamic content in web collections	web documents;web pages;red www;analisis datos;static method;reseau web;resumen;multi document summarization;fenetre coulissante;data mining;similitude;data analysis;internet;fouille donnee;resume;decouverte connaissance;similarity;world wide web;descubrimiento conocimiento;analyse donnee;ventana deslizante;retrospective;metodo estatico;similitud;dynamic content;abstract;busca dato;retrospectiva;sliding window;knowledge discovery;methode statique	This paper describes a new research proposal of multi-document summarization of dynamic content in web pages. Much information is lost in the Web due to the temporal character of web documents. Therefore adapting summarization techniques to the web genre is a promising task. The aim of our research is to provide methods for summarizing volatile content retrieved from collections of topically related web pages over defined time periods. The resulting summary ideally would reflect the most popular topics and concepts found in retrospective web collections. Because of the content and time diversities of web changes, it is necessary to apply different techniques than standard methods used for static documents. In this paper we propose an initial solution to this summarization problem. Our approach exploits temporal similarities between web pages by utilizing sliding window concept over dynamic parts of the collection.	automatic summarization;dynamic data;dynamic web page;experiment;multi-document summarization;usability;world wide web	Adam Jatowt;Mitsuru Ishizuka	2004		10.1007/978-3-540-30116-5_24	sliding window protocol;web mining;web modeling;the internet;data web;web analytics;web mapping;similarity;multi-document summarization;web standards;computer science;similitude;dynamic web page;automatic summarization;social semantic web;web page;data mining;semantic web stack;database;knowledge extraction;web intelligence;data analysis;world wide web;website parse template;information retrieval	Web+IR	-35.28931832626823	-58.31562515885856	97687
8a66cd7d7a0fabe1bd4d5beaea913ecf9e3b6187	a machine learning approach to web page filtering using content and structure analysis	content management;busqueda informacion;document structure;experimental design;analisis contenido;filtering;search engine;filtrage;buscador;learning algorithm;teleenseignement;keyword;web pages;analisis estadistico;red www;backpropagation neural network;estructura documental;analisis estructural;information retrieval;structure document;lexicon;teoria conjunto;filtrado;reseau web;plan experiencia;customization;personnalisation;theorie ensemble;gestion contenido;palabra clave;intelligence artificielle;algorithme apprentissage;mot cle;set theory;data mining;link analysis;classification a vaste marge;content analysis;internet;statistical analysis;machine learning;plan experience;fouille donnee;recherche information;filter;backpropagation algorithm;analyse statistique;personalizacion;gestion contenu;filtre;algorithme retropropagation;web mining;artificial intelligence;world wide web;teleensenanza;inteligencia artificial;support vector machine;moteur recherche;maquina ejemplo soporte;analyse contenu;vector support machine;analyse structurale;lexico;learning artificial intelligence;reseau neuronal;remote teaching;structural analysis;web page classification;algoritmo aprendizaje;article;busca dato;filtro;red neuronal;structure analysis;neural network;lexique;algoritmo retropropagacion;apprentissage intelligence artificielle	As the Web continues to grow, it has become increasingly difficult to search for relevant information using traditional search engines. Topic-specific search engines provide an alternative way to support efficient information retrieval on the Web by providing more precise and customized searching in various domains. However, developers of topic-specific search engines need to address two issues: how to locate relevant documents (URLs) on the Web and how to filter out irrelevant documents from a set of documents collected from the Web. This paper reports our research in addressing the second issue. We propose a machine-learning-based approach that combines Web content analysis and Web structure analysis. We represent each Web page by a set of content-based and link-based features, which can be used as the input for variousmachine learning algorithms. The proposed approach was implemented using both a feedforward/backpropagation neural network and a support vector machine. Two experiments were designed and conducted to compare the proposed Web-feature approach with two existing Web page filtering methods — a keyword-based approach and a lexicon-based approach. The experimental results showed that the proposed approach in general performed better than the benchmark approaches, especially when the number of training documents was small. The proposed approaches can be applied in topic-specific search engine development and other Web applications such as Web content management. © 2007 Elsevier B.V. All rights reserved.	algorithm;artificial neural network;backpropagation;benchmark (computing);experiment;feedforward neural network;information retrieval;lexicon;machine learning;relevance;support vector machine;web application;web content management system;web page;web search engine;world wide web	Michael Chau;Hsinchun Chen	2008	Decision Support Systems	10.1016/j.dss.2007.06.002	web service;web mining;web modeling;site map;data web;web analytics;web mapping;web design;content analysis;computer science;artificial intelligence;web crawler;machine learning;social semantic web;data mining;structural analysis;web intelligence;web search query;world wide web;search engine;mashup	Web+IR	-36.22995713411682	-58.44103370378216	97827
f42bdf00ed325b8e8291e534f79980f6ca7d7b9b	relevance as an aid to evaluation in opacs	on line processing;systeme documentaire;opac;information retrieval;estudio comparativo;pertinencia;tratamiento en linea;etude comparative;automated catalog;pertinence;sistema recuperacion documental;document retrieval system;comportement utilisateur;comparative study;evaluation;user behavior;relevance;evaluacion;information need;traitement en ligne;catalogue automatise;catalogo automatizado;comportamiento usuario	The relevance of retrieved documents or document descriptions has been a central measurement in the evaluation of information retrieval (IR) systems. Online public access catalogues (OPACs) are similar in many ways and so relevance should be an appropriate evaluation toool in measuring their performance. The development of relevance in evaluating IR systems is described and also the important differences between such systems and OPACs. Characteristics of the endusers of both systems are discussed with the conclusion that end-users' motivation and behaviour are as yet not fully understood	online public access catalog;relevance	Ann O'Brien	1990	J. Information Science	10.1177/016555159001600407	information needs;relevance;computer science;evaluation;comparative research;law;world wide web;information retrieval	NLP	-37.41389089272959	-59.55319545569272	98142
9b1ccecf2b9769479be244cd70616f41b8e28b1e	associative document retrieval techniques using bibliographic information	computer experiment;document retrieval	Automatic documentation systems which use the words contained in the individual documents as a principal source of document identifications may not perform satisfactorily under all circumstances. Methods have therefore been devised within the last few years for computing association measures between words and between documents, and for using such associated words, or information contained in associated documents, to supplement and refine the original document identifications. It is suggested in this study that bibliographic citations may provide a simple means for obtaining associated documents to be incorporated in an automatic documentation system. The standard associative retrieval techniques are first briefly reviewed. A computer experiment is then described which tends to confirm the hypothesis that documents exhibiting similar citation sets also deal with similar subject matter. Finally, a fully automatic document retrieval system is proposed which uses bibliographic information in addition to other standard criteria for the identification of document content, and for the detection of relevant information.	computer experiment;document retrieval;documentation generator;subject matter expert turing test	Gerard Salton	1963	J. ACM	10.1145/321186.321188	natural language processing;document retrieval;computer experiment;relevance;document clustering;computer science;database;information retrieval;human–computer information retrieval	Web+IR	-35.5873575495673	-64.81271674779178	98643
3a6654d6d50910ba196c139fef374ac893278a3a	on the minimum vocabulary problem	lexicography;definitions;dictionaries;information retrieval;indexing	The “minimum vocabulary problem” for a dictionary has applications in indexing and other domains of information retrieval. A simple directed-graph model of a dictionary results in a linear-time algorithm for this problem. Since it is known that many minimum vocabularies can exist for a dictionary, a computationally useful criterion for finding a “desirable minimum vocabulary” is suggested and an O(lV13) algorithm is outlined, where /VI is the number of words in the dictionary, duplicates eliminated. Furthermore, some enrichments to the model and the computational intractability of a variant called the l-lexicon problem are discussed. Directions for further work are indicated.	algorithm;computational complexity theory;dictionary;directed graph;information retrieval;lexicon;time complexity;vocabulary	N. Chandrasekharan;R. Sridhar;S. Sitharama Iyengar	1987	JASIS	10.1002/(SICI)1097-4571(198707)38:4%3C234::AID-ASI3%3E3.0.CO;2-O	natural language processing;search engine indexing;speech recognition;computer science;lexicography;linguistics;information retrieval	Theory	-34.84769233663897	-64.92715704643433	98858
770a6176b5bc814d7a2b90cadc74c52da0f37d28	continuous haptic information in target tracking from a moving platform	tâche poursuite;sensibilite;tracking systems;remote control;pedestrian safety;ergonomia;platforms;poison control;etude experimentale;injury prevention;simulation;hombre;simulacion;safety literature;offshore platforms;ergonomie;percepcion;motion;traffic safety;injury control;loads;remote operation;motors;visual load;motor loads;stimulus movement;home safety;error analysis;sensitivity;injury research;safety abstracts;human factors;teleaccion;tactile perception;occupational safety;perceptionperception;cognition;safety;human;cognicion;tarea persecucion;safety research;platform motions;accident prevention;violence prevention;bicycle safety;tracking task;perception;mouvement stimulus;target tracking;poisoning prevention;target detection;falls;estudio experimental;ergonomics;suicide prevention;training simulators;tracking;sensibilidad;teleoperation;movimiento estimulo;homme;control devices	The present study was conducted to gain insight into the effect of different forms of continuous haptic information (CHI) on operator performance with a moving unmanned platform. In a simulator experiment, participants tracked a moving target with a disturbed viewfinder (moving platform). While the participants performed this combined pursuit and compensatory tracking task, haptic information was provided to them concerning translatory disturbances of the platform. Two steering variables were manipulated between participants: presence or absence of CHI provided at the control device and automated or manual stabilization of the platform. The other factors were image degradation, motor task load, and visual task load. Haptic information was generated by movements of an active joystick that was used for steering the platform. It was shown that both CHI and platform stabilization substantially reduced tracking error. These effects were not additive; CHI improved tracking performance only when the platform was not stabilized, and it did not significantly degrade pursuit tracking performed with a stabilized viewfinder. The magnitude of the CHI effect was independent of image degradation, motor load, and visual load. CHI at the joystick improves tracking performance when it involves relevant control information; when it provides other information, tracking performance is only marginally degraded. Actual or potential applications of this research include performance of missions in environments that are difficult to access, potentially harmful to humans, or both, such as reconnaissance behind enemy lines, tracing of environmental pollution at sea, and assessment of damage in nuclear disaster areas.	chi;elegant degradation;environmental pollution;haptic device component;haptic technology;humans;joystick device component;magee1 gene;movement;radiology information systems;religious missions;simulation;simulators;unmanned aerial vehicle;utility functions on indivisible goods	J. E. Korteling;M. L. van Emmerik	1998	Human factors	10.1518/001872098779480488	psychology;cognitive psychology;control engineering;teleoperation;simulation;cognition;tracking system;sensitivity;computer science;engineering;suicide prevention;human factors and ergonomics;injury prevention;motion;tracking;perception;computer security;remote control;mechanical engineering	HCI	-45.950751408740885	-54.50212787111755	99000
26809c9d0d0ca1feb716d52adfc89a6e7e3ba283	applying quality control charts to the analysis of single-subject data sequences	performance measure;stress;sensitivity and specificity;factor riesgo;evaluation performance;factor humano;aviation safety;metodologia;performance evaluation;ergonomia;risk factor;evaluacion prestacion;hombre;highway safety;ergonomie;facteur risque;methodologie;performance tests;control chart;risk factors;human factors;human factor;health and safety;cognitive performance;cognition;human;controle qualite;risk assessment;methodology;quality control;physical condition;facteur humain;ergonomics;control calidad;homme;exponentially weighted moving average	Techniques from the field of quality control can be used to classify the quality of individual samples of physical or cognitive performance. After stable baselines have been established for an individual, deviations in performance can be evaluated using control charts. The effectiveness of this approach in evaluating cognitive performance was tested using databases collected under a variety of risk factors. The sensitivity and specificity characteristics of Shewhart, cumulative-sum (CUSUM), and exponentially weighted moving average (EWMA) control charts were determined for a total of 174 trials involving 10 participants and 23 cognitive performance assessment measures. The most effective technique in each case was typically a function of the specific performance measure and the type of performance change being evaluated. Sensitivity and specificity for the best techniques were as high as 100%. This study demonstrated the usefulness of quality control charts as a tool to evaluate individual participant performance over time. Actual or potential applications of this research include readiness-to-perform screening of industrial workers in order to improve the health and safety of the workforce.		Randa L. Shehab;Robert E. Schlegel	2000	Human factors	10.1518/001872000779698033	psychology;simulation;medicine;engineering;human factors and ergonomics;social psychology;risk factor;statistics;mechanical engineering	SE	-45.88645940367067	-55.82573241700915	99052
aa01e3ec5f6b60aebd478c1dbfbc7d6e7be75582	parallel document retrieval using the connection machine	document retrieval		connection machine;document retrieval	Mohammad Lotfi-Jam;Alan J. Kent	1996			data mining;visual word;computer science;document retrieval;information retrieval;document clustering;vector space model	NLP	-34.24786468296713	-62.87055216637079	99316
5d09cfe1dfbefc96c3eec9888a731739c8f35b30	the interpretation of cas	busqueda informacion;information retrieval;xml language;interrogation base donnee;interrogacion base datos;recherche information;requete structurelle;requerimiento estructura;database query;langage xml;lenguaje xml;structural query	There has been much debate over how to interpret the structure in queries that contain structural hints. At INEX 2003 and 2004, there were two interpretations: SCAS in which the user specified target element was interpreted strictly, and VCAS in which it was interpreted vaguely. But how many ways are there that the query could be interpreted? In the investigation at INEX 2005 (discussed herein) four different interpretations were proposed, and compared on the same queries. Those interpretations (SSCAS, SVCAS, VSCAS, and VVCAS) are the four interpretations possible by interpreting the target elements, and the support elements, either strictly or vaguely. An analysis of the submitted runs shows that those that share an interpretation of the target element correlate that is, the previous decision to divide CAS into the SCAS and VCAS (as done at INEX 2003 and 2004) was sound. The analysis is supported by the fact that the best performing VSCAS run was submitted to the VVCAS task and the best performing SVCAS run was submitted to the SSCAS task.	conformance testing;experiment;information retrieval;vagueness;xml retrieval	Andrew Trotman;Mounia Lalmas	2005		10.1007/978-3-540-34963-1_5	computer science;data mining;database;world wide web	NLP	-34.50138726487054	-60.29089116012142	99346
8134d8cf946a535df94cdee1afb345a1b7af7dce	long-term learning for web search engines	search engine;buscador;red www;perforation;reseau web;intelligence artificielle;document representation;long terme;log data;long term;web search engine;recherche documentaire;largo plazo;internet;incremental learning;recuperacion documental;artificial intelligence;world wide web;document retrieval;inteligencia artificial;moteur recherche;donnee session;test collection	This paper considers how web search engines can learn from the successful searches recorded in their user logs. Document Transformation is a feasible approach that uses these logs to improve document representations. Existing test collections do not allow an adequate investigation of Document Transformation, but we show how a rigorous evaluation of this method can be carried out using the referer logs kept by web servers. We also describe a new strategy for Document Transformation that is suitable for long-term incremental learning. Our experiments show that Document Transformation improves retrieval performance over a medium sized collection of webpages. Commercial search engines may be able to achieve similar improvements by incorporating	experiment;http referer;information retrieval;server (computing);web search engine;web server	Charles Kemp;Kotagiri Ramamohanarao	2002		10.1007/3-540-45681-3_22	document retrieval;the internet;web search engine;computer science;database;world wide web;information retrieval;search engine	Web+IR	-36.36690734048367	-58.432417752410906	99429
7fec30601cd6763cd91159bcf0aef2825f8daf90	status conspicuity, peripheral vision, and text editing	computadora;presentation information;text editor;peripheral vision;ergonomia;vision periferica;ordinateur;movimiento ocular;relacion hombre maquina;information layout;vision peripherique;hombre;man machine relation;ergonomie;percepcion;computer;ecran visualisation;pantalla visualizacion;eye movement;cognition;human;editor texto;cognicion;editeur texte;relation homme machine;display screen;perception;mouvement oculaire;presentacion informacion;ergonomics;homme	Abstract This HCI study involved use of both an automated keystroke protocol analyser and eye movement monitoring equipment. Four levels of peripheral salience (conspicuity) of status information were employed. This increased the possibility of using peripheral vision whilst decreasing the probability of relying on eye movements. Superior performance was found in the condition with high conspicuity. It was suggested that this was accounted for by the relative lack of disruptive saccades, reading and relocating one's place. The main study was supported by a fine-grained eye movement analysis of the visual tasks involved. It was concluded that the peripheral presentation of text-editing status information is at present grossly underused, perhaps particularly so for novices.	peripheral vision;text editor	Derek Scott	1993	Behaviour & IT	10.1080/01449299308924363	psychology;computer vision;cognition;peripheral vision;artificial intelligence;human factors and ergonomics;communication;perception;eye movement	HCI	-45.082339978583434	-55.360414839103484	99830
955b348948c9e24bfd3c8fcde29366a438e0f8e2	domain-specific track clef 2005: overview of results and approaches, remarks on the assessment analysis	query language;vocabulaire;linguistique;sociologia;vocabulary;interrogation base donnee;interrogacion base datos;vocabulario;lenguaje interrogacion;linguistica;social science;comportement utilisateur;langage interrogation;user behavior;information system;sociologie;multilinguisme;database query;sociology;systeme information;multilingualism;domain specificity;comportamiento usuario;multilinguismo;sistema informacion;linguistics	The challenge of the CLEF domain-specific track is to map user queries in one language to documents in different languages adapting the systems used to the vocabulary and wording of the social science domain. In addition to a general overview of this track and its tasks, some details on the approaches of the participating groups and their results are reported. One of the outcomes is the considerable improvement in results if the retrieval systems make use of the thesauri provided or the intellectually assigned descriptors. Other findings for IR in a domain-specific context are also given. Finally, considerations on the topic creation and assessment processes are made on the basis of empirical data mainly from the GIRT corpus.	domain-specific language	Michael Kluck;Maximilian Stempfhuber	2005		10.1007/11878773_25	natural language processing;speech recognition;computer science;artificial intelligence;database;linguistics;information system;algorithm;query language	SE	-35.60147547881838	-62.921070568296834	99999
40a05154bc3536500dca8f9487d7581669e6fd54	exploring individual differences in raybased selection: strategies and traits	human computer interaction;individual differences;user strategies user centered design user difference user aptitude user experience virtual environment raybased techniques user interaction psychology aptitude tests questionnaires user behavior;selection;virtual reality;user difference;user centered design;psychology;virtual environments;human computer interaction psychology user interfaces virtual reality user centred design;user strategies;testing virtual environment psychology performance evaluation feedback measurement standards virtual reality user centered design performance analysis chromium;user experience;virtual environment raybased techniques;user centred design;user behavior;questionnaires;user aptitude;virtual environment;individual difference;aptitude tests;user interaction;user interfaces	User-centered design is often performed without regard to individual user differences in aptitude and experience. The methodology of this study is an anthropological and observational approach observing users performing a selection task using common virtual environment raybased techniques and analyzes the interaction through psychology aptitude tests, questionnaires and observation. The results of this study show the approach yields useful information about users even in a simple task. The study indicates correlations between performance and aptitude test and user behavior performed to overcome difficulties in the task.	computer performance;user-centered design;virtual reality;aptitude	Chadwick A. Wingrave;Ryan Tintner;Bruce N. Walker;Doug A. Bowman;Larry F. Hodges	2005	IEEE Proceedings. VR 2005. Virtual Reality, 2005.	10.1109/VR.2005.35	questionnaire;selection;user-centered design;human–computer interaction;computer science;knowledge management;virtual machine;virtual reality;multimedia;user interface	Visualization	-44.69044359729404	-56.24380130295074	100621
18f98b030bce2b9228ffa77702d90089cd4b2f39	on the reliability and intuitiveness of aggregated search metrics	intuitiveness;diversity;reliability;metric;aggregated search;discriminative power;evaluation	Aggregating search results from a variety of diverse verticals such as news, images, videos and Wikipedia into a single interface is a popular web search presentation paradigm. Although several aggregated search (AS) metrics have been proposed to evaluate AS result pages, their properties remain poorly understood. In this paper, we compare the properties of existing AS metrics under the assumptions that (1) queries may have multiple preferred verticals; (2) the likelihood of each vertical preference is available; and (3) the topical relevance assessments of results returned from each vertical is available. We compare a wide range of AS metrics on two test collections. Our main criteria of comparison are (1) discriminative power, which represents the reliability of a metric in comparing the performance of systems, and (2) intuitiveness, which represents how well a metric captures the various key aspects to be measured (i.e. various aspects of a user's perception of AS result pages). Our study shows that the AS metrics that capture key AS components (e.g., vertical selection) have several advantages over other metrics. This work sheds new lights on the further developments and applications of AS metrics.	image;programming paradigm;relevance;web search engine;wikipedia	Ke Zhou;Mounia Lalmas;Tetsuya Sakai;Ronan Cummins;Joemon M. Jose	2013		10.1145/2505515.2505691	metric;evaluation;data mining;reliability;world wide web;information retrieval	Web+IR	-33.83298839237037	-54.812081383190005	100622
e52f482820dbee7196b8e19afdd109d47d751c80	toward understanding novices' search process in programming problem solving		The usage of online search engine is growing rapidly not only in daily life, but also in education. We are interested in understanding what strategies students apply during search, especially the tactics they use to decompose a programming task. In this paper, we report a lab study to investigate students' programming information seeking behavior via Google search engine. Students were given a programming task with limited time, and they were also required to report their online search process including search query and web pages browsed. We analyze the web pages they browsed, model student's behavior, and cluster them into groups with different search tactics. The results show that the web pages they browsed during the task consisted of either conceptual knowledge or coding technical content. The students who performed better would browsed more about conceptual knowledge. Students who set more and smaller unit of subgoals outperformed the students with fewer and larger subgoals.	google search;information seeking behavior;online search;problem solving;web page;web search engine	Yihan Lu;I-Han Hsiao	2017	2017 IEEE Frontiers in Education Conference (FIE)	10.1109/FIE.2017.8190706	sociology;knowledge management;data mining;coding (social sciences);web search query;web page;hidden markov model;information seeking behavior;search engine;online search	Web+IR	-35.92110898622599	-53.14592566729396	100696
28d1cfe208b00eccbda77e7a81535e9118746d22	the telltale dynamic hypertext environment: approaches to scalability	gestion informacion;hipertexto;procesamiento informacion;information retrieval;recherche documentaire;recherche information;information management;information processing;recuperacion documental;document retrieval;recuperacion informacion;information system;gestion information;traitement information;hypertexte;systeme information;hypertext;sistema informacion	Methods and tools for nding documents relevant to a user's needs in document corpora can be found in the information retrieval, library science, and hypertext communities. Typically, these systems provide retrieval capabilities for fairly static corpora, their algorithms are dependent on the language for which they are written, e.g. English, and they don't perform well when presented with misspelled words or text that has been degraded by OCR (optical character recognition) techniques. In this chapter, we present the TELLTALE system. TELLTALE is a dynamic hypertext environment that provides full-text search from a hypertext-style user interface for text corpora that may be garbled by OCR or transmission errors, and that may contain languages other than English by using several techniques based on n-grams (n character sequences of text). In this chapter, we identify methods and techniques that we have applied to the n-gram data structures. We also discuss algorithms that we used to enhance the scalabilty of the TELLTALE Dynamic Hypertext System.	algorithm;computation;data structure;document retrieval;dynamic linker;grams;hypertext;information retrieval;library science;lookup table;n-gram;optical character recognition;scalability;tell-tale;text corpus;user interface	Claudia Pearce;Ethan L. Miller	1997		10.1007/BFb0023962	natural language processing;document retrieval;hypertext;information processing;computer science;database;information management;world wide web;information retrieval;information system	Web+IR	-35.4743781611145	-63.32919633666867	101045
60b483ad0621cfc267a7d204b8346ddb7a080276	viewing stemming as recall enhancement	linguistic stemmer;recall enhancement;average precision;information retrieval;query languages;algorithms;informatics;information analysis;test collection;linguistics	Previous research on stemming has shown both positive and negative effects on retrieval performance. This paper describes an experiment in which several linguistic and non-linguistic stemmers are evaluated on a Dutch test collection. Experiments especially focus on the measurement of Recall. Results show that linguistic stemming restricted to inflection yields a significant improvement over full linguistic and non-linguistic stemming, both in average Precision and R-Recall. Best results are obtained with a linguistic stemmer which is enhanced with compound analysis. This version has a significantly better Recall than a system without stemming, without a significant deterioration of Precision.	experiment;information retrieval;stemming	Wessel Kraaij;Renée Pohlmann	1996		10.1145/243199.243209	natural language processing;speech recognition;computer science;data analysis;informatics;information retrieval;query language	NLP	-33.97182358348995	-63.27765569979846	101098
b80d3f5ea61cba0ce4c076489bdea7bcba6b95ef	document indexing: a concept-based approach to term weight estimation	busqueda informacion;estimacion;cluster;wall street journal;vector space model;amas;indexation automatique;information retrieval;index documentation;weighting;index;indexing terms;term frequency;ponderacion;indexing method;algorithme;algorithm;estimation;recherche information;weighting function;indice;term weighting;indexation;automatic indexing;index weight estimation;monton;ponderation;weight function;classification automatique;automatic classification;experimentation;clasificacion automatica;indizacion automatica;experimentacion;algoritmo	Traditional index weighting approaches for information retrieval from texts depend on the term frequency based analysis of the text contents. A shortcoming of these indexing schemes, which consider only the occurrences of the terms in a document, is that they have some limitations in extracting semantically exact indexes that represent the semantic content of a document. To address this issue, we developed a new indexing formalism that considers not only the terms in a document, but also the concepts. In this approach, concept clusters are defined and a concept vector space model is proposed to represent the semantic importance degrees of lexical items and concepts within a document. Through an experiment on the TREC collection of Wall Street Journal documents, we show that the proposed method outperforms an indexing method based on term frequency (TF), especially in regard to the few highest-ranked documents. Moreover, the index term dimension was 80% lower for the proposed method than for the TF-based method, which is expected to significantly reduce the document search time in a real environment. 2004 Elsevier Ltd. All rights reserved.	human body weight;information retrieval;semantics (computer science);text retrieval conference;tf–idf;the wall street journal	Bo-Yeong Kang;Sang-Jo Lee	2005	Inf. Process. Manage.	10.1016/j.ipm.2004.08.005	weight function;index term;document clustering;computer science;data mining;database;vector space model;information retrieval;statistics	Web+IR	-35.61426724205514	-61.941487069545694	101107
359d2260784bb85fd3a6852f6f9fb837a9f8180b	monitoring task loading with multivariate eeg measures during complex forms of human-computer interaction	computadora;aircraft pilotage;sensitivity and specificity;female;carga mental;mental load;human performance;human computer interaction;females;multivariate analysis;systeme nerveux central;human subject testing;electroencefalografia;neural nets;personal computer;ordinateur;relacion hombre maquina;simulation;general aviation;nasa discipline space human factors;male;hombre;man machine relation;flight;simulacion;clinical trial;electrophysiology;air transportation;attention;encefalo;computer;task difficulty;electroencephalographie;dificultad tarea;non nasa center;vol;sistema nervioso central;males;difficulte tâche;human factors;adults;prospective studies;encephale;support u s gov t non p h s;sensitivity analysis;adult;indexation;cognition;computerized simulation;human;comparative study;electroencephalography methods;pattern recognition;analysis of variance;cognicion;electrofisiologia;relation homme machine;user interfaces computer science;user computer interface;electroencephalography;neural networks computer;flight simulators;vuelo;charge mentale;electrophysiologie;computer simulation;central nervous system;task performance and analysis;human computer interface;homme;multivariate statistical analysis;brain vertebrata	Electroencephalographic (EEG) recordings were made while 16 participants performed versions of a personal-computer-based flight simulation task of low, moderate, or high difficulty. As task difficulty increased, frontal midline theta EEG activity increased and alpha band activity decreased. A participant-specific function that combined multiple EEG features to create a single load index was derived from a sample of each participant's data and then applied to new test data from that participant. Index values were computed for every 4 s of task data. Across participants, mean task load index values increased systematically with increasing task difficulty and differed significantly between the different task versions. Actual or potential applications of this research include the use of multivariate EEG-based methods to monitor task loading during naturalistic computer-based work.	electroencephalography;flight simulator;human–computer interaction;modality (human–computer interaction);personal computer;simulation;test data;version	Michael E. Smith;Alan S. Gevins;Halle Brown;Arati Karnik;Robert Du	2001	Human factors	10.1518/001872001775898287	psychology;computer simulation;prospective cohort study;human performance technology;electrophysiology;simulation;cognition;attention;analysis of variance;pathology;human–computer interaction;electroencephalography;computer science;engineering;artificial intelligence;human factors and ergonomics;central nervous system;comparative research;flight simulator;clinical trial;flight;multivariate analysis;communication;sensitivity analysis;aviation;mechanical engineering	ML	-45.951234021991645	-55.46950856862709	101704
497b5e93ee0ecaa76e361890f26c9f0b3c360b90	estamos juntas: expert system to support the identification and denunciation of violence against women		In relation to alarming rates of feminicide and hospitalization of women due to domestic / familial / conjugal violence in Brazil and the high level of naturalization of violence by society, we see the need for mechanisms to address this phenomenon through prevention. Thus, the objective of this research is to support women in the identification of abusive relationships, through an expert system, as well as to alert about the different manifestations of violence and to inform the procedures for reporting. In order to achieve this goal, a web application called Estamos Juntas has been designed and developed. Preliminary validations have pointed promising results related to the use of expert systems associated with a website to solve the problem.		Giulia Bordignon Silveira;Daniela Scherer dos Santos;Gabriela Felten da Maia	2018		10.1145/3229345.3229381		AI	-45.868299457092206	-64.09237777079346	102069
0b158961549f46d0c962a216b77e196b9fb972ab	semiotic term expansion as the basis for thematic models in narrative systems	qa75 electronic computers computer science	Narratives are a method of communicating information that comes naturally to people and is present in much of our digital and non-digital lives. While work has been undertaken investigating the nature of plot and content within narrative systems little has been done to model subtext or themes. In this thesis a machine understandable thematic model is presented for representing themes within narrative. Each instance of this model forms a definition of a theme and how it may be deconstructed into other thematic elements and their related features. The model is based on semiotic term expansion where terms may be shown to denote motifs which in turn connote themes. An authoring method has been developed to allow for instances of the model to be created. The effectiveness of this approach is demonstrated in four experiments presented within this thesis centred around the concept of creating thematic definitions and generating thematically relevant images. The first experiment explored a semiotic term expansion method for creating thematic definitions in terms of the model and a guide to support authors in doing so. This demonstrated that, though further support for authors is needed, creating valid definitions of themes was possible using the method. The following two experiments used a system called the Thematic Montage Builder; a prototype using definitions of the model to create themed photo montages. The first of these experiments compares the ability of this system to generate montages relevant to specific titles containing themes to Flickr keyword searches while the second compares this system to a term expansion system based on co-occurrence. In both cases the TMB generates montages that are judged by participants to better represent the theme in question. In the final experiment the effect of thematic emphasis on narrative cohesion is investigated. In this experiment a set of variables for measuring narrative cohesion are identified and the impact of using themed illustrations from the TMB on short stories is measured. The illustrations reduced the thematic noise of the short stories and further analysis shows a correlation between thematic cohesion and the perceived `logical sense' and `genre cohesion' of the narratives. This work shows that better machine understandable models of narrative can benefit from an understanding of themes, and that semiotic term expansion may be used to build successful thematic models.	semiotics	Charlie Hargood	2011			computer science;artificial intelligence;multimedia;communication;thematic analysis	NLP	-41.350099318111596	-57.784538541676596	102635
25ee1e16bbad7ce8554e646e0b9555c6d56e5087	image seeking in multilingual environments: a study of the user experience	article δημοσίeυση πeριοδικού	Introduction. There has been considerable activity in the development and testing of multilingual information retrieval systems, but little exploration of how users behave when using such systems. This paper provides a detailed understanding of how one group of users behaved when searching for images in a multilingual environment. #R##N#Method. Observation, retrospective thinking aloud and interview were used to collect user data when searching on FlickLing, an experimental multilingual image retrieval system. Each of the twenty-four participants was required to conduct three searches for images which were described in a foreign language. #R##N#Analysis. Data analysis led to the identification and the coding of users' core actions, along with associated reasons or explanations. #R##N#Results. The investigation led to the description of the 'user experience' as subject to the four key influences of users' knowledge of system, search experience, knowledge of query domain and knowledge of languages prevalent across the actions of the search process and further interrelating with each other. #R##N#Conclusions. The insight from treating information searching as a process and subject to key influences is presented in this paper as a detailed description of the user experience. The resulting user perspective offers insights on the relation held between core factors that influence use of the multilingual information retrieval system.	user experience	Evgenia Vassilakaki;Frances C. Johnson;Richard J. Hartley	2012	Inf. Res.		library science;computer science;multimedia;sociology;world wide web;information retrieval	HCI	-36.453922348076716	-53.06810071064741	103321
209e73ef78f8753ca6270357df2195333bc4467f	effect of verbal comprehension skill and self-reported features on reliability of crowdsourced relevance judgments	reliability;information retrieval evaluation;verbal comprehension;relevance judgment;crowdsourcing	Test collection is extensively used to evaluate information retrieval systems in laboratory-based evaluation experimentation. In a classic setting of a test collection, human assessors involve relevance judgments, which are both a costly and time-consuming task, and scales poorly. Researchers are still being challenged in performing reliable and low-cost evaluation of retrieval systems. Crowdsourcing as a novel method provides a cost-effective and quick solution for creating relevance judgments. However, crowdsourcing comes with the risk of a heterogeneous mass of potential workers who create the relevance judgments with varied levels of accuracy. It is, therefore, essential to understand the factors that affect the reliability of crowdsourced judgments. In this article, we measured various cognitive characteristics of workers, and explored the effects of these characteristics on judgment reliability, in comparison with a human gold standard. We discovered a significant correlation between judgment reliability and the level of verbal comprehension skill. This association conveys an idea for improving the reliability of judgments by discriminating workers into various groups according to their cognitive abilities and to filter out (or to include) certain group(s) of workers. Aside from that, we also discovered a significant association between reliability of judgments and self-reported difficulty of judgment as well as confidence in the task. © 2016 Elsevier Ltd. All rights reserved.	categorization;cognition;content-based image retrieval;crowdsourcing;design of experiments;garbage collection (computer science);information retrieval;list comprehension;relevance feedback;text retrieval conference	Parnia Samimi;Sri Devi Ravana;Yun Sing Koh	2016	Computers in Human Behavior	10.1016/j.chb.2016.07.058	psychology;computer science;data mining;reliability;communication;social psychology;crowdsourcing;statistics	Web+IR	-41.88644203650398	-58.981013587514646	104224
9660ff10c4a49d0104b6118d25ff7b5aa7b64461	relation between psychological amount in polyhedron recognition and 3d object image entropy	entropy	In this paper, the authors examine the relationship between a psychological information amount quantified by humans and the 3D object image entropy representing a physical information amount in relation to the ease of perceiving the shape of a 3D object that varies according to the view direction. They propose a method of using the pair comparison method and Thurstone's law of comparative judgment to construct a psychological information amount representing the psychological amount for which the shape of a 3D object is felt to be easy to perceive. The 3D object image entropy, which is a one-dimensional scale that is defined based on the number of visible surfaces in the 3D object image and the proportions of their surface areas, represents the amount of information that reflects the qualitative characteristic called aspect that the 3D object has and the quantitative characteristic called the proportion of the surface area. The authors limit the target objects to polyhedrons and measure the psychological information amount and 3D object image entropy for various types of shapes to examine the relationship between these kinds of information. Although some shapes were exceptions, the results showed a monotonically increasing relationship between the psychological information amount and the 3D object image entropy for many types of 3D objects. © 2005 Wiley Periodicals, Inc. Syst Comp Jpn, 36(3): 75–84, 2005; Published online in Wiley InterScience (). DOI 10.1002&sol;scj.10526	polyhedron	Hiroyo Ishikawa;Yukio Sato	2005	Systems and Computers in Japan	10.1002/scj.10526	entropy;object model;artificial intelligence;machine learning;mathematics	Robotics	-42.60233865261681	-52.41578851332514	104586
9aa2b83d773f3898014008292e200c6f63710f0f	combining web document representations in a bayesian inference network model using link and content-based evidence	extraction information;web documents;red www;information extraction;analisis estructural;information retrieval;decision bayes;bayesian inference;reseau web;information retrieval model;bayes decision;systeme recherche information;evidential reasoning;reseau bayes;red bayes;recherche information;traitement document;network model;information retrieval systems;bayes network;ranking algorithm;world wide web;document processing;recuperacion informacion;analyse structurale;structural analysis;extraccion informacion;tratamiento documento	This paper introduces an expressive formal Information Retrieval model developed for the Web. It is based on the Bayesian inference network model and views IR as an evidential reasoning process. It supports the explicit combination of multiple Web document representations under a single framework. Information extracted from the content of Web documents and derived from the analysis of the Web link structure is used as source of evidence in support of the ranking algorithm. This content and link-based evidential information is utilised in the generation of the multiple Web document representations used in the combination.	algorithm;backlink;bayesian network;experiment;hyperlink;information retrieval;network model;text corpus;the hub (forum);usb hub;web page;world wide web	Theodora Tsikrika;Mounia Lalmas	2002		10.1007/3-540-45886-7_4	document processing;computer science;network model;social semantic web;pattern recognition;bayesian network;data mining;structural analysis;evidential reasoning approach;bayesian inference;information extraction;information retrieval	Web+IR	-36.02476199802505	-58.744461272878794	104687
13127887ed1185bed05780a6147b85006c776cff	effectiveness of thesauri-aided retrieval	document analysis;average precision;information retrieval;optical character recognition;analyse documentaire;recherche documentaire;indexing;recherche information;indexation;recuperacion documental;reconocimento optico de caracteres;indizacion;analisis documental;automatic query expansion;document retrieval;recuperacion informacion;reconnaissance optique caractere;experience design	In this report, we describe the results of an experiment designed to measure the effects of automatic query expansion on retrieval effectiveness. In particular, we used a collection-specific thesaurus to expand the query by adding synonyms of the searched terms. Our preliminary results show no significant gain in average precision and recall.	information retrieval;precision and recall;query expansion;thesaurus (information retrieval)	Kazem Taghva;Julie Borsack;Allen Condit	1999		10.1117/12.335810	document retrieval;search engine indexing;query expansion;ranking;speech recognition;relevance;cognitive models of information retrieval;experience design;computer science;data mining;database;term discrimination;optical character recognition;world wide web;vector space model;data retrieval;information retrieval;human–computer information retrieval	Web+IR	-34.703927813550905	-62.27918670585022	104721
2436093dfb41ad093500f96728b8ab4fe3c52dcc	an hci method to improve the human performance reduced by local-lag mechanism	human performance;hci;collaborative virtual environments;echo;collaborative virtual environment;local lag mechanism	Local-lag mechanism can maintain consistency for replicated continuous applications, but with a tradeoff of adding delay to local operations. To relieve the negative effects of the delay, this paper proposes an HCI method named echo. With the help of the echo method users can immediately perceive the results of their operations and how large the lag is. In order to evaluate the proposed method, a desktop collaborative virtual environment (CVE) system and a virtual object control task were employed to study the effects of the echo method on human performance (including task completion time, error count, and interaction quality). Experimental results indicate that when the lag exceeds 100 ms the echo method can improve human performance with the effects becoming more evident when a larger lag is used. 2006 Elsevier B.V. All rights reserved.	collaborative virtual environment;common vulnerabilities and exposures;desktop computer;effective method;feedback;human reliability;human–computer interaction;mega man network transmission;pc game;virtual reality	Ling Chen;Gencai Chen;Hong Chen;Jack March;Steve Benford;Zhi-Geng Pan	2007	Interacting with Computers	10.1016/j.intcom.2006.05.009	human performance technology;real-time computing;simulation;human–computer interaction;computer science	HCI	-46.935703395113926	-52.49296792828052	105095
9700fb0f3ed994597b288aab307e01eef34d464b	atypical visual display for monitoring multiple cctv feeds	user experience design;surveillance;serial processing;cctv;cognitive system engineering;visualization;security	Despite advances in surveillance technologies, security and command and control (C2) centers still rely strongly on human operators to detect critical events. Human factors-such as cognitive workload and limited attentional capacity-have been shown to affect operators' ability to detect critical incidents. The current standard surveillance environment comprises a large screen layout that simultaneously displays multiple camera feeds. Although having access to all sources of information at once seems intuitively appealing, there is ample evidence to suggest that it can, in fact, lead to poor detection performance. We propose a design solution that is based on principles grounded in cognitive psychology and user experience design. One key objective is to test empirically whether an atypical design pattern that is consistent with serial cognitive processes induces better performance than the current standard surveillance environment. Three variations of the alternative display pattern will be tested by comparing their effects on detection performance within a surveillance microworld.	closed-circuit television;human factors and ergonomics;software design pattern;user experience design	Serge Pelletier;Joel Suss;François Vachon;Sébastien Tremblay	2015		10.1145/2702613.2732840	computer vision;user experience design;simulation;visualization;human–computer interaction;computer science;operating system;world wide web;computer security;serial memory processing	HCI	-46.72154623546944	-52.87888903410793	105359
2bd58ea7470ee6afd7a6bd3ff6960db44e31c9ef	heart rate measures reflect the interaction of low mental workload and fatigue during driving simulation	workload;operator fatigue;heart rate variability	The objective of this study was to assess the monotonic mental workload under changing conditions of operator fatigue during a night time driver simulation study. Several cardiovascular measures were used in order to differentiate between driving and a continuous tracking task. From all of the standard cardiovascular measures, heart rate in beats per minute emerged as the most sensitive for workload discrimination. Heart rate was higher during driving than during the tracking task, pointing to a slightly higher demanding workload for the driving task. This result was stable over the course of the night and showed only a minimal fatigue influence. Heart rate variability in milliseconds, on the other hand, was on average higher for the continuous tracking task in comparison to the driving. This was especially the case for the sessions with high subjective sleepiness. It can thus be concluded that the fatigue state of the operator was more impaired during the tracking task than during driving.	driving simulator;heart rate variability;performance per watt;simulation	Udo Trutschel;Martin Golz;Christian Heinze;David Sommer;Bill Sirois;David Edwards	2012		10.1145/2390256.2390299	psychology;real-time computing;simulation;operations management	HCI	-47.09460441854446	-52.96407985931836	105392
644a47840ed3f3eb144f3cc48913db570716fee9	filling the gap between researchers studying different materials and different methods: a proposal for structured keywords	structured;lenguaje natural;ontologie;keyword;langage naturel;chercheur;tratamiento lenguaje;palabra clave;methode;mot cle;litterature scientifique;research worker;literatura cientifica;language processing;natural language;traitement langage;materials science;bibliographic approach;ontologia;investigador;metodo;scientific literature;ontology;method;structured keywords;natural language processing	Scientific publications written in natural language still play a central role as our knowledge source. However, due to the flood of publications, obtaining a comprehensive view even on a topic of limited scope, from a stack of publications is becoming an arduous task. Examples are presented from our recent experiences in the materials science field, where information is not shared among researchers studying different materials and different methods. To overcome the limitation, we propose a structured keywords method to reinforce the functionality of a future e-library.	natural language	Yuya Kajikawa;Koji Abe;Suguru Noda	2006	J. Information Science	10.1177/0165551506067125	natural language processing;method;epistemology;computer science;artificial intelligence;ontology;linguistics;natural language	AI	-36.357550983118166	-65.22748413869527	105519
a6dc6c92e280fad3ebf177aac592d33ef6898db3	human error and commercial aviation accidents: an analysis using the human factors analysis and classification system	human factors analysis and classification system;factor humano;civil aviation;aeronautique;aviation safety;regional difference;pedestrian safety;crash causes;poison control;injury prevention;general aviation;accident;hombre;safety literature;traffic safety;injury control;classification;erreur humaine;home safety;aircraft crashes;error analysis;federal aviation administration;injury research;safety abstracts;human factors;error humano;organizational factors;human factor;environment;occupational safety;safety;human;national transportation safety board;safety research;aeronautica;accident prevention;violence prevention;bicycle safety;accidente;commuter airlines;airlines;aeronautics;poisoning prevention;human error;air pilots;falls;supervision;facteur humain;ergonomics;clasificacion;suicide prevention;homme	OBJECTIVE The aim of this study was to extend previous examinations of aviation accidents to include specific aircrew, environmental, supervisory, and organizational factors associated with two types of commercial aviation (air carrier and commuter/ on-demand) accidents using the Human Factors Analysis and Classification System (HFACS).   BACKGROUND HFACS is a theoretically based tool for investigating and analyzing human error associated with accidents and incidents. Previous research has shown that HFACS can be reliably used to identify human factors trends associated with military and general aviation accidents.   METHOD Using data obtained from both the National Transportation Safety Board and the Federal Aviation Administration, 6 pilot-raters classified aircrew, supervisory, organizational, and environmental causal factors associated with 1020 commercial aviation accidents that occurred over a 13-year period.   RESULTS The majority of accident causal factors were attributed to aircrew and the environment, with decidedly fewer associated with supervisory and organizational causes. Comparisons were made between HFACS causal categories and traditional situational variables such as visual conditions, injury severity, and regional differences.   CONCLUSION These data will provide support for the continuation, modification, and/or development of interventions aimed at commercial aviation safety.   APPLICATION HFACS provides a tool for assessing human factors associated with accidents and incidents.	accident analysis;aircraft accidents;aviation;cns disorder;categories;causality;classification;continuation;correlation does not imply causation;hoc (programming language);human error;human factors and ergonomics;human reliability	Scott Shappell;Cristy Detwiler;Kali Holcomb;Carla Hackworth;Albert Boquet;Douglas A. Wiegmann	2007	Human factors	10.1518/001872007X312469	civil aviation;human error;biological classification;engineering;suicide prevention;human factors and ergonomics;injury prevention;aviation safety;transport engineering;natural environment;forensic engineering;computer security;mechanical engineering	HCI	-46.205545306627265	-54.54313469320718	105528
e69fecff975817f62f065ccb233eee1b0e852b5c	search by screenshots for universal article clipping in mobile apps		To address the difficulty in clipping articles from various mobile applications (apps), we propose a novel framework called UniClip, which allows a user to snap a screen of an article to save the whole article in one place. The key task of the framework is search by screenshots, which has three challenges: (1) how to represent a screenshot; (2) how to formulate queries for effective article retrieval; and (3) how to identify the article from search results. We solve these by (1) segmenting a screenshot into structural units called blocks, (2) formulating effective search queries by considering the role of each block, and (3) aggregating the search result lists of multiple queries. To improve efficiency, we also extend our approach with learning-to-rank techniques so that we can find the desired article with only one query. Experimental results show that our approach achieves high retrieval performance (F1 = 0.868), which outperforms baselines based on keyword extraction and chunking methods. Learning-to-rank models improve our approach without learning by about 6%. A user study conducted to investigate the usability of UniClip reveals that ours is preferred by 21 out of 22 participants for its simplicity and effectiveness.	acm transactions on information systems;baseline (configuration management);conditional random field;embedded system;error message;experiment;ground truth;image processing;keyword extraction;learning to rank;map;mobile app;mobile device;quality of results;screenshot;shallow parsing;usability testing;web search engine;web search query;workaround;world wide web	Kazutoshi Umemoto;Ruihua Song;Jian-Yun Nie;Xing Xie;Katsumi Tanaka;Yong Rui	2017	ACM Trans. Inf. Syst.	10.1145/3091107	snap;market segmentation;data mining;information retrieval;keyword extraction;computer science;chunking (psychology);usability;clipping (audio)	Web+IR	-33.72125569268248	-52.35697109727761	105869
9d5b0f8659af02fd1e7d2d4b8e3961e39bdf6325	x2-search: contextual expert search in social networks	online social networkings;practical problems;query processing;search engines;context steiner trees approximation algorithms search problems linkedin algorithm design and analysis;social networking online bibliographies query processing;bibliographies;dblp bibliography data x 2 search contextual expert search online social networking services linked in social structure personal skills contextual knowledge social connections social contexts specialist finding team formation query requirement;expert searches;contextual knowledge;social connection;social network;team formation;effectiveness and efficiencies;social networking online;bibliography datum;artificial intelligence;team formation expert search social network;expert search	Searching experts in online social networking services, such as Linked In, is an important and practical problem which has been studied recently. While existing works rely on simply social structure or only personal skills to locate experts, the contextual knowledge, derived from combining skills with social connections, is missed. For example, one may wish to find experts who master at financial and are well-connected with engineers in the Bay Area. By leveraging the social contexts as the search clues, this work proposes and develops a Contextual Expert Search (X2-Search) system to discover desired experts and teams. X2-Search provides two major functions, Specialist Finding and Team Formation. Given a set of target and context labels of skills, our system aims to return a ranked list of individuals or teams satisfying the query requirement. Experiments conducted on DBLP bibliography data show the promising effectiveness and efficiency of X2-Search. In the application practice, X2-Search system is built on Linked In, and can be extended to the social and expertise data in other domains.	experiment;report;social network;social structure;steiner tree problem	Cheng-Te Li;Man-Kwan Shan	2013	2013 Conference on Technologies and Applications of Artificial Intelligence	10.1109/TAAI.2013.44	computer science;knowledge management;artificial intelligence;data mining	AI	-33.81885117130997	-56.72971266862196	106083
8351e29a7a134476dd2795d91238730d53a0b002	measuring and improving data quality of media collections for professional tasks	personality;group performance;search process;collaborative information seeking	Carrying out research tasks on data collections is hampered, or even made impossible, by data quality issues of different types, such as incompleteness or inconsistency, and severity. We identify research tasks carried out by professional users of data collections that are hampered by inherent quality issues. We investigate what types of issues exist and how they influence these research tasks. To measure the quality perceived by professional users, we develop a quality metric. This allows us to measure the suitability of the data quality for a chosen user task. For a chosen task, we study how the data quality can be improved using crowdsourcing. We validate our quality metric by investigating whether professionals perform better on the chosen research task.	crowdsourcing;data quality	Myriam C. Traub	2014		10.1145/2637002.2637056	data quality;computer science;knowledge management;data mining;personality;world wide web	HCI	-42.53794453585127	-58.65891556823031	106412
a6947cabbadc048b4fbf1937a557d34561da2750	the contactfinder agent: answering bulletin board questions with referrals	communications;information retrieval;computer networks;artificial intelligence;mathematics computers information science management law miscellaneous	ContactFinder is an intelligent agent whose approach to assisting users is valuable and innovative in the following four ways. First, ContactFinder operates proactively in reading and responding to messages on electronic bulletin boards rather than acting in response to user queries. Second, ContactFinder assists users by referring them to other people who can help them, rather than attempting to find information that directly answers the user’s specific question. Third, ContactFinder categorizes messages and extracts their topic areas using a set of heuristics that are very efficient and demonstrably highly effective. Fourth, ContactFinder posts its referrals back to the bulletin boards rather than simply communicating with specific users, to increase the information density and connectivity of the system. This paper discusses these aspects of the system and demonstrates their effectiveness in over six months of use on a large-scale internal bulletin board. 1. Electronic information systems The explosive growth of the Internet by individuals and corporations, and the growing use of corporate information repositories based on Internet technology or systems such as Lotus NotesTM, has led to an unprecedented number of people using network-based systems for finding solutions to problems. In addition to document browsing systems such as the Internet’s World Wide Web, a continuing interest remains in electronic bulletin board systems that allow large numbers of distributed users discuss issues, ask questions, and give answers. Unfortunately, a number of operational problems make it difficult to get high quality, fast answers to questions on bulletin boards, or to search bulletin boards for previous messages that can help solve a problem. First, the explosion in the number of bulletin boards makes it less likely that true experts will read any single bulletin board on a very frequent basis. Second, the increased volume of messages on these systems leads to more frequent routine deletion or migration of messages. For a user trying to find a quick solution to a problem, bulletin boards can be as frustrating as they are useful. This paper describes an intelligent agent called ContactFinder, that has been developed to address this problem. ContactFinder is similar to intelligent agents under development for question answering [Hammond et. al., 19951, e-mail filtering [Maes and Kozierok, 1993; Lashkari et. al., 19941, Usenet message filtering [Sheth, 19941, or other information search and retrieval domains [Holte and Drummond, 1994; Knoblock and Arens, 1994; Levy et. al., 1994; Pazzani et. al., 1996; Krulwich and Burkey, 19961. Like these other systems, ContactFinder extracts information from a large number of documents in order to present it to users in a more focused and productive fashion. Unlike these previous approaches, however, ContactFinder’s task is not to present the user with a subset of the information that can be used directly in problem solving. The agent instead keeps track of people who are key contacts in various topic areas, and helps questionaskers by referring them to the appropriate key contact. More specifically, ContactFinder monitors the bulletin board for indications of message authors who are key contacts in some specific area, and stores these contacts for later use. The agent simultaneously watches for questions, and responds to the questions with a referral. This is a very valuable function for an intelligent agent to perform for several reasons. First, an agent that attempts to provide information that is directly relevant to the user’s goals will always be limited by the information that is available. While this is not a problem in solving problems that are very basic or frequently asked [Hammond et. al., 19951, it may make it difficult to be helpful in novel or very focused situations. In such a situation, however, a referral to a human contact will be available more often [Kantz and Selman, 19961. As we discuss in detail in section 5, ContactFinder has been able to make a referral for over 13% of the questions in a large-scale technology-related bulletin board, most of which were for very specific questions. Second, extracting contacts and facilitating human expertise transfer fits very well into current work styles and facilitates good learning from bulletin boards, which make the system easier to apply and test. 10 Agents From: AAAI-96 Proceedings. Copyright © 1996, AAAI (www.aaai.org). All rights reserved.	display resolution;email filtering;fits;heuristic (computer science);information design;information system;intelligent agent;internet;lotus 1-2-3;problem solving;question answering;usenet;world wide web	Bruce Krulwich;Chad Burkey	1996			computer science;artificial intelligence;machine learning;multimedia;world wide web	AI	-35.30686721827501	-56.183097116900676	106499
959ffb66477898a565b52a247888c9cb416a4523	the research on the detection of noteworthy symptom descriptions	text mining;sentiment analysis;medical symptoms;noteworthy messages	The advance of mobile devices and communication technologies enable patients to communicate with their doctors in a more convenient way. We have developed an App that allows patients to record their symptoms and submit them to their doctors. Physicians can keep track of patients’ conditions by looking at the self-report messages. Nevertheless, physicians are usually busy and may be overwhelmed by the large amount of incoming messages. As a result, critical messages may not receive immediate attentions, and patient care is compromised. It is imperative to identify the messages that require physicians’ attention, called noteworthy messages. In this research, we propose an approach that applies text-mining technologies to identify medical symptoms conveyed in the messages and their associated sentiment orientation, as well as other factors. Noteworthy messages are subsequently characterized by symptom sentiment and symptom change features. We then construct a prediction model to identify messages that are noteworthy to the physicians. We show from our experiments using data collected from a teaching hospital in Taiwan that the different features have different degrees of impact on the performance of the prediction model, and our proposed approach can effectively identify noteworthy messages.	experiment;imperative programming;mobile device;text mining	Yu-Ling Chen;Shanlin Chang;San-Yih Hwang;Kai-Sheng Hsieh	2015			text mining;computer science;data mining;world wide web;information retrieval;sentiment analysis	NLP	-46.3463792666342	-62.704127708179946	106988
50399e6357eeb40898fcf866dfe5f40c88ba6a29	a comparative study of the effectiveness of sentiment tools and human coding in sarcasm detection		PurposernrnrnrnrnSarcasm is often used in everyday speech and writing and is prevalent in online contexts. The purpose of this paper is to investigate the analogy between sarcasm comments from sentiment tools and the human coder.rnrnrnrnrnDesign/methodology/approachrnrnrnrnrnUsing the Verbal Irony Procedure, eight human coders were engaged to analyse comments collected from an online commercial page, and a dissimilarity analysis was conducted with sentiment tools. Three constants were tested, namely, polarity from sentiment tools, polarity rating by human coders; and sarcasm-level ratings by human coders.rnrnrnrnrnFindingsrnrnrnrnrnResults found an inconsistent ratio between these three constants. Sentiment tools used did not have the capability or reliability to detect the subtle, contextualized meanings of sarcasm statements that human coders could detect. Further research is required to refine the sentiment tools to enhance their sensitivity and capability.rnrnrnrnrnPractical implicationsrnrnrnrnrnWith these findings, it is recommended that further research and commercialization efforts be directed at improving current sentiment tools – for example, to incorporate sophisticated human sarcasm texts in their analytical systems. Sarcasm exists frequently in media, politics and human forms of communications in society. Therefore, more highly sophisticated sentiment tools with the abilities to detect human sarcasm would be vital in research and industry.rnrnrnrnrnSocial implicationsrnrnrnrnrnThe findings suggest that presently, of the sentiment tools investigated, most are still unable to pick up subtle contexts within the text which can reverse or change the message that the writer intends to send to his/her receiver. Hence, the use of the relevant hashtags (e.g. #sarcasm; #irony) are of fundamental importance in detection tools. This would aid the evaluation of product reviews online for commercial usage.rnrnrnrnrnOriginality/valuernrnrnrnrnThe value of this study lies in its original, empirical findings on the inconsistencies between sentiment tools and human coders in sarcasm detection. The current study proves these inconsistencies are detected between human and sentiment tools in social media texts and points to the inadequacies of current sentiment tools. With these findings, it is recommended that further research and commercialization efforts be directed at improving current sentiment tools – to incorporate sophisticated human sarcasm texts in their analytical systems. The system can then be used as a reference for psychologists, media analysts, researchers and speech writers to detect cues in the inconsistencies in behaviour and language.		Phoey Lee Teh;Pei Boon Ooi;Nee Nee Chan;Yee Kang Chuah	2018	J. Systems and IT	10.1108/JSIT-12-2017-0120	management science;coding (social sciences);sarcasm;irony;computer science;commercialization;social media;originality;analogy	SE	-44.3872980249793	-61.170341590873	107231
872f9322705e36cc278fb14ae261f196ebc24927	a flexible delegation-type interface enhances system performance in human supervision of multiple robots: empirical studies with roboflag	automatic control;analytical models;unmanned vehicles automation delegation human robot interaction playbook;empirical study;computational analysis;roboflag;multiple unmanned vehicles;system performance robots robotics and automation computer interfaces computational modeling analytical models vehicles automatic control human factors testing;mobile robots;task network modeling;testing;remotely operated vehicles;human robot interaction;indexing terms;system performance;delegation;unmanned vehicles;empirical evidence;human robot interface flexible delegation type interface multiple robots roboflag computational analysis multiple unmanned vehicles task network modeling monte carlo simulation;computational modeling;human factors;levels of abstraction;network model;robots;multi robot systems;egat;multiple robots;man machine systems user interfaces multi robot systems remotely operated vehicles telerobotics mobile robots monte carlo methods;telerobotics;robot interaction playbook;vehicles;computer analysis;playbook;monte carlo simulation;computer interfaces;user interfaces;man machine systems;unmanned vehicles automation delegation human 8211;robotics and automation;autonomous robot;monte carlo methods;human robot interface;flexible delegation type interface;automation	"""Three experiments and a computational analysis were conducted to investigate the effects of a delegation-type interface on human supervision of simulated multiple unmanned vehicles. Participants supervised up to eight robots using automated behaviors (""""plays""""), manual (waypoint) control, or both to capture the flag of an opponent with an equal number of robots, using a simple form of a delegation-type interface, Playbook. Experiment 1 showed that the delegation interface increased mission success rate and reduced mission completion time when the opponent """"posture"""" was unpredictably offensive or defensive. Experiment 2 showed that performance was superior when operators could flexibly use both automated behaviors and manual control, although there was a small increase in subjective workload. Experiment 3 investigated additional dimensions of flexibility by comparing delegation interfaces to restricted interfaces. Eight interfaces were tested, varying in the level of abstraction at which robot behavior could be tasked and the level of aggregation (single or multiple robots) to which plays could be assigned. Performance was superior with flexible interfaces for four robots, but this benefit was eliminated when eight robots had to be supervised. Finally, a computational analysis using task-network modeling and Monte Carlo simulation gave results that closely paralleled the empirical data on changes in workload across interface type. The results provide initial empirical evidence for the efficacy of delegation-type interfaces in human supervision of a team of multiple autonomous robots."""	autonomous robot;capture the flag;computation;experiment;human–robot interaction;monte carlo method;poor posture;principle of abstraction;simulation;unmanned aerial vehicle;waypoint	Raja Parasuraman;Scott Galster;Peter Squire;H. Furukawa;C. A. Miller	2005	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2005.850598	human–robot interaction;embedded system;simulation;computer science;artificial intelligence;monte carlo method	Robotics	-45.40529296876249	-54.02590562615272	108163
09e9259553e24469019914ffa4b143a6e55f472b	evaluating wordnets in cross-language information retrieval: the item search engine	indexation;indexing terms;natural language processing;search engine;morphological analysis	This paper presents the ITEM multilingual search engine. This search engine performs full lexical processing (morphological analysis, tagging and Word Sense Disambiguation) on documents and queries in order to provide language-neutral indexes for querying and retrieval. The indexing terms are the EuroWordNet/ITEM InterLingual Index records that link wordnets in 10 languages of the European Community (the search engine currently supports Spanish, English and Catalan). The goal of this application is to provide a way of comparing in context the behavior of different Natural Language Processing strategies for Cross-Language Information Retrieval (CLIR) and, in particular, different Word Sense Disambiguation strategies for query translation and conceptual indexing.	cross-language information retrieval;eurowordnet;natural language processing;web search engine;word sense;word-sense disambiguation;wordnet	M. Felisa Verdejo;Julio Gonzalo;Anselmo Peñas;Fernando López-Ostenero;David Fernández-Amorós	2000			concept search;query expansion;natural language processing;speech recognition;human–computer information retrieval;artificial intelligence;semeval;cross-language information retrieval;information retrieval;search engine;search engine indexing;question answering;computer science	Web+IR	-33.9223405888262	-63.43311598963853	108374
4a2676b1568279db5b6c29b03c952b91c2a4be26	assigned tasks are not the same as self-chosen web search tasks	information retrieval;user interfaces information retrieval;user behavior web search tasks short assigned question answering style task;web search;web search information analysis search engines pattern analysis usability system testing user interfaces laboratories probes educational institutions;user behavior;user interfaces;question answering	"""Short assigned question-answering style tasks are often used as a probe to understand how users do search. While such assigned tasks are simple to test and are effective at eliciting the particulars of a given search capability, they are not the same as naturalistic searches. We studied the quantitative differences between assigned tasks and self-chosen """"own"""" tasks finding that users behave differently when doing their own tasks, staying longer on the task, but making fewer queries and different kinds of queries overall. This finding implies that user's own tasks should be used when testing user behavior in addition to assigned tasks, which remain useful for feature testing in lab settings"""	question answering;web search engine	Daniel M. Russell;Carrie Grimes	2007	2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)	10.1109/HICSS.2007.91	question answering;computer science;database;multimedia;user interface;world wide web;information retrieval	Robotics	-34.74932235334446	-52.81970396063539	108565
1fc58482e394c7f508e89e8c7b7593c8c7bf9139	other spaces for spatial hypertext	spatial hypertext;informacion documentacion;ciencias sociales	Spatial hypertext programs typically display a portion of a larger flat space in which items can be arranged, grouped, and manipulated. In terms of an old philosophical dispute, the space of spatial hypertext is a Newtonian absolute space rather than the Leibnizian relational space created by web pages and other node-and-link hypertexts. After discussing this difference, this essay proposes other spaces, including topologically complex spaces and an Aristotelian space that is polarized and oriented. These offer new possibilities for information triage and meaningful spatial configurations.	hypertext;spaces	David Kolb	2009	J. Digit. Inf.		computer science;artificial intelligence;theoretical computer science;world wide web	DB	-39.93476115749521	-56.786329061320224	109115
70b1735b2955e81381d9b61be1ca560d619a11c2	partition-based field normalization: an approach to highly specialized publication records	field partition;individual research performance;field normalization;bibliometric indicators;partition based field normalization	Field normalized citation rates are well-established indicators for research performance from the broadest aggregation levels such as countries, down to institutes and research teams. When applied to still more specialized publication sets at the level of individual scientists, also a more accurate delimitation is required of the reference domain that provides the expectations to which a performance is compared. This necessity for sharper accuracy challenges standard methodology based on pre-defined subject categories. This paper proposes a way to define a reference domain that is more strongly delimited than in standard methodology, by building it up out of cells of the partition created by the pre-defined subject categories and their intersections. This partition approach can be applied to different existing field normalization variants. The resulting reference domain lies between those generated by standard field normalization and journal normalization. Examples based on fictive and real publication records illustrate how the potential impact on results can exceed or be smaller than the effect of other currently debated normalization variants, depending on the case studied. The proposed Partition-based Field Normalization is expected to offer advantages in particular at the level of individual scientists and other very specific publication records, such as publication output from interdisciplinary research.		Nadine Rons	2012	J. Informetrics	10.1016/j.joi.2011.09.008	computer science;data science;data mining	ML	-42.99145338857913	-64.99779242648825	109720
69d5620176af3f8ad954b5b3a8aec123bbf23a40	feature selection by fuzzy inference and its application to spam-mail filtering	utilisation information;filtering;filtrage;uso informacion;text;electronic mail;donnee textuelle;web pages;two phase medium;methode essai;red www;securite;information use;dato textual;taux erreur;medio difasico;filtrado;logique floue;reseau web;milieu diphasique;logica difusa;llamada;inference mechanisms;correo electronico;intelligence artificielle;texte;fuzzy logic;internet;filter;safety;fuzzy inference;inferencia;textual data;recall;filtre;error rate;chi square test;artificial intelligence;world wide web;feature selection;inteligencia artificial;test method;indice error;texto;seguridad;rappel;information gain;filtro;inference;mecanisme inferentiel;courriel;metodo ensayo	We present a feature selection method by fuzzy inference and its application to spam-mail filtering in this work. The proposed fuzzy inference method outperforms information gain and chi squared test methods as a feature selection method in terms of error rate. In the case of junk mails, since the mail body has little text information, it provides insufficient hints to distinguish spam mails from legitimate ones. To address this problem, we follow hyperlinks contained in the email body, fetch contents of a remote web page, and extract hints from both original email body and fetched web pages. A two-phase approach is applied to filter spam mails in which definite hint is used first, and then less definite textual information is used. In our experiment, the proposed two-phase method achieved an improvement of recall by 32.4% on the average over the 1st phase or the 2nd phase only works.	feature selection;fuzzy logic;spamming	Jong-Wan Kim;Sin-Jae Kang	2005		10.1007/11596448_52	fuzzy logic;filter;the internet;chi-square test;filter;word error rate;computer science;artificial intelligence;machine learning;web page;data mining;database;recall;kullback–leibler divergence;test method;feature selection;world wide web;algorithm;statistics	NLP	-36.47768375549429	-58.76623854622007	110046
eb7e220c1caec33f063c81fe9108ecafff9fec26	indexing aids at corporate websites: the use of robots.txt and meta tags	site web;document access;search engine;corporations;entreprise;buscador;keywords;metadata;indexation automatique;information retrieval;empresa;corporate web sites;internet;indexing;web indexing;recherche information;indexation;firm;indizacion;metadonnee;automatic indexing;world wide web;metadatos;recuperacion informacion;moteur recherche;sitio web;web site;indizacion automatica	"""Sixty corporate websites selected from the Fortune Global 500 companies were examined in 2000 and again in 2001 to see if they provided support for automatic indexing. In particular, use of the robots.txt and Meta tags for """"keywords"""" and """"description"""" was examined. Slightly fewer than half of the sites provided one or both of these aids. Among sites providing indexing aids there was a clear under-representation of Asian sites. Nearly 80% of the sites used Java, suggesting a reasonable level of technical sophistication among website creators. About one-third of the sites used cookies, raising the possibility that repeat visitors might find the navigation of the site customized to their needs. Overall an increase in the use of indexing aids, especially Meta tags, represents one way in which web robots could index sites more quickly and thus improve overall index coverage of the web."""	meta element;robots exclusion standard	M. Carl Drott	2002	Inf. Process. Manage.	10.1016/S0306-4573(01)00039-5	search engine indexing;the internet;computer science;data mining;metadata;world wide web;information retrieval;search engine	HCI	-35.872902904194554	-60.94828607596696	110532
c82e24e8ccfb508347d713d99eb1080c7d6c94c3	position paper: promoting user engagement and learning in search tasks by effective document representation.		Much research in information retrieval (IR) focuses on optimization of the rank of relevant retrieval results for single shot ad hoc IR tasks with straightforward information needs. Relatively little research has been carried out to study and support user learning and engagement for more complex search tasks. We introduce an approach intended to improve topical knowledge of a user while undertaking IR tasks. Specifically, we propose to explore methods of finding useful and informative textual units (semantic concepts) within retrieved documents, with the objective of creating improved document surrogates for presentation within the search process. We hypothesize that this strategy will promote improved implicit learning within search activities. We believe that the richer document representations proposed in the paper would help to promote engagement, understanding and learning as compared to more traditional search engine document snippets. We propose a framework for holistic evaluation of our proposed document representations and their use in search.	document;hoc (programming language);holism;information needs;information retrieval;mathematical optimization;surrogates;web search engine	Piyush Arora;Gareth J. F. Jones	2016			computer science;user requirements document;multimedia;world wide web;information retrieval	Web+IR	-34.54552618519986	-54.4727859329709	110844
d11923075c47f8fd2cee47b8d6ae2ad0e7c966e8	a survey on question answering technology from an information retrieval perspective	natural language interfaces;information retrieval;retrieval and ranking models;natural language;part of speech tagging;natural language interface;retrieval model;discourse analysis;bag of words;question answering;knowledge base	This article provides a comprehensive and comparative overview of question answering technology. It presents the question answering task from an information retrieval perspective and emphasises the importance of retrieval models, i.e., representations of queries and information documents, and retrieval functions which are used for estimating the relevance between a query and an answer candidate. The survey suggests a general question answering architecture that steadily increases the complexity of the representation level of questions and information objects. On the one hand, natural language queries are reduced to keyword-based searches, on the other hand, knowledge bases are queried with structured or logical queries obtained from the natural language questions, and answers are obtained through reasoning. We discuss different levels of processing yielding bagof-words-based and more complex representations integrating part-of-speech tags, classification of the expected answer type, semantic roles, discourse analysis, translation into a SQL-like language and logical representations. 2011 Elsevier Inc. All rights reserved.	information retrieval;mental representation;natural language;part-of-speech tagging;question answering;relevance;sql	Oleksandr Kolomiyets;Marie-Francine Moens	2011	Inf. Sci.	10.1016/j.ins.2011.07.047	natural language processing;knowledge base;universal networking language;question answering;cognitive models of information retrieval;natural language user interface;computer science;bag-of-words model;discourse analysis;data mining;natural language;information retrieval;human–computer information retrieval	Web+IR	-33.94571113553587	-63.01476634325809	111197
2437710eff33cdbd11e8785f6b160226012c3075	reproduction of experiments in recommender systems evaluation based on explanations		The offline evaluation of recommender systems is typically based on accuracy metrics such as the Mean Absolute Error (MAE) and the Root Mean Squared Error (RMSE), while on the other hand Precision and Recall is used to measure the quality of the top-N recommendations. However, it is difficult to reproduce the results since there are different libraries that can be used for running experiments and also within the same library there are many different settings that if not taken into consideration when replicating the result might vary. In this paper, we show that it is challenging to reproduce results using a different library but with the use of the same library an explanation based approach can be used to assist in the reproducibility of experiments. Our proposed approach has been experimentally evaluated using a real dataset and the results show that it is both practical and effective.	recommender system	Nikolaos Polatidis;Elias Pimenidis	2018		10.1007/978-3-319-98204-5_16	statistics;recommender system;precision and recall;mean absolute error;computer science;mean squared error	NLP	-37.523053027417845	-54.85975399459383	111253
5e666e5b7291ab84fe3ad09cde2cadb9c774aece	modelling the effect of human anticipation on driving maneuvers in lane changing process		Drivers depend on anticipating ability controlling their vehicles to avoid collision and unnecessary speed loss. This study intended to identify how anticipating ability works and affects drivers' behaviors in lane changing. Lane changing driver and the immediate car following driver were assumed to adjust their maneuvers based on evaluations of current driving condition and anticipation of surrounding vehicles' future movements. Drivers' anticipation was abstracted as latent variable. Its hypothetic relationships with external stimulus the drivers perceive and their responses were formulated under the framework of structural equation model. The model was estimated based on the vehicle trajectory and field observation data. The influence transmission paths of external stimuli to adjusted driving behaviors in virtue of the anticipations were identified. Results show that both strategic lane changing type and speed of lane changing vehicle have significant influences on subject driver's anticipation. Other stimulus, like vehicle gap or speed difference at start of lane changing period, could affect drivers' anticipation of specific driving relationship. The influencing degree of a stimulus can be calculated based on the estimated path effects. The findings of this study could be referred when develop the algorithms of microscopic traffic simulation and autonomous vehicle control.	algorithm;autonomous robot;cross-sectional data;device driver;emulator;latent variable;linear programming relaxation;simulation;structural equation modeling	Li Li;Yun-Tao Chang;Dong Zhang;Hong-Feng Xu	2017	2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2017.8317698	simulation;anticipation;structural equation modeling;trajectory;engineering;latent variable;traffic simulation;transmission (mechanics)	Robotics	-47.78425250727929	-52.68489599479011	112082
43e957ab993aa408457cce26d0fbe7572199ffb6	supporting collaborative hierarchical classification: bookmarks as an example	filtering;filtrage;social bookmarking;bookmark;red www;hierarchized structure;implementation;filtrado;signet;reseau web;structure hierarchisee;prototipo;vecino mas cercano;large scale;hierarchical classification;marcador;recommender system;collaborative filtering;signal classification;classification hierarchique;classification signal;plus proche voisin;world wide web;nearest neighbour;classification automatique;implementacion;nearest neighbor classifier;automatic classification;clasificacion automatica;clasificacion jerarquizada;prototype;recommender systems;estructura jerarquizada;bookmark classification;www	Bookmarks (or Favorites, Hotlists) are a popular strategy to relocate interesting websites on the WWW by creating a personalized URL repository. Most current browsers offer a facility to locally store and manage bookmarks in a hierarchy of folders; though, with growing size, users reportedly have trouble to create and maintain a stable organization structure. This paper presents a novel collaborative approach to ease bookmark management, especially the “classification” of new bookmarks into a folder. We propose a methodology to realize the collaborative classification idea of considering how similar users have classified a bookmark. A combination of nearest-neighbour-classifiers is used to derive a recommendation from similar users on where to store a new bookmark. A prototype system called CariBo has been implemented as a plugin of the central bookmark server software SiteBar. All findings have been evaluated on a large scale real user dataset with promising results, and possible implications for shared and social bookmarking systems are discussed.	access control;bookmark (world wide web);cluster analysis;cold start;information source;k-nearest neighbors algorithm;microsoft outlook for mac;naive bayes classifier;prototype;recommender system;sitebar;synergy;taxonomy (general);vanity domain;www	Dominik Benz;Karen H. L. Tso-Sutter;Lars Schmidt-Thieme	2007	Computer Networks	10.1016/j.comnet.2007.06.014	filter;computer science;collaborative filtering;data mining;database;prototype;implementation;world wide web;recommender system	HCI	-36.30302224090015	-57.753430814251566	112226
524fd6d2bd91bc9fc6ff7f4ee19e52f652718644	best-worst scaling more reliable than rating scales: a case study on sentiment intensity annotation		Rating scales are a widely used method for data annotation; however, they present several challenges, such as difficulty in maintaining interand intra-annotator consistency. Best–worst scaling (BWS) is an alternative method of annotation that is claimed to produce high-quality annotations while keeping the required number of annotations similar to that of rating scales. However, the veracity of this claim has never been systematically established. Here for the first time, we set up an experiment that directly compares the rating scale method with BWS. We show that with the same total number of annotations, BWS produces significantly more reliable results than the rating scale.		Svetlana Kiritchenko;Saif Mohammad	2017		10.18653/v1/P17-2074	natural language processing;data annotation;artificial intelligence;best–worst scaling;computer science;information retrieval;scaling;rating scale;annotation	Web+IR	-38.98637456793772	-64.253551392424	112554
f17cc3f0a827e1e337aadd5b44d965332e2b2b47	zipf's law and the frequency of characters or words of oracles.		—The article discusses the frequency of characters of Oracle,concluding that the frequency and the rank of a word or character is fit to Zipf-Mandelboit Law or Zipf's law with three parameters,and figuring out the parameters based on the frequency,and pointing out that what some researchers of Oracle call the assembling on the two ends is just a description by their impression about the Oracle data.	zipf's law	Xiuli Wang	2014	CoRR		figuring;artificial intelligence;machine learning;computer science;oracle;zipf's law	NLP	-38.38424900567586	-63.007275784583214	112555
189e82e97914c5524075cbb8f84c6b5801ade335	a study of the overlap among document representations	difference set;document retrieval	Most previous investigations comparing the performance of different representations have used recall and precision as performance measures. However, there is evidence to show that these measures are insensitive to an important difference between representations. To explain, two representations may perform similarly on these measures, while retrieving very different sets of documents. Equivalence of representations should be decided on the basis of similarity in performance and similarity in the documents retrieved. This study compared the performance of four representations in the PsycAbs database. In addition, overlap between retrieved sets was also computed where overlap is the proportion of retrieved documents that are the same for pairs of document representations. Results indicate that for any two representations considered, performance values differed slightly while overlap scores were also low, thus supporting the evidence that recall and precision as performance measures mask differences between the sets of retrieved documents. Results are interpreted to propose an optimal ordering of the representations and to examine the contribution of each representation given this combination.	overlap–add method;precision and recall;turing completeness	Jeffrey Katzer;Judith A. Tessier;William B. Frakes;Padmini Das-Gupta	1983	SIGIR Forum	10.1145/3130348.3130352	document retrieval;computer science;pattern recognition;data mining;world wide web;information retrieval;difference set;statistics	Web+IR	-33.77497446361298	-61.60837827664565	112585
8ba0108c2ce2c4613d64801c4523ca3438329879	training effective human performance in the management of stressful emergencies	emergency response;human performance;system approach;pedestrian safety;poison control;injury prevention;uncertainty management;safety literature;traffic safety;injury control;home safety;injury research;safety abstracts;human factors;cognitive strategies;occupational safety;safety;safety research;accident prevention;violence prevention;bicycle safety;poisoning prevention;human error;falls;ergonomics;suicide prevention	Emergency situations in industry occur suddenly and often unexpectedly; operators must make critical decisions under stress, and the consequences of errors can be immediate and catastrophic. Training effective performance under stress becomes an important aspect in the management of emergencies. This article proposes a taxonomy of cognitive strategies that enable operators to regulate their thinking and adapt decisions to changes in the demands of the situation. Cognitive strategies, such as, tolerating uncertainty, managing workload, planning for contingencies, and self-monitoring, provide the content of training emergency response. Stress, however, interferes with the learning of strategies, which presents a challenge to the design of training methods. Exposure to stress during training, degree of task decomposition, guidance, contextual variety and feedback are some of the training methods explored to facilitate the acquisition and transfer of cognitive strategies. Diversions from the traditional systems approach to training are pointed out and areas for further training research are identified.	taxonomy (general)	Tom Kontogiannis	1999	Cognition, Technology & Work	10.1007/s101110050007	human performance technology;simulation;human error;medicine;environmental health;engineering;suicide prevention;human factors and ergonomics;injury prevention;forensic engineering;computer security;mechanical engineering	HCI	-48.128340844919734	-55.37134237266743	112858
d1d565418d3772299c6bafe19af0b9b740405622	knowledge-based clustering scheme for collection management and retrieval of library books	bibliotheque;library books;agregacion;collection management;systeme documentaire;information retrieval;base connaissance;classification;aggregation;similitude;domain knowledge;recherche documentaire;recherche information;sistema recuperacion documental;document retrieval system;recuperacion documental;similarity;agregation;base conocimiento;document retrieval;recuperacion informacion;similitud;biblioteca;similarity measure;clasificacion;library;knowledge base	Abstract   We propose a knowledge-based clustering scheme for grouping books in a library. Such a grouping is achieved with the help of domain knowledge in the form of the ACM CR (Computing Reviews) category hierarchy. A new knowledge-based similarity measure is defined and used in clustering books. The proposed scheme is useful in overcoming several problems associated with the existing book collection management and document retrieval systems. More specifically, it can be used in: (1) helping the user select an appropriate collection of books in a library which contains the topics of interest; (2) assigning a classification number to a new book; (3) designing a more appropriate and uniform classification scheme for books; and (4) comparison of libraries based on their collections. Initial experiments on a collection of hundred books using the proposed clustering scheme have given us encouraging results.	book;cluster analysis	M. Narasimha Murty;Anil K. Jain	1995	Pattern Recognition	10.1016/0031-3203(94)00173-J	document retrieval;knowledge base;similarity;library;biological classification;computer science;similitude;data mining;world wide web;information retrieval;domain knowledge	Vision	-35.08706153816199	-60.36073511591131	113230
c82e5de8a89d8fd4ef7a6f0eace6d74bba3b9475	novel human-centered rehabilitation robot with biofeedback for training and assessment	biofeedback;rehabilitation robot;control strategy;human centered	"""We present the novel human-centered rehabilitation methods from the research as well as literature to provide the robot assisted rehabilitation control strategies and motor function assessment methods. The research is based on the upper extremity compound movements (UECM) rehabilitation training robot [1], which is applied to the rehabilitation of upper extremity functions in patients with movement disorders. So called """"human-centered"""" [2]or """"patient-cooperative"""" strategies can take into account the patient's individual situations, intentions and efforts rather than imposing predefined instructions. It is considered that such robot-assisted methods can improve the therapeutic outcome compared to classical rehabilitation methods."""	robot	Runze Yang;Linhong Ji;Hongwei Chen	2011		10.1007/978-3-642-21657-2_51	psychology;simulation;physical medicine and rehabilitation;physical therapy	Robotics	-44.26764853408389	-53.1115753159225	113253
c5802e21ac986e650795d607569c3a6410bf145b	certain-uncertain, true-false, good-evil in italian political speeches	political speeches;truth falsehood;certainty uncertainty;good evil	The present study aimed at analyzing the communication of Certainty and Uncertainty in a corpus of Italian political speeches pre-election rallies and parliamentary speeches broadcast on television. The qualitative and quantitative analysis of the corpus shows that 1 in both kinds of speech the communication of Certainty clearly prevails over the Uncertainty; 2 the latter, even if it is at a low level, is slightly higher in parliamentary speeches than in pre-election rallies; 3 in the whole corpus a contraposition constantly occurs also between Good/Evil and True/False: every speaker, regardless of his faction, describes his own party as positive and truthful Good and True and the opposition party as negative and untruthful Evil and False.		Ramona Bongelli;Ilaria Riccioni;Andrzej Zuczkowski	2010		10.1007/978-3-642-41545-6_13	political science;communication;social psychology	NLP	-44.121467232352295	-61.61292227074685	113338
9b09bdbd0a22ffe2ba531aabeb419590415f1100	estudio de fiabilidad y viabilidad de la web 2.0 y la web semántica para enriquecer lexicones en el dominio farmacológico	enhancement;computacion informatica;filologias;lexicons;info eu repo semantics article;farmacologia;informacion documentacion;reconocimiento de entidades nombradas;linguistica;named entity recognition;lexicones;ciencias basicas y experimentales;web semantica;pharmacology;web 2 0;semantic web;grupo a;ciencias sociales;enriquecimiento;grupo b	Nowadays Named Entity Recognition systems in the pharmacological domain, which are needed to help healthcare professional during pharmacological treatment prescription, suffer limitations related to the lack of coverage in official databases. Therefore, it seems necessary to analyse the reliability of existing resources, both in the Semantic Web and Web 2.0, and determine whether it is feasible or not to use these resources for additional information to generate and/or enhance lexicons used by Named Entity Recognition systems. For this reason, this paper analyses the main sources of information related to the pharmacological domain available on the Internet. This analysis leads to the conclusion that there is reliable information and it would enhance existing lexicons with synonyms, variations and even historical information not collected or maintained in official databases.	database;internet;lexicon;linear algebra;named entity;named-entity recognition;semantic web;web 2.0	Isabel Moreno;Paloma Moreda;María Teresa Romá-Ferri	2015	Procesamiento del Lenguaje Natural		computer science;semantic web;web 2.0;world wide web	Web+IR	-36.45398945414373	-64.75928254072576	114126
49e5022593a5a8945975ca9e9af9eb5eac02a8db	bridging lexical gaps between queries and questions on large online q&a collections with compact translation models	common word;retrieval purpose;compact translation model;retrieval performance;translation model;collection show;major issue;lexical gap;large online question;question retrieval;unimportant word;empirical method;noise control	Lexical gaps between queries and questions (documents) have been a major issue in question retrieval on large online question and answer (Q&A) collections. Previous studies address the issue by implicitly expanding queries with the help of translation models pre-constructed using statistical techniques. However, since it is possible for unimportant words (e.g., non-topical words, common words) to be included in the translation models, a lack of noise control on the models can cause degradation of retrieval performance. This paper investigates a number of empirical methods for eliminating unimportant words in order to construct compact translation models for retrieval purposes. Experiments conducted on a real world Q&A collection show that substantial improvements in retrieval performance can be achieved by using compact translation models.	algorithm;bridging (networking);closing (morphology);elegant degradation;noise (electronics);one-to-many (data model)	Jung-Tae Lee;Sang-Bum Kim;Young-In Song;Hae-Chang Rim	2008			natural language processing;computer science;theoretical computer science;data mining;noise control;empirical research;information retrieval	Web+IR	-35.079883573390575	-63.943253296190164	114534
3e07f6272d62849390b2bbb22eb35f1e78a613ba	ricoh at trec-10: web track ad-hoc task	site web;titre;text;information retrieval;performance;automatisation;texte;automatizacion;titulo;internet;title;recherche information;evaluation;recuperacion informacion;evaluacion;sitio web;rendimiento;texto;web site;automation		hoc (programming language)	Hideo Itoh;Hiroko Mano;Yasushi Ogawa	2001			the internet;titer;performance;computer science;evaluation;automation;law;world wide web;information retrieval;title	NLP	-36.403057476560164	-60.17217216438678	114623
3d372ff8c6795d66b97eec9b09affdca7ac796fb	how can we design 3d auditory interfaces which enhance traffic safety for chinese drivers?	chinese drivers;drivers information requirement study;normal drive;traffic situation awareness;3d sound traffic information system	With the rapid motorization in countries such as China, the large number of vehicles has caused a dramatic increase of accidents. Thus, the needs for active safety systems are urgent; the system designs must fulfill drivers' needs and local situations. Many studies have indicated that auditory design has great potentials for presenting traffic information to the drivers. In this study, the aim is to understand Chinese drivers' attitudes toward 3D Auditory Traffic Information System, and to explore Chinese drivers' requirements for traffic information support. In the experiment, 23 car drivers were invited to focus group discussions. The results illustrated that the majority (19 out of 23) of participants are interested in 3D auditory design concept and have strong preferences towards using meaningful sounds as the basis for auditory icons. Results also showed differences between experienced and novice drivers regarding how to prioritize traffic information under different traffic conditions. The findings have implications for future design of 3D auditory traffic information system development.	3d computer graphics;chinese wall;focus group;information system;requirement	Min Juan Wang;Yi Ci Li;Fang Chen	2012		10.1145/2390256.2390268	simulation;engineering;transport engineering;computer security	HCI	-47.05508899866204	-52.08889038436964	114957
da640c92a91d00843309aff3096be3467fe67d7f	extracting relation information from text documents by exploring various types of knowledge	busqueda informacion;extraction information;performance evaluation;support vector machines;information extraction;information retrieval;relation semantique;maquina vector soporte;relacion semantica;knowledge extraction;tratamiento del lenguaje natural;relation extraction;machine vecteur support;semantic information;performance improvement;recherche information;feature based relation extraction;information processing;extraction connaissances;extraccion conocimiento;semantic relation;knowledge exploration;support vector machine;extraccion informacion;natural language processing;traitement du langage naturel	Extracting semantic relationships between entities from text documents is challenging in information extraction and important for deep information processing and management. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using support vector machines. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while current commonly used features from full parsing give limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. This indicates that a cheap and robust solution in relation extraction can be achieved without decreasing too much in performance. We also demonstrate how semantic information such as WordNet, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE benchmark corpora shows that effective incorporation of diverse features enables our system outperform previously best-reported systems. It also shows that our feature-based system significantly outperforms tree kernel-based systems. This suggests that current tree kernels fail to effectively explore structured syntactic information in relation extraction.	information retrieval	Guodong Zhou;Min Zhang	2007	Inf. Process. Manage.	10.1016/j.ipm.2006.09.012	natural language processing;relationship extraction;support vector machine;information processing;computer science;machine learning;pattern recognition;data mining;knowledge extraction;information extraction;information retrieval	NLP	-35.79545471920246	-63.34199594529824	115417
1ad4f35de0e3dd6497c57574e50340914ad2b82b	category-based filtering in recommender systems for improved performance in dynamic domains	categorisation;customization;personnalisation;filtrage collaboratif;systeme adaptatif;moteur recommandation;dynamical system;systeme dynamique;hypermedia;categorizacion;engineering and technology;teknik och teknologier;recommender system;internet;collaborative filtering;adaptive system;personalizacion;sistema adaptativo;sistema dinamico;hipermedia;categorization	In Recommender systems, collaborative filtering is the most commonly used technique. Although often successful, collaborative filtering encounters the latency problem in domains where items are frequently added, as the users have to review new items before they can be recommended. In this paper a novel approach to reduce the latency problem is proposed, based on category-based filtering and user stereotypes.	collaborative filtering;recommender system	Mikael Sollenborn;Peter Funk	2002		10.1007/3-540-47952-X_55	the internet;simulation;computer science;artificial intelligence;adaptive system;collaborative filtering;dynamical system;machine learning;database;distributed computing;multimedia;world wide web;recommender system;categorization	Web+IR	-37.45234070742527	-57.51974418980674	115595
63b55643720cd74a8d6ee8acdecf54d32dfccbd9	threshold setting and performance optimization in adaptive filtering	qualitative method;elargissement question;adaptacion;description systeme;filtering;search engine;filtrage;buscador;system description;thresholds;seuil;information retrieval;filtrado;systeme recherche;threshold;search system;accuracy;precision;retroaccion;evaluation measure;okapi moteur de recherche;retroaction;sistema investigacion;recherche information;adaptation;text retrieval;feedback regulation;optimization;descripcion sistema;umbral;recuperacion informacion;moteur recherche;query expansion;quantitative method;performance optimization;adaptive filter	An experimental adaptive filtering system, built on the Okapi search engine, is described. In addition to the regular text retrieval functions, the system requires a complex set of procedures for setting score thresholds and adapting them following feedback. These procedures need to be closely related to the evaluation measures to be used. A mixture of quantitative methods relating a threshold to the number of documents expected to be retrieved in a time period, and qualitative methods relating to the probability of relevance, is defined. Experiments under the TREC-9 Adaptive Filtering Track rules are reported. The system is seen to perform reasonably well in comparison with other systems at TREC. Some of the variables that may affect performance are investigated.	adaptive filter;computer performance;document retrieval;program optimization;relevance;text retrieval conference;web search engine	Stephen E. Robertson	2002	Information Retrieval	10.1023/A:1015702129514	computer science;artificial intelligence;accuracy and precision;information retrieval;algorithm;statistics	Web+IR	-36.54703129205662	-61.43203043466137	115883
9d8baa7ba4e2b89baebcc65e513e51ccdcf4253d	query-based document skimming: a user-centred evaluation of relevance profiling	passage;modelizacion;evaluation performance;interfase usuario;pasaje;navegacion informacion;performance evaluation;user interface;information retrieval;navigation information;estudio comparativo;evaluacion prestacion;index documentation;information browsing;lenguaje;pertinencia;systeme recherche;langage;index;livre;systeme conversationnel;search system;modelisation;etude comparative;electronic book;recherche documentaire;sistema investigacion;interactive system;recherche information;indice;pertinence;libro;indexation;recuperacion documental;comparative study;sistema conversacional;interface utilisateur;findskim;not significant;ground truth;document retrieval;profileskim;recuperacion informacion;relevance;language;book;similarity function;modeling	We present a user-centred, task-oriented, comparative ev aluation of two query-based document skimming tools. ProfileSkim bases w ithin-document retrieval on computing a relevance profile for a document a nd query; FindSkim provides similar functionality to the web browser Find-command. A novel simulated work task was devised, where experiment participa nts are asked to identify (index) relevant pages of an electronic book, giv en subjects from the existing book index. This subject index provides the ground truth , against which the indexing results can be compared. Our major hypothesi s was confirmed, namely ProfileSkim proved significantly more eff icient than Find-Skim, as measured by time for task. Moreover, indexing task effe ctiveness, measured by typical IR measures, demonstrated that ProfileSkim was better than FindSkim in identifying relevant pages, although not significant ly so. The experiments confirm the potential of relevance profiling to improve query-based document skimming, which should prove highly beneficial for users trying to identify relevant information within long documents.	book;credit card fraud;david watt (computer scientist);design of experiments;document retrieval;e-book;experiment;ground truth;information retrieval;kelly criterion;language model;linear search;newton;null-terminated string;profiling (computer programming);profiling (information science);relevance feedback;subject matter expert turing test;tf–idf	David J. Harper;Ivan Koychev;Sun Yixing	2003		10.1007/3-540-36618-0_27	document retrieval;web query classification;systems modeling;relevance;ground truth;computer science;comparative research;data mining;language;user interface;world wide web;information retrieval	Web+IR	-36.9365457542932	-59.848884066408296	116279
a109b8e7f1aa8b6473a6306ee4a861df43577b2a	trends in gaming indicators: on failed attempts at deception and their computerised detection		Counting articles and citations, analyzing citations and co-authors graphs have become ways to assess researchers and institutions performance. Fairly enough, these measures are becoming targets for institutions and individual researchers thus triggering new behaviors. As a matter of fact, scientometrics and informetrics systems of all kinds have to separate the grain form the chaff. Among others, fields like Information Retrieval, network analysis and natural language processing may offers answers to deal with this kind of problems. Through several emblematic case studies (fake researcher, generated papers, paper mills), we show evidences of attempts to game indicators together with automatic ways to detect them (automatic detection of generated papers, errors detection. Pour en savoir plus : http://ceur-ws.org/Vol-2080/paper1.pdf (keynote du workshop BIR@ECIR 2018)	information retrieval;informetrics;natural language processing;scientometrics;social network analysis	Cyril Labbé	2018			social psychology;deception;psychology	Web+IR	-43.35704994863772	-61.72066269223317	116372
cc930c3e5fbddc0bbc300554b2fa02bdb6563acd	fujitsu laboratories trec2001 report	site web;information retrieval;performance;automatisation;automatizacion;page web;internet;ranking;elargissment question;recherche information;classement;evaluation;web page;recuperacion informacion;evaluacion;sitio web;rendimiento;query expansion;web site;automation	This year a Fujitsu Laboratory team participated in web tracks. Both for ad hoc task, and entry point search task, we combined the score of normal ranking search and that of page ranking techniques. For ad hoc style task, the eect of page ranking was very limitted. We only got very little improvement for title eld search, and the page rank was not eective for description, and narrative eld search. For entry point search task, we compared three heuristics. The rst heuristics supposed that entry point page contains key word and had high page rank score. The second heuristics supposed that entry point page contains key word in its head part and had high page ran score. The third heuristics supposes that entry point is pointed by the pages whose anchor string contains key word, and has high page rank score. The page rank improved the result of entry point search about 20-30% in rather small VLC10 test set, and the rst heuristics got the best result because of its high recall. For TREC2001, we added the new functions to trec exec for entry point search. The functions includes score merging, evaluation of reciprocal rank and so on. We u s e d W eb Recommener Agent to get page ranking score. Except above modications, the framework is same as that of TREC9[1]. 1.0.1 Tera Tera[2] is a fulltext search library, designed to provide an adequate number of ecient functions for commercial service, and to provide parameter combination testing and easy extension for experiments in IR. 1.0.2 trec exec trec exec is designed for automatic processing of TREC. It contains a procedure controller, evaluation module , logging module, and all non-searching units such as query generation, query expansion and so on. trec exec can execute all the TREC processing for one run in a few minutes, and it can be used for system tuning by hill-climing. The new functions added for TREC2001 Web track are heuristics for entry point search, evaluation of reciprocal rank, and accepting non-digit query number. We used web recommeder agent tool developed for automatic domain specic web directory Tsuda et al[3] to get page ranking score. The page rank score is put into Tera, and it is marged with normal ranking score.	entry point;exec (amiga);experiment;heuristic (computer science);hoc (programming language);pagerank;query expansion;terabit;test set;text retrieval conference	Isao Namba	2001			query expansion;the internet;performance;ranking;computer science;evaluation;automation;web page;database;world wide web;information retrieval	Web+IR	-35.729667924298475	-61.54121822771622	116501
ea5faad5037288997c216f289593936294b12f1f	towards more effective techniques for automatic query expansion	elargissement question;automatic system;average precision;information retrieval;sistema automatico;interrogation;recherche information;comparative study;systeme automatique;query;automatic query expansion;recuperacion informacion;query expansion	Techniques for automatic query expansion from top retrieved documents have recently shown promise for improving retrieval effectiveness on large collections but there is still a lack of systematic evaluation and comparative studies. In this paper we focus on term-scoring methods based on the differences between the distribution of terms in (pseudo-)relevant documents and the distribution of terms in all documents, seen as a complement or an alternative to more conventional techniques. We show that when such distributional methods are used to select expansion terms within Rocchio’s classical reweighting scheme, the overall performance is not likely to improve. However, we also show that when the same distributional methods are used to both select and weight expansion terms the retrieval effectiveness may considerably improve. We then argue, based on their variation in performance on individual queries, that the set of ranked terms suggested by individual distributional methods can be combined to further improve mean performance, by analogy with ensembling classifiers, and present experimental evidence supporting this view. Taken together, our experiments show that with automatic query expansion it is possible to achieve performance gains as high as 21.34% over non-expanded query (for non-interpolated average precision). We also discuss the effect that the main parameters involved in automatic query expansion, such as query difficulty, number of selected documents, and number of selected terms, have on retrieval effectiveness.	document;experiment;information retrieval;interpolation;naive bayes classifier;query expansion;relevance feedback	Claudio Carpineto;Giovanni Romano	1999		10.1007/3-540-48155-9_10	sargable;query optimization;query expansion;web query classification;ranking;computer science;comparative research;data mining;database;information retrieval	Web+IR	-36.19561985034925	-62.18740023143825	116597
10cb34f93538e329475c431ea47ee4bb2f1eb97c	rendering expressions to improve accuracy of relevance assessment for math search	relevance assessment;mathematical information retrieval mir;search engine results page serp	Finding ways to help users assess relevance when they search using math expressions is critical for making Mathematical Information Retrieval (MIR) systems easier to use. We designed a study where participants completed search tasks involving mathematical expressions using two different summary styles, and measured response time and relevance assessment accuracy. The control summary style used Google's regular hit formatting where expressions are presented as text (e.g. in LaTeX), while the second summary style renders the math expressions. Participants were undergraduate and graduate students. Participants in the rendered summary style (n=19) had on average a 17.18% higher assessment accuracy than those in the non-rendered summary style (n=19), with no significant difference in response times. Participants in the rendered condition reported having fewer problems reading hits than participants in the control condition. This suggests that users will benefit from search engines that properly render math expressions in their hit summaries.	information retrieval;latex;relevance;rendering (computer graphics);response time (technology);web search engine	Matthias S. Reichenbach;Anurag Agarwal;Richard Zanibbi	2014		10.1145/2600428.2609457	computer science;machine learning;multimedia;world wide web;information retrieval	HCI	-36.96283843312496	-53.71086293394962	116824
8697affbffed8403dd5244b12f72b5dc5b243e06	modeling and predicting information search behavior	aging;information search;cognitive factors;modeling	This paper looks at two limitations of cognitive models of web-navigation: first, they do not account for the entire process of information search and second, they do not account for the differences in search behavior caused by aging. To address these limitations, data from an experiment in which two types of information search tasks (simple and difficult), presented to both young and old participants was used. We found that in general difficult tasks demand significantly more time, significantly more clicks, significantly more reformulations and are answered significantly less accurately than simple tasks. Older persons inspect the search engine result pages significantly longer, produce significantly fewer reformulations with difficult tasks than younger persons, and are significantly more accurate than younger persons with simple tasks. We next used a cognitive model of web-navigation called CoLiDeS to predict which search engine result a user would choose to click. Old participants were found to click more often only on search engine results with high semantic similarity with the query. Search engine results generated by old participants were of higher semantic similarity value (computed w.r.t the query) than those generated by young participants only in the second cycle. Match between model-predicted clicks and actual user clicks was found to be significantly higher for difficult tasks compared to simple tasks. Potential improvements in enhancing the modeling and its applications are discussed.	cognitive model;semantic similarity;web navigation;web search engine	Saraschandra Karanam;Herre van Oostendorp;Mylène Sanchiz;Aline Chevalier;Jessie Chin;Wai-Tat Fu	2015		10.1145/2797115.2797123	simulation;computer science;data mining;information retrieval	HCI	-34.740135492687656	-53.13133777500343	116948
026dd5d3b0cd1512ce20ccb0b9293222d89c203d	"""distraction and driving behavior by presenting information on an """"emissive projection display"""" compared to a head-up display"""	workload;head up display hud;emissive projection display epd;distraction;reaction time;driver assistance system	A study in the static driving simulator examines whether the use of an Emissive Projection Display (EPD) causes significant effects on attention and cognitive load in addition to the driving task. Moreover, a comparison with a conventional Head-Up Display (HUD) is drawn. Conclusions regarding the driver’s stress resulting parallel to the driving task are objectively determined by reaction times that are needed to perform a visual secondary task. The results of the analysis of objective data show significant extensions in the response times the performing the secondary task for the EPD, compared to HUD. Also data for the subjective assessments of workload during the different test runs are discussed.	automotive head-up display;driving simulator;epd;focus group;logistics;material flow	Verena C. Knott;Stefan Demmelmair;Klaus Bengler	2015		10.1007/978-3-319-20373-7_2	embedded system;simulation;engineering;computer graphics (images)	HCI	-46.852261994640976	-53.0069740778013	117129
66126aba720f776728f9840d40c64ef734d7803e	thread structure prediction for mooc discussion forum		Discussion forums are an indispensable interactive component for Massive Open Online Courses (MOOC). However, the organization of current discussion forums is not well-designed. Trouble-shooting threads are valuable for both learners and instructors, but they are drowned out in the forums with huge amounts of threads. This work first built a labeled data set for trouble-shooting thread structure prediction by crowdsourcing and then proposed methods for trouble-shooting thread detection and thread structure prediction on the data set. The output of this work can be used to spot trouble-shooting threads and show them along with structure tags in MOOC discussion forums.	conditional random field;crowdsourcing;massive open online course;performance;selection algorithm	Chengjie Sun;Shang-wen Li;Lei Lin	2016		10.1007/978-981-10-2098-8_13	simulation;human–computer interaction;computer science;world wide web	Web+IR	-40.82846747747275	-59.33203326677742	117150
75d50fb7b23b23720cdfcb024f421f7c29cb0518	emotion recognition from blog articles	teachers;blog web sites;blog;college students;emotion recognition;blog emotion recognizing system;psychology;information services;data mining;blog articles;internet;depression prevention;emotion classification;information processing technologies;dictionaries;information processing;web sites;psychological consultants;emotion recognition information services web sites internet educational institutions psychology natural language processing information science intelligent systems systems engineering and theory;structural characteristics;emotion recognition suicide depression blog affective computing natural language processing emotion classification structural characteristics;natural language processing;depression;blog emotion recognizing system blog articles suicide college students internet information processing technologies blog web sites teachers psychological consultants depression prevention affective computing natural language processing;suicide;affective computing;web sites emotion recognition natural language processing psychology;emotional expression	Suicide of college students has been a universal phenomenon in the world. And the phenomenon has become more and more sever because of the complex and drastic competitions. With the popularization of Internet and the development of information processing technologies, a lot of people have established their own blog Websites to write down their experiences and express their feelings at times. It will be very helpful if the computer can recognize the emotions expressed in blog pages automatically. And then it will be convenient for teachers or psychological consultants to monitor the affective information of college students and take measures for the depression prevention when necessary. Owing to the advances in affective computing and natural language processing, researches have begun to pay more attention to the emotion recognition in NLP all over the world. This paper outlines the approach we have developed to construct a blog emotion-recognizing system. It is based on the lexical contents of words and structural characteristics of blog articles. For the emotion computing of articles, two methods are proposed and the experimental results are compared and analyzed. Finally, the implications of the results are discussed for the future's direction of the research.	affective computing;algorithm;analysis of algorithms;blog;data dictionary;emotion recognition;experience;experiment;information processing;internet;natural language processing;system testing;vocabulary	Ji Li;Fuji Ren	2008	2008 International Conference on Natural Language Processing and Knowledge Engineering	10.1109/NLPKE.2008.4906757	psychology;multimedia;communication;social psychology	AI	-45.24164846527158	-61.30814738296175	117606
596708d43098d9ed1084b084f75b63ee5f28427d	"""""""i ain't tellin' white folks nuthin"""": a quantitative exploration of the race-related problem of candour in the wpa slave narratives"""		"""From 1936-38, the Works Progress Administration interviewed thousands of former slaves about their life experiences. While these interviews are crucial to understanding the""""peculiar institution""""from the standpoint of the slave himself, issues relating to bias cloud analyses of these interviews. The problem I investigate is the problem of candour in the WPA slave narratives: it is widely held in the historical community that the strict racial caste system of the Deep South compelled black ex-slaves to tell white interviewers what they thought they wanted to hear, suggesting that there was a significant difference candour depending on whether their interviewer was white or black. In this work, I attempt to quantitatively characterise this race-related problem of candour. Prior work has either been of an impressionistic, qualitative nature, or utilised exceedingly simple quantitative methodology. In contrast, I use more sophisticated statistical methods: in particular word frequency and sentiment analysis and comparative topic modelling with LDA to try and identify differences in the content and sentiment expressed by ex-slaves in front of white interviewers versus black interviewers. While my sentiment analysis methodology was ultimately unsuccessful due to the complexity of the task, my word frequency analysis and comparative topic modelling methods both showed strong evidence that the content expressed in front of white interviewers was different from that of black interviewers. In particular, I found that the ex-slaves spoke much more about unfavourable aspects of slavery like whipping and slave patrollers in front of interviewers of their own race. I hope that my more-sophisticated statistical methodology helps improve the robustness of the argument for the existence of this problem of candour in the slave narratives, which some would seek to deny for revisionist purposes."""	experience;frequency analysis;sentiment analysis;wi-fi protected access;word lists by frequency	Soumya Chakrabarti Kambhampati	2018	CoRR			NLP	-41.096239882197146	-62.64076617616848	117745
30320a06b1cdd1121055a00f89b0e42cee42a9e0	automation-induced monitoring inefficiency: role of display location	psychomotor performance;female;human performance;females;adolescent;display devices;aerospace medicine;resource manager;male;attention;speed accuracy trade off;data display;position location;fuel management;males;adults;support u s gov t non p h s;adult;human;human engineering;user computer interface;aircraft instrumentation;youth;reaction time;task performance and analysis;aircraft;human computer interface;automation	Operators can be poor monitors of automation if they are engaged concurrently in other tasks. However, in previous studies of this phenomenon the automated task was always presented in the periphery, away from the primary manual tasks that were centrally displayed. In this study we examined whether centrally locating an automated task would boost monitoring performance during a flight-simulation task consisting of system monitoring, tracking and fuel resource management sub-tasks. Twelve nonpilot subjects were required to perform the tracking and fuel management tasks manually while watching the automated system monitoring task for occasional failures. The automation reliability was constant at 87.5% for six subjects and variable (alternating between 87.5% and 56.25%) for the other six subjects. Each subject completed four 30 min sessions over a period of 2 days. In each automation reliability condition the automation routine was disabled for the last 20 min of the fourth session in order to simulate catastrophic automation failure (0 % reliability). Monitoring for automation failure was inefficient when automation reliability was constant but not when it varied over time, replicating previous results. Furthermore, there was no evidence of resource or speed accuracy trade-off between tasks. Thus, automation-induced failures of monitoring cannot be prevented by centrally locating the automated task.	automation;concurrency (computer science);disabled persons;flight simulator;heart failure;liver failure, acute;maxima and minima;simulation;system monitor;system monitoring	Indramani L. Singh;Robert Molloy;Raja Parasuraman	1997	International journal of human-computer studies	10.1006/ijhc.1996.0081	mental chronometry;human performance technology;real-time computing;simulation;attention;human–computer interaction;computer science;artificial intelligence;human factors and ergonomics;operating system;automation;aviation medicine;display device	SE	-47.58070035772903	-55.53147540570784	117873
d539e72435f9fa7b0078ba4a30aef9ba8f6e0a34	topic-independent web high-quality page selection based on k-means clustering	busqueda informacion;documento electronico;search engine;analyse amas;buscador;base donnee;web pages;red www;redundancia;algorithme k moyenne;information retrieval;data collection;reseau web;database;base dato;classification;qualite service;web search engine;document electronique;cluster analysis;internet;redundancy;recherche information;world wide web;algoritmo k media;k means algorithm;analisis cluster;moteur recherche;clasificacion;k means clustering;service quality;redondance;electronic document;calidad servicio	One of the web search engines’ challenges is to identify the quality of web pages independent of a given user request. Web high-quality pages provide readers proper entries to get more concentrated required information on the web. This paper focuses on topic-independent web high-quality page selection to reduce web information redundancies and clean noise. Different non-content features and their effects on high-quality page selection are studied. Then K-means clustering with these features is performed to separate high-quality pages from common ones. Experiments on 19GB (document size) TREC web data set (.GOV data) have been made. By this proposed approach, less than 50% of web pages are obtained as high-quality ones, covering about 90% key information in the whole set. Information retrieval on this high-quality page set achieves more than 40% improvement, compared with that on the whole data collection.	computer cluster;k-means clustering	Canhui Wang;Yiqun Liu;Min Zhang;Shaoping Ma	2005		10.1007/11562382_43	web service;static web page;site map;data web;web analytics;web search engine;page view;computer science;machine learning;web page;data mining;database;same-origin policy;hits algorithm;world wide web;website parse template;information retrieval;web server;k-means clustering	ML	-35.7088200024692	-59.08082420893299	117900
c83291977707b9ad6709e7098c96ed25a587b78f	learning user similarity and rating style for collaborative recommendation	busqueda informacion;consumidor;commerce electronique;apprentissage automatique;filtering;evaluation performance;filtrage;learning algorithm;prediction error;comercio electronico;performance evaluation;consommateur;information retrieval;e commerce;cooperation;evaluacion prestacion;filtrado;information filtering;customization;usuario;personnalisation;systeme recherche;recommandation;utilisateur;cooperacion;apprentissage machine;similitude;search system;algorithme;algorithm;user profile;recommender system;machine learning;sistema investigacion;collaborative filtering;recherche information;consumer;user similarity;similarity;personalizacion;recomendacion;recommendation;user;similitud;similarity measure;rating style;recommender systems;electronic trade;algoritmo;profil utilisateur	Information filtering is an area getting more important as we have long been flooded with too much information, where product brokering in e-commerce is a typical example. Systems which can provide personalized product recommendations to their users (often called recommender systems) have gained a lot of interest in recent years. Collaborative filtering is one of the commonly used approaches which normally requires a definition of user similarity measure. In the literature, researchers have proposed different choices for the similarity measure using different approaches, and yet there is no guarantee for optimality. In this paper, we propose the use of machine learning techniques to learn the optimal user similarity measure as well as user rating styles for enhancing recommendation acurracy. Based on a criterion function measuring the overall prediction error, several ratings transformation functions for modeling rating styles together with their learning algorithms are derived. With the help of the formulation and the optimization framework, subjective components in user ratings are removed so that the transformed ratings can then be compared. We have evaluated our proposed methods using the EachMovie dataset and succeeded in obtaining significant improvement in recommendation accuracy when compared with the standard correlation-based algorithm.	algorithm;boosting (machine learning);collaborative filtering;customer relationship management;e-commerce;information filtering system;loss function;machine learning;mathematical optimization;nonlinear system;personalization;recommender system;richardson number;similarity measure;virtual community	William Kwok-Wai Cheung;Lily F. Tian	2004	Information Retrieval	10.1023/B:INRT.0000011212.66249.b7	filter;user;simulation;similarity;consumer;computer science;similitude;collaborative filtering;machine learning;mean squared prediction error;data mining;world wide web;cooperation;recommender system	Web+IR	-36.598263055769344	-58.08652083536647	117956
f7a46546db9cf76dfa52019b145acc63f059e344	training techniques for visual search in complex task environments	simulators;stress;training;unmanned aerial vehicles;visual search	OBJECTIVE The goal for this study was to evaluate several visual search training techniques in an unmanned aerial vehicle (UAV) simulated task environment.   BACKGROUND Operators controlling remote unmanned vehicles often must perform complex visual search tasks (e.g., target search). These tasks may pose substantial demands on the operator due to various environmental factors. Visual search training may reduce errors and mitigate stress, but the most effective form of training has not been determined.   METHODS Participants were assigned to one of four training conditions: target, cue, visual scanning, or control. After the training, the effectiveness of the training techniques was tested during a 30-minute simulated UAV flight. A secondary task manipulation was included to further simulate the demands of a realistic UAV control and target search task. Subjective stress and fatigue were also assessed.   RESULTS Target training produced superior target search performances in more hits and fewer false alarms (FAs) when compared to the control condition. The visual scanning and cue trainings were moderately effective. Only target training performance was vulnerable to the secondary task load. The task was stressful, but training did not mitigate stress response.   CONCLUSION Training participants on the target and the cue appearance as well as active scanning of the visual field is promising for promoting effective target search for this simulated UAV environment.   APPLICATION These training techniques could be used in preparation for intelligence, surveillance, and reconnaissance (ISR) missions that involve target search, especially where target appearance change is likely.		Svyatoslav Guznov;Gerald Matthews;Joel S. Warm;Marc Pfahler	2017	Human factors	10.1177/0018720817712307	visual search;engineering;simulation;visual search tasks	HCI	-47.33426749292581	-54.076289708751595	118003
3b9a4400cad357448776b289eba470a9a744afab	heart rate variability in eye-level and low monitor conditions	workload;computadora;ritmo cardiaco;variabilidad;carga mental;mental load;heart;heart rate variability;input output equipment;ordinateur;hombre;rythme cardiaque;computer;coeur;man machine system;ecran visualisation;pantalla visualizacion;circulatory system;equipement entree sortie;heart rate;corazon;human;charge travail;equipo entrada salida;sistema hombre maquina;computer hardware;display screen;variability;carga trabajo;appareil circulatoire;variabilite;charge mentale;aparato circulatorio;materiel informatique;material informatica;homme;systeme homme machine		heart rate variability	Dennis R. Ankrum;Kaoru Suzuki	1997			heart rate variability;simulation;telecommunications;engineering;electrical engineering;circulatory system;heart	HCI	-45.64313658007494	-55.638181913384116	118084
9cc5bb4e28b228e1bb72de963960975265171e0f	using web metrics to analyze digital libraries	national science digital library;using web metrics analyze digital libraries;analyze;selected works;bepress selected works;e commerce;digital library;digital libraries;data mining;using;library of congress;bepress;evaluation;web metrics;usability;usage;web analytics	This paper discusses the use of web metrics tools at four digital libraries, the Instructional Architect, the Library of Congress, the National Science Digital Library, and WGBH Teachers' Domain. We describe some of the issues involved in using web metrics to report on ongoing web site performance. We also describe how web metrics can be used for focused data mining, using session length metrics as our example; and here, we recommend that session length metrics, which were developed to track e-commerce, need to be carefully considered when they are applied in non-e-commerce settings, such as digital libraries. We conclude by discussing some of the current limitations and possibilities of using web metrics to analyze and evaluate digital library use and impact.	data mining;e-commerce;library (computing);national science digital library	Michael Khoo;Joe Pagano;Anne L. Washington;Mimi Recker;Bart Palmer;Robert A. Donahue	2008		10.1145/1378889.1378956	web modeling;digital library;web analytics;usability;computer science;data science;evaluation;world wide web	Web+IR	-38.51277996067354	-55.4122210018663	118143
652c9953414532161d9208805a3603d8c833ccb7	evaluation effort, reliability and reusability in xml retrieval	information retrieval;xml retrieval;retrieval effectiveness;evaluation;field formatted data	The Initiative for the Evaluation of XML retrieval (INEX) provides a TREC-like platform for evaluating contentoriented XML retrieval systems. Since 2007, INEX has been using a set of precision-recall based metrics for its ad hoc tasks. The authors investigate the reliability and robustness of these focused retrieval measures, and of the INEX pooling method. They explore four specific questions: How reliable are the metrics when assessments are incomplete, or when query sets are small? What is the minimum pool/query-set size that can be used to reliably evaluate systems? Can the INEX collections be used to fairly evaluate “new” systems that did not participate in the pooling process? And, for a fixed amount of assessment effort, would this effort be better spent in thoroughly judging a few queries, or in judging many queries relatively superficially? The authors’ findings validate properties of precision-recall-based metrics observed in document retrieval settings. Early precision measures are found to be more error-prone and less stable under incomplete judgments and small topic-set sizes. They also find that system rankings remain largely unaffected even when assessment effort is substantially (but systematically) reduced, and confirm that the INEX collections remain usable when evaluating nonparticipating systems. Finally, they observe that for a fixed amount of effort, judging shallow pools for many queries is better than judging deep pools for a smaller set of queries. However, when judging only a random sample of a pool, it is better to completely judge fewer topics than to partially judge many topics.This result confirms the effectiveness of pooling methods.	cognitive dimensions of notations;convolutional neural network;document retrieval;hoc (programming language);query language;text retrieval conference;xml retrieval	Sukomal Pal;Mandar Mitra;Jaap Kamps	2011	JASIST	10.1002/asi.21403	computer science;evaluation;data mining;database;world wide web;information retrieval	Web+IR	-38.68410599721792	-61.251186628210355	118518
2f6d28f1f37db4332e7d081d63ccb0e07e652292	a hybrid method for patterns mining and outliers detection in the web usage log	analyse amas;high dimensionality;red www;methode noyau;logique floue;reseau web;logica difusa;outlier;data mining;feature space;classification;log data;pattern mining;outlier detection;fuzzy logic;observacion aberrante;fuzzy clustering;cluster analysis;internet;hybrid method;fouille donnee;clustering method;metodo nucleo;machine exemple support;comportement utilisateur;observation aberrante;world wide web;possibility theory;kernel method;analisis cluster;user behavior;vector support machine;donnee session;busca dato;clasificacion;teoria posibilidad;comportamiento usuario;theorie possibilite	"""This paper presents a novel approach to mining patterns and outliers detection in the Web Usage log. This approach involves kernel methods and fuzzy clustering methods. Web log records are considered as vectors with numeric and nominal attributes. These vectors are mapped by means of a special kernel to a high dimensional feature space, where the possibilistic clustering method is used to calculate the measure of """"typicalness"""" of vectors. If the value of this measure for a particular record is less than specified threshold this record is labeled as an outlier. The records with high """"typicalness"""" are considered as access patterns of user activity. The performance of the approach is demonstrated experimentally."""	world wide web	Mikhail Petrovskiy	2003		10.1007/3-540-44831-4_33	fuzzy logic;possibility theory;kernel method;anomaly detection;outlier;the internet;feature vector;fuzzy clustering;biological classification;computer science;artificial intelligence;machine learning;data mining;cluster analysis	ML	-36.05993724108397	-57.63523914234989	118520
dacc3d6d45ec9bec3d2eedccd9dbb35881f0a225	the automatic indexing system air/phys - from research to applications	fault tolerant;heuristic method;large scale;indexation;automatic indexing	Since October 1985, the automatic indexing system AIR/PHYS has been used in the input production of the physics data base of the Fachinformationsentrum Karlsruhe/West Germany. The texts to be indexed are abstracts written in English. The system of descriptors is prescribed. For the application of the AIR/PHYS system a large-scale dictionary containing more than 600 000 word-descriptor relations reap. phrase-descriptor relations has been developed. Most of these relations have been obtained by means of statistical and heuristical methods. In consequence, the relation system is rather imperfect. Therefore, the indexing system needs some fault- tolerating features. An appropriate indexing approach and the corresponding structure of the AIR/PHYS system are described. Finally, the conditions of the application as well as problems of further development are discussed.	ccir system a;database;dictionary;heuristic	Peter Biebricher;Norbert Fuhr;Gerhard Lustig;Michael Schwantner;Gerhard Knorz	1988		10.1145/62437.62470	fault tolerance;computer science;theoretical computer science;data mining;world wide web;information retrieval	DB	-34.981475923737804	-63.9903649683502	118938
1866150c2cd968be69214fda4d814f048aee5b3c	node popularity as a hypertext browsing aid	popularity;control group;user study;browsing;node relevance;user behavior;hypertext	We have performed a user study where the popularity of each node in a hypertext database was presented with the links leading to that node. Popularity was computed by counting the number of users who had previously visited the node. Our users clearly incorporated popularity information in their decisions; we compare their browsing patterns with a control group for whom the popularity information was not provided. One possible use of popularity can be to offset the previously documented trait of users to over-select items near the top or bottom of a linear list. We document that popularity information affects user behavior, but we do not necessarily advocate its use. Incorporating popularity information raises other questions of design and ethics which are beyond the scope of this paper.	browsing;hypertext;usability testing	Randy F. Pausch;J. Detmer	1990	Electronic Publishing		hypertext;computer science;database;multimedia;internet privacy;world wide web;information retrieval;scientific control	DB	-35.33406017698487	-52.64376677819271	118939
754e2088880fcbf3079e08be9e9cf4ef43286f63	presentation ordering effects on assessor agreement		Consistency of relevance judgments is a vital issue for the construction of test collections in information retrieval. As human relevance assessments are costly, and large collections can contain many documents of varying relevance, collecting reliable judgments is a critical component to building reusable test collections. We explore the impact of document presentation order on human relevance assessments. Our primary goal is to determine if assessor disagreement can be minimized through the order in which documents are presented to assessors. To achieve this goal, we compare two commonly used presentation orderings with a new ordering designed to aid assessors to more easily discriminate between relevant and non-relevant documents. By carefully controlling the presentation ordering, assessors can more quickly converge on a consistent notion of relevance during the assessment exercise, leading to higher overall judging agreement. In addition, important interactions between presentation ordering and topic difficulty on assessor agreement are highlighted. Our findings suggest that document presentation order does indeed have a substantial impact on assessor agreement , and that our new ordering is more robust than previous approaches across a variety of different topic types.	converge;document;information retrieval;interaction;relevance feedback	Tadele Tedla Damessie;J. Shane Culpepper;Jaewon Kim;Falk Scholer	2018		10.1145/3269206.3271750	information retrieval;data mining;computer science	Web+IR	-38.8283850252187	-61.112589840332475	119083
19315fc9f805484ff407da60706488f5be9d3e50	cleartype sub-pixel text rendering: preference, legibility and reading performance	legibility;evaluation performance;resolution;text;performance evaluation;display equipment;readability;implementation;evaluacion prestacion;reading;sub pixel rendering;texte;gray scale;red green and blue;performance improvement;cleartype;equipement affichage;legibilidad;lisibilite;equipo visualizacion;implementacion;texto;font;echelle gris;escala gris	ClearType is an onscreen text rendering technology in which the red, green, and blue sub-pixels are separately addressed to increase text legibility. However, it results in colored borders on characters that can be bothersome. This paper describes five experiments measuring subject preference, text legibility, reading performance, and discomfort symptoms for five implementation levels of ClearType rendered text. The results show that, while ClearType rendering does not improve text legibility, reading speed or comfort compared to perceptually-tuned grayscale rendering, subjects prefer text with moderate ClearType rendering to text with grayscale or higher-level ClearType contrast. Reasons for subject preference and for lack of performance improvement are discussed. 2007 Elsevier B.V. All rights reserved.	cleartype;experiment;grayscale;pixel	James E. Sheedy;Yu-Chi Tai;Manoj V. Subbaram;Sowjanya Gowrisankaran;John R. Hayes	2008	Displays	10.1016/j.displa.2007.09.016	resolution;computer science;multimedia;implementation;reading;grayscale;computer graphics (images)	HCI	-43.930603551300976	-54.588604652211956	119110
e512db68bfbc19f9e4e1f94bc8dd53d65718d152	a usefulness-based approach for measuring the local and global effect of iir services	term suggestion;query suggestion;evaluation;usefulness;iir	In Interactive Information Retrieval (IIR) different services such as search term suggestion can support users in their search process. The applicability and performance of such services is either measured with different user-centered studies (like usability tests or laboratory experiments) or, in the context of IR, with their contribution to measures like precision and recall. However, each evaluation methodology has its certain disadvantages. For example, user-centered experiments are often costly and small-scaled; IR experiments rely on relevance assessments and measure only relevance of documents. In this work we operationalize the usefulness model of Cole et al. (2009) on the level of system support to measure not only the local effect of an IR service, but the impact it has on the whole search process. We therefore use a log-based evaluation approach which models user interactions within sessions with positive signals and apply it for the case of a search term suggestion service. We found that the usage of the service significantly often implicates the occurrence of positive signals during the following session steps.	experiment;infinite impulse response;information retrieval;interaction;precision and recall;relevance;usability;user-centered design	Daniel Hienert;Peter Mutschke	2016		10.1145/2854946.2854962	computer science;data mining;world wide web;information retrieval	Web+IR	-35.23477239910843	-52.68401931265225	119135
991af541a4fdcfde71077015def28015fe729c10	evaluation of the citation matching algorithms of cwts and ifq in comparison to web of science	accuracy;bibliometrics;evaluation	The results of bibliometric studies provided by bibliometric research groups, e.g. the Centre for Science and Technology Studies (CWTS) and the Institute for Research Information and Quality Assurance (iFQ), are often used in the process of research assessment. Their databases use Web of Science (WoS) citation data, which they match according to their own matching algorithms – in the case of CWTS for standard usage in their studies and in the case of iFQ on an experimental basis. Since the problem of non-matched citations in WoS persists because of inaccuracies in the references or inaccuracies introduced in the data extraction process, it is important to ascertain how well these inaccuracies are rectified in these citation matching algorithms. This paper evaluates the algorithms of CWTS and iFQ in comparison to WoS in a quantitative and a qualitative analysis. The analysis builds upon the methodology and the manually verified corpus of a previous study. The algorithm of CWTS performs best, closely followed by that of iFQ. The WoS algorithm still performs quite well (F1 score: 96.41%), but shows deficits in matching references containing inaccuracies. An additional problem is posed by incorrectly provided cited reference information in source articles by WoS.	algorithm;bibliometrics;database;f1 score;web of science	Marlies Olensky;Marion Schmidt;Nees Jan van Eck	2016	JASIST	10.1002/asi.23590	bibliometrics;computer science;data science;evaluation;data mining;accuracy and precision;world wide web;information retrieval;statistics	Logic	-39.899799146193715	-64.72512733391842	119347
6dc601970e0e33169f48452bb0ec761db4980aa2	experiments in unix command prediction	unix command prediction	Most users demonstrate regularities in their work with a computer system. Even when a user’s activities are unique, those interactions often exhibit systematic patterns. Accordingly, there has been a range of work developing systems that recognize regularities in computer usage (Cypher 1993; Mitchell et al. 1994; Schlimmer & Hermens 1993). Our current effort is to consider different methods that would enable us to build similar pattern-recognition into the UNIX command shell (Hirsh & Davison 1997; Davison & Hirsh 1997), although the concept of command prediction is similarly applicable to other user interfaces. The prediction of a user’s next command by the shell requires some mechanism that considers the user’s current session and from that information generates a prediction for the next command. We gather data by recording the sequence of commands executed as well as other pertinent information about the state of the shell. This forms the basis for applying an off-the-shelf inductive-learning method. To determine the potential for such methods, we collected command histories from over 70 users and computed online predictive accuracies for five methods on their data (Davison & Hirsh 1997). Online evaluation (train on the previous data points; test on the current one) was used instead of the traditional training and test set partitioning or crossvalidation because of the sequential nature of the data. We found that relatively straightforward, knowledge-free methods were able to correctly predict the next command (without arguments) that the user would execute up to 45% of the time. The best performance was achieved using C4.5 (Quinlan 1993) on two features (the two previous commands), and a training window of 1000 data points. Each subject contributed histories with, on average, 2184 commands, each with an average length of 3.77 characters. Assuming that a correct prediction could be inserted with a single character, this method would have saved just over 31% of the keystrokes typed, which is very close to the	c4.5 algorithm;computer;data point;event (computing);interaction;pattern recognition;relevance;ross quinlan;test set;unix;user interface	Brian D. Davison;Haym Hirsh	1997			unix signal;unix architecture;command-line interface;operating system;database;programming language;environment variable	HCI	-46.51612786528511	-59.562012097240235	119378
e37aadbaaa060389612439b4284d94bdfc07d338	does indexing exhaustivity matter?	information science;indexing;indexation;information processing;experiments	Indexing exhaustivity, which may be broadly defined as the number of terms assigned to a document, is thought to be of some importance in retrieval, and it has been suggested that there may be an optimal level of exhaustivity for a particular collection. Experiments with two distinct collections, using three levels of indexing exhaustivity for both documents and requests, show that substantially the same performance is obtained for very different levels of document indexing, if suitable choices are made of request level.		Karen Spärck Jones	1973	JASIS	10.1002/asi.4630240502	search engine indexing;information processing;information science;computer science;data mining;database;world wide web;information retrieval	Web+IR	-34.42473567372675	-60.67501650863285	119598
09c046946048376098f9032db7fb152d1be32197	on cultural, textual and experiential aspects of music mood		We study the impact of the presence of lyrics on music mood perception for both Canadian and Chinese listeners by conducting a user study of Canadians not of Chinese origin, Chinese-Canadians, and Chinese people who have lived in Canada for fewer than three years. While our original hypotheses were largely connected to cultural components of mood perception, we also analyzed how stable mood assignments were when listeners could read the lyrics of recent popular English songs they were hearing versus when they only heard the songs. We also showed the lyrics of some songs to participants without playing the recorded music. We conclude that people assign different moods to the same song in these three scenarios. People tend to assign a song to the mood cluster that includes “melancholy” more often when they read the lyrics without listening to it, and having access to the lyrics does not help reduce the difference in music mood perception between Canadian and Chinese listeners significantly. Our results cause us to question the idea that songs have “inherent mood”. Rather, we suggest that the mood depends on both cultural and experiential context.	usability testing	Abhishek Singhi;Daniel G. Brown	2014			mood;social psychology;speech recognition;experiential learning;lyrics;computer science;active listening;perception;chinese people	Web+IR	-44.48192146160548	-59.09789997775878	119713
3c5c5083fc0f42abb9c6bffb58985609fd1f06bf	exploiting session context for information retrieval: a comparative study	user participation;information retrieval;implicit feedback;vector space;comparative study;retrieval model;relevance feedback;language model	Hard queries are known to benefit from relevance feedback provided by users. It is, however, also known that users are generally reluctant to provide feedback when searching for information. A natural resort not demanding any active user participation is to exploit implicit feedback from the previous user search behavior, i.e., from the context of the current search session. In this work, we present a comparative study on the performance of the three most prominent retrieval models, the vector-space, probabilistic, and language-model based retrieval frameworks, when additional session context is incorporated.	information retrieval	Gaurav Pandey;Julia Luxenburger	2008		10.1007/978-3-540-78646-7_73	relevance;cognitive models of information retrieval;vector space;computer science;comparative research;multimedia;world wide web;information retrieval;language model;human–computer information retrieval	Web+IR	-33.7331297110708	-54.21578875054551	119973
c553cf0831dda6259348a0c32a9804b63651a63f	effects of inconsistent relevance judgments on information retrieval test results: a historical perspective	busqueda informacion;text;perspectiva;case history;performance evaluation;information retrieval system;information retrieval;lancaster f wilfrid frederick wilfrid 1933;pertinencia;gold standard;sistema de recuperacion de informacion;perspective;systeme de recherche d information;historique;recherche information;pertinence;relevance;estudio historico	The main objective of information retrieval (IR) systems is to retrieve information or information objects relevant to user requests and possible needs. In IR tests, retrieval effectiveness is established by comparing IR systems retrievals (systems relevance) with users’ or user surrogates’ assessments (user relevance), where user relevance is treated as the gold standard for performance evaluation. Relevance is a human notion, and establishing relevance by humans is fraught with a number of problems—inconsistency in judgment being one of them. The aim of this critical review is to explore the relationship between relevance on the one hand and testing of IR systems and procedures on the other. Critics of IR tests raised the issue of validity of the IR tests because they were based on relevance judgments that are inconsistent. This review traces and synthesizes experimental studies dealing with (1) inconsistency of relevance judgments by people, (2) effects of such inconsistency on results of IR tests and (3) reasons for retrieval failures. A historical context for these studies and for IR testing is provided including an assessment of Lancaster’s (1969) evaluation of MEDLARS and its unique place in the history of IR evaluation. LIBRARY TRENDS, Vol. 56, No. 4, Spring 2008 (“The Evaluation and Transformation of Information Systems: Essays Honoring the Legacy of F. W. Lancaster,” edited by Lorraine J. Haricombe and Keith Russell), pp. 763–783 (c) 2008 The Board of Trustees, University of Illinois Effects of Inconsistent Relevance Judgments on Information Retrieval Test Results: A Historical Perspective1	algorithm;database;experience;information systems;information retrieval;medline;network switch;observable;performance evaluation;process (computing);relevance;surrogates;tracing (software);web search engine	Tefko Saracevic	2008	Library Trends	10.1353/lib.0.0000	perspective;relevance;relevance;gold standard;computer science;artificial intelligence;sociology;information retrieval	Web+IR	-38.20672965185907	-61.02919620863396	120476
7d88fc61354803993cfe4254656711d359781a24	from subjective to objective metrics for evolutionary story narration using event permutations	automatic story narration;genotype phenotype mapping	The use of evolutionary computation to automatically narrate a story in a natural language, such as English, is a very daunting task. Two main challenges are addressed in this paper. First, how to represent a story in a form that is simple for evolution to work on? Second, how to evaluate stories using proper objective metrics? We address the first challenge by introducing a permutation-based linear representation that relies on capturing the events in a story in a genome, and on transforming any sequence represented by this genome into a valid story using a genotype-phenotype mapping. This mapping uses causal relationships in a story as constraints. The second challenge is addressed by conducting human-based experiments to collect subjective measurements of two categories of familiar and unfamiliar stories to the participants. The data collected from this exercise are then correlated with objective metrics that we designed to capture the quality of a story. Results reveal interesting relationships that are discussed in details in the paper.		Kun Wang;Vinh Bui;Eleni Petraki;Hussein A. Abbass	2012		10.1007/978-3-642-34859-4_40	computer science;artificial intelligence;machine learning;mathematics;algorithm	NLP	-42.86568251365946	-58.57239936471771	120501
8386212688ca969cdfdf703b05f53fc67031f800	micro and macro predictions: using sgoms to predict phone app game playing and emergency operations centre responses		In this study, we examine the ability of SGOMS models to predict human behaviour on two different scales, in micro cognitive task performance and in high level problem solving roles to better understand strategy use and training. To do this, two experiments were designed to isolate the role of knowledge structures in task performance. The first experiment involves modelling an application-based game, played on mobile phones. Results were compared to two models: the SGOMS model that matched the knowledge structures the players had learned during training, and a model optimized for speed, resulting in the fastest game play possible using ACT-R. In the second experiment we examined SGOMS predictions in a high level problem space of an Emergency Operations Center (EOC) simulation, with many interruptions and communication demands, comparing professional EOC managers and undergraduate performance. By comparing results between tasks, HCI design can be augmented using predictive modeling to inform the design to produce efficient and effective training programs.		Robert West;Lawrence Ward;Kate Dudzik;Nathan Nagy;Fraydon Karimi	2018		10.1007/978-3-319-91122-9_41	phone;emergency operations center;human–computer interaction;cognition;macro;computer science	HCI	-48.111661592687405	-53.70929259321296	121239
fe05157199c579e8d97ecc2383f6f3667bf8fe7b	the use of genetic programming to build boolean queries for text retrieval through relevance feedback	logica booleana;genetic program;information retrieval;query formulation;pertinencia;formulacion pregunta;algoritmo genetico;formulation question;inconveniente;inconvenient;retroaccion;disadvantage;retroaction;recherche information;pertinence;text retrieval;feedback regulation;algorithme genetique;logique booleenne;genetic algorithm;ejemplo;recuperacion informacion;relevance;boolean logic;example;relevance feedback;exemple	We report here on preliminary work on the use of evolutionary computing techniques which aims to improve Boolean information retrieval system performance through relevance feedback. There are many evolutionary techniques in computing, such as neural networks and genetic algorithms. One specific form of genetic algorithm technique has been used in our study: that of genetic programming. Terms from relevant documents are used to randomly create Boolean queries. Boolean queries are thought of as genetic programming organisms and, as such, are used for breeding to produce new organisms. Organisms which perform well, in terms of how good they are at retrieval, are given a better chance of being selected for breeding, with the result being that the overall fitness of the organisms improve to some extent. The aim is to develop the best Boolean query for an information need, given a small corpus of test documents, and then to use that query on the full collection to retrieve yet more relevant documents.	document retrieval;genetic programming;relevance feedback	Martin P. Smith;Martin M Smith	1997	J. Information Science	10.1177/016555159702300603	boolean algebra;genetic algorithm;relevance;standard boolean model;computer science;artificial intelligence;data mining;database;law;information retrieval;algorithm	Web+IR	-36.705987815732556	-60.454616018528654	121759
773d6f75bb15b4ca7e6ed379ecea71a02fd78cfb	a performance evaluation framework for library search engines	libraries;search engine;libraries search engines catalogs performance evaluation query processing usability;performance evaluation;query processing;search engines;digital libraries;catalogs;controlled experiment;brigham young university performance evaluation framework library search engine library query formulation query matching enlibs library patron;folksonomy;library of congress;word smilarity;word smilarity performance evaluation framework library search engine folksonomy;performance evaluation framework;library search engine;usability;evaluation model;evaluation framework;search engines digital libraries performance evaluation query processing	Libraries offer valuable resources to library patrons. Unfortunately, formulating library queries that match the rigid keywords chosen by the Library of Congress in library records to retrieve relevant results can be difficult. In solving this problem, we have developed a library search engine, called EnLibS, which allows library patrons to post a query Q with commonly-used words and ranks the retrieved library records according to their degrees of resemblance with Q. To evaluate the performance of EnLibS, it is imperative to conduct a thorough assessment. However, this performance evaluation cannot be conducted due to the lack of benchmark datasets and standardized metrics. To address this issue, in this paper we introduce an evaluation framework which (i) statistically determines the size of a test dataset, (ii) includes a controlled experiment that employs technically-sound approaches for calculating the ideal number of appraisers and queries to be used in the experiment, and (iii) establishes standard metrics for evaluating a library search engine. The proposed evaluation model can be applied to assess the performance of library search engines in (i) reducing the number of keyword queries that retrieve no results, (ii) obtaining high precision in retrieving and accurately ranking relevant library records, and (iii) achieving an acceptable query processing time. We present a case study in which we apply the proposed evaluation framework on the library search engine at Brigham Young University and EnLibS to assess, compare, and contrast their performance.	benchmark (computing);database;imperative programming;information needs;library (computing);library of congress classification;performance evaluation;web search engine	Maria Soledad Pera;Yiu-Kai Ng	2010	2010 Sixth International Conference on Signal-Image Technology and Internet Based Systems	10.1109/SITIS.2010.56	digital library;computer science;data mining;database;world wide web;information retrieval;search engine	DB	-37.251432076105885	-60.1394457975589	121918
544d6844d8d2aaa9b7fe8ccc7b96cfa92b8cbd40	why do you think this query is difficult?: a user study on human query prediction	query difficulty;information retrieval;difficulty understanding	Predicting if a query will be difficult for a system is important to improve retrieval effectiveness by implementing specific processing. There have been several attempts to predict difficulty, both automatically and manually; but without high accuracy at a pre-retrieval stage. In this paper, we focus rather on understanding Why a query is perceived by humans as difficult. We ran two separated but related experiments in which we asked humans to provide both a query difficulty prediction and reasons to explain their prediction. Results show that: (i) reasons can be categorized into 4 classes; (ii) reasons can be framed into closed questions to be answered on a Likert scale; and (iii) some reasons correlate in a coherent way with the human predicted numerical difficulty. On the basis of these results it is possible to derive hints to be provided to help users when formulating their queries and to avoid them to rely on their wrong perception of difficulty.	categorization;coherence (physics);experiment;numerical analysis;query language;usability testing	Stefano Mizzaro;Josiane Mothe	2016		10.1145/2911451.2914696	web query classification;computer science;data mining;information retrieval	HCI	-35.37230957048744	-53.872221793811605	121945
32de398985422eff9f93652b7794b036fe518f87	toward a definition of visual complexity as an implicit measure of cognitive load	web pages;web accessibility;semantic web;visual impairment;implicit measure;visual complexity;visual perception;knowledge elicitation;cognitive load	The visual complexity of Web pages is much talked about; “complex Web pages are difficult to use,” but often regarded as a subjective decision by the user. This subjective decision is of limited use if we wish to understand the importance of visual complexity, what it means, and how it can be used. We theorize that by understanding a user's visual perception of Web page complexity, we can understand the cognitive effort required for interaction with that page. This is important because by using an easily identifiable measure, such as visual complexity, as an implicit marker of cognitive load, we can design Web pages which are easier to interact with. We have devised an initial empirical experiment, using card sorting and triadic elicitation, to test our theories and assumptions, and have built an initial baseline sequence of 20 Web pages along with a library of qualitative and anecdotal feedback. Using this library, we define visual complexity, ergo perceived interaction complexity, and by taking these pages as “prototypes” and ranking them into a sequence of complexity, we are able to group them into: simple, neutral, and complex. This means we can now work toward a definition of visual complexity as an implicit measure of cognitive load.	baseline (configuration management);card sorting;human factors and ergonomics;requirements elicitation;theory;web page	Simon Harper;Eleni Michailidou;Robert Stevens	2009	TAP	10.1145/1498700.1498704	visual rhetoric;human–computer interaction;visual perception;computer science;artificial intelligence;semantic web;web accessibility;web page;multimedia;cognitive load;communication	HCI	-39.10087142123242	-52.543925928202675	122017
50adcc9d837664424d4aea9c695063da47ba8864	building an efficient indexing for crawling the web site with an efficient spider	truncation;indexing efficiency;web search engine;spiders and crawler.;inverse document frequency;object model;document processing;term frequency;indexation;search engine	With the present effort, we propose to investigate results of applying the Right-Truncated Index-Based Web Search Engine in order to determine its usefulness for storing and retrieving Arabic documents. The Right-Truncated Index- Based Web Search Engine, being a program for reading any set of Arabic documents accepts a query, and then processes both the documents and the query. Thus, it selects (predicts) those documents most relevant to the query which has been inserted. The program encompasses both a morphological component and a mathematical one. The morphological component allows the researcher to run either a stemming algorithm or a right-truncated algorithm. The chief advantage of the stemming algorithm is that it uses the least possible amount of storage for indexing by mapping the inflected and derived terms into a single, indexed-stem word. On the other hand, the right-truncated algorithm reduces the amount of storage to a lesser degree, but increases the probability of retrieving relevant (user-favorable) documents, compared to the stemming algorithm. One of the purposes of our investigation is to compare the efficiency of these two indexing mechanisms. The mathematical component of the algorithm accepts the output of the right truncation algorithm, and then employs both term-frequency and inverse document-frequency (TF-IDF) in order to establish the relative importance of each document, respective to the terms of the query. This paper also describes building a simple search engine based on a crawler or a spider. The clawer which indexes different types of documents is an algorithm to crawl the file systems from specified folder. A basic design and object model was developed to support single search word results as well as multiple search words results. It is capable of finding data to index by following (tracing) web links rather than searching directory listings in the file system. In this process files are downloaded through HTTP and HTML pages parsed in order to obtain more links without getting into a recursive loop. Also, this paper discusses how to improve indexing mechanism efficiency using a right truncated stemmer in terms of Arabic documents processing.	spider (solitaire)	Ghaleb Al-Gaphari	2009	Int. Arab J. Inf. Technol.		world wide web;dielectric;substrate (chemistry);doping;semiconductor device;silicon nitride;substrate (electronics);computer science;conductivity;parallel array	DB	-33.89550051198047	-59.82683033143615	122069
bf279a1736cd4597ff180bbaab0ea19a11bacc05	number frequency on the web	corpus statistics;wikipedia;numbers;benford s law;common crawl;zipf s law;web science;power law	In this article we investigate the properties of the frequency distribution of numbers on the Web. We work with a part of the Common Crawl dataset comprising 3.8 billion Web documents and a recent dump of the English language Wikipedia. We show that, like words, numbers on the Web follow a Power law distribution, and obey Benford's law of first-digits. We show and explain regularities in the distribution, and compare the regularities in Common Crawl to those in Wikipedia. The comparison stresses which patterns in the frequency distributions follow from human thought.	web page;wikipedia;world wide web	Willem Robert van Hage;Thomas Ploeger;Jesper Hoeksema	2014		10.1145/2567948.2576962	zipf's law;power law;computer science;data science;data mining;brand;world wide web;benford's law	NLP	-38.773857982904374	-62.916957299842686	122094
d999ebf183758337d02bd370191d9dd1422a60bb	estimating effective display size in online retrieval systems	special case;previous investigator;retrieval status value;relevant record;estimated number;effective display size;online retrieval system;relevant literature;commercial online retrieval system;online retrieval system display	This paper outlines a problem in commercial online retrieval systems, provides a review of the relevant literature, and presents a solution for a special case of the problem. Previous investigators have considered how to best determine, for a ranked list of records retrieved from an online retrieval system, whether or not the user should continue to display the output. This paper examines the problem of how effective display size can be estimated as a means of assisting the users of commercial online retrieval systems. Although no experimental results are as yet available, the approach presented here will provide a guide to and prolegomenon for systematic study of the problem, as well as a method for providing the estimated number of relevant records remaining in a retrieved set ranked by a retrieval status value.	display size	Danny P. Wallace;Bert R. Boyce;Donald H. Kraft	1987		10.1145/42005.42032	computer science;data mining;world wide web;information retrieval	Web+IR	-36.946751908614125	-55.534272920045545	122243
d1c5e415aa50903e62569c27f3e4e09c56728c37	the good, the bad, the difficult, and the easy: something wrong with information retrieval evaluation?	information retrieval evaluation;topic ease and difficulty;experimental validation;evaluation;trec	When we have an idea of student’s preparation (e.g., because of a previous written exam, or a term project, or after having asked the first questions), we even do something more. We ask difficult questions to good students, and we ask easy questions to bad students: ! What’s the point in asking easy questions to good students? They will almost certainly answer correctly, as expected, without providing much information about their preparation. ! What’s the point in asking difficult questions to bad students? They will almost certainly answer wrongly, without providing much information — and incidentally increase examiner’s stress level.	information retrieval	Stefano Mizzaro	2008		10.1007/978-3-540-78646-7_71	evaluation;data mining;information retrieval	HCI	-40.525631661679725	-64.07474901915033	122423
94e793e2442a0cb08edecadac5b5c68d164878c8	the strengths and limitations of teams for detecting problems	errors;pedestrian safety;poison control;injury prevention;safety literature;traffic safety;injury control;teams;home safety;injury research;safety abstracts;human factors;occupational safety;safety;sensemaking;safety research;problem detection;accident prevention;violence prevention;bicycle safety;team coordination;poisoning prevention;falls;ergonomics;suicide prevention	Problem detection in operational settings requires expertise and vigilance. It is a difficult task for individuals. If a problem is not detected early enough, the opportunity to avoid or reduce its consequences may be lost. Teams have many strengths that individuals lack. The team can attend to a wider range of cues than any of the individuals can. They can offer a wider range of expertise, represent different perspectives, reorganize their efforts to adapt to situational demands, and work in parallel. These should improve problem detection. However, teams can also fall victim to a wide range of barriers that may reduce their alertness, mask early problem indicators, confound attempts to make sense of initial data, and restrict their range of actions. Therefore, teams may not necessarily be superior to individuals at problem detection. The capability of a team to detect problems may be a useful measure of the team’s maturity and competence.		Gary Klein	2005	Cognition, Technology & Work	10.1007/s10111-005-0024-6	simulation;medicine;environmental health;engineering;suicide prevention;human factors and ergonomics;injury prevention;forensic engineering;computer security;mechanical engineering	ML	-48.148975001928264	-55.22739786792537	122681
37220f99e2369a7ba0b22b4b50ce1b5a66b05da7	experiences in evaluating multilingual and text-image information retrieval	busqueda informacion;besoin de l utilisateur;text;methode essai;recherche image;information retrieval;llamada;necesidad usuario;texte;accord frequence;necesidad informacion;tuning;user need;recherche information;besoin information;recall;detecteur ir;sintonizacion frecuencia;information need;test method;multilinguisme;infrared detector;texto;rappel;detector rayos infrarrojos;multilingualism;multilinguismo;metodo ensayo;image retrieval	One important step during the development of information retrieval ~IR! processes is the evaluation of the output regarding the information needs of the user. The “high quality” of the output is related to the integration of different methods to be applied in the IR process and the information included in the retrieved documents, but how can “quality” be measured? Although some of these methods can be tested in a stand-alone way, it is not always clear what will happen when several methods are integrated. For this reason, much effort has been put into establishing a good combination of several methods or to correctly tuning some of the algorithms involved. The current approach is to measure the precision and recall figures yielded when different combinations of methods are included in an IR process. In this article, a short description of the current techniques and methods included in an IR system is given, paying special attention to the multilingual aspect of the problem. Also a discussion of their influence on the final performance of the IR process is presented by explaining previous experiences in the evaluation process followed in two projects ~MIRACLE and OmniPaper! related to multilingual information retrieval.	algorithm;continuation;display resolution;image retrieval;incidence matrix;information needs;information retrieval;natural language processing;point of view (computer hardware company);precision and recall;relevance;response time (technology);significant figures;software framework;word-sense disambiguation	Ana M. García-Serrano;José Luis Martínez-Fernández;Paloma Martínez	2006	Int. J. Intell. Syst.	10.1002/int.20154	information needs;speech recognition;image retrieval;computer science;artificial intelligence;recall;test method	Web+IR	-35.697463330929864	-62.93329020247941	123167
d7177707dbd59c50062980c06c4f52749b77cd03	analogy and metaphors in images		Museums have large databases of images. The librarians that are using these databases are doing two types of images search: either they know what they are looking for in the database (a specific image or a specific set of well defined images such as kings of France), or they do not know precisely what they are looking for (e.g., when they are required to build images portfolios about some concepts such as “decency”). As each image is having a number of metadata, searching for a well-defined image, or for set of images, is easily solved. On the contrary, this is a hard problem when the task is to illustrate a given concept such as “freedom”, “decency”, “bread”, or “transparency” since these concepts are not metadata. How to find images that are somewhat analogs because they illustrate a given concept? We collected and analyzed the search results of librarians that were given themselves the task of finding images related to a given concept. Seven relations between the concept and the images were found as explanation of the selection of images for any concept: conceptual property, causality, effectivity semantic, anti-logic, metaphorical-vehicle and metaphorical-topic. The interrate agreement of independent judges that evaluated the relations was of .78. Finally, we designed an experiment to evaluate how much metaphor in images can be understandable.	causality;database;librarian	Charles Candau;Geoffrey Ventalon;Javier Barcenilla;Charles Tijus	2014		10.1007/978-3-319-08855-6_14	natural language processing	Vision	-41.24643415859252	-56.57049480903462	123183
09323de10bf78ab4e6004a5bfd07b95f4e6543ba	stuff i've seen: a system for personal information retrieval and re-use	interactive information retrieval;personal information management;web pages;user interface;information retrieval;user study;user studies;information discovery;user interfaces;knowledge work	Most information retrieval technologies are designed to facilitate information discovery. However, much knowledge work involves finding and re-using previously seen information. We describe the design and evaluation of a system, called Stuff I've Seen (SIS), that facilitates information re-use. This is accomplished in two ways. First, the system provides a unified index of information that a person has seen, whether it was seen as email, web page, document, appointment, etc. Second, because the information has been seen before, rich contextual cues can be used in the search interface. The system has been used internally by more than 230 employees. We report on both qualitative and quantitative aspects of system use. Initial findings show that time and people are important retrieval cues. Users find information more easily using SIS, and use other search tools less frequently after installation.	email;information discovery;information retrieval;web page	Susan T. Dumais;Edward Cutrell;Jonathan J. Cadiz;Gavin Jancke;Raman Sarin;Daniel C. Robbins	2003	SIGIR Forum	10.1145/2888422.2888425	executive information system;relevance;cognitive models of information retrieval;computer science;information filtering system;personal information management;management information systems;group information management;multimedia;user interface;world wide web;information retrieval;search engine;human–computer information retrieval	Web+IR	-36.478325239326665	-53.05802251092664	123213
8610d0a7e85b5c946bec4bdef81930819ba760bd	a decision theoretic approach to combining information filtering	busqueda informacion;modelizacion;information structure;filtering;filtrage;structure information;information retrieval;filtrado;information filtering;estructura informacion;global optimisation;controle information;modelisation;user profile;spam filtering;control informacion;recherche information;decision theoretic;information modelling;information system;information control;modeling;empirical evaluation;systeme information;test collection;design methodology;sistema informacion	Purpose – The purpose of this paper is to present an extension to a framework based on the information structure (IS) model for combining information filtering (IF) results. The main goal of the framework is to combine the results of the different IF systems so as to maximise the expected payoff (EP) to the user. In this paper we compare three different approaches to tuning the relevance thresholds of individual IF systems that are being combined in order to maximise the EP to the user. In the first approach we set the same threshold for each of the IF systems. In the second approach the threshold of each IF system is tuned independently to maximise its own EP (“local optimisation”). In the third approach the thresholds of the IF systems are jointly tuned to maximise the EP of the combined system (“global optimisation”).Design/methodology/approach – An empirical evaluation is conducted to examine the performance of each approach using two IF systems based on somewhat different filtering algorithms (TFIDF,...	information filtering system;theory	Alexander Binun;Bracha Shapira;Yuval Elovici	2009	Online Information Review	10.1108/14684520911001918	filter;systems modeling;design methods;computer science;artificial intelligence;machine learning;data mining;information retrieval;information system	ML	-36.51730617128937	-58.10163518385895	123356
16a3a0ed4543b40d4d85d1c6f0e845bb99e5e237	uncovering non-verbal semantic aspects of collaborative meetings: iterative design and evaluation of the meeting miner	analisis coocurrencia;busqueda informacion;iterative method;experience feedback;besoin de l utilisateur;concepcion ingenieria;analyse cooccurrence;engineering design;pistage;navegacion informacion;multimedia;usability evaluation;conception ingenierie;information retrieval;navigation information;real time;localization;retour experience;exigence usager;retorno experiencia;information browsing;exigencia usuario;rastreo;meeting browsing;semantics;necesidad usuario;online collaboration;localizacion;semantica;semantique;cooccurrence analysis;integrated design;concepcion integrada;preparacion serie fabricacion;metodo iterativo;qa75 electronic computers computer science;localisation;user need;recherche information;methode iterative;user requirement;temps reel;comportement utilisateur;iterative design;utilisabilite;tiempo real;user behavior;process planning;ingenierie simultanee;usabilidad;ingenieria simultanea;analisis semantico;analyse semantique;usability;preparation gamme fabrication;conception integree;multimedia meetings;comportamiento usuario;tracking;concurrent engineering;semantic analysis;qa76 computer software	This paper describes the process of iterative design and comprehensive evaluation of the Meeting Miner, a tool for browsing of recorded multimedia meetings. The Meeting Miner provides access to the speech content of recordings by tracking non-verbal interaction events collected in real time during on-line collaborative meeting activities. It emphasises semantic relationships between speech and discrete actions performed during the meeting and aggregates information through patterns of co-location and co-occurrence of actions. We report on the experience gained through developing functionality to enhance the user’s browsing experience, requirements regarding information feedback and the importance of flexibility in the browsing tool. In particular, we demonstrate how iterative development and evaluation can reveal areas where interface adaptation can play a useful role in enhancing the system’s usability.	iteration;iterative and incremental development;iterative design;iterative method;online and offline;requirement;usability	Matt-Mouley Bouamrane;Saturnino Luz	2008	Signal, Image and Video Processing	10.1007/s11760-008-0085-0	iterative design;simulation;internationalization and localization;usability;computer science;user requirements document;semantics;iterative method;tracking;multimedia;world wide web;concurrent engineering	HCI	-40.10732142976341	-54.80875667874099	123544
e54c702b570be08536ccd2dcc14e3f8c2e6fbb16	escam: a mobile application to capture and enhance text images		Taking high resolution photos with mobile devices anytime anywhere is becoming increasingly common. Therefore, images of all kinds of text documents are recorded. This work presents esCam, an application for Android platform, whose goal is to preprocess the images of those text documents, in particular, perspective correction and image cleaning and enhancing. What truly differentiates our application is that esCam focuses on treatment of text that may appear in the image, using neural networks. These preprocessing steps are needed to make easier the digitalization and also to benefit subsequent steps such as document analysis and text recognition.	mobile app	Joan Pastor-Pellicer;María José Castro Bleda;J. L. Adelantado-Torres	2015		10.1007/978-3-319-19222-2_50	computer vision;multimedia;information retrieval	HCI	-39.1665885446098	-53.99352607140435	124196
8805cebad253c4f74c35d9f92bc6ab9fc884adc1	developing a framework for an advisory message board for female victims after disasters: a case study after east japan great earthquake		After the sudden occurrence of East Japan Great Earthquake on 11 March 2011, triple disasters crippled the regular life of citizens of East Japan. A lot of people were affected, especially women victims suffered from different problems and worries: they had to care for elders, raise children, and find new jobs. Women also had specific needs of commodities for everyday life. Administrative authorities wanted to recognize women victims’ specific problems and provide them appropriate supports. However, it was difficult to grasp women victims’ requirements properly, because they were really patient and their needs were sometimes neglected under the environmental pressure. Conducting interviews and taking questionnaire from women victims is one way to gauge their needs, but it is time-consuming and labor intensive. This work proposes a framework for the development of an advisory message board for women victims on the web in which women victims can post their messages freely. The computational technologies are used here for analyzing digital media data in order to improve the lives of underserved or underprivileged people in case of disasters like earthquake. Text mining technologies are developed for automatic analysis of the messages to find out the specific needs of the victims and the change of needs with time and support them with proper advice. The proposed method uses latent semantic analysis (LSA) to extract the hidden topics and change of topics over time. As a case study, text messages from several on-line social media over a period of 2 years after the East Japan Great earthquake are collected and analyzed. It has been found that LSA-based technique is more effective in extracting the change of needs over time than graph-based topic extraction method. The final aim of this work is to develop the framework of advisory message board for women which will help the authority to find out the special needs of women victims after any future disaster and support them.		Takako Hashimoto;Yukari Shirota;Basabi Chakraborty	2016	DSH	10.1093/llc/fqv017	geography;genealogy;computer security;cartography	Visualization	-46.10191402836908	-62.52283021478742	124621
2330c59c0d4800131bc56e9ec2f815ea681862f0	a query-strategy-focused taxonomy and a customizable benchmarking framework for peer-to-peer information retrieval techniques	p2p system;comparative analysis;information retrieval;p2p;search strategy;p2p information retrieval techniques;p2p networks	P2P IR techniques are gaining momentum in both academic and industrial research communities, mainly due to the fact that they are extensively used-in-practice in a wide set of advanced applications ranging from e-business to e-government and e-procurement systems. P2P IR research is devoted to design innovative search strategies over P2P networks, whit the goal of making these strategies as more efficient and sophisticated as possible. In this respect, benchmarking P2P IR techniques is a leading aspect, and, at the same time, a nontrivial engagement as modeling the strongly decentralized nature and the rapidly-evolving dynamics of real-life P2P systems is still an open and uncompletely solved research challenge. Starting from the proposal of a taxonomy of P2P IR techniques, which emphasizes the query strategy used to retrieve information and knowledge from peers, this paper focuses on a customizable benchmarking framework that allows us to study, analyze, and benchmark P2P IR techniques according to several useful metrics, and under the ranging of a number of input parameters. Finally, a comparative analysis of some state-of-the-art P2P IR techniques developed on top of the proposed frame-work is presented and discussed in detail. This analysis further confirms the effectiveness and the reliability of our benchmarking framework for P2P IR techniques.	information retrieval;peer-to-peer	Alfredo Cuzzocrea	2007		10.1007/978-3-540-74819-9_90	computer science;data science;data mining;information retrieval	Web+IR	-41.34743764050641	-61.38198503879899	125061
f5df3896073df70f835d5326e2acb43b8fa3c237	chunking for experience		"""Human game players rely heavily on the experience gained by playing over the games of masters. A player may recall a previous game to either obtain the best move (if he has previously seen the identical position) or suggest a best move (if similar to others seen). However, game-playing programs operate in isolation, relying on the combination of search and programmed knowledge to discover the best move, even in positions well-known to humans. At best, programs have only a limited amount of information about previous games. This paper discusses enhancing a chess-playing program to discover and extract implicit knowledge from previously played grandmaster games, and using it to improve the chess program’s performance. During a game, a database of positions is queried looking for identical or similar positions to those on the board. Similarity measures are determined by chunking the position and using these patterns as indices into the database. Relevant information is subsequently passed back to the chess program and used in its decision making process. As the number of games in the database increases, the """"experience"""" available to the program improves the likelihood that relevant, useful information can be found for a given position."""	brute-force search;chess engine;cognitive dimensions of notations;epam;experience;graphical user interface;isolation (database systems);manhunters;open research;real-time clock;shallow parsing	Michael George;Jonathan Schaeffer	1990	ICGA Journal	10.3233/ICG-1990-13303	natural language processing;artificial intelligence;chunking (psychology);computer science	PL	-35.72814665155593	-53.979292138891566	125579
007a6d9546829f21c76cbbc6373c2228e83f49cc	comparing web logs: sensitivity analysis and two types of cross-analysis	busqueda informacion;metodo directo;analisis sensibilidad;search engine;optimisation;tratamiento transaccion;buscador;discriminator;optimizacion;information retrieval;metric;fichier log;fichero actividad;recherche information;sensitivity analysis;web log analysis;analyse sensibilite;metrico;optimization;discriminador;discriminateur;moteur recherche;transaction processing;methode directe;metrique;direct method;log file;traitement transaction	Different Web log studies calculate the same metrics using different search engines logs sampled during different observation periods and processed under different values of two controllable variables peculiar to the Web log analysis: a client discriminator used to exclude clients who are agents and a temporal cut-off used to segment logged client transactions into temporal sessions. How much are the results dependent on these variables? We analyze the sensitivity of the results to two controllable variables. The sensitivity analysis shows significant varying of the metrics values depending on these variables. In particular, the metrics varies up to 30-50% on the commonly assigned values. So the differences caused by controllable variables are of the same order of magnitude as the differences between the metrics reported in different studies. Thus, the direct comparison of the reported results is an unreliable approach leading to artifactual conclusions. To overcome the method-dependency of the direct comparison of the reported results we introduce and use a cross-analysis technique of the direct comparison of logs. Besides, we propose an alternative easy-accessible comparison of the reported metrics, which corrects the reported values accordingly to the controllable variables used in the studies.	blog;discriminator;log analysis;web search engine;world wide web	Nikolai Buzikashvili	2006		10.1007/11880592_39	direct method;transaction processing;metric;computer science;data mining;database;world wide web;sensitivity analysis;information retrieval;search engine	SE	-36.71648645271617	-59.228781251067424	125638
8aeb8bc596b557d9c19ead785dd539f7459748d8	metadata quality assessment metrics into ocw repositories		In this research work, it has been justified that ensuring OCW repositories metadata quality should be approached in haste, because of the relevance and influence it make search and discovery tools of resources and its direct impact on reuse of them. Through data analysis made, it has been possible to determine a quality level of OCW metadata repositories, regarding completeness and consistence metrics, where it has been determined that the completeness of the metadata of a resource does not necessarily ensure that it is described consistently. Finally, it is encouraged to apply other metrics in order to get a more complete evaluation.	opencourseware;quality engineering;relevance	Audrey Romero Pelaez;Pedro P. Alarcon	2017		10.1145/3175536.3175579	information retrieval;metadata;reuse;computer science	HPC	-39.18991844349754	-60.308163510809614	126107
b8f73e56324f65f018a293417b0f23ab605f5d89	intelwiki: recommending resources to help users contribute to wikipedia		We describe an approach to facilitating user-generated content within the context of Wikipedia. Our approach, embedded in the IntelWiki prototype, aims to make it easier for users to create or enhance the free-form text in Wikipedia articles by: i) recommending potential reference materials, ii) drawing the users’ attention to key aspects of the recommendations, and iii) allowing users to consult the recommended materials in context. A laboratory evaluation with 16 novice Wikipedia editors revealed that, in comparison to the default Wikipedia design, IntelWiki’s approach has positive impacts on editing quantity and quality, and perceived mental load.	embedded system;prototype;relevance feedback;user-generated content;wikipedia	Mohammad Noor Nawaz Chowdhury;Andrea Bunt	2014		10.1007/978-3-319-08786-3_35	computer science;data mining;multimedia;world wide web	HCI	-37.407456900853695	-53.95644627087446	126297
66725fe28c1431c182393b0e9249566535c87f66	chinese document re-ranking based on term distribution and maximal marginal relevance	busqueda informacion;document structure;lenguaje natural;estructura documental;information retrieval;structure document;langage naturel;interrogation base donnee;pertinencia;interrogacion base datos;chino;hierarchical classification;lenguaje descripcion;chinese information retrieval;marginal distribution;recherche information;natural language;pertinence;retroaction pertinence;classification hierarchique;ley marginal;relevance;chinois;chinese;clasificacion jerarquizada;relevance feedback;database query;document frequency;loi marginale;langage description;description language	In this paper, we propose a document re-ranking method for Chinese information retrieval where a query is a short natural language description. The method bases on term distribution where each term is weighted by its local and global distribution, including document frequency, document position and term length. The weight scheme lifts off the worry that very fewer relevant documents appear in top retrieved documents, and allows randomly setting a larger portion of the retrieved documents as relevance feedback. It also helps to improve the performance of MMR model in document re-ranking. The experiments show our method can get significant improvement against standard baselines, and outperforms relevant methods consistently.	marginal model;maximal set;relevance	Lingpeng Yang;Dong-Hong Ji;Mun-Kew Leong	2005		10.1007/11562382_23	marginal distribution;speech recognition;relevance;data mining;linguistics;natural language;chinese;information retrieval	Web+IR	-35.47404416402559	-61.82010536048988	127018
8d00c4a497fced79e15d4b81ea3565cf15611014	profile based information retrieval from printed document images	keyword based search;digital documents;information retrieval digital libraries document image processing;bilingual document images;information retrieval;digital library;text processing;digital libraries;document images;printed document images;information retrieval image retrieval image converters image recognition character recognition image segmentation optical character recognition software pixel software libraries natural languages;indexation;document image processing;word profiles;profile based information retrieval;word profiles document images;text processing profile based information retrieval printed document images bilingual document images keyword based search digital libraries	This paper performs a profile based Information Retrieval from printed document image collections. Keywords are valuable indexing tools and if they can be identified at the image level, extensive computation during recognition will be avoided. Printed documents can be scanned to produce document images. Instead of converting entire document images into text equivalent, word profiles are identified to match the word images in Bilingual document images.(English and Tamil). During retrieval, the same profile could be extracted from the user specified word and can be matched with the word images in the document. This yields a faster result even in a quality-degraded document. This kind of Information Retrieval (Keyword Based Search) can be adapted in Digital Libraries, which employs digitized documents instead of text processing. This promotes efficient search in document images irrespective of the language.	computation;digital library;experiment;image scanner;information retrieval;printing	S. Abirami;D. Manjula	2007	Computer Graphics, Imaging and Visualisation (CGIV 2007)	10.1109/CGIV.2007.67	natural language processing;document retrieval;visual word;speech recognition;document clustering;computer science;document layout analysis;information retrieval	Web+IR	-34.02636627756301	-65.1895535381225	127127
40f840d0760fea1185282b9718371e565a3e70a4	detection and application of influence rankings in small group meetings	behavioral research;small group research;psychology;learning systems;machine learning;automatic detection;mathematical models;dominance detection;influence detection;static analysis	We address the problem of automatically detecting participant's influence levels in meetings. The impact and social psychological background are discussed. The more influential a participant is, the more he or she influences the outcome of a meeting. Experiments on 40 meetings show that application of statistical (both dynamic and static) models while using simply obtainable features results in a best prediction performance of 70.59% when using a static model, a balanced training set, and three discrete classes: high, normal and low. Application of the detected levels are shown in various ways i.e. in a virtual meeting environment as well as in a meeting browser system.	experiment;sensor;test set	Rutger Rienks;Dong Zhang;Daniel Gatica-Perez;Wilfried Post	2006		10.1145/1180995.1181047	simulation;computer science;artificial intelligence;machine learning;mathematical model;mathematics;static analysis	HCI	-40.32904217269871	-53.532797626524584	127397
96546febeb3b50fb1d748868dcc75fb931e48221	author name disambiguation: what difference does it make in author-based citation analysis?	co citation analysis;filing;analisis citas;homonymie;citation analysis;analyse citation;auteur;autor;classement;cocitacion;bibliometrics;author;clasificacion;homonymy;cocitation;homonimia	In this paper, we explore how strongly author name disambiguation (AND) affects the results of an author-based citation analysis study, and identify conditions under which the commonly used simplified approach of using surnames and first initials may suffice in practice. We compare author citation ranking and co-citation mapping results in the stem cell research field 2004-2009 between two AND approaches: the traditional simplified approach of using author surnames and first initials, and a sophisticated algorithmic approach. We find that the traditional approach leads to extremely distorted rankings and substantially distorted mappings of authors in this field when based on firstor all-author citation counting, whereas last-author based citation ranking and co-citation mapping both appear relatively immune to the author name ambiguity problem. This is largely because romanized names of Chinese and Korean authors, who are very active in this field, are extremely ambiguous, but few of these researchers consistently publish as last authors in by-lines. We conclude that more earnest effort is required to deal with the author name ambiguity problem in both citation analysis and information retrieval, especially given the current trend towards globalization. In the stem cell field, where lab heads are traditionally listed as last authors in by-lines, last-author based citation ranking and co-citation mapping using the traditional simple approach to author name disambiguation may serve as a simple workaround, but likely at the price of largely filtering out Chinese and Korean contributions to the field as well as important contributions by young researchers.	algorithm;american cryptogram association;approximation;bibliometrics;citation analysis;co-citation;distortion;emoticon;factor analysis;information retrieval;internationalized domain name;lumpers and splitters;scientometrics;scopus;word-sense disambiguation;workaround	Andreas Strotmann;Dangzhi Zhao	2012	JASIST	10.1002/asi.22695	bibliometrics;computer science;data mining;citation analysis;world wide web;auteur theory;information retrieval;homonym	Web+IR	-39.0261242877611	-63.86549014458118	127631
62c4f1028fc5b2d0d88f728eccbff3e47dd7f1ee	a study on the effect of camera motion on human visual attention	visual saliency;fixation points distribution;image motion analysis;video summarization;saliency map;video signal processing;human attention model;statistical test;statistical significance;data mining;user generated video;visual saliency map;eye tracking human attention model regions of interest identification visual saliency map;camera motion;visualization;video cameras;region of interest;eye movement;video signal processing image motion analysis video cameras;cameras humans layout testing video sequences image processing laboratories psychology information analysis shape;statistical tests;humans;eye tracking;regions of interest identification;regions of interest identification camera motion effect human visual attention user generated video human attention model video summarization statistical tests eye movement visual saliency fixation points distribution;human visual attention;visual attention;camera motion effect;calibration;cameras;tracking;conferences	The aim of this paper is to examine the effect of camera motion in user generated video with respect to human visual attention. Having a more accurate human attention model is particularly useful in applications such as video summarization where the identification of visual importance is crucial to the quality of the results. Most models proposed thus far, have not considered camera motion as an independent factor in identifying visual saliency. In this study eye movement was recorded while subjects watched videos with different types of camera motion. The distribution of the fixation points indicated high correlation between the visual saliency and the type and direction of camera movement. The statistical tests confirmed that the results are statistically significant.	significant figures	Golnaz Abdollahian;Zygmunt Pizlo;Edward J. Delp	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4711849	computer vision;statistical hypothesis testing;computer science;mathematics;multimedia;statistics;computer graphics (images)	Robotics	-40.76672683462484	-52.476445045084446	127636
890288494f0779f4ad9f7668d9477b0193f7e4ee	an exploratory study of user goals and strategies in podcast search	universiteitsbibliotheek	We report on an exploratory, qualitative user study designed to identify users’ goals underlying podcast search, the strategies used to gain access to podcasts, and how currently available tools influence podcast search. We employed a multi-method approach. First, we conducted an online survey to obtain broad information on overall trends and perceptions regarding podcasts in general and podcast search in particular. Second, we used a diary study and contextual interviews to gain more detailed insights into the goals and behavior of key users. We find that goals underlying podcast search may be similar to those for blog search. Study participants report searching for podcasts to look for personal opinions and ideas, and favor topics like technology, news, and entertainment. A variety of search strategies is used to gain access to interesting podcasts, such as query-based search, directed and undirected browsing, and requested and unrequested recommendations. We find indications that goals and search strategies for podcast search are strongly influenced by perceptions of available tools, most notably the perceived lack of tools for online audio search.	blog;contextual inquiry;diary studies;directed graph;exploratory testing;futures studies;graph (discrete mathematics);informatics;podcast;population;relevance;search algorithm;usability testing;web search engine	Jana Besser;Katja Hofmann;Martha Larson	2008			simulation;computer science;multimedia;world wide web	HCI	-35.476836537016815	-52.42552747542921	127692
57c1da5bbd75d824509f8d192c45cd5edbfdc215	inferring frequently asked questions from student question answering forums		Question answering forums in online learning environments provide a valuable opportunity to gain insights as to what students are asking. Understanding frequently asked questions and topics on which questions are asked can help instructors in focusing on specific areas in the course content and correct students’ confusions or misconceptions. An underlying task in inferring frequently asked questions is to identify similar questions based on their content. In this work, we use hierarchical agglomerative clustering that exploits similarities between words and their distributed representations, reflecting both lexical and semantic similarity of questions. We empirically evaluate our results on real world labeled dataset to demonstrate the effectiveness of the method. In addition, we report the results of inferring frequently asked questions from discussion forums of online learning environment providing lectures to middle school and high school students.	cluster analysis;hierarchical clustering;online machine learning;question answering;semantic similarity	Renuka Sindhgatta;Smit Marvaniya;Tejas I. Dhamecha;Bikram Sengupta	2017			natural language processing;machine learning;computer science;question answering;artificial intelligence	NLP	-40.20150035467074	-59.865327074560845	127802
eca5307da2732c579e7347e776474a212006199b	difference in readability of mobile devices by age groups		We carried out experiments to evaluate the readability of e-books under various conditions of illuminance. We used two types of e-paper, Amazon Kindle Paperwhite and Sony Reader, as well as plain paper as a reference. In this study, we focused on the effects of the contrast ratios between characters and background of e-book readers in terms of readability. This study found a dependency between the contrast ratio of the text of each device and their readability according to age groups.		Kohei Iwata;Yuki Ishii;Tatsuya Koizuka;Takehito Kojima;Ranson Paul Lege;Masaru Miyao	2015		10.1007/978-3-319-20678-3_29	multimedia;advertising;world wide web	Robotics	-45.19868863479727	-58.52792216292563	129466
ca4b55801c8d02aa905c4b68f59da7c4b809097a	disputed sentence suggestion towards credibility-oriented web search	disputed sentence suggestion;users issue query;conventional query suggestion technique;web search;web search process;relevant disputed sentence;conventional query suggestion;web page;credibility-oriented web search;users search;suspicious web information	We propose a novel type of query suggestion to support credibilityoriented Web search. When users issue queries to search for Web pages, our system collects disputed sentences about queries from the Web. Then, the system measures how typical and relevant each of the collected disputed sentences are to the given queries. Finally, the system suggests some of the most typical and relevant disputed sentences to the users. Conventional query suggestion techniques focus only on making it easy for users to search for Web pages matching their intent. Therefore, when users search for Web pages to check the credibility of specific opinions or statements, queries suggested by conventional techniques are not always useful in searching for evidence for credibility judgments. In addition, if users are not careful about the credibility of information in the Web search process, it is difficult to be aware of the existence of suspicious Web information through conventional query suggestions. Our disputed sentence suggestion enhances users’ awareness of suspicious statements so that they can search for Web pages with careful attention to them.	mobile phone;relevance;web page;web search engine;world wide web	Yusuke Yamamoto	2012		10.1007/978-3-642-29253-8_4	web query classification;semantic search;computer science;data mining;database;web search query;world wide web;information retrieval	Web+IR	-34.612319260813315	-54.039959577687	129488
6b81788a586581f45e5c51e83e32617470683f07	analyzing question quality through intersubjectivity: world views and objective assessments of questions on social question-answering	satisfaction;social q a;question quality	Social question-answering (SQA) allows people to ask questions in natural language and receive answers from others. While research on SQA has focused on the quality of answers provided with implications for system-based interventions, few studies have examined whether the questions asked to elicit these answers accurately depict an asker’s information need. To address this gap, the current study explores the viability for system based interventions to improve questions by comparing human, non-textual assessments of question quality to automatic, textual features extracted from the questions’ content in order to determine whether there is a significant relationship between subjective judgments on one hand, and objective ones on the other. Findings indicate that not only is there a significant relationship between human-based ratings of question quality criteria and extracted textual features, but also that distinct textual features contribute to explaining the variability of each human-based rating. These findings encourage further study of the relationship between the reasons for why a question might be of poor quality and textual features that can be extracted from the question. This relationship can ultimately inform design of intervention-based systems that can not only automatically assess question quality, but also provide reasons that can be understood by the asker as to why the quality of his or her question is poor and suggest how to revise the question.	information needs;intersubjectivity;natural language;question answering;software quality assurance;spatial variability	Vanessa L. Kitzie;Erik Choi;Chirag Shah	2013		10.1002/meet.14505001052	divergent question;knowledge management;data mining;social psychology	Web+IR	-42.67004483040342	-58.76670002066382	129600
27f8db4a43ec5ea02d98c09be86959d62e0a9212	a hierarchical search history for web searching	search engine;user centered design;web search;task complexity	A hierarchical search history for Web searching was developed based on user-centered design principles and was proposed to assist users in controlling a Web search process. Two experimental search engines and browsers were developed. One was based on currently available search engines and the other was based on the user-centered search history design. An experiment was conducted to test the effectiveness of the search history. The dependent variables were the number of relevant Web sites identified during a 1-hr test period and satisfaction. The independent variables were type of search engine and task complexity. The experimental results suggested that with a high complexity task, search history improved users' search performance by 124% and satisfaction of use by 42.5% as compared to the current search engine.		Xiaowen Fang	2000	Int. J. Hum. Comput. Interaction	10.1207/S15327590IJHC1201_3	search engine indexing;user-centered design;database search engine;metasearch engine;human–computer interaction;semantic search;computer science;spamdexing;online search;data mining;search analytics;web search query;world wide web;information retrieval;search engine	Web+IR	-33.88821063286148	-52.0942480050905	129609
a7205a99bb5a4923dd24a3ed5ec5994ecd610921	a chinese mobile phone input method based on the dynamic and self-study language model	modelizacion;dynamic and self study language model;prensa;calculateur embarque;client server architecture;architecture client serveur;anpk;telephone portable;smoothing method;handheld computer;pervasive computing;chino;mobile phone;informatica difusa;modelisation;press;smoothing methods;telefono movil;hrfc;informatique diffuse;methode lissage;indexation;comportement utilisateur;boarded computer;presse;arquitectura cliente servidor;chinese digital input method;architecture of input method;user behavior;chinois;chinese;modeling;calculador embarque;language model;comportamiento usuario	This paper birefly introduces a Chinese digital input method named as CKCDIM (CKC Digital Input Method) and then applies it to the Symbian OS as an example, and it also proposes a framework of input method which adopted the Client/Server architecture for the handheld computers. To improve the performance of CKCDIM, this paper puts forward a dynamic and self-study language model which based on a general language model and user language model, and proposes two indexes which are the average number of pressed-keys (ANPK) and the hit rate of first characters (HRFC) to measure the performance of the input method. Meanwhile, this paper brings forward a modified Church-Gale smoothing method to reduce the size of general language model to meet the need of mobile phone. At last, the experiments prove that the dynamic and self-study language model is a steady model and can improve the performance of CKCDIM.	computer;digital data;experiment;input method;language model;mobile device;mobile phone;operating system;smoothing;symbian	Qiaoming Zhu;Peifeng Li;Ping Gu;Peide Qian	2006		10.1007/11807964_84	simulation;speech recognition;computer science;artificial intelligence;operating system;machine learning;database;distributed computing;programming language;world wide web;chinese;computer security;ubiquitous computing;algorithm;language model	NLP	-37.84001451310962	-58.24116664437538	129651
a259f887f7fbcb7d73b74d69ea384251134d0ee9	the role of the air traffic controller in future air traffic management: an empirical study of active control versus passive monitoring	memoire;carga mental;air traffic control;mental load;empirical study;conflict detection;controleur trafic;civil aviation;aviation safety;traffic controller;pedestrian safety;heart rate variability;ergonomia;poison control;conflict;role professionnel;injury prevention;mental workload;occupational role;general aviation;traffic control;hombre;safety literature;air transportation;ergonomie;traffic safety;injury control;home safety;injury research;passive monitoring;safety abstracts;human factors;memoria;conflicto;occupational safety;safety;human;air travel;safety research;accident prevention;passive control;violence prevention;supervisor trafico;bicycle safety;regulation trafic;conflit;poisoning prevention;flight simulators;falls;charge mentale;ergonomics;air traffic;suicide prevention;regulacion trafico;memory;rol profesional;active control;homme;air traffic controllers;empirical methods;air traffic management	Proposals for air traffic management such as Free Flight call for a transfer of responsibility for separation between aircraft from air traffic controllers (ATCos) to pilots. Under many proposals, the role of the ATCo will change from one of active control to passive monitoring. The present study directly compared these types of control with respect to ATCo mental workload, conflict detection, and memory. Eighteen ATCos participated in an air traffic control simulation of Free Flight procedures under moderate and high traffic load. Dependent variables included accuracy and timeliness in detecting potential conflicts, accepting and handing off aircraft, mental workload (as assessed by a secondary task, heart rate variability, and subjective ratings), and memory for aircraft location. High traffic density and passive control both degraded conflict detection performance. Actual or potential applications of this research include the recommendation that designs for future air traffic management keep authority for separation of aircraft with the controller.		Ulla Metzger;Raja Parasuraman	2001	Human factors	10.1518/001872001775870421	psychology;free flight;simulation;engineering;human factors and ergonomics;air traffic control;transport engineering;empirical research;computer security	AI	-46.21927352908033	-54.25320695545565	129685
c81abcee99864f2665ca18c6fc1213f495571332	putting google scholar to the test: a preliminary study	busqueda informacion;performance measure;evaluation performance;search engine;buscador;comparative analysis;performance evaluation;small sample size;search engines;information retrieval;estudio comparativo;evaluacion prestacion;google scholar;scholarly communication;information search;ask com;etude comparative;accuracy;precision;recherche information;comparative study;scholarship;moteur recherche;quantitative evaluation;design methodology;yahoo	Purpose – To describe a small-scale quantitative evaluation of the scholarly information search engine, Google Scholar. Design/methodology/approach – Google Scholar’s ability to retrieve scholarly information was compared to that of three popular search engines: Ask.com, Google and Yahoo! Test queries were presented to all four search engines and the following measures were used to compare them: precision; Vaughan’s Quality of Result Ranking; relative recall; and Vaughan’s Ability to Retrieve Top Ranked Pages. Findings – Significant differences were found in the ability to retrieve top ranked pages between Ask.com and Google and between Ask.com and Google Scholar for scientific queries. No other significant differences were found between the search engines. This may be due to the relatively small sample size of eight queries. Results suggest that, for scientific queries, Google Scholar has the highest precision, relative recall and Ability to Retrieve Top Ranked Pages. However, it achieved the lowest score for these three measures for non-scientific queries. The best overall score for all four measures was achieved by Google. Vaughan’s Quality of Result Ranking found a significant correlation between Google and scientific queries. Research limitations/implications – As with any search engine evaluation, the results pertain only to performance at the time of the study and must be considered in light of any subsequent changes in the search engine’s configuration or functioning. Also, the relatively small sample size limits the scope of the study’s findings. Practical implications – These results suggest that, although Google Scholar may prove useful to those in scientific disciplines, further development is necessary if it is to be useful to the scholarly community in general. Originality/value – This is a preliminary study in applying the accepted performance measures of precision and recall to Google Scholar. It provides information specialists and users with an objective evaluation of Google Scholar’s abilities across both scientific and non-scientific disciplines and paves the way for a larger study.	google scholar;precision and recall;web search engine	Mary L. Robinson;Judith Wusteman	2007	Program	10.1108/00330330710724908	google panda;search engine optimization;computer science;google penguin;data mining;database;accuracy and precision;world wide web;information retrieval;search engine	Web+IR	-37.31650817642478	-60.03632533777866	129686
6f1cfc6f2bd879b628e255cec1ba5eb9ddaf9089	the research on fuzzy data mining applied on browser records	consumidor;extraction information;association statistique;entreprise;navegacion informacion;markets;web pages;red www;analisis datos;mercado;information extraction;fuzzy data;consommateur;navigation information;algoritmo borroso;empresa;logique floue;comercializacion;reseau web;information browsing;statistical association;logica difusa;data mining;fuzzy set theory;fuzzy logic;commercialisation;data analysis;fichier log;maximum forward reference;asociacion estadistica;fichero actividad;regle association;internet;regla asociacion;association rule;fouille donnee;marketing;consumer;fuzzy algorithm;marche;firm;preferencia;algorithme flou;world wide web;analyse donnee;preference;publicidad;publicite;busca dato;extraccion informacion;log file;everyday life;advertising	With the technological advances, the Internet has been an important part of everyday life. Governmental institutions and enterprises tend to advertise and market through the internet. With the travelling records of browsers, one can analyze the preference of web pages, further understand the demands of consumers, and promote the advertising and marketing. In this study, we use Maximum Forward Reference (MFR) algorithm to find the travel pattern of browsers from web logs. Simultaneously, experts are asked to evaluate the fuzzy importance weightings for different webs. Finally, we employ fuzzy data mining technique that combines apriori algorithm with fuzzy weights to determine the association rules. From the yielded association rules, one can be accurately aware of the information consumers need and which webs they prefer. This is important to governmental institutions and enterprises. Enterprises can find the commercial opportunities and improve the design of webs by means of this study. Governmental institutions can realize the needs of people from the obtained association rules, make the promotion of policy more efficiently, and provide better services.	data mining	Qingzhang Chen;Jianghong Han;Yungang Lai;Wenxiu He;Keji Mao	2005		10.1007/11527503_63	fuzzy logic;association;the internet;association rule learning;consumer;computer science;web page;data mining;fuzzy set;data analysis;world wide web;information extraction	ML	-36.49148920314464	-57.06632379279794	130118
e47c8f9d9a6bc2c3d50593e341cce3703e3df803	spam mail filtering system using semantic enrichment	bayes estimation;filtering;evaluation performance;filtrage;bayesian classifier;sistema experto;haute performance;bayesian classification;electronic mail;performance evaluation;red www;rule based system;evaluacion prestacion;filtrado;reseau web;semantics;base connaissance;correo electronico;semantica;semantique;classification;enrichment;courrier electronique;estimacion bayes;internet;enrichissement;comportement utilisateur;alto rendimiento;world wide web;base conocimiento;user behavior;information system;systeme expert;high performance;enriquecimiento;clasificacion;systeme information;comportamiento usuario;estimation bayes;sistema informacion;knowledge base;expert system	As the Internet infrastructure has been developed, E-mail is regarded as one of the most important methods for exchanging information because of easy usage and low cost. Meanwhile, exponentially growing unwanted mails in users' mailbox have been raised as a main problem. To solve this problem, researchers have suggested many methodologies that are based on Bayesian classification. The kind of system usually shows high performances of precision and recall. But they have several problems. First, it has a cold start problem, that is, training phase has to be done before execution of the system. The system must be trained about spam and non-spam mail. Second, its cost for filtering spam mail is higher than rule-based system. Third, if E-mail has only few terms those represent its contents, the filtering performance is fallen. In this paper, we have focused on the last issued problem and we suggest spam mail filtering system using Semantic Enrichment. For the experiment, we tested the performance by using the measurements like precision, recall, and F1-measure. As compared with Bayesian classifier, the proposed system obtained 4.1%, 10.5% and 7.64% of improved precision, recall and Fl-measure, respectively.	gene ontology term enrichment	Hyun-Jun Kim;Heung-Nam Kim;Jason J. Jung;GeunSik Jo	2004		10.1007/978-3-540-30480-7_64	knowledge base;naive bayes classifier;computer science;artificial intelligence;data mining;database;semantics;world wide web;expert system	Robotics	-36.38350448869811	-58.44332995942438	130239
3ae6f4cdbdb936a93462495cbff5b4fc7bc9ce75	on-line tutorials: what kind of inference leads to the most effective learning?	empirical study	This paper presents an empirical study comparing the effectiveness of four different versions of an on-line database tutorial, each of which calls upon the student to perform a different kind of inference. The general-to-specific version presents instructions in the form of general rules, from which the students expected to infer how to apply the rule in the give context. The explanation-to-specific version supplies information about the functional organization of the database program in addition to general rules. The specific-to-specific condition gives an example of the use of a command; the student must infer how to apply the command in a slightly different context. The control version gives explicit instructions. The best performance on a post-test consisting of realistic tasks was obtained from the general-to-specific and explanation-to-specific conditions.	command (computing);online and offline	John B. Black;J. Scott Bechtold;Marco Mitrani;John M. Carroll	1989		10.1145/67449.67467	simulation;computer science;operating system;machine learning;data mining;empirical research	NLP	-46.52922619044507	-59.575621957488934	130665
49c235228dc998fa029e3999c1f85d49c3cdfe0b	user-defined relevance criteria: an exploratory study	exploratory study;information needs;information retrieval;content analysis;interviews;evaluation methods;relevance information retrieval;higher education	The objective of this study was to describe the criteria mentioned by users evaluating the information within documents as it related to the users’ information need situations. Data were collected by asking users in an academic environment to evaiuate representations and the full text of documents that had been retrieved specifically for each user’s information need situation. Users were asked to mark the portions of the document representations or of the full text of documents that indicated to the users whether they would or would not pursue the information within documents. An open-ended interview technique was then employed to discuss each marked portion with users. The interviews were audiotaped, the tapes transcribed, and the transcriptions were content analyzed in order to identify and describe evaluation criteria. The results indicate that the criteria employed by users included tangible characteristics of documents (e.g., the information content of the document, the provision of references to other sources of information), subjective qualities (e.g., agreement with the information provided by the document) and situational factors (e.g., the time constraints under which the user was working). The implications of this research for our understanding of the concept of relevance, and for the design and evaluation of information retrieval systems, are discussed.	common criteria;document;exploratory testing;information needs;information retrieval;nonlinear gameplay;relevance;self-information	Carol L. Barry	1994	JASIS	10.1002/(SICI)1097-4571(199404)45:3%3C149::AID-ASI5%3E3.0.CO;2-J	document retrieval;information needs;relevance;content analysis;computer science;artificial intelligence;operations research;information retrieval	Web+IR	-36.20630746178787	-55.146499405782606	130862
57033c305d1028c60260a272d14b8697a7728d2d	a roland for an oliver? subjective perception of cooperation during conditionally automated driving	conditional automation human machine cooperation subjective metrics cooperative lane change;subjective metrics;human machine cooperation;traffic engineering computing intelligent transportation systems road safety road traffic;conditional automation;vehicles timing automation semantics roads safety user interfaces;lane change situations cooperation perception conditionally automated driving cooperative driving lane change scenarios cooperative behavior driving simulator satisfaction relaxation accordance asymmetric perception exploratory factor analysis;cooperative lane change	Cooperative driving, as observed during lane-change scenarios, reminds one of the saying “a Roland for an Oliver”, also known as quid pro quo: being offered a gap for moving in needs someone offering it in the left lane. This article investigates the subjectively perceived differences of cooperative behavior from the perspective of driving in the left and in the right lane. We design and use a questionnaire in a driving simulator study. As predictor variables, we modify timing and success of the lane-change. Cooperation is measured in the aspects satisfaction, relaxation, and accordance. The results show differences in the performance: Subjects perceive a successful lane-change as more cooperative than an unsuccessful one. The findings furthermore reveal the asymmetric perception: The right lane perspective is experienced more cooperative than the left lane perspective. Modifications in timing or success furthermore lead to a lower level of trust. An exploratory factor analysis of the questionnaire's results furthermore suggests two hidden factors intrinsic and extrinsic contribution to cooperation. Several lane-change situations are analyzed by means of the two factors.	autonomous car;device driver;driving simulator;exploratory factor analysis;failure;kerrison predictor;linear programming relaxation;self-organized criticality	Markus Zimmermann;Larissa Fahrmeier;Klaus Bengler	2015	2015 International Conference on Collaboration Technologies and Systems (CTS)	10.1109/CTS.2015.7210400	simulation;artificial intelligence;management;computer security	Robotics	-47.90141102931416	-52.47211478993015	131122
02bf758329a59ff5821cd765f20070c21a12b422	pilots' latency of first fixation and dwell among regions of interest on the flight deck	attention distribution;flight deck design;fixation duration;eye movement	The purpose of this pilot study is to investigate the differences of eye movements among three different flight backgrounds. There were eleven par- ticipants (2 military pilots with average 2,250 flying hours, 6 commercial pilots with average 5,360 flying hours, and 3 novices). All participants wear a mobile eye tracker during the experiment operating a Boeing 747 flight simulator for landing. The eye tracker recorded all participants' eye movement data auto- matically. The average values of the latency of first fixation (LFF) and the total contact time (TCT) for five regions of interest (ROIs) are used to examine proposed hypotheses. The findings include: (1) participants of different flight backgrounds have different sequences of viewing ROIs; (2) participants of military pilots and novices spent most of time viewing the outside of cockpit (ROI-3); however, participants of commercial pilots spent most of time viewing the Primary Flight Display (ROI-1). Current research findings might be applied for developing conversion training for military pilots conversed to civil airlines pilots. The fundamental reasons of why pilots viewing ROIs in different sequence and spending significant different time on the ROIs needed to be studied further in the future.	interrupt latency;region of interest	Hong-Fa Ho;Hui-Sheng Su;Wen-Chin Li;Chung-san Yu;Graham Braithwaite	2016		10.1007/978-3-319-40030-3_38	simulation;eye movement	Theory	-46.40709525607255	-53.08254707641356	131422
cd8e75bcf1dbcf9b8a4ba0bac7a0cb54479d5599	can user tagging help health information seekers?	expert metadata;information retrieval;user tags;social networks	The Web is becoming a regular source of information for health information (HI) seekers. In the United States for example, several studies report that more than 80% of Internet users routinely utilize the Web to obtain medical and health information. While Google searching is by far the default point of access for HI seekers, using expert Web sites such as Medline+ and WebMD is also another alternative for users that are looking for quality content. In addition, health expert Web sites are supported with several advanced querying such as browsing using categories organized either as a hierarchy or in an A-Z search list. This mode of searching supported by expert generated taxonomies or metadata is particularly suitable for discovery purposes. In parallel social Web sites such as Delicious and StumbleUpon are providing Web users with a new opportunity to participate in the process of annotating Web resources. This is particularly true for health data as supported by the large amount of health resources annotated by Delicious for example. In this paper we investigate the relationship between expert health metadata and user annotations where the objective is to determine how they can be combined to provide HI seekers with better means to finding their HI needs. The results of the study show that while there is noticeable overlapping between these two types of data, user tags are clearly a new type of metadata that will play an important role in supporting HI seekers searching/browsing health resources.		Malika Mahoui;Josette F. Jones;Andrew Meyerhoff;Syed Ahmed Toufeeq	2011		10.1007/978-3-642-21657-2_42	computer science;data mining;world wide web;information retrieval	HCI	-34.515726734944295	-55.26135672483601	131578
c8a893b01231b6f0b85f0705f95c7aacfd93aaa4	color universal design: analysis of color category dependency on color vision type	color difference;visual communication;color vision;visual communications;universal design	This report is af ollow-up to SPIE-IS+T / Vol. 7528 7528051-8, SPIE-IS+T / Vol. 7866 78660J-1-8 and SPIE-IS+T / Vol. 8292 829206-1-8.nColors are used to communicate information in various situations, not just for design and apparel. However, visual information given only by color may be perceived differently by individuals with different color vision types. Human color vision is non-uniform and variation in most cases is genetically linked to L-cones and M-cones. Therefore, color appearance is not same for all color vision types. Color Universal Design is an easy-to-understand system that was created to convey color-coded information accurately to most people, taking color vision types into consideration. In present research, we studied trichromat (C-type), prolan (P-type), and deutan (D-type) forms of color vision.nWe here report result of two experiments. The first was validation of colors using color chart on CIELAB uniform color space. We made an experimental color chart (total of color cells is 622, color difference between color cells is 2.5) for fhis experiment, and subjects have P-type or D-type color vision. From data we were able to determine the limits with high probability of confusion and the limits with possible confusionnaround various basing points. The direction of former matched with theoretical locus, but range did not extend across entire a* range. The latter formed a belt-like zone above and below theoretical locus. This way we re-analyzed a part of theoretical locus suggested by Pitt-Judd. The second was an experiment in color classification of subjects with C-type, P-type, or D-type color vision. The color caps of fhe 100 Hue Test were classified into seven categories for each color vision type. The common and different points of color sensation were compared for each color vision type, and we were able to find a group of color caps fhat people with C-, P-, and D-types could all recognize as distinguishable color categories. The result could be used as basis of a color scheme for futurenColor Universal Design.	color vision	Yasuyo G. Ichihara;Natsuki Kojima;Kei Ito	2010		10.1117/12.838802	computer vision;color model;hsl and hsv;color difference;color balance;physics;visual communication	Robotics	-42.17315474919128	-52.52023713875425	131681
a588e889590ed858755b4faf9154fe54c2534642	using a light-weighted cache pool to leverage the performance of web picture databases	busqueda informacion;characteristic;base donnee;red www;information retrieval;digital library;interrogation base donnee;reseau web;database;interrogacion base datos;base dato;cache memory;caracteristica;antememoria;antememoire;biblioteca electronica;internet;extremite;end;recherche information;extremidad;caracteristique;world wide web;electronic library;enseignement;educacion;database query;bibliotheque electronique;teaching;ensenanza	In most web digital libraries, picture databases play an important role in making the contents more completed, illustrative and attractive. However, in some special purpose (e.g. for education or for news publication) digital libraries, in which picture databases are absolutely necessary at the back end, the phenomenon of hotspot often pulls down the response speed of queries on picture databases and makes the whole performance decline seriously. This paper brings forward the idea of caching hot data in a pool to alleviate the above problem. And by utilizing the characteristics of hotspot, we implement a light-weighted cache pool which is very effective in improving picture access performance.	database	Chao Li;Chunxiao Xing;Lizhu Zhou	2003		10.1007/978-3-540-24594-0_53	end;digital library;the internet;cpu cache;computer science;artificial intelligence;operating system;database;distributed computing;multimedia;characteristic;world wide web;computer security	DB	-37.14536328169274	-58.348090675189326	131834
4709c3b3ae4e778906a59338293085f82e5da12e	effect of a virtual reality interface on the learning curve and on the accuracy of a surgical planner for total hip replacement	total hip replacement;surgical planning;genie biomedical;learning curve;haptic device;virtual reality;biomedical engineering;ingenieria biomedica;clinical assessment	The aim of this study is to evaluate the performance of a non-conventional input and output device (virtual reality) in a total hip replacement surgical planner. A test was performed asking five users to position a cup in a defined position. Every user performed the task using three different hardware configurations: (I) conventional mouse and monitor, (II) mouse and auto-stereoscopic monitor, and (III) 12-DOF tracker (haptic device) and auto-stereoscopic monitor. The results were evaluated in terms of root mean square error of the obtained position with respect to the target one and in terms of learning curve. The results showed that the examined VR technology does not show a sufficient positioning accuracy to be considered for clinical assessment.		Luca Petrolo;Debora Testi;Fulvia Taddei;Marco Viceconti	2010	Computer methods and programs in biomedicine	10.1016/j.cmpb.2009.11.002	simulation;computer science;virtual reality;haptic technology;learning curve;surgery	HCI	-42.85783969193485	-54.041178412958054	131840
bd828faa5068f4d624db094ee6e5d3dccb35f65a	deciphering cluster representations	etude utilisateur;documento;representation;user evaluation;cluster;critical study;red www;search result;amas;information retrieval;user study;browsing;estudio usuario;user feedback;produit recherche;problems;percepcion;etude critique;resultado busqueda;task difficulty;document;estudio critico;dificultad tarea;difficulte tâche;internet;recherche information;clustering;information representation;world wide web;search strategies;reseau www;monton;recuperacion informacion;perception;representation information;information seeking;representacion	There are several recent studies that propose search output clustering as an alternative representation method to ranked output. Users are provided with cluster representations instead of lists of titles and invited to make decisions on groups of documents. This paper discusses the diculties involved in representing clusters for usersÕ evaluation in a concise but easily interpretable form. The discussion is based on ®ndings and user feedback from a user study investigating the eectiveness of search output clustering. The overall impression created by the experiment results and usersÕ feedback is that clusters cannot be relied on to consistently produce meaningful document groups that can easily be recognised by the users. They also seem to lead to unrealistic user expectations.	cluster analysis;high-level programming language;openness;session (web analytics);usability testing;utility	Yasemin Kural;Stephen E. Robertson;Susan Jones	2001	Inf. Process. Manage.	10.1016/S0306-4573(00)00037-6	the internet;computer science;data mining;multimedia;cluster analysis;perception;representation;world wide web;information retrieval;cluster	Web+IR	-38.480762457564126	-58.01865618124484	131977
8bee912c58d5c1019718ce4ea01ba91d8b430898	using librarian techniques in automatic text summarization for information retrieval	information extraction;user interface;automatic text summarization;information retrieval;information technology;information retrieval user interfaces;computer science;reference librarian techniques	A current application of automatic text summarization is to provide an overview of relevant documents coming from an information retrieval (IR) system. This paper examines how Centrifuser, one such summarization system, was designed with respect to methods used in the library community. We have reviewed these librarian expert techniques to assist information seekers and codified them into eight distinct strategies. We detail how we have operationalized six of these strategies in Centrifuser by computing an informative extract, indicative differences between documents, as well as navigational links to narrow or broaden a user's query. We conclude the paper with results from a preliminary evaluation.	automatic summarization;information retrieval;librarian	Min-Yen Kan;Judith L. Klavans	2002		10.1145/544220.544227	relevance;multi-document summarization;cognitive models of information retrieval;computer science;automatic summarization;data mining;user interface;information technology;world wide web;information extraction;information retrieval;human–computer information retrieval	Web+IR	-34.275284162064814	-55.55377295758056	132565
e5825f81ebc4a751abd2278eafae143a2f5689d3	detecting duplicate biological entities using markov random field-based edit distance	sensibilidad contexto;context aware;duplicate detection;genie biomedical;proceso markov;integration information;digital library;data collection;edit distance;token protocol;database;chaine caractere;base dato;emparejamiento de cadenas;bioinformatique;probabilistic approach;protocole jeton;markov random field;information integration;biblioteca electronica;biomedical engineering;enfoque probabilista;approche probabiliste;processus markov;cadena caracter;integracion informacion;markov process;base de donnees;tâche appariement;distancia;tarea apareamiento;ingenieria biomedica;electronic library;appariement chaine;bioinformatica;biological data;sensibilite contexte;biomedical digital libraries;string matching;matching task;protocolo testigo;bibliotheque electronique;distance;character string;bioinformatics	Detecting duplicate entities in biological data is an important research task. In this paper, we propose a novel and context-sensitive Markov random field-based edit distance (MRFED) for this task. We apply the Markov random field theory to the Needleman–Wunsch distance and combine MRFED with TFIDF, a token-based distance algorithm, resulting in SoftMRFED. We compare SoftMRFED with other distance algorithms such as Levenshtein, SoftTFIDF, and Monge–Elkan for two matching tasks: biological entity matching and synonym matching. The experimental results show that SoftMRFED significantly outperforms the other edit distance algorithms on several test data collections. In addition, the performance of SoftMRFED is superior to token-based distance algorithms in two matching tasks.	biological database;clique (graph theory);coefficient;context-sensitive grammar;edit distance;entity;experiment;information retrieval;instruction path length;linkage (software);markov chain;markov random field;minimum-weight triangulation;motif;needleman–wunsch algorithm;precision and recall;quantum field theory;sensor;substring;test data;tf–idf;word-sense disambiguation	Min Song;Alex Rudniy	2009	Knowledge and Information Systems	10.1007/s10115-009-0254-7	damerau–levenshtein distance;digital library;speech recognition;edit distance;string;biological data;computer science;information integration;machine learning;database;markov process;distance;jaro–winkler distance;algorithm;statistics;string searching algorithm;data collection	ML	-37.13399056695707	-65.39773158089193	132681
041e513bdd10dff8224785bf0289be8b8fae5329	exploiting citation overlaps for information retrieval: generating a boomerang effect from the network of scientific papers	evaluation performance;pilot study;performance evaluation;and forward;information retrieval;evaluacion prestacion;diachrone;synchronous;systeme recherche;search strategy;citation;synchrone;search system;sincronico;sistema investigacion;recherche information;strategie recherche;citacion;document retrieval;recuperacion informacion;polyrepresentation;estrategia investigacion	"""A new citation search strategy is proposed for Information Retrieval (IR) based on the principle of polyrepresentation (Ingwersen, 1992, 1996). The strategy exploits logical overlaps between a range of cognitively different interpretations of the same documents in a structured manner, i.e. so-called cognitive overlaps of representations. The strategy is essentially a """"cycling strategy"""" starting with documents retrieved by a subject search, wherefrom new documents are identified automatically by following the network of citations in scientific papers backwards and forwards in time. In contrast to earlier citation search strategies the proposed strategy does not require known relevant documents (seed documents) as a starting point, but may be based on a subject search. A pilot study is reported where the ability of the strategy to retrieve additional relevant documents is analysed. Results show that a very large amount of documents can be retrieved by the strategy, and that these may be segmented in a number of distinct """"overlap levels"""". It is demonstrated that the combined core of the higher-level overlaps contains higher relevance density than found in the original retrieval results. Based on these results it is suggested that the documents be displayed in order of their presence in higher-level overlaps, so as to maximise the chances that as many relevant documents as possible will be presented first to a user."""	boomerang;citation index;document;high- and low-level;information retrieval;relevance;scientific literature	Birger Larsen	2002	Scientometrics	10.1023/A:1016011326300	document retrieval;computer science;data mining;world wide web;synchronous learning;information retrieval	Web+IR	-35.657724912997715	-61.08335135111681	133035
9fdbd9a06944ee27ed9947b24d49f66a79e0884a	estimating effective display size in online retrieval systems	on line processing;affichage;visualizacion;information retrieval;user assistance;cite;tratamiento en linea;assistance utilisateur;display;recherche information;asistencia usuario;recuperacion informacion;traitement en ligne;sire	Abstract   This article outlines a problem in commercial online retrieval systems, provides a review of the relevant literature, and presents a solution for a special case of the problem. Previous investigators have considered how to best determine, for a ranked list of records retrieved from an online retrieval system, whether or not the user should continue to display the output. This article examines the problem of how effective display size can be estimated as a means of assisting the users of commercial online retrieval systems. Although no experimental results are as yet available, the approach presented here will provide a guide to and prolegomenon for systematic study of the problem, as well as a method for providing the estimated number of relevant records remaining in a retrieved set ranked by a retrieval status value.	display size	Danny P. Wallace;Bert R. Boyce;Donald H. Kraft	1988	Inf. Process. Manage.	10.1016/0306-4573(88)90099-4	iliffe vector;simulation;computer science;world wide web;information retrieval	Web+IR	-36.964120893079404	-55.64587163688792	133072
3d1167f1596ea59dacf8f80a5248dffc3f60530e	efficient information representation method for driver-centered ar-hud system		Providing a suitable and efficient representation of a driver’s perspective is a way to reduce traffic accidents. In this paper, we first introduce a driver-centered AR (augmented-reality) HUD (head-up-display) system that superimposes augmented virtual objects onto a real scene under all types of driving situations including unfavorable weather (such as rainy, foggy, overcast, and snowy) conditions. We next explain the scenario and method used in our comparative experiments on a method for improving both the cognitive usability and visibility of drivers. For this, we comparatively analyzed not only information display locations but also information representation for six information types using a driving simulator with thirty subjects. For the effects on safety, the situational driver awareness of safety-related road events was measured. To determine the differences in the visual cognitive workload placed upon drivers, we tracked their eye movements. The subjective workload of the participants was assessed using the RSME (Rating Scale Mental Effort).	ar (unix);augmented reality;device driver;display device;driving simulator;experiment;head-up display;rating scale;usability	Hyesun Park;Kyong-ho Kim	2013		10.1007/978-3-642-39238-2_43	computer vision;simulation;geography;computer graphics (images)	HCI	-46.74333097581492	-53.04182172082573	133136
8f221e994e95384ccb9e2f0428fd168c6a555252	an approach for measurement of passenger comfort: real-time classification based on in-cabin and exterior data		The comfort level of passengers is an important factor in measuring user experience in any form of transportation, including in autonomous vehicles. One of the main factors that determines user acceptance of autonomous vehicles is the passenger's level of discomfort in a ‘control- and authority-less’ experience. In this paper, we propose an approach for formulating discomfort through ‘on-the-road’ field studies, with human driven vehicles, while the passenger provides real-time explicit feedback on discomfort via a potentiometer. While previous studies focused on the association between vehicle dynamics and passenger discomfort, we demonstrate here how we can improve the classification ability of passenger discomfort by employing a multi-dimensional model that also takes into account the external scenario (contextual information). This is achieved by processing image data (e.g. distance from nearest bicycle) recorded through an outward looking camera in addition to location/route data obtained from other sensors like GPS. As such, the focus of this paper is on classification of external information.		Ariel Telpaz;Michael Baltaxe;Ron M. Hecht;Guy Cohen-Lazry;Asaf Degani;Gila Kamhi	2018	2018 21st International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2018.8569653		Robotics	-48.016125118177996	-52.476818012029696	133222
6d28081467e6aa69e815414b23241b76189b7240	evolutionary programming based recommendation system for online shopping	retail data processing;human computer interaction;evolutionary computation;retail data processing evolutionary computation gaze tracking human computer interaction internet recommender systems;gaze tracking;evolutionary computation sociology statistics programming length measurement genetic algorithms visualization;internet;online shopping eye tracking system fixation length fixation count eye movement patterns eye movement analysis human preference interactive evolutionary programming based recommendation system;recommender systems	In this paper, we propose an interactive evolutionary programming based recommendation system for online shopping that estimates the human preference based on eye movement analysis. Given a set of images of different clothes, the eye movement patterns of the human subjects while looking at the clothes they like differ from clothes they do not like. Therefore, in the proposed system, human preference is measured from the way the human subjects look at the images of different clothes. In other words, the human preference can be measured by using the fixation count and the fixation length using an eye tracking system. Based on the level of human preference, the evolutionary programming suggests new clothes that close the human preference by operations such as selection and mutation. The proposed recommendation is tested with several human subjects and the experimental results are demonstrated.	evolutionary programming;eye tracking;online shopping;recommender system;tracking system	Jehan Jung;Yuka Matsuba;Rammohan Mallipeddi;Hiroyuki Funaya;Kazushi Ikeda;Minho Lee	2013	2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference	10.1109/APSIPA.2013.6694236	computer vision;simulation;computer science;multimedia	AI	-40.318282745400346	-53.1312005198712	133748
32207dd0f11db642f435bdc16aa9b5ce61843596	keep it simple: reward and task design in crowdsourcing	user interface;cognition;motivation;crowdsourcing	Crowdsourcing is emerging as an effective method for performing tasks that require human abilities, such as tagging photos, transcribing handwriting and categorising data. Crowd workers perform small chunks of larger tasks in return for a reward, which is generally monetary. Reward can be one factor for motivating workers to produce higher quality results. Yet, as highlighted by previous research, the task design, in terms of its instructions and user interface, can also affect the workers' perception of the task, thus affecting the quality of results. In this study we investigate both factors, reward and task design, to better understand their role in relation to the quality of work in crowdsourcing. In Experiment 1 we test a variety of reward schemas while in Experiment 2 we measure the effects of the complexity of tasks and interface on attention. The long-term goal is to establish guidelines for designing tasks with the aim to maximize workers' performance.	crowdsourcing;effective method;experiment;quality of results;tag (metadata);user interface	Ailbhe Finnerty;Pavel Kucherbaev;Stefano Tranquillini;Gregorio Convertino	2013		10.1145/2499149.2499168	simulation;computer science;knowledge management;social psychology	HCI	-37.38233624751968	-52.14385607806598	133795
ca217e100ece989cc33da82d2b94a9f6c0277862	locomotor training through a 3d cable-driven robotic system for walking function in children with cerebral palsy: a pilot study	training legged locomotion pelvis three dimensional displays pediatrics robot sensing systems;walking distance locomotor training 3d cable driven robotic system walking function children with cerebral palsy robot assisted treadmill training locomotor ability strenuous work reduction robotic gait training systems locomotor function improvement overground walking speed;patient rehabilitation cables mechanical gait analysis human robot interaction medical robotics mobile robots motion control	Locomotor training using treadmill has been shown to elicit significant improvements in locomotor ability for some children with cerebral palsy (CP), the functional gains are relatively small and it requires greater involvement from a physical therapist. Current robotic gait training systems are effective in reducing the strenuous work of a physical therapist during locomotor training, but are less effective in improving locomotor function in some children with CP due to the limitations of the systems. Thus, a 3D cable-driven robotic gait training system was developed and tested in five children with CP through a 6 week of long-term gait training. Results indicated that both overground walking speed and 6 minute walking distance improved after robot assisted treadmill training through the cable-driven robotic system, and partially retained at 8 weeks after the end of training. Results from this pilot study indicated that it seems feasible to conduct locomotor training in children with CP through the 3D cable-driven robotic system.	3d computer graphics;cerebral palsy;robot;treadmill, device;walking speed	Ming Wu;Janis Kim;Pooja Arora;Deborah J. Gaebler-Spira;Yunhui Zhang	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944384	simulation;physical medicine and rehabilitation;engineering;physical therapy	Robotics	-44.70386174958374	-52.30668239992738	134182
0389a271492922baa7697f266531f5e1c4503dd2	system and user attributes associated with successful searching	cognitive attributes user attributes successful searching highly experienced retrieval system users boolean system natural language system search mechanics user satisfaction attributes instance recall system oriented analysis user preference equivalent instance recall user oriented analysis occupation type user satisfaction search terms personality attributes;information retrieval;user preferences;software libraries information retrieval telescopes multimedia databases protocols biomedical informatics natural languages demography mechanical variables measurement performance analysis;personalized search;online front ends;human factors;natural language;online front ends information retrieval human factors user interfaces interactive systems;interactive systems;user satisfaction;user interfaces	"""A study was performed to assess system and user attributes associated with successful searching. A group of highly experienced retrieval system users were given eight queries-half to be completed with a Boolean system and the other half to be completed with a natural language system-and asked to identify """"instances"""" of information related to a topic. A variety of demographic, experience, cognitive, personality, search mechanics, and user satisfaction attributes were measured and evaluated for association with """"instance recall."""" The system oriented analysis showed user preference for the Boolean system but equivalent instance recall with the two systems. The user oriented analysis showed improved performance with occupation type, user satisfaction, and number of search terms per cycle. A variety of cognitive and personality attributes did not show any association with better searching."""		William R. Hersh;Susan Price;Benjamin Chan;Dale Kraemer;Lynetta Sacherek;Daniel Olson	1999		10.1109/ADL.1999.777691	user;user modeling;computer user satisfaction;computer science;multimedia;world wide web;information retrieval	Vision	-35.24083792498786	-52.893566766777745	134406
011fa29a98a326d3ac022bc21203ffedd5befe69	judging the spatial relevance of documents for gir	busqueda informacion;pilot study;systeme information geographique;geographic information system;information retrieval;pertinencia;recherche documentaire;recherche information;busqueda documental;pertinence;geographic information retrieval;document retrieval;relevance;information system;systeme information;sistema informacion geografica;sistema informacion	Geographic Information Retrieval (GIR) is concerned with the retrieval of documents based on both thematic and geographic content. An important issue in GIR, as for all IR, is relevance. In this paper we argue that spatial relevance should be considered independently from thematic relevance, and propose an initial scheme. A pilot study to assess this relevance scheme is presented, with initial results suggesting that users can distinguish between these two relevance dimensions, and that furthermore they have different properties. We suggest that spatial relevance requires greater assessor effort and more localised geographic knowledge than judging thematic relevance.	information retrieval;relevance	Paul D. Clough;Hideo Joho;Ross Purves	2006		10.1007/11735106_62	document retrieval;relevance;relevance;computer science;data mining;geographic information system;world wide web;information retrieval;information system	Web+IR	-35.95162612485165	-62.09541458462762	134489
25ac33c776c027589f8c7da3510fbe4e64d572e0	the naive bayes mystery: a classification detective story	naive bayes;supervised classification;comparative studies;meta analysis;comparative study;statlog	Many studies have been made to compare the many different methods of supervised classification which have been developed. While conducting a large meta-analysis of such studies, we spotted some anomalous results relating to the Naive Bayes method. This paper describes our detailed investigation into these anomalies. We conclude that a very large comparative study probably mislabelled another method as Naive Bayes, and that the Statlog project used the right method, but possibly incorrectly reported its provenance. Such mistakes, while not too harmful in themselves, can become seriously misleading if blindly propagated by citations which do not examine the source material in detail. ! 2005 Elsevier B.V. All rights reserved.	machine learning;naive bayes classifier;supervised learning	Adrien Jamain;David J. Hand	2005	Pattern Recognition Letters	10.1016/j.patrec.2005.02.001	computer science;data science;machine learning;comparative research;pattern recognition;data mining;statistics	AI	-41.44814700788828	-65.99053592116013	134868
27a645af8cfe2a355d86d51957ad0558e7726339	a granular approach for analyzing the degree of affability of a web site	site web;granular approach;analisis datos;page utility;satisfaccion;data mining;satisfaction;data analysis;calcul granulaire;internet;granular computation;fouille donnee;comportement utilisateur;web mining;analyse donnee;user behavior;sitio web;design factors;busca dato;comportamiento usuario;web site	Due to the competitive environment in which we are moving, web sites need to be very attractive for visitors. In this paper we propose an approach to analyze and determine the level of affability of a web site that tends to secure useru0027s satisfaction, based on both the kind of page and the kind of user. We propose a granular approach based on the idea that a page can be considered as a set of features or factors. In fact, each of them can be perceived at different granularity levels. The proposed approach makes it possible to estimate a measure of affinity of a user for each level of each particular factor. On any particular page, each factor takes a certain level or value. The global measure of affinity for a certain page will be calculated jointly considering the levels or values that the attributes of this particular page have for each design factor.	data mining;real-time web;web mining;web navigation	Esther Hochsztain;Socorro Millán;Ernestina Menasalvas Ruiz	2002		10.1007/3-540-45813-1_63	web mining;the internet;computer science;data mining;database;data analysis;world wide web	ML	-36.49574942388638	-57.06632040047472	135218
e21b03c8661bc5e7937d83148448931f15a7f5f5	planning optimal paths: a simple assessment of survey spatial knowledge in virtual environments	cognition spatiale;spatial cognition;aptitude espacial;realite virtuelle;realidad virtual;survey map;virtual reality;hombre;predictive factor;psychometrie;planificacion;optimal path;individual ability;cognition;human;psychometrics;spatial ability;cognicion;planning;evaluation;evaluacion;planification;virtual environment;psicometria;cognicion espacial;homme;planning in advance;aptitude spatiale	In spatial cognition studies several cognitive factors were analysed in order to identify the aspect that could constitute the basis for the capacity of organising spatial knowledge into survey maps. This study presents a method for evaluating spatial ability, based on the capacity of obtaining a survey-type spatial knowledge organisation, in a recently explored virtual environment. The ability to plan optimal paths in virtual environments was examined in 40 female adult subjects. Spatial evaluation deriving from navigation of a simple virtual environment was compared with classical spatial survey tasks (wayfinding, pointing and sketch maps) performed after the active exploration of a complex virtual environment. Results show that there is a relationship between planning optimal paths and other spatial tasks related to survey representation. These findings highlight how the navigation-supported learning capacity results in a predictive factor for individuals’ assessment of spatial ability. 2006 Published by Elsevier Ltd.	cognition;map;planning;virtual reality	Francesca Morganti;Antonella Carassa;Giuliano Carlo Geminiani	2007	Computers in Human Behavior	10.1016/j.chb.2006.02.006	psychology;planning;simulation;cognition;developmental psychology;computer science;virtual machine;artificial intelligence;evaluation;virtual reality;psychometrics;management	HCI	-43.95166744241464	-55.81791305751021	135304
c97281b7ec79c323b1f6cd126d13657e27e0b37b	a framework for effective retrieval	optimisation;optimizacion;almacenamiento informacion;information retrieval;query formulation;formulacion pregunta;classification;formulation question;probabilistic model;information storage;recherche information;estimacion parametro;modele probabiliste;stockage information;optimization;recuperacion informacion;information system;parameter estimation;estimation parametre;clasificacion;systeme information;modelo probabilista;sistema informacion	The aim of an effective retrieval system is to yield high recall and precision (retrieval effectiveness). The nonbinary independence model, which takes into consideration the number of occurrences of terms in documents, is introduced. It is shown to be optimal under the assumption that terms are independent. It is verified by experiments to yield significant improvement over the binary independence model. The nonbinary model is extended to normalized vectors and is applicable to more general queries. Various ways to alleviate the consequences of the term independence assumption are discussed. Estimation of parameters required for the nonbinary independence model is provided, taking into consideration that a term may have different meanings.	experiment;independence day: resurgence;precision and recall	Clement T. Yu;Weiyi Meng;S. Park	1989	ACM Trans. Database Syst.	10.1145/63500.63519	statistical model;biological classification;binary independence model;artificial intelligence;data mining;estimation theory;information system;algorithm	Web+IR	-36.3692225428185	-62.14529645464006	135461
ce0fef56e9dd27caf7547d2ac38421ab241ded27	"""effects of """"advanced search"""" on user performance and search efforts: a case study with three digital libraries"""	libraries;computers;user interface;information retrieval;digital library;digital libraries;search methods;search method;information search;interface design;user experience;search interface design;computer science;ieee xplore;usability;user interfaces;interaction design	"""This study investigated into the effects of the """"Advanced search"""" feature of three digital libraries on users' search performance and search efforts. Three operational digital libraries, i.e., the ACM digital library, the IEEE Computer Society digital library, and the IEEE Xplore digital library, were used in this study. Thirty-five students participated in an experiment and completed the assigned search task, using their preferred search feature(s). The results demonstrate that for ACM and IEEE CS, the use of Advanced search did not have a significant effect on improving search performance. Only in Xplore, the use of Advanced search significantly improved search performance. The search efforts increased significantly for the combined-use of Advanced search and Basic search in ACM and IEEE CS. The reasons leading to the results are discussed."""	digital library;digital rights management;ieee information theory society;ieee xplore;library (computing)	Xiangmin Zhang;Yuelin Li	2012	2012 45th Hawaii International Conference on System Sciences	10.1109/HICSS.2012.234	digital library;human–computer interaction;computer science;multimedia;user interface;world wide web	Visualization	-36.81341715379492	-52.748653618433366	135661
08c2cf74a1d1bfb1d72e6630a617d2c7406825b8	widit: fusion-based approach to web search optimization	busqueda informacion;optimisation;optimizacion;information retrieval;library and information science;information discovery;web information retrieval;interactive system;recherche information;pattern recognition;web search;optimization;knowledge discovery	To facilitate both the understanding and the discovery of information, we need to utilize multiple sources of evidence, integrate a variety of methodologies, and combine human capabilities with those of the machine. The Web Information Discovery Integrated Tool (WIDIT) Laboratory at the School of Library and Information Science, Indiana University-Bloomington, houses several projects that employ this idea of multi-level fusion in the areas of information retrieval and knowledge discovery. This paper describes a Web search optimization study by the TREC research group of WIDIT, who explores a fusion-based approach to enhancing retrieval performance on the Web. In the study, we employed both static and dynamic tuning methods to optimize the fusion formula that combines multiple sources of evidence. By static tuning, we refer to the typical stepwise tuning of system parameters based on training data. “Dynamic tuning”, the key idea of which is to combine the human intelligence, especially pattern recognition ability, with the computational power of the machine, involves an interactive system tuning process that facilitates fine-tuning of the system parameters based on the cognitive analysis of immediate system feedback. The rest of the paper is organized as follows. The next section discusses related work in Web information retrieval (IR). Section 3 details the WIDIT approach to Web IR, followed by the description of our experiment using the TREC .gov data in section 4 and the discussion of results in section 5.	mathematical optimization;web search engine	Kiduk Yang;Ning Yu	2005		10.1007/11562382_16	web modeling;relevance;cognitive models of information retrieval;computer science;artificial intelligence;machine learning;data mining;database;adversarial information retrieval;knowledge extraction;web intelligence;world wide web;information retrieval;human–computer information retrieval	ML	-34.982653342371734	-57.97115296646114	135821
8e97d1a01e1b61be3c9512171a849d9c017b8b81	evaluation of p2p multimedia clustering techniques in jxta-overlay	busqueda informacion;p2p system;analisis contenido;evaluation performance;document multimedia;algoritmo busqueda;performance evaluation;information retrieval;implementation;algorithme recherche;par a par;evaluacion prestacion;reseau ordinateur;search algorithm;p2p multimedia clustering;p2p;multimedia application;multimedia document;computer network;partage des ressources;content analysis;recherche documentaire;poste a poste;recherche information;busqueda por contenido;busqueda documental;p2p network;multimedia communication;resource sharing;signal classification;jxta;particion recursos;classification signal;red informatica;semantic search;file sharing;document retrieval;p2p networks;analyse contenu;classification automatique;class based semantic search;analisis semantico;implementacion;analyse semantique;communication multimedia;automatic classification;peer to peer;clasificacion automatica;content based retrieval;recherche par contenu;semantic analysis;documento multimedia	Efficient file searching is an essential feature in P2P systems. While many current approaches use brute force techniques to search files by meta information (file names, extensions or user-provided tags), the interest is in implementing techniques that allow content-based search in P2P systems. Recently, clustering techniques have been used for searching text documents to increase the efficiency of document discovery and retrieval. Integrating such techniques into P2P systems is important to enhance searching in P2P file sharing systems. While some effort has been taken for content-based searching for text documents in P2P systems, there has been few research work for applying these techniques to multimedia content in P2P systems. In this paper, we introduce two P2P content-based clustering techniques for multimedia documents. These techniques are an adaptation of the existing Class-based Semantic Search algorithm for text documents. The proposed algorithms have been integrated into a JXTA-based Overlay P2P platform, and evaluation results are provided. The JXTA-Overlay together with the considered clustering techniques is thus very useful for developing P2P multimedia applications requiring efficient searching of multimedia contents in peer nodes.	brute-force search;cascading style sheets;centralized computing;cluster analysis;collaborative software;file sharing;jxta;peer-to-peer;relevance;scalability;search algorithm;semantic search	Fatos Xhafa;Leonard Barolli;Enric Jaén Villoldo;Santi Caballé	2009	Multimedia Systems	10.1007/s00530-009-0169-x	document retrieval;shared resource;content analysis;semantic search;computer science;peer-to-peer;data mining;database;implementation;world wide web;file sharing;search algorithm	DB	-36.00019347713816	-59.726500912343326	136054
4da914d945a1b9287c1402a33ffa36e527fa107c	discrimination of authorship using visualization	tarea discriminatoria;traitement texte;text;attribution sociale;certification;visualizacion;analisis estilistico;atribucion social;automatisation;methode;texte;automatizacion;litterature scientifique;visualization;tâche discrimination;visualisation;visualization technique;literatura cientifica;word length;auteur;autor;certificacion;tratamiento textos;discrimination task;social attribution;stylistic analysis;author;analyse stylistique;experimentation;texto;metodo;scientific literature;method;word processing;experimentacion;automation	Abstract   Visualization techniques help organize the vast amount of data generated in computational studies of literary style. These techniques are demonstrated by showing two-dimensional representations of the style of the authors of  The Federalist Papers . The techniques are used to determine the authorship of the 12 unattributed papers. The authorship assigned to these papers is consistent with that found in other studies.		Bradley Kjell;W. Addison Woods;Ophir Frieder	1994	Inf. Process. Manage.	10.1016/0306-4573(94)90029-9	visualization;computer science;artificial intelligence;programming language	Visualization	-38.70806686803289	-58.23442150813852	136096
2e9ed1bc8157b6bfb70b229505f9b8f44b4d1d5c	exploring trends in trinidad steelband music through computational ethnomusicology.		We present an interdisciplinary case study combining traditional and computational methodologies to study Trinidad steelband music in a collection of recordings of the annual Panorama competition spanning over 50 years. In particular, the ethnomusicology literature identifies a number of trends and hypotheses about this practice of music involving tempo, tuning, and dynamic range. Some of these are difficult to address with traditional, manual methodologies. We investigate these through the computational lens of Music Information Retrieval (MIR) methods. We find that the tempo range measured on our corpus is consistent with values reported in ethnomusicological literature, and add further details about how tempo has changed for the best judged performances at Panorama. With respect to the use of dynamics, we find limited usefulness of a standardised measures of loudness on these recordings. When it comes to judging the tuning frequency of the acoustic recordings, we find what looks to be a narrowing of the range, but these might be unreliable given the diversity of recording media over the past decades.		Elio Quinton;Florabelle Spielmann;Bob L. Sturm	2017		10.1007/978-3-030-01692-0_5	loudness;ethnomusicology;music information retrieval;natural language processing;multimedia;panorama;computer science;artificial intelligence	NLP	-41.04981436230799	-62.754843669280426	136351
68f7599c7acf0a3f08bb6f4d7b2d37680ac2d558	split size-rank models for the distribution of index terms	databases;word frequency;information retrieval;indexing terms;statistical distributions;indexing;tables data;models;subject index terms	Abstract#R##N##R##N#Since the introduction of the Zipf distribution, many functions have been suggested for the frequency of words in text. Some of these models have also been applied to the distribution of index terms in a set of documents. The models are of two forms: rank-frequency and frequency-size. The former serve well to describe the distribution of high-frequency terms; the latter the distribution of low-frequency terms. In this article, a split model is proposed, which uses both a rank function for the high frequency terms and a size function for the low frequency terms, with the point of transition being determined either empirically or by rule. This model is fitted to the marginal empirical term distributions for four document datasets. Distributions to describe index term exhaustivity and term co-occurrence are also considered briefly.		Michael J. Nelson;Jean Tague-Sutcliffe	1985	JASIS	10.1002/asi.4630360502	probability distribution;search engine indexing;index term;computer science;pattern recognition;data mining;mathematics;word lists by frequency;world wide web;information retrieval	NLP	-38.527490502523456	-62.93379091583594	136546
d831362fa0e718a5ddf1180cee5cb5305fc6fad5	knowledge generation from digital libraries and persistent archives	tratamiento paralelo;distributed memory;documento electronico;archivo electronico;liverpool;informacion numerica;linguistica matematica;linguistic tool;procesamiento informacion;traitement parallele;almacenamiento informacion;information retrieval;base donnee tres grande;digital library;memoria compartida;alimentacion maquina;instrumento linguistico;digital archive;repository;grid;document electronique;digital information;large scale;information storage;biblioteca electronica;computational linguistic;amenage piece;rejilla;machine feed;information processing;digital preservation;alimentation machine;grille;linguistique mathematique;stockage information;information numerique;alimentacion pieza;university;electronic library;computational linguistics;archive electronique;very large databases;memoire repartie;traitement information;feeder;outil linguistique;parallel processing;bibliotheque electronique;data grid;electronic document	This poster describes the ongoing research of the Cheshire project with a particular focus on knowledge generation and digital preservation. The infrastructure described makes use of tools from computational linguistics, distributed parallel processing and storage, information retrieval and digital preservation environments to produce new knowledge from very large scale datasets present in the data grid.	computational linguistics;digital library;information retrieval;parallel computing	Paul B. Watry;Ray R. Larson;Robert Sanderson	2006		10.1007/11863878_54	parallel processing;digital library;distributed memory;information processing;computer science;artificial intelligence;computational linguistics;data grid;grid;world wide web	HPC	-35.91602755443334	-63.84716403655566	136696
4c9220305d47f68f5107fc0be290a03b6c2735ca	set-based model: a new approach for information retrieval	answer sets;vector space model;time complexity;average precision;information retrieval;information retrieval model;set theory;indexing terms;data mining;association rule mining;association rule;term weighting;closed association rule mining;information retrieval models;weighting index term co occurrences	The objective of this paper is to present a new technique for computing term weights for index terms, which leads to a new ranking mechanism, referred to as set-based model. The components in our model are no longer terms, but termsets. The novelty is that we compute term weights using a data mining technique called association rules, which is time efficient and yet yields nice improvements in retrieval effectiveness. The set-based model function for computing the similarity between a document and a query considers the termset frequency in the document and its scarcity in the document collection. Experimental results show that our model improves the average precision of the answer set for all three collections evaluated. For the TReC-3 collection, our set-based model led to a gain, relative to the standard vector space model, of 37% in average precision curves and of 57% in average precision for the top 10 documents. Like the vector space model, the set-based model has time complexity that is linear in the number of documents in the collection.	archive;association rule learning;data mining;information retrieval;stable model semantics;time complexity	Bruno Pôssas;Nivio Ziviani;Wagner Meira;Berthier A. Ribeiro-Neto	2002		10.1145/564376.564417	ranking;association rule learning;computer science;machine learning;pattern recognition;data mining;term discrimination;vector space model;information retrieval;divergence-from-randomness model	Web+IR	-34.57815150632827	-61.06356941626742	136730
01b71433f8f974396470ddde42253d93a05458ef	the effect of traffic on situation awareness and mental workload: simulator-based study	workload;traffic simulation;simulated driving;mental workload;situation awareness;3 dimensional;subjective assessment	In the present study, we investigated the effects of the different traffic on the driver's situation awareness and the mental workload (MWL). The task used in this study was a medium fidelity, 3-dimensional simulation of a driving environment. The simulation required participants to drive the user's car and perform a real-world driving task. After the simulated driving, participants were asked to complete two tests which assessed their situation awareness (SA). The mental workload measures in this study consisted of the physiological measures and the subjective assessment. Every participant performed two different traffic simulated driving conditions, one was low traffic, the other was high traffic. The results showed that with the increasing of traffic, the driving performance did not worsen, however participant's mental workload increased, at the same time, the participant's situation awareness performance deteriorated. Meanwhile, our results also demonstrated that recall-based SA test and recognition-based test was heterogeneous.		Xueqin Hao;Zhiguo Wang;Fan Yang;Ying Wang;Yanru Guo;Kan Zhang	2007		10.1007/978-3-540-73331-7_31	simulation;engineering;social psychology;computer security	HCI	-47.03957326052304	-53.00348326105677	137057
7ef2009ed310eb0b831601a374ee01cf12338ded	evaluating auditory contexts and their impacts on hearing aid outcomes with mobile phones	hearing aids;context aware sensing;computer based ema	This paper evaluates the relationship between auditory contexts, hearing aid features, and hearing outcomes based on real-world measurements. We use a mobile phone application to concurrently evaluate the auditory contexts and hearing aid outcomes using Ecological Momentary Assessments. The collected dataset includes 3437 surveys collected from nineteen patients over ten months. Our analysis indicates that the most frequent listening activities were conversations (32.7% of the time) and listening to media (30.7% of the time), commonly occurring at home, in predominantly quiet environments. Subjects do not attribute equal importance to hearing well in all auditory contexts: it is more important to hear well in contexts that involve social interactions. We show that hearing aid outcomes measures are moderately correlated. By leveraging on these correlations, we propose a method of combining measurements of hearing aid outcomes into a single score to reduce measurement error. Finally, we show that it is possible to discriminate between poor and good hearing aid outcomes with an accuracy of 78% solely based on auditory contexts and hearing aid features. This shows the central role that auditory contexts play in understanding hearing aid outcomes in situ.	interaction;mobile app;mobile phone	Syed Shabih Hasan;Octav Chipara;Yu-Hsiang Wu;Nazan Aksan	2014		10.4108/icst.pervasivehealth.2014.254952	speech recognition	HCI	-38.369467453761395	-53.104460792141346	137261
e47a8fc2ccfbddc68de8e6ae2030a6ee9aff38ff	four eyes see more than two: shared gaze in the car		Purposeful collaboration of driver and front-seat passenger can help in demanding driving situations and therefore increase safety. The characteristics of the car, as a context, limit the collaboration possibilities of the driver and front-seat passenger, though. In this paper, we present an approach that supports successful collaboration of the driver and front-seat passenger with regard to the contextual specifics. By capturing the front-seat passenger’s gaze and visualizing it for the driver, we create a collaborative space for information sharing in the car. We present the results from a study investigating the potentials of the co-driver’s gaze as means to support the driver during a navigational task. Our results confirm that the co-driver’s gaze can serve as helpful means to support the collaboration of driver and front-seat passenger in terms of perceived distraction and workload of the driver.		Sandra Trösterer;Magdalena Gärtner;Martin Wuchse;Bernhard Maurer;Axel Baumgartner;Alexander Meschtscherjakov;Manfred Tscheligi	2015		10.1007/978-3-319-22668-2_26	workload;human–computer interaction;gaze;distraction;information sharing;computer science;eye tracking	HCI	-48.02503765780078	-52.396849638010956	137340
7561281d24ae2c85348bca82a866cb037480abff	salt: a semantic approach for generating document representations	semantic annotation;latex;document representation;information processing;pdf;semantic document	"""The structure of a document has an important influence on the perception of its content. Considering scientific publications, we can affirm that by making use of the ordinary linear layout, a well organized publication, following a """"red wire"""", will always be better understood and analyzed than one having a poor or chaotic structure, but not necessarily poor content. Reading a publication in a linear way, from the first page to the last page means a lot of unnecessary information processing to the reader. Looking at a publication from another perspective by accessing the key-points or argumentative structure directly can give better insights into the author's thoughts, and for certain tasks (i.e. getting a first impression of an article) a representation of the document reduced to its core could be more important than its linear structure. In this paper, we will show how one can build different representations of the same document, by exploiting the semantics captured in the text. The focus will be on scientific publications and as building foundation we use the SALT (Semantically Annotated LATEX) annotation framework for creating Semantic PDF Documents."""	information processing;latex;portable document format;scientific literature	Tudor Groza;Alexander Schutz;Siegfried Handschuh	2007		10.1145/1284420.1284462	information processing;computer science;data mining;database;world wide web;information retrieval;design document listing	Web+IR	-39.41102812274248	-57.14607316506531	137385
dea298b4ba2934a30955fc804c000750e0157ec4	optimizing search results for human learning goals		While past research has shown that learning outcomes can be influenced by the amount of effort students invest during the learning process, there has been little research into this question for scenarios where people use search engines to learn. In fact, learning-related tasks represent a significant fraction of the time users spend using Web search, so methods for evaluating and optimizing search engines to maximize learning are likely to have broad impact. Thus, we introduce and evaluate a retrieval algorithm designed to maximize educational utility for a vocabulary learning task, in which users learn a set of important keywords for a given topic by reading representative documents on diverse aspects of the topic. Using a crowdsourced pilot study, we compare the learning outcomes of users across four conditions corresponding to rankings that optimize for different levels of keyword density. We find that adding keyword density to the retrieval objective gave significant learning gains on some topics, with higher levels of keyword density generally corresponding to more time spent reading per word, and stronger learning gains per word read. We conclude that our approach to optimizing search ranking for educational utility leads to retrieved document sets that ultimately may result in more efficient learning of important concepts.	crowdsourcing;optimizing compiler;personalization;search algorithm;vocabulary;web search engine	Rohail Syed;Kevyn Collins-Thompson	2017	Information Retrieval Journal	10.1007/s10791-017-9303-0	active learning (machine learning);keyword density;information retrieval;data mining;search engine;ranking;vocabulary;computer science	Web+IR	-35.77827452965984	-53.30707977240087	137606
6629cfe4e3d0392be6b74fe374905384aa8e48f3	effects of computer monitor viewing angle and related factors on strain, performance, and preference outcomes	workload;computadora;task performance;condicion trabajo;muscle activity;ergonomia;input output equipment;ordinateur;fatigue visuelle;performance;hombre;poste travail;ergonomie;computer;diseases of the osteoarticular system;ecran visualisation;posture;pantalla visualizacion;equipement entree sortie;visual fatigue;human;postura;fatiga visual;charge travail;equipo entrada salida;sistema osteoarticular patologia;puesto trabajo;computer hardware;rendimiento;display screen;working condition;carga trabajo;workplace layout;musculoskeletal diseases;condition travail;systeme musculosquelettique pathologie;materiel informatique;ergonomics;material informatica;systeme osteoarticulaire pathologie;homme	A model of visual and musculoskeletal strain associated with computer monitor placement was developed. The main premise of which is that monitor placement decisions must take into consideration development of both visual and musculoskeletal strains. Certain factors in the model that were thought to affect one or both types of strain. or that were considered important to rule out for effect, were tested in a lab setting. These factors were viewing angle (eye level, midlevel, low level), monitor size (14 in., 19 in.), keyboard familiarity (touch typist, nontouch typist), and task (reading, mousing, typing). Outcomes included indicators of visual and musculoskeletal strain, preference, and performance. Muscle activity was generally greater for the low viewing angle, for the standard monitor (14 in.), and for non-touch typists. Participants preferred the midlevel placement. Task performance was slightly diminished with eye-level placement. Results are interpreted in relation to the model and to several hypotheses that were formed to focus the inquiry. Actual or potential applications of this research include monitor placement decisions in the design or modification of computer workstations.		Carolyn M. Sommerich;Sharon M. B. Joines;Jennie P. Psihogios	2001	Human factors	10.1518/001872001775992480	psychology;simulation;medicine;performance;engineering;human factors and ergonomics;communication;engineering drawing;mechanical engineering	HCI	-45.57335548815741	-55.551332241717624	137883
d8e1a1f27d12a509c6522aa284b31b99a3a63363	effects of active and passive secondary tasks in a take-over situation during automated driving		This study investigated the effect of a secondary task when a takeover request (TOR) is received from an automated driving car, with a focus on nature of the secondary task (active or passive). Participants were asked to engage in one type of secondary task during automated driving, and we measured 1) forward gaze and 2) latency after the TOR to switch from automated to manual operation. Playing a game with a smartphone (active task) decreased driving-related performance in terms of shorter forward-gaze duration and longer delays in switching from TOR. In contrast, the performance in the other tasks (passively watching a video and conversing with another person) was little different from that in a condition with no secondary task. Compared to the tasks, not an active but a passive task would contribute to maintain the readiness for TOR.	autonomous car;smartphone	Yutaka Nakajima;Kenji Tanaka	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122769	latency (engineering);machine learning;real-time computing;artificial intelligence;computer science	SE	-47.718511363518346	-52.212706249717804	137894
af3adfe8e9bfbdc7e3676436ce2e311dfe1dfb87	the search engine iq test based on the internet iq evaluation algorithm	search engine	Abstract   This paper initiates an innovative concept and basic measurements on testing the IQ (Intelligence Quotient) on Internet search engines. It first proposes the stipulation of 2014 Internet intelligence scale and designs an IQ test question bank to aim at the search engine. To show its applicability, the paper then carries out the IQ test on seven classic search engines, such as Google, Baidu, Sogou, Bing, Zhongsou, panguso, so, etc., compared with the results of the same IQ test on three groups of Children whose ages are 6, 12, 18. Based on the absolute IQ and relative IQ of ten testing objects, this paper finds that the IQ search engine of Internet is behind far away that of human being in the field of creative testing.	algorithm;web search engine	Feng Liu;Yong Shi	2014		10.1016/j.procs.2014.05.361	simulation;telecommunications	EDA	-38.98361039956267	-55.97576442647451	138065
f775d8f678adfd1e2782ce8252b9f77810a12c66	users interaction with the hierarchically structured presentation in xml document retrieval	busqueda informacion;document structure;hierarchical system;hierarchical structure;interfase usuario;tratamiento transaccion;systeme documentaire;analisis estadistico;estructura documental;user interface;information retrieval;structure document;xml language;systeme hierarchise;interface design;xml retrieval;sistema jerarquizado;fichier log;fichero actividad;recherche documentaire;statistical analysis;recherche information;busqueda documental;traitement document;sistema recuperacion documental;document retrieval system;analyse statistique;xml document;interface utilisateur;document retrieval;document processing;transaction processing;user interaction;langage xml;lenguaje xml;log file;traitement transaction;tratamiento documento	Some changes were made in the interface design of this year’s XML documents retrieval system according to the outcomes of the Interactive track in INEX 2004. One of the major changes was the hierarchical structure of the presentation in the search results. The main purpose of our study was to investigate how the hierarchical presentation of interface influences the searchers’ behavior in XML document retrieval. To achieve this objective we analyzed the transaction logs from this year’s experiment and compared the results to those of last year’s experiment. The subjects’ comments on the experiment and the system were also examined. The Daffodil XML retrieval system was used and 12 test persons participated in the experiment. SPSS for Windows 12.0 was used for statistical analysis.	document retrieval	Heesop Kim;Heejung Son	2005		10.1007/978-3-540-34963-1_32	computer science;database;world wide web;information retrieval	Web+IR	-37.39202745030311	-58.72116291948125	138098
965c95859300242a433d9b34f4f0a794901c7f20	improving e-discovery using information retrieval	information retrieval;query formulation;data fusion;boolean query;e discovery;information need	"""E-discovery is the requirement that the documents and information in electronic form stored in corporate systems be produced as evidence in litigation. It has posed great challenges for legal experts. Legal searchers have always looked to find """"any and all"""" evidence for a given case. Thus, a legal search system would essentially be a recall-oriented system. It has been a common practice among expert searchers to formulate Boolean queries to represent their information need. We want to work on three basic problems: Boolean query formulation - Our primary goal is to study Boolean query formulation in the light of the E-discovery task. This will include automatic Boolean query generation, expansion and learning the effect of proximity operators in Boolean searches. Data fusion - We would also like to explore the effectiveness of data fusion techniques in improving recall. Error modeling - Finally, we will work on error modeling methods for noisy legal documents."""	electronic discovery;information needs;information retrieval;knowledge discovery metamodel	Kripabandhu Ghosh	2012		10.1145/2348283.2348420	natural language processing;information needs;query expansion;ranking;boolean conjunctive query;standard boolean model;computer science;theoretical computer science;machine learning;data mining;sensor fusion;world wide web;information retrieval;query language	DB	-33.74635318569949	-57.74013750741805	138126
c6f61f344c919493886bf67d0e64e0242ae83547	size matters: word count as a measure of quality on wikipedia	wikipedia;artificial intelligent;information quality;natural language processing;word count	"""Wikipedia, """"the free encyclopedia"""", now contains over two million English articles, and is widely regarded as a high-quality, authoritative encyclopedia. Some Wikipedia articles, however, are of questionable quality, and it is not always apparent to the visitor which articles are good and which are bad. We propose a simple metric -- word count -- for measuring article quality. In spite of its striking simplicity, we show that this metric significantly outperforms the more complex methods described in related work."""	wikipedia	Joshua Evan Blumenstock	2008		10.1145/1367497.1367673	computer science;data mining;brand;information quality;world wide web;information retrieval	AI	-38.60504685889787	-56.06319883385023	138794
16f3c8de7bcf553c11ee169d922907baca1d1098	learning shared control by demonstration for personalized wheelchair assistance		An emerging research problem in assistive robotics is the design of methodologies that allow robots to provide personalized assistance to users. For this purpose, we present a method to learn shared control policies from demonstrations offered by a human assistant. We train a Gaussian process (GP) regression model to continuously regulate the level of assistance between the user and the robot, given the useru0027s previous and current actions and the state of the environment. The assistance policy is learned after only a single human demonstration, i.e., in one-shot. Our technique is evaluated in a one-of-a-kind experimental study, where the machine-learned shared control policy is compared to human assistance. Our analyses show that our technique is successful in emulating human shared control, by matching the location and amount of offered assistance on different trajectories. We observed that the effort requirement of the users were comparable between human-robot and human-human settings. Under the learned policy, the jerkiness of the useru0027s joystick movements dropped significantly, despite a significant increase in the jerkiness of the robot assistantu0027s commands. In terms of performance, even though the robotic assistance increased task completion time, the average distance to obstacles stayed in similar ranges to human assistance.	cdisc adas-cog - commands summary score;dropping;emulator;experiment;gaussian process;joystick device component;mobile manipulator;movement;normal statistical distribution;personalization;policy;robot (device);robotics	Ayse Kucukyilmaz;Yiannis Demiris	2018	IEEE Transactions on Haptics	10.1109/TOH.2018.2804911	simulation;task analysis;control engineering;jerkiness;wheelchair;joystick;software;robot;mobile robot;computer science;artificial intelligence;robotics	Robotics	-44.503604254807286	-52.414265358453136	139086
a8b5b5ec2a836da7e2a2906674d0755a3f99a0c3	impact of blue force tracking on combat identification judgments	judgment;combat;pedestrian safety;decision aid;poison control;injury prevention;hombre;prise de decision;safety literature;ayuda decision;traffic safety;injury control;classification;home safety;combate;injury research;jugement;safety abstracts;militar;militaire;human factors;toma de conciencia;awareness;identification;occupational safety;cognition;personal decision aid;safety;human;blue force tracking;situation awareness;aide decision;cognicion;safety research;identificacion;accident prevention;violence prevention;military;juicio;bicycle safety;fighting;toma decision;poisoning prevention;falls;ergonomics;clasificacion;suicide prevention;homme;prise conscience	OBJECTIVE We examined the effectiveness of blue force tracking (BFT) decision support for dismounted infantry soldiers.   BACKGROUND Technologies to support combat identification (CID) are rapidly evolving and may be deployable to dismounted soldiers in the future. BFT systems are designed to mitigate the risk of fratricide by supplying positional information regarding friendly units to enhance situation awareness.   METHOD Participants played the role of a dismounted infantry soldier in a first-person perspective gaming environment and made engagement decisions for a series of simulated targets, half of which were enemies and half of which were friends.   RESULTS Participants performed better overall when they were able to use a BFT system than when they performed the task without assistance. When a 10-s latency was added to the updating of position information in the BFT, participants made significantly more false alarms (engaged a friendly target) regardless of whether they knew about the latency.   CONCLUSION The results indicate the promise of a personal BFT device to reduce the likelihood of fratricide by dismounted infantry soldiers. The results, however, also indicate that the effectiveness of such a device can be dramatically reduced when it does not provide real-time data.   APPLICATION Potential applications of this research include development of performance standards for BFT devices and assessment of decision support for dismounted soldiers.		David J. Bryant;David G. Smith	2013	Human factors	10.1177/0018720812451595	psychology;identification;judgment;situation awareness;simulation;cognition;awareness;medicine;environmental health;biological classification;engineering;suicide prevention;human factors and ergonomics;injury prevention;forensic engineering;computer security;mechanical engineering	HCI	-46.12561016943411	-54.439377455916365	139270
bd5f4c28e62a7f54be92960d752cb2d41978c446	diagnostic evaluation of information retrieval models	diagnostic test;formal models;formal model;information retrieval;information retrieval model;retrieval model;diagnostic evaluation;tf idf weighting;empirical evaluation;constraints;retrieval heuristics	Developing effective retrieval models is a long-standing central challenge in information retrieval research. In order to develop more effective models, it is necessary to understand the deficiencies of the current retrieval models and the relative strengths of each of them. In this article, we propose a general methodology to analytically and experimentally diagnose the weaknesses of a retrieval function, which provides guidance on how to further improve its performance. Our methodology is motivated by the empirical observation that good retrieval performance is closely related to the use of various retrieval heuristics. We connect the weaknesses and strengths of a retrieval function with its implementations of these retrieval heuristics, and propose two strategies to check how well a retrieval function implements the desired retrieval heuristics. The first strategy is to formalize heuristics as constraints, and use constraint analysis to analytically check the implementation of retrieval heuristics. The second strategy is to define a set of relevance-preserving perturbations and perform diagnostic tests to empirically evaluate how well a retrieval function implements retrieval heuristics. Experiments show that both strategies are effective to identify the potential problems in implementations of the retrieval heuristics. The performance of retrieval functions can be improved after we fix these problems.	constraint (mathematics);experiment;heuristic (computer science);information retrieval;relevance	Hui Fang;Tao Tao;ChengXiang Zhai	2011	ACM Trans. Inf. Syst.	10.1145/1961209.1961210	cognitive models of information retrieval;standard boolean model;computer science;machine learning;data mining;term discrimination;vector space model;information retrieval;diagnostic test;human–computer information retrieval;divergence-from-randomness model	Web+IR	-33.91206297466646	-57.97995358184266	139308
d3350235d30b4f3d47172326a5bcb510e852413b	searchers seeking: what happens when you frustrate searchers?		People searching for information occasionally experience difficulties finding what they want on the Web. This might happen if they cannot quite come up with the right search terms. What do searchers do when this happens? Intuitively one imagines that they will try a number of associated search terms to zero in on their intended search target. Certainly the provision of spelling suggestions and related search terms assume that frustrated searchers will use these to implement this strategy. Is this assumption correct? What do people really do? We ran an experiment where we asked people to find some relevant links, but we prevented them from using the most obvious search terms, which we termed “taboo words”. To make the experiment more interesting we also provided the traditional forms of assistance: spelling suggestions and related search suggestions. We assigned participants using a magic square to get no assistance, one kind of assistance, or both. Forty eight people participated in the experiment. What emerged from the analysis was that when people are frustrated in their searching attempts, a minority soldier on, attempting to find other terms, but the majority will stick with their original query term and simply progress from page to page in a vain attempt to find something relevant. This confirms findings by other researchers about the difficulties of query reformulation. Our finding will serve to inform the developers of user interfaces to search engines, since it would be helpful if we could find a better way of supporting frustrated searchers.	assistive technology;clutter;collaborative information seeking;egosurfing;experiment;eye tracking;paging;spelling suggestion;user interface;web page;web search engine;world wide web	Gareth Renaud	2014	CoRR		computer science;artificial intelligence;data mining;world wide web;information retrieval	Web+IR	-35.16833373112657	-53.5456770847232	139430
801f4a82d4f913ad0f28432001d7119fc3c9a397	search engines and web information retrieval	busqueda informacion;distributed system;search engine;buscador;systeme reparti;algorithmique;information retrieval;web search engine;combinatorial problem;probleme combinatoire;problema combinatorio;sistema repartido;web information retrieval;algorithmics;algoritmica;recherche information;moteur recherche	This survey describes the main components of web information retrieval, with emphasis on the algorithmic aspects of web search engine research.	information retrieval;learning to rank;web search engine	Alejandro López-Ortiz	2004		10.1007/11527954_18	search engine indexing;web development;query expansion;web modeling;web mapping;web design;web search engine;web standards;computer science;web crawler;web navigation;data mining;adversarial information retrieval;web intelligence;web search query;algorithmics;world wide web;information retrieval;search engine;human–computer information retrieval	Web+IR	-35.80252531514319	-58.32780204204496	139581
15081ad720e2ef276777009bcc32186733b5e3a8	hierarchical learning of dimensional biases in human categorization	bf psychology;rc0321 neuroscience biological psychiatry neuropsychiatry	Existing models of categorization typically represent to-be-classified items as points in a multidimensional space. While from a mathematical point of view, an infinite number of basis sets can be used to represent points in this space, the choice of basis set is psychologically crucial. People generally choose the same basis dimensions – and have a strong preference to generalize along the axes of these dimensions, but not “diagonally”. What makes some choices of dimension special? We explore the idea that the dimensions used by people echo the natural variation in the environment. Specifically, we present a rational model that does not assume dimensions, but learns the same type of dimensional generalizations that people display. This bias is shaped by exposing the model to many categories with a structure hypothesized to be like those which children encounter. The learning behaviour of the model captures the developmental shift from roughly “isotropic” for children to the axis-aligned generalization that adults show.	apache axis;basis set (chemistry);categorization	Katherine A. Heller;Adam Sanborn;Nick Chater	2009			computer science;artificial intelligence;machine learning;mathematics;statistics	ML	-41.64190418840957	-52.54124129155655	139703
7363703d082a119789f13041b4e2bcc4cc8bdc4c	automatic credibility assessment of popular medical articles available online		This paper presents the design concept of a credibility evaluation tool for medical web-documents and describes the implementation of its part. There have been numerous attempts to create such tool but most of them were strictly subject-specific. In this study, we aim to create a universal classifier for non-credible articles from the medical domain. Unlike most of the latest fact-checking solutions, it evaluates overall the credibility of the document instead of assessing separate claims. We collected a database of articles and sentences evaluated by experts, conducted the study of sentence’s context in the task of credibility assessment, then performed statistical analysis in order to verify and fine-tune the design. The proposed scheme is constructed in such a way that it should be easy to update and has an easily interpretable output for Internet users with no expert knowledge about medicine.		Aleksandra Nabozny;Bartlomiej Balcerzak;Adam Wierzbicki	2018		10.1007/978-3-030-01159-8_20	the internet;data mining;computer science;social informatics;classifier (linguistics);credibility;sentence	NLP	-41.38232771157075	-64.36264950631625	139942
e12d45a09078876bb8437ba96f84d1551a20b45f	tools for searching, annotation and analysis of speech, music, film and video - a survey	busqueda informacion;etude utilisateur;anotacion;m music;document audiovisuel;information retrieval;user study;estudio usuario;annotation;recherche information;documento audiovisual;audiovisual document	This paper examines the actual and potential use of software tools in research in the arts and humanities focussing on audiovisual materials such as recorded speech, music, video and film. The quantity of such materials available to researchers is massive and rapidly expanding. Researchers need to locate the material of interest in the vast quantity available, and to organise and process the material once collected. Locating and organising often depend on metadata and tags to describe the actual content, but standards for metadata for audiovisual materials are not widely adopted. Content-based search is becoming possible for speech, but is still beyond the horizon for music, and even more distant for video. Copyright protection hampers research with audiovisual materials, and digital rights management systems (DRM) threaten to prevent research altogether. Once material has been located and accessed, much research proceeds by annotation, for which many tools exist. Many researchers make some kind of transcription of materials, and would value tools to automate this process. Such tools exist for speech, though with important limits to their accuracy and applicability. For music and video, researchers can make use of visualisations. A better understanding (in general terms) by researchers of the processes carried out by computer software and of the limitations of its results would lead to more effective use of ICT.	digital rights management;transcription (software)	Alan Marsden;Adrian Mackenzie;Adam Lindsay;Harriet Nock;John S. Coleman;Greg Kochanski	2007	LLC	10.1093/llc/fqm021	natural language processing;speech recognition;computer science;multimedia;world wide web;java annotation	HCI	-36.020973967150866	-55.38801718287378	140119
f25c2fc2124c28dfc9f016bf325c2dde10e2f30f	poster: arranging the layout of alphanumeric buttons-the role of passwords	passwords;alphanumeric button;touchscreen	A typical but trivial layout of alphanumeric buttons in the touchscreen setting is to arrange the 10 digits and 26 letters in a natural order. This arrangement does not take into account the frequencies of letters and digits when the users touch the buttons to key in their passwords or messages. We examine large scale datasets of over 141 million passwords collected from several leading websites for social networking, Internet forums, gaming, dating, and various other online service providers in China, and find that the distribution of letters in passwords is quite close to that in Chinese language. Based on the letter/digit frequencies, we further propose an alphanumeric button layout scheme with the following advantages: the buttons are clicked as uniformly as possible, so that the lifetime of the touchscreen can be prolonged and finger oil residues may scatter more evenly over the button area of the screen; and in the meantime, the movements of users' fingers are improved to enhance good user experience when inputting messages. The idea behind the layout is potentially applicable to diversified races.	online service provider;password;touchscreen;user experience	Xiangxue Li;Yu Yu;Qiang Li;Haifeng Qian;Yuan Zhou;Jian Weng	2014		10.1145/2660267.2662368	computer science;internet privacy;world wide web;password;computer security	Security	-45.600890928931456	-58.36891423253912	140394
c5c3f527233e2e47a34d32672121d067bb9a37a8	time, location and interest: an empirical and user-centred study	interactive information retrieval;context information;free text search;information retrieval;contextual information;statistical significance;information sharing;data analysis;information need;interaction effect;ontology;mobile user	The importance of context in meeting user information needs has gained increasing interest. When developing interactive information retrieval systems, we do need to consider how contextual information might be used to improve information retrieval. In this paper, we present a user-centred experiment that focuses on three potential context attributes. These are time, location, and user's interest. The experiment involved tasks using a scenario that would be suitable for mobile situations - one very promising area for the application of context information that can help to deliver personalised services. The scenario involves situations with local events such as jazz concerts and includes the use of a simplified map to help visualise locations. The effect of the three attributes and the interactions between them are analysed and discussed. The effects in most cases were considerable and data analysis showed statistically significant effects. The study shows that time, location, and interest matter to users in mobile situations. There appears to be a priority emerging in the relative importance of these attributes for the mobile user. Also, the results show high order interaction effects between the attributes.	information needs;information retrieval;interaction;map	Ralf Bierig;Ayse Göker	2006		10.1145/1164820.1164838	information needs;interaction;relevance;cognitive models of information retrieval;computer science;information filtering system;ontology;data mining;statistical significance;data analysis;world wide web;information retrieval;statistics;human–computer information retrieval	HCI	-34.49341207453295	-52.24115933606843	140730
2bb28e68660acf55bda0f316b66e1a5f49a1610b	using learned predictions as feedback to improve control and communication with an artificial limb: preliminary findings.		Many people suffer from the loss of a limb. Learning to get by without an arm or hand can be very challenging, and existing prostheses do not yet fulfil the needs of individuals with amputations. One promising solution is to provide greater communication between a prosthesis and its user. Towards this end, we present a simple machine learning interface to supplement the control of a robotic limb with feedback to the user about what the limb will be experiencing in the near future. A real-time prediction learner was implemented to predict impact-related electrical load experienced by a robot limb; the learning system’s predictions were then communicated to the device’s user to aid in their interactions with a workspace. We tested this system with five able-bodied subjects. Each subject manipulated the robot arm while receiving different forms of vibrotactile feedback regarding the arm’s contact with its workspace. Our trials showed that communicable predictions could be learned quickly during human control of the robot arm. Using these predictions as a basis for feedback led to a statistically significant improvement in task performance when compared to purely reactive feedback from the device. Our study therefore contributes initial evidence that prediction learning and machine intelligence can benefit not just control, but also feedback from an artificial limb. We expect that a greater level of acceptance and ownership can be achieved if the prosthesis itself takes an active role in transmitting learned knowledge about its state and its situation of use.	artificial intelligence;assistive technology;computation;data assimilation;electrical load;expect;feedback;interaction;machine learning;personalization;real life;real-time transcription;robot;robotic arm;transmitter;trust (emotion);workspace	Adam S. R. Parker;Ann L. Edwards;Patrick M. Pilarski	2014	CoRR		robot;prosthesis;workspace;computer science;simulation;simple machine;robotic arm	AI	-44.53779070551744	-52.484403412527826	141109
29d3d90a2e87b51b80e2c3d55420638a594e7bf1	a personalized recommender system for travel information	adaptive interfaces;tourisme;case base reasoning;information retrieval;mobility;systemes de recommandations;filtrage collaboratif;adaptive interface;transport;on trip;pre trip;tourism;recommender system;collaborative filtering;case based reasoning;traveler information;point of view;information yoyageur;recommender systems;raisonnement a partir de cas;on line learning	"""This article concerns an emerging research field related to mobility from the transport point of view, linked to the travel information retrieval. To facilitate such a retrieval, we propose the use of recommender systems in a mobility context: these systems facilitate information retrieval, and help prepare the user's trip (""""pre-trip"""": choice of the transport mode, schedule, route, time of the trip, ...) and carry it out (""""on-trip"""": interactive guidance, way visualization, destination planning). This double impact is rarely exploited today and we propose, after a description of the used technologies, to illustrate the benefits of this new approach on a traditional tourist visit example. The originality of this approach lies in 1) its capacities to adapt the recommendations to the user's behavior during his information retrieval correlated to his own movement and 2) the on-line learning capabilities of such a system supporting information retrieval."""	information retrieval;online and offline;online machine learning;personalization;recommender system	Sergiu Theodor Chelcea;George Gallais;Brigitte Trousse	2004		10.1145/1050873.1050905	simulation;cognitive models of information retrieval;computer science;multimedia;world wide web;human–computer information retrieval	HCI	-37.41194794937196	-57.375308488263926	141195
f7c89c8280c59f350e59cfab97bc2390207cfe1f	digital human modeling. applications in health, safety, ergonomics, and risk management: ergonomics and design		To prevent falls, many experts are committed to studying the balance of risk factors and assessment methods fall. Berg Balance Scale (BBS) is a clinical assessment method that most commonly used yet its characteristic of subjective and time-consuming may the consequence in different results. The purpose of this study is to use the force platform system parameters and measuring the amount of income derived factors information and related research of BBS findings and to explore the possibility of subjective and objective information to assess the assistance results. Thirty-eight elderly adults residing at the Tai Shun Senior Centre react to sit-to-stand (STS) action on the force platform with the ergonomic chair. Thereafter, 12 parameters recorded or derived from the recording of the force platforms on measured operation time and the change of force data, then assess the results of BBS for correlation analysis. The results show that the relevance of BBS and force-related parameters is lower than the relevance of BBS and time-related parameters.Whereas, Ls seatoff (the duration from the onset of leg to the time of seatoff) has high correlation with BBS results. This achievement can be an effective initial assessment of early warning on the results of BBS elderly people’s ability to, thus reducing subjective measurement results of possible bias.	berg connector;human factors and ergonomics;onset (audio);operation time;relevance;risk factor (computing);risk management	Vincent G. Duffy	2017		10.1007/978-3-319-58463-8	reliability engineering;participatory ergonomics;systems engineering;engineering;manufacturing engineering	HCI	-47.248565870372914	-54.16781524299506	141715
f60ec57c82296aebcab0cba8a8707670f94e60bf	is google enough? comparison of an internet search engine with academic library resources	busqueda informacion;europa;evaluation performance;search engine;buscador;base donnee;university student;performance evaluation;search engines;information retrieval;estudio comparativo;evaluacion prestacion;biblioteca ensenanza superior;database;base dato;pregunta documental;informacion documentacion;etude comparative;internet;bibliotheque enseignement superieur;google moteur recherche;recherche information;system design;royaume uni;united kingdom;comparative study;reino unido;query;web search;higher education library;moteur recherche;europe;grupo a;ciencias sociales;academic libraries;requete	Purpose – The purpose of the study was to compare an internet search engine, Google, with appropriate library databases and systems, in order to assess the relative value, strengths and weaknesses of the two sorts of system. Design/methodology/approach – A case study approach was used, with detailed analysis and failure checking of results. The performance of the two systems was assessed in terms of coverage, unique records, precision, and quality and accessibility of results. A novel form of relevance assessment, based on the work of Saracevic and others was devised. Findings – Google is superior for coverage and accessibility. Library systems are superior for quality of results. Precision is similar for both systems. Good coverage requires use of both, as both have many unique items. Improving the skills of the searcher is likely to give better results from the library systems, but not from Google. Research limitations/implications – Only four case studies were included. These were limited to the kind of queries likely to be searched by university students. Library resources were limited to those in two UK academic libraries. Only the basic Google web search functionality was used, and only the top ten records examined. Practical implications – The results offer guidance for those providing support and training for use of these retrieval systems, and also provide evidence for debates on the “Google phenomenon”. Originality/value – This is one of the few studies which provide evidence on the relative performance of internet search engines and library databases, and the only one to conduct such in-depth case studies. The method for the assessment of relevance is novel.	accessibility;code coverage;database;google search;library (computing);quality of results;relevance;value (ethics);web search engine	Jan Brophy;David Bawden	2005	Aslib Proceedings	10.1108/00012530510634235	computer science;world wide web;information retrieval;search engine	Web+IR	-37.561101768068845	-59.75384856867967	141921
21aef781319da8674880e9bf4a83f3c3d658c5f4	fact retrieval and deductive question-answering information retrieval systems	information retrieval system;natural language;document retrieval;question answering	Information Retrieval systems may be classified either as Document Retrieval systems or Fact Retrieval systems. It is contended that at least some of the latter will require the capability for performing logical deductions among natural language sentences. The problem of developing systems of logical inference for natural languages is discussed, and an example of such an analysis of a sublanguage of English is presented. An experimental Fact Retrieval system which incorporates this analysis has been programmed for the IBM 7090 computer, and its main algorithms are stated.	algorithm;document retrieval;ibm 7090;information retrieval;natural language;question answering;sublanguage;uniform memory access	William S. Cooper	1964	J. ACM	10.1145/321217.321218	natural language processing;document retrieval;question answering;relevance;cognitive models of information retrieval;computer science;database;adversarial information retrieval;natural language;vector space model;data retrieval;information retrieval;human–computer information retrieval	Web+IR	-35.30573675106882	-64.04771012773966	141976
1392e5ddd9095bd18b55fcfddd108dd4242e8d11	exploiting collaborative filtering techniques for automatic assessment of student free-text responses	assessment of student response;collaborative filtering;feature based matrix factorization	The automatic assessment of free-text responses of students is a relatively newer task in both computational linguistics and educational technology. The goal of the task is to produce an assessment of student answers to explanation and definition questions typically asked in problems seen in practice exercises or tests. Unlike some conventional methods which assess the student responses based on only information about their corresponding questions, this paper exploits idea of collaborative filtering to analyze student responses and used an effective collaborative filtering model -- feature-based matrix factorization model to deal with this challenge. The experimental results show that our feature-based matrix factorization model outperforms the baseline models and the model with a re-ranking phase can achieve a better and competitive performance -- 63.6% overall accuracy on the Beetle dataset.	baseline (configuration management);collaborative filtering;computational linguistics	Tao Ge;Zhifang Sui;Baobao Chang	2013		10.1145/2505515.2507827	computer science;artificial intelligence;collaborative filtering;machine learning;data mining;multimedia;world wide web;information retrieval	AI	-40.12482743751411	-59.775111537625676	142100
51a528e66981ab295b945492a37ae302bc6b1dd1	influence of in-vehicle adaptive stop display on driving behavior and safety	mobile communication systems;advanced driver information systems;adaptive sign un signalized intersections stop signs connected vehicles its adaptive human factors driving behavior compliance;computer crashes;intelligent transportation systems;vehicles safety roads computer crashes vehicle crash testing;human factors salient in vehicle adaptive stop display driving behavior driving safety stop sign controlled intersections fatal traffic incidents united states fatal crashes traffic control device connected vehicle technology mobility virginia smart road adjacent traffic equipment malfunctions total equipment failures;human factors;vehicle to infrastructure communications;roads;stop signs;safety;vehicle crash testing;vehicles;behavior;unsignalized intersections;driver support systems;traffic engineering computing behavioural sciences computing human factors road accidents road safety road traffic control	In 2012, 683 000 crashes occurred at stop-sign-controlled intersections, with 2434 of those crashes being fatal and composing 5.3% of all fatal traffic incidents in the United States. Roughly 50% of all fatal crashes at stop-sign-controlled intersections involve crossing over (i.e., running) the traffic control device. With the advent of connected-vehicle technology, it is possible to provide a salient in-vehicle adaptive stop display to a driver. This display could alert a driver when he or she will have to stop at an intersection due to oncoming traffic. The same display could also permit drivers to pass through an intersection without stopping when a conflicting vehicle is not present. The purpose of this paper was to evaluate these potential improvements in safety and mobility through an empirical research study. An in-vehicle adaptive stop display was developed and tested on the Virginia Smart Road. Forty-nine drivers were exposed to multiple intersection scenarios they would experience in the real world while using connected-vehicle technology. The scenarios included variations in adjacent traffic, equipment malfunctions, and total equipment failures. There were no indications of a safety detriment to using the adaptive stop display in terms of compliance (likelihood of driver adhering to the information presented), driver complacency, or driver risk taking. Furthermore, the study indicates that, with a higher measured rate of compliance, an in-vehicle adaptive stop display would have a positive impact on safety.	smart highway	Alexandria M. Noble;Thomas A. Dingus;Zachary R. Doerzaph	2016	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2016.2523438	computer science;engineering;human factors and ergonomics;automotive engineering;transport engineering;computer security;behavior	HCI	-46.863288955338234	-52.51331499220616	142475
691e814ead6f541af5b90974e88ea694fad17e26	dublin city university at clef 2005: cross-language speech retrieval (cl-sr) experiments	busqueda informacion;modelizacion;linguistique;universite;limite libre;traduccion automatica;transcription automatique;metadata;pseudo relevance feedback;free boundary;average precision;information retrieval;frase;domain specific translation lexicons;semantics;speech retrieval;semantica;semantique;automatic generation;transcripcion automatica;modelisation;sentence;linguistica;traduction automatique;emotion emotionality;indexing;recherche information;indexation;adaptive method;indizacion;metadonnee;machine translating;retroaction pertinence;emotion emotivite;university;phrase;document retrieval;emocion emotividad;metadatos;information system;multilinguisme;automatic transcription;modeling;universidad;relevance feedback;systeme information;multilingualism;machine translation;multilinguismo;sistema informacion;automatic translation;linguistics	The Dublin City University participation in the CLEF 2005 CL-SR task concentrated on exploring the application of our existing information retrieval methods based on the Okapi model to the conversational speech data set. This required an approach to determining approximate sentence boundaries within the free-flowing automatic transcription provided to enable us to use our summary-based pseudo relevance feedback (PRF). We also performed exploratory experiments on the use of the metadata provided with the document transcriptions for indexing and relevance feedback. Topics were translated into English using Systran V3.0 machine translation. In most cases Title field only topic statements performed better than combined Title and Description topics. PRF using our adapted method is shown to be affective, and absolute performance is improved by combining the automatic document transcriptions with additional metadata fields.		Adenike M. Lam-Adesina;Gareth J. F. Jones	2005		10.1007/11878773_87	natural language processing;document retrieval;search engine indexing;speech recognition;systems modeling;computer science;artificial intelligence;database;semantics;linguistics;machine translation;metadata;information retrieval;information system	NLP	-35.377218682674176	-63.180433226821066	143318
d4aca699a49ee59fe4fb85bbea7f8f1f3a3fdae2	querying across languages: a dictionary-based approach to multilingual information retrieval	multilingual information retrieval;query translation	The multilingual information retrieval system of the future will need to be able to retrieve documents across language boundaries. This extension of the classical IR problem is particularly challenging, ax significant resources are required to perform query translation. At Xerox, we are working to build a multilingual IR system and conducting a series of experiments to understand what factors are most important in making the system work. Using translated queries and a bilingual transfer dictionary, we have learned that crosslartguage multilingual IR is feasible, although performance lags considerably behind the monolingual standard. The experiments suggest that correct identification and translation of multi-word terminology is the single most important source of error in the system, although amblguit y in translation also contributes to poor performance.	dictionary;experiment;information retrieval	David A. Hull;Gregory Grefenstette	1996		10.1145/243199.243212	natural language processing;query expansion;computer science;database;information retrieval;query language;human–computer information retrieval	Web+IR	-33.73560601915926	-63.77921862013449	143357
7134c3396e50676d6f1ebbe5b2d9a518ccbc41d3	the small world web	community;search engine;red www;measurement;conception;small world;prototipo;motor investigacion;internet;medida;connectivite;diseno;world wide web;comunidad;design;reseau www;mesure;moteur recherche;connectivity;small world network;prototype;communaute	I show that the World Wide Web is a small world, in the sense that sites are highly clustered yet the path length between them is small. I also demonstrate the advantages of a search engine which makes use of the fact that pages corresponding to a particular search query can form small world networks. In a further application, the search engine uses the small-worldness of its search results to measure the connectedness between communities on the Web.	connected component (graph theory);java;jim christy;prototype;random graph;string searching algorithm;strongly connected component;web search engine;world wide web	Lada A. Adamic	1999		10.1007/3-540-48155-9_27	design;community;web query classification;simulation;computer science;connectivity;prototype;world wide web;search engine;measurement	Web+IR	-35.66720451199555	-58.09697609002037	143374
4538af1386b23c61fdc159607e2f75966019d751	search transition as a measure of effort in information retrieval interaction	interactive information retrieval;search transitions;vdp samfunnsvitenskap 200 biblioteks og informasjonsvitenskap 320 kunnskapsgjenfinning og organisering 323;vdp samfunnsvitenskap 200 biblioteks og informasjonsvitenskap 320;effort	In this article we introduce the concept of search transitions as a unit for measuring the effort invested by searchers in information retrieval interaction. The concept is discussed and compared to traditional measures of effort, such as time. To investigate the usability of the search transition measure we have performed an analysis of 149 logs in an IR system indexing a collection of 650.000 Wikipedia articles. Our findings show that search transitions correlate with other, more mechanistic, effort measures. Additional experiments are necessary to investigate if it is a better measure of effort than e.g. number of documents examined.	document;experiment;information retrieval;usability;wikipedia	Ragnar Nordlie;Nils Pharo	2013		10.1002/meet.14505001044	computer science;data science;data mining;information retrieval	Web+IR	-34.2716579965338	-53.71327571006819	144146
ef9f6d33e0cac4ce2661ddbc733d0d58cc76d58f	the effect of handle angle on mawl, wrist posture, rpe, and heart rate	workload;hand;whole body;ritmo cardiaco;ergonomia aplicada;position;ergonomie conception;handling;caja embalaje;manual materials handling;hombre;posicion;packing case;manutention;manutencion;rythme cardiaque;rate of perceived exertion;posture;heart rate;maniobrabilidad;manual activity;actividad manual;human;postura;charge travail;maneuverability;mano;activite manuelle;esfuerzo;angulo;effort;main;carga trabajo;angle;manoeuvrabilite;applied ergonomics;homme;caisse	In manual material handling tasks, the handle serves as the interface between the human operator and the box (the materials). Handle angle design can affect both wrist posture and lifting ability. This study was designed to evaluate the effect of handle angle on maximal acceptable weight of lifting (MAWL), perceived whole-body exertion, whole-body workload, wrist posture, and perceived wrist exertion. The results indicate that handle angle had a significant effect on wrist posture and wrist rating of perceived exertion (RPE). A box with a 0 degrees handle angle induced the greatest ulnar deviation and the highest wrist RPE. A 75 degrees handle angle induced the greatest radial deviation and a relatively high wrist RPE. A 30 degrees handle angle resulted in the greatest MAWL and the lowest level of wrist RPE. Overall, these findings suggest that 30 degrees and 45 degrees handle angles can provide favorable coupling conditions for the cutout-type handhold container handle. Actual or practical applications include the ergonomic design of container handles for manual material handling tasks industry.		Mao-Jiun J. Wang;Hsiu-Chen Chung;Hsin-Chung Chen	2000	Human factors	10.1518/001872000779698079	simulation;position;engineering;angle;engineering drawing;mechanical engineering	HCI	-45.72945248319343	-55.704946532445945	144250
f97b2c02d239dc36a0f33605d27ed22a25914aff	a note on a set of functions for information retrieval	information retrieval	Abstract   An information retrieval system for experimental use is considered and a notation is introduced to help in its description. The system is experimental in that it is designed to facilitate the examination of the effect of different keyword classifications on retrieval performance. The notation derives from the basic operations encountered in information retrieval and examples are given of its use in a system which has already been implemented on a computer. These operations and their relevance to retrieval system is described in detail. Consideration is given to store allocation in the implementation of such a system and a simple method of automatic allocation is presented.	information retrieval	David M. Jackson	1969	Information Storage and Retrieval	10.1016/0020-0271(69)90004-7	relevance;cognitive models of information retrieval;computer science;theoretical computer science;data mining;vector space model;data retrieval;information retrieval;human–computer information retrieval	Web+IR	-34.15409207980158	-60.72236957790033	144353
e4107ec85367130f1053aee61874992508570932	annotations for power relations on email threads		Social relations like power and influence are difficult concepts to define, but are easily recognizable when expressed. In this paper, we describe a multi-layer annotation scheme for social power relations that are recognizable from online written interactions. We introduce a typology of four types of power relations between dialog participants: hierarchical power, situational power, influence and control of communication. We also present a corpus of Enron emails comprising of 122 threaded conversations, manually annotated with instances of these power relations between participants. Our annotations also capture attempts at exercise of power or influence and whether those attempts were successful or not. In addition, we also capture utterance level annotations for overt display of power. We describe the annotation definitions using two example email threads from our corpus illustrating each type of power relation. We also present detailed instructions given to the annotators and provide various statistics on annotations in the corpus.	biological anthropology;conversation threading;email;interaction;layer (electronics);text corpus;dialog	Vinodkumar Prabhakaran;Huzaifa Neralwala;Owen Rambow;Mona T. Diab	2012			html email;database;internet privacy;world wide web	NLP	-44.43205398334969	-61.53041756617415	144788
370edfdb04ae7f8abbf4be7d386a5c449754aa88	dual task model - an evaluation model for the complex operation	dual task;evaluation model	An evaluation model for the dual task situation was developed. The model estimates the total duration of the operation for any bench–mark task based on the unit operations.		Masaaki Kurosu	1994		10.1145/259963.260105	computer science	NLP	-47.43183927332918	-54.06179207502186	144875
9e5c8edc5e1a66a92b74dc87a976bfcd0824b611	the effect of topic set size on retrieval experiment error	experimental design;research needs;measurement error;test collections;error rate;difference set;information need;test collection	"""Retrieval mechanisms are frequently compared by computing the respective average scores for some effectiveness metric across a common set of information needs or topics, with researchers concluding one method is superior based on those averages. Since comparative retrieval system behavior is known to be highly variable across topics, good experimental design requires that a """"sufficient"""" number of topics be used in the test. This paper uses TREC results to empirically derive error rates based on the number of topics used in a test and the observed difference in the average scores. The error rates quantify the likelihood that a different set of topics of the same size would lead to a different conclusion. We directly compute error rates for topic sets up to size 25, and extrapolate those rates for larger topic set sizes. The error rates found are larger than anticipated, indicating researchers need to take care when concluding one method is better than another, especially if few topics are used."""	design of experiments;extrapolation;information needs;information retrieval;mean squared error;text retrieval conference	Ellen M. Voorhees;Chris Buckley	2002		10.1145/564376.564432	information needs;word error rate;computer science;data mining;design of experiments;information retrieval;difference set;statistics;observational error	Web+IR	-37.93021204323387	-61.88271935240843	145138
68f472e0c32f2e2a7f85759b2535deb455768a01	finding related forum posts through content similarity over intention-based segmentation		We study the problem of finding related forum posts to a post at hand. In contrast to traditional approaches for finding related documents that perform content comparisons across the content of the posts as a whole, we consider each post as a set of segments, each written with a different goal in mind. We advocate that the relatedness between two posts should be based on the similarity of their respective segments that are intended for the same goal, i.e., are conveying the same intention. This means that it is possible for the same terms to weigh differently in the relatedness score depending on the intention of the segment in which they are found. We have developed a segmentation method that by monitoring a number of text features can identify the parts of a post where significant jumps occur indicating a point where a segmentation should take place. The generated segments of all the posts are clustered to form intention clusters and then similarities across the posts are calculated through similarities across segments with the same intention. We experimentally illustrate the effectiveness and efficiency of our segmentation method and our overall approach of finding related forum posts.	experiment;mind	Dimitra Papadimitriou;Georgia Koutrika;Yannis Velegrakis;John Mylopoulos	2017	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2017.2699965	data mining;computer science;cluster analysis;text segmentation;segmentation;ranking;vocabulary	Web+IR	-33.974692982516096	-55.863923730170406	145144
afa51715c8e685af81189f62078303917325ffe6	knowledge in the head and on the web: using topic expertise to aid search	search engine;query formulation;domain knowledge;navigation;information scent;background knowledge;web search;expertise	The importance of background knowledge for effective searching on the Web is not well understood. Participants were given trivia questions on two topics and asked to answer them first using background knowledge and second by searching on the Web. Knowledge of a topic predicted search performance on that topic for all questions and, more importantly, for questions for which participants did not already know the answer. In terms of process, greater topic knowledge led to less time being spent on each Webpage, faster decisions to give up a line of inquiry and shorter queries being entered into the search engine. A more complete theory-led understanding of these effects would assist workers in a whole range of Web-related professions.	web page;web search engine;world wide web	Geoffrey B. Duggan;Stephen J. Payne	2008		10.1145/1357054.1357062	navigation;computer science;knowledge management;artificial intelligence;data mining;world wide web;information retrieval;domain knowledge;search engine	HCI	-34.761616630728426	-53.385143582393255	145287
4d7ad254fca332b36b2d3f86f3501d1f6bbd41d3	automatic cataloguing and searching for retrospective data by use of ocr text	languages;information retrieval;indexing	This article describes our efforts in supporting information retrieval from OCR degraded text. In particular, we report our approach to an automatic cataloging and searching contest for books in multiple languages. In this contest, 500 books in English, German, French, and Italian published during the 1770s to 1970s are scanned into images and OCRed to digital text. The goal is to use only automatic ways to extract information for sophisticated searching. We adopted the vector space retrieval model, an n-gram indexing method, and a special weighting scheme to tackle this problem. Although the performance by this approach is slightly inferior to the best approach, which is mainly based on regular expression match, one advantage of our approach is that it is less language dependent and less layout sensitive, thus is readily applicable to other languages and document collections. Problems of OCR text retrieval for some Asian languages are also discussed in this article, and solutions are suggested.	optical character recognition	Yuen-Hsien Tseng	2001	JASIST	10.1002/1532-2890(2001)9999:9999%3C::AID-ASI1080%3E3.0.CO;2-A	computer science;database;world wide web;information retrieval	HCI	-34.23016868140929	-64.21619343236048	145333
b1889b124421d742362af6eb95e219fcbe407f51	the proximity of co-citation	co citation analysis;citation contextual;pubmed central;co citation proximity	Traditional co-citation analysis has not taken the proximity of co-cited references into account. As long as two references are cited by the same article, they are retreated equally regardless the distance between where citations appear in the article. Little is known about what additional insights into citation and co-citation behaviours one might gain from studying distributions of co-citation in terms of such proximity. How are citations distributed in an article? What insights does the proximity of co-citation provide? In this article, the proximity of a pair of co-cited reference is defined as the nearest instance of the co-citation relation in text. We investigate the proximity of co-citation in full text of scientific publications at four levels, namely, the sentence level, the paragraph level, the section level, and the article level. We conducted four studies of co-citation patterns in the full text of articles published in 22 open access journals from BioMed Central. First, we compared the distributions of co-citation instances at four proximity levels in journal articles to the traditional article-level co-citation counts. Second, we studied the distributions of co-citations of various proximities across organizational sections in articles. Third, the distribution of co-citation proximity in different co-citation frequency groups is investigated. Fourth, we identified the occurrences of co-citations at different proximity levels with reference to the corresponding traditional co-citation network. The results show that (1) the majority of co-citations are loosely coupled at the article level, (2) a higher proportion of sentence-level co-citations is found in high co-citation frequencies than in low co-citation frequencies, (3) tightly coupled sentence-level co-citations not only preserve the essential structure of the corresponding traditional co-citation network but also form a much smaller subset of the entire co-citation instances typically considered by traditional co-citation analysis. Implications for improving our understanding of underlying factors concerning co-citations and developing more efficient co-citation analysis methods are discussed.	citation analysis;citation network;co-citation;journal citation reports;loose coupling;scientific literature	Shengbo Liu;Chaomei Chen	2011	Scientometrics	10.1007/s11192-011-0575-7	computer science;data mining;world wide web;information retrieval	Web+IR	-39.60090168125914	-62.600572580377694	145630
e97efd42e8d51572b0782c1f3b70f47070070cba	determining information retrieval and filtering performance without experimentation	prediccion;experimental method;evaluation performance;metodo analitico;inverse document frequency;aplicacion;performance evaluation;methode mesure;optimal filtering;information retrieval;evaluacion prestacion;model performance;metodo medida;systeme recherche;recherche developpement;documentation data processing;search system;modelo;research and development;recherche documentaire;sistema investigacion;investigacion desarrollo;analytical method;recuperacion documental;methode analytique;performance prediction;retrieval model;modele;document retrieval;measurement method;application;relevance feedback;prediction;models;filtrado optimo;filtrage optimal;value prediction;informacion documental;informatique documentaire	-The performance of an information retrieval or text and media filtering system may be determined through analytic methods as well as by traditional simulation or experimental methods. These analytic methods can provide precise statements about expected performance. They can thus determine which of two similarly performing systems is superior. For both a single query term and for a multiple query term retrieval model, a method for comparing the performance of different probabilistic retrieval methods is developed. This method may be used in computing the average search length for a query, given only knowledge of database parameter values. Predictive models for inverse document frequency, binary independence, and relevance feedback based retrieval and filtering are described. Simulations illustrate how the single term model performs and sample performance predictions are given for single term and multiple term problems.	computer simulation;information retrieval;predictive modelling;probabilistic database;relevance feedback;tf–idf	Robert M. Losee	1995	Inf. Process. Manage.	10.1016/0306-4573(95)00072-O	document retrieval;query expansion;prediction;computer science;artificial intelligence;term discrimination;tf–idf;vector space model;information retrieval;algorithm;divergence-from-randomness model	Web+IR	-36.53176733921532	-61.49869934127602	146292
1c9e7f2c8aef7ab332f154067e41b4bc54554346	characterization of expertise to build an augmented skill training system for construction industry		Rebar Bending is one of the important trade in the construction industry. Major motor skill like force of bend, bend angle accuracy, lever positioning, lever tilting are guiding factor for doing the manual rebar bending process. To be an expert in rebar bending, one should need at least 3 to 4 years of field experience, to learn all motor skill parameters. This paper describes in detail the skill parameters involved during the manual rebar bending process through experimental methods. Comparing the expert data with novice which outline the design of real time guidance system for effective skill transfer and learning. The lever positioning skill learning in manual rebar bending process projected a high accuracy with the help of feedback system.	any-angle path planning;guidance system;instability;kinect	Sasi Deepu;Mohan T. Harish;Rao R. Bhavani	2018	2018 IEEE 18th International Conference on Advanced Learning Technologies (ICALT)	10.1109/ICALT.2018.00034	knowledge management;bending;simulation;training system;guidance system;motor skill;computer science;rebar;lever	Robotics	-46.991121578158655	-57.72094551593095	146342
238c23de0e37c8dbcb6ee64b43d8fe0fc4bdd548	question generation and formulation: an indication of information need	research design;data collection;information needs;inquiry;statistical analysis;research methodology;tables data;information need;cognitive processes;information seeking	Abstract#R##N##R##N#A methodology is developed and applied to a data collection consisting of questions in order to discern the nature of the cognitive need “to know”: the information need. The experimental design for the generation and formulation of questions was achieved by means of a “closed” problem situation. A “closed” problem situation has within it sufficient and available data with which to reach solution of said problem. The question is an observable behavioral act which reflects information need. Because it is a hypothetical construct, information need had to be operationalized indirectly via the question as the dependent variable and the data input as the independent variable. The varying of this independent variable was achieved by partitioning the “closed” problem. Two experimental hypotheses guided this methodology for a “closed” problem situation: H1 where the number of questions generated varies directly as the Information need varies, and H2 where the number of questions generated varies inversely as the data input varies. The results of the statistical analysis supported the indicated trend to decreasing questioning activity as the data input rose. The results of the linguistic analysis revealed a consistent question formulation pattern between the two “closed” problems. Thus it follows in theory that the nature of information need can be discerned indirectly through question generation and formulation.	information needs	Esther E. Home	1983	JASIS	10.1002/asi.4630340103	information needs;computer science;artificial intelligence;data mining;mathematics;information retrieval;algorithm;statistics	NLP	-41.11446081359545	-55.70973394965632	146394
08e9faa592fe669ba66f4f19c866440f31592f9b	is it necessary to consider suburbs (or small cities in the close proximity) and name variants in a citation impact analysis for bigger cities? an investigation using munich as an example	impact analysis	In a recently published Nature report van Noorden (2010) presented some studies and Web applications for the comparson of bigger cities worldwide with regard to publication output and citation impact (see here also Frenken, Hardeman, & oekman, 2009). Results like the following are presented from the studies: “Beijing, which churned out 0.76% of the global utput in 1996, produced 2.74% in 2008 (319,000 research papers). Other fast-growing areas are Tehran, Istanbul, Seoul, ingapore City and São Paulo” (van Noorden, 2010, p. 907). However, the results of the different studies and Web applicaions concerning output and impact are not directly comparable: How one scientist defines a city area is not consistent with ow another would do this (and it depends especially on familiarity with the city and institutes involved). For example, atthiessen, Schwarz, and Find (2010, p. 1882) have combined neighbouring cities if the cities are reachable with a “less han 45 minutes of surface transport”. In the Web applications presented as online collection by van Noorden (2010) the itation impact and publication output of each city is analysed without considering neighbouring cities. Thus, the question rises of city performance differences in dependency on the grouping approaches of the scientists: does it make a difference o consider (1) only the main city variant used by authors of publications (without considering neighbouring cities) or (2) all ity name variants in addition to cities in proximity? We investigated this question using Munich (in German: München) as n example. The results of our analyses are presented in Table 1. The data come from a static version of the Scopus database xtracted in 2009. We restricted not only to articles, reviews and conference papers but also to a “core” Scopus journal set hich excludes stand-alone conference proceedings or very patchily covered journals. The table shows article counts and relative citation rates of articles published with certain city name variants used y authors. Besides München, Munchen (without umlaut) is also indexed as Munich; Garching, Oberschleissheim and artinsried are suburbs of Munich (or small cities closed to Munich). Publications for a given city variant (for example, Garching”) involves no double-counting – all publications that contain the term “Garching” in the AFFILCITY field in the nderlying metadata are counted here, and not in any other variant’s count. So, the data shown pertains solely to that ariant in the dataset. The relative citation rates shown for the different name variants in the table are journal-oriented eld-normalized citation ratios: each city’s observed citation count is divided by the expected citation count for the ame document types published within the same publication year in the same field(s) (defined by a journal-level field lassification). Where a city has published documents in journals classified in more than one field, an average of the xpected citation counts in these fields is used to derive the expected citation count. A relative citation rate of 1 means that he city in question is no more and no less cited than expected for an “average” city publishing in the corresponding field(s). The numbers for the name variant München in Table 1 are exactly the same numbers like those which can be extracted rom the Web application “Output versus Impact” on http://www.nature.com/news/specials/cities/best-cities.html. This ariant incorporates clearly more articles than all other variants: For example, whereas München has 34,260 articles in he year 2000, Oberschleissheim has only 258. Thus München is the dominant variant for Munich and its close proximity hich should be compared with all other different variants (see the notes of the table). The relative citation rates for ll different variants are shown in the line total. As the small differences between München and total in Table 1 reveals he city name variants do no add much citation impact to München. The differences are between 0.07 (year: 2006) and .01 (year: 2001). These results indicate that for Munich the citation impact values do not depend on the consideration of suburbs or small ities, respectively, in close proximity and different name variants besides the variant München. Since our analyses refer to nly one city, our results are not generalizable. They furnish first evidence which should be proofed with other cities. This etter to the Editor is intended to motivate other researchers to undertake these investigations and to develop standards in his respect. Citation impact analyses for city areas are often done in a more or less subjective way.	han unification;journal citation reports;langrisser schwarz;maplestory;scopus;web application;world wide web	Lutz Bornmann;Andrew M. Plume	2011	J. Informetrics	10.1016/j.joi.2011.05.002	computer science;operations research	ML	-41.51367979065393	-65.29707258427725	146629
c23540b648ce874e0113df0cf1d587d788376490	a day in the life of web searching: an exploratory study	busqueda informacion;etude utilisateur;etude utilisation;search engine;buscador;query reformulation;analisis estadistico;red www;information retrieval;user study;estudio utilizacion;query formulation;reseau web;estudio usuario;pregunta documental;formulacion pregunta;formulation question;web search engine;question documentaire;fast;statistical analysis;recherche information;distribution temporelle;comportement utilisateur;analyse statistique;cost efficiency;query;world wide web;web search;excite;user behavior;moteur recherche;exploratory study;comportamiento usuario;use study;distribucion temporal;time distribution	Understanding Web searching behavior is important in developing more successful and cost-efficient Web search engines. We provide results from a comparative time-based Web study of US-based Excite and Norwegian-based Fast Web search logs, exploring variations in user searching related to changes in time of the day. Findings suggest: (1) fluctuations in Web user behavior over the day, (2) user investigations of query results are much longer, and submission of queries and number of users are much higher in the mornings, and (3) some query characteristics, including terms per query and query reformulation, remain steady throughout the day. Implications and further research are discussed.	cost efficiency;excite;web search engine;web search query	Seda Özmutlu;Amanda Spink;Huseyin Cenk Özmutlu	2004	Inf. Process. Manage.	10.1016/S0306-4573(03)00044-X	query expansion;web query classification;web analytics;web search engine;computer science;data mining;database;web search query;world wide web;exploratory research;search engine;cost efficiency	Web+IR	-37.20766897940916	-56.42708709672938	146672
85903d12cb5607a51c68bd5b269722371e6d6e52	an adaptive web cache access predictor using neural network	site web;controle acces;reseau communication;log files;web pages;red www;reseau web;propagacion larga distancia;cache memory;long distance propagation;adaptation and learning;log data;antememoria;antememoire;internet;propagation longue distance;backpropagation algorithm;prediction accuracy;algorithme retropropagation;world wide web;access control;long range;sitio web;learning artificial intelligence;reseau neuronal;red de comunicacion;web caching;donnee session;back propagation;communication network;red neuronal;web site;neural network;algoritmo retropropagacion;apprentissage intelligence artificielle	This paper presents a novel approach to successfully predict Web pages that are most likely to be re-accessed in a given period of time. We present the design of an intelligent predictor that can be implemented on a Web server to guide caching strategies. Our approach is adaptive and learns the changing access patterns of pages in a Web site. The core of our predictor is a neural network that uses a back-propagation learning rule. We present results of the application of this predictor on static data using log files; it can be extended to learn the distribution of live Web page access patterns. Our simulations show fast learning, uniformly good prediction, and up to 82% correct prediction for the following six months based on a oneday training data. This long-range prediction accuracy is attributed to the static structure of the test Web site.	artificial neural network;backpropagation;data logger;kerrison predictor;learning rule;server (computing);simulation;software propagation;web cache;web page;web server	Stella Wen Tian;Ben Choi;Vir V. Phoha	2002		10.1007/3-540-48035-8_44	simulation;computer science;artificial intelligence;backpropagation;operating system;machine learning;database;distributed computing;world wide web;artificial neural network	ML	-37.91262420479896	-57.356857311565854	146772
84db9dd3aabc6885f29d9ab17e8d4f1803f028cc	measurement of momentary user experience in an automotive context	user experience;day reconstruction method;momentary ux;methodology	Increased competition between manufacturers made it necessary for them to create products that provide great experiences in order to stand out from the rest of the market. For a positive product evaluation by the user, not only usability, but also user experience plays an important role. Recent research revealed that the most common method of using post-test questionnaires to assess user experience as a remembered episode could miss information about the experience [19]. This study presents a method for measuring user experience through fulfillment of psychological needs momentarily ('momentary UX') in situations instead of in a whole interaction episode. 28 participants used a novel navigation device that is designed to help explore the environment during a leisure car ride as a team together with other passengers. A story approach was used to create a context for the experimental product interaction. The results show that psychological needs relatedness and stimulation addressed by design could be measured specifically and momentarily for relevant events during the interaction. Further, a positive relationship to positive feelings in the interaction was found. Beyond that, a similar questionnaire for the whole episode is validated in this interaction and comparison between episodic and momentary UX is made. It is demonstrated that the method for measuring momentary UX is specific for needs and situations, and possible use cases are discussed.	a/ux;usability;user experience	Moritz Körber;Klaus Bengler	2013		10.1145/2516540.2516555	simulation;human–computer interaction;engineering;social psychology	HCI	-48.040288387396075	-52.29728249600397	147075
5e26ff617b8c61a352365e82f90dbcce6f29e302	cross-language information retrieval using multiple resources and combinations for query expansion	recurso internet;lenguaje documental;base donnee;keyword;query reformulation;analisis estadistico;internet resource;information retrieval;data collection;database;pertinencia;base dato;palabra clave;ressource internet;mot cle;langage documentaire;internet;statistical analysis;recherche information;pertinence;analyse statistique;retroaction pertinence;information language;ambiguity;recuperacion informacion;relevance;query expansion;ambiguedad;relevance feedback;cross language information retrieval;ambiguite	As Internet resources become accessible to more and more countries, there is a need to develop efficient methods for information retrieval across languages. In the present paper, we focus on query expansion techniques to improve the effectiveness of an information retrieval. A combination to a dictionary-based translation and statistical-based disambiguation is indispensable to overcome translation’s ambiguity. We propose a model using multiple sources for query reformulation and expansion to select expansion terms and retrieve information needed by a user. Relevance feedback, thesaurus-based expansion, as well as a new feedback strategy, based on the extraction of domain keywords to expand user’s query, are introduced and evaluated. We tested the effectiveness of the proposed combined method, by an application to a French-English Information Retrieval. Experiments using CLEF data collection proved a great effectiveness of the proposed combined query expansion techniques.	categorization;cross-language information retrieval;data dictionary;eurowordnet;experiment;query expansion;relevance feedback;statistical model;thesaurus;web page;word-sense disambiguation;wordnet	Fatiha Sadat;Masatoshi Yoshikawa;Shunsuke Uemura	2002		10.1007/3-540-36077-8_11	query optimization;query expansion;web query classification;the internet;ranking;relevance;computer science;concept search;data mining;database;web search query;world wide web;information retrieval;query language;statistics;data collection;human–computer information retrieval	Web+IR	-35.32499160935614	-61.8147899211151	147448
f9a4e740b6c4123b0b79b9223a6093d6340d2a11	too quick? log analysis of quick links from an academic library website	transactional log analysis;log files;log analysis;website design;interface design;worldwide web;web sites;academic libraries;transactional analysis	Purpose – To study the use of “Quick Links”, a common navigational element, in the context of an academic library website.Design/methodology/approach – Transaction log files and web server logs are analyzed over a four‐year period to detect patterns in Quick Link usage.Findings – Provides information about what Quick Links have been used over time, as well as the relationship of Quick Link usage to the rest of the library website. Finds generally that Quick Link usage is prevalent, tilted toward a few of the choices, and is drawn largely from the library homepage as referral source.Research limitations/implications – Log analysis does not include IP referral data, which limits the ability to determine different patterns of use by specific locations including services desks, off‐campus, and in‐house library usage.Practical implications – This paper is useful for website usability in terms of design decisions and log analysis.Originality/value – This paper targets a specific website usability issue over time.	log analysis	Jimmy Ghaphery	2005	OCLC Systems & Services	10.1108/10650750510612353	computer science;interface design;web log analysis software;data mining;database;transactional analysis;world wide web	Logic	-38.48076673224751	-55.19994362745915	147541
61fa1e9404060cd5d51a31e31ba5fde415f385b7	the role of human factors in stereotyping behavior and perception of digital library users: a robust clustering approach	hierarchical clustering;cognitive style;robust clustering;digital library;digital libraries;k means;gender difference;stereotypes;fuzzy clustering;research paper;human factors;user behavior;perception;behavior	To deliver effective personalization for digital library users, it is necessary to identify which human factors are most relevant in determining the behavior and perception of these users. This paper examines three key human factors: cognitive styles, levels of expertise and gender differences, and utilizes three individual clustering techniques: k-means, hierarchical clustering and fuzzy clustering to understand user behavior and perception. Moreover, robust clustering, capable of correcting the bias of individual clustering techniques, is used to obtain a deeper understanding. The robust clustering approach produced results that highlighted the relevance of cognitive style for user behavior, i.e., cognitive style dominates and justifies each of the robust clusters created. We also found that perception was mainly determined by the level of expertise of a user. We conclude that robust clustering is an effective technique to analyze user behavior and perception.	cluster analysis;cognitive tutor;computer cluster;digital library;experiment;exponent bias;external validity;filter (signal processing);fuzzy classification;fuzzy clustering;hierarchical clustering;human factors and ergonomics;interaction;k-means clustering;neuro-fuzzy;personalization;relevance;signal-to-noise ratio;stereotype (uml);universal quantification;user modeling;web application;web search engine	Enrique Frías-Martínez;Sherry Y. Chen;Robert D. Macredie;Xiaohui Liu	2007	User Modeling and User-Adapted Interaction	10.1007/s11257-007-9028-7	digital library;cognitive style;fuzzy clustering;computer science;machine learning;data mining;hierarchical clustering;multimedia;perception;world wide web;behavior;conceptual clustering	HCI	-44.349652750378	-56.53381496162301	147792
e773be3e97c7b7604a0afae11d1d622cba0d87b1	secondary task boundaries influence drivers' glance durations	driving;in vehicle system interface;task switching;distraction;eye glance	Drivers show a wide range of behavior while performing a secondary task behind the wheel. In the current study, we categorized drivers into groups based on their glance behavior at task boundary (i.e., pressing a touch screen button after reading driving-related messages) and compared driving capabilities of drivers in each group. The comparison between the groups identifies different eye glance strategies, or task switching decisions, and associated vehicle control behaviors. Senders' uncertainty model was adapted to explain the results and to suggest future directions in developing driver models.	categorization;device driver;touchscreen;tree accumulation	Ja Young Lee;Madeleine Gibson;John D. Lee	2015		10.1145/2799250.2799269	computer vision;simulation;engineering;communication	HCI	-46.90322245147875	-52.136873468749286	147922
c13259fc3fb9f9a46eafce8ef6b9a39325bdca2a	extending tree-maps to three dimensions: a comparative study	interfase usuario;empirical study;methode empirique;modelo 3 dimensiones;three dimensions;hierarchized structure;user interface;modele 3 dimensions;relacion hombre maquina;metodo empirico;remplissage;espacio 3 dimensiones;empirical method;three dimensional model;man machine relation;structure hierarchisee;information visualization;filling;three dimensional;analyse tâche;espace 3 dimensions;task analysis;three dimensional space;data visualization;comparative study;interface utilisateur;visualisation donnee;relation homme machine;estructura jerarquizada;relleno	This paper presents StepTree, an information visualization tool designed for depicting hierarchies, such as directory structures. StepTree is similar to the hierarchy-visualization tool, Treemap, in that it uses a rectangular, space-filling methodology, but differs from Treemap in that it employs threedimensional space, which is used to more clearly convey the structural relationships of the hierarchy. The paper includes an empirical study comparing typical search and analysis tasks using StepTree and Treemap. The study shows that users perform significantly better on tasks related to interpreting structural relationships when using StepTree. In addition, users achieved the same performance with StepTree and Treemap when doing a range of other common interpretative and navigational tasks.	information visualization;treemapping	Thomas Bladh;David A. Carr;Jeremiah Scholl	2004		10.1007/978-3-540-27795-8_6	three-dimensional space;information visualization;computer science;artificial intelligence;data visualization	HCI	-44.083989971870736	-55.080705135138466	148025
c14205091b4c68f8d269b9990a1fcb558a02326f	text retrieval through corrupted queries	information retrieval system;information retrieval;degraded text;indexation;text retrieval;language model	Our work relies on the design and evaluation of experimental information retrieval systems able to cope with textual misspellings in queries. In contrast to previous proposals, commonly based on the consideration of spelling correction strategies and a word language model, we also report on the use of character n-grams as indexing support.	document retrieval;grams;information retrieval;language model;n-gram;null character	Juan Otero Pombo;Jesús Vilares;Manuel Vilares Ferro	2008		10.1007/978-3-540-88309-8_37	natural language processing;document retrieval;visual word;question answering;relevance;cognitive models of information retrieval;computer science;concept search;data mining;term discrimination;vector space model;information retrieval;human–computer information retrieval	Web+IR	-34.03888455634226	-63.1330111269779	148224
84ea7980715a4a28748f6d517a8422d4bf5cc347	analyzing the behavior of professional video searchers using rai query logs	interactive searches professional video searchers rai query logs web search engines multimedia database management systems mdbms rai multimedia catalogue query reformulation strategies;query processing;search engines;tv search engines web search multimedia databases multimedia communication focusing database languages;multimedia databases;search engines multimedia databases query processing	A large number of studies have investigated the query logs of Web search engines, but there is a lack of analogous studies for multimedia database management systems (MDBMSs) used by professional searchers. In this paper we perform an extensive analysis of the query logs of the RAI multimedia catalogue, both at the query level and at the session level. Based on the observation that a large proportion of the queries returned zero or, conversely, too many hits, we identified three query reformulation strategies to reduce or enlarge the set of results. Our study indicates that the desire of controlling the amount of output may have a relatively limited (moderate-to-little) impact on the user's behavior, while at the same time some counter-intuitive findings suggest a suboptimal utilization of the system. The findings are useful for MDBMS developers and for trainers of professional searchers to improve the performance of interactive searches, and for researchers to conduct further work.	database;web search engine	Claudio Carpineto;Giovanni Romano;Andrea Bernardini	2012	2012 10th International Workshop on Content-Based Multimedia Indexing (CBMI)	10.1109/CBMI.2012.6269795	sargable;query optimization;query expansion;web query classification;computer science;database;web search query;world wide web;information retrieval;query language;search engine	DB	-35.83179019854615	-53.04417199379272	148272
3e47ff2ce9da716c34be122ce9325a03b9009792	towards expert systems for the selection of search keys	decision rule;expert system;information systems;requirement analysis;information retrieval system;text analysis;decision tree;controlled vocabulary	Intermediary expert systems are designed to mediate between end-users and complex information retrieval systems. However, since most of these expert systems are based on text analysis rather than on models of hum man searching, they cannot process requestrelated criteria, such as precision or recall requirements. Analysis of the searching behavior of human intermediaries revealed a routine for the selection of search keys-freetext or controlled vocabulary-along a decision tree. Examples of decision rules demonstrate that although further research is required, these rules can be automated to significantly enhance the adaptability of inter. mediary expert systems.	controlled vocabulary;decision tree;expert system;information retrieval;requirement	Raya Fidel	1986	JASIS	10.1002/(SICI)1097-4571(198601)37:1%3C37::AID-ASI6%3E3.0.CO;2-P	document retrieval;legal expert system;requirements analysis;controlled vocabulary;computer science;artificial intelligence;decision tree;data mining;decision rule;database;world wide web;expert system;information retrieval;information system	AI	-34.30722545514574	-59.34668364595396	148414
852772bafd828b814865067c8bb10a8d706cac30	a mathematical model of retrieval system performance	information retrieval;system performance;mathematical model;relevance information retrieval;mathematical models	There have been a number of major evaluations of the performance of retrieval systems against large full text and surrogate (bibliographic) databases. These evaluations have concentrated on the experimental determination of the Precision Ratio, the fraction of retrieved items that are relevant to an information request, and the Recall Ratio, the fraction of the total number of relevant items that were actually retrieved. While these measures have met with general acceptance, they have also generated much controversy. The purpose of this article is to review the results of several of the largest evaluations and to propose a simple model for the performance of such systems that may help explain the relationship between these measures and user behavior.	concentrate dosage form;database;evaluation;mathematical model	Davis B. McCarn;Craig M. Lewis	1990	Journal of the American Society for Information Science. American Society for Information Science	10.1002/(SICI)1097-4571(199010)41:7%3C495::AID-ASI3%3E3.0.CO;2-S	document retrieval;computer science;artificial intelligence;mathematical model;information retrieval	Web+IR	-37.174971671837184	-61.223144980491874	148786
f14ec40e8aac64f87df55d00ed4e62e2b13efc86	understanding the 'quality motion' of wikipedia articles through semantic convergence analysis			wikipedia	Huijing Deng;Bernadetta Tarigan;Mihai Grigore;Juliana Sutanto	2015		10.1007/978-3-319-20895-4_7	computer science;data mining;world wide web;information retrieval	AI	-38.35017100891095	-65.54852037157455	148894
37ce2d9acc98d27487f64bde50a04921c9255157	a typology of course of motion in simulated environments based on bézier curve analysis	modelizacion;navegacion;goodness of fit;factor humano;curva bezier;trajectoire;realite virtuelle;realidad virtual;diagnostico;virtual reality;courbure;acquisition connaissances;modelisation;qa75 electronic computers computer science;navigation;trajectory;human factors;courbe bezier;human factor;knowledge acquisition;typology;segment droite;curvatura;segmento recta;curvature;trayectoria;line segment;typologie;adquisicion de conocimientos;virtual environment;diagnosis;modeling;facteur humain;simulation environment;trajectory modelling;tipologia;bezier curve;spatial knowledge;diagnostic	This paper proposes a novel method of analysing trajectories followed by people while they perform navigational tasks. The results indicate that modelling trajectories with Bézier curves provides a basis for the diagnosis of navigational patterns. The method offers five indicators: goodness of fit, average curvature, number of inflexion points, lengths of straight line segments, and area covered. Study results, obtained in a virtual environment show that these indicators carry important information about user performance, specifically spatial knowledge acquisition.	biological anthropology;bézier curve;knowledge acquisition;virtual reality	Corina Sas;Nikita Schmidt	2007	Knowledge and Information Systems	10.1007/s10115-007-0065-7	navigation;simulation;systems modeling;typology;line segment;computer science;virtual machine;artificial intelligence;human factors and ergonomics;trajectory;bézier curve;curvature;goodness of fit	Robotics	-43.992241314672974	-55.79750987769643	149053
180e85253a6fe68faf70f3210749d16098497d7e	aplicación de tecnologías de procesamiento de lenguaje natural y tecnología semántica en brand rain y anpro21	ontologias;analisis de reputacion;computacion informatica;text mining;filologias;data mining;info eu repo semantics article;informacion documentacion;aprendizaje automatico;procesamiento de lenguaje natural;linguistica;machine learning;ciencias basicas y experimentales;web semantica;sentiment analysis;semantic web;ontologies;analisis de sentimiento;grupo a;ciencias sociales;reputation analysis;grupo b;natural language processing;mineria de datos	This paper presents the application and results on research about natural language processing and semantic technologies in Brand Rain and Anpro21. The related projects are explained and the obtained benefits from the research on this new technologies developed are presented. All this research have been applied on the monitoring and reputation system of Brand Rain.	natural language processing;reputation system	Oscar Trabazos;Silvia Suarez;Remei Bori;Oriol Flo	2014	Procesamiento del Lenguaje Natural		natural language processing;text mining;computer science;ontology;semantic web;sentiment analysis	AI	-37.137057663617426	-64.40177871904292	149420
832b7d8fe358cc3fc1352b6a7a5385366a9d250f	are web-based informational queries changing?	web search engine;search engines;navigation	This brief communication describes the results of a questionnaire examining certain aspects of the webbased information seeking practises of University students. The results are contrasted with past work showing that queries to web search engines can be assigned to one of a series of categories: namely navigational, informational and transactional. The survey results suggest that a large group of queries, which in the past, would have been classified as informational have become at least partially navigational. We contend that this change has occurred because of the rise of large web sites holding particular types of information, such as Wikipedia and IMDB.	information seeking;internet movie database (imdb);web search engine;web search query;wikipedia	Chadwyn Tann;Mark Sanderson	2009	JASIST	10.1002/asi.21053	web search engine;computer science;data mining;multimedia;world wide web;information retrieval;search engine	Web+IR	-34.877908431136206	-52.89439722899781	149738
624617ae398e62680d6848fa7a07829bedcc3170	mathematical model of time-effective information retrieval system based on the theory of fuzzy sets	fuzzy set;information retrieval system;mathematical model	Abstract   Search patterns of documents and information requests are their better or worse representatives only, so it is important to carry on examinations on possibilities of designing self-learning information retrieval systems. Another important question is to elaborate such an organization of document search pattern set as to obtain an acceptable response time of the information system to a given information request.  A self-learning process of the proposed information system consists in the determination—on a set of document and information request search patterns—of the similarity relation according to L. A. Zadeh.  The organization of a set of document search patterns proposed in the paper ensures the limitation of document search pattern set searching process—when retrieving a response to a given information request—to one (or several) subset from previously determined subsets. This makes the information system response time acceptable. The proposed information retrieval strategy is discussed in terms of fuzzy sets.	fuzzy set;information retrieval;mathematical model	Tadeusz Radecki	1977	Inf. Process. Manage.	10.1016/0306-4573(77)90044-9	document retrieval;relevance;document clustering;cognitive models of information retrieval;computer science;information integration;theoretical computer science;information filtering system;concept search;mathematical model;data mining;fuzzy set;vector space model;information retrieval;search engine;divergence-from-randomness model	AI	-33.72550280185766	-59.71435381746885	150863
5531e4207947e07fb8deceaf33d9e11c5d5fa5c4	improving xed for extracting content from arabic pdfs	physical structure;document model;xed;xml;xml document;arabic pdf documents;ocd;reverse engineering	PDF documents are widely used but the extraction and the manipulation and of their structured content is not an easy task. It requires sophisticated pre-processing and reverse engineering techniques to get such achievements. In this paper, we present an improvement of XED in order to handle unresolved issues related to the analysis of Arabic documents. A set of rules were proposed and implemented to enhance the extraction of Arabic content, by taking care of the different Arabic fonts, through mapping the un-interpreted Unicode values to the other interpreted sets as well as applying a reverse algorithm whenever needed. We finally expose concrete evaluations for the improvement of XED.	algorithm;care-of address;portable document format;preprocessor;reverse engineering;structured content;unicode	Karim Hadjar;Rolf Ingold	2010		10.1145/1815330.1815378	xml;speech recognition;computer science;data mining;programming language;information retrieval	NLP	-34.154572810520015	-65.28979499433808	151019
cbd36ed581f3124f493e1c17ec8183b139572f83	context effects in subjective mental workload ratings	workload;carga mental;mental load;ergonomia;mental workload;hombre;ergonomie;task difficulty;dificultad tarea;evaluation subjective;difficulte tâche;contexto;human;charge travail;contexte;carga trabajo;subjective evaluation;charge mentale;ergonomics;context;homme;evaluacion subjetiva;context effect	The impact of performance context on subjective mental workload ratings was assessed with the Subjective Workload Assessment Technique (SWAT) and the NASA Task Load Index (TLX). In Experiment 1, a strong context effect was demonstrated. A low range of task difficulty produced considerably higher ratings on a common set of difficulty levels than did a high range of task difficulty. In Experiment 2, increasing the participants' range of experiences during practice eliminated the context effect. We recommend that methods for standardizing context, such as providing experience with the complete difficulty range, be developed for subjective mental workload evaluations. Actual or potential applications of this research include providing methodologies for controlling context effects in practical assessments of mental workload to increase the validity of subjective measures.		Herbert A. Colle;Gary B. Reid	1998	Human factors	10.1518/001872098779649283	psychology;cognitive psychology;simulation;context effect;human factors and ergonomics;social psychology;mechanical engineering	HCI	-45.7769746991858	-55.10395800777932	151171
adf240e50483a6d9d899cffe9d7911ab5770d648	behavioral sequences: a new log-coding scheme for effective prediction of web user accesses	site web;prediccion;navegacion informacion;red www;navigation information;reseau web;information browsing;customization;personnalisation;systeme adaptatif;codificacion;fichier log;internet;comportement utilisateur;coding;adaptive system;personalizacion;sistema adaptativo;world wide web;user behavior;sitio web;prediction;comportamiento usuario;web site;log file;adaptive web site;codage	Mining web site logs for predicting user actions is a central issue in the field of adaptive web site development. In order to match the dynamic nature of today web sites we propose in this paper a new scheme for coding Web server log data into sessions of behavioral sequences. Following the proposed coding scheme the navigation sessions are coded as a sequence of hypothetical actions that may explain the transition from one page to another. The output of a prediction algorithm will now be an action that can be evaluated in the context of the current navigation in order to find pages that to be visited by the user.		Rushed Kanawati;Maria Malek	2002		10.1007/3-540-47952-X_48	web service;the internet;simulation;prediction;computer science;adaptive system;operating system;web log analysis software;database;multimedia;coding;world wide web;web server;statistics	Crypto	-37.8427734386581	-57.33236034047719	151701
7baf303531ad2c22fd4ce54f31e10dfe8d4925b8	citation analysis as a literature search method for systematic reviews	information retrieval;bibliometrics	Systematic reviews are essential for evaluating biomedical treatment options, but the growing size and complexity of the available biomedical literature combined with the rigor of the systematic review method mean that systematic reviews are extremely difficult and labor-intensive to perform. In this article, I propose a method of searching the literature by systematically mining the various types of citation relationships between articles. I then test the method by comparing its precision and recall to that of 14 published systematic reviews. The method successfully retrieved 74% of the studies included in these reviews and 90% of the studies it could reasonably be expected to retrieve. The method also retrieved fewer than half of the total number of publications retrieved by these reviews and can be performed in substantially less time. This suggests that the proposed method offers a promising complement to traditional text-based methods of literature identification and retrieval for systematic reviews.		Christopher W. Belter	2016	JASIST	10.1002/asi.23605	bibliometrics;computer science;data science;data mining;information retrieval	NLP	-39.39167020425208	-64.088025307756	151835
572fd77c34925ab998a6f2f9c92b86d3a99c22aa	the mind behind crowdfunding: an empirical study of speech emotion in fundraising success		In online crowdfunding, individuals gather information from two primary sources, video pitches and text narratives. However, while the attributes of the attached video may have substantial effects on fundraising, previous literature has largely neglected effects of the video information. Therefore, this study focuses on speech information embedded in videos. Employing the machine learning techniques including speech recognition and linguistic style classifications, we examine the role of speech emotion and speech style in crowdfunding success, compared to that of text narratives. Using Kickstarter dataset in 2016, our preliminary results suggest that speech information –the linguistic styles– is significantly associated with the crowdfunding success, even after controlling for text and other project-specific information. More interestingly, linguistic styles of the speech have a more profound explanatory power than text narratives do. This study contributes to the growing body of crowdfunding research by providing the unexplored aspect of retrieving speech information from the video.	crowdfunding;embedded system;machine learning;primary source;speech recognition	Jongho Kim;Daegon Cho;Byungtae Lee	2016			marketing;empirical research;computer science;public relations	HCI	-42.82723810481169	-58.98185754922614	151848
0d1c9ac6021d814285f78ae513712325ae8b5fb4	towards a taxonomy of error-handling strategies in recognition-based multi-modal human-computer interfaces	interfaz multimodal;interfase usuario;multimodal interface;articulo sintesis;modelo 3 dimensiones;three dimensions;user interface;article synthese;man machine dialogue;modele 3 dimensions;conversacion;three dimensional model;prevention;recognition based technology;human machine interface;feature extraction;robustesse;taxonomy;error handling;conversation;pattern recognition;dialogo hombre maquina;multi modal interfaces;robustness;interface utilisateur;difference set;reconnaissance forme;extraction caracteristique;reconocimiento patron;review;interaction design;traitement erreur;interface multimodale;human computer interface;robustez;dialogue homme machine;prevencion;interaction robustness	In this paper, we survey the different types of error-handling strategies that have been described in the literature on recognition-based human–computer interfaces. A wide range of strategies can be found in spoken human–machine dialogues, handwriting systems, and multi-modal natural interfaces. We then propose a taxonomy for classifying errorhandling strategies that has the following three dimensions: the main actor in the error-handling process (machine versus user), the purpose of the strategy (error prevention, discovery, or correction), and the use of different modalities of interaction. The requirements that different error-handling strategies have on different sets of interaction modalities are also discussed. The main aim of this work is to establish a classification that can serve as a tool for understanding how to develop more efficient and more robust multi-modal human–machine interfaces. r 2006 Elsevier B.V. All rights reserved.	cognitive dimensions of notations;command-line interface;context-aware pervasive systems;error detection and correction;exception handling;interaction design;modal logic;multimodal interaction;rapid refresh;requirement;signal processing;taxonomy (general);usability;user interface	Marie-Luce Bourguet	2006	Signal Processing	10.1016/j.sigpro.2006.02.047	human–machine interface;exception handling;three-dimensional space;speech recognition;feature extraction;computer science;artificial intelligence;interaction design;user interface;difference set;taxonomy;robustness	HCI	-44.67922510532993	-55.58277493641833	151925
90648a8a71763ae94401b6bdb6d8fa92e2282e05	new graphics as computerized displays for human information processing	human information processing;interfase usuario;financial data processing;affichage graphique;procesamiento informacion;user interfaces decision support systems financial data processing;decision aid;user interface;estudio comparativo;relacion hombre maquina;computerised decision support systems dss user interfaces financial statistics graphic representation bar graphics star graphics computerized displays human information processing tabular representations interactive decision setting man machine communication cognitive style;man machine relation;ayuda decision;graphic display;systeme conversationnel;etude comparative;interactive system;decision support systems;information processing;comparative study;sistema conversacional;aide decision;visualizacion grafica;interface utilisateur;relation homme machine;traitement information;user interfaces;computer displays computer graphics humans information processing management information systems cathode ray tubes man machine systems environmental management demography system testing	Altmcf -Most compul(crized deeiion support systems use both graphid u*i tabular displays RD improve man-machine interaction. Much work has been done on c o m r i n g the effectiveness bar and line charts to tables in these interactive systeinq but little has fd on alternative graphics that have been &scribed in (he statistical literature for many years. A comparative sludy of different graphic and tabular representations of financial and accounting ststistia in an interactive decision setting Is -led. Contmh are placed on demographic vnriables, cognitive style, abilities, .crl math and awnputer anxiety. Bar graphics are found to be a faster but more inaccurrite fom of man-machine communlcation than tabulv presentation d infomation in credit rating and industry classification decisions. Star grrplhics take more time on the part of the user but improve the ability to mike decisions which involve ranking alternatives. Persons whose cognitive style is more directed toward thinking than	chart;graphics;information processing;ruby document format;table (information);xerox star	Marion G. Sobol;Gary Klein	1989	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.35357	simulation;human–computer interaction;information processing;computer science;artificial intelligence;machine learning;multimedia;user interface	HCI	-44.878130993285204	-55.70981689750448	152495
c4b18f0910029466c81472f77cf3621592a68653	term necessity prediction	relevance model;inverse document frequency;supervised learning;necessity;term weighting;retrieval model;document retrieval;prediction model;mismatch;ad hoc retrieval models	The probability that a term appears in relevant documents (P(t | R)) is a fundamental quantity in several probabilistic retrieval models, however it is difficult to estimate without relevance judgments or a relevance model. We call this value term necessity because it measures the percentage of relevant documents retrieved by the term - how necessary a term's occurrence is to document relevance. Prior research typically either set this probability to a constant, or estimated it based on the term's inverse document frequency, neither of which was very effective.  This paper identifies several factors that affect term necessity, for example, a term's topic centrality, synonymy and abstractness. It develops term- and query-dependent features for each factor that enable supervised learning of a predictive model of term necessity from training data. Experiments with two popular retrieval models and 6 standard datasets demonstrate that using predicted term necessity estimates as user term weights of the original query terms leads to significant improvements in retrieval accuracy.	centrality;experiment;predictive modelling;probabilistic database;relevance;supervised learning;tf–idf	Le Zhao;James P. Callan	2010		10.1145/1871437.1871474	document retrieval;computer science;machine learning;data mining;predictive modelling;supervised learning;term discrimination;tf–idf;vector space model;information retrieval	Web+IR	-37.9196597719291	-62.62802494769989	152501
6cbdfe3da495e8bc4c8169bebef39bb8834d4aff	quantifying the benefits of using an interactive decision support tool for creating musical accompaniment in a particular style		We present a human-centered experiment designed to measure the degree of support for creating musical accompaniment provided by an interactive composition decision-support system. We create an interactive system with visual and audio cues to assist users in the choosing of chords to craft an accompaniment in a desired style. We propose general measures for objectively evaluating the effectiveness and usability of such systems. We use melodies of existing songs by Radiohead as tests. Quantitative measures of musical distance – percentage correct and closely related chords, and average neo-Riemannian distance – compare the user-created accompaniment with the original, with and without decision support. Numbers of backward edits, unique chords explored, and repeated chord choices during composition help quantify composition behavior. We present experimental data from musicians and non-musicians. We observe that decision support reduces the time spent in composition, the number of revisions of earlier choices, redundant behavior such as repeated chord choices, and the gap between musicians’ and non-musicians’ work, without significantly limiting the range of users’ choices.	decision support system;interactivity;usability	Ching-Hua Chuan;Elaine Chew	2010			simulation;multimedia;world wide web	HCI	-41.90970481742223	-53.733822481435205	152766
92793df38d9e9fd49d2c219dbc537d0973cfca6d	different strokes of different folks: searching for health narratives in weblogs	web sites health care indexing medical information systems relevance feedback text analysis user interfaces;information retrieval;blogs medical services filtering educational institutions boring internet;text analysis;weblog storytelling utility health narrative search healthcare providers large scale story collection health related inquiry personal weblogs patient populations clinical settings personal story identification indexing english language weblogs user interfaces strokes gender differences relevance feedback;indexing;storytelling;medical information systems;web sites;weblogs;health;information retrieval weblogs storytelling health;relevance feedback;user interfaces;health care	The utility of storytelling in the interaction between healthcare providers and patients is now firmly established, but the potential use of large-scale story collections for health-related inquiry has not yet been explored. In particular, the enormous scale of storytelling in personal web logs offers investigators in health-related fields new opportunities to study the behavior and beliefs of diverse patient populations outside of clinical settings. In this paper we address the technical challenges in identifying personal stories about specific health issues from corpora of millions of web log posts. We describe a novel infrastructure for collecting and indexing the stories posted each day to English-language web logs, coupled with user interfaces designed to support targeted searches of these collections. We evaluate the effectiveness of this search technology in an effort to identify hundreds of first person and third person accounts of strokes, for the purpose of studying gender differences in the way that these health emergencies are described. Results indicate that the use of relevance feedback significantly improves the effectiveness of the search. We conclude with a discussion of sample biases that are inherent in web log storytelling and heightened by our approach, and propose ways to mitigate these biases.	blog;first-person (video games);population;relevance feedback;text corpus;user interface;virtual camera system	Andrew S. Gordon;Christopher Wienberg;Sara Owsley Sood	2012	2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing	10.1109/SocialCom-PASSAT.2012.43	text mining;computer science;data mining;health;multimedia;world wide web;health care	HCI	-33.72650511421906	-53.12389428522284	153957
4500a32214c49b2971b84e84ca60710dc714fc9d	dual-task performance as a measure of mental effort in searching a library system and the web	libraries;dual task;online searching;social sciences;lr opac systems;comparison;ls search engines;information and library science;bi user interfaces usability;world wide web;computer science;usability;information seeking;cb user studies	This paper examines a dual-task method for the assessment of mental effort during online searching, having the users engage in two tasks simultaneously. Searching was assigned as a primary task and a visual observation was set up as a secondary task. The study participants were asked to perform two searches, one on the Web and the other in a webbased library system. Perceived search difficulty and mental effort for searching on the two types of systems were compared through participant self-reports, dual-task performance, and search log analysis. After the searches were completed, the subjects reported that library searching was more difficult to conduct and they had to concentrate more than when Web searching. However, the results of dual-task performance do not reveal much difference in mental effort or concentration during searches in the two systems. Rather, they invested mental effort differently when viewing search results and reading retrieved documents. The findings indicate that a dual-task method provides a useful technique to measure mental effort in online searching, and it has a great potential to be used to measure other aspects of information retrieval such as task complexity and multitasking information behavior.	computer multitasking;information behavior;information retrieval;library classification;log analysis;world wide web	Yong-Mi Kim;Soo Young Rieh	2005		10.1002/meet.14504201155	usability;computer science;online search;multimedia;world wide web;information retrieval	HCI	-36.141199857823	-52.90848401420839	154125
85e38a6247434be3ba246173bc96f15af1c45a66	evaluation of ride comfort of passenger craft	marine vehicles acceleration humans motion measurement motion analysis psychology systems engineering and theory motion estimation frequency cities and towns;questionnaire survey;ships;indexation;subjective evaluation;ergonomics;human psychological response passenger craft passenger ships car ferry ship motion navigation bridge ride comfort index;ships ergonomics	The purpose of this study is to establish a method for evaluating ride quality of passenger ships. The authors have conducted an experiment on board a car ferry running across the Tsugaru-Straits in North Japan. Ship motion was measured at the navigation bridge, and a questionnaire survey was also carried out so as to obtain passengers' subjective evaluation of ride quality. This paper deals with analysis of ship motion and passengers' ride comfort. A 'ride comfort index' RCI was introduced for relating human psychological response with ship motion. To conclude, the RCI combining lateral and vertical motions can be a useful index for estimating ride quality of passenger ships.	lateral thinking	Masakazu Arima;Yuuki Tamura;Masaaki Yoshihira	2006	2006 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2006.384486	questionnaire;human factors and ergonomics;statistics	Robotics	-46.007764734071635	-53.077547822405926	154424
0e0e5a1dfc10e59610280785bd20abedae0498dc	recommending typical usage examples for component retrieval in reuse repositories	source code	Programmers tend to reuse existing components to reduce development cost as well as improve productivity. While retrieving components from the reuse repository, developers often need to know how the components are used in different ways in order to judge which one is more appropriate. An efficient way guiding developers to know how the components are utilized is by leveraging the example code. However, usually the examples provided in handbooks and online documents are not adequate enough. To address this problem, we propose an approach recommending typical usage examples to developers by leveraging source code acquired from the Internet. For each component developers want to utilize, our method first retrieves relevant code downloaded from the Internet as candidate examples. The candidate examples are then clustered and we choose a typical one from each cluster. Finally, the selected ones are ranked and returned to the developers. We implemented our method with a prototype system and conducted an experimental study to evaluate its effectiveness. The experimental results demonstrate that our approach can provide examples to help developers know different usages of the component and thus has the potential to assist developers in reuse.		Yan Li;Liangjie Zhang;Ge Li;Bing Xie;Jiasu Sun	2008		10.1007/978-3-540-68073-4_7	computer science;data mining;database;world wide web	SE	-35.17074952507677	-54.41839452324167	154577
6302d277a942df40f2baa2957a50dbdcd4422ea2	who can replace xavi? a passing motif analysis of football players	cs si;physics soc ph	Traditionally, most of football statistical and media coverage has been focused almost exclusively on goals and (ocassionally) shots. However, most of the duration of a football game is spent away from the boxes, passing the ball around. The way teams pass the ball around is the most characteristic measurement of what a team’s “unique style” is. In the present work we analyse passing sequences at the player level, using the different passing frequencies as a “digital fingerprint” of a player’s style. The resulting numbers provide an adequate feature set which can be used in order to construct a measure of similarity between players. Armed with such a similarity tool, one can try to answer the question: ‘Who might possibly replace Xavi at FC Barcelona?’	fingerprint;motif	Javier López Peña;Raúl Sánchez Navarro	2015	CoRR		simulation;artificial intelligence;mathematics;multimedia	HCI	-44.236876822732796	-58.54987023718056	154588
4801d1c597426c08c6fee904f31175ae533d7412	development and pilot testing of a kneeling ultralight wheelchair design	wheelchairs biomechanics handicapped aids medical disorders paediatrics;wheelchairs prototypes wheels floors protocols manuals;mobility disabilities pilot testing kneeling ultralight wheelchair design on the fly seating adjustments appropriate seat position user participation dynamic wheelchairs dynamic features dynamic wheeled mobility functionality low to the ground tasks floor transfers children playing user evaluation protocol	“Dynamic wheeled mobility” offers “on the fly” seating adjustments for wheelchair users such that various activities performed throughout the day can be matched by an appropriate seat position. While this has benefits for user participation and health, the added weight in existing dynamic wheelchairs may impact the user's ability to transport the frame, e.g. into cars. Other dynamic features to enable more participation avenues are also desirable. This paper outlines the development of a “kneeling” ultralight wheelchair design that offers dynamic wheeled mobility functionality at a weight that is comparable to many existing ultralight wheelchairs. In addition, the wheelchair's kneeling function allows a lowered seat position to facilitate low-to-the-ground tasks such as floor transfers and other activities where sustained low level reaching may be required (e.g. playing with children, changing a tire, etc.). This paper also describes the development and pilot testing of an end user evaluation protocol designed to validate the wheelchair's functionality and performance. Successful realization and commercialization of the technology would offer a novel product choice for people with mobility disabilities, and that may support daily activities, health, improved quality of life, and greater participation in the community.	ability to kneel question;kneeling chair;on the fly;outlines (document);benefit;wheelchair	Johanne L. Mattie;Danny Leland;Jaimie F. Borisoff	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7319520	simulation;engineering;physical therapy;forensic engineering	HCI	-44.845585478053195	-52.82740928867939	154716
f913a0074e84af9be27e709305d4c3bb6eff5de7	mining top - k ranked webpages using simulated annealing and genetic algorithms	parallelisme;search engine;buscador;systeme documentaire;red www;localization;computer science information science engineering;reseau web;localizacion;simulated annealing;algoritmo genetico;data mining;parallelism;recuit simule;recherche documentaire;localisation;internet;paralelismo;fouille donnee;busqueda documental;sistema recuperacion documental;document retrieval system;algorithme genetique;world wide web;genetic algorithm;recocido simulado;document retrieval;moteur recherche;busca dato;parallel simulation	Searching on the Internet has grown in importance over the last few years, as huge amount of information is invariably accumulated on the Web. The problem involves locating the desired information and corresponding URLs on the WWW. With billions of webpages in existence today, it is important to develop efficient means of locating the relevant webpages on a given topic. A single topic may have thousands of relevant pages of varying popularity. Top k document retrieval systems identifies the top k ranked webpages pertaining to a given topic. In this paper, we propose an efficient top-k document retrieval method (TkRSAGA), that works on the existing search engines using the combination of Simulated Annealing and Genetic Algorithms. The Simulated Annealing is used as an optimized search technique in locating the top-k relevant webpages, while Genetic Algorithms helps in faster convergence via parallelism. Simulations were conducted on real datasets and the results indicate that TkRSAGA outperforms the existing algorithms.	computer simulation;document retrieval;genetic algorithm;internet;iteration;parallel computing;search engine optimization;simulated annealing;software release life cycle;www;web page;web search engine;world wide web	P. Deepa Shenoy;K. G. Srinivasa;Achint Oommen Thomas;K. R. Venugopal;Lalit M. Patnaik	2004		10.1007/978-3-540-30176-9_18	document retrieval;the internet;genetic algorithm;internationalization and localization;simulated annealing;computer science;artificial intelligence;machine learning;data mining;database;world wide web;information retrieval;search engine	Web+IR	-35.489429909146715	-57.88576987932441	154729
099bafb09c5821e8fc53b0c5f08b2688be39b653	learning feature taxonomies for case indexing	sistema interactivo;raisonnement base sur cas;razonamiento fundado sobre caso;bottom up;asymmetry;systematique;intelligence artificielle;asymetrie;similitude;systeme conversationnel;induccion;sistematica;induction;indexing;interactive system;indexation;decouverte connaissance;similarity;taxonomy;indizacion;asimetria;artificial intelligence;descubrimiento conocimiento;inteligencia artificial;similitud;case based reasoning;knowledge discovery	Taxonomic case retrieval systems significantly outperform standard conversational case retrieval systems. However, their feature taxonomies, which are the principal reason for their superior performance, must be manually developed. This is a laborious and error prone process. In an earlier paper, we proposed a framework for automatically acquiring features and organizing them into taxonomies to reduce the taxonomy acquisition effort. In this paper, we focus on the second part of this framework: automated feature organization. We introduce TAXIND, an algorithm for inducing taxonomies from a given set of features; it implements a step in our FACIT framework for knowledge extraction. TAXIND builds taxonomies using a novel bottom up procedure that operates on a matrix of asymmetric similarity values. We introduce measures for evaluating taxonomy induction performance and use them to evaluate TAXIND’s learning performance on two case bases. We investigate both a knowledge poor and a knowledge rich variant of TAXIND. While both outperform a baseline approach that does not induce taxonomies, there is no significant performance difference between the TAXIND variants. Finally, we discuss how a more comprehensive representation for features should improve measures on TAXIND’s learning and performance tasks.	algorithm;baseline (configuration management);case-based reasoning;cognitive dimensions of notations;machine learning;mathematical induction;organizing (structure);taxonomy (general);top-down and bottom-up design	Kalyan Moy Gupta;David W. Aha;Philip G. Moore	2004		10.1007/978-3-540-28631-8_17	case-based reasoning;search engine indexing;similarity;computer science;artificial intelligence;similitude;machine learning;top-down and bottom-up design;data mining;database;algorithm;asymmetry;taxonomy	AI	-36.52093313142494	-63.809558385360354	154765
62aaab0d71bf4046c0509ae3f5bbade781524c31	modelling a user population for designing information retrieval metrics		Although Average Precision (AP) has been the most widely-used retrieval effectiveness metric since the advent of Text Retrieval Conference (TREC), the general belief among researchers is that it lacks a user model. In light of this, Robertson recently pointed out that AP can be interpreted as a special case of Normalised Cumulative Precision (NCP), computed as an expectation of precision over a population of users who eventually stop at different ranks in a list of retrieved documents. He regards AP as a crude version of NCP, in that the probability distribution of the user’s stopping behaviour is uniform across all relevant documents. In this paper, we generalise NCP further and demonstrate that AP and its graded-relevance version Q-measure are in fact reasonable metrics despite the above uniform probability assumption. From a probabilistic perspective, these metrics emphasise long-tail users who tend to dig deep into the ranked list, and thereby achieve high reliability. We also demonstrate that one of our new metrics, called NCU gu,β=1, maintains high correlation with AP and shows the highest discriminative power, i.e., the proportion of statistically significantly different system pairs given a confidence level, by utilising graded relevance in a novel way. Our experimental results are consistent across	bootstrapping (statistics);computer user satisfaction;eval;experiment;extended precision;hoc (programming language);information retrieval;long tail;paging;relevance;text retrieval conference;utility;zobel network	Tetsuya Sakai;Stephen E. Robertson	2008			human–computer information retrieval;user modeling;probability distribution;information retrieval;probabilistic logic;data mining;relevance (information retrieval);ranking;population;text retrieval conference;computer science	Web+IR	-38.47285108556745	-61.459744684118135	155607
aee7ce62780664336b289ba5c7c5d5708c32a782	building topic maps using a text mining approach	information exploration;document access;text;lien hypertexte;navegacion informacion;norme iso;red www;iso;text mining;enlace hipertexto;navigation information;topic maps;norma iso;reseau web;information browsing;acces document;hyperlink;texte;data mining;iso standard;automatic generation;feasibility;internet;fouille donnee;world wide web;texto;acceso documento;busca dato;practicabilidad;faisabilite;knowledge organization	Topic maps standard (ISO-13250) has been gradually recognized as an emerging standard for information exploration and knowledge organization in the web era. One advantage of topic maps is that they enable a user to navigate and access the documents he wants in an organized manner, rather than browsing through hyperlinks that are generally unstructured and often misleading. Nowadays, the topic maps are generally manually constructed by domain experts or users since the functionality and feasibility of automatically generated topic maps still remain unclear. In this work we propose a semi-automatic scheme to construct topic maps. We first apply a text mining process on a corpus of information resources to identify the topics and discover the relations among these topics. Necessary components of topic maps such as topics, topic types, topic occurrences, and topic associations may be automatically revealed by our method.	hyperlink;knowledge organization;map;semiconductor industry;text corpus;text mining;topic maps	Hsin-Chang Yang;Chung-Hong Lee	2003		10.1007/978-3-540-39592-8_42	topic maps;text mining;the internet;iso image;computer science;artificial intelligence;data mining;database;hyperlink;world wide web;information retrieval	NLP	-35.84722561781122	-59.65509356323486	156316
a693b15b23eadc4549559603daea4a3f286b7f58	quality of color coding in maps for color deficient observers		For a color deficient observer, the quality of a map or other information design may be defined as the ability to extract features. As color is such important conveyor of information, the colors need to appear correct and be perceived in the desired and intended way. As color appearance is affected by the size of the stimuli, the task of discriminate colors may be even more difficult for a color vision deficient observer. In order to investigate the discriminability of the color coding in an official Norwegian map product, we conducted an experiment involving both color deficient and color normal observers. Also, we investigate to what extent the ability to discriminate colors is influenced by size of the visual field. The experiment revealed that the color vision deficient observers made significant more errors than the normal observers, especially when the visual angle was reduced. Introduction It is established that color is undoubted guide to visual attention [1]. Color vision deficiency (CVD) is a condition that affects the ability to distinguish and discriminate colors, and it is believed that about 8% of all males and 0,5 % females is affected by such condition [2, 3]. Studies reviewed in the work of Cole [4] reveal that up to 60% of the users with abnormal color vision reported problems in observational task as reading color coded charts, slides and prints. Further, they are slower and make more errors in search when color is an attribute of the target object or is used to organize the visual display. It is well known that the color appearance is affected by size of the stimuli, which is often referred to as the color size effect. Carter and Silverstein [5] state that a common experience is difficulty discriminating the multiple small-subtense colors on weather, financial, and other maps and charts viewed on print media and conventional-size information displays. Color is a cartographic element that is used both aesthetically and as a conveyor of information and is well documented in literature [6-9]. Color is the cartographic element that is most frequently misused and is often considered as a difficult cartographic element to use, as it easily draws attention away from the data and goals for the map when it is used poorly [8]. Based on the assumption that quality of a map can be based on the ability for a user to extract features, the main goal in this study is to investigate the following research questions:  Are the colors in an official Norwegian map product distinguishable for a color deficient observer?  Is the ability to distinguish or recognize colors more influenced by variation of visual angle for color deficient observers compared to observers with normal color vision? The paper is organized as follows: First, an overview of basic concepts and related work necessary to understand the research topic is given. Then the experimental setup is described, followed by results, discussion and conclusion. Background and related work This section gives an overview of some of the basic concepts and related work which are essential for the understanding of this research topic. An overview on cartography and color coding, color vision deficiencies, color size effect and visual field is provided in the following pages. Maps and cartography In reference maps, hue is used to symbolize different kinds of features like the blue and green for water and land. Color may be used to represent qualities or quantities and often in a combination, like a topographic map will use hue in order to present qualitative data (like water) or classify data (like class of roads) and value to classify and represent quantitative data (like deepness of the water). Bertin [9] states that “Above all, color exercises an undeniable psychological attraction. In contrast to black-and-white it is richer in cerebral stimulation, and in numerous cases where it can appear as a luxury, this luxury nonetheless “pays off”. It captures and holds attention, multiplies the number of readers, assures better retention of the information, and, in short, increases the scope of the message. Color is particularly applicable to graphic messages of a pedagogic nature”. The use of color in maps is most based on conventions and traditions. Some of the conventions are to choose colors that have a similarity to real life objects, like green and blue to represent land and water. Other conventions are to use strong colors to emphasize important objects, like the use of red to represent highways or cities. Standards or specifications exists for thematic maps in different industries like geology [10, 11], hydrographic services and nautical charts [12] and sports like orienteering [13, 14]. There is also a standard from the International Standardization Organization, ISO 19117:2012 Geographic informations Portrayal [15]. This standard handles presentation of geographic data, but the focus is more on symbolization than use of colors. Then there are the national standards or specifications (often adopting on the ISO 19117), like the standard of the Norwegian Mapping Authority [16] and the specification for cartography on digital displays [17]. Color vision and color deficiency Trichromacy is the ability to perceive colors through the three types of cones (L, M and S) that respond to different wavelengths of the visible spectrum. People with full color vision perceive color with all three cones, while people with different color vision anomalies are affected by either a lack of or dysfunction of one or more of the cones. Color vision deficiencies are most often a congenital condition, but may also be caused by medical conditions. Color vision deficiencies are well described in literature, for example in the book by Hansen [2]. As for the classification of the color vision it is categorized according to available cones (monochromatism, dichromatism and anomalous trichromatism) and further categorized by the cones that are missing or dysfunctional:  Protanopia or Protanomaly (missing or dysfunctional L-cone (red)).  Deuteranopia or Deuteranomaly (missing or dysfunctional Mcone (green)).  Tritanopia or Tritanomaly (missing or dysfunctional S-cone (blue)). A missing or dysfunctional cone does not make the observer “color blind”, but the color perception is limited and the observer may have problems perceiving certain colors accurately and may confuse color ranges. Image enhancement methods for color vision deficient observers A typical method for enhancing images to improve their quality for CVD observers [18] is to change the color (re-coloring methods). Specializations of such methods are the daltonization methods, aiming to compensate poor color discrimination by targeting a specific type of CVD. An overview of existing methods and evaluation of these by measuring accuracy and response time in sample-to-match and visual search methods involving CVD are well described in the work of Simon-Liedtke et al. [19]. Other examples of re-coloring methods is the palette method proposed by Green [20] and a version of the palette method applied to maps proposed by Kvitle et al. [21]. Simulations of different color deficiencies and daltonization methods have been applied in order to create better maps for the CVD observer. The ColorBrewer [7, 22] and Colororacle [23], both based on algorithms like those described in Brettel et al. [24] are examples of applications dedicated to maps. One major problem with simulations is that they operate more like a guideline for a designer with normal color vision. The ColorBrewer is evaluated by CVD observers in experiments involving multiple choice map reading questions as described in the work of Gardner [25], but in a limited group of subjects. The color palettes described in ColorBrewer are well suited for thematic mapping such as choropleth maps, but are not directly applicable to the common reference map (which is the aim of our study) because of the limited set of colors. The Ordnance Survey [26] has developed new reference maps for CVD observers, where the maps are actually developed in an iterative way in cooperation with CVD observers. The group of ten CVD observers was recruited among employees, and these users contributed in several phases of the development. These maps have been embraced by both CVD and color normal observers, even though some of the color coding are unconventional. Color size effect and visual angle The visual angle [27] or the object's angular size is the relationship between the viewing distance and the size of the object. Years of research states that color appearance changes according to different sizes of the same color stimulus. An overview of history of this research and a structural model is given in the work of Carter and Silverstein [5]. An attempt to take this effect into account in the CIECAM02 color appearance model is described by Luo and Li [28] and experimental work on variation of size on display are described in Xiao et al. [29]. Exploring the color size effect for CVD observers, like the work of Pokorny and Smith [30], Nagy et al. [31] and Paramai et al. [32] indicate a correlation between stimulus size and performance in color-naming tasks as many dichromats show trichromacy in color matching and name the colors in fair agreement with the normal observers in larger fields of view (≥3°). Also, the experiments [31] revealed large individual difference between the observers and that for some observers the ability to discriminate varied considerably as a function of the stimulus parameters, such as field size, luminance level and method of presentation. As summarized in Pokorny and Smith [30], in large fields dichromats show weak residual trichromatic vision and at higher luminances deuteranopes and protanopes show residual deuteranomaly and protanomaly. Experimental work Based on the goals to test if the colors of a known official map are discriminable for CVD observers, and if color size 	algorithm;angular defect;angularjs;cartography;categorization;color vision;computer simulation;display device;experiment;graph coloring;information design;iteration;nautical chart;palette (computing);real life;recession cone;response time (technology);simon;thematic map;topography	Anne Kristin Kvitle;Marius Pedersen;Peter Nussbaum	2016			computer vision	HCI	-42.360109740837515	-52.67836232091165	156619
64f96c5325462791c3a2bb8b9b8b91a70bce6088	a game-based learning framework for controlling brain-actuated wheelchairs		Paraplegia is a disability caused by impairment in motor or sensory functions of the lower limbs. Most paraplegic subjects use mechanical wheelchairs for their movement, however, patients with reduced upper limb functionality may benefit from the use of motorised, electric wheelchairs. Depending on the patient, learning how to control these wheelchairs can be hard (if at all possible), timeconsuming, demotivating, and to some extent dangerous. This paper proposes a game-based learning framework for training these patients in a safe, virtual environment. Specifically, the framework utilises the Emotiv EPOC EEG headset to enable brain wave control of a virtual electric wheelchair in a realistic virtual world game environment created with the Unity 3D game engine.	epoc (operating system);electroencephalography;emotiv systems;game engine;headset (audio);neural oscillation;unity;virtual reality;virtual world	Rolf-Magnus Hjorungdal;Filippo Sanfilippo;Ottar L. Osen;Adrian Rutle;Robin T. Bye	2016		10.7148/2016-0554	computer science	HCI	-44.282401571149116	-52.92256695307489	156672
d661a09902838e2dbc6a937c42f6fe1a58a8c54e	mina: a sensorimotor robotic orthosis for mobility assistance		While most mobility options for persons with paraplegia or paraparesis employ wheeled solutions, significant adverse health, psychological, and social consequences result from wheelchair confinement. Modern robotic exoskeleton devices for gait assistance and rehabilitation, however, can support legged locomotion systems for those with lower extremity weakness or paralysis. The Florida Institute for Human and Machine Cognition (IHMC) has developed the Mina, a prototype sensorimotor robotic orthosis for mobility assistance that provides mobility capability for paraplegic and paraparetic users. This paper describes the initial concept, design goals, and methods of this wearable overground robotic mobility device, which uses compliant actuation to power the hip and knee joints. Paralyzed users can balance and walk using the device over level terrain with the assistance of forearm crutches employing a quadrupedal gait. We have initiated sensory substitution feedback mechanisms to augment user sensory perception of his or her lower extremities. Using this sensory feedback, we hypothesize that users will ambulate with a more natural, upright gait and will be able to directly control the gait parameters and respond to perturbations. This may allow bipedal (with minimal support) gait in future prototypes.	apache mina;cognition;feedback;integrated development environment;prototype;robot;robotics;sensory substitution;sociotechnical system;wearable computer	Anil K. Raj;Peter D. Neuhaus;Adrien M. Moucheboeuf;Jerryll H. Noorden;David Lecoutre	2011	J. Robotics	10.1155/2011/284352	simulation	Robotics	-44.13074445659952	-53.02260390826819	156723
47f125146d5c53377e384bf8f0a72e94443035f2	ad-hoc mono- and bilingual retrieval experiments at the university of hildesheim	busqueda informacion;optimisation;linguistique;universite;hongrois;optimizacion;hungarian;information retrieval;multilingual retrieval;pertinencia;indexing and retrieval;linguistica;hungaro;indexing;recherche information;pertinence;indexation;indizacion;optimization;university;relevance;information system;multilinguisme;universidad;systeme information;multilingualism;multilinguismo;sistema informacion;linguistics	This paper reports on our participation in CLEF 2005‘s ad-hoc multi-lingual retrieval track. The ad-hoc task introduced Bulgarian and Hungarian as new languages. Our experiments focus on the two new languages. Naturally, no relevance assessments are available for these collections yet. Optimization was mainly based on French data from last year. Based on experience from last year, one of our main objectives was to improve and refine the n-gram-based indexing and retrieval algorithms within our system.	algorithm;experiment;hoc (programming language);n-gram;relevance	René Hackl;Thomas Mandl;Christa Womser-Hacker	2005		10.1007/11878773_3	natural language processing;search engine indexing;speech recognition;relevance;computer science;linguistics;information retrieval;information system	Web+IR	-35.4526293435427	-63.05239944982285	156750
f08d063bf6425e064c8b8ee05072d2567215e709	a pdf document re-finding system with a q&a wizard interface		Abstract Re-finding electronic documents from personal computers is a frequent demand. For a simple re-finding task, people can use many methods to retrieve the target document, such as navigating directly to its folder, searching with desktop search engines, or checking the Recent Files List. When encountering difficult re-finding tasks, people usually cannot rely on attributes exploited by conventional re-finding methods, the re-finding would fail. We propose a new method to support difficult re-finding tasks, by collecting extra new attributes of a document, such as number of pages, number of images, reading frequency, and coverage percentage. If the document is quested later, the collected attributes and experiences are used to filter out it. A question and answer wizard interface is utilized to alleviate cognitive burden when recollecting. Finally, we develop a PDF document re-finding system to evaluate the effectiveness of the method.	desktop computer;personal computer;portable document format;web search engine;wizard (software)	Gang-Li Liu;Baihui Jiang;Ling Feng	2017	Knowl.-Based Syst.	10.1016/j.knosys.2017.05.028	data mining;information retrieval;computer science;autonomous system (internet);desktop search;user interface;design document listing;wizard;cognition	Web+IR	-35.1325241530983	-53.76949326716705	156764
838007b1b36c6c775323bed802b968b18ebbe569	evaluating the impact of variation in automatically generated embodied object descriptions	thesis or dissertation;informatics;computer science	The primary task for any system that aims to automatically generate human-readable output is choice: the input to the system is usually well-specified, but there can be a wide range of options for creating a presentation based on that input. When designing such a system, an important decision is to select which aspects of the output are hard-wired and which allow for dynamic variation. Supporting dynamic choice requires additional representation and processing effort in the system, so it is important to ensure that incorporating variation has a positive effect on the generated output. In this thesis, we concentrate on two types of output generated by a multimodal dialogue system: linguistic descriptions of objects drawn from a database, and conversational facial displays of an embodied talking head. In a series of experiments, we add different types of variation to one of these types of output. The impact of each implementation is then assessed through a user evaluation in which human judges compare outputs generated by the basic version of the system to those generated by the modified version; in some cases, we also use automated metrics to compare the versions of the generated output. This series of implementations and evaluations allows us to address three related issues. First, we explore the circumstances under which users perceive and appreciate variation in generated output. Second, we compare two methods of including variation into the output of a corpus-based generation system. Third, we compare human judgements of output quality to the predictions of a range of automated metrics. The results of the thesis are as follows. The judges generally preferred output that incorporated variation, except for a small number of cases where other aspects of the output obscured it or the variation was not marked. In general, the output of systems that chose the majority option was judged worse than that of systems that chose from a wider range of outputs. However, the results for non-verbal displays were mixed: users mildly preferred agent outputs where the facial displays were generated using stochastic techniques to those where a simple rule was used, but the stochastic facial displays decreased users’ ability to identify contextual tailoring in speech while the rule-based displays did not. Finally, automated metrics based on simple corpus similarity favour generation strategies that do not diverge far from the average corpus examples, which are exactly the strategies that human judges tend to dislike. Automated metrics that measure other properties of the generated output correspond more closely to users’ preferences.		Mary Ellen Foster	2007			human–computer interaction;computer science;theoretical computer science	NLP	-43.82236318848439	-57.74882537151158	157105
cf179d652127c0e598e0147a7941a6144c230217	extraction of semantic text portion related to anchor link	explotacion texto;busqueda informacion;link structure;extraction information;tecnologia electronica telecomunicaciones;fouille web;red www;semantic text portion;information extraction;text mining;information retrieval;reseau web;fouille texte;recherche information;anchor;web mining;world wide web;analisis semantico;tecnologias;grupo a;analyse semantique;extraccion informacion;user experiment;semantic analysis	Recently, semantic text portion (STP) is getting popular in the field of Web mining. STP is a text portion in the original page which is semantically related to the anchor pointing to the target page. STPs may include the facts and the people's opinions about the target pages. STPs can be used for various upper-level applications such as automatic summarization and document categorization. In this paper, we concentrate on extracting STPs. We conduct a survey of STP to see the positions of STPs in original pages and find out HTML tags which can divide STPs from the other text portions in original pages. We then develop a method for extracting STPs based on the result of the survey. The experimental results show that our method achieves high performance. Keyword text mining, web mining, semantic text portion, link structure, anchor, user experiment	automatic summarization;categorization;document classification;experiment;html element;relevance;text mining;universal storage platform;web mining	Bui Quang Hung;Masanori Otsubo;Yoshinori Hijikata;Shogo Nishida	2006	IEICE Transactions	10.1093/ietisy/e89-d.6.1834	web mining;text mining;computer science;data mining;world wide web;information extraction;information retrieval	Web+IR	-35.6805936397999	-59.26187122266187	157214
08df5794a1788cec3b3708813a56c3eee8121bbd	graphic display of information: how do individuals judge one and two variable charts?	affichage graphique;reaccion utilizador;relacion hombre maquina;man machine relation;user reaction;graphic display;modelo;reaction utilisateur;comportement utilisateur;visualizacion grafica;modele;relation homme machine;user behavior;models;comportamiento usuario	"""The aim of this paper is to build models of be haviour when individuals have to assess one and two variable static displays. The charts investigated are the one and two bargraph views and the rectangular one. The assessment is made in relation to the adjective """"large"""", this being a priori affected by the size of all the stimuli Then, the judgment is obtained through a degree of agreement between a view and an assertion. The data are studied through correspondence analysis. The outcomes are large differences between the individuals and a higher variability with the rectangle than with the two bargraph charts"""	chart	P. Loslever;F. Bourlon	1993	J. Information Science	10.1177/016555159301900114	computer science;artificial intelligence;algorithm	HCI	-44.9121504303521	-55.92632640062318	157259
5c792774c99c3a62458225d73f1fd37faf56f131	enhancing knowledge base with knowledge transfer	information sources;text mining;information filtering;transfer network;satisfiability;network analysis;knowledge base enhancement;research methodology;machine learning;transfer learning;knowledge transfer;knowledge representation;knowledge base	A Knowledge Base (KB) stores, organizes, and shares information pertinent to entities (i.e. KB nodes) such as people, organizations, and events. A large KB system, such as Wikipedia, relies on human curators to create and maintain the content in the systems. However, it has become challenging for human curators to sift through the rapidly growing amount of information and filter out the information irrelevant to a KB node. The area of Knowledge Base Enhancement (KBE) aims to explore and identify automatic methods to assist human curators to accelerate the process. KBE can be viewed as a special case of Information Filtering (IF). However, the lack of high-quality labelled data introduces a major challenge to train a satisfying model for the task. Transfer learning provides solutions to the problem and has explored applications in the area of text mining, whereas direct application to KBE or IF remains absent.  Transfer learning is a research area in machine learning, emphasizing the reuse of previously acquired knowledge to another applicable task. The method is particularly useful in the situations where labeled instances are absent or difficult to obtain. To accelerate the growth of a KB, a transfer learning approach enables leveraging the heuristics and models learned from one KB node to another. For example, reusing the learned filtering models from Willie Nelson, a famous country singer, to Eddie Rabbitt, another country singer.  Transfer learning requires three components: the target task (e.g. the problem to be solved), the source task(s) (e.g. auxiliary data, previously studied problem), and criteria to select appropriate source tasks. The objectives of my dissertation are twofold. First, it explores methods to identify informative source nodes from which to transfer. Second, it constructs a knowledge transfer network to represent the transfer learning relationship between KB nodes.  This proposed research applies a transfer learning method -- Segmented Transfer (ST) -- and a knowledge representation -- Knowledge Transfer Network (KTN) -- to approach the area of KBE. The primary research questions include: What are the transferable objects in information filtering algorithms? What are the KB nodes of high transferability? What are the factors that determine the transfer learning relationship? Does it manifest on a knowledge transfer network representation?  This interdisciplinary research crosses the study area of information filtering, machine learning, knowledge representation, and network analysis. This proposal motivates the problem of KBE, discusses the research methodology and proposed experiments, and reviews related works in information filtering and transfer learning. This line of research hopes to extend the application of transfer learning to KBE and to explore a new dimension of IF. The proposed ST and KTN intends to bring interdisciplinary approaches in the emerging field of KBE.	algorithm;eddie (text editor);eddie dombrower;entity;experiment;heuristic;information filtering system;knowledge base;knowledge representation and reasoning;machine learning;network theory;reinforcement learning;relevance;text mining;wikipedia	Si-Chi Chin	2012		10.1145/2348283.2348419	natural language processing;multi-task learning;knowledge base;text mining;network analysis;transfer of learning;computer science;knowledge management;artificial intelligence;machine learning;methodology;data mining;inductive transfer;world wide web;information retrieval;satisfiability	AI	-33.772822368119094	-56.84321637038169	157275
ef2319fa8f5360cc2aa3dc54b5605527764e909a	recommendations supporting situation awareness in partially automated driver assistance systems	situation awareness sa driving simulator human machine interaction partially automated driving;alertness;road safety driver information systems human computer interaction;highway safety;human factors;current measurement;monitoring;intelligent vehicles;vehicles automation wheels safety devices current measurement monitoring;vehicles;steering wheel partially automated driver assistance systems automated driving traffic safety system behavior monitoring human machine interface mechanism evaluation hmi mechanisms driver situation awareness driving safety driving simulator vehicle head up display;automobile drivers;driver support systems;wheels;safety devices;automation	It is believed that automated driving is able to fulfill the demand for comfort and safety in traffic. Recent developments have been able to take over all parts of the driving task in specific situations. However, it remains to the driver to monitor the system's behavior upon errors and to intervene in the case of critical situations. On account of the automation, the driver may not be able to overlook the whole situation in the same way he would do while driving manually. In addition to that, the driver's motivation to perform secondary tasks while being driven enhances these challenges. The research activities presented in this paper are concentrated on the development and the evaluation of certain human-machine interface (HMI) mechanisms. It is hypothesized that such mechanisms have a positive impact on driver's situation awareness (SA) and resulting driving safety (DS). The evaluation of these mechanisms occurred in a driving simulator study. Based on the evaluation results, a system presenting the secondary task's display in the vehicle's head-up display is proposed. Thereby, the driver is expected to be able to keep the vehicle's environment in his peripheral field of view without being distracted too much. At the same time, relevant elements to operate the secondary task should be located on the steering wheel. This is expected to reduce the time needed for steering reactions, as drivers keep at least one hand on the steering wheel. The results suggest that such a system design appears reasonable in order to enhance driver's SA and DS while driving partially automated.	autonomous car;do while loop;driving simulator;head-up display;peripheral;recommender system;simulation;steering wheel;systems design;user interface	Felix Wulf;Maria Rimini-Doring;Marc Arnon;Frank Gauterin	2015	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2014.2376572	embedded system;alertness;simulation;advanced driver assistance systems;computer science;engineering;human factors and ergonomics;automation;automotive engineering	HCI	-46.82983824372991	-52.101651425558146	157286
bf636a69953228962a853b54fc2a3eb6016d6e4f	evaluating user-computer interaction: a framework	human computer interaction;evaluation base utilisateur;interaction homme machine;evaluation design;conception evaluation;article;user based evaluation;human machine interaction	A framework is described, which classifies usability evaluations in terms of three dimensions; the approach to evaluation, the type of evaluation and the time of evaluation in the context of the product life cycle. The approaches described are user-based, theory-based and expert-based. The approach to evaluation reflects the source of the data which forms the basis of the evaluation. The types of evaluation are diagnostic, summative and metrication. These reflect the purpose of the evaluation and therefore the nature of the data and likely use of the results. The time of testing reflects the temporal location in the product life cycle at which the evaluation is conducted. This dictates the representation of the product which is available for evaluation		M. Sweeney;Martin C. Maguire;Brian Shackel	1993	International Journal of Man-Machine Studies	10.1006/imms.1993.1032	simulation;human–computer interaction;computer science	Arch	-44.86075832805449	-55.97704049730413	157381
414f628e41f6f9a53ac2e4634d91a95d41e7e2e1	neighbor selection and weighting in user-based collaborative filtering: a performance prediction approach	trust;neighbor weighting;performance prediction;neighbor selection;user based collaborative filtering;article;recommender systems	User-based collaborative filtering systems suggest interesting items to a user relying on similar-minded people called neighbors. The selection and weighting of these neighbors characterize the different recommendation approaches. While standard strategies perform a neighbor selection based on user similarities, trust-aware recommendation algorithms rely on other aspects indicative of user trust and reliability. In this article we restate the trust-aware recommendation problem, generalizing it in terms of performance prediction techniques, whose goal is to predict the performance of an information retrieval system in response to a particular query. We investigate how to adopt the preceding generalization to define a unified framework where we conduct an objective analysis of the effectiveness (predictive power) of neighbor scoring functions. The proposed framework enables discriminating whether recommendation performance improvements are caused by the used neighbor scoring functions or by the ways these functions are used in the recommendation computation. We evaluated our approach with several state-of-the-art and novel neighbor scoring functions on three publicly available datasets. By empirically comparing four neighbor quality metrics and thirteen performance predictors, we found strong predictive power for some of the predictors with respect to certain metrics. This result was then validated by checking the final performance of recommendation strategies where predictors are used for selecting and/or weighting user neighbors. As a result, we have found that, by measuring the predictive power of neighbor performance predictors, we are able to anticipate which predictors are going to perform better in neighbor-scoring-powered versions of a user-based collaborative filtering algorithm.	algorithm;collaborative filtering;computation;information retrieval;performance prediction;scoring functions for docking;unified framework	Alejandro Bellogín;Pablo Castells;Iván Cantador	2014	TWEB	10.1145/2579993	computer science;collaborative filtering;machine learning;data mining;trustworthy computing;world wide web;computer security;information retrieval;recommender system	Web+IR	-35.53706186679225	-55.66310904101439	157618
341263b486c7990533ed9ed73f4cfed6fe16f2ec	term dependence statistical measures for information retrieval tasks		In the information retrieval (IR) research community, it is commonly accepted that independence assumptions in probabilistic IR models are inaccurate. The need for modeling term dependencies has been stressed in the literature. However, little or nothing has been said on the statistical nature of these dependencies. We investigate statistical measures of term-to-query and document term-to-term pairs dependence, using several test collections. We show that document entropy is highly correlated to dependence, but that high ratios of linearly uncorrelated pairs, do not necessarily mean independent pairs. A robust IR model should then consider both dependence and independence phenomena.	information retrieval	Francis C. Fernández-Reyes;Jorge Hermosillo Valadez;Yasel Garcés Suárez	2015		10.1007/978-3-319-27060-9_7	machine learning;pattern recognition;term discrimination;information retrieval	Vision	-38.047924794074326	-62.65539414344387	157684
893e5902c18b3dcaf19d6eb2e0fedd97de175271	revealing patterns of twitter emoji usage in barcelona and madrid		Emojis are small sized images which are naturally combined with free text to visually complement or condense the meaning of a message. The set of available emojis is fixed, irrespective of a user’s location. However, their interpretation and the way they are used may vary. In this paper, we compare the meaning and usage of emojis across two Spanish cities: Barcelona and Madrid. Our results suggest that the overall semantics of the subset of emojis we studied is preserved over these cities. However, some of them are interpreted differently, which suggests there may exist cultural differences between inhabitants of Barcelona and Madrid, and that these are reflected in how they communicate in social networks.	emoji;social network	Francesco Barbieri;Luis Espinosa Anke;Horacio Saggion	2016		10.3233/978-1-61499-696-5-239	emoji;cartography;catalan;geography	NLP	-41.37525988217455	-57.86118275221674	157698
658fd4836f8f6dc00d7d0243da518d03ecc31068	a study on tolerable waiting time: how long are web users willing to wait?	busqueda informacion;etude utilisateur;tiempo respuesta;system response;tiempo espera;interfase usuario;web pages;red www;selected works;user interface;information retrieval;reponse systeme;user study;reseau web;estudio usuario;response time;user reaction;temps attente;temps reponse;reaction utilisateur;recherche information;tolerancia;waiting time;world wide web;bepress;interface utilisateur;tolerance;respuesta sistema;reaccion usuario	The WWW has become an important channel for information access, electronic commerce, and publication. With an exponential increase in the number of Web users and the popularity of multimedia technology, users often face a long waiting time for downloading Web pages. Although various technologies and techniques have been implemented to alleviate the situation and to comfort the impatient users, little research has been done to assess what constitutes an acceptable and tolerable waiting time for Web users. This research reviews the literature on computer response time and users’ waiting time, and assesses Web users’ tolerable waiting time in information retrieval. It addresses the following question through an empirical study: How long are users willing to wait for a Web page to be downloaded before abandoning it? The results from this study suggest that the tolerable waiting time for information retrieval is approximately 2 seconds.	download;e-commerce;information access;information retrieval;response time (technology);time complexity;www;web page	Fiona Fui-Hoon Nah	2003	Behaviour & IT	10.1080/01449290410001669914	telecommunications;computer science;web page;user interface;world wide web;response time	Web+IR	-37.40758555143004	-56.46926847804626	157726
a15201e810f3bc897a444d272c87b4b9149c3084	subjective probability and information retrieval: a review of the psychological literature	indizador;probability;indexer;cognitive psychology;statistical bias;information retrieval;subjective probability;usuario;utilisateur;psychology;literature reviews;recherche information;probabilidad;probabilite;psychologie;user;recuperacion informacion;relevance information retrieval;psicologia;indexeur;evaluative thinking	The psychological literature on subjective probability estimation is reviewed to determine the feasibility of designing probabilistic information retrieval systems using such estimates. Their use has been considered by some writers, but psychological issues have not been addressed. Research pertinent to probabilistic information retrieval is examined and implications for probabilistic information retrieval are drawn. It is shown that accurate human probability estimation is possible, both in the laboratory and in real world tasks, e.g., in meteorological forecasting; but that it is also a task subject to systematic bias, or inaccuracy. Proposed techniques for debiasing are considered. The highly task‐dependent nature of such estimates is also discussed; two implications are that results from laboratory studies may have limited relevance to real world tasks and that empirical studies specific to the context of information retrieval need to be made. Human probability estimation appears to be a difficult tas...	information retrieval	Paul Thompson	1988	Journal of Documentation	10.1108/eb026821	user;indexer;social science;relevance;cognitive models of information retrieval;epistemology;computer science;artificial intelligence;probability;data mining;social psychology;world wide web;information retrieval;statistics	Vision	-38.00349476616307	-61.38683006206915	157893
4b5aa9cc397f7f1313ca1f25f37bd2774e4a15ed	ranganathan and the net: using facet analysis to search and organise the world wide web	concept;search engine;red www;faceted classification;classification facette;index words;information retrieval;classification;information organization;repertoire;organizacion informacion;motor investigacion;indexing;recherche information;indexation;indizacion;organisation information;world wide web;directory;repertorio;reseau www;recuperacion informacion;moteur recherche;clasificacion facetada;concepto	This paper documents the continuing relevance of facet analysis as a technique for searching and organising World Wide Web based materials. The two approaches underlying WWW searching and indexing – word and concept based indexing – are outlined. It is argued that facet analysis as an a posteori approach to classification using words from the subject field as the concept terms in the classification derived represents an excellent approach to searching and organising the results of WWW searches using either search engines or search directories. Finally, it is argued that the underlying philosophy of facet analysis is better suited to the disparate nature of WWW resources and searchers than the assumptions of contemporary IR research.	faceted classification;world wide web	David Ellis;Ana Cristina Vasconcelos	1999	Aslib Proceedings	10.1108/EUM0000000006956	search engine indexing;biological classification;computer science;data mining;concept;world wide web;information retrieval;search engine	ML	-34.586245787765776	-60.456898591642776	157910
89503e44dd462b419c444c382d63436090138742	intelligent information access systems (sinai) at clef 2001: calculating translation probabilities with semcor	linguistique;systeme intelligent;information retrieval;sistema inteligente;information access;linguistica;anglais;recherche information;spanish;intelligent system;acces information;acceso informacion;english;recuperacion informacion;information system;espagnol;multilinguisme;ingles;systeme information;multilingualism;multilinguismo;sistema informacion;espanol;probabilite translation;linguistics	This work aims to present an approach for the retrieval of bilingual Spanish-English information based on EuroWordNet and basing itself on another linguistic source such as SemCor, the latter used to calculate the translation probability in words that share same meaning in EuroWordNet. It is, therefore, about evaluating the linguistic aid SemCor, of long-standing tradition in IR tasks, in a bilingual ambience.	cross-language information retrieval;eurowordnet;information access;parallel text;while	Fernando Martínez Santiago;Luis Alfonso Ureña López;Manuel Carlos Díaz-Galiano;Manuel García Vega;María Teresa Martín-Valdivia	2001		10.1007/3-540-45691-0_16	natural language processing;speech recognition;computer science;artificial intelligence;english;linguistics;information system;spanish	NLP	-35.482620254827765	-63.32713306221222	157923
fc6f40e4a0bf87497d2ffa0b015780c65f734c22	a framework for mining evolving trends in web data streams using dynamic learning and retrospective validation	busqueda informacion;estensibilidad;tratamiento datos;evaluation performance;performance evaluation;learning;text mining;information retrieval;reconfigurable architectures;noisy data;data stream;computer model;evaluacion prestacion;simulation;data processing;simulacion;traitement donnee;high precision;data mining;similitude;mining evolving data streams;aprendizaje;data clustering;dynamic environment;user profile;apprentissage;fouille donnee;recherche information;precision elevee;signal classification;poursuite cible;similarity;precision elevada;classification signal;web mining;user profiles;extensibilite;scalability;similitud;classification automatique;target tracking;automatic classification;clasificacion automatica;similarity measure;busca dato;web clickstreams;architecture reconfigurable	The expanding and dynamic nature of the Web poses enormous challenges to most data mining techniques that try to extract patterns from Web data, such as Web usage and Web content. While scalable data mining methods are expected to cope with the size challenge, coping with evolving trends in noisy data in a continuous fashion, and without any unnecessary stoppages and reconfigurations is still an open challenge. This dynamic and single pass setting can be cast within the framework of mining evolving data streams. The harsh restrictions imposed by the ‘‘you only get to see it once’’ constraint on stream data calls for different computational models that may furthermore bring some interesting surprises when it comes to the behavior of some well known similarity measures during clustering, and even validation. In this paper, we study the effect of similarity measures on the mining process and on the interpretation of the mined patterns in the harsh single pass requirement scenario. We propose a simple similarity measure that has the advantage of explicitly coupling the precision and coverage criteria to the early learning stages. Even though the cosine similarity, and its close relative such as the Jaccard measure, have been prevalent in the majority of Web data clustering approaches, they may fail to explicitly seek profiles that achieve high coverage and high precision simultaneously. We also formulate a validation strategy and adapt several metrics rooted in information retrieval to the challenging task of validating a learned stream synopsis in dynamic environments. Our experiments confirm that the performance of the MinPC similarity is generally better than the cosine similarity, and that this outperformance can be expected to be more pronounced for data sets that are more challenging in terms of the amount of noise and/or overlap, and in terms of the level of change in the underlying profiles/topics (known sub-categories of the input data) as the input stream unravels. In our simulations, we study the task of mining and tracking trends and profiles in evolving text and Web usage data streams in a single pass, and under different trend sequencing scenarios. 2005 Elsevier B.V. All rights reserved.	amd firestream;algorithm;ant colony;cluster analysis;computational model;cosine similarity;dspace;data mining;data point;emergence;emoticon;experiment;flocking (behavior);geodetic datum;gradient;information retrieval;jaccard index;k-means clustering;kinetic data structure;knowledge base;logistics;mathematical optimization;mined;personalization;real-time locating system;recommender system;requirement;streams;scalability;self-similarity;sensitivity and specificity;signal-to-noise ratio;similarity measure;simulation;stream (computing);streaming media;throughput;unsupervised learning;usage data;user profile;video synopsis;web application;web content;world wide web;zero suppression	Olfa Nasraoui;Carlos Rojas;Cesar Cardona	2006	Computer Networks	10.1016/j.comnet.2005.10.021	web mining;text mining;scalability;similarity;data processing;computer science;similitude;operating system;data mining;database;data stream mining;cluster analysis;world wide web;computer security;statistics	ML	-35.26387938576676	-57.15632471651747	158095
5522f7d9b11017190fb7e7a0552b05b445206e96	a diagnostic study of search result diversification methods	perturbation;diversification;diagnostic	Search result diversification aims to maximize the coverage of different pieces of relevant information in the search results. Many diversification methods have been proposed and studied. However, the advantage and disadvantage of each method still remain unclear. In this paper, we conduct a diagnostic study over two state of the art diversification methods with the goal of identifying the weaknesses of these methods to further improve the performance. Specifically, we design a set of perturbation tests that isolate individual factors, i.e., relevance and diversity, which affect the diversification performance. The test results are expected to provide insights on how well each method deals with these factors in the diversification process. Experimental results suggest that some methods perform better in queries whose originally retrieved documents are more relevant to the query while other methods perform better when the documents are more diversified. We therefore propose methods to combine these existing methods based on the predicted factor of the query. The experimental results show that the combined methods can outperform individual methods on TREC collections.	diversification (finance);information retrieval;java collections framework;relevance	Wei Zheng;Hui Fang	2013		10.1145/2499178.2499190	engineering;marketing;operations management;data mining	Web+IR	-34.06562440064853	-57.91037259654765	158244
fb994770e8acbdf78431e0e94bcc621e0ffb8a75	generation and evaluation of indexes for chemistry articles	document analysis;user interface;indexation;automatic indexing	This paper describes AIMS (Assisted Indexing atMississippi State), a system that aids human document analystsin the assignment of indexes to physical chemistry journalarticles. There are two major components of AIMS—a naturallanguage processing (NLP) component and an index generation (IG)component. The focus of this article is the IG. We describethe techniques and structures used by the IG in the selection ofappropriate indexes for a given article. We also describe theresults of evaluations of the system in terms of recall,precision, and overgeneration. We provide a description of agraphical user interface that we have developed for AIMS.Finally, we discuss future work.	natural language processing;user interface	Julia E. Hodges;Shiyun Yie;Sonal Kulkarni;Ray Reighart	1997	Journal of Intelligent Information Systems	10.1023/A:1008653600557	computer science;data mining;database;user interface;world wide web;information retrieval	NLP	-34.351876143091374	-62.068360840702695	158295
8294ce9669a7bb888c04b3f8c9e00631cfb945dd	web pre-fetching using adaptive weight hybrid-order markov model	modelizacion;metodo adaptativo;optimisation;web navigation;navegacion informacion;red www;modelo markov;optimizacion;navigation information;modelo hibrido;reseau web;information browsing;methode adaptative;modele hybride;hybrid model;modelisation;markov model;internet;adaptive method;comportement utilisateur;prediction accuracy;world wide web;optimization;user behavior;information system;modele markov;modeling;systeme information;comportamiento usuario;sistema informacion	Markov models have been widely utilized for modeling user web navigation behavior. In this paper, we propose a novel adaptive weighting hybrid-order Markov model - HFTMM for Web pre-fetching based on optimizing HTMM (hybrid-order tree-like Markov model). The model can minimize the number of nodes in HTMM and improve the prediction accuracy, which are two significant sources of overhead for web pre-fetching. The experimental results show that HFTMM excels HTMM in better predicting performance with fewer nodes.	markov chain;markov model	Shengping He;Qin Zheng	2004		10.1007/978-3-540-30480-7_32	maximum-entropy markov model;the internet;simulation;systems modeling;computer science;artificial intelligence;web navigation;markov model;world wide web;information system;hidden markov model;variable-order markov model	Theory	-37.060878173539784	-57.84852630438943	158369
e67b463fdd0065743f554ec66a6ae9179caf088c	relevance dimensions in preference-based ir evaluation	diversity;user preferences;relevance criteria	Evaluation of information retrieval (IR) systems has recently been exploring the use of preference judgments over two search result lists. Unlike the traditional method of collecting relevance labels per single result, this method allows to consider the interaction between search results as part of the judging criteria. For example, one result list may be preferred over another if it has a more diverse set of relevant results, covering a wider range of user intents. In this paper, we investigate how assessors determine their preference for one list of results over another with the aim to understand the role of various relevance dimensions in preference-based evaluation. We run a series of experiments and collect preference judgments over different relevance dimensions in side-by-side comparisons of two search result lists, as well as relevance judgments for the individual documents. Our analysis of the collected judgments reveals that preference judgments combine multiple dimensions of relevance that go beyond the traditional notion of relevance centered on topicality. Measuring performance based on single document judgments and NDCG aligns well with topicality based preferences, but shows misalignment with judges' overall preferences, largely due to the diversity dimension. As a judging method, dimensional preference judging is found to lead to improved judgment quality.	experiment;information retrieval;relevance	Jinyoung Kim;Gabriella Kazai;Imed Zitouni	2013		10.1145/2484028.2484168	data mining;information retrieval	Web+IR	-33.85164871528228	-55.03858128012303	158372
095d09f27d610ba54e50791ce6ea477ce36515f5	nano language and distribution of article title terms according to power laws	compound words;search strategy;power laws;nanoscience;bibliometrics;terminology;subject categories;lexical analysis	Scientometric evaluation of nanoscience/nanotechnology requires complex search strategies and lengthy queries which retrieve massive amount of information. In order to offer some insight based on the most frequently occurring terms our research focused on a limited amount of data, collected on uniform principles. The prefix nano comes about in many different compound words thus offering a possibility for such assessment. The aim is to identify the scatter of nanoconcepts, among and within journals, as well as more generally, in the Web of Science (WOS). Ten principal journals were identified along with all unique nanoterms in article titles. Such terms occur on average in half of all titles. Terms were thoroughly investigated and mapped by lemmatization or stemming to the appropriate roots—nanoconcepts. The scatter of concepts follows the characteristics of power laws, especially Zipf’s law, exhibiting clear inversely proportional relationship between rank and frequency. The same three nanoconcepts are most frequently occurring in as many as seven journals. Two concepts occupy the first and the second rank in six journals. The same six concepts are the most frequently occurring in ten journals as well as full WOS database, representing almost two thirds of all nanotitled articles, in both instances. Subject categories don’t play a decisive role. Frequency falls progressively, quickly producing a long tail of rare concepts. Drop is almost linear on the log scale. The existence of hundreds of different closed-form compound nanoterms has consequences for the retrieval on the Internet search engines (e.g. Google Scholar) which do not permit truncation.	existential quantification;gnu nano;google scholar;lemmatisation;long tail;nanolanguage;scientometrics;stemming;text corpus;truncation;web of science;web search engine;wildcard character;world wide web;zipf's law	Tomaz Bartol;Karmen Stopar	2015	Scientometrics	10.1007/s11192-015-1546-1	power law;epistemology;lexical analysis;bibliometrics;computer science;artificial intelligence;data mining;mathematics;compound;terminology;world wide web;information retrieval;statistics	Web+IR	-39.2401362783292	-62.64555238952998	158450
040d1c139710745934816148842f5b30e9f22e72	prototypicality gradient and similarity measure: a semiotic-based approach dedicated to ontology personalization	semantic measure;lexical prototypicality;semiotics;gradient;ontology personalization;conceptual prototypicality;similarity measure	This paper introduces a new approach dedicated to the Ontology Personalization. Inspired by works in Cognitive Psychology, our work is based on a process which aims at capturing the user-sensitive relevance of the categorization process, that is the one which is really perceived by the end-user. Practically, this process consists in decorating the Specialization/Generalization links (i.e. the is-a links) of the hierarchy of concepts with 2 gradients. The goal of the first gradient, called Conceptual Prototypicality Gradient, is to capture the user-sensitive relevance of the categorization process, that is the one which is perceived by the end-user. As this gradient is defined according to the three aspects of the semiotic triangle (i.e. intentional, extensional and expressional dimension), we call it Semiotic based Prototypicality Gradient. The objective of the second gradient, called Lexical Prototypicality Gradient, is to capture the user-sensitive relevance of the lexicalization process, i.e. the definition of a set of terms used to denote a concept. These gradients enrich the initial formal semantics of an ontology by adding a pragmatics defined according to a context of use which depends on parameters like culture, educational background and/or emotional context of the end-user. This paper also introduces a new similarity measure also defined in the context of a semiotic-based approach. The first originality of this measure, called SEMIOSEM, is to consider the three semiotic dimensions of the conceptualization underlying an ontology. Thus, SEMIOSEM aims at aggregating and improving existing extensional-based and intentional-based measures. The second originality of this measure is to be context-sensitive, and in particular user-sensitive. This makes SEMIOSEM more flexible, more robust and more close to the end-user’s judgment than the other similarity measures which are usually only based on one aspect of a conceptualization and never take the end-user’s perceptions and purposes into account.	categorization;conceptualization (information science);context-sensitive grammar;context-sensitive language;gradient;is-a;partial template specialization;personalization;relevance;semantics (computer science);semiotics;similarity measure;triangle of reference	Xavier Aimé;Frédéric Fürst;Pascale Kuntz;Francky Trichet	2010	Intelligent Information Management	10.4236/iim.2010.22009	artificial intelligence;mathematics;semiotics;gradient	AI	-40.83866160179387	-57.41828208287087	158623
5162b981335e217e90ecd88929e5d14f6ec6c3da	a comparative assessment of answer quality on four question answering sites	social reference;crowd sourcing;social q a;q a sites;information quality;community question answering;question answering	Question answering (Q&A) sites, where communities of volunteers answer questions, may provide faster, cheaper, and better services than traditional institutions. However, like other Web 2.0 platforms, user-created content raises concerns about information quality. At the same time, Q&A sites may provide answers of different quality because they have different communities and technological platforms. This paper compares answer quality on four Q&A sites: Askville, WikiAnswers, Wikipedia Reference Desk, and Yahoo! Answers. Findings indicate that: 1) the use of similar collaborative processes on these sites results in a wide range of outcomes. Significant differences in answer accuracy, completeness, and verifiability were found; 2) answer multiplication does not always result in better information. Answer multiplication yields more complete and verifiable answers but does not result in higher accuracy levels; and 3) a Q&A site’s popularity does not correlate with its answer quality, on all three measures.	askville;formal verification;information quality;question answering;user-generated content;web 2.0;wikianswers;wikipedia	Pnina Fichman	2011	J. Information Science	10.1177/0165551511415584	question answering;computer science;data mining;information quality;world wide web;information retrieval	Web+IR	-37.701265975552666	-54.813393451297344	158634
6020cd9289fecbeee58717f6a3712d808a5a714a	evaluating the diversification of similarity query results	evaluation methods;space mapping;result diversification;similarity search	"""The data currently generated and collected increase not only in volume, but also in complexity, requiring new query operators to be searched. Similarity queries have been acknowledged as one of the most useful resources to retrieve complex data, but the basic similarity operators are not enough to meet the requirements of the applications, largely because their result sets tend to include many elements too similar to the query center and among themselves. To tackle this problem, variations and extensions of basic operators have been studied pursuing result diversification, i.e, to search for elements sufficiently similar to the query center, but also diverse from each other.  Result diversification has been studied considering either extra information related to the data or the distance among result set elements. The problem with the former approach is that ''extra information''  rarely exists and, even when it does, the corresponding processing cost is commonly too high. Moreover, the distance-based algorithms are often good alternatives even for data domains that can rely on other information, besides the elements and their distances. The main drawback of distance-based algorithms is the lack of evaluation methods to understand how diverse the retrieved answer is. This article reports on the development of several statistical measurements able to evaluate the diversity of the result set. The concept of the """"answer space"""",  has also been created, aimed at highlighting the distribution of the several result sets that can be the answers  to a given similarity-diversified query, which enables the evaluation of the query quality regarding several different criteria. Finally, we describe an extensive set of experiments to validate our proposals and highlight the analysis that could be performed by the system analyst, using four real datasets that span up to  72k  elements and  761  dimensions."""	diversification (finance)	Lucio F. D. Santos;Willian D. Oliveira;Mônica Ribeiro Porto Ferreira;Robson Leonardo Ferreira Cordeiro;Agma J. M. Traina;Caetano Traina	2013	JIDM		web query classification;ranking;theoretical computer science;data mining;mathematics;information retrieval	DB	-33.952298388919885	-55.17270269349989	158977
596597e2ac543933100456668525aeeafd514f2f	building test collections: an interactive tutorial for students and others without their own evaluation conference series	test collections;cranfield paradigm	While existing test collections and evaluation conference efforts may sufficiently support one's research, one can easily find oneself wanting to solve problems no one else is solving yet. But how can research in IR be done (or be published!) without solid data and experiments? Not everyone can talk TREC, CLEF, INEX, or NTCIR into running a track to build a collection.   This tutorial aims to teach how to build a test collection using resources at hand, how to measure the quality of that collection, how to understand its limitations, and how to communicate them. The intended audience is advanced students who find themselves in need of a test collection, or actually in the process of building a test collection, to support their own research. The goal of this tutorial is to lay out issues, procedures, pitfalls, and practical advice.		Ian Soboroff	2013		10.1145/2484028.2484190	computer science;data mining;multimedia;world wide web;information retrieval	Visualization	-40.13113287554835	-56.01688236878724	159118
4f2dad979f3c12a6c4a477943328d7d624a6be31	indexing consistency, quality and efficiency	reliability;information retrieval;mathematical formulas;indexing;evaluation criteria;indexation;tables data;quality control;subject index terms	Indexing quality determines whether the information content of an indexed document is accurately represented. Indexing effectiveness measures whether an indexed document is correctly retrieved every time it is relevant to a query. Measurement of these criteria is cumbersome and costly; data base producers therefore prefer inter-indexer consistency as a measure of indexing quality or effectiveness. The present article assesses the validity of this substitution in various environments.		Loll N. Rolling	1981	Inf. Process. Manage.	10.1016/0306-4573(81)90028-5	search engine indexing;quality control;formula;computer science;data mining;reliability;database;information retrieval	DB	-34.67262814516474	-60.928043789694186	159319
67dfa427016b26447895037175f63583420ad802	a test of genetic algorithms in relevance feedback	comparative analysis;information retrieval;pertinencia;pregunta documental;test collections;test;algoritmo genetico;evaluation methods;genetics;question documentaire;ensayo;essai;feedback;recherche information;pertinence;algorithme genetique;query;artificial intelligence;algorithms;genetic algorithm;genetic algorithms;evaluation;recuperacion informacion;relevance;reformulation;evaluacion;relevance information retrieval;relevance feedback;test collection;collection test	There have been recent applications of genetic algorithms to information retrieval, mostly with respect to relevance feedback. Nevertheless, they are yet to be evaluated in a way that allows them to be compared with each other and with other relevance feedback techniques. We here implement the different genetic algorithms that have been applied in the literature together with some of our own variations, and evaluate them using the residual collection method described by Salton in 1990 for the evaluation of relevance feedback techniques. We compare the results with those of the Ide dec-hi method, which is one of the traditional methods that yields the best results. 2002 Elsevier Science Ltd. All rights reserved.	genetic algorithm;information retrieval;integrated development environment;relevance feedback	Cristina López-Pujalte;Vicente P. Guerrero Bote;Félix de Moya Anegón	2002	Inf. Process. Manage.	10.1016/S0306-4573(01)00061-9	genetic algorithm;computer science;artificial intelligence;evaluation;data mining;information retrieval;algorithm	Web+IR	-36.69414801756729	-60.43424201829525	159890
52465ac3b86444c5332fda51d3c7a58f03b89db8	driving under voluntary and involuntary distraction: an empirical study of compensatory behaviors		To minimize the risk of distracted driving, drivers will take compensatory behaviors, such as deceleration and raising mental efforts. Moreover, it has been proved to be significantly different between voluntary and involuntary distractions which worth further exploration. Therefore, this study carried out an experiment of mobile communication distracted behaviors in simulated driving environment among 34 nonprofessional drivers. Independent variables include two triggers of driving distraction and two communication ways of mobile phone with complete within-subjects design. Dependent variables contain four dimensions, including driving behaviors, physiological indexes, mobile phone usage and NASA task load index (NASA-TLX). The results of vehicle driving simulator experiment reveal that drivers will take compensatory behaviors when taking driving distraction tasks, and the degree of compensatory behaviors is significantly different between voluntary and involuntary driving distraction. Generally, drivers would like to compensate more under involuntary driving distraction than voluntary driving distraction. The results of this paper give a new way to improve driving safety.		Yuhan Shi;Ronggang Zhou	2017		10.1007/978-3-319-58481-2_29	empirical research;distracted driving;variables;social psychology;distraction;mobile phone;turnover;mobile telephony;business;driving simulator	SE	-47.702407961757295	-52.516814991427495	160414
c00b1e74bd34f372b580a62e7d4d2600e8e68dfd	supporting user selection of digital libraries		This research aims to support users in identifying collections (e.g. digital libraries) that are authorities on the topic they are searching for. These collections should contain a large proportion and quantity of relevant documents, such that they may serve both current and (related) future information needs. This paper presents our research goals for this search task, and the steps taken thus far to achieve them. In addition, we provide our plans for future research in this area.	digital library;information needs;library (computing)	Helen Dodd	2011	TCDL Bulletin		multimedia;digital library;computer science	HCI	-33.865831095030636	-55.658716573128125	160454
696b215f4b372ddf99663ca16f74d6050c70739a	an approach for the analysis of perceptual and gestural performance during critical situations		Our objective is the design of a Virtual Learning Environment to train a person performing a work activity, to acquire non-technical skills during the experience of a critical situation. While the person’s performance level is due to carefully acquired technical skills, how it is maintained in front of criticality depends on non-technical skills, such as decision-making, situation awareness or stress management. Following previous break downs of the domains ill-defined aspects, we focus in this paper on the design of an approach to evaluate the variation of a learner’s performance in front of learning situations showing varying degrees of criticality, in the domains of driving and midwifery.		Yannick Bourrier;Francis Jambon;Catherine Garbay;Vanda Luengo	2017		10.1007/978-3-319-66610-5_29	break downs;multimedia;human–computer interaction;computer science;criticality;perception;artificial neural network;situation awareness;stress management;virtual learning environment;artificial intelligence	Vision	-48.19364463380435	-54.73271810443114	160588
2f9737a3addf009b3936caa45f18c6787553ea50	sequential pattern mining for structure-based xml document classification	busqueda informacion;document structure;limpieza;methode a pas;labeled tree;estructura documental;classification supervisee;information retrieval;structure document;xml language;supervised classification;data mining;step method;nettoyage;fouille donnee;recherche information;clasificacion supervisada;xml document;sequential pattern mining;busca dato;langage xml;lenguaje xml;cleaning;metodo a paso	This article presents an original supervised classification technique for XML documents which is based on structure only. Each XML document is viewed as an ordered labeled tree, represented by his tags only. Our method has three steps. After a cleaning step, we characterize each predefined cluster in terms of frequent structural subsequences. Then we classify the XML documents based on the mined patterns of each cluster.	document classification;machine learning;mined;plasma cleaning;sequential pattern mining;supervised learning;xml	Calin Garboni;Florent Masseglia;Brigitte Trousse	2005		10.1007/978-3-540-34963-1_35	well-formed document;xml validation;simple api for xml;xml schema;computer science;data mining;database;information retrieval	DB	-35.02324772750241	-59.9006136197028	160743
3789bfd4c33994df265e43de03eb326c3cb55393	audio features affected by music expressiveness: experimental setup and preliminary results on tuba players	music information retrieval;performance analysis;sentiment analysis;tuba;music and emotions	Within a Music Information Retrieval perspective, the goal of the study presented here is to investigate the impact on sound features of the musician's affective intention, namely when trying to intentionally convey emotional contents via expressiveness. A preliminary experiment has been performed involving 10 tuba players. The recordings have been analysed by extracting a variety of features, which have been subsequently evaluated by combining both classic and machine learning statistical techniques. Results are reported and discussed.	information retrieval;jaquet-droz automata;machine learning;sound card	Alberto Introini;Giorgio Presti;Giuseppe Boccignone	2016		10.1145/2911451.2914690	natural language processing;speech recognition;computer science;multimedia;music and emotion;sentiment analysis	AI	-43.821384312411595	-58.538314878984494	160746
ed109ffe41ec8e7373b1e88db5fb71fc23cede0c	modulation of attention by faces expressing emotion: evidence from visual marking	interfase usuario;user interface;expressed emotion;man machine system;emotion emotionality;cognition;visual control;cognicion;sistema hombre maquina;controle visuel;emotion emotivite;interface utilisateur;emocion emotividad;control visual;systeme homme machine	Recent findings demonstrated that negative emotional faces (sad, anger or fear) tend to attract attention more than positive faces do. This study used the paradigm of visual marking to test the perspective that mentioned and explored whether the preview benefit still existed when using schematic faces as materials. The results found that preview benefit was significant in the search of affective materials. In a gap condition, it was faster to search negative faces than to search positive faces. However, this advantage did not appear in half-element condition when negative faces as distractors, which indicated that the view that negative faces capture attention more efficiently is not always like this.	item unique identification;modulation;programming paradigm;schematic	Fang Hao;Hang Zhang;Xiaolan Fu	2005		10.1007/11573548_17	cognition;computer science;operating system;user interface	HCI	-44.8886306270915	-55.312951482758656	160818
d4d43093c868082479c9cf75b5d91325805bea3b	evaluation of a safety system using its technologies for streetcar stops without safety barriers	streetcar stops without safety barriers;safety measures using its technologies	Streetcar stops in some Japanese cities lack safety barriers, which has led to fatal accidents. As a result, urgent safety measures are required. This study evaluates a safety system using ITS technologies for streetcar stops without safety barriers in Kochi. We conducted a questionnaire survey of car drivers and streetcar users. The safety system made car drivers pay more attention to streetcar users and decreased the level of anxiety among streetcar users. To avoid conflicts between cars and streetcar users at the streetcar stops, boarding and alighting from right-side doors with a self-service fare collection system was also investigated.	safety engineering	Takashi Omatsu;Nobuaki Ohmori;Shuichi Matsumoto;Kenji Okamura;Yasuhiko Kumagai;Noboru Harata	2011	Int. J. Intelligent Transportation Systems Research	10.1007/s13177-011-0032-2	computer science;engineering;transport engineering;forensic engineering;computer security	HCI	-46.929201089369606	-52.19356257822693	161148
134ad9793bc48704f24e535d19e98c280fc1ddb5	autobib: automatic extraction of bibliographic information on the web	semilla;extraction information;site web;hypermedia markup languages;modelo markov oculto;bibliographic database;base donnee;analisis estadistico;red www;modelo markov;information extraction;modele markov cache;hidden markov model;information retrieval;heuristic method;semence;base donnee bibliographique;reseau web;database;base dato;automatisation;metodo heuristico;information access;probabilistic approach;automatizacion;information presentation;data mining hidden markov models humans html computer science databases training data automation engineering profession programming profession;hidden markov model autobib automatic information extraction bibliographic information extraction world wide web information access html bootstrapping method statistical method heuristic method bibliographic web sites bibliographic records record boundary discovery techniques;base datos bibliografica;internet bibliographic systems information retrieval hidden markov models computer bootstrapping hypermedia markup languages;access to information;lines of code;markov model;hidden markov models;internet;statistical analysis;enfoque probabilista;approche probabiliste;analyse statistique;success rate;autogeneration mutuelle;acces information;analizador sintaxico;world wide web;seed;bootstrapping;bibliographic systems;acceso informacion;parser;methode heuristique;sitio web;modele markov;analyseur syntaxique;extraccion informacion;langage html;web site;html language;lenguaje html;computer bootstrapping;automation	"""The Web has greatly facilitated access to information. However, information presented in HTML is mainly intended to be browsed by humans, and the problem of automatically extracting such information remains an important and challenging task. In this work, we focus on building a system called AUTOBIB to automate extraction of bibliographic information on the Web. We use a combination of bootstrapping, statistical, and heuristic methods to achieve a high degree of automation. To set up extraction from a new site, we only need to provide a few lines of code specifying how to download pages containing bibliographic information. We do not need to be concerned with each site's presentation format, and the system can cope with changes in the presentation format without human intervention. AUTOBIB bootstraps itself with a small seed database of structured bibliographic records. For each bibliographic Web site, we identify segments within its pages that represent bibliographic records, using state-of-the-art record-boundary discovery techniques. Next, we find matches for some of these """"raw records"""" in the seed database using a set of heuristics. These matches serve as a training set for a parser based on the hidden Markov model (HMM), which is then used to parse the rest of the raw records into structured records. We have found an effective HMM structure with special states that correspond to delimiters and HTML tags in raw records. Experiments demonstrate that for our application, this HMM structure achieves high success rates without the complexity of previously proposed structures."""	algorithm;automatic taxonomy construction;bibliographic record;bootstrapping (compilers);comparison shopping website;delimiter;download;encode;freedom of information laws by country;html;heuristic (computer science);hidden markov model;information extraction;lineage (evolution);markov chain;parsing;pointer (computer programming);source lines of code;test set;web page;world wide web	Junfei Geng;Jun Yang	2004	Proceedings. International Database Engineering and Applications Symposium, 2004. IDEAS '04.	10.1109/IDEAS.2004.14	the internet;html;computer science;automation;data mining;database;markov model;source lines of code;world wide web;hidden markov model;bootstrapping	Web+IR	-36.41669858000173	-60.13096547369272	161253
18070571981103a93fb4cb531f512a4ebbb98dd1	automatic identification of parallel documents with light or without linguistic resources	busqueda informacion;lenguaje natural;analisis contenido;front end;interfase usuario;text;linguistique;gollete estrangulamiento;availability;user interface;disponibilidad;information retrieval;lexicon;juego de funciones;langage naturel;tratamiento lenguaje;intelligence artificielle;texte;jeu role;data mining;goulot etranglement;content analysis;linguistica;language processing;fouille donnee;recherche information;natural language;traitement langage;artificial intelligence;interface utilisateur;inteligencia artificial;analyse contenu;lexico;multilinguisme;role playing;texto;disponibilite;bottleneck;content based retrieval;busca dato;parallel corpora;natural language processing;recherche par contenu;multilingualism;multilinguismo;lexique;linguistics	Parallel corpora are playing a crucial role in multilingual natural language processing. Unfortunately, the availability of such a resource is the bottleneck in most applications of interest. Mining the web for parallel corpora is a viable solution that comes at a price: it is not always easy to identify parallel documents among the crawled material. In this study we address the problem of automatically identifying the pairs of texts that are translation of each other in a set of documents. We show that it is possible to automatically build particularly efficient content-based methods that make use of very little lexical knowledge. We also evaluate our approach toward a front-end translation task and demonstrate that our parallel text classifier yields better performances than another approach based on a rich lexicon.	automatic identification and data capture;lexicon;natural language processing;parallel text;performance;text corpus	Alexandre Patry;Philippe Langlais	2005		10.1007/11424918_37	natural language processing;availability;speech recognition;content analysis;computer science;front and back ends;linguistics;natural language;user interface	NLP	-36.24359008121309	-63.825027146325965	161504
d25f2cce71792bc94093a3779b8df79f6e7be36e	analysis of driving reliability under non-free flow condition	vehicles reliability estimation accidents acceleration visualization roads;driving simulation;non free flow;driving simulation driving reliability non free flow characteristics of driving behavior;characteristics of driving behavior;driving reliability	Congestion on road often leads to vehicles driving in a non-free flow condition. In order to study the driving reliability under non-free flow condition, a driving simulation experiment was designed and then conducted on the simulator with eight degrees of freedom to collect characteristics of driving behavior in the driving process of perception, decision and operation. Driver's reaction time of acceleration and deceleration, change rate of visual angle, estimation of speed and space between two vehicles, and memory capacity of information in surrounding environment were determined as four main factors affecting the driving reliability. They were explored in depth respectively to obtain the variation trend under non-free flow condition. The results were analyzed in a quantitative method and offers practical information to the further study of driving reliability and road safety.	driving simulator;network congestion;open road tolling;simulation	Pin Wang;Shou'en Fang;Junhua Wang	2013	2013 IEEE Eleventh International Symposium on Autonomous Decentralized Systems (ISADS)	10.1109/ISADS.2013.6513427	simulation	Arch	-46.79270372380274	-53.08686271688425	162355
413d13774baa7c77aa0a2e8418f31b3d98c47510	determinants of conflict detection: a model of risk judgments in air traffic control	factor riesgo;air traffic control;conflict detection;judgment;pedestrian safety;ergonomia;risk factor;poison control;conflict;risk analysis;injury prevention;trafico aereo;hombre;safety literature;ergonomie;satisfiability;traffic safety;injury control;facteur risque;judgment human characteristics;home safety;injury research;jugement;safety abstracts;human factors;mathematical models;conflicto;occupational safety;aircraft separation;seguridad trafico;cognition;safety;human;cognicion;training program;safety research;accident prevention;violence prevention;juicio;bicycle safety;securite trafic;trafic aerien;individual difference;conflit;poisoning prevention;falls;ergonomics;air traffic;suicide prevention;distance;homme;experience personnel;air traffic controllers	OBJECTIVE A model of conflict judgments in air traffic control (ATC) is proposed.   BACKGROUND Three horizontal distances determine risk judgments about conflict between two aircraft: (a) Dt(o) is the distance between the crossing of the aircraft trajectories and the first aircraft to reach that point; (b) Dt(h) is the distance between the two aircraft when they are horizontally closest; and (c) Dt(v) is the horizontal distance between the two aircraft when their growing vertical distance reaches 1000 feet.   METHODS Two experiments tested whether the variables in the model reflect what controllers do. In Experiment 1, 125 certified controllers provided risk judgments about situations in which the model variables were manipulated. Experiment 2 investigated the relationship between the model and expertise by comparing a population of certified controllers with a population of ATC students.   RESULTS Across both experiments, the model accounted for 44% to 50% of the variance in risk judgments by certified controllers (N=161) but only 20% in judgments by ATC students (N=88). There were major individual differences in the predictive power of the model as well as in the contributions of the three variables. In Experiment 2, the model described experts better than novices.   CONCLUSION The model provided a satisfying account of the data, albeit with substantial individual differences. It is argued that an individual-differences approach is required when investigating the strategies involved in conflict judgment in ATC.   APPLICATION These findings should have implications for developing user-friendly interfaces with conflict detection devices and for devising ATC training programs.	advanced tactical center;advanced transportation controller;air traffic control;distance;emoticon;experiment;file synchronization;foot;judgment;model checking;one thousand;sample variance;training programs;usability	Stéphanie Stankovic;Eric Raufaste;Philippe Averty	2008	Human factors	10.1518/001872008X250584	psychology;simulation;medicine;environmental health;pathology;engineering;human factors and ergonomics;air traffic control;transport engineering;computer security;mechanical engineering	AI	-46.073687961170094	-54.43195298399597	162746
1051c3e66c258fb2e049a291aa7ce88253fe380b	building an earthquake evacuation ontology from twitter	earthquake;evacuation center earthquake evacuation ontology twitter tohoku earthquake landlines mobile phone lines information exchange unstructured data map information shop information;real time;social web;social networking online data structures earthquakes emergency services;earthquakes;mobile phone;ontologies educational institutions earthquakes twitter vocabulary buildings computers;social web earthquake twitter ontology;data structures;social networking online;twitter;ontology;emergency services	During the massive Tohoku earthquake, while landlines and mobile phone lines got stuck, Twitter were used to exchange information about evacuation. On 11 March, the number of tweets from Japan dramatically increased to about 33 million, 1.8 times higher than the average figure. However, since texts on Twitter are unstructured data, are more complex than other text media, it is difficult to find a suitable evacuation center. Not only information in Twitter, people but also need to combine other information such as map information, shop information to find the suitable evacuation center. Since it takes much time to do these tasks, these manual processing are not suitable in the emergency status. Therefore, we need an approach to help computers to understand the meaning of evacuation, and to provide the most suitable evacuation center based on earthquake victims' behaviors in real-time. In this paper, we firstly design an earthquake evacuation ontology. Secondly, we indicate that by using this ontology, computers can provide the most suitable evacuation center based on earthquake victims' behaviors in real-time.	computer;landline;mobile phone;real-time clock;real-time computing;real-time transcription;scalability;social media;telephone line;world wide web	Isabel Shizu Miyamae Iwanaga;The-Minh Nguyen;Takahiro Kawamura;Hiroyuki Nakagawa;Yasuyuki Tahara;Akihiko Ohsuga	2011	2011 IEEE International Conference on Granular Computing	10.1109/GRC.2011.6122613	social web;computer science;ontology;internet privacy;world wide web;computer security	Visualization	-46.03010465992177	-62.49710590547314	163177
740aa5362af10f65a3df238123a906c19e15beee	fixation differences in visual search of accident scenes by novices and expert emergency responders	disaster response;emergency medicine;expert–novice differences;eye tracking;visual search	OBJECTIVE: We sought to investigate whether expert-novice differences in visual search behavior found in other domains also apply to accident scenes and the emergency response domain.   BACKGROUND: Emergency service professionals typically arrive at accidents only after being dispatched when a civilian witness has called an emergency dispatch number. Differences in visual search behavior between the civilian witness (usually a novice in terms of emergency response) and the professional first responders (experts at emergency response) could thus result in the experts being given insufficient or erroneous information, which would lead them to arrive unprepared for the actual situation.   METHOD: A between-subjects, controlled eye-tracking experiment with 20 novices and 17 experts (rescue and ambulance service personnel) was conducted to explore expert-novice differences in visual search of accident and control images.   RESULTS: The results showed that the experts spent more time looking at task-relevant areas of the accident images than novices did, as predicted by the information reduction hypothesis. The longer time was due to longer fixation durations rather than a larger fixation count.   CONCLUSION: Expert-novice differences in visual search are present in the emergency domain. Given that this domain is essential to saving lives and also relies heavily on novices as the first link in the chain of response, such differences deserve further exploration.   APPLICATION: Visual search behavior from experts can be used for training purposes. Eye-tracking studies of novices can be used to inform the design of emergency dispatch interviews.	accident and emergency department;ambulances;area striata structure;dynamic dispatch;eye tracking;large	Erik Prytz;Caroline Norén;Carl-Oscar Jonson	2018	Human factors	10.1177/0018720818788142	social psychology;witness;visual search;applied psychology;engineering;eye tracking	HCI	-48.180971204446706	-55.12925850609111	163356
521648d7ae002a2da9f0a562632fad4665886817	using clustering methods to improve ontology-based query term disambiguation	analyse amas;representacion conocimientos;categorisation;ontologie;cluster;amas;relation semantique;interrogation base donnee;relacion semantica;interrogacion base datos;semantics;pregunta documental;semantica;semantique;classification;desambiguisacion;categorizacion;cluster analysis;clustering method;disambiguation;representation connaissance;query;ontologia;analisis cluster;semantic relation;monton;desambiguisation;knowledge representation;semantic relations;discriminacion;ontology;database query;clasificacion;discrimination;requete;categorization	In this article we describe results of our research on the disambiguation of user queries using ontologies for categorization. We present an approach to cluster search results by using classes or “Sense Folders” ~prototype categories! derived from the concepts of an assigned ontology, in our case WordNet. Using the semantic relations provided from such a resource, we can assign categories to prior, not annotated documents. The disambiguation of query terms in documents with respect to a user-specific ontology is an important issue in order to improve the retrieval performance for the user. Furthermore, we show that a clustering process can enhance the semantic classification of documents, and we discuss how this clustering process can be further enhanced using only the most descriptive classes of the ontology. © 2006 Wiley Periodicals, Inc.	categorization;cluster analysis;document classification;information retrieval;john d. wiley;ontology (information science);prototype;result set;rover (the prisoner);word-sense disambiguation;wordnet	Ernesto William De Luca;Andreas Nürnberger	2006	Int. J. Intell. Syst.	10.1002/int.20155	upper ontology;discrimination;biological classification;computer science;ontology;ontology;data mining;database;semantics;cluster analysis;ontology-based data integration;information retrieval;process ontology;categorization;cluster	Web+IR	-35.26386736377564	-60.45567968072512	163449
643d32234aa80cc9d41f1c28b8ccdbd00fb818f9	terminological knowledge structure for intermediary expert systems	intermediary expert system;terminological knowledge structure;indexing;databases;expert system;indexes;term frequency;expert systems;knowledge base;classification;query expansion;database search;information retrieval	An intermediary expert system (IES) helps both end users and professional searchers to conduct their online database searching. To provide advice about term selection and query expansion, an IES should include a terminological knowledge structure. Terminological attributes as well as other properties could provide the starting point for building a knowledge base, and knowledge acquisition could rely on knowledge-base techniques coupled with statistical techniques. The searching behavior of expert online searchers would provide one source of knowledge. The knowledge structure would include three constructs for each term: frequency data, a hedge, and a position in a classification scheme. Switching vocabularies or languages could provide a meta-schema and facilitate the interoperability of databases in similar subject domains. To develop such knowledge structure, future research should focus on terminological attributes, word and phrase disambiguation, automated text processing, and the role of thesauri and classification schemes in indexing and retrieval. In particular, such research should develop techniques that combine knowledge-base and statistical methods and that consider user	comparison and contrast of classification schemes in linguistics and metadata;database;expert system;integrated encryption scheme;interoperability;knowledge acquisition;knowledge base;query expansion;statistical classification;thesaurus (information retrieval);vocabulary;word-sense disambiguation	Raya Fidel;Efthimis N. Efthimiadis	1995	Inf. Process. Manage.	10.1016/0306-4573(95)80003-C	database index;structure;search engine indexing;controlled vocabulary;query expansion;database search engine;construction;biological classification;computer science;artificial intelligence;data mining;database;terminology;user interface;metadata;tf–idf;world wide web;expert system;information retrieval	Web+IR	-33.87576433525843	-60.24704491657744	163474
8c52c9ea2f32f12816b5f514d3e75e6165efa1d4	an indirect method of information retrieval	information retrieval	Summery--The information retrieval process, treated strictly as a matching procedure, has the defects that the whole file must be probed for each query, and that it overlooks the fact that the relevance of the information from one document depends upon what is already known about the subject, and in turn affects the relevance of other documents subsequently examined. A mathematical model of a search technique in which the defects of the direct method are taken into account is demonstrated by an experiment in which a given paper is treated as an enquiry and the references cited in the paper are treated as relevant answers. The results in two tests show much better results than those achieved by the direct method. No spurious material was retrieved by either method.	direct method in the calculus of variations;information retrieval;matching (graph theory);mathematical model;relevance	William Goffman	1968	Information Storage and Retrieval	10.1016/0020-0271(68)90030-2	computer science;data mining;world wide web;information retrieval	Web+IR	-37.32866016422009	-62.615990024356904	163593
37bbbc01ffbfa9329ba406a188934bfd61c0aeb7	handling orthographic varieties in japanese ir: fusion of word-, n-gram-, and yomi-based indices across different document collections	busqueda informacion;sistema multiple;text;prononciation;japonais;information retrieval;speech processing;edit distance;logique floue;interrogation base donnee;tratamiento palabra;interrogacion base datos;traitement parole;logica difusa;multiple system;texte;fuzzy logic;indexing;recherche information;pronunciation;indexation;indizacion;distancia;appariement chaine;string matching;texto;pronunciacion;japones;database query;distance;test collection;systeme multiple;japanese	Orthographic varieties are common in the Japanese language and represent a serious problem for Japanese information retrieval (IR), as IR systems run the risk of missing documents that contain variant forms of the search term. We propose two different strategies for handling orthographic varieties: pronunciation or yomi-based indexing and “Fuzzy Querying”, comparing katakana terms based on edit distance. Both strategies were integrated into our multiple index and fusion system [1] and tested using two different test collections, newspaper articles (Mainichi Shimbun ’98) and scientific abstracts (NTCIR-1), to compare their performance across text genres.		Nina Kummer;Christa Womser-Hacker;Noriko Kando	2005		10.1007/11562382_65	fuzzy logic;natural language processing;japanese;speech recognition;edit distance;computer science;artificial intelligence;speech processing;database;distance;information retrieval;algorithm	NLP	-35.27249511093615	-63.37524009816436	163659
141cb9ee401f223220d3468592effa90f0c255fa	longitudinal study of automatic face recognition	databases;market research;probability automatic face recognition system intersubject variability subject specific aging analysis statistical model commercial off the shelf cots matcher false accept rate far;aging;face recognition;statistics;face databases face recognition aging sociology statistics market research;face;statistical analysis face recognition image matching probability;sociology	The two underlying premises of automatic face recognition are uniqueness and permanence. This paper investigates the permanence property by addressing the following: Does face recognition ability of state-of-the-art systems degrade with elapsed time between enrolled and query face images? If so, what is the rate of decline w.r.t. the elapsed time? While previous studies have reported degradations in accuracy, no formal statistical analysis of large-scale longitudinal data has been conducted. We conduct such an analysis on two mugshot databases, which are the largest facial aging databases studied to date in terms of number of subjects, images per subject, and elapsed times. Mixed-effects regression models are applied to genuine similarity scores from state-of-the-art COTS face matchers to quantify the population-mean rate of change in genuine scores over time, subject-specific variability, and the influence of age, sex, race, and face image quality. Longitudinal analysis shows that despite decreasing genuine scores, 99% of subjects can still be recognized at 0.01% FAR up to approximately 6 years elapsed time, and that age, sex, and race only marginally influence these trends. The methodology presented here should be periodically repeated to determine age-invariant properties of face recognition as state-of-the-art evolves to better address facial aging.	database;facial recognition system;heart rate variability;image quality;mugshot;wikipedia	Lacey Best-Rowden;Anil K. Jain	2015	2015 International Conference on Biometrics (ICB)	10.1109/ICB.2015.7139087	market research;facial recognition system;face;computer vision;computer science;artificial intelligence;machine learning;data mining;database;geometry;statistics	Vision	-38.42133379237489	-62.66795759517152	163961
5046dbddf7356af15e9d6b2f9dc9481e7cce3c84	the application of fuzzy logic to the construction of the ranking function of information retrieval systems	boolean model;information retrieval system;search engine;natural language;information retrieval;vector space model;fuzzy logic;fuzzy set	The quality of the ranking function is an important factor that determines the quality of the Information Retrieval system. Each document is assigned a score by the ranking function; the score indicates the likelihood of relevance of the document given a query. In the vector space model, the ranking function is defined by a mathematic expression such as: ∑ ∈ = q t d q score ) , ( tf(t in d) * idf(t) * getBoost(t.field in d) * lengthNorm(t.field in d) * overlap(q,d) * queryNorm(q)	baseline (configuration management);fuzzy logic;information retrieval;ranking (information retrieval);relevance;semantics (computer science);tf–idf	Neil Rubens	2006	CoRR		fuzzy logic;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;data mining	Web+IR	-34.70037996973468	-61.25612991830877	164306
756d208f158f05d7a8ffa2ea03e2405e2f3361cc	link and co-inlink network diagrams with url citations or title mentions	world wide web	Webometric network analyses have been used to map the connectivity of groups of web sites in order to identify clusters, important sites or overall structure. Such analyses have mainly been based upon hyperlink counts, the number of hyperlinks between a pair of web sites, although some have used title mentions or URL citations instead. The ability to automatically gather hyperlink counts from Yahoo! ceased in April 2011 and the ability to manually gather such counts was due to cease by early 2012, creating a need for alternatives. This article assesses URL citations and title mentions as possible replacements for hyperlinks in both binary and weighted direct link and co-inlink network diagrams. It also assesses three different types of data for the network connections: hit count estimates, counts of matching URLs and filtered counts of matching URLs. Results from analyses of US library and information science departments and UK universities give evidence that metrics based upon URLs or titles can be appropriate replacements for metrics based upon hyperlinks for both binary and weighted networks, although filtered counts of matching URLs are necessary to give the best results for co-title mention and co-URL citation network diagrams.	backlink;bitwise operation;citation network;diagram;hyperlink;inline linking;library and information science;weighted network	Mike Thelwall;Pardeep Sud;David Wilkinson	2012	JASIST	10.1002/asi.21709	semantic url;url normalization;computer science;data mining;world wide web;information retrieval	Web+IR	-38.36735477524939	-55.368709158119756	164397
7659b01be6f27873f902ad9d265684a29026188e	legal knowledge acquisition and multimedia applications	semantic web	Search, retrieval, and management of multimedia contents are challenging tasks for users and researchers alike. The aim of ESentencias Project is to develop a software-hardware system for the global management of the multimedia contents produced by the Spanish Civil Courts. We apply technologies such as the Semantic Web, ontologies, NLP techniques, audio-video segmentation and IR. The ultimate goal is to obtain an automatic classification of images and segments of the audiovisual records that, coupled with textual semantics, allows an efficient navigation and retrieval of judicial documents and additional	knowledge acquisition;natural language processing;ontology (information science);semantic web	Ciro Gracia;Pompeu Casanovas;Marta Poblet;Xavier Binefa;Jordi Carrabina	2007			semantic web;knowledge acquisition;multimedia;ontology (information science);information retrieval;semantics;computer science	Web+IR	-33.97352747308145	-64.21805116674021	164857
a75f4ed47a8f7a0f103efd57410c78249ac31814	automatic content creation for games to train students distinguishing similar chinese characters	similar chinese characters;automatic content creation;e learning;game;radical extraction;student learning;similarity measure	In learning Chinese, many students often have the problem of mixing up similar characters. This can cause misunderstanding and miscommunication in daily life. It is thus important for students learning the Chinese language to be able to distinguish similar characters and understand their proper usage. In this paper, we propose a game style framework in which the game content in identifying similar Chinese characters in idioms and words is created automatically. Our prior work on analyzing students' Chinese handwriting can be applied in the similarity measure of Chinese characters. We extend this work by adding the component of radical extraction to speed up the search process. Experimental results show that the proposed method is more accurate and faster in finding more similar Chinese characters compared with the baseline method without considering the radical information.		Kwong-Hung Lai;Howard Leung;Jeff K. T. Tang	2009		10.1007/978-3-642-03426-8_28	games;computer science;artificial intelligence;multimedia	HCI	-40.80117761492843	-59.726419243686244	165002
38186bf16fc138711e0ed98ae8f92d177846de0f	music plagiarism at a glance: metrics of similarity and visualizations		The plagiarism is a debated topic in different fields and in particular in music, given the huge amount of money that music is able to generate. Moreover, it is controversial aspect in the law's field given the subjectivity of the judges that have to pronounce on a suspicious case. Automatic detection of music plagiarism is fundamental to overcome these limits by representing an useful support for judges during their pronouncements and an important result to avoid musicians to spend more time in court than on composing and playing music.In this paper we address this issue by defining a new metric to discover pop music similarity and we study whether visualization can assist domain experts in judging suspicious cases. We describe a user study in which subjects performed different tasks on a song collection using different visual representations to investigate which one is best in terms of intuitiveness and accuracy. Results provided us with positive feedback about our choices and some useful suggestions for future directions.	emoticon;fuzzy logic;graphical user interface;machine learning;money;positive feedback;usability testing	Roberto De Prisco;Antonio Esposito;Nicola Lettieri;Delfina Malandrino;Donato Pirozzi;Gianluca Zaccagnino;Rocco Zaccagnino	2017	2017 21st International Conference Information Visualisation (IV)	10.1109/iV.2017.49	information retrieval;visualization;data mining;popular music;subjectivity;rhythm;computer science	DB	-34.910195134555906	-54.334031164416096	165288
432ed941958594a886753f4fb874e143e4e6995c	creating health typologies with random forest clustering	cluster algorithm;pattern clustering;local authority;public health domain;health typologies;random forest clustering;health specific geodemographic classification system;heating;statistical analysis;determinants of health;random forest;classification system;k means algorithm;statistical analysis demography health care pattern clustering public administration;demography;public health;k means algorithm health typologies random forest clustering health specific geodemographic classification system birmingham uk public health domain;open source;birmingham uk;health care;public administration	In this paper, we describe the creation of a health-specific geodemographic classification system for the whole city of Birmingham UK. Compared to some existing open source and commercial systems, the proposed work has a couple of distinct advantages: (i) It is particularly designed for the public health domain by combining most reliable health data and other sources accounting for the main determinants of health. (ii) A novel random forest clustering algorithm is used for generating clusters and it has several obvious advantages over the commonly used k-means algorithm in practice. These resultant health typologies will help local authorities to understand and design customized health interventions for the population. A Birmingham map illustrating the distribution of all health typologies is produced.	algorithm;cluster analysis;k-means clustering;open-source software;random forest;resultant	Ping Sun;Irena Begaj;Iris Fermin;Jim McManus	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596554	random forest;public health;computer science;machine learning;data mining;management science;operations research;health care;k-means clustering;social determinants of health	AI	-47.80646782330264	-61.75544734332662	165362
e1bbc6157bb0ebb5d05a78c517cd0f9f9ecd8b4d	a comparison among different front-end input versions of a medical record system	mouse;input time;front end;user interfaces expert systems light pens medical administrative data processing mouse controllers computers;mice;pen computers;medical records;multimedia;medical administrative data processing;expert systems;intelligent medical record entry system;time measurement;computer graphics;medical tests;user satisfaction front end input medical record system intelligent medical record entry system color graphics multimedia pen computers mouse input time;physics computing;pen computing;medical tests system testing intelligent systems machine intelligence costs computer graphics velocity measurement time measurement mice physics computing;color graphics;light pens;front end input;machine intelligence;intelligent systems;system testing;mouse controllers computers;velocity measurement;medical record system;user satisfaction;user interfaces;medical record systems;medical students	This paper describes a series of experiments to test whether there are significant differences between several different versions of the Intelligent Medical Record-Entry system (IMR-E). As the costs of including color graphics decrease, the use of multimedia technologies is becoming an importnnt issue. The development of pen computers provides a whole new range of possibilities for data input. We are planning to measure the effects of color, machine speed, and the use of a mouse or a pen computer on input time and user satisfaction when the system is used to input patient dnta by physicians and medical students.	apple ii graphics;color;computer;experiment;pen computing	Kuo-Lane Chen;Martha W. Evens;David A. Trace;Frank Naeymi-Rad	1993		10.1109/CBMS.1993.262967	simulation;intelligent decision support system;human–computer interaction;computer science;artificial intelligence;front and back ends;operating system;database;multimedia;computer graphics;user interface;system testing;expert system;medical record;time	HCI	-42.84456959169524	-54.33619729930662	165405
9192445a3d29d4c156b78f23160be08931e54004	architecture of an hybrid system for experimentation on web information retrieval incorporating clustering techniques	search engine;empirical study;web pages;web information retrieval;clustering;hybrid system;architecture;hybrid systems	The results retrieved from common search engines are frequently not acceptable because of the large amounts of Web pages that are returned and the ambiguity of the results. Several authors have suggested grouping the Web pages by topics using clustering techniques.#R##N##R##N#The application of these techniques involves different alternatives in different areas. Carrying out these research works could be easier and their cost lower if an experimentation hybrid system could be available that implemented the different alternatives or new ones could be added with a low cost.#R##N##R##N#This work proposes an architecture of a Web information retrieval experimentation hybrid system that incorporates clustering techniques. This hybrid system is composed of different replaceable components, so that we could test different alternatives with a low cost. Later, using the proposed and the implemented system an empirical study is exposed in order to evaluate what type of clustering techniques is the most appropriate.	hybrid system;information retrieval;interaction technique	Montserrat Mateos Sánchez;Carlos G. Figuerola	2007		10.1007/978-3-540-74827-4_54	computer science;data mining;world wide web;information retrieval	Web+IR	-35.1545759636986	-54.390608888753405	165460
e1f321dba71985fedc2a8faad4dac30c5da5e2da	pairwise hits: quality estimation from pairwise comparisons in creator-evaluator crowdsourcing process.		A common technique for improving the quality of crowdsourcing results is to assign a same task to multiple workers redundantly, and then to aggregate the results to obtain a higher-quality result; however, this technique is not applicable to complex tasks such as article writing since there is no obvious way to aggregate the results. Instead, we can use a two-stage procedure consisting of a creation stage and an evaluation stage, where we first ask workers to create artifacts, and then ask other workers to evaluate the artifacts to estimate their quality. In this study, we propose a novel quality estimation method for the two-stage procedure where pairwise comparison results for pairs of artifacts are collected at the evaluation stage. Our method is based on an extension of Kleinberg’s HITS algorithm to pairwise comparison, which takes into account the ability of evaluators as well as the ability of creators. Experiments using actual crowdsourcing tasks show that our methods outperform baseline methods especially when the number of evaluators per artifact is small.	aggregate data;algorithm;baseline (configuration management);crowdsourcing;experiment;interpreter (computing)	Takeru Sunahase;Yukino Baba;Hisashi Kashima	2017			data science;data mining;statistics	Web+IR	-38.69618336694475	-64.39900786294555	165516
11d2c4bc781178eee010c7f910895e47ab4f14c5	digital library retrieval model using subject classification table and user profile	modelizacion;filtering;filtrage;keyword;mise a jour;mesa;digital library;filtrado;palabra clave;mot cle;actualizacion;modelisation;user profile;biblioteca electronica;hierarchical classification;table;classification hierarchique;retrieval model;electronic library;modeling;clasificacion jerarquizada;bibliotheque electronique;updating	Existing library retrieval systems present users with massive results including irrelevant information. Thus, we propose SURM, a Retrieval Model using “Subject Classification Table” and “User Profile,” to provide more relevant results. SURM uses Document Filtering technique for the classified data and Document Ranking technique for the non-classified data in the results from keyword-based retrieval system. We have performed experiment on the performance of filtering technique, updating method of user profile, and document ranking technique with the retrieval results.		Seon-Mi Woo;Chun-Sik Yoo	2004		10.1007/978-3-540-30544-6_53	filter;document retrieval;visual word;digital library;systems modeling;document clustering;computer science;table;database;adversarial information retrieval;world wide web;vector space model;data retrieval;information retrieval	NLP	-36.01407840627772	-58.687769570122796	165770
3b7a36f53e9f38d3fa36c3d73f0fdebdd29a89ab	the use of query suggestions during information search	query suggestion;search behavior;search tactics;temporal search segment;topic difficulty	Query suggestion is a common feature of many information search systems. While much research has been conducted about how to generate suggestions, fewer studies have been conducted about how people interact with and use suggestions. The purpose of this paper is to investigate how and when people integrate query suggestions into their searches and the outcome of this usage. The paper further investigates the relationships between search expertise, topic difficulty, and temporal segment of the search and query suggestion usage. A secondary analysis of data was conducted using data collected in a previous controlled laboratory study. In this previous study, 23 undergraduate research participants used an experimental search system with query suggestions to conduct four topic searches. Results showed that participants integrated the suggestions into their searching fairly quickly and that participants with less search expertise used more suggestions and saved more documents. Participants also used more suggestions towards the end of their searches and when searching for more difficult topics. These results show that query suggestion can provide support in situations where people have less search expertise, greater difficulty searching and at specific times during the search.		Xi Niu;Diane Kelly	2014	Inf. Process. Manage.	10.1016/j.ipm.2013.09.002	query expansion;web query classification;computer science;phrase search;data mining;world wide web;information retrieval;search engine	DB	-35.534289305990356	-53.26508470770404	165887
1494f18f8d563ae73b2a7eb71b08f2e47f37137d	mean normalized retrieval order (mnro): a new content-based image retrieval performance measure	image retrieval performance measures;average normalized modified retrieval rank;mean average precision	The results of a content based image retrieval system can be evaluated by several performance measures, each one employing different evaluation criteria. Many of the methods used in the field of information retrieval have been adopted for use in image retrieval systems. This paper reviews the most widely used performance measures for retrieval evaluation with particular emphasis on the assumptions made during their design. More specifically, it focuses on the design principles of the commonly used Mean Average Precision (MAP) and Average Normalized Modified Retrieval Rank (ANMRR), pinpointing their limitations. It also proposes a new performance measure for image retrieval systems, the Mean Normalized Retrieval Order (MNRO), whose effectiveness is demonstrated through a wide range of experiments. Initial experiments were conducted on artificially produced query trials and evaluations. Experiments on a large database demonstrate the ability of MNRO to take into account the generality of the queries during the retrieval procedure. Furthermore, the results of a case study show that the proposed performance measure is closer to human evaluations, in comparison to MAP and ANMRR. Lastly, in order to encourage researchers and practitioners to use the proposed performance measure, we present the experimental results produced by a large number of state of the art descriptors applied on three well-known benchmarking databases.	content-based image retrieval;database;experiment;information retrieval	Savvas A. Chatzichristofis;Chryssanthi Iakovidou;Yiannis S. Boutalis;Elli Angelopoulou	2012	Multimedia Tools and Applications	10.1007/s11042-012-1192-z	computer science;machine learning;data mining;information retrieval	Web+IR	-40.49606718901754	-62.86990055276612	166071
0e0dbd09f723e12e9b964481b9711e5ad64c0b98	dynamic prediction model of situation awareness in flight simulation		Dynamic prediction for pilot situation awareness (SA) is an important issue in aviation safety. This paper presents a dynamic prediction model on the basis of the progressive triggering relationship between low and high-level SA. Six typical cognitive status (“Unnoticed”, “Attention of situation element (SE) but not reaching perception”, “Perception of SE”, “Perception but not matching the best rule”, “Triggering of the best rule” and “Decision making and operation”) were proposed for the description of the cognitive process of SE. Eighteen participants were selected to conduct the flight simulation tasks, and the situation awareness global assessment technique (SAGAT) method was adopted to measure the performance data (including accuracy and response time) at 13 typical time points. Statistical analysis showed that the theoretical value of the proposed SA dynamic prediction model was significantly correlated with accuracy and response time, which validated the model in the flight simulation environment preliminary. The proposed SA dynamic model in flight scenarios can give some references for cockpit’s human-computer interface design and flight tasks optimization assignment.	flight simulator;simulation	Chuanyan Feng;Xiaoru Wanyan;Shuang Liu;Damin Zhuang;Xu Wu	2018		10.1007/978-3-319-91122-9_10	interface design;simulation;cockpit;flight simulator;response time;aviation safety;situation awareness;cognition;computer science;cognitive assessment system	AI	-47.412657569904866	-54.069262297128354	166103
3c6c6647c3ad8c2271e5ea98cb7fc60173b94d41	linear time series models for term weighting in information retrieval	linear time;information retrieval;weighting;discrete time;linear model;term frequency;time series data;word frequency;ranking;time series	Common measures of term importance in information retrieval (IR) rely on counts of term frequency; rare terms receive higher weight in document ranking than common terms receive. However, realistic scenarios yield additional information about terms in a collection. Of interest in this paper is the temporal behavior of terms as a collection changes over time. We propose capturing each term’s collection frequency at discrete time intervals over the lifespan of a corpus and analyzing the resulting time series. We hypothesize the collection frequency of a term x at time t is predictable by a linear model of the term’s prior observations. On the other hand, a linear time series model for a strong discriminators’ collection frequency will yield a poor fit to the data. Operationalizing this hypothesis, we induce three time-based measures of term importance and test these against state-of-the-art term weighting models.	c date and time functions;information retrieval;linear model;ranking (information retrieval);text corpus;tf–idf;time complexity;time series	Miles Efron	2010	JASIST	10.1002/asi.21315	computer science;linear model;time series;data mining;term discrimination;information retrieval;statistics	Web+IR	-38.19688744168021	-62.65340544802655	166194
dfb988c0fb9f32f97b455a2b0a807d70de179a1d	how 3d-displays in atc permit direct event perception	air traffic control;conflict detection;judgment certainty;expertise;three dimensional display	Three different stereoscopic 3D visualizations are compared with regard to the quality of the event representation to the 2D reference currently used at air-traffic-control controller working positions. Both air-traffic-controllers and pilots judge safety critical air-traffic events showing two converging aircrafts. The level of cognitive demand that arises in peak-traffic situations is simulated by an additional auditory task that has to be conducted in parallel to the conflict assessment. The results indicate that 3D visualizations represent the event structure best, and enhance the efficacy of the air-traffic-controllers in detecting conflicts without compensating this advantage at the cost of efficiency due to a higher number of false alarms. The increase of this advantage with increased cognitive demand indicates benefits of 3D visualizations regarding mental workload and situation-awareness. These displays furthermore proof advantageous for judging vertical distances and the acquisition of conflict assessment skills, therewith indicating their usefulness for controlling areas with strong vertical aircraft movements as well as training decision skills.	advanced transportation controller	A Leonie Baier;Alf C. Zimmer	2015		10.1007/978-3-319-20373-7_29	simulation;engineering;artificial intelligence;social psychology	HCI	-47.390712891252825	-53.707417344440756	166551
c6f29bd2e2de2b059d4d2349b414a7b050530bf9	exploiting a large thesaurus for information retrieval	extraction information;thesaurus;evaluation systeme;document analysis;vector space model;information extraction;indexation automatique;specialist;information retrieval;grande dimension;improvement;evaluacion sistema;representation par terme indexation;large dimension;test;similitude;metalangage;ensayo;texto completo;texte integral;essai;analyse documentaire;system evaluation;analyse syntaxique;metalanguage;syntagme;analisis sintaxico;tesaurus;syntactic analysis;amelioration;similarity;umls test collection;automatic indexing;palabra;tâche appariement;modele espace vectoriel;metathesaurs;tarea apareamiento;level;nivel;mejoria;analisis documental;umls unified medical language system;word;niveau;similitud;search pattern;full text;gran dimension;matching task;syntagm;sintagma;test collection;representacion por termino indexacion;indizacion automatica;mot;metathesaurus;extraction informacion;collection test;metalenguaje	1. Background Accuracy in information retrieval, that is, achieving both high recall and precision, is challenging because the relationship between natural language and semantic conceptual structure is not straightforward. However, effective retrieval requires that the semantic conceptual structure (or content) of both queries and documents be known. Natural language processing is one way to	document;information retrieval;natural language processing;precision and recall;thesaurus	Alan R. Aronson	1994			natural language processing;language identification;semantic computing;explicit semantic analysis;question answering;similarity;latent semantic analysis;metalanguage;syntagmatic analysis;computer science;similitude;parsing;word;data mining;software testing;vector space model;information extraction;information retrieval	Web+IR	-35.68786933291951	-62.73575082713743	166996
dd2e576ad782b6912907fdc8341f863036cac064	conceptualizing information need in context	fi artikkeli aikakauslehdessa en journal article;tiedontarve;information need;information seeking;tiedonhankinta	Introduction. This paper examines the contextual features of information need. An attempt is made to demonstrate that information need can be conceptualized differently, depending on the context in which it appears. Method. Concept analysis of about fifty articles and books thematizing information need in diverse contexts. First, the main contexts and their constituents were identified. Second, it was examined how the nature of such constituents is reflected in the conceptualization of information need. Results. The study identified three major contexts affecting the formation and satisfaction of information need. First, studies conceptualizing information need in situations of action draw the main attention to the temporal and spatial factors. However, information need is approached as a black-boxed entity. Second, in the context of task performance, information need is perceived as a derivative and summary category indicating the information requirements of task or problem at hand. Third, in the context of dialogue, information need is conceptualized as jointly constructed understanding about the extent to which additional information is required to make sense of the issue at hand. Conclusion. Contextualist analysis of information need enriches the picture of this construct. The contextualist approaches can be elaborated further by conducting comparative studies focusing on related concepts such as uncertainty.	book;conceptualization (information science);formal concept analysis;information needs;requirement	Reijo Savolainen	2012	Inf. Res.		psychology;information needs;computer science;knowledge management;social psychology;information retrieval	Web+IR	-40.62156168929283	-57.294821135671086	167638
a5ad4be71e6380c63c781cadee11b9853bacfdf7	an exploration of total recall with multiple manual seedings		Four different reviewers participated in every topic. For each topic, a reviewer was assigned to manually seed that topic either by doing a single query (one-shot) and flagging (reviewing) the first (i.e. not necessarily the best) 25 documents returned by the query, or run as many queries (interactive) as desired within a short time period, but stop after 25 documents had been reviewed. In the one-shot approach, the reviewer is not allowed to examine any documents before issuing the query, i.e. the single query is issued ”blind” after only reading the topic title and description. The first 25 documents returned by that query are flagged as having been reviewed, but because there is no further interaction with the system, it does not matter whether or not the reviewer spends any time looking at those documents. In the interactive case, reviewers were free to read documents in as much or little depth as they wished, issue as many or as few queries as they wished, and use whatever affordances were available to them from the system to find documents (e.g. synonym expansion, timeline views, communication tracking views, etc.) Every document that the reviewer laid eyeballs on during this interactive period had to be flagged as having been seen and submitted to the Total Recall server, whether or not the reviewer believed the document to be relevant. This was of course done in order to correctly assess total effort, and therefore correctly measure gain (recall as a function of effort). We also note that the software did not strictly enforce the 25 document guideline. As a result, sometimes the interactive reviewers went a few documents over their 25 document limit and sometimes they went a few documents under, as per natural human variance and mistake, but we do not consider this to be significant. Regardless, all documents reviewed, even with duplication, were noted and sent to the Total Recall server. The reviewers working on each topic were randomized, assigned to run each topic either in one-shot or in interactive mode. Each topic had two one-shot and two interactive 25-document starting points. For our one allowed official manual run, these starting points were combined (unioned) into a separate starting point. Because we did not control for overlap or duplication of effort, the union of these reviewed documents is often smaller than the sum. Reviewers working asynchronously and without knowledge of each other often found (flagged as seen) the same exact documents. In this paper, we augment the official run with a number of unofficial runs, four for each topic, two one-shot starting points and two interactive starting points. This will be discussed further in Section 3	baseline (configuration management);document;ground truth;iteration;iterative method;observable;random seed;randomized algorithm;relative change and difference;server (computing);speculative execution;timeline	Jeremy Pickens;Tom Gricks;Bayu Hardi;Mark Noel;John Tredennick	2016			data mining;information retrieval;computer science;recall	Web+IR	-40.633355630694695	-58.71114952348178	167944
21c673c842aab8f90f9d6e2c4106f3e4ca27fb3e	a geometric model for information retrieval systems	information retrieval system;information retrieval;digital library;computational complexity;linear model;geometric model	This decade has seen a great deal of progress in the development of information retrieval systems. Unfortunately, we still lack a systematic understanding of the behavior of the systems and their relationship with documents. In this paper we present a completely new approach towards the understanding of the information retrieval systems. Recently, it has been observed that retrieval systems in TREC 6 show some remarkable patterns in retrieving relevant documents. Based on the TREC 6 observations, we introduce a geometric linear model of information retrieval systems. We then apply the model to predict the number of relevant documents by the retrieval systems. The model is also scalable to a much larger data set. Although the model is developed based on the TREC 6 routing test data, I believe it can be readily applicable to other information retrieval systems. In Appendix, we explained a simple and efficient way of making a better system from the existing systems.	geometric modeling;information retrieval;linear model;routing;scalability;test data	Myung Ho Kim	2000	Complex Systems		digital library;relevance;cognitive models of information retrieval;standard boolean model;computer science;theoretical computer science;geometric modeling;linear model;data mining;adversarial information retrieval;term discrimination;computational complexity theory;vector space model;data retrieval;information retrieval;human–computer information retrieval;divergence-from-randomness model	Web+IR	-39.75871447729493	-61.45704492148786	168215
541110bb96a97b5cfc3895ec3382db2b8a45bb40	searching for historical word forms in text databases using spelling-correction methods: reverse error and phonetic coding methods	orthographe;ortografia;banque donnee;evaluation performance;on line processing;text;base donnee;history;performance evaluation;implementation;databank;evaluacion prestacion;evolucion;database;base dato;pregunta documental;methode;texte;orthography;correction automatique;question documentaire;tratamiento en linea;ejecucion;automatic correction;recherche documentaire;text database;banco dato;correccion automatica;recuperacion documental;palabra;query;word;document retrieval;historia;traitement en ligne;texto;metodo;method;histoire;mot;evolution	An increasing volume of historical text is being converted into machine‐readable form so as to allow database searches to be carried out. The age of the material in these databases means that they contain many spellings that are different from those used today. This characteristic means that, once the databases become available for general online access, users will need to be familiar with all of the possible historical spellings for their topic of interest if a search is to be carried out successfully. This paper investigates the use of computational techniques that have been developed for the correction of spelling errors to identify historical spellings of a user's search terms. Two classes of spelling correction method are tested, these being the reverse error and phonetic coding methods. Experiments with words from the Hartlib Papers Collection show that these methods can correctly identify a large number of historical forms of modern‐day word spellings.	phonetic algorithm	Heather J. Rogers;Peter Willett	1991	Journal of Documentation	10.1108/eb026883	document retrieval;method;speech recognition;orthography;computer science;artificial intelligence;word;evolution;implementation;algorithm	NLP	-35.984344313982454	-64.4698755446637	168315
19f6905b53795a50749fafcadd20cd2129963896	mixed-methods approach to measuring user experience in online news interactions	text mining;content filtering;automatic classification	When it comes to evaluating online information experiences, what metrics matter? We conducted a study in which 30 people browsed and selected content within an online news website. Data collected included psychometric scales (User Engagement, Cognitive Absorption, System Usability Scales), self-reported interest in news content, and performance metrics (i.e., reading time, browsing time, total time, number of pages visited, and use of recommended links); a subset of the participants had their physiological responses recorded during the interaction (i.e., heart rate, electrodermal activity, electrocmytogram). Findings demonstrated the concurrent validity of the psychometric scales and interest ratings and revealed that increased time on tasks, number of pages visited, and use of recommended links were not necessarily indicative of greater self-reported engagement, cognitive absorption, or perceived usability. Positive ratings of news content were associated with lower physiological activity. The implications of this research are twofold. First, we propose that user experience is a useful framework for studying online information interactions and will result in a broader conceptualization of information interaction and its evaluation. Second, we advocate a mixed-methods approach to measurement that employs a suite of metrics capable of capturing the pragmatic (e.g., usability) and hedonic (e.g., fun, engagement) aspects of information interactions. We underscore the importance of using multiple measures in information research, because our results emphasize that performance and physiological data must be interpreted in the context of users' subjective experiences.	interaction;user experience	Heather L. O'Brien;Mahria Lebow	2013	JASIST	10.1002/asi.22871	text mining;simulation;computer science;data mining;multimedia;world wide web;information retrieval	HCI	-37.94720043399795	-53.01478511318799	168528
fe8f1fb52280e76e822a906c73d6adca8f2b2bad	comparative evaluation for recommender systems for book recommendations		Recommender System (RS) technology is often used to overcome information overload. Recently, several open-source platforms have been available for the development of RSs. Thus, there is a need to estimate the predictive accuracy of such platforms to select a suitable framework. In this paper we perform an offline comparative evaluation of commonly used recommendation algorithms of collaborative filtering. They are implemented by three popular RS platforms (LensKit, Mahout, and MyMediaLite) using the BookCrossing data set containing 1,149,780 user ratings on books. Our main goal is to find out which of these RSs is the most applicable and has high performance and accuracy on these data. We consider performing a fair objective comparison by benchmarking the evaluation dimensions such as the data set and the evaluation metric. Our evaluation shows the disparity of evaluation results between the RS frameworks. This points to the need of standardizing evaluation methodologies for recommendation algorithms.	algorithm;apache mahout;binocular disparity;book;collaborative filtering;discrepancy function;information overload;online and offline;open-source software;rss;recommender system;reed–solomon error correction	Araek Tashkandi;Lena Wiese;Marcus Baum	2017			recommender system;computer science;knowledge management	Web+IR	-37.63997684673075	-54.88881468381088	168746
f4e3b11284b0809e7e0ca9ca00b27d48ea0e1a0c	effect of transliteration on readability	transliteration;readability;eye tracking	We present our efforts on studying the effect of transliteration, on the human readability. We have tried to explore the effect by studying the changes in the eye-gaze patterns, which are recorded with an eye-tracker during experimentation. We have chosen Hindi and English languages, written in Devanagari and Latin scripts respectively. The participants of the experiments are subjected to transliterated words and asked to speak the word. During this, their eye movements are recorded. The eye-tracking data is later analyzed for eye-fixation trends. Quantitative analysis of fixation count and duration as well as visit count is performed over the areas of interest.		Sambhav Jain;Kunal Sachdeva;Ankush Soni	2013		10.1007/978-3-642-39360-0_6	natural language processing;speech recognition;computer science;communication	NLP	-44.43164025003978	-58.700219403869724	169327
92c23378dfb1f4a6e9f15535a1b5180abfe57115	the art of creating an informative data collection for automated deception detection: a corpus of truths and lies		One of the novel research directions in Natural Language Processing and Machine Learning involves creating and developing methods for automatic discernment of deceptive messages from truthful ones. Mistaking intentionally deceptive pieces of information for authentic ones (true to the writer’s beliefs) can create negative consequences, since our everyday decision-making, actions, and mood are often impacted by information we encounter. Such research is vital today as it aims to develop tools for the automated recognition of deceptive, disingenuous or fake information (the kind intended to create false beliefs or conclusions in the reader’s mind). The ultimate goal is to support truthfulness ratings that signal the trustworthiness of the retrieved information, or alert information seekers to potential deception. To proceed with this agenda, we require elicitation techniques for obtaining samples of both deceptive and truthful messages from study participants in various subject areas. A data collection, or a corpus of truths and lies, should meet certain basic criteria to allow for meaningful analysis and comparison of socio-linguistic behaviors. In this paper we propose solutions and weigh pros and cons of various experimental set-ups in the art of corpus building. The outcomes of three experiments demonstrate certain limitations with using online crowdsourcing for data collection of this type. Incorporating motivation in the task descriptions, and the role of visual context in creating deceptive narratives are other factors that should be addressed in future efforts to build a quality dataset.	categorization;centrality;computer-mediated communication;crowdsourcing;experiment;futures studies;ground truth;information;machine learning;natural language processing;requirements elicitation;tell-tale;text corpus;trust (emotion)	Victoria L. Rubin;Niall Conroy	2012		10.1002/meet.14504901045	artificial intelligence;social psychology;world wide web	NLP	-43.83156022019544	-61.464893702070334	169494
6437ab349b114163eeebb28e33a067e084e69900	spatial learning in a virtual multilevel building: evaluating three exocentric view aids	tiempo respuesta;image tridimensionnelle;reponse temporelle;navegacion informacion;representacion espacial;realite virtuelle;realidad virtual;navigation information;information browsing;virtual reality;response time;acquisition connaissances;exocentric view;user assistance;temps reponse;large scale;assistance utilisateur;navigation aid;spatial learning;time response;knowledge acquisition;asistencia usuario;spatial representation;tridimensional image;representation spatiale;adquisicion de conocimientos;virtual environment;respuesta temporal;imagen tridimensional	The present study explores how the design of the exocentric view aid affects the acquisition of survey knowledge in virtual environments. The exocentric view was provided by either a 3D floor map, a 3D building map or the elevation of viewpoint in air. Participants navigated a virtual multilevel building and their survey knowledge was measured by the judgment of spatial relative direction. The results showed that (1) the accuracy of spatial judgment along the horizontal direction and response time were improved for participants with the exocentric view aid; (2) the accuracy of spatial judgment along the vertical direction was worst in the condition with a 3D floor map; (3) in general participants with a 3D building map performed best. The data suggested that the large scale of an exocentric view aid and the increased number of exocentric perspective through which the spatial layout is observed can facilitate the acquisition of survey knowledge in a virtual building. Potential applications of the findings include the design of a 3D map for navigation in both real and virtual buildings.		Zhiqiang Luo;Wenshu Luo;Christopher D. Wickens;I-Ming Chen	2010	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2010.06.004	spatial memory;computer vision;simulation;computer science;virtual machine;artificial intelligence;operating system;virtual reality;response time	HCI	-44.108752636935954	-55.15734638519592	169572
45ebdd71576e3f030ecc9694b44faa5cedd04a25	an efficient user-oriented clustering of web search results	busqueda informacion;search engine;analyse amas;buscador;keyword;red www;information retrieval;interrogation base donnee;reseau web;interrogacion base datos;palabra clave;mot cle;classification;cluster analysis;internet;recherche information;comportement utilisateur;search result clustering;world wide web;web search;identificateur;analisis cluster;topic identification;user behavior;moteur recherche;synthetic data;database query;clasificacion;identificador;comportamiento usuario;identifier	As a featured function of search engine, clustering display of search results has been proved an efficient way to organize the web resource. However, for a given query, clustering results reached by any user are totally identical. In this paper, we explored a user-friendly clustering scheme that automatically learns users’ interests and accordingly generates interest-centric clustering. The basis of this personal clustering is a keyword based topic identifier. Trained by users’ individual search histories, the identifier provides most of personal topics. Each topic will be the clustering center of the retrieved pages. The scheme proposed distinguishes the functionality of clustering from that of topic identification, which makes the clustering more personal and flexible. To evaluate the proposed scheme, we experimented with sets of synthetic data. The experimental results prove it an effective scheme for search results clustering.	web search engine	Keke Cai;Jiajun Bu;Chun Chen	2005		10.1007/11428862_111	correlation clustering;constrained clustering;data stream clustering;the internet;document clustering;identifier;fuzzy clustering;biological classification;flame clustering;computer science;canopy clustering algorithm;consensus clustering;cure data clustering algorithm;data mining;database;cluster analysis;brown clustering;world wide web;dbscan;biclustering;information retrieval;search engine;affinity propagation;hierarchical clustering of networks;synthetic data;clustering high-dimensional data;conceptual clustering	Web+IR	-35.88423310047261	-58.18466334267848	169743
a74eece319a9acf7fdad0f681614411c5cfab5ed	contextualisation of information retrieval process and document ranking task in web search tools	queries reformulation;google;performance evaluation;search engines;information retrieval;relevance judgements;document ranking;user profiles;web search;static context;dynamic context;bing;documents indexing;yahoo	The work presented in this paper takes place in the field of the contextualisation of information retrieval (IR), and makes contributions to two complementary axes: first improving the search process and then enhancing the evaluation of search tools. For this purpose, we use an effective mechanism in our proposals; the context of the various elements around the search process. The goal is to increase the selectivity of search tools and improve the way in which these tools are evaluated. These proposals have been tested, and the gain in terms of returned results relevance was measured using three search engines (Google, Yahoo and Bing). The results show that taking into account the context in IR increases the relevance of the returned results and thus reduces both documentary noise and silence.	information retrieval;ranking (information retrieval);web search engine	Abdelkrim Bouramoul	2016	IJSSC	10.1504/IJSSC.2016.077970	search engine indexing;ranking;semantic search;computer science;concept search;data mining;okapi bm25;world wide web;information retrieval;search engine;human–computer information retrieval	Web+IR	-34.007245253781846	-55.31740114640531	169911
0d1a6e5451870157b3dfbe25025f396f441a722a	from retrieval status values to probabilities of relevance for advanced ir applications	busqueda informacion;resource selection;evaluation performance;none;probability;formal models;performance evaluation;formal model;information retrieval system;information retrieval;evaluacion prestacion;probability of inference;pertinencia;logistic model;systeme recherche;logistic map;search system;upper bound;probabilistic model;000 informatik informationswissenschaft allgemeine werke 000 informatik wissen systeme;parameter learning;sistema investigacion;recherche information;pertinence;probabilidad;fakultat fur ingenieurwissenschaften informatik und angewandte kognitionswissenschaft;probabilite;modele probabiliste;rsv retrieval status value;evaluation;document retrieval;relevance;retrieval status value;modelo probabilista	Information Retrieval systems typically sort the result with respect to document retrieval status values (RSV). According to the Probability Ranking Principle, this ranking ensures optimum retrieval quality if the RSVs are monotonously increasing with the probabilities of relevance (as e.g. for probabilistic IR models). However, advanced applications like filtering or distributed retrieval require estimates of the actual probability of relevance. The relationship between the RSV of a document and its probability of relevance can be described by a “normalisation” function which maps the retrieval status value onto the probability of relevance (“mapping functions”). In this paper, we explore the use of linear and logistic mapping functions for different retrieval methods. In a series of upper-bound experiments, we compare the approximation quality of the different mapping functions. We also investigate the effect on the resulting retrieval quality in distributed retrieval (only merging, without resource selection). These experiments show that good estimates of the actual probability of relevance can be achieved, and that the logistic model outperforms the linear one. Retrieval quality for distributed retrieval is only slightly improved by using the logistic function.	approximation;document retrieval;experiment;filter (signal processing);information retrieval;logistic map;relevance	Henrik Nottelmann;Norbert Fuhr	2003	Information Retrieval	10.1023/A:1026080230789	document retrieval;statistical model;relevance;logistic map;computer science;artificial intelligence;evaluation;machine learning;probability;data mining;logistic regression;upper and lower bounds;term discrimination;vector space model;information retrieval;statistics;divergence-from-randomness model	Web+IR	-36.5332283679917	-61.53276321404479	170370
02de168c5791edab4ca8a5f63533efe7507bfab4	query expansion with terms selected using lexical cohesion analysis of documents	busqueda informacion;interactive information retrieval;search term;interactive query expansion;analyse lexicale;evaluation performance;performance evaluation;query processing;information retrieval;elargissement requete;evaluacion prestacion;pertinencia;tratamiento lenguaje;terme recherche;cohesion;language processing;recherche information;pertinence;traitement langage;traitement de la requete;lexical cohesion;automatic query expansion;tratamiento pregunta;relevance;query expansion;expansion busqueda;relevance feedback;analisis lexical;lexical analysis;termino busqueda	We present new methods of query expansion using terms that form lexical cohesive links between the contexts of distinct query terms in documents (i.e., words surrounding the query terms in text). The link-forming terms (link-terms) and short snippets of text surrounding them are evaluated in both interactive and automatic query expansion (QE). We explore the effectiveness of snippets in providing context in interactive query expansion, compare query expansion from snippets vs. whole documents, and query expansion following snippet selection vs. full document relevance judgements. The evaluation, conducted on the HARD track data of TREC 2005, suggests that there are considerable advantages in using link-terms and their surrounding short text snippets in QE compared to terms selected from full-texts of documents. 2006 Elsevier Ltd. All rights reserved.	cohesion (computer science);document;query expansion;relevance	Olga Vechtomova;Murat Karamuftuoglu	2007	Inf. Process. Manage.	10.1016/j.ipm.2006.09.004	natural language processing;query optimization;query expansion;web query classification;ranking;relevance;lexical analysis;computer science;cohesion;database;web search query;information retrieval;query language	NLP	-35.5153673441792	-61.866425796710324	170548
42102feb87f31462633fe12daf682838ad3113c1	recent results on fusion of effective retrieval strategies in the same information retrieval system	busqueda informacion;distributed system;systeme reparti;condicion necesaria;information retrieval system;information retrieval;systeme recuperation;data fusion;sistema repartido;recherche documentaire;necessary condition;recherche information;voting;busqueda documental;fusion donnee;voto;document retrieval;condition necessaire;information system;vote;fusion datos;systeme information;sistema informacion;retrieval systems	Prior efforts have shown that data fusion techniques can be used to improve retrieval effectiveness under certain situations. Although the precise conditions necessary for fusion to improve retrieval have not been identified, it is widely believed that as long as component result sets used in fusion have higher relevant overlap than non-relevant overlap, improvements due to fusion can be observed. We show that this is not the case when systemic differences are held constant and different highly effective document retrieval strategies are fused within the same information retrieval system. Furthermore, our experiments have shown that the ratio of relevant to non-relevant overlap is a poor indicator of the likelihood of fusion’s effectiveness, and we propose an alternate hypothesis of what needs to happen in order for fusion to improve retrieval when standard voting/merging algorithms such as CombMNZ are employed.	algorithm;document retrieval;experiment;information retrieval	Steven M. Beitzel;Eric C. Jensen;Abdur Chowdhury;David A. Grossman;Nazli Goharian;Ophir Frieder	2003		10.1007/978-3-540-24610-7_8	document retrieval;voting;computer science;artificial intelligence;data mining;sensor fusion;information retrieval;information system;electoral-vote.com	Web+IR	-36.342618193136104	-62.35651748000815	170651
369d072100ae5cb331aea9630e9ec5d36ebd5e23	assessing author self-citation as a mechanism of relevant knowledge diffusion	latent dirichlet allocation;semantic dissimilarity;author self citation;knowledge diffusion	Author self-citation is a practice that has been historically surrounded by controversy. Although the prevalence of self-citations in different scientific fields has been thoroughly analysed, there is a lack of large scale quantitative research focusing on its usefulness at guiding readers in finding new relevant scientific knowledge. In this work we empirically address this issue. Using as our main corpus the entire set of PLOS journals research articles, we train a topic discovery model able to capture semantic dissimilarity between pairs of articles. By dividing pairs of articles involved in intra-PLOS citations into self-citations (articles linked by a cite which share at least one author) and non-self-citations (articles linked by a cite which share no author), we observe the distribution of semantic dissimilarity between citing and cited papers in both groups. We find that the typical semantic distance between articles involved in self-citations is significantly smaller than the observed one for articles involved in non-self-citations. Additionally, we find that our results are not driven by the fact that authors tend to specialize in particular areas of research, make use of specific research methodologies or simply have particular styles of writing. Overall, assuming shared content as an indicator of relevance and pertinence of citations, our results indicate that self-citations are, in general, useful as a mechanism of knowledge diffusion.	relevance	Ramiro H. Gálvez	2017	Scientometrics	10.1007/s11192-017-2330-1	latent dirichlet allocation;computer science;data science;data mining;world wide web;information retrieval	Web+IR	-39.433522141519816	-62.52772987388242	170664
8bfe74ceac9cae4a9028375c37a3c0fcb80a2c19	multi-document summarization using a clustering-based hybrid strategy	busqueda informacion;analyse amas;linguistique;information retrieval;frase;modelo hibrido;recommandation;resumen;multi document summarization;classification;modele hybride;hybrid model;busca local;sentence;hybrid approach;linguistica;cluster analysis;recherche information;resume;recomendacion;recommendation;analisis cluster;phrase;abstract;local search;clasificacion;recherche locale;linguistics	In this paper we propose a clustering-based hybrid approach for multi-document summarization which integrates sentence clustering, local recommendation and global search. For sentence clustering, we adopt a stabilitybased method which can determine the optimal cluster number automatically. We weight sentences with terms they contain for local sentence recommendation of each cluster. For global selection, we propose a global criterion to evaluate overall performance of a summary. Thus the sentences in the final summary are determined by not only the configuration of individual clusters but also the overall performance. This approach successfully gets top-level performance running on corpus of DUC04.	automatic summarization;cluster analysis;eisenstein's criterion;experiment;multi-document summarization;tree traversal	Nie Yu;Dong-Hong Ji;Lingpeng Yang;Zheng-Yu Niu;Tingting He	2006		10.1007/11880592_53	correlation clustering;speech recognition;multi-document summarization;biological classification;computer science;artificial intelligence;local search;machine learning;data mining;cluster analysis;information retrieval	NLP	-35.41750583095011	-61.07968339415463	171154
5426ffc22cc5a7eb9e6bcb7bf5de794ad9c85bcb	user-centered evaluation of a discovery layer system with google scholar	google scholar;user centered evaluation;search results;relevance;discovery layer	Discovery layer systems allow library users to obtain search results from multiple library resources and view results in a consistent format. The implementation of a discovery layer is expected to simplify users’ workflow of searching for scholarly information. Previous studies on discovery layer systems focused on functionality and content, but not quality of search results from the user’s perspective. The objective of this study was to obtain users’ assessment of search results of a discovery layer system (Ex Libris Primo) and compare that with a widely used scholarly search tool (Google Scholar). Results showed that Primo’s search results relevancy is comparable to Google Scholar, but it received significantly lower usability and preference ratings. A number of usability issues of Primo were also identified from the study. Results of the study are used to improve the interface of Primo and adjust relevancy ranking options. The empirical method of search results assessment and feedback collection used in this study can be extended to similar user-centered system implementation and evaluation efforts.	google scholar;primos;relevance;usability;user-centered design	Tao Zhang	2013		10.1007/978-3-642-39253-5_34	computer science;data mining;world wide web;information retrieval	Web+IR	-37.03646021368984	-53.51217144830701	171239
71fe2dc3be7b579e7a8d9315fb790481114bb523	the necessity of a meeting recording and playback system, and the benefit of topic-level annotations to meeting browsing	busqueda informacion;interfase usuario;groupware;desviacion tipica;effect size;navegacion informacion;selected works;user interface;information retrieval;navigation information;standard deviation;user study;relacion hombre maquina;computer supported cooperative work;information browsing;user survey;prior information;man machine relation;effet dimensionnel;discourse structure;informacion a priori;recherche information;size effect;video recording;ecart type;registro video;bepress;interface utilisateur;relation homme machine;ingenierie simultanee;efecto dimensional;ingenieria simultanea;enregistrement video;collecticiel;information a priori;concurrent engineering	Much work in the area of Computer Supported Cooperative Work (CSCW) has targeted the problem of supporting meetings between collaborators who are non-collocated, enabling meetings to transcend boundaries of space. In this paper, we explore the beginnings of a proposed solution for allowing meetings to transcend time as well. The need for such a solution is motivated by a user survey in which busy professionals are questioned about meetings they have either missed or forgotten the important details about after the fact. Our proposed solution allows these professionals to transcend time in a sense by revisiting a recorded meeting that has been structured for quick retrieval of sought information. Such a solution supports complete recovery of prior discussions, allowing needed information to be retrieved quickly, and thus potentially facilitating the effective continuation of discussions from the past. We evaluate the proposed solution with a formal user study in which we measure the impact of the proposed structural annotations on retrieval of information. The results of the study show that participants took significantly less time to retrieve the answers when they had access to discourse structure based annotation than in a control condition in which they had access only to unannotated video recordings (p < 0.01, effect size 0.94 standard deviations).	browsing;computer-supported cooperative work;continuation;usability testing;user experience professionals association	Satanjeev Banerjee;Carolyn Penstein Rosé;Alexander I. Rudnicky	2005		10.1007/11555261_52	simulation;computer science;artificial intelligence;computer-supported cooperative work;database;distributed computing;multimedia;standard deviation;user interface;world wide web;computer security;effect size;algorithm;statistics;concurrent engineering	HCI	-40.01298078118289	-54.76249327507435	172071
08af513b4a736a918917da937d49db4aac8e2bf5	towards distributed information retrieval in the semantic web: query reformulation using the omap framework	busqueda informacion;resource selection;terminologie;distributed system;data transmission;agregacion;ontologie;learning algorithm;systeme reparti;terminologia;correspondance ontologie;query reformulation;red www;ontology mapping;resource allocation;information retrieval;web semantique;metodo formal;interrogation base donnee;reseau web;methode formelle;service web;interrogacion base datos;semantics;intelligence artificielle;algorithme apprentissage;data fusion;probabilistic approach;web service;distributed information retrieval;semantica;semantique;classification;maximum matching;aggregation;formal method;sistema repartido;machine learning;recherche information;enfoque probabilista;approche probabiliste;web semantica;transmission donnee;rank aggregation;semantic web;agregation;distributed search;artificial intelligence;world wide web;ontologia;terminology;asignacion recurso;inteligencia artificial;allocation ressource;algoritmo aprendizaje;ontology;correspondencia ontologia;database query;clasificacion;transmision datos;servicio web;ontology alignment	This paper introduces a general methodology for performing distributed search in the Semantic Web. We propose to define this task as a three steps process, namely resource selection, query reformulation/ontology alignment and rank aggregation/data fusion. For the second problem, we have implemented oMAP, a formal framework for automatically aligning OWL ontologies. In oMAP, different components are combined for finding suitable mapping candidates (together with their weights), and the set of rules with maximum matching probability is selected. Among these components, traditional terminological-based classifiers, machine learning-based classifiers and a new classifier using the structure and the semantics of the OWL ontologies are proposed. oMAP has been evaluated on international test sets.	distributed web crawling;information retrieval;machine learning;matching (graph theory);omap;ontology (information science);ontology alignment;semantic web;web ontology language	Umberto Straccia;Raphaël Troncy	2006		10.1007/11762256_29	web service;ontology alignment;semantic integration;biological classification;resource allocation;computer science;artificial intelligence;semantic web;ontology;data mining;database;semantics;sensor fusion;linguistics;terminology;world wide web;matching;data transmission	Web+IR	-35.3598020842799	-60.40911972206033	172152
a29ce139586fc4212b4103c6662e947cabdc81a2	podcast search: user goals and retrieval technologies	busqueda informacion;etude utilisateur;search engine;baladodiffusion;user needs;online survey;audio media;metadata;search engines;information retrieval;user study;estudio usuario;search strategy;user studies;universiteitsbibliotheek;indexing method;automatic speech recognition;recherche information;busqueda por contenido;indexation;strategie recherche;metadonnee;speech recognition;metadatos;content based retrieval;diary study;recherche par contenu;estrategia investigacion;design methodology	Purpose - This research aims to identify users' goals and strategies when searching for podcasts and their impact on the design of podcast retrieval technology. In particular, the paper seeks to explore the potential to address user goals with indexing based on podcast metadata and automatic speech recognition (ASR) transcripts. Design/methodology/approach - The paper conducted a user study to obtain an overview of podcast search behaviour and goals, using a multi-method approach of an online survey, a diary study, and contextual interviews. In a subsequent podcast retrieval experiment, the paper investigated the retrieval performance of the two choices of indexing features for search goals identified during the study. Findings - The paper found that study participants used a variety of search strategies, partially influenced by available tools and their perceptions of these tools. Furthermore the experimental results revealed that retrieval using ASR transcripts performed significantly better than metadata-based searching. However, a detailed result analysis suggested that the efficacy of the indexing methods was search-goal dependent. Research limitations/implications - The research constitutes a step towards a future framework for investigating user needs and addressing them in an experimental set-up. It was primarily qualitative and exploratory in nature. Practical implications - Podcast search engines require evidence about suitable indexing methods in order to make an informed decision concerning whether it is worth the resources to generate speech recognition transcripts. Originality/value - Systematic studies of podcast searching have not previously been reported. Investigations of this kind hold the potential to optimise podcast retrieval in the long term.		Jana Besser;Martha Larson;Katja Hofmann	2010	Online Information Review	10.1108/14684521011054053	computer science;database;multimedia;world wide web;information retrieval;search engine	Web+IR	-36.02040799905766	-55.16749974376671	172203
197cd966c90ec8f7312cdb6d8b15c77e7aef5350	supporting literature exploration with granular knowledge structures	granular computing;literature exploration;granular knowledge structures;research method;support system;research methods;research support systems;knowledge structure;rough set	Reading and literature exploration are important tasks of scientific research. However, conventional retrieval systems provide limited support for these tasks by concentrating on identifying relevant materials. New generation systems should provide additional support functionality by focusing on analyzing and organizing the retrieved materials. A framework of literature exploration support systems is proposed. Techniques of granular computing are used to construct granular knowledge structures from the contents, structures, and usages of scientific documents. The granular knowledge structures provide a high level understanding of scientific literature and hints regarding what has been done and what needs to be done. As a demonstration, we examine granular knowledge structures obtained from an analysis of papers from two rough sets related conferences.	granular computing;high-level programming language;organizing (structure);rough set;scientific literature;video post-processing;web search engine	Yiyu Yao;Yi Zeng;Ning Zhong	2007		10.1007/978-3-540-72530-5_21	rough set;granular computing;computer science;artificial intelligence;data science;machine learning;data mining;research	HCI	-34.38648538877559	-56.204018432075635	172309
4ea1fec789af14a66d83cddc2d865f5f9888ed8e	the potential of collaborative document evaluation for science	citation analysis;alternative;quality evaluation;research policy;scientific communication;inproceedings;open peer review;open peer review citation analysis alternative research policy	Peer review and citation analysis are the two most common approaches for quality evaluations of scientific publications, although they are subject to criticism for various reasons. This paper outlines the problems of citation analysis and peer review and introduces Collaborative Document Evaluation as a supplement or possibly even a substitute. Collaborative Document Evaluation aims to enable the readers of publications to act as peer reviewers and share their evaluations in the form of ratings, annotations, links and classifications via the internet. In addition, Collaborative Document Evaluation might well enhance the search for publications. In this paper the implications of Collaborative Document Evaluation for the scientific community are discussed and questions are asked as to how to create incentives for scientists to participate.	citation analysis;scientific literature	Jöran Beel;Bela Gipp	2008		10.1007/978-3-540-89533-6_48	peer review;computer science;data mining;citation analysis;world wide web	Web+IR	-37.70889651482696	-54.15376816803176	172658
54ba80726741a97884f2ce76a3b64fac2326dbaf	adapting to new technology in the operating room.	medecin;operating room;adaptacion;new technology;informatica biomedical;biomedical data processing;heart;ergonomia;physician;genie biomedical;relacion hombre maquina;informatique biomedicale;hombre;man machine relation;ergonomie;coeur;anesthesie;biomedical engineering;monitoring;corazon;adaptation;medico;chirurgie;human;computer aid;surgery;asistencia ordenador;cirugia;ingenieria biomedica;relation homme machine;monitorage;monitoreo;ergonomics;assistance ordinateur;anestesia;anesthesia;homme	The effects of new technology on human performance in domains such as anesthesiology, commercial aviation, and nuclear power operations remain controversial. To study the impact of new technology on skilled practitioner performance, we observed the introduction of a new, highly integrated, microprocessor-based physiological monitoring system for use in cardiac anesthesia. The new computer system differed from its predecessors in method of display, human interface, level of integration, and automation of functions. A process-tracing technique was used to examine physician-computer interaction in the context of 22 anesthesia procedures for cardiothoracic surgery, most of which involved cardiopulmonary bypass. Practitioners experienced a series of problems with the new computer system. Computer system characteristics relative to the specific context of cardiac surgery created new cognitive and physical burdens that tended to congregate at times of high demand, the characteristic feature of clumsy automation. Practitioners as individuals and as a group tried to overcome these problems by adapting the computer system (system tailoring) and their behavior (task tailoring) as they learned about the interaction between characteristics of the new system and characteristics of their field of practice.		Richard I. Cook;David D. Woods	1996	Human factors	10.1518/001872096778827224	psychology;simulation;medicine;human–computer interaction;computer science;engineering;artificial intelligence;human factors and ergonomics;heart;adaptation;mechanical engineering	HCI	-46.11384644122082	-55.90971395907378	172669
37901360b76b7948e0483d31a15b2a7aabfe0a4f	information access to historical documents from the early new high german period	historical documents information access early new high german historical language information retrieval word similarity approximate matching;004;information retrieval;spectrum;information access;data mining;approximate matching;language change	With the new interest in historical documents insight grew that electronic access to these texts causes many specific problems. In the first part of the paper we survey the present role of digital historical documents. After collecting central facts and observations on historical language change we comment on the difficulties that result for retrieval and data mining on historical texts. In the second part of the paper we report on our own work in the area with a focus on special matching strategies that help to relate modern language keywords with old variants. The basis of our studies is a collection of documents from the Early New High German period. These texts come with a very rich spectrum on word variants and spelling variations.	approximation algorithm;conception;data mining;database index;historical document;information access;optical character recognition;plausibility structure;printing;vocabulary;xml retrieval	Andreas Hauser;Markus Heller;Elisabeth Leiss;Klaus U. Schulz;Christiane Wanzeck	2006			natural language processing;computer science;data mining;information retrieval	Web+IR	-35.342278861435375	-64.21454209612392	173274
61950ed70d3c62b1554ea6e01794ba2262c99718	motor learning affects car-to-driver handover in automated vehicles		Vehicles in the foreseeable future will be required to transition between autonomous driving (without human involvement) and full human control. During this transition period, the human, who has not been actively engaged in the driving process, must resume the motor control necessary to steer the car. The in-car study presented here demonstrates that when human drivers are presented with a steering behavior that is different from the last time they were in control, specifically the ratio of hand wheel angle to road wheel angle (emulating a change in vehicle speed), they undergo a significant period of adaptation before they return to their previous steering behavior. However, drivers do not require an adaptation period to return to previous driving behavior after changes in steering torque. These findings have implications for the design of vehicles that transition from automated to manual driving and for understanding of human motor control in real-world tasks.	autonomous car;emulator;steering wheel;torque	Lene K. Harbott;Ilana Nisky;Selina Pan;Allison M. Okamura;J. Christian Gerdes	2016	Science Robotics	10.1126/scirobotics.aah5682	control engineering;simulation;engineering;automotive engineering	HCI	-47.581750600486856	-52.67639573040134	173449
305fc616629bdc6e4513c8357b44099b245eb2de	sistemas de recuperación de información adaptados al dominio biomédico	explotacion texto;biomedicina;bioner;information retrieval system;text mining;information retrieval;recuperacion de informacion;tratamiento del lenguaje natural;fouille texte;proceso del lenguaje natural;sistema de recuperacion de informacion;ls search engines;informacion documentacion;systeme de recherche d information;ll automated language processing;lm automatic text retrieval;biomedecine;biomedicine;grupo a;bionlp;ciencias sociales;grupo b;natural language processing;traitement du langage naturel;nlp	The terminology used in Biomedicine shows lexical peculiarities that have required the elaboration of terminological resources and information retrieval systems with specific functionalities. The main characteristics are the high rates of synonymy and homonymy, due to phenomena such as the proliferation of polysemic acronyms and their interaction with common language. Information retrieval systems in the biomedical domain use techniques oriented to the treatment of these lexical peculiarities. In this paper we review some of the techniques used in this domain, such as the application of Natural Language Processing (BioNLP), the incorporation of lexical-semantic resources, and the application of Named Entity Recognition (BioNER). Finally, we present the evaluation methods adopted to assess the suitability of these techniques for retrieving biomedical resources.	biomedical text mining;consensus (computer science);information retrieval;internet slang;knowledge organization system;machine learning;named-entity recognition;natural language processing;text corpus;usability	Mónica Marrero;Sonia Sánchez-Cuadrado;Julián Urbano;Jorge Morato;José A. Moreiro	2010	CoRR	10.3145/epi.2010.may.04	biomedicine;text mining;computer science	NLP	-36.33472841345918	-65.01984006177099	173771
f3db75fa913ca6a0c3d1c42dc056157e0f16d6a7	visual distractions effects on reading in digital environments: a comparison of first and second english language readers	l1 readers;distractions;conference paper;reading behaviour;eye tracking;l2 readers;eye gaze	Reading in digital environments can be very distracting. Using eye-tracking technology, we investigate if text readability affects distraction rate, eye movements, and reading comprehension in a visually distracting digital environment. We compared an easy-to-read text and a hard-to-read text on both first language English (L1) readers and second language English (L2) readers. Text readability was measured using the standard readability formulas such as the Flesch-Kincaid Grade level. Results show that text readability does cause different eye movements and produce reading comprehension results that deviate from what is normally expected. Readers are affected more by the distractions when reading easy-to-read text compared to when reading hard-to-read text. Furthermore, L2 readers are affected more than L1 readers. These findings can be used in the design of eLearning materials when distractions cannot be overcome.	digital environment;e-reader;eye tracking	Leana Copeland;Tamás D. Gedeon	2015		10.1145/2838739.2838762	speech recognition;eye tracking;computer science;multimedia;reading	HCI	-44.791293199549074	-58.69853608423394	174039
a5be514bb702e0abfeeb0012d3c2cae6f69f4e70	analytic prediction of medical document retrieval system performance	text fragments;relevance judgments medical document retrieval system performance historical data formal probabilistic methods artificial intelligence statistical decision theoretic considerations text fragments hypertext system;hypertext system;medical administrative data processing;information retrieval system;probabilistic method;information retrieval;formal probabilistic methods;statistical decision theoretic considerations;system performance;hypertext systems;performance analysis system performance costs information retrieval predictive models feedback artificial intelligence spatial databases moon hypertext systems;artificial intelligent;feedback;relevance judgments;moon;decision theoretic;spatial databases;performance analysis;information retrieval system evaluation;artificial intelligence;predictive models;document retrieval;information need;medical administrative data processing information retrieval system evaluation;medical document retrieval system performance;historical data	The performance of medical information retrieval systems is measured using historical data or predicted using formal probabilistic methods derived from artificial intelligence and statistical decision theoretic considerations. Technique have been described that assist the searcher with a query or information need by providing graphs showing the quality of past retrieval performance for that specific query, as well as expected future performance. Documents or text fragments (from a hypertext system) are ranked for possible presentation to the searcher based on the document of fragment's odds of relevance. The expected performance is computed from knowledge gained from relevance judgments provided by the searcher about the quality of the retrieved documents, as well as any system knowledge available about possible initial values of parameters of distributions describing the occurrence of features of relevance and all text fragments. The individual documents do not need to be examined to predict performance. >	document retrieval	Robert M. Losee;Sung Been Moon	1990		10.1109/CBMSYS.1990.109436	document retrieval;information needs;relevance;computer science;natural satellite;data science;probabilistic method;machine learning;data mining;feedback;database;predictive modelling;information retrieval	Vision	-37.30422786204869	-61.28883102273349	174240
cbd83a1f7fc38498043f29eec97664d7b03d3d0e	corpus-based cross-language information retrieval in retrieval of highly relevant documents	swedish;finnois;evaluation performance;translating;performance evaluation;information retrieval system;query processing;estudio comparativo;evaluacion prestacion;finlandes;vector space;finnish;pertinencia;coeficiente recuperacion;sistema de recuperacion de informacion;etude comparative;traduction;accuracy;systeme de recherche d information;precision;pertinence;comparative study;traitement de la requete;sueco;traduccion;tratamiento pregunta;espace vectoriel;relevance;multilinguisme;suedois;recall ratio;espacio vectorial;cross language information retrieval;multilingualism;taux rappel;multilinguismo	Abstract#R##N##R##N#Information retrieval systems' ability to retrieve highly relevant documents has become more and more important in the age of extremely large collections, such as the World Wide Web (WWW). The authors' aim was to find out how corpus-based cross-language information retrieval (CLIR) manages in retrieving highly relevant documents. They created a Finnish–Swedish comparable corpus from two loosely related document collections and used it as a source of knowledge for query translation. Finnish test queries were translated into Swedish and run against a Swedish test collection. Graded relevance assessments were used in evaluating the results and three relevance criterion levels—liberal, regular, and stringent—were applied. The runs were also evaluated with generalized recall and precision, which weight the retrieved documents according to their relevance level. The performance of the Comparable Corpus Translation system (COCOT) was compared to that of a dictionary-based query translation program; the two translation methods were also combined. The results indicate that corpus-based CLIR performs particularly well with highly relevant documents. In average precision, COCOT even matched the monolingual baseline on the highest relevance level. The performance of the different query translation methods was further analyzed by finding out reasons for poor rankings of highly relevant documents.	cross-language information retrieval	Tuomas Talvensaari;Martti Juhola;Jorma Laurikkala;Kalervo Järvelin	2007	JASIST	10.1002/asi.20495	natural language processing;ranking;speech recognition;relevance;accuracy and precision;information retrieval;statistics	Web+IR	-35.67766581598456	-62.61273811625755	174264
8d336f0a979a29115bbcbfeb7482bde7f509c628	audio music similarity and retrieval: evaluation power and stability	music similarity	In this paper we analyze the reliability of the res ults in the evaluation of Audio Music Similarity and Retrieval systems. We focus on the power and stability of the evaluati on, that is, how often a significant difference is found bet w en systems and how often these significant differences ar e incorrect. We study the effect of using different effect iveness measures with different sets of relevance judgments , for varying number of queries and alternative statistic al procedures. Different measures are shown to behave simil arly overall, though some are much more sensitive and st able than others. The use of different statistical proce dur s does improve the reliability of the results, and it allo ws using as little as half the number of queries currently used in MIREX evaluations while still offering very similar relia bility levels. We also conclude that experimenters can be very con fident that if a significant difference is found between t wo systems, the difference is indeed real.	naruto shippuden: clash of ninja revolution 3;relevance;sound card	Julián Urbano;Diego Martín;Mónica Marrero;Jorge Morato	2011			speech recognition;computer science;data mining;mathematics;world wide web;information retrieval	Web+IR	-37.852147782532334	-61.728416408239454	174402
8dd5a25ac57c19b6280445fbd1ee1d95d3af00f0	the university of new south wales at geoclef 2006	cross language evaluation forum;genetic program;genetic programming;ranking function;geographic knowledge base;learning methods;geo textual indexing;indexation;new south wales;geographic information retrieval;document retrieval;geographical indication;boolean model;knowledge base	This paper describes our participation in the GeoCLEF monolingual English task of the Cross Language Evaluation Forum 2006. The main objective of this study is to evaluate the retrieve performance of our geographic information retrieval system. The system consists of four modules: the geographic knowledge base that provides information about important geographic entities around the world and relationships between them; the indexing module that creates and maintains textual and geographic indices for document collections; the document retrieval module that uses the Boolean model to retrieve documents that meet both textual and geographic criteria; and the ranking module that ranks retrieved results based on ranking functions learned using Genetic Programming. Experiments results show that the geographic knowledge base, the indexing module and the retrieval module are useful for geographic information retrieval tasks, but the proposed ranking function learning method doesn't work well.	algorithm;computation;document retrieval;entity;experiment;genetic programming;information retrieval;knowledge base;parallel computing;ranking (information retrieval);relevance;standard boolean model;windows rally	You-Heng Hu;Linlin Ge	2006		10.1007/978-3-540-74999-8_115	natural language processing;document retrieval;genetic programming;knowledge base;boolean model;computer science;data mining;database;world wide web;information retrieval	Web+IR	-34.74036030488181	-61.5442179336689	174854
978ca8d04bc6cd15b6bedbdade454953b2cb589b	word weighting based on user's browsing history	keyword;navegacion informacion;web pages;red www;navigation information;weighting;reseau web;information browsing;pertinencia;cache memory;palabra clave;information access;mot cle;ponderacion;antememoria;support system;antememoire;internet;pertinence;comportement utilisateur;keyword extraction;acces information;world wide web;acceso informacion;ponderation;user behavior;relevance;proxy server;comportamiento usuario;user model	We developed a word-weighting algorithm based on the information access history of a user. The information access history of a user is represented as a set of words, and is considered to be a user model. We weight words in a document according to their relevancy to the user model. The relevancy is measured by the biases of co-occurrence, called IRM (Interest Relevance Measure), between a word in a document and words in the user model. We evaluate IRM through a constructed browsing support system, which monitors word occurrences on the user’s browsed Web pages and highlights keywords in the current page. Our system consists of three components: a proxy server that monitors access to the Web, a frequency server that stores the frequencies of words appearing on the accessed Web pages, and a keyword extraction module.	algorithm;browsing;information access;information rights management;keyword extraction;personalization;protologism;proxy server;relevance;server (computing);user modeling;web page;world wide web	Yutaka Matsuo	2003		10.1007/3-540-44963-9_7	the internet;user modeling;relevance;cpu cache;computer science;operating system;web page;database;weighting;world wide web;information retrieval	Web+IR	-37.628803495196536	-58.35823705794147	175042
a92cca580adf19a5a65017ce158ef0a33ee095b8	predicting document retrieval system performance: an expected precision measure	modele mathematique;information retrieval;performance;predictive measurement;pertinencia;automatisation;modelo matematico;automatizacion;system performance;graphs;feedback;recherche documentaire;user satisfaction information;pertinence;recuperacion documental;mathematical model;evaluation;document retrieval;relevance;evaluacion;relevance information retrieval;models;documentation;automation	Document retrieval systems based on probabilistic or fuzzy logic considerations may order documents for retrieval. Users then examine the ordered documents until deciding to stop, based on the estimate that the highest ranked unretrieved document will be most economically not retrieved. We propose an expected precision measure useful in estimating the performance expected if yet unretrieved documents were to be retrieved, providing information that may result in more economical stopping decisions. An expected precision graph, comparin, 0 expected precision versus document rank, may graphically display the relative expected precision of retrieved and unretrieved documents and may be used as a stopping aid for online searching of text data bases. The effectiveness of relevance feedback may be examined as a search progresses. Expected precision values may also be used as a cutoff for systems consistent with probabilistic models operating in batch modes. Techniques are given for computing the best expected precision obtainable and the expected precision of subject neutral documents.	database;document retrieval;fuzzy logic;relevance feedback;text corpus	Robert M. Losee	1987	Inf. Process. Manage.	10.1016/0306-4573(87)90057-4	document retrieval;relevance;performance;documentation;computer science;evaluation;automation;machine learning;mathematical model;data mining;feedback;world wide web;information retrieval;statistics	Web+IR	-36.63259659400484	-61.37366233364416	175289
4c12a8b7b419e55e2f4fb8dca389c25fd5b58983	mental search in image databases: implicit versus explicit content query		In comparison with the classic query-by-example paradigm, the “mental image search” paradigm lifts the strong assumption that the user has a relevant example at hand to start the search. In this chapter, we review different methods that implement this paradigm, originating from both the content-based image retrieval and the object recognition fields. In particular, we present two complementary methods. The first one allows the user to reach the target mental image by relevance feedback, using a Bayesian inference. The second one lets the user specify the mental image visual composition from an automatically generated visual thesaurus of segmented regions. In this scenario, the user formulates the query with an explicit representation of the image content, as opposed to the first scenario which accommodates an implicit representation. In terms of usage, we will show that the second approach is particularly suitable when the mental image has a well-defined visual composition. On the other hand, the Bayesian approach can handle more “semantic” queries, such as emotions for which the visual characterization is more implicit.		Simon P. Wilson;Julien Fauqueur;Nozha Boujemaa	2008		10.1007/978-3-540-75171-7_8	query expansion;theoretical computer science;multimedia;information retrieval;query language	DB	-33.71773552039412	-58.9026677471487	175347
b993370e67a3d54969be6dc93eb1e40de8f1b425	a framework for individual cognitive performance assessment in real-time for elderly users		Changing cognitive performance in human elderly users requires a real-time assessment of current performance to provide appropriate, i.e. adaptive, assistance without bothering them. In the presented approach, the assessment of cognitive performance is done by simulating the user’s cognitive functions through a computational cognitive model that is highly individualized for the specific user. The behavior of this model is evaluated with respect to the current cognitive performance by employing computerized psychological tests that allow a realtime assessment of cognitive performance with respect to a range of cognitive functions. By doing this, a realtime assessment is possible without involving the user in explicit performance tests.	cognition;cognitive model;computation;real-time clock;real-time transcription;simulation	Sandra H. Budde;Thomas Barkowsky	2008			effects of sleep deprivation on cognitive performance;machine learning;computer science;artificial intelligence;cognitive model;cognition;psychological testing	HCI	-46.89809550875507	-55.61241652569738	175385
70bab8ad54d7cc6f07b6ac0a9130307f73824932	testing google scholar bibliographic data: estimating error rates for google scholar citation parsing		We present some systematic tests of the quality of bibliographic data exports available from Google Scholar. While data quality is good for journal articles and conference proceedings, books and edited collections are often wrongly described or have incomplete data. We identify a particular problem with material from online repositories.		David Zeitlyn;Megan Beardmore-Herd	2018	First Monday		multimedia;information retrieval;parsing;computer science;citation;data quality	ML	-39.077384841313936	-65.33176801005965	175456
9e0fe0ff12d3b2091aa27cf98d75b678ae3b4eff	feedback-driven structural query expansion for ranked retrieval of xml data	quality assurance;search engine;buscador;text;base donnee;keyword;elargissement requete;xml language;interrogation base donnee;database;pertinencia;interrogacion base datos;base dato;palabra clave;texte;mot cle;xml retrieval;feasibility;aseguracion calidad;pertinence;retroaction pertinence;relevance;information system;moteur recherche;query expansion;expansion busqueda;requete structurelle;texto;assurance qualite;relevance feedback;requerimiento estructura;content based retrieval;database query;systeme information;practicabilidad;recherche par contenu;langage xml;lenguaje xml;faisabilite;structural query;sistema informacion	Relevance Feedback is an important way to enhance retrieval quality by integrating relevance information provided by a user. In XML retrieval, feedback engines usually generate an expanded query from the content of elements marked as relevant or nonrelevant. This approach that is inspired by text-based IR completely ignores the semistructured nature of XML. This paper makes the important step from content-based to structural feedback. It presents an integrated solution for expanding keyword queries with new content, path, and document constraints. An extensible framework evaluates such query conditions with existing keyword-based XML search engines while allowing to easily integrate new dimensions of feedback. Extensive experiments with the established INEX benchmark show the feasibility of our approach.	benchmark (computing);experiment;query expansion;relevance feedback;text-based (computing);web search engine;xml retrieval	Ralf Schenkel;Martin Theobald	2006		10.1007/11687238_22	xml validation;quality assurance;query expansion;xml;relevance;computer science;document structure description;xml framework;xml schema;database;world wide web;information retrieval;information system;search engine;efficient xml interchange	DB	-36.02647802478323	-58.62268745258745	175553
1fad2ae026f545216e2a62eeaf806945067cba7c	statistical reform in information retrieval?	algorithms;design;experimentation;measurement;information search and retrieval;performance	"""IR revolves around evaluation. Therefore, IR researchers should employ sound evaluation practices. Nowadays many of us know that statistical significance testing is not enough, but not all of us know exactly what to do about it. This paper provides suggestions on how to report effect sizes and confidence intervals along with p-values, in the context of comparing IR systems using test collections. Hopefully, these practices will make IR papers more informative, and help researchers form more reliable conclusions that """"add up."""" Finally, I pose a specific question for the IR community: should IR journal editors and SIGIR PC chairs require (rather than encourage) reporting of effect sizes and confidence intervals."""	information retrieval;lazy evaluation	Tetsuya Sakai	2014	SIGIR Forum	10.1145/2641383.2641385	computer science;data science;data mining;world wide web;information retrieval	Web+IR	-40.45401792376899	-64.16063336131026	175626
9c8e0f3a7bf2ee4823ee8fb30006e500b79af3dc	"""on the correction of """"old"""" omitted citations by bibliometric databases"""		Omitted citations – i.e., missing links between a cited paper and the corresponding citing papers – are the main consequence of several bibliometric-database errors. To reduce these errors, databases may undertake two actions: (i) improving the control of the (new) papers to be indexed, i.e., limiting the introduction of “new” dirty data, and (ii) detecting and correcting errors in the papers already indexed by the database, i.e., cleaning “old” dirty data. The latter action is probably more complicated, as it requires the application of suitable errordetection procedures to a huge amount of data. Based on an extensive sample of scientific papers in the Engineering-Manufacturing field, this study focuses on old dirty data in the Scopus and WoS databases. To this purpose, a recent automated algorithm for estimating the omitted-citation rate of databases is applied to the same sample of papers, but in three different-time sessions. A database’s ability to clean the old dirty data is evaluated considering the variations in the omitted-citation rate from session to session. The major outcomes of this study are that: (i) both databases slowly correct old omitted citations, and (ii) a small portion of initially corrected citations can surprisingly come off from databases over time. Conference Topic Data Accuracy and disambiguation Introduction An important branch of the bibliometric literature examines errors in bibliometric databases. Several studies show that the major consequence of database errors is represented by omitted citations, i.e., citations that should be ascribed to a certain (cited) paper but, for some reason, are lost (Moed, 2005; Buchanan, 2006; Jacsó, 2006, Li et al., 2010; Olensky, 2013). Franceschini et al. (2013) proposed an automated algorithm for estimating the omittedcitation rate of bibliometric databases. This algorithm requires the combined use of two or more bibliometric databases and is based upon the hypothesis that the mismatch between the citations occurring in one database and another one is evidence of possible errors/omissions. In a further study by Franceschini et al. (2014), this algorithm was applied to a relatively large set of publications, showing that, depending on the bibliometric database in use (Scopus or WoS), omitted citations are not distributed uniformly among publishers; e.g., regarding the publications in the Engineering-Manufacturing field, citations from papers published by Wiley-Blackwell are more likely to be omitted by Scopus, while those from papers published by ASME (American Society of Mechanical Engineers) are more likely to be omitted by WoS. A reason behind this result is that some editorial styles imposed by certain publishers can probably hamper the correct identification of the cited papers by some databases. The presence of database errors, as well as journal coverage or author disambiguation, is probably one of the major concerns of database administrators. In the authors’ opinion, database administrators may undertake two actions for reducing database errors: 1. Limiting the introduction of “new” dirty data in a database, i.e., errors concerning new papers to be indexed; 2. Cleaning “old” dirty data, i.e., errors concerning papers/journals already indexed by a database.	algorithm;bibliometrics;blackwell (series);database;dirty data;john d. wiley;plasma cleaning;scientific literature;scopus;sensor;word lists by frequency;word-sense disambiguation	Fiorenzo Franceschini;Domenico A. Maisano;Luca Mastrogiacomo	2015			computer science;data mining;world wide web;information retrieval	DB	-39.991426539233096	-64.73963671281551	175983
99ca336ca67df148aeed3614be00f1413416699f	utd hltri at trec 2017: precision medicine track		In this paper, we describe the system designed for the TREC 2017 Precision Medicine track by the University of Texas at Dallas (UTD) Human Language Technology Research Institute (HLTRI). Our system incorporates an aspect-based retrieval paradigm wherein each of the four structured components of the topic is cast as a separate aspect, along with two “hidden” aspects encoding the need that retrieved documents be within the domain of precision medicine and that retrieved documents have a focus on treatment. To this end, we construct knowledge graph encoding the relationships between drugs, genes, and mutations. Our experiments reveal that the aspect-based approach leads to improved quality of retrieved scientific articles and clinical trials.	experiment;knowledge graph;language technology;precision medicine;programming paradigm;scientific literature;uniform theory of diffraction	Travis R. Goodwin;Michael A. Skinner;Sanda M. Harabagiu	2017			precision medicine;information retrieval;computer science	NLP	-46.538070258852485	-65.42719585925812	176029
f17036a08d84be25b50048a711de0f62bf31d995	topic exploration and distillation for web search by a similarity-based analysis	search engine;buscador;lien hypertexte;red www;enlace hipertexto;reseau web;hyperlink;pregunta documental;similitude;question documentaire;internet;similarity;query;world wide web;web search;similitud;moteur recherche;point of view;algorithle hits	Topic distillation is the process of finding representative pages relevant to a given query. Well-known topic distillation approaches such as the HITS algorithm have shown to be useful in identifying high quality pages. In this paper, we attempt to revisit the behaviour of HITS from a different point of view. Namely, a similarity-based analysis model is applied to observing the distillation procedure. By defining a generalized similarity, an algorithm is proposed, which can improve the quality of distillation using only hyperlinks. A topic exploration function is also integrated into the algorithm framework, which enables end-users to search less popular topics when multi-topics are involved in queries. The experimental results reveal two benefits from the new algorithm: the improvement of distillation quality without utilizing any content information of pages, and an additional ability to explore the topics emerging in the query results.	web search engine	Xiaoyu Wang;Zhiguo Lu;Aoying Zhou	2002		10.1007/3-540-45703-8_29	the internet;similarity;computer science;artificial intelligence;similitude;machine learning;data mining;database;hyperlink;world wide web;information retrieval;search engine	Web+IR	-35.73555301440428	-57.95338809415465	176058
0f5702eddbdfa688c0a003abbdabe1141ee5b148	web-based information access: multilingual automatic authoring	data mining large scale systems space technology streaming media information retrieval computer science production systems natural language processing prototypes multimedia systems;information resources;information retrieval;text analysis;natural languages;information access;authoring systems;hypermedia;settore ing inf 05 sistemi di elaborazione delle informazioni;text analysis information resources information retrieval natural languages authoring systems hypermedia;news agencies multilingual information categorisation world wide web based information access multilingual automatic authoring similar documents document languages electronic information news streams text processing text structuring cross lingual hypertext links natural language processing information extraction monolingual authoring namic system;natural language processing	The needs for managing similar documents in different languages increases with the growing amounts of electronic information available in documents of the same type (e.g. news streams). This paper proposes a viable approach to information access emphasizing the hypertextual paradigm in a multilingual framework. This task of processing/structuring text so that cross-lingual hypertext links are generated will be called Multilingual Authoring (MA). Methods from Natural Language Processing, especially Information Extraction, to both monolingual and Multilingua l Authoring will be described and a general architecture for MA will be defined. Effectiveness of the proposed approach will be discussed the description of the NAMIC prototype system1	common object request broker architecture;hyperlink;hypertext;information access;information extraction;java;natural language processing;personalization;programming paradigm;prototype;scalability;software development process;software engineering;string searching algorithm;web service	Roberto Basili;Maria Teresa Pazienza;Fabio Massimo Zanzotto	2002		10.1109/ITCC.2002.1000446	natural language processing;computer science;world wide web;information extraction;information retrieval	Web+IR	-34.60723961601139	-65.46248038174411	176359
0626e7919dfe15b9e8fe60033defad68e324b7ad	subjective interestingness in exploratory data mining	discovery	Exploratory data mining has as its aim to assist a user in improving their understanding about the data. Considering this aim, it seems self-evident that in optimizing this process the data as well as the user need to be considered. Yet, the vast majority of exploratory data mining methods (including most methods for clustering, itemset and association rule mining, subgroup discovery, dimensionality reduction, etc) formalize interestingness of patterns in an objective manner, disregarding the user altogether. More often than not this leads to subjectively uninteresting patterns being reported. Here I will discuss a general mathematical framework for formalizing interestingness in a subjective manner. I will further demonstrate how it can be successfully instantiated for a variety of exploratory data mining problems. Finally, I will highlight some connections to other work, and outline some of the challenges and research opportunities ahead.	association rule learning;cluster analysis;data mining;dimensionality reduction;exploratory testing;flickr;information needs	Tijl De Bie	2013		10.1007/978-3-642-41398-8_3	computer science;data science;data mining;database	ML	-34.74016710957547	-54.89947393479567	176670
940e313531617911ac317a3c47ae8c4ccfc10215	patterns of query reformulation during web searching	human computer interaction;query refinement;assisted searching;search behavior;predictive models	Query reformulation is a key user behavior during Web search. Our research goal is to develop predictive models of query reformulation during Web searching.This article reports results from a study in which we automatically classified the query-reformulation patterns for 964,780 Web searching sessions, composed of 1,523,072 queries, to predict the next query reformulation. We employed an n-gram modeling approach to describe the probability of users transitioning from one query-reformulation state to another to predict their next state. We developed first-, second-, third-, and fourth-order models and evaluated each model for accuracy of prediction, coverage of the dataset, and complexity of the possible pattern set. The results show that Reformulation and Assistance account for approximately 45% of all query reformulations; furthermore, the results demonstrate that the firstand second-order models provide the best predictability, between 28 and 40% overall and higher than 70% for some patterns. Implications are that the n-gram approach can be used for improving searching systems and searching assistance.	aggregate data;algorithm;context-sensitive help;e-commerce;grams;n-gram;predictive modelling;query language;sensor;state transition table;stepwise regression;stochastic process;web search engine	Bernard J. Jansen;Danielle L. Booth;Amanda Spink	2009	JASIST	10.1002/asi.21071	query optimization;query expansion;web query classification;computer science;data science;data mining;web search query;information retrieval	Web+IR	-33.744873417060504	-52.43261366568557	176692
648d91239d6b0210e48b4d8e0eddd204adf8ff51	matchsimile: a flexible approximate matching tool for searching proper name	extraction information;software;nombre propio;description systeme;nom propre;evaluation performance;system description;architecture systeme;performance evaluation;logiciel;information extraction;computer software development;search engines;information retrieval;evaluacion prestacion;similitude;proper noun;algorithme;algorithm;recherche information;proper names;similarity;tâche appariement;tarea apareamiento;matchsimile logiciel;logicial;algorithms;arquitectura sistema;approximate matching;descripcion sistema;recuperacion informacion;similitud;system architecture;electronic text;matching task;extraccion informacion;algoritmo	We present the architecture and algorithms behind Matchsimile, an approximate string matching lookup tool especially designed for extracting person and company names from large texts. Part of a larger information extraction environment, this specific engine receives a large set of proper names to search for, a text to search, and search options; and outputs all the occurrences of the names found in the text. Beyond the similarity search capabilities applied at the intraword level, the tool considers a set of specific person name formation rules at the word level, such as combination, abbreviation, duplicity detections, ordering, word omission and insertion, among others. This engine is used in a successful commercial application (also named Matchsimile), which allows searching for lawyer names in official law publications.	approximate string matching;business models for open-source software;formation rule;information extraction;lookup table;regular expression;sensor;similarity search;string searching algorithm	Gonzalo Navarro;Ricardo A. Baeza-Yates;João Marcelo Azevedo Arcoverde	2003	JASIST	10.1002/asi.10178	computer science;artificial intelligence;proper noun;data mining;world wide web;information extraction;information retrieval;algorithm	AI	-35.90514428244369	-61.324335982926485	176771
16e4f5ae230cb778cd6108a134e35697472f3fcf	adaptive time windows for real-time crowd captioning	real time crowdsourcing;assistive technology	Real-time captioning provides deaf and hard of hearing users with access to live spoken language. The most common source of real-time captions are professional stenographers, but they are expensive (up to $200/hr). Recent work shows that groups of non-experts can collectively caption speech in real-time by directing workers to different portions of the speech and automatically merging the pieces together. This work uses 'one size fits all' segment durations regardless of an individual worker's ability or preferences. In this paper, we explore the effect of adaptively scaling the amount of content presented to each worker based on their past and recent performance. For instance, giving fast typists longer segments and giving workers shorter segments as they fatigue. Studies with 24 remote crowd workers, using ground truth in segment calculations, show that this approach improves average coverage by over 54%, and F1 score (harmonic mean) by over 44%.	f1 score;fits;ground truth;image scaling;microsoft windows;real-time transcription	Matthew J. Murphy;Christopher D. Miller;Walter S. Lasecki;Jeffrey P. Bigham	2013		10.1145/2468356.2468360	speech recognition;computer science;multimedia;world wide web	HCI	-42.430710338455235	-56.775564620357585	177276
e275f1273a4dc42a61ae052c23d4de84498ec251	improving keyword indexing	problems;indexing;indexation;permuted indexes	The continuing debate concerning the utility of titlederivative keyword indexing (e.g., KWIC or KWOC) has been received with some critical interest by practitioners of this technique. Wellisch [ 11 refers to keyword indexing as “crude methods of indexing which have a place, if at all, only in ‘quick and dirty’ applications.” Leatherdale [2] concurs on this point of view stating that keyword indices “do not reflect that degree of precision that we should expect. . .” This discussion of keyword applications to the information industry is not unprecedented. Several articles have described local uses of this kind of programming [3] as well as some of the limitations [4], and evaluations are continuing [ 51 . The purpose of this brief communication is to examine the more prevalent objections to keyword indices and to suggest some possible solutions. (1) Perhaps the most often mentioned shortcoming of keyword indexing is the lack of any uniform subject approach. Leatherdale points out that in a certain index, keyword processes would have required 16 divergent terms to find all 333 entries subsumed under the collective subject heading “Dairy Cattle.” Leatherdale’s objections can be addressed in two ways. He speaks of the “unambiguous heading ‘Dairy Cattle’ ’’ as preferable to the other 16 specific terms. Continuing this example, Leatherdale’s approach would isolate all 333 entries on the subject. This is good practice if researchers are interested in assembling lists of everything on “Dairy Cattle.” On the other hand, if researchers are interested in a breed of dairy cattle (e.g., Holstein), or an age group (e.g., Heifers), their search, by Leatherdale’s procedure, would be just beginning; they would necessarily need to read through all 333 titles to find those specifically related to their true	cab direct (database);course (navigation);expect;key word in context;keyword-driven testing	John N. Olsgaard;John Edward Evans	1981	JASIS	10.1002/asi.4630320109	search engine indexing;speech recognition;computer science;data mining;world wide web;information retrieval	DB	-40.10518202829447	-61.609461195089175	177485
8c85c77f06fe60868e759a2e67e4cc7482333ce5	manipulating the relevance models of existing search engines	busqueda informacion;modelizacion;search engine;relevance model;buscador;text;information retrieval;texte;similitude;modelisation;hierarchical classification;recherche information;metabuscador;metamoteur recherche;comportement utilisateur;similarity;retroaction pertinence;classification hierarchique;meta search engine;user behavior;similitud;moteur recherche;anchor text;texto;modeling;clasificacion jerarquizada;relevance feedback;comportamiento usuario	Collaborative search refers to how the search behavior of communities of users can be used to influence the ranking of search results. In this poster we describe how this technique, as instantiated in the I-SPY meta-search engine can be used as a general mechanism for implementing a different relevance feedback strategy. We evaluate a relevance feedback strategy based on anchor-text and query similarity using the TREC2004 Terabyte track document collection.	learning to rank;relevance;web search engine	Oisín Boydell;Cathal Gurrin;Alan F. Smeaton;Barry Smyth	2005		10.1007/978-3-540-31865-1_44	ranking;systems modeling;similarity;metasearch engine;anchor text;computer science;similitude;data mining;world wide web;information retrieval;search engine	ML	-36.10887555355321	-58.24479994184137	177577
43d8cfa027a51248b25c32de5f261220ce8bb75d	bullseye: structured passage retrieval and document highlighting for scholarly search	interactive information retrieval;focused retrieval;implementations of information systems	We present the Bullseye system for scholarly search. Given a collection of research papers, Bullseye: 1) identifies relevant passages using any off-the-shelf algorithm; 2) automatically detects document structure and restricts retrieved passages to user-specified sections; and 3) highlights those passages for each PDF document retrieved. We evaluate Bullseye with regard to three aspects: system effectiveness, user effectiveness, and user effort. In a system-blind evaluation, users were asked to compare passage retrieval using Bullseye vs. a baseline which ignores document structure, in regard to four types of graded assessments. Results show modest improvement in system effectiveness while both user effectiveness and user effort show substantial improvement. Users also report very strong demand for passage highlighting in scholarly search across both systems considered.	algorithm;baseline (configuration management);portable document format	Xi Zheng;Akanksha Bansal;Matthew Lease	2017		10.1145/3014812.3014846	computer science;data mining;world wide web;information retrieval	Web+IR	-34.112239711608574	-55.65598581493451	177839
16a069148f6adeaec1bc011473021fbe38b3cdad	measuring a cross language image retrieval system	busqueda informacion;caption;lenguaje documental;text;traduccion automatica;measures of effectiveness;recherche image;usability study;sous titrage;information retrieval;query formulation;subtitulo;formulacion pregunta;texte;formulation question;langage documentaire;traduction automatique;recherche information;target language;utilisabilite;source language;information language;usabilidad;multilinguisme;foreign language;usability;texto;cross language information retrieval;multilingualism;test collection;multilinguismo;automatic translation;image retrieval	Cross language information retrieval is a field of study that has received significant research attention, resulting in systems that despite the errors of automatic translation (from query to document), on average, produce relatively good retrieval results. Traditionally, most work has focussed on retrieval from sets of newspaper articles; however, other forms of collection are being searched: one example being the cross language retrieval of images by text caption. Limited past work has established, through test collection evaluation, that as with traditional CLIR, image CLIR is effective. This paper presents two studies that start to establish the usability of such a system: first, a test collection-based examination, which avoids traditional measures of effectiveness, is described and results from it are discussed; second, a preliminary usability study of a working cross language image retrieval system is presented. Together the examinations show that, in general, searching for images captioned in a language unknown to a searcher is usable.	cross-language information retrieval;display resolution;experiment;image retrieval;machine translation;usability testing	Mark Sanderson;Paul D. Clough;Catherine Paterson;Wai Tung Lo	2004		10.1007/978-3-540-24752-4_26	foreign language;natural language processing;visual word;speech recognition;usability;image retrieval;computer science;database;information retrieval	Web+IR	-35.45566275150307	-62.936404058398054	178273
1c69a3074c65abb456acb0fd694e1572657b4f98	signposts on the digital highway: the effect of semantic and pragmatic hyperlink previews	advance organisers;learning effectiveness;goal orientation;previews;navigation;hypertext	In this article, the effect of a local, content (as opposed to structure) oriented navigation tool is investigated, i.e. mouse-over hyperlink previews. A usability experiment is described in which three groups of participants were exposed to three different versions of a website: without hyperlink previews, with content oriented, semantic previews, and with task-oriented, pragmatic previews. Participants were asked to execute search and recall tasks, and to evaluate task and hypertext. The results showed a decisive overall advantage for previews in terms of efficiency, but no effects on effectiveness or appreciation. Although semantic and pragmatic previews did not differ significantly, a post hoc analysis showed a learning effect of pragmatic previews that was absent in the semantic preview condition. It was concluded that previews fit in with the step-by-step goal orientation of hypertext users. Once users are acquainted with them, pragmatic previews speed up decision making. Apart from the experimental part, the article surveys research into the usability of navigation tools, thereby focusing on the analysis of navigation tools. The bottom line of this review is that most navigation tools as they are used in the experiments provide users with different types of information, e.g. local vs. global, content vs. structure oriented. This complicates the unequivocal explanation of their effect and may explain, together with user and task differences, the variety and inconsistencies observed in the results. q 2005 Elsevier B.V. All rights reserved.	declarative programming;ecology;expect;experiment;hoc (programming language);hyperlink;hypertext;internet;mobile phone;nonlinear system;usability	Alfons Maes;Arjan van Geel;Reinier Cozijn	2006	Interacting with Computers	10.1016/j.intcom.2005.05.004	navigation;hypertext;human–computer interaction;computer science;knowledge management;goal orientation;multimedia;management;world wide web	HCI	-35.41654111479344	-52.459963358453116	178806
38babfb9479f7205ee75eb2c09cbf0dcc74cf2db	color classification using color vision models	color vision	According to recent physiological research results, there are many individual differences already at the detection level of our color vision system. It is not completely clear yet, how the other levels of color vision system compensate the detection differences. Instead of detecting and analyzing colors exactly in the same way, we all just have learned to classify colors in a certain way, which seems to lead almost always to the same result independent of the individual differences in the color vision system. In this article, we experiment with four models developed for replicating certain properties of human color vision. We examine the color classification abilities of these models and show the differences and similarities in their behavior. © 2010 Wiley Periodicals, Inc. Col Res Appl, 2010.	color vision	Tuija Jetsu;Yasser Essiarab;Ville Heikkinen;Timo Jääskeläinen;Jussi Parkkinen	2008			computer vision;color model;color normalization;color balance;color vision;optics;physics;computer graphics (images)	Vision	-42.226294789041894	-52.62964927428825	179177
14161bf5f9ade56b4b98f9693e00bca33fe8dfa6	evaluación de resúmenes automáticos mediante qarla	computacion informatica;marco de evaluacion;automatic summarization;filologias;qarla;info eu repo semantics article;informacion documentacion;linguistica;resumen automatico;ciencias basicas y experimentales;duc;grupo a;ciencias sociales;grupo b;evaluation framework	This article shows an application of the QARLA evaluation framework on DUC-2004 (tasks 2 and 5). The QARLA framework allows to evaluate summaries with regard to different features. Second, it allows to combine and meta-evaluate different similarity metrics, giving more weigh to metrics which characterize models (manual summaries) regarding automatic summaries.		Enrique Amigó;Anselmo Peñas;Julio Gonzalo;M. Felisa Verdejo	2005	Procesamiento del Lenguaje Natural		natural language processing;computer science;artificial intelligence;automatic summarization;world wide web	NLP	-35.724755535864446	-61.75254517255349	179412
b065027f58bb1ad4bfcace910906ff1fe48795f1	the errors of our ways: using metadata quality research to understand common error patterns in the application of name headings		Using data culled during a metadata quality research project for the Social Network and Archival Context (SNAC) project, this article discusses common errors and problems in the use of standardized languages, specifically unambiguous names for persons and corporate bodies. Errors such as misspelling, qualifiers, format, and miss-encoding point to several areas where quality control measures can improve aggregation of data. Results from a large data set indicate that there are predictable problems that can be retrospectively corrected before aggregation. This research looked specifically at name formation and expression in metadata records, but the errors detected could be extended to other controlled vocabularies as well.		Katherine Wisser	2014		10.1007/978-3-319-13674-5_9	computer science;data mining;database;world wide web;information retrieval;metadata repository	HCI	-39.6114455286291	-65.69085374033921	179455
435fd49789e65c2ea480dd8a26aced5b07107f95	a query-dependent duplicate detection approach for large scale search engines	tiempo respuesta;reponse temporelle;search engine;buscador;duplicate detection;tiempo busqueda;web pages;red www;interrogation base donnee;reseau web;pertinencia;interrogacion base datos;response time;ejecucion programa;temps recherche;program execution;temps reponse;large scale;fichier log;fichero actividad;internet;hybrid method;time response;execution programme;duplication;pertinence;indexation;duplicacion;world wide web;relevance;moteur recherche;escala grande;respuesta temporal;search time;query logs;database query;log file;echelle grande	Duplication of Web pages greatly hurts the perceived relevance of a search engine. Existing methods for detecting duplicated Web pages can be classified into two categories, i.e. offline and online methods. The offline methods target to detect all duplicates in a large set of Web pages, but none of the reported methods is capable of processing more than 30 million Web pages, which is about 1% of the pages indexed by today’s commercial search engines. On the contrary, the online methods focus on removing duplicated pages in the search results at run time. Although the number of pages to be processed is smaller, these methods could heavily increase the response time of search engines. Our experiments on real query logs show that there is a significant difference between popular and unpopular queries in terms of query number and duplicate distributions. Then, we propose a hybrid query-dependent duplicate detection method which combines both advantage of offline and online methods. This hybrid method provides not only an effective but also scalable solution for duplicate detection.	experiment;online and offline;randomness;relevance;response time (technology);run time (program lifecycle phase);scalability;sensor;web page;web search engine;world wide web	Shaozhi Ye;Ruihua Song;Ji-Rong Wen;Wei-Ying Ma	2004		10.1007/978-3-540-24655-8_6	web query classification;the internet;relevance;computer science;operating system;web page;data mining;database;world wide web;response time;search engine;gene duplication	Web+IR	-35.654923656559625	-57.68717593788648	179984
ed78e195077dad015aea79fb199aa7e78eee0d2f	improving access to digital music through content-based analysis	library design;delivery system;multimedia;technology development;information retrieval;digital library;digital music;music information retrieval;audio recordings;music	Purpose – The purpose of this paper is to report recent advances on a collaborative project that aims to develop content-based methods for music information retrieval (MIR) as an alternative to standard text-based modes of access to digital music libraries. Design/methodology/approach – The paper describes current practices and ongoing research, and it discusses potential applications for future use. Findings – Content-based MIR approaches can extend and enhance the capabilities of traditional text-based discovery and delivery systems and thus support the work of expert users such as musicians and musicologists. Examples of technologies developed in the context of the project include novel methods for automatic chord identification, motif finding, the visualization of musical structure, and retrieval of musical variations using harmonic and structural information. Practical implications – The paper looks at new, non-verbal modes of interaction with digital music archives based on musically substantive features such as chords, motifs, rhythms, etc. By building more sophisticated dimensions of interactivity into a discovery-and-delivery system, these tools could give the end-user a more meaningful and rewarding experience. The tools potentially would be less costly and more scalable than textual annotation and markup, and their applicability extends beyond digital libraries to other music services. Originality/value – This article discusses the advantages and challenges posed by audio-based MIR and shows, via project-specific examples, its relevance to supporting the needs of digital music library users.	archive;digital library;information retrieval;interactivity;library (computing);markup language;relevance;scalability;sequence motif;text-based (computing)	Juan Pablo Bello;Kent Underwood	2012	OCLC Systems & Services	10.1108/10650751211197040	digital library;computer science;music;multimedia;world wide web;information retrieval	HCI	-35.80449837819303	-55.20654902374282	179997
3186ae2721566e7b6548690761e683f93a790d7f	evaluating score normalization methods in data fusion	busqueda informacion;information retrieval;data fusion;recherche information;fusion donnee;fusion datos	In data fusion, score normalization is a step to make scores, which are obtained from different component systems for all documents, comparable to each other. It is an indispensable step for effective data fusion algorithms such as CombSum and CombMNZ to combine them. In this paper, we evaluate four linear score normalization methods, namely the fitting method, Zero-one, Sum, and ZMUV, through extensive experiments. The experimental results show that the fitting method and Zero-one appear to be the two leading methods.	algorithm;database normalization;document;experiment;information processing;performance prediction	Shengli Wu;Fabio Crestani;Yaxin Bi	2006		10.1007/11880592_57	computer science;pattern recognition;data mining;sensor fusion;information retrieval	DB	-35.9857440046544	-61.86856044126524	180302
32012bf08c152ea561718fe9e25be624fe1f5371	aesthetics and usability of in-vehicle navigation displays	navegacion;evaluation performance;interfase usuario;affichage;usability evaluation;performance evaluation;visualizacion;user interface;evaluacion prestacion;abstraction;abstraccion;aesthetics;navigation;display;research evaluation;electronic map displays;utilisabilite;interface utilisateur;esthetique;usabilidad;usability;estetica	This research evaluates the aesthetics and usability of various in-vehicle electronic navigation map configurations. Study 1 adapted the aesthetics scale (Lavie and Tractinsky, 2004) to accommodate evaluations of map displays. Study 2 examined map displays that vary in the amount of data presented, their abstraction level and color schema, using objective and subjective usability measures. Maps with minimal detail produced best performances and highest evaluations. Abstractions were found to be advantageous when combined with reduced amount of detail and specific color schemas. Moderate abstractions were sufficient for obtaining the desired benefits. The color schema mainly affected the objective measures, pointing to the importance of good contrast between the cursor and the map colors. Study 3 further examined map schemas. Color schemas again had no effect on the perceptions of aesthetics and usability. Overall, similar results and high correlations were found for the perceived aesthetics and usability scales, indicating the connection between perceived aesthetics and usability. Lower correlations were found between the actual usability (performance) and the aesthetics scale. Finally, users’ usability evaluations were not always in line with their actual performance, pointing to the importance of using objective usability measures. & 2010 Elsevier Ltd. All rights reserved.	abstraction layer;color;cursor (databases);map;nautical chart;performance;usability	Talia Lavie;Tal Oron-Gilad;Joachim Meyer	2011	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2010.10.002	computer vision;navigation;cognitive walkthrough;simulation;usability;computer science;abstraction;multimedia;user interface	HCI	-43.97776149045874	-54.55554440102714	180401
956d6bd3676bd93197e900e594274cfa17ab9348	progress report on automatic information retrieval	information retrieval;technical report;computer science	The modern online information retrieval environment is first briefly described. A variety of recent enhancements of the existing operational retrieval systems are then examined, such as the use of sophisticated user-system interfaces, the utilizing of fast computational devices capable of speeding up the search operations, the existence of fast text-matching algorithms, and finally, the introduction of sophisticated retrieval models leading to improved	algorithm;computation;information retrieval	Gerard Salton	1979		10.1145/511706.511707	relevance;computer science;technical report;world wide web;information retrieval;human–computer information retrieval	Web+IR	-34.599957455482915	-56.68230380598635	180941
91c184e7fb0c7cce5319b8db85c1488b3861976f	visual question answer diversity		Visual questions (VQs) can lead multiple people to respond with different answers rather than a single, agreed upon response. Moreover, the answers from a crowd can include different numbers of unique answers that arise with different relative frequencies. Such answer diversity arises for a variety of reasons including that VQs are subjective, difficult, or ambiguous. We propose a new problem of predicting the answer distribution that would be observed from a crowd for any given VQ; i.e., the number of unique answers and their relative frequencies. Our experiments confirm that the answer distribution can be predicted accurately for VQs asked by both blind and sighted people. We then propose a novel crowd-powered VQA system that uses the answer distribution predictions to reason about how many answers are needed to capture the diversity of possible human responses. Experiments demonstrate this proposed system accelerates capturing the diversity of answers with considerably less human effort than is required with a state-of-art system.	crowdsourcing;experiment;vector quantization	Chun-Ju Yang;Kristen Grauman;Danna Gurari	2018			artificial intelligence;machine learning;computer science	Vision	-35.27549819343321	-53.529041595220335	181279
3ebe821577a580ef6da05558272eceef02ab5020	document ranking for variable bit-block compression signatures	documento;filing;evaluation performance;performance evaluation;modele agrege;avbc aggregate variable bit block compression;data compression;information retrieval;evaluacion prestacion;indexing and document ranking;weighting;modelo agregado;methode;vbc variable bit block compression;term frequency;ponderacion;document;signature file;algorithme;upper bound;algorithm;information storage;scaling;mathematical formulas;indexing;recherche information;almacenamiento;wvbc weighted variable bit block compression;indexation;classement;signature;indizacion;stockage;aggregate model;search strategies;upper and lower bounds;ponderation;compresion dato;recuperacion informacion;compression;signing;metodo;firma;storage;method;clasificacion;compression donnee;algoritmo;subject index terms	Variable bit-block compression (VBC) signature is extended for document ranking. Two different extensions were experimented: the weighted VBC (WVBC) scheme and the aggregate VBC (AVBC) scheme. For both, analytical bounds of the additional storage for the term frequencies were derived. The upper and lower bounds of WVBC signatures were better than the corresponding bounds for AVBC signatures. In general, these bounds are functions of the word size (in bits) of the term frequencies. Therefore, term frequencies were scaled to reduce the word size. Experiments showed that the additional storage cost is closer to the lower than the upper bound for both WVBC and AVBC signatures. In addition, WVBC signatures were better than AVBC signatures in terms of storage and retrieval speed. Logarithmic scaling was found to be significantly better than linear scaling, in measuring the agreement of document ranking against the case without scaling, using the Kendall rank-order correlation. If a 75% ranking performance is acceptable, then the additional storage of the term frequencies is only 3.4% of all the indexed documents.	data compression;ranking (information retrieval);type signature	Robert Wing Pong Luk;C. M. Chen	2001	Inf. Process. Manage.	10.1016/S0306-4573(00)00020-0	computer science;theoretical computer science;data mining;mathematics;upper and lower bounds;information retrieval;algorithm;statistics	DB	-36.15759059962864	-61.797342870089516	181445
275a13cdda4d561b7601f21ddc4cea2f59794073	portable weather applications for general aviation pilots	biological patents;hazard evaluation;biomedical journals;aviation safety;pedestrian safety;text mining;poison control;europe pubmed central;lotsenassistenz;injury prevention;weather routing;simulation;general aviation;citation search;safety literature;traffic safety;injury control;citation networks;home safety;injury research;safety abstracts;human factors;research articles;portable equipment;abstracts;open access;occupational safety;safety;life sciences;clinical guidelines;safety research;accident prevention;violence prevention;bicycle safety;full text;behavior;poisoning prevention;falls;ergonomics;rest apis;suicide prevention;orcids;europe pmc;biomedical research;driver support systems;aircraft;bioinformatics;literature search	OBJECTIVE The objective of this study was to examine the potential benefits and impact on pilot behavior from the use of portable weather applications.   METHOD Seventy general aviation (GA) pilots participated in the study. Each pilot was randomly assigned to an experimental or a control group and flew a simulated single-engine GA aircraft, initially under visual meteorological conditions (VMC). The experimental group was equipped with a portable weather application during flight. We recorded measures for weather situation awareness (WSA), decision making, cognitive engagement, and distance from the aircraft to hazardous weather.   RESULTS We found positive effects from the use of the portable weather application, with an increased WSA for the experimental group, which resulted in credibly larger route deviations and credibly greater distances to hazardous weather (≥30 dBZ cells) compared with the control group. Nevertheless, both groups flew less than 20 statute miles from hazardous weather cells, thus failing to follow current weather-avoidance guidelines. We also found a credibly higher cognitive engagement (prefrontal oxygenation levels) for the experimental group, possibly reflecting increased flight planning and decision making on the part of the pilots.   CONCLUSION Overall, the study outcome supports our hypothesis that portable weather displays can be used without degrading pilot performance on safety-related flight tasks, actions, and decisions as measured within the constraints of the present study. However, it also shows that an increased WSA does not automatically translate to enhanced flight behavior.   APPLICATION The study outcome contributes to our knowledge of the effect of portable weather applications on pilot behavior and decision making.		Ulf Ahlstrom;Oliver Ohneiser;Eamon Caddigan	2016	Human factors	10.1177/0018720816641783	simulation;medicine;environmental health;pathology;computer science;engineering;suicide prevention;human factors and ergonomics;injury prevention;aviation safety;forensic engineering;computer security;behavior;mechanical engineering	HCI	-46.69646052635878	-54.52209459708784	181530
bb3db006db30d8baa6ac4349d16dae7297af2c15	balancing structural and temporal constraints in multitasking contexts	conferenceobject	Recent research has shown that when people multitask, both the subtask structure and the temporal constraints of the component tasks strongly influence people’s task-switching behavior. In this paper, we propose an integrated theoretical account and associated computational model that aims to quantify how people balance structural and temporal constraints in everyday multitasking. We validate the theory using data from an empirical study in which drivers performed a visual-search task while navigating a driving environment. Through examination of illustrative protocols from the model and human drivers as well as the overall fit on the aggregate glance data, we explore the implications of the theory and model for time-critical multitasking domains.	aggregate data;computational model;computer multitasking;window of opportunity	Dario D. Salvucci;Tuomo Kujala	2016			cognitive psychology;psychology;human multitasking	NLP	-48.193008048900765	-52.58941866332991	181726
049fb555b6267dd1855a5dfdde4b5c66435fa73d	neuropsychological function for accessibility of computer program for people with mental retardation	computer program;interfase usuario;user interface;accesibilidad;information technology;technologie information;atencion visual;interface ordinateur;user assistance;programa informatico;saber hacer;visual languages;assistance utilisateur;internet;know how;accessibility;langage visuel;savoir faire;asistencia usuario;mental retardation;neuropsychological tests;interface utilisateur;attention visuelle;tecnologia informacion;computer interfaces;visual attention;modem;accessibilite;programme ordinateur;human computer interface	The extent to which people with mental retardation are benefit from the modem information technology is not well explored. A better understanding on ways that the existing human-computer interface would challenge people with mental retardation would shed light on this issue. This study was investigated the neuropsychological functions which are important for enhancing the competence of people with mental retardation to operate on the Internet Explorer (IE) program. Sixty-two participants with mental retardation were invited to conduct a set of neuropsychological tests. Their computer performance was also evaluated. Resulted indicated that some specific neuropsychological functions including attention and visual scanning, psychomotor and language were predictive of their overall computer competence. The implication on ways to improve the design for computer programs for this population was discussed.	accessibility;computer program	Alex W. W. Wong;Chetwyn C. H. Chan;Cecilia W. P. Li-Tsang;Chow S. Lam	2004		10.1007/978-3-540-27817-7_156	the internet;simulation;human–computer interaction;computer science;artificial intelligence;accessibility;multimedia;user interface;information technology	HCI	-45.176801241204885	-55.591625465983995	182191
073def6b05cdd4540454f51d85219f83e67b4a94	large on-line files of bibliographic data:: an efficient design and a mathematical predictor of retrieval behavior				P. L. Long;K. B. L. Rastogi;James E. Rush;J. A. Wyckoff	1971		10.1300/J299v01n04_07	computer science;data mining;database;information retrieval	ECom	-38.441300950910396	-65.37622698180613	182314
d598ecb3de1964fab989447ef00bcbfd1b27d2b5	ub at imageclefmed 2006	medical image;text retrieval;content based image retrieval	This paper presents the results of the University at Buffalo in the 2006 ImageCLEFmed task. Our approach for this task combines Content Based Image Retrieval (CBIR) and text retrieval to improve retrieval of medical images. Our results are comparable to other approaches presented in the task. Our results show that text retrieval performs well across the three different types of topics (visual, visual-semantic and semantic) and that the combination of CBIR and text retrieval yields moderate improvements.	buffalo network-attached storage series;content-based image retrieval;document retrieval;undefined behavior	Miguel E. Ruiz	2006		10.1007/978-3-540-74999-8_87	visual word;image retrieval;computer science;data mining;multimedia;information retrieval	Web+IR	-33.98135489428353	-62.880974771834985	183209
3e44b351aef832c8b259abc2e5a2249e9eb04345	improving the learning of boolean queries by means of a multiobjective iqbe evolutionary algorithm	busqueda informacion;multiobjective programming;logica booleana;programmation multiobjectif;genetic program;information retrieval system;boolean information retrieval systems;information retrieval;query formulation;query learning;informing science;systeme recherche;pregunta documental;genetic programming;algoritmo genetico;multiobjective evolutionary algorithm;systeme recherche information;search system;sistema investigacion;recherche information;text retrieval;algorithme genetique;query;logique booleenne;algorithme evolutionniste;genetic algorithm;algoritmo evolucionista;inductive query by example;evolutionary algorithm;query by example;information need;boolean logic;relevance feedback;multiobjective evolutionary algorithms;requete;programacion multiobjetivo	The Inductive Query By Example (IQBE) paradigm allows a system to automatically derive queries for a specific Information Retrieval System (IRS). Classic IRSs based on this paradigm [Smith, M., & Smith, M. (1997). The use of genetic programming to build Boolean queries for text retrieval through relevance feedback. Journal of Information Science, 23(6), 423–431] generate a single solution (Boolean query) in each run, that with the best fitness value, which is usually based on a weighted combination of the basic performance criteria, precision and recall. A desirable aspect of IRSs, especially of those based on the IQBE paradigm, is to be able to get more than one query for the same information needs, with high precision arid recall values or with different trade-offs between both. In this contribution, a new IQBE process is proposed combining a previous basic algorithm to automatically derive Boolean queries for Boolean IRSs [Smith, M., & Smith, M. (1997). The use of genetic programming to build Boolean queries for text retrieval through relevance feedback. Journal of Information Science, 23(6), 423–431] and an advanced evolutionary multiobjective approach [Coello, C. A., Van Veldhuizen, D. A., & Lamant, G. B. (2002). Evolutionary algorithms for solving multiobjective problems. Kluwer Academic Publishers], which obtains several queries with a different precision–recall trade-off in a single run. The performance of the new proposal will be tested on the Cranfield and CACM collections and compared to the well-known Smith and Smith s algorithm, showing how it improves the learning of queries and thus it could better assist the user in the query formulation process. 2005 Elsevier Ltd. All rights reserved.	boolean algebra;document retrieval;evolutionary algorithm;genetic programming;information needs;information science;precision and recall;programming paradigm;query by example;relevance feedback;smith–volterra–cantor set;whole earth 'lectronic link	Oscar Cordón;Enrique Herrera-Viedma;María Luque	2006	Inf. Process. Manage.	10.1016/j.ipm.2005.02.006	genetic programming;boolean algebra;information needs;genetic algorithm;standard boolean model;computer science;artificial intelligence;query by example;machine learning;evolutionary algorithm;mathematics;information retrieval;algorithm	Web+IR	-36.743285474763915	-60.44107169729479	183652
bfca58d69b80ead742b46298852b1205537c8c04	exploring document retrieval features associated with improved short- and long-term vocabulary learning outcomes		A growing body of information retrieval research has studied the potential of search engines as effective, scalable platforms for self-directed learning. Towards this goal, we explore document representations for retrieval that include features associated with effective learning outcomes. While prior studies have investigated different retrieval models designed for teaching, this study is the first to investigate how document-level features are associated with actual learning outcomes when users get results from a personalized learning-oriented retrieval algorithm. We also conduct what is, to our knowledge, the first crowdsourced longitudinal study of long-term learning retention, in which we gave a subset of users who participated in an initial learning and assessment study a delayed post-test approximately nine months later. With this data, we were able to analyze how the three retrieval conditions in the original study were associated with changes in long-term vocabulary knowledge. We found that while users who read the documents in the personalized retrieval condition had immediate learning gains comparable to the other two conditions, they had better long-term retention of more difficult vocabulary.	algorithm;baseline (configuration management);crowdsourcing;data science;document retrieval;information retrieval;personalization;scalability;vocabulary;web search engine	Rohail Syed;Kevyn Collins-Thompson	2018		10.1145/3176349.3176397	data mining;computer science;personalization;information retrieval;search engine;document retrieval;scalability;vocabulary;longitudinal study	Web+IR	-35.574437870112895	-53.441562401728305	183680
e0a0c7b16b4db89f3d6e9c00d11e12acdd24bf39	what does time constraint mean to information searchers?	search experience;task difficulty;knowledge acquisition;mood;time constraint	In this paper, we explore the relationship between time constraints and users' assessment of their search. A user experiment was conducted. Participants were asked to search under two conditions: with time constraint (TC) and with no time constraint (NTC). The results showed that time constraint did not significantly influence participants' assessment of task difficulty, but significantly influenced users' search confidence and their evaluation of search performance. Particularly, participants were less confident and considered their search performance worse in TC than in NTC conditions. We also found users acquired more new knowledge and had more positive affective states after searching in NTC than in TC conditions. Interestingly, we found time constraints also affect participants' anticipation of time needed to complete the task; participants thought they would need significantly less time to complete the search task when they were given time constraints than without time constraints. These preliminary results suggested that time constraints had remarkable influence upon users' perception of search tasks and their search experience.		Chang Liu;Fan Yang;Yu Zhao;Qin Jiang;Lu Zhang	2014		10.1145/2637002.2637029	simulation;artificial intelligence	HCI	-36.421849855952274	-53.19198164975476	183862
c9ec8556d256d4c98ee36e4f71a6f041c3ba79a7	using matched samples to estimate the effects of exercise on mental health via twitter	public health;social media	Recent work has demonstrated the value of social media monitoring for health surveillance (e.g., tracking influenza or depression rates). It is an open question whether such data can be used to make causal inferences (e.g., determining which activities lead to increased depression rates). Even in traditional, restricted domains, estimating causal effects from observational data is highly susceptible to confounding bias. In this work, we estimate the effect of exercise on mental health from Twitter, relying on statistical matching methods to reduce confounding bias. We train a text classifier to estimate the volume of a user’s tweets expressing anxiety, depression, or anger, then compare two groups: those who exercise regularly (identified by their use of physical activity trackers like Nike+), and a matched control group. We find that those who exercise regularly have significantly fewer tweets expressing depression or anxiety; there is no significant difference in rates of tweets expressing anger. We additionally perform a sensitivity analysis to investigate how the many experimental design choices in such a study impact the final conclusions, including the quality of the classifier and the construction of the control group.	activity tracker;causal filter;design of experiments;social media measurement	Virgile Landeiro Dos Reis;Aron Culotta	2015				AI	-42.688795900406376	-62.122185768661005	184723
71e59bfcbedfbf731bb91847fb2a085d0d52aaac	webuser: mining unexpected web usage	belief;red www;analisis datos;log analysis;web accessibility;reseau web;customization;belief systems;personnalisation;recommendations;semantics;recommandation;critical event prediction;analyse temporelle;prior knowledge;data mining;semantica;semantique;analisis temporal;vigilancia economica;time analysis;data analysis;fichier log;fichero actividad;croyance;internet;temporal relations;fouille donnee;user behaviour;unexpected usage;web usage mining;comportement utilisateur;personalizacion;recomendacion;concept hierarchy;world wide web;economic intelligence;analyse donnee;recommendation;site structure optimisation;user behavior;web access logs;creencia;sequence rules;veille economique;concept hierarchies;busca dato;comportamiento usuario;log file;web content personalisation	Web usage mining has been much concentrated on the discovery of relevant user behaviours from Web access record data. In this paper, we present WebUser, an approach to discover unexpected usage in Web access log. We present a belief-driven method for extracting unexpected Web usage sequences, where the belief system consists of a temporal relation and semantics constrained sequence rules acquired with respect to prior knowledge. Our experiments show the effectiveness and usefulness of the proposed approach. Further, discovered rules of unexpected Web usage can be used for Web content personalisation and recommendation, site structure optimisation, and critical event prediction.	application domain;data mining;experiment;internet access;mathematical optimization;personalization;statistical classification;web content;web mining	Dong Li;Anne Laurent;Pascal Poncelet	2011	IJBIDM	10.1504/IJBIDM.2011.038276	web mining;web modeling;data web;web analytics;computer science;belief;social semantic web;data mining;database;semantics;web intelligence;world wide web	ML	-36.20526582267669	-57.421195677429694	184727
8307443fb56314c32d7eb080390940ef20acc16a	a model of metacognition for bushfire fighters	cognitive task analysis;bushfire;naturalistic decision making;metacognition;expertise	Large-scale bushfires are complex or macrocognitive decision environments (Klein et al.). They involve many people, such as incident management teams, firefighting crews, and resident communities. Those people can also be geographically dispersed. This means that they need to coordinate their bushfire response efforts and manage multiple, often competing, cognitive demands. To do this, bushfire responders need to use metacognitive skills to regulate their thinking, particularly under stressful high cognitive load conditions. We explored these types of issues with career and volunteer bushfire fighters (Frye and Wearing in J Cogn Technol 16(2): 33–44, 2011). We found that rule-based procedures can sometimes reduce errors, and increase safety, because they reduce cognitive load (e.g., ‘you just do it’). However, fixation on other rules and procedures can increase errors, and erode safety, because people fail to adapt to the current situation (e.g., ‘you really need to think about that’). In this paper we use a model of metacognition to describe how experts regulate their thinking and thus avoid errors associated with cognitive overload (such as tunnel vision and goal fixation) during large-scale bushfires. The implications for bushfire training and procedures are discussed.	incident management;logic programming;safety engineering	Lisa M. Frye;Alexander J. Wearing	2016	Cognition, Technology & Work	10.1007/s10111-016-0372-4	psychology;metacognition;simulation;human–computer interaction;task analysis;naturalistic decision-making;social psychology;operations research	HCI	-48.11099436122664	-55.4163579451836	184736
d21099ea121ab76e5e9306cab8f051d8976759f0	look-ahead and look-behind shortcuts in large item category hierarchies: the impact on search performance	shortcuts;look ahead;controlled experiment;website design;time on task;lostness	0953-5438/$ see front matter 2009 Elsevier B.V. A doi:10.1016/j.intcom.2009.05.008 * Corresponding author. Tel.: +1 434 544 8167. E-mail addresses: hpardue@usouthal.edu (J.H. Pa (J.P. Landry), kyper@lynchburg.edu (E. Kyper), rlievan Websites use shortcuts to facilitate navigation of large hierarchies of item categories. Two common types of shortcuts used for this purpose are location breadcrumbs and down-to-child/up-to-parent links; frequently both are employed simultaneously. The combined used of these shortcuts provide proximal cues which enable the user to look-ahead and look-behind in the navigational structure. In this study, the impact of shortcut usage on search performance on a known-item search task is estimated. A controlled experiment was conducted using a realistic hypertext hierarchy of item categories. The results indicate that greater use of shortcuts decreases both time on task and lostness for the user, and that the decrease is associated with increased depth in the hierarchy. These findings provide insight into possible performance trade-offs involved in website designs that include look-ahead shortcuts for navigating large item category hierarchies. 2009 Elsevier B.V. All rights reserved.	breadcrumb (navigation);hypertext;keyboard shortcut;like button	J. Harold Pardue;Jeffery Paul Landry;Eric S. Kyper;Rodrigo J. Lievano	2009	Interacting with Computers	10.1016/j.intcom.2009.05.008	computer science;look-ahead;data mining;world wide web;information retrieval	AI	-35.23949544227675	-52.17954142589517	184907
c16c2d4734ca16d999561505e65ded417e562224	university of hagen at clef 2003: natural language access to the girt4 data	busqueda informacion;lenguaje natural;query language;vocabulaire;linguistique;base donnee;information retrieval;vocabulary;langage naturel;interrogation base donnee;database;interrogacion base datos;base dato;vocabulario;indexing terms;lenguaje interrogacion;linguistica;recherche information;natural language;langage base donnee;natural language interface;langage interrogation;information need;multilinguisme;database languages;lenguaje formal;database query;multilingualism;formal language;multilinguismo;langage formel;linguistics	A natural language interface to databases allows a natural formulation of information needs, requiring little or no previous knowledge about database intrinsics or formal retrieval languages. Aiming at a full understanding of unrestricted natural language queries, the transformation into database queries and search failures caused by a vocabulary mismatch between query terms and database index terms become major problems to solve. This paper investigates methods of constructing query variants and their use in automated retrieval strategies. Performance results for an experimental setup with a GIRT4 database are presented.	natural language	Johannes Leveling	2003		10.1007/978-3-540-30222-3_40	natural language processing;database theory;speech recognition;question answering;data control language;computer science;database;linguistics;view;database schema;database design;query language	Theory	-35.62284492838501	-62.94298151724311	185117
9dba5206f39d2d4abc805560f330a77cd05e8aa2	evaluation of sensors as input devices for computer music interfaces	busqueda informacion;input device;input output equipment;etude experimentale;information retrieval;musica;asservissement visuel;acoustique musicale;interface design;musical acoustics;captador medida;musique;measurement sensor;equipement entree sortie;capteur mesure;marcador;pointer;computer music;acustica musical;recherche information;equipo entrada salida;pointeur;visual feedback;visual servoing;estudio experimental;music;audio acoustics;servomando visual;acoustique audio	This paper presents ongoing research into the design and creation of interfaces for computer music. This work concentrates on the use of sensor as the primary means of interaction for computer music, and examines the relationships between types of sensors and musical functions. Experiments are described which aim to discover the particular suitability of certain sensors for specific musical tasks. The effects of additional visual feedback on the perceived suitability of these sensors is also examined. Results are given, along with a discussion of their possible implications for computer music interface design and pointers for further work on this topic.	experiment;sensor	Mark T. Marshall;Marcelo M. Wanderley	2005		10.1007/11751069_12	pointer;speech recognition;acoustics;computer science;interface design;musical acoustics;music;multimedia;computer music;programming language;input device	HCI	-44.37214683210305	-55.413814853298916	185624
1cb261d968817e70eeee6dc895420740a4f2acf4	prediction of the human response time with the similarity and quantity of information	tiempo respuesta;human response time;interfase usuario;brain;human performance;systeme nerveux central;user interface design evaluation;execution time;user interface;solution similitude;hombre;response time;similarity solution;encefalo;similitude;temps reponse;sistema nervioso central;cerebro;solucion semejanza;encephale;cerveau;similarity;human;temps execution;interface utilisateur;user interface design;prediction model;encephalon;similitud;tiempo ejecucion;central nervous system;homme	Memory is one of brain processes that are important when trying to understand how people process information. Although a large number of studies have been made on the human performance, little is known about the similarity effect in human performance. The purpose of this paper is to propose and validate the quantitative and predictive model on the human response time in the user interface with the concept of similarity. However, it is not easy to explain the human performance with only similarity or information amount. We are confronted by two difficulties: making the quantitative model on the human response time with the similarity and validating the proposed model by experimental work. We made the quantitative model based on the Hick’s law and the law of practice. In addition, we validated the model with various experimental conditions by measuring participants’ response time in the environment of computer-based display. Experimental results reveal that the human performance is improved by the user interface’s similarity. We think that the proposed model is useful for the user interface design and evaluation phases. q 2005 Elsevier Ltd. All rights reserved.	coefficient;hick's law;human computer;human reliability;human–computer interaction;mcgurk effect;predictive modelling;response time (technology);self-similarity;user interface design	Sungjin Lee;Gyunyoung Heo;Soon Heung Chang	2006	Rel. Eng. & Sys. Safety	10.1016/j.ress.2005.06.004	user interface design;human performance technology;simulation;similarity;computer science;engineering;artificial intelligence;similitude;central nervous system;predictive modelling;user interface;response time;cerebro;similarity heuristic	AI	-45.38830398735031	-55.73554438330875	185742
36a823258fac2f4851145e59c0be75c0227adbc7	mairievox: a voice-activated information system	server;systeme a activation vocale;modele markov cache;reconocimiento palabra;relacion hombre maquina;man machine relation;red telefonica;mairievox;isolated words;serveur;application industrielle;mot isole;palabra aislada;speech recognition;industrial application;relation homme machine;industrial applications;reconnaissance parole;information system;isolated word;telephone network;reseau telephonique;voice activated system;systeme information;aplicacion industrial;sistema informacion	In this paper, we describe the voice-activated interactive system MAIRIEVOX which has been developed at the CNET. We introduce successively the main features of the system (speech recognition algorithms, hardware, vocal dialogue between the user and the system). We present the associated ergonomy studies and we give some recent results of field evaluations conducted during the last two years. Finally, we describe some of the French industrial applications, developed from this experimental system MAIRIEVOX.	algorithm;cnet;experimental system;human factors and ergonomics;information system;interactivity;speaker recognition;speech recognition	Christian Gagnoulet;Denis Jouvet;J. Damay	1991	Speech Communication	10.1016/0167-6393(91)90025-O	information transfer;speech recognition;system of record;telephone network;telecommunications;computer science;artificial intelligence;information filtering system;management information systems;information processor;automated information system;information system;server	NLP	-44.61994531143478	-63.4955017287661	185775
9fdd5e7cbc13df1ed1323fe4b3496ea004df8112	dynamic test collections for retrieval evaluation	information retrieval;user simulation;sessions;test collections;evaluation	Batch evaluation with test collections of documents, search topics, and relevance judgments has been the bedrock of IR evaluation since its adoption by Salton for his experiments on vector space systems. Such test collections have limitations: they contain no user interaction data; there is typically only one query per topic; they have limited size due to the cost of constructing them. In the last 15-20 years, it has become evident that having a log of user interactions and a large space of queries is invaluable for building effective retrieval systems, but such data is generally only available to search engine companies. Thus there is a gap between what academics can study using static test collections and what industrial researchers can study using dynamic user data.  In this work we propose dynamic test collections to help bridge this gap. Like traditional test collections, a dynamic test collection consists of a set of topics and relevance judgments. But instead of static one-time queries, dynamic test collections generate queries in response to the system. They can generate other actions such as clicks and time spent reading documents. Like static test collections, there is no human in the loop, but since the queries are dynamic they can generate much more data for evaluation than static test collections can. And since they can simulate user interactions across a session, they can be used for evaluating retrieval systems that make use of session history or other user information to try to improve results.	experiment;interaction;relevance;simulation;web search engine	Ben Carterette;Ashraf Bah Rabiou;Mustafa Zengin	2015		10.1145/2808194.2809470	computer science;database;world wide web;information retrieval	Web+IR	-33.814450745346896	-53.401135710791294	186049
17b79f5fead0c5523ea58474746166962320d74e	everything you always wanted to know about a dataset: studies in data summarisation		Summarising data as text helps people make sense of it. It also improves data discovery, as search algorithms can match this text against keyword queries. In this paper, we explore the characteristics of text summaries of data in order to understand how meaningful summaries look like. We present two complementary studies: a data-search diary study with 69 students, which offers insight into the information needs of people searching for data; and a summarisation study, with a lab and a crowdsourcing component with overall 80 data-literate participants, which produced summaries for 25 datasets. In each study we carried out a qualitative analysis to identify key themes and commonly mentioned dataset attributes, which people consider when searching and making sense of data. The results helped us design a template to create more meaningful textual representations of data, alongside guidelines for improving data-search experience overall.	automatic summarization;crowdsourcing;diary studies;information needs;search algorithm	Laura Koesten;Elena Paslaru Bontas Simperl;Emilia Kacprzak;Tom Blount;Jeni Tennison	2018	CoRR			HCI	-34.73694389212446	-54.899318907040005	186117
9556b793c678e57249880186fc177dff6a14e94e	interactive interfaces to detect conceptual difference for knowledge acquisition	interfase usuario;groupware;decision tree;adquisicion del conocimiento;ergonomia;user interface;hombre;ergonomie;acquisition connaissance;arbol decision;knowledge acquisition;human;interface utilisateur;collecticiel;ergonomics;arbre decision;homme	Conceptual difference is a serious problem in group knowledge acquisition systems, especially when different people with different background participate in a group. In our former research, each a peace of knowledge held by people is expressed by a single decision tree, and conceptual difference is detected by checking the difference. Sometimes it occurs that the attributes and the values with conceptual difference do not appear in decision trees and fails to detect conceptual difference. In this paper, we propose a method to solve this problem by using decision trees with diverse structures which are produced by our genetic algorithm.	knowledge acquisition	Teruyuki Kondo;Naoki Saiwaki;Tetsuya Yoshida;Shogo Nishida	1997			conceptual model;computer science;knowledge management;artificial intelligence;human factors and ergonomics;operating system;decision tree;data mining;user interface	HCI	-44.57376189739112	-56.05346516302163	186206
cee47a3c059ed0fe729f4ad7fbce488465a593f1	empirical analysis and classification of database errors in scopus and web of science	web of science;omitted citation;data accuracy;phantom citation;error classification;scopus;database error	In the last decade, a growing number of studies focused on the qualitative/quantitative analysis of bibliometric-database errors. Most of these studies relied on the identification and (manual) examination of relatively limited samples of errors. Using an automated procedure, we collected a large corpus of more than 10,000 errors in the two multidisciplinary databases Scopus and Web of Science (WoS), mainly including articles in the Engineering-Manufacturing field. Based on the manual examination of a portion (of about 10%) of these errors, this paper provides a preliminary analysis and classification, identifying similarities and differences between Scopus and WoS. The analysis reveals interesting results, such as: (i) although Scopus seems more accurate than WoS, it tends to forget to index more papers, causing the loss of the relevant citations given/obtained, (ii) both databases have relatively serious problems in managing the socalled Online-First articles, and (iii) lack of correlation between databases, regarding the distribution of the errors in several error categories. The description is supported by practical examples concerning a variety of errors in the Scopus and WoS databases. © 2016 Elsevier Ltd. All rights reserved.	bibliometrics;database;scopus;text corpus;web of science	Fiorenzo Franceschini;Domenico A. Maisano;Luca Mastrogiacomo	2016	J. Informetrics	10.1016/j.joi.2016.07.003	scopus;computer science;data science;data mining;information retrieval	Web+IR	-39.86592719777552	-64.65321444857877	186339
03dd72487691c918a6b524a2f796ef3eb7b566c9	self-supervised relation extraction from the web	unsupervised learning;extraction information;modelizacion;entity relationship model;evaluation performance;web extraction;text;performance evaluation;red www;information extraction;text mining;evaluacion prestacion;extraction forme;reseau web;pattern learning;modelo entidad relacion;texte;apprentissage non supervise;modele entite relation;data mining;classification;automatic generation;relation extraction;modelisation;internet;extraccion forma;fouille donnee;pattern language;world wide web;texto;relationship extraction;modeling;busca dato;pattern extraction;clasificacion;extraccion informacion	Web extraction systems attempt to use the immense amount of unlabeled text in the Web in order to create large lists of entities and relations. Unlike traditional Information Extraction methods, the Web extraction systems do not label every mention of the target entity or relation, instead focusing on extracting as many different instances as possible while keeping the precision of the resulting list reasonably high. SRES is a self-supervised Web relation extraction system that learns powerful extraction patterns from unlabeled text, using short descriptions of the target relations and their attributes. SRES automatically generates the training data needed for its pattern-learning component. The performance of SRES is further enhanced by classifying its output instances using the properties of the instances and the patterns. The features we use for classification and the trained classification model are independent from the target relation, which we demonstrate in a series of experiments. We also compare the performance of SRES to the performance of the state-of-the-art KnowItAll system, and to the performance of its pattern learning component, which learns simpler pattern language than SRES.	entity;experiment;information extraction;ner model;pattern language;relationship extraction;statistical classification;world wide web	Benjamin Rozenfeld;Ronen Feldman	2007	Knowledge and Information Systems	10.1007/s10115-007-0110-6	relationship extraction;the internet;systems modeling;entity–relationship model;biological classification;computer science;artificial intelligence;machine learning;data mining;pattern language;database;world wide web	NLP	-35.975980894102804	-58.80042654258788	186607
7ae7563bbfd6ed2c986a2f7077782e46957758b0	augmented reality for aircraft maintenance training and operations support	augmented reality aircraft maintenance training operations support aviation accident air transportation safety human error maintenance employees computer based tools;computer based tools;computer based training aerospace computing air accidents air safety aircraft maintenance augmented reality;human computer interaction;air accidents;maintenance employees;computer graphics;graphics and multimedia;maintenance engineering;three dimensional displays feature extraction maintenance engineering cameras solid modeling streaming media;operations support;aerospace computing;streaming media;air safety;three dimensional displays;feature extraction;graphics and multimedia augmented reality aircraft maintenance computer graphics human computer interaction;solid modeling;computer based training;air transportation safety;augmented reality;aviation accident;human error;aircraft maintenance training;cameras;aircraft maintenance	Recent statistics on causes of aviation accidents and incidents demonstrate that to increase air-transportation safety, we must reduce human errors' impact on operations. So, the industry should first address human factors related to people in stressful roles to significantly minimize such errors. In particular, aviation maintenance employees work under high-pressure conditions- that is, they're under strict time constraints and must adhere to stringent guidelines. Because of such constraints, they might be prone to making errors. Unfortunately, many of these errors might not become apparent until an accident occurs. Although maintenance errors are a recognized threat to aviation safety, there are few simulation and computer-based tools for managing human factor issues in this field. The main advantages in using computer-based systems to train or support technicians are that computers don't forget and that they can help humans clearly understand facts. Such features can help reduce errors due to procedure violations, misinterpretation of facts, or insufficient training. Toward that end, augmented reality (AR) is a promising technology to build advanced interfaces using interactive and wearable visualization systems to implement new methods to display documentation as digital data and graphical databases. Nevertheless, many factors-such as cumbersome hardware, the need to put markers on the aircraft, and the need to quickly create digital content-seem to hinder its effective implementation in industry.	aircraft accidents;augmented reality;aviation;computers;digital data;digital recording;documentation;graphical user interface;hl7publishingsubsection <operations>;human factors and ergonomics;humans;published database;simulation;software maintenance;standard operating procedure;wearable computer	Francesca De Crescenzio;Massimiliano Fantini;Franco Persiani;Luigi di Stefano;Pietro Azzari;Samuele Salti	2011	IEEE Computer Graphics and Applications	10.1109/MCG.2011.4	maintenance engineering;computer vision;augmented reality;simulation;human error;aircraft maintenance;feature extraction;computer science;operating system;solid modeling;computer graphics;computer graphics (images);mechanical engineering	HCI	-48.03492504557708	-57.36050916411504	186714
a6a4e33f1ec24952fcea873aa0298b4aa40be480	haptic shared control in tele-manipulation: effects of inaccuracies in guidance on task execution	haptic transparency quality haptic shared control tele manipulated task execution control actions haptic guidance forces human in the loop experiment tele manipulated assembly task virtual environment natural haptic feedback quality task performance constrained translational movement;measurement;control design;remote handling;welding;haptic shared control tele manipulation remote handling haptics;haptics;force;visualization;haptic interfaces electron tubes force welding measurement visualization control design;virtual reality haptic interfaces;tele manipulation;haptic interfaces;haptic shared control;electron tubes	Haptic shared control is a promising approach to improve tele-manipulated task execution, by making safe and effective control actions tangible through guidance forces. In current research, these guidance forces are most often generated based on pre-generated, errorless models of the remote environment. Hence such guidance forces are exempt from the inaccuracies that can be expected in practical implementations. The goal of this research is to quantify the extent to which task execution is degraded by inaccuracies in the model on which haptic guidance forces are based. In a human-in-the-loop experiment, subjects (n = 14) performed a realistic tele-manipulated assembly task in a virtual environment. Operators were provided with various levels of haptic guidance, namely no haptic guidance (conventional tele-manipulation), haptic guidance without inaccuracies, and haptic guidance with translational inaccuracies (one large inaccuracy, in the order of magnitude of the task, and a second smaller inaccuracy). The quality of natural haptic feedback (i.e., haptic transparency) was varied between high and low to identify the operator's ability to detect and cope with inaccuracies in haptic guidance. The results indicate that haptic guidance is beneficial for task execution when no inaccuracies are present in the guidance. When inaccuracies are present, this may degrade task execution, depending on the magnitude and the direction of the inaccuracy. The effect of inaccuracies on overall task performance is dominated by effects found for the Constrained Translational Movement, due to its potential for jamming. No evidence was found that a higher quality of haptic transparency helps operators to detect and cope with inaccuracies in the haptic guidance.	elegant degradation;genetic translation process;haptic device component;haptic technology;human factors and ergonomics;large;percent (qualifier value);radio jamming;referential transparency;small;tcap gene;television;virtual reality;benefit;positive regulation of water channel activity involved in maintenance of lens transparency	Jeroen van Oosterhout;Jeroen G. W. Wildenbeest;Henri Boessenkool;Cock Heemskerk;Marco R. de Baar;Frans C. T. van der Helm;David A. Abbink	2015	IEEE Transactions on Haptics	10.1109/TOH.2015.2406708	stereotaxy;control engineering;computer vision;simulation;visualization;computer science;haptic technology;force;physics;quantum mechanics;measurement;welding	Visualization	-43.48163582420419	-52.19742998586653	187067
adc98ac754f2c6ed33133a771f8cb40c58757113	the evaluation of adaptive and personalised information retrieval systems: a review	user adaptive systems;adaptive information retrieval systems;user centred evaluations;personalised information retrieval systems	A current problem with the research of adaptive systems is the inconsistency of evaluation applied to the adaptive systems. However, evaluating an adaptive system is a difficult task due to the complexity of such systems. Evaluators need to ensure correct evaluation methods and measurement metrics are used. This paper reviews a variety of evaluation techniques applied in adaptive and user-adaptive systems. More specifically, it focuses on the user-centred evaluation of adaptive systems such as personalised recommender systems and adaptive information retrieval systems. The review tackles the question of ‘How have user-centred evaluations of adaptive and user-adaptive systems been conducted and how can these evaluation practices be improved?’ Based on the analysed results of the: (a) evaluation approaches, (b) user-centred evaluation techniques, and (c) evaluation metrics, we propose an evaluation framework for end-user experience in evaluating adaptive systems (EFEx).	adaptive system;emoticon;information retrieval;next-generation network;portals;recommender system;usability;user experience	Catherine Mulwa;Séamus Lawless;Mary Sharp;Vincent P. Wade	2011	I. J. Knowledge and Web Intelligence	10.1504/IJKWI.2011.044120	computer science;data mining;multimedia;information retrieval	Web+IR	-42.21765915291389	-58.3374664748197	187205
b87aaf31b85a86054f90c910a3b3df3b9b3c2bfa	design and usability evaluation of a novice user-oriented control panel for lighting and air conditioning	use;computadora;ergonomia aplicada;air conditioning;ergonomie conception;usability evaluation;ordinateur;relacion hombre maquina;hombre;man machine relation;computer;utilizacion;utilisation;acondicionamiento aire;conditionnement air;eclairage;human;evaluation;relation homme machine;lighting;evaluacion;control panel;novice;panel de mando;tableau commande;applied ergonomics;alumbrado;homme	The objective of this paper is to design a control panel for novice users based upon new concepts for lighting and air conditioning. First we investigated the contents of tasks and the terms used in commercial sheets and control panels of lighting and air c<nditioning. Second we made up design concepts of new panels for novice users, and designed four panels separately by iterative design method and one panel by the parallel design method. Usability tests carried out those five control panels. Thirteen subjects were instructed to do four tasks in the experiment. Parameters for assessing usability were operating time, error rates, the number of times referring to the users' manual and subjective ratings. The results showed that the panel designed by parallel design method was superior to the other panels in operating time, error rate and the number of referring to operating manuals.	control panel (software);usability	Kazunari Morimoto;Takao Kurokawa;Masahito Hata;Noriyuki Kushiro;Masahiro Inoue	1997			simulation;air conditioning;computer science;engineering;electrical engineering;evaluation;lighting;engineering drawing	HCI	-45.554529970579544	-55.41918657220121	187235
ada9d264d5a57dc1ec2431525ff79154e0364f36	e-learning for health issues based on rule-based reasoning and multi-criteria decision making	multi criteria decision making;rule based reasoning	The paper presents an e-learning system called INTATU, which provides education on Atheromatosis. Atheromatosis is a disease that is of interest not only to doctors, but also to common users without any medical background. For this purpose, the system maintains and processes information about the users’ interests and background knowledge and provides individualized learning for the domain of Atheromatosis. More specifically, the reasoning mechanism in INTATU uses a novel combination of rule-based reasoning and a multi-criteria decision making theory called SAW for selecting the theory topics that appear to be most appropriate for a particular user with respect to his/her background knowledge and interest.	logic programming;personalization;stereotype (uml)	Katerina Kabassi;Maria Virvou;George A. Tsihrintzis	2007			rule-based system;r-cast;influence diagram;qualitative reasoning;decision analysis;decision engineering;computer science;knowledge management;model-based reasoning;decision rule;management science;reasoning system;evidential reasoning approach;business decision mapping	AI	-48.204295250643945	-63.72827826264167	187456
08f93aa24c93f27b970edb9630071fda4bf58d25	beyond topical similarity: a structural similarity measure for retrieving highly similar documents	busqueda informacion;document structure;text order;raisonnement base sur cas;text;razonamiento fundado sobre caso;haute performance;estructura documental;information retrieval;structure document;user study;texttiling;optimal matching;recommandation;metric;texte;similitude;recherche information;similarity;alto rendimiento;recomendacion;document structural similarity;recommendation;metrico;similitud;case based reasoning;texto;high performance;bag of words;similarity measure;subtopic structure;structural similarity;metrique;document similarity	Accurately measuring document similarity is important for many text applications, e.g. document similarity search, document recommendation, etc. Most traditional similarity measures are based only on “bag of words” of documents and can well evaluate document topical similarity. In this paper, we propose the notion of document structural similarity, which is expected to further evaluate document similarity by comparing document subtopic structures. Three related factors (i.e. the optimal matching factor, the text order factor and the disturbing factor) are proposed and combined to evaluate document structural similarity, among which the optimal matching factor plays the key role and the other two factors rely on its results. The experimental results demonstrate the high performance of the optimal matching factor for evaluating document topical similarity, which is as well as or better than most popular measures. The user study shows the good ability of the proposed overall measure with all three factors to further find highly similar documents from those topically similar documents, which is much better than that of the popular measures and other baseline structural similarity measures.	bag-of-words model;baseline (configuration management);direction finding;experiment;many-to-many;one-to-one (data model);optimal matching;similarity measure;similarity search;structural similarity;usability testing	Xiaojun Wan	2006	Knowledge and Information Systems	10.1007/s10115-006-0047-1	case-based reasoning;semantic similarity;similarity;metric;computer science;bag-of-words model;similitude;document structure description;structural similarity;optimal matching;data mining;mathematics;world wide web;information retrieval	Web+IR	-36.125290786595514	-58.75632075442233	187657
df7522d1d0f53e47cb6dbddca93de652096e1e19	in vitro evaluation of a program for machine-aided indexing	engineering;bibliotheque;machine aided indexing;lenguaje natural;linguistique;comparative analysis;document analysis;indexation automatique;information retrieval;key phrase indexing;langage naturel;controlled indexing;tratamiento lenguaje;thesauri;evaluation methods;ingenierie;free indexing;linguistica;multi word terms;indexing;language processing;natural language;human evaluation;indexation;traitement langage;indizacion;automatic indexing;ingenieria;evaluation;evaluacion;biblioteca;scientific knowledge;library;language engineering;indizacion automatica;linguistics	This article presents the human evaluation of ILIAD, a program for machine-aided indexing (MAI). It consists of two language engineering modules and is designed to assist expert librarians in computer-aided indexing and document analysis. Our aim is the expert evaluation of automatic multi-word term indexing. Evaluation is performed by documentary engineers. Cataloging and indexing are their principal tasks. They also have a good scientific knowledge of the domain to which the indexed documents belong.We first present the ILIAD program and the two systems submitted to this evaluation, the methodology (protocol) adopted, the differences between the protocol and the implementation, and the results of these evaluations. Human evaluation is divided into three parts: firstly the evaluation of controlled indexing, then free indexing and finally term variant extraction performed during controlled indexing. Finally, we analyze the relevance of this evaluation by calculating the agreement frequency and the Kappa coefficient and propose some future developments.		Christian Jacquemin;Béatrice Daille;Jean Royauté;Xavier Polanco	2002	Inf. Process. Manage.	10.1016/S0306-4573(01)00050-4	natural language processing;speech recognition;computer science;evaluation;linguistics;world wide web;information retrieval	DB	-34.917617162362596	-63.255965583324844	188371
b3c21d28cfbdf8e9d393e9c11394b42b53102629	retrieval language of social sciences and natural sciences: a statistical investigation	word frequency;information retrieval;social sciences;thesauri;natural sciences;social science;statistical analysis;indexing;tables data;natural science;subject index terms	Abstract#R##N##R##N#Current guidelines for retrieval thesaurus design and maintenance recommend the use of relative frequency of occurrence of descriptors for the design and updating of a thesaurus. This study examined the extent to which Zipf and Whatmough's theory of quantitative semantics provides the theoretical justification for the current practice. The retrieval language of social sciences and science/technology was analyzed to test if the observed data on relative frequency conforms to the theoretical distribution of Zipf's law. Findings demonstrate that the principle of least effort serves to explain the pattern of use of the retrieval language in social sciences but not in science/technology. For social sciences, one may assume that the frequently occurring descriptors are indeed becoming semantically ambiguous thus requiring a greater variety of access points to the concepts that the more frequently occurring descriptors represent. No theoretical justification was found for using the frequency data in the design and updating of the descriptors of science/technology. Thus, this finding challenges the theoretical basis for the present practice of using frequency data for the design and updating of the retrieval thesauri of science/technology.		Chai Kim	1982	JASIS	10.1002/asi.4630330102	natural language processing;library science;natural science;social science;computer science;data mining;database;linguistics;world wide web;information retrieval	Theory	-39.30334530126441	-62.61206039299625	189028
1ede815b17c091bdbe55d86842059027317d66b7	multimodal fusion for video search reranking	busqueda informacion;search engine;buscador;video retrieval multimodal fusion video search reranking search engine log top ranked documents cr reranking method cross reference strategy;multimedia;modele agrege;multimodal fusion;supervised learning;recherche image;search engines;fuses;information retrieval;pertinencia;modelo agregado;video retrieval;video retrieval content based retrieval search engines sensor fusion;layout;search engines information retrieval metasearch layout feedback boosting chromium fuses image retrieval multimedia databases;high precision;classification;top ranked documents;journal;cross reference strategy;metasearch;cr reranking method;base donnee multimedia;fichier log;feedback;boosting;fichero actividad;senal video;signal video;recherche information;clustering;chromium;video search reranking;precision elevee;pertinence;multimedia databases clustering image video retrieval;multimedia databases;precision elevada;aggregate model;video signal;busqueda de imagen;relevance;apprentissage supervise;moteur recherche;sensor fusion;image video retrieval;aprendizaje supervisado;multimedia database;content based retrieval;clasificacion;video search;log file;search engine log;image retrieval	Analysis on click-through data from a very large search engine log shows that users are usually interested in the top-ranked portion of returned search results. Therefore, it is crucial for search engines to achieve high accuracy on the top-ranked documents. While many methods exist for boosting video search performance, they either pay less attention to the above factor or encounter difficulties in practical applications. In this paper, we present a flexible and effective reranking method, called CR-Reranking, to improve the retrieval effectiveness. To offer high accuracy on the top-ranked results, CR-Reranking employs a cross-reference (CR) strategy to fuse multimodal cues. Specifically, multimodal features are first utilized separately to rerank the initial returned results at the cluster level, and then all the ranked clusters from different modalities are cooperatively used to infer the shots with high relevance. Experimental results show that the search quality, especially on the top-ranked results, is improved significantly.	acm computing classification system;boosting (machine learning);cross-reference;modality (human–computer interaction);multimodal interaction;rc circuit;relevance;web search engine	Shikui Wei;Yao Zhao;Zhenfeng Zhu;Nan Liu	2010	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2009.145	image retrieval;computer science;machine learning;data mining;database;multimedia;supervised learning;world wide web;information retrieval;search engine	Web+IR	-34.48916760713157	-58.55246749059679	190127
5bf74a9b5bae41fbaacb147b30ac150753bcad5b	car backlight position and fog density bias observer-car distance estimates and time-to-collision judgments	150 psychologie;driving;pedestrian safety;percepcion espacio;automovil;poison control;rear lighting;conduccion vehiculo;injury prevention;perception espace;simulation;conduite vehicule;hombre;safety literature;vehicle driving;location;traffic safety;distance estimation;injury control;collision avoidance system;colision;judgment human characteristics;home safety;vehicle lighting;collision avoidance systems;injury research;control system;perception temps;safety abstracts;human factors;visibility;road safety engineering and vehicles;automobile;motor car;occupational safety;safety;human;distance perception;vehicle design;weather condition;experiments;rear light;distancia;safety research;brouillard;estimating;accident prevention;visual perception;drivers;violence prevention;vehicle safety;bicycle safety;collision;poisoning prevention;niebla;time to collision;falls;computer simulation;visibility distance;ergonomics;suicide prevention;space perception;distance;time perception;percepcion tiempo;homme;fog	OBJECTIVE A series of experiments assessed biases in perceived distance that occur while driving as a function of the backlight position of the car ahead and fog density.   BACKGROUND V. Cavallo, M. Colomb, and J. Doré (2001) have shown that smaller horizontal backlight separation and fog may result in increased estimates of the distance between an observer and a car of which only the backlights are visible. They also predicted that raising the height of the car backlights would lead to increasing distance estimates.   METHOD Distance perception was assessed in both static and dynamic computer-simulated scenarios in which the distance estimates were performed using a familiarized analog scale or using time-to-collision judgments for both pairs of backlights and single backlights.   RESULTS In a series of five experiments, the horizontal separation and fog density effects were replicated. In addition, distance estimates were consistently larger with higher than with lower vertical backlight positions.   CONCLUSION There is reason to believe that biases in distance perception may be augmented by car backlight positions and by low-visibility weather conditions.   APPLICATION Car designers should take backlight placement seriously. Speed-dependent car-to-car distance control systems seem desirable to counteract biases in distance perception.	analog;backlight;chimeric antigen receptor;computer simulation;control system;estimated;experiment;judgment;large;collision	Axel Buchner;Martin Brandt;Raoul Bell;Judith Weise	2006	Human factors	10.1518/001872006777724363	psychology;estimation;simulation;fog;visual perception;visibility;computer science;engineering;suicide prevention;human factors and ergonomics;injury prevention;transport engineering;forensic engineering;location;distance;time perception;collision	Visualization	-45.93952111219355	-54.37748599427543	190187
23fabb4c37529ca889253436902026497654af22	iclef 2004 track overview: pilot experiments in interactive cross-language question answering	busqueda informacion;sistema interactivo;cross language evaluation forum;repondeur;information retrieval;communication ecrite;tratamiento lenguaje;comunicacion escrita;information access;systeme conversationnel;evaluation measure;language processing;interactive system;recherche information;traitement langage;responder;acces information;acceso informacion;written communication;multilinguisme;multilingualism;contestador;question answering;multilinguismo	For the 2004 Cross-Language Evaluation Forum (CLEF) interactive track (iCLEF), five participating teams used a common evaluation design to assess the ability of interactive systems of their own design to support the task of finding specific answers to narrowly focused questions in a collection of documents written in a language different from the language in which the questions were expressed. This task is an interactive counterpart to the fully automatic cross-language question answering task at CLEF 2003 and 2004. This paper describes the iCLEF 2004 evaluation design, outlines the experiments conducted by the participating teams, and presents some initial results from analyses of official evaluation measures that were reported to each participating team.	question answering	Julio Gonzalo;Douglas W. Oard	2004		10.1007/11519645_32	natural language processing;question answering;computer science;artificial intelligence;database;writing;world wide web;information retrieval	NLP	-35.392023578368466	-62.91510438942513	191262
433fc57d1a7dfe2ed624fd358d49f8c4593e9339	medical imagistic database query for educational purpose	topic maps;color feature;medical image databases;image database;texture features;medical image database;medical image;medical e learning;content based visual query;access method;multimedia database;database query;medical diagnosis;texture feature	The paper presents original query modalities on a multimedia database that stores medical images and the associated information, for educational goal. So, a modern and efficient system for professional accomplishment is offered to the medical superior education (including residents, young specialists, family doctors and medical assistants). Specialists can update the medical image database with images acquired from different patients in the diagnosis and treatment process. A series of alphanumerical information: diagnosis, treatment and patient evolution can be added for each image. The database can be browsed, simply text-based queried or content-based queried using colour and texture characteristics automatically extracted from medical images at their loading in the database. An original element is the presence of a topic map based on a part of MeSH thesaurus, the part that includes the medical diagnosis names. The student can navigate through topic map depending on its interest subject, having in this way big advantages. He does not have to be familiar with the logic of the database, he will learn about the semantic context, in which a collection and its single items are embedded and he may find useful items he would not have expected to find in the beginning. Also, semantic queries against the multimedia database can be automatically launched with the help of the topic map. All these access paths can be combined for retrieving the interest information. Using content-based visual query with other access methods on a teaching image database allows students to see images and associated information from database in a simple and direct manner. This method stimulates learning, by comparing similar cases along with their particularities, or by comparing cases that are visually similar, but with different diagnoses.		Liana Stanescu;Dumitru Dan Burdescu;Gabriel Mihai;Anca Loredana Ion;Cosmin Stoica Spahiu	2008		10.1007/978-3-540-89962-4_13	intelligent database;computer science;data mining;database;view;database schema;information retrieval;alias;database design	DB	-48.248263836521694	-63.41412165442862	191599
f5043c254278de566b7a51d5e17a5a41aeb2efb8	on the decaying utility of news recommendation models		For how long will a recommendation model provide adequate recommendations? The answer to this question depends on the kind of model, its underlying data, and the domain among other factors. We analyse four types of models in the news domain on how their predictive performances change. Our observations show that replacing or updating models is necessary to maintain high predictive performance. The evaluation suggests that an exponential decay model describes the changing predictive performance accurately.	code coverage;interaction;performance;recommender system;time complexity	Benjamin Kille;Sahin Albayrak	2017			computer science;data mining;information retrieval	Web+IR	-33.711964046288685	-53.852962691906264	191957
49a27b932b957939c6a990c56a38aa37c95d77dd	term impacts as normalized term frequencies for bm25 similarity scoring	term frequency;document retrieval	The  BM25  similarity computation has been shown to provide effective document retrieval. In operational terms, the formulae which form the basis for  BM25  employ both term frequency and document length normalization. This paper considers an alternative form of normalization using document-centric impacts, and shows that the new normalization simplifies  BM25  and reduces the number of tuning parameters. Motivation is provided by a preliminary analysis of a document collection that shows that impacts are more likely to identify documents whose lengths resemble those of the relevant judgments.Experiments on TREC data demonstrate that impact-based  BM25  is as good as or better than the original term frequency-based  BM25  in terms of retrieval effectiveness.		Vo Ngoc Anh;Raymond Wan;Alistair Moffat	2008		10.1007/978-3-540-89097-3_7	document retrieval;computer science;pattern recognition;data mining;term discrimination;tf–idf;information retrieval	NLP	-33.74103894230567	-61.75978501563551	191979
a1693e282803bc5a6c466fbbab54dd676c569308	ranking web news via homepage visual layout and cross-site voting	busqueda informacion;modelizacion;news;red www;event graph;information source;source information;information retrieval;reseau web;modelisation;hierarchical classification;internet;recherche information;voting;graphe evenement;preferencia;classification hierarchique;noticias;world wide web;preference;voto;grafo aconticimiento;information system;vote;actualites;modeling;clasificacion jerarquizada;systeme information;fuente informacion;sistema informacion	Reading news is one of the most popular activities when people surf the internet. As too many news sources provide independent news information and each has its own preference, detecting unbiased important news might be very useful for users to keep up to date with what are happening in the world. In this paper we present a novel method to identify important news in web environment which consists of diversified online news sites. We observe that a piece of important news generally occupies visually significant place in some homepage of a news site and import news event will be reported by many news sites. To explore these two properties, we model the relationship between homepages, news and latent events by a tripartite graph, and present an algorithm to identify important news in this model. Based on this algorithm, we implement a system TOPSTORY to dynamically generate homepages for users to browse important news reports. Our experimental study indicates the effectiveness of proposed approach.	cross-site scripting	Jinyi Yao;Jue Wang;Zhiwei Li;Mingjing Li;Wei-Ying Ma	2006		10.1007/11735106_13	voting;news;computer science;data mining;world wide web;information system;news aggregator	HCI	-36.07468204994615	-57.47295798003292	192201
cfb27e8903122a2dcbc2c58ed3d5cf4530f62d49	gender-specific information search behavior	information search	This paper presents an empirical gender study in the context of information science. It discusses an exploratory investigation, which provides empirical data about differences of information seeking activities by female and male students. The research focus was on whether there are gender-specific differences when people perform searches with the aid of general search engines and specialized Deep Web information services. It has been observed how the participants behaved in getting information and how the gender groups differ from each other. Which search system (search engine or professional information supplier) has been preferred by the gender groups at first? How did the gender groups search (applied operators, modifications of query formulations, targeted search)? How far were the users satisfied with their results? For data collection paper-questionnaires have been used and the answers have been statistically analyzed with the help of SPSS. The questionnaire consisted of four different search tasks each with seven sub-questions. The research and the obtained result data indicate at least in the choice of search sources, the satisfaction with this source and the results obtained a gender-specific difference. Men tried to use professional information services as well as search engines for search, regardless of the difficulty of the search task and its formulation. In contrast, women behaved cautiously in choosing search sources. They decided either on sources, which they knew skillfully or where their use was assigned. Women were generally more satisfied with the obtained results as men. These data can represent an initial approach for further analysis of gender-specific Web search behavior.		Parinaz Maghferat;Wolfgang G. Stock	2010	Webology		computer science;data mining;world wide web;information retrieval	Web+IR	-35.830458657616724	-53.208487298348366	192351
0e66c4c8d64372c065812e6c195de5ffdf47baeb	a basic framework to build a test collection for the vietnamese text catergorization	col;text categorization;test collection	The aim of this paper is to present a basic framework to build a test collection for a Vietnamese text categorization. The presented content includes our evaluations of some popular text categorization test collections, our researches on the requirements, the proposed model and the techniques to build the BKTexts test collection for a Vietnamese text categorization. The XML specification of both text and metadata of Vietnamese documents in the BKTexts also is presented. Our BKTexts test collection is built with the XML specification and currently has more than 17100 Vietnamese text documents collected from e-newspapers.	categorization;document classification;requirement;xml	Hoang Anh Viet;Thu Dinh-Thi-Phuong;Huynh Quyet Thang	2008			computer science;data mining;database;mountain pass;information retrieval	Web+IR	-33.700059667377076	-65.18050230470277	192607
94d30aa5ca31dc4a28c9223884e853f39c09be47	measuring structural similarity of document pages for searching document image databases	database system;pattern analysis;document management;document structure;image retrieval;structural similarity	Current document management and database systems provide text search and retrieval capabilities, but generally lack the ability to utilize the documents’ logical and physical structures. This paper describes a general system for document image retrieval that is able to make use of document structure. It discusses the use of structural similarity for retrieval; it defines a measure of structural similarity between document images based on content area overlap, and also compares similarity ratings based on this measure with human relevance judgments.	database;image retrieval;relevance;similarity measure;structural similarity	Christian K. Shin;David S. Doermann;Azriel Rosenfeld	2003			automatic image annotation;visual word;content-based image retrieval;document retrieval;full text search;database;information retrieval;data mining;image retrieval;computer science;document management system;document clustering	Web+IR	-34.642460762585515	-60.52709987431623	193091
c63858cf12d021a59a0ae2b173bd742c49e87e4a	comparing student and expert-based tagging of recorded lectures	recorded lectures;vector space modelling;cosine similarity;tagging	In this paper we analyse the way students tag recorded lectures. We compare their tagging strategy and the tags that they create with tagging done by an expert. We look at the quality of the tags students add, and we introduce a method of measuring how similar the tags are, using vector space modelling and cosine similarity. We show that the quality of tagging by students is high enough to be useful. We also show that there is no generic vocabulary gap between the expert and the students. Our study shows no statistically significant correlation between the tag similarity and the indicated interest in the course, the perceived importance of the course, the number of lectures attended, the indicated difficulty of the course, the number of recorded lectures viewed, the indicated ease of finding the needed parts of a recorded lecture, or the number of tags used by the student.	cosine similarity;tag (metadata);vocabulary	Pierre Gorissen;Jan van Bruggen;Wim M. G. Jochems	2013	Education and Information Technologies	10.1007/s10639-013-9271-y	computer science;data science;multimedia;world wide web;vector space model;information retrieval	HCI	-39.95369905263822	-59.93760912882653	193623
468f5b1b60c72e97313cdc6b2cfbdb4572327e8d	on caption bias in interleaving experiments	interleaving;implicit feedback;information retrieval evaluation;user preferences;universiteitsbibliotheek;ranking function;personalized search;evaluation	Information retrieval evaluation most often involves manually assessing the relevance of particular query-document pairs. In cases where this is difficult (such as personalized search), interleaved comparison methods are becoming increasingly common. These methods compare pairs of ranking functions based on user clicks on search results, thus better reflecting true user preferences. However, by depending on clicks, there is a potential for bias. For example, users have been previously shown to be more likely to click on results with attractive titles and snippets. An interleaving evaluation where one ranker tends to generate results that attract more clicks (without being more relevant) may thus be biased.  We present an approach for detecting and compensating for this type of bias in interleaving evaluations. Introducing a new model of caption bias, we propose features that model bias based on (1) per-document effects, and (2) the (pairwise) relationships between a document and surrounding documents. We show that our model can effectively capture click behavior, with best results achieved by a model that combines both per-document and pairwise features. Applying this model to re-weight observed user clicks, we find a small overall effect on real interleaving comparisons, but also identify a case where initially detected preferences vanish after caption bias re-weighting is applied. Our results indicate that our model of caption bias is effective and can successfully identify interleaving experiments affected by caption bias.	bias–variance tradeoff;comparison shopping website;experiment;exponent bias;forward error correction;information retrieval;personalized search;relevance;sensor;user (computing)	Katja Hofmann;Fritz Behr;Filip Radlinski	2012		10.1145/2396761.2396780	interleaving;computer science;evaluation;multimedia;world wide web;information retrieval	Web+IR	-33.7818932055464	-52.34544423487584	193654
62041d2787ee911a2a76726f63b7159460966a18	algorithm for web services matching	busqueda informacion;linear algebra;decomposition valeur singuliere;keyword;information retrieval;singular value decomposition;service web;semantics;palabra clave;mot cle;web service;semantica;semantique;feasibility;internet;keyword search;recherche information;algebre lineaire;defaillance;algebra lineal;decomposicion valor singular;functionality;fonctionnalite;failures;fallo;funcionalidad;practicabilidad;faisabilite;servicio web	UDDI is a standard for publishing and discovery of web services. UDDI registries provide keyword searches for web services. The search functionality is very simple and fails to account for relationships between web services. In this paper, we propose an algorithm which retrieves closely related web services. The proposed algorithm is based on singular value decomposition (SVD) in linear algebra, which reveals semantic relationships among web services. The preliminary evaluation shows the effectiveness and feasibility of the algorithm.	algorithm;linear algebra;singular value decomposition;web services discovery;web service	Atul Sajjanhar;Jingyu Hou;Yanchun Zhang	2004		10.1007/978-3-540-24655-8_72	web service;web modeling;the internet;data web;computer science;linear algebra;social semantic web;data mining;ws-addressing;database;semantics;ws-i basic profile;singular value decomposition;law;world wide web;universal description discovery and integration	Web+IR	-35.342216224497115	-59.21014225893369	193901
feca3f9b8c35266c4ab5520c2d969fe1f6db1083	integrating text retrieval and image retrieval in xml document searching	busqueda informacion;test hypothese;document structure;occupation time;caption;search engine;contenu image;image content;buscador;text;cbir;multimedia;information retrieval system;estructura documental;retrieval;recherche image;sous titrage;information retrieval;structure document;test hipotesis;xml language;subtitulo;image;texte;080106 image processing;080305 multimedia programming;recherche documentaire;080704 information retrieval and web search;temps occupation;recherche information;busqueda documental;image search;text retrieval;tiempo ocupacion;xml;xml document;image analysis;document retrieval;information system;moteur recherche;contenido imagen;texto;content based retrieval;systeme information;recherche par contenu;langage xml;lenguaje xml;sistema informacion;hypothesis test;image retrieval	Many XML documents contain a mixture of text and images. Images play an important role in webpage or article presentation. However, popular Information Retrieval systems still largely depend on pure text retrieval as it is believed that text descriptions including body text and the caption of images contain precise information. On the other hand, images are more attractive and easier to understand than pure text. We assume that if the image content is used in addition to the pure text-based retrieval, the retrieval result should be better than text-only or image-only retrieval. We test this hypothesis by doing a series of experiments using the Lonely Planet XML document collection. Two search engines, an XML document search engine using both content and structure based on text, and a content-based image search engine were used at the same time. The results generated by these two search engines were merged together to form a new result. This paper presents our current work, initial results and vision into future work.	archive;computer vision;document retrieval;experiment;expression (computer science);image retrieval;information needs;information retrieval;long tail;principal component analysis;relevance;search algorithm;software quality;text-based (computing);text-based user interface;web page;web search engine;xml namespace	Dian Tjondronegoro;Jinglan Zhang;Jinfeng Gu;Anthony N. Nguyen;Shlomo Geva	2005		10.1007/978-3-540-34963-1_39	document retrieval;xml validation;full text search;visual word;cognitive models of information retrieval;computer science;concept search;database;world wide web;information retrieval;human–computer information retrieval	Web+IR	-35.942033802080694	-58.57940048573415	194032
027ddd609d616db74447d1163168c5c455819e81	analyzing effect of roles on search performance and query formulation in collaborative search	role based collaborative search;collaborative information retrieval	We investigate how explicit search roles assigned to group members affect their search performance and behavior in collaborative information seeking (CIS). Although several roles have been proposed in CIS, how these roles affect the search performances and behaviors of the members has not yet been explored. We focus on the existing Gatherer and Surveyor roles and analyze their effects on search performances and query formulation behaviors. The goal of our study is to understand the relationships between the roles and search behaviors and get insights into developing algorithms such as query suggestions or document rankings adaptive to the roles and behaviors. We conducted a user study with 20 participants in 10 pairs, where each pair of Gatherer and Surveyor were asked to perform a recall-oriented collaborative search task. We first analyzed the search performance of the two roles in terms of recall and diversity. We also analyzed how their queries were affected by their preceding queries or webpages that were visited through a questionnaire and log analysis. Finally, we discussed what algorithms would be required to support role-based CIS.	algorithm;collaborative information seeking;log analysis;performance;usability testing	Takehiro Yamamoto;Mitsuo Yamamoto;Katsumi Tanaka	2015		10.1145/2812376.2812377	computer science;data mining;world wide web;information retrieval	HCI	-35.661183876362095	-52.785069320097385	194065
f719f5a1c5b752f48115e31c58dba8d9251e35d9	a sampling-based tool for plagiarism detection in student texts		This paper introduces AntiPlag, an advanced plagiarism detection tool intended for use on student texts. It is capable of both hermetic detection that scrutinizes only local collections of documents (other students’ texts and lecture materials, for example) and web plagiarism detection, in which the aim is at identifying instances of plagiarism that have been sourced from the Internet. The main feature of the system is the sampling-based web plagiarism detection, a novel approach to plagiarism detection that is based on combining web and hermetic search technologies. The system uses standard web search engines to locate documents on the Internet that might have been used as sources of plagiarism by the writer of a text. During this sampling phase, the suspected sources are downloaded, converted to ASCII text and saved to the local database so that they can be later processed by using the hermetic detection methods. We evaluated the system by using a test set that contained instances of verbatim copying as well as texts in which plagiarism was concealed by minor editing, replacing words with synonyms and by paraphrasing. We compared the results achieved by AntiPlag to an earlier evaluation study of four web plagiarism detection systems, SafeAssignment, TurnitIn, EVE2 and Plagiarism-Finder. AntiPlag performed better than any of these systems, achieving the accuracy 95.8% over all the test items.	edit distance;fingerprint;internet;landauer's principle;latent semantic analysis;sampling (signal processing);sensor;test set;tracking system;web search engine;word lists by frequency	Tuomo Kakkonen;Niko Myller	2009	CoRR		computer science;data mining;world wide web;information retrieval	ML	-35.37364011710779	-65.63142802069889	194419
42d9af483312dd671f5662e4aa0bcb2065a3f30b	machine learning for automatic annotation of references in dh scholarly papers			machine learning	Youngmin Kim;Patrice Bellot;Elodie Faath;Marin Dacos	2012			information retrieval;computer science;annotation	NLP	-38.385929180812546	-65.58053556111207	194452
4581859a526eb0f1f59a62b5ac1496aca4d38317	beyond keyword and cue-phrase matching: a sentence-based abstraction technique for information extraction	extraction information;shallow text processing;modelizacion;automatic;continuous function;connectionist models;text;linguistique;keyword;donnee textuelle;extraction connaissance;multimedia;information extraction;dato textual;frase;text processing;analisis cuantitativo;knowledge extraction;pertinencia;abstraction;fonction continue;palabra clave;automatico;texte;mot cle;abstraccion;modelisation;sentence;linguistica;analyse quantitative;funcion continua;continuous simulation;pertinence;decouverte connaissance;textual data;extraction connaissances;automatique;quantitative analysis;extraccion conocimiento;descubrimiento conocimiento;phrase;relevance;connectionist model;texto;modeling;automatic summary;extraccion informacion;knowledge discovery;linguistics	With the explosion in the quantity of on-line text and multimedia information in recent years, there has been a renewed interest in the automated extraction of knowledge and information in various disciplines. In this paper, we provide a novel quantitative model for the creation of a summary by extracting a set of sentences that represent the most salient content of a text. The model is based on a shallow linguistic extraction technique. What distinguishes it from previous research is that it does not work on the detection of specific keywords or cue-phrases to evaluate the relevance of the sentence concerned. Instead, the attention is focused on the identification of the main factors in the textual continuity. Simulation experiments suggest that this technique is useful because it moves away from a purely keyword-based method of textual information extraction and its associated limitations. D 2005 Elsevier B.V. All rights reserved.	experiment;information extraction;online and offline;relevance;scott continuity;simulation	Samuel W. K. Chan	2006	Decision Support Systems	10.1016/j.dss.2004.11.017	natural language processing;continuous function;speech recognition;systems modeling;relevance;continuous simulation;computer science;quantitative analysis;artificial intelligence;data mining;abstraction;knowledge extraction;automatic transmission;law	AI	-36.45106127486747	-65.16158946912032	195366
a0c936c1924b2f62933cc95883755cda71d3dc88	evaluating on-line and off-line searching behavior using thinking-aloud protocols to detect navigation barriers	line search;searching behavior;navigation;world wide web;thinking aloud;hypertext	1. ABSTRACT To describe and analyse the searching behavior of people looking for information on-line or off-line we set up two small-scale experiments. The experiments showed that the subjects were able to find information off-fine more quickly than on the Web sites, but that on the other hand the answers given in the on-line mode were more accurate The search strategies in the on-line mode were also more divergent. Hyperlinks are often scanned separately from their context and subjects often seem to have very vague predictions of the contents behind the link (kind of information, amount of information).	experiment;hyperlink;online and offline;vagueness;world wide web	Luuk van Waes	1998		10.1145/296336.296374	think aloud protocol;navigation;hypertext;human–computer interaction;computer science;web navigation;multimedia;line search;world wide web	ECom	-35.189385596421836	-52.35749266742204	195497
08576ee3c90f41282fa4ba8c84d323a04f570ef4	scientific citations in wikipedia	empirical study	The Internet-based encyclopædia Wikipedia has grown to become one of the most visited web-sites on the Internet. However, critics have questioned the quality of entries, and an empirical study has shown Wikipedia to contain errors in a 2005 sample of science entries. Biased coverage and lack of sources are among the “Wikipedia risks”. The present work describes a simple assessment of these aspects by examining the outbound links from Wikipedia articles to articles in scientific journals with a comparison against journal statistics from Journal Citation Reports such as impact factors. The results show an increasing use of structured citation markup and good agreement with the citation pattern seen in the scientific literature though with a slight tendency to cite articles in high-impact journals such as Nature and Science. These results increase confidence in Wikipedia as an good information organizer for science in general. Wikipedia increases in popularity and will probably get further importance for organization and dissemination of scientific research. But how can the articles of this freely edited Internet-based encyclopædia be trusted? Inbound links can to some extent quantify the quality of a work, and examples include Google’s PageRank for web-pages and the impact factor of scientific journals. The algorithms behind the PageRank and Kleinberg’s HITS can be adapted to Wikipedia, but it is not clear whether high-scoring articles are also quality articles with respect to content. It has been suggested that Wikipedia content surviving over a long period and many edits may be deemed of high quality. On the other hand studies have found that highly edited articles are likely quality articles. Other proposals for quality assessment use revision history to compute a trust index for an article or an author reputation index. Another feature of an article that may correlate with article quality is the amount of outbound citation to “trusted” material, e.g., scientific articles. How prolific are these and does Wikipedia use them across scientific fields? Critics have noted that Wikipedia may be biased on the corpus level—leaned towards topics that interest the “young and Internet-savvy”—and a possible lack of sources has been noted. Authors can include scientific references in Wikipedia by different means, most simply, by listing them at the bottom of the article. A more structured approach uses the	algorithm;display resolution;hyperlink;image organizer;inbound marketing;index (publishing);internet;journal citation reports;markup language;pagerank;scientific literature;wikipedia	Finn Årup Nielsen	2007	First Monday		computer science;data science;data mining;world wide web	Web+IR	-38.56878556806335	-55.83960001747055	195661
fdb6dd87b172a9725353fc917083bcb005e21956	information search processes in complex tasks		Our understanding of search processes triggered by complex tasks are limited [1]. It is not well known how does the information search process evolve during task performance and how search behavior varies by task process. How do changes in in information needs reflect in search formulation and tactics, in selecting contributing sources and interacting with sources for task outcome? A better understanding of these issues helps in identifying success criteria for various parts of search process. The results contribute also to designing support tools for complex search tasks.   In the talk I analyze information search processes in complex tasks. By task I mean larger tasks, which lead people to engage in search tasks for finding information to advance those tasks [2]. Search process consists of activities from query formulation to working with sources selected for task outcome [3]. I approach task performance from cognitive point of view conceptualizing it as changes in cognitive structures [4,5]. These structures consist of concepts and their relations representing some phenomenon. I analyze how changes in knowledge structures are associated to query formulation and search tactics, selecting contributing sources and working with sources for creating a task outcome. As a result I suggest hypotheses concerning associations between changes in knowledge structures and search behaviors. I present also some ideas for success indicators at various stages of search process.	information needs;information retrieval;information search process;interaction	Pertti Vakkari	2018		10.1145/3176349.3176570	information search process;computer science;management science;data mining;information needs;cognition;phenomenon	AI	-40.87379716363328	-55.643930520438374	195788
19d442b8982bcf01158d449926e7c9e506eff46f	comparing cross-language query expansion techniques by degrading translation resources	translation resources;query translation;query expansion;relevance feedback;parallel corpora;cross language information retrieval	The quality of translation resources is arguably the most important factor affecting the performance of a cross-language information retrieval system. While many investigations have explored the use of query expansion techniques to combat errors induced by translation, no study has yet examined the effectiveness of these techniques across resources of varying quality. This paper presents results using parallel corpora and bilingual wordlists that have been deliberately degraded prior to query translation. Across different languages, translingual resources, and degrees of resource degradation, pre-translation query expansion is tremendously effective. In several instances, pre-translation expansion results in better performance when no translations are available, than when an uncompromised resource is used without pre-translation expansion. We also demonstrate that post-translation expansion using relevance feedback can confer modest performance gains. Measuring the efficacy of these techniques with resources of different quality suggests an explanation for the conflicting reports that have appeared in the literature.	cross-language information retrieval;elegant degradation;parallel text;query expansion;relevance feedback;text corpus	Paul McNamee;James Mayfield	2002		10.1145/564376.564406	natural language processing;query expansion;computer science;database;world wide web;information retrieval;query language	Web+IR	-34.01910168888787	-63.4618588252415	196014
62b143f68548334c2dc2ae6a35e30e9a6d94dac0	optimal search strategy for web-based 3d model retrieval	busqueda informacion;modelizacion;red www;estrategia optima;loi probabilite;ley probabilidad;information retrieval;lagrange multiplier method;reseau web;recherche aleatoire;search strategy;lagrange multiplier;modelisation;optimal strategy;3d model retrieval;recherche information;probability distribution;multiplicateur lagrange;strategie recherche;multiplicador lagrange;investigacion aleatoria;world wide web;mobile agent;modeling;random search;strategie optimale;estrategia investigacion;search theory	In this paper we propose an optimal search strategy for webbased 3D model retrieval. Special considerations are given to the determination of the target’s initial probability distributions. Two optimal search strategies are derived by using Lagrange multiplier method. Experimental results shows that this approach is more efficient than random search.	3d modeling;lagrange multiplier;linear search;mean squared error;random search;randomness;search problem;web application	Qingxin Zhu;Bo Peng	2006		10.1007/11610496_111	search theory;beam search;simulation;computer science;artificial intelligence;mathematics;lagrange multiplier	Vision	-37.12247621809858	-58.07685868986392	196643
3011bc03ec9c929fa8967d08f90b59e780bbbb7a	order effects: a study of the possible influence of presentation order on user judgments of document relevance	relevance information retrieval;information retrieval;comparative analysis;rating scales	Studies concerned with the evaluation of information systems have typically relied on judgments of relevance as the fundamental measure in determining system performance. In most cases, subjects are asked to assign a relevance score using some category rating scale (l-4, l-11, or simply relevant/non-relevant) to each document in a set retrieved in response to some information need or query. While the extensive studies of relevance conducted in the 1960s indicated that relevance judgments are influenced by a range of variables, llttle attention has been paid to the possible effects of the order In which the stimuli are presented to judges. This effect of “stimulus order” has been found to exist in measuring variables in other fields (Stevens 1975, Gescheider 1965). Questioning possible “presentable order effects” Is particularly appropriate in that systems are being developed and evaluated in information science which present documents In some systematic way (e.g., with the documents considered by the system to be most relevant presented first). This article describes an effort to study whether the order of document presentation to judges influences the relevance scores assigned to those documents. A query and set of documents with relevance judgments were available from a previous study. Subjects were randomly assigned one of two orders (one ranked high to low, the other low to high) of fifteen document descriptions. They were then asked to assign a score to each document description to match their judgment of relevance in relation to the stated informatlon query. Both a category rating (l-7) and open-ended, magnitude estimation scaling procedure were tested, and It was found that the judgments were influenced by the order of document presentation.	document;estimation theory;image scaling;information needs;information science;information system;nonlinear gameplay;randomness;rating scale;relevance	Michael Eisenberg;Carol L. Barry	1988	JASIS	10.1002/(SICI)1097-4571(198809)39:5%3C293::AID-ASI1%3E3.0.CO;2-I	qualitative comparative analysis;ranking;relevance;rating scale;computer science;data science;data mining;database;world wide web;information retrieval;statistics	Web+IR	-37.82888831951872	-61.64191389400478	196751
4fe5887dd5a62d3e699cd6b5700c0765571b6d03	bayesian web document classification through optimizing association word	bayes estimation;web documents;optimisation;fiabilidad;reliability;red www;optimizacion;test bayes;relation semantique;bayes test;reseau web;relacion semantica;semantics;intelligence artificielle;algoritmo genetico;data mining;semantica;semantique;classification;estimacion bayes;internet;fouille donnee;fiabilite;algorithme genetique;artificial intelligence;world wide web;genetic algorithm;semantic relation;refinacion;optimization;refining;inteligencia artificial;document classification;semantic relations;bayes classifier;busca dato;clasificacion;raffinage;estimation bayes	Previous Bayesian document classification has a problem because it does not reflect semantic relation accurately in expressing characteristic of document. In order to resolve this problem, this paper suggests Bayesian document classification method through mining and refining of association word. Apriori algorithm extracts characteristic of test document in form of association words that reflects semantic relation and it mines association words from learning documents. If association word from learning documents is mined only with Apriori algorithm, inappropriate association word is included within them. Accordingly it has disadvantage of lack of accuracy in document classification. In order to complement the disadvantage, we adopt method to refine association words through use of genetic algorithm. Naive Bayes classifier classifies test documents based on refined association words.	document classification;optimizing compiler	Su-Jeong Ko;Jun Hyeog Choi;Jung-Hyun Lee	2003		10.1007/3-540-45034-3_57	bayes classifier;the internet;genetic algorithm;refining;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;data mining;reliability;semantics	Web+IR	-35.56022835058387	-59.653285951796214	197057
e1f6074b5513fcc89d73e00cbd9dfe49569a38b9	spectral properties of human cognition and skill	error detection and correction;human cognition;pedestrian safety;poison control;injury prevention;real time;safety literature;traffic safety;injury control;home safety;injury research;safety abstracts;human factors;error correction;occupational safety;safety;safety research;accident prevention;spectral properties;violence prevention;bicycle safety;poisoning prevention;falls;ergonomics;suicide prevention;mental model;error correction model	 Many interactive human skills are based on real-time error detection and correction. Here we investigate the spectral properties of such skills, focusing on a synchronization task. A simple autoregressive error correction model, based on separate ‘motor’ and ‘cognitive’ sources, provides an excellent fit to experimental spectral data. The model can also apply to recurrent processes not based on error correction, allowing commentary on previous claims of 1/ f-type noise in human cognition. A comparison of expert and non-expert subjects suggests that performance skill is not only based on reduced variance and bias, but also on the construction of richer mental models of error correction.	autoregressive model;cognition;error correction model;error detection and correction;mental model;real-time locating system;sample variance;synchronization (computer science)	Jeff Pressing;Garry Jolley-Rogers	1997	Biological Cybernetics	10.1007/s004220050347	psychology;simulation;error detection and correction;computer science;engineering;human factors and ergonomics;mathematics;forensic engineering;computer security;statistics	ML	-42.65952487084615	-55.04391566371751	197955
e23c0443ed64ae185e207f744f60326c24c41b39	voting techniques for expert search	document structure;modelizacion;entreprise;sistema experto;estructura documental;information retrieval;structure document;heuristic method;weighting;empresa;interrogation base donnee;interrogacion base datos;metodo heuristico;expert finding;zona dato;document representation;data fusion;voting expert finding expertise modelling;ponderacion;modelisation;hierarchical classification;ranking;voting;fusion donnee;firm;classification hierarchique;voto;methode heuristique;ponderation;systeme expert;vote;data field;fusion datos;zone donnee;modeling;expertise modelling;clasificacion jerarquizada;database query;expert search;expert system	In an expert search task, the users’ need is to identify people who have relevant expertise to a topic of interest. An expert search system predicts and ranks the expertise of a set of candidate persons with respect to the users’ query. In this paper, we propose a novel approach for predicting and ranking candidate expertise with respect to a query, called the Voting Model for Expert Search. In the Voting Model, we see the problem of ranking experts as a voting problem. We model the voting problem using 12 various voting techniques, which are inspired from the data fusion field. We investigate the effectiveness of the Voting Model and the associated voting techniques across a range of document weighting models, in the context of the TREC 2005 and TREC 2006 Enterprise tracks. The evaluation results show that the voting paradigm is very effective, without using any query or collection-specific heuristics. Moreover, we show that improving the quality of the underlying document representation can significantly improve the retrieval performance of the voting techniques on an expert search task. In particular, we demonstrate that applying field-based weighting models improves the ranking of candidates. Finally, we demonstrate that the relative performance of the voting techniques for the proposed approach is stable on a given task regardless of the used weighting models, suggesting that some of the proposed voting techniques will always perform better than other voting techniques.	computation;concordance (publishing);document;formal language;heuristic (computer science);jsp model 2 architecture;language model;overhead (computing);programming paradigm;ranking (information retrieval);world wide web	Craig MacDonald;Iadh Ounis	2007	Knowledge and Information Systems	10.1007/s10115-007-0105-3	systems modeling;voting;ranking;computer science;data field;artificial intelligence;document structure description;machine learning;data mining;database;weighting;sensor fusion;expert system;information retrieval;electoral-vote.com	Web+IR	-36.313834419401715	-58.597457249979165	198797
e19b2d92860fe617e8bc732e4171c1f65add3b5a	ranking retrieval systems without relevance judgments	search engine;information retrieval system;sampling technique;indexing;evaluation methodology;text retrieval;world wide web;compression;experimental methodology;test collection	The most prevalent experimental methodology for comparing the effectiveness of information retrieval systems requires a test collection, composed of a set of documents, a set of query topics, and a set of relevance judgments indicating which documents are relevant to which topics. It is well known that relevance judgments are not infallible, but recent retrospective investigation into results from the Text REtrieval Conference (TREC) has shown that differences in human judgments of relevance do not affect the relative measured performance of retrieval systems. Based on this result, we propose and describe the initial results of a new evaluation methodology which replaces human relevance judgments with a randomly selected mapping of documents to topics which we refer to aspseudo-relevance judgments.Rankings of systems with our methodology correlate positively with official TREC rankings, although the performance of the top systems is not predicted well. The correlations are stable over a variety of pool depths and sampling techniques. With improvements, such a methodology could be useful in evaluating systems such as World-Wide Web search engines, where the set of documents changes too often to make traditional collection construction techniques practical.	document;information retrieval;pool (computer science);randomness;relevance;sampling (signal processing);text retrieval conference;web search engine;world wide web	Ian Soboroff;Charles K. Nicholas;Patrick Cahan	2001		10.1145/383952.383961	sampling;search engine indexing;ranking;relevance;computer science;data mining;world wide web;compression;information retrieval;search engine	Web+IR	-37.65130872016739	-61.7673409378946	198819
f933be6d4a56860c93f6a57b5c26338c0b50665f	a collaborative cbr recommender system to support patients, its relatives and caregivers in chronic and palliative care		In the last decades, many medical assistance systems have been developed, and the interest in computer-aided problem-solving in medical and healthcare is constantly growing. Most of the software systems in this domain focus on decision support and recommendation of effective medication for patients. The combination of statistical analysis and case-based reasoning can facilitate a better medical diagnosis [1] [2] [3]. Within the case comparison mechanism of CBR, feature selection, similarity measurement, and adaptation methods play an important role to retrieve and revise cases. In this research, DePicT (Detect and Predict diseases using image classification and Text Information from patient health records) uses image interpretation and word associations for feature selection and recommendation of medical solutions [4]. All gathered patient records are stored in relational databases as structured or closed-format (e.g. parameters and statistics), or unstructured or open-format e.g. texts and images. For example, images of affected areas of a melanoma skin cancer can contribute and support early stage diagnosis. Also, further information on answering questions or writing a statement about the patient's health condition is added to the knowledge base. Domain Experts can validate and verify the collected information and also update the case-base to correct the data records of patients. In the other hand, more over than assisting for detecting and predict the disease, the Vocational Educational Training (VET) and Technology Enhanced Learning (TEL) [5] is a research field which is investigated continuously. DePicT CLASS (Detect and Predict diseases using image classification and Text information in Case-based Learning Assistant System) is a CBR system by enrichment of cases with learning materials (e.g. reference images and textbook) [6]. It is utilized smart (knowledge-based) and accessible systems to provide vocational educational learning opportunities and achieving higher education. CBR is applied in various problem-solving domains, and it is appropriate in medicine to integrate the system and for explicit experience, cognitive adequateness, the duality of objective/subjective knowledge, and to extract subjective knowledge [7]. Design and development of the DePicT and DePicT CLASS are the main contributions of this investigation. It is a case-based system which uses DePicT Profile Matrix of the association strength between title phrase and identified keywords of cases. Making experiments to validate the research and this recommender system lead us to do it in the	case-based reasoning;computer vision;decision support system;experiment;feature selection;gene ontology term enrichment;knowledge base;knowledge-based systems;problem solving;recommender system;relational database;sensor;software system;the european library	Sara Nasiri	2017			recommender system;family medicine;computer science	AI	-48.19995727961147	-63.7678082718699	198823
051c656fea813a0ddb9568b0ccdca66cd1562611	online learning with social computing based interest sharing	computer science online learning with social computing based internet sharing utah state university seungjin lim muhlestein;modelizacion;association statistique;reseau social;news;confidencialidad;social computing;navegacion informacion;red www;plugicial;social learning;apprentissage social;model analysis;navigation information;pervasive computing;reseau web;information browsing;statistical association;online learning;data mining;confidentiality;informatica difusa;modelisation;vida privada;confidentialite;social network;asociacion estadistica;internet;private life;association rule;fouille donnee;informatique diffuse;autoorganizacion;noticias;world wide web;vie privee;self organization;aprendizaje social;plugiciel;plug in software;interest sharing;actualites;modeling;busca dato;autoorganisation;red social;dennis j	Communities on the Internet are highly self-organizing, dynamic, and ubiquitous. One objective of peers in such a community is sharing common interests, even when compromising privacy. This paper presents a model for peers on the Internet that allows them to discover their common interests in terms of sets of frequently visited URLs. This model assists online learning by automatically presenting users with URLs related to what they are currently browsing, thus saving users’ time searching for additional information and helping to educate them on the current topic. To implement the model and collect test data, FireShare was developed as a plugin for the popular Web browser Firefox. Data was collected and analyzed on the number of discovered frequently visited URL sets, relevancy of mined association rules, and the overhead FireShare imposes on a network. While FireShare favorably validated the proposed model, analysis of the submitted test data shows high potential for success with future versions.	apriori algorithm;association rule learning;booting;central processing unit;centralized computing;client (computing);communications protocol;data mining;firefox;firewall (computing);http persistent connection;heuristic (computer science);instant messaging;internet;lookup table;mined;network socket;online machine learning;organizing (structure);overhead (computing);peer-to-peer;plug-in (computing);privacy;relevance;self-organization;server (computing);social computing;test data;transmitter;user-generated content	Dennis Muhlestein;Seungjin Lim	2009	Knowledge and Information Systems	10.1007/s10115-009-0265-4	social learning;confidentiality;news;computer science;artificial intelligence;data science;machine learning;data mining;database;world wide web;social computing;social network	Web+IR	-37.971224120602244	-57.35519029010988	198903
82352f98be7212fbb8bf2841edd6464a5e135c63	collaborative querying for enhanced information retrieval	busqueda informacion;search engine;analyse amas;buscador;information retrieval;drntu engineering computer science and engineering information systems information storage and retrieval;query formulation;interrogation base donnee;interrogacion base datos;recommandation;formulacion pregunta;data mining;classification;formulation question;similitude;user assistance;conference paper;biblioteca electronica;fichier log;fichero actividad;cluster analysis;assistance utilisateur;thesis;fouille donnee;recherche information;decouverte connaissance;asistencia usuario;similarity;recomendacion;descubrimiento conocimiento;recommendation;analisis cluster;electronic library;similitud;moteur recherche;information seeking;busca dato;drntu library and information science libraries information retrieval and analysis;database query;clasificacion;bibliotheque electronique;log file;knowledge discovery	Communication and collaboration with other people is a major theme in the information seeking process. Collaborative querying addresses this issue by sharing other users’ search experiences to help users formulate appropriate queries to a search engine. This paper describes a collaborative querying system that helps users with query formulation by finding previously submitted similar queries through mining web logs. The system operates by clustering and recommending related queries to users using a hybrid query similarity identification approach. The system employs a graph-based approach to visualize the query recommendations.	blog;cluster analysis;complex adaptive system;computer cluster;experience;experiment;html;information retrieval;information seeking;network interface device;similarity measure;usability;web search engine	Lin Fu;Dion Hoe-Lian Goh;Schubert Foo;Yohan Supangat	2004		10.1007/978-3-540-30230-8_34	query expansion;web query classification;similarity;biological classification;computer science;similitude;data mining;database;knowledge extraction;cluster analysis;web search query;world wide web;information retrieval;query language;search engine	Web+IR	-35.49173492198051	-58.121623986570285	198958
5cdfc86c71b91415f5ce8cafcecb539ba230820c	the iit intranet mediator: an overview	digital library;data type	Introduction A mediator sits between a user and a variety of data sources. It acts on behalf of the user to provide “one stop shopping” to an organization’s data. An intranet, or digital library, is an excellent example of the need for a mediator because of the myriad of data types that exist in the digital library. A user query such as “What are the three best restaurants in Chicago?” should not search for the word three. A mediator will recognize that this query needs to access structured information and will pose the query as a conventional SQL query to a data warehouse of structured information. The mediator may submit a request to unstructured sources as well. The result will be both a suggested answer to the user’s question as well as a set of related links to other potentially useful information. We note that our key goal is simply to answer natural language questions that cross both structured and unstructured data. Presently, work was done on the TREC Question Answering track that focuses on unstructured sources [Hara00]. Our work builds on this as we wish to include structured sources as well.	digital library;integrated information theory;intranet;matchware mediator;natural language;question answering;sql;select (sql);text retrieval conference	David A. Grossman;Steven M. Beitzel;Eric C. Jensen;Ophir Frieder	2000			data type;sql;digital library;database;natural language;intranet;question answering;data warehouse;unstructured data;computer science	DB	-33.81596876218363	-57.44563783438818	198966
4c78d6b35eb7ef0754524e84dff11d1433c0e1db	the effect of aggregated search coherence on search behavior	assimilation effects;user study;aggregated search;aggregated search coherence;search behavior;evaluation	"""Aggregated search is the task of blending results from different specialized search services, or verticals, into the web search results. Aggregated search coherence refers to the degree to which results from different systems focus on similar senses of the query. While cross-component coherence has been cited as an important criterion for whole-page evaluation, its effect on search behavior has not been deeply investigated in prior research. In this work, we focus on the coherence between two aggregated search components: images and web results. In particular, we investigate whether the query-senses associated with the blended image results can influence user interaction with the web results. For example, if a user wants web results about """"jaguar"""" the animal, are they more likely to examine the web results if the image results contain pictures of the animal instead of pictures of the car? Based on two large user studies, our results show that the image results can systematically affect user interaction with the web results. If the web results are largely consistent with the search task, then the effect of the image results is small. However, if the web results are only marginally consistent with the search task, such as when they are highly diversified across query-senses, the image results have a significant effect on user interaction with the web results. Our findings have implications on current research in whole-page evaluation, aggregated search, and diversity ranking."""		Jaime Arguello;Robert G. Capra	2012		10.1145/2396761.2398432	semantic search;computer science;evaluation;multimedia;world wide web;information retrieval;search engine	Web+IR	-34.56747916329479	-52.88163922181854	199014
5829110f946aee7e55bb0e5059e0d68afab3afca	children's conceptual structures of science categories and the design of web directories	site web;categorisation;representation cartographique;cartographic representation;hierarchized structure;estudio comparativo;sciences;hombre;conception;structure hierarchisee;ciencia;information organization;enfant;etude comparative;organizacion informacion;categorizacion;nino;portal tematico;comportement utilisateur;human;comparative study;diseno;child;organisation information;subject portal;design;user behavior;sitio web;representacion cartografica;portail thematique;estructura jerarquizada;comportamiento usuario;web site;categorization;homme	gory, children constructed a pair of maps: one without links and one with links. Forty-four maps were analyzed to identify similarities and differences. The structures of the maps were compared to the structures employed by the directories. Children were able to construct hierarchical maps and articulate the relationships among the concepts. At the global level (whole map), children's maps were not alike and did not match the structures of the Web directories. At the local levels (superordinate and subordinate), however, children shared similarities in the conceptual configurations, especially for the concrete concepts. For these concepts, substantial overlap was found between the children's structures and those employed in the directories. For the abstract concepts the configurations were diverse and did not match those in the directories. The findings of this study have implications for design of systems that are more supportive of children's conceptual structures.	map;world wide web	Dania Bilal;Peiling Wang	2005	JASIST	10.1002/asi.20216	design;artificial intelligence;linguistics;world wide web;categorization	SE	-40.24605000847876	-57.70990858160873	199245
579b14cd60b222c4a120252f3a78a8b45faeef5d	effect of visualization training on uncertain spatial trajectory predictions	cognition;decision making;metacognition;transfer of training;visual displays	OBJECTIVE The goal of this study was to explore the ways in which visualizations influence the prediction of uncertain spatial trajectories (e.g., the unknown path of a downed aircraft or future path of a hurricane) and participant overconfidence in such prediction.   BACKGROUND Previous research indicated that spatial predictions of uncertain trajectories are challenging and are often associated with overconfidence. Introducing a visualization aid during training may improve the understanding of uncertainty and reduce overconfidence.   METHOD Two experiments asked participants to predict the location of various trajectories at a future time. Mean and variance estimates were compared for participants who were provided with a visualization and those who were not.   RESULTS In Experiment 1, participants exhibited less error in mean estimations when a linear visualization was present but performed worse than controls once the visualization was removed. Similar results were shown in Experiment 2, with a nonlinear visualization. However, in both experiments, participants who were provided with a visualization did not retain any advantage in their variance estimations once the visualization was removed.   CONCLUSIONS Visualizations may support spatial predictions under uncertainty, but they are associated with benefits and costs for the underlying knowledge being developed.   APPLICATION Visualizations have the potential to influence how people make spatial predictions in the presence of uncertainty. Properly designed and implemented visualizations may help mitigate the cognitive biases related to such predictions.		Ashley J. Pugh;Christopher D. Wickens;Nathan Herdener;Benjamin A. Clegg;C. A. P. Smith	2018	Human factors	10.1177/0018720818758770	simulation;visualization;machine learning;engineering;overconfidence effect;trajectory;transfer of training;metacognition;cognition;artificial intelligence	HCI	-47.62638535930294	-53.82022144252937	199304
d235032eb8b4a176549a6a0763786465f2559721	xml fuzzy ranking	busqueda informacion;document structure;estructura documental;information retrieval;structure document;xml language;logique floue;interrogation base donnee;interrogacion base datos;logica difusa;fuzzy logic;hierarchical classification;proximite;proximidad;recherche information;proximity;classification hierarchique;xml document;levenshtein distance;systeme gestion base donnee;sistema gestion base datos;database management system;clasificacion jerarquizada;database query;langage xml;lenguaje xml	This paper proposes a method of ranking XML documents with respect to an Information Retrieval query by means of fuzzy logic. The proposed method allows imprecise queries to be evaluated against an XML document collection and it provides a model of ranking XML documents. In addition the proposed method enables sophisticated ranking of documents by employing proximity measures and the concept of editing (Levenshtein) distance between terms or XML paths.	archive;fuzzy logic;information retrieval;levenshtein distance;relevance;xml retrieval	Evangelos Kotsakis	2006		10.1007/11766254_14	well-formed document;xml validation;binary xml;simple api for xml;xml;ranking;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;world wide web;xml schema editor;information retrieval;efficient xml interchange	Web+IR	-35.371542632777036	-60.34136817471705	199588
42e310043d536bddef3c73ca2cd9d0dcefb3b41a	evaluating the concept specialization distance from an end-user perspective: the case of agrovoc		PurposernrnrnrnrnThe common understanding of generalization/specialization relations assumes the relation to be equally strong between a classifier and any of its related classifiers and also at every level of the hierarchy. Assigning a grade of relative distance to represent the level of similarity between the related pairs of classifiers could correct this situation, which has been considered as an oversimplification of the psychological account of the real-world relations. The paper aims to discuss these issues.rnrnrnrnrnDesign/methodology/approachrnrnrnrnrnThe evaluation followed an end-user perspective. In order to obtain a consistent data set of specialization distances, a group of 21 persons was asked to assign values to a set of relations from a selection of terms from the AGROVOC thesaurus. Then two sets of representations of the relations between the terms were built, one according to the calculated concept of specialization weights and the other one following the original order of the thesaurus. In total, 40 persons were asked to choose between the two sets following an A/B test-like experiment. Finally, short interviews were carried out after the test to inquiry about their decisions.rnrnrnrnrnFindingsrnrnrnrnrnThe results show that the use of this information could be a valuable tool for search and information retrieval purposes and for the visual representation of knowledge organization systems (KOS). Furthermore, the methodology followed in the study turned out to be useful for detecting inconsistencies in the thesaurus and could thus be used for quality control and optimization of the hierarchical relations.rnrnrnrnrnOriginality/valuernrnrnrnrnThe use of this relative distance information, namely, “concept specialization distance,” has been proposed mainly at a theoretical level. In the current experiment, the authors evaluate the potential use of this information from an end-user perspective, not only for text-based interfaces but also its application for the visual representation of KOS. Finally, the methodology followed for the elaboration of the concept specialization distance data set showed potential for detecting possible inconsistencies in KOS.	agrovoc;partial template specialization	David Martín-Moncunill;Miguel-Ángel Sicilia;Elena García Barriocanal;Christian M. Stracke	2017	Online Information Review	10.1108/OIR-03-2016-0094	world wide web;computer science;end user;knowledge organization;hierarchy;data mining;elaboration;originality;user interface;vocabulary;information seeking	Vision	-40.34271000054581	-57.452533982844656	199668
a238aa1e06afa0427ee11e54e3458cbd9ab93857	balanced scoring method for multiple-mark questions		Advantages and disadvantages of a learning assessment based on multiple-choice questions (MCQs) are a long and widely discussed issue in the scientific community. However, in practice this type of questions is very popular due to the possibility of automatic evaluation and scoring. Consequently, an important research question is to exploiting the strengths and mitigate the weaknesses of MCQs. In this work we discuss one particularly important issue of MCQs, namely methods for scoring results in the case, when the MCQ has several correct alternatives (multiple-mark questions, MMQs). We propose a general approach and mathematical model to score MMQs, that aims at recognizing guessing while at the same time resulting in a balanced score. In our approach conventional MCQs are viewed as a particular case of multiple-mark questions, thus, the formulas can be applied to tests mixing MCQs and MMQs. The rational of our approach is that scoring should be based on the guessing level of the question. Our approach can be added as an option, or even as a replacement for manual penalization. We show that our scoring method outperforms existing methods and demonstrate that with synthetic and real experiments.	algorithm;experiment;mathematical model;penalty method;synthetic intelligence	Darya Tarasowa;Sören Auer	2013		10.5220/0004384304110416	knowledge management;computer science	AI	-39.34749717317172	-64.34543686453475	199946
