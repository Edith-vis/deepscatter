id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
eda5046843634dc3382384e00c08bcba08bd13be	a speaking atlas of the regional languages of france		The aim is to show and promote the linguistic diversity of France, through field recordings, a computer program (which allows us to visualise dialectal areas) and an orthographic transcription (which represents an object of research in itself). A website is presented (https://atlas.limsi.fr), displaying an interactive map of France from which Aesop’s fable “The North Wind and the Sun” can be listened to and read in French and in 140 varieties of regional languages. There is thus both a scientific dimension and a heritage dimension in this work, insofar as a number of regional or minority languages are in a critical situation.	computer program;map;orthographic projection;transcription (software)	Philippe Boula de Mareüil;Albert Rilliard;Frédéric Vernier	2018			speech recognition;natural language processing;artificial intelligence;computer science;atlas (anatomy)	ML	-33.80297341827819	-77.35890588099514	88293
da6c3fdf8ef9aae979a5dd156e074ba6691b2e2c	genia corpus—a semantically annotated corpus for bio-textmining	semantic annotation;information extraction;text mining;reference material;computational molecular biology;natural language processing	MOTIVATION Natural language processing (NLP) methods are regarded as being useful to raise the potential of text mining from biological literature. The lack of an extensively annotated corpus of this literature, however, causes a major bottleneck for applying NLP techniques. GENIA corpus is being developed to provide reference materials to let NLP techniques work for bio-textmining.   RESULTS GENIA corpus version 3.0 consisting of 2000 MEDLINE abstracts has been released with more than 400,000 words and almost 100,000 annotations for biological terms.	abstract summary;british informatics olympiad;medline;natural language processing;text mining;uterine corpus carcinosarcoma	Jin-Dong Kim;Tomoko Ohta;Yuka Tateisi;Jun'ichi Tsujii	2003	Bioinformatics	10.1093/bioinformatics/btg1023	natural language processing;text mining;computer science;data mining;certified reference materials;information extraction;information retrieval	NLP	-35.018283683966715	-69.78036913131572	88528
ebf4e4c0807e0a4ca11100a5ece76b4d608b2bcd	biomedical text mining using a grid computing approach	simd;computational grid;text mining;rule based system;field data;biomedical document analysis;grid computing;knowledge discovery	Extracting useful information from a very large amount of biomedical texts is an important and difficult activity in biomedicine field. Data to be examined are generally unstructured and the available computational resources do not still provide adequate mechanisms for retrieving and analyse very large amount of contents. In this paper we present a rule-based system for Text Mining process applied in biomedical textual documents. This application requires a strongly use of the computational resource to perform intensive operations. We propose a grid computing approach to improve application performance.	biomedical text mining;computation;computational resource;grid computing;rule-based system	Marcello Castellano;Giuseppe Mastronardi;Giacinto Decataldo;Luca Pisciotta;Gianfranco Tarricone;Lucia Cariello;Vitoantonio Bevilacqua	2008		10.1007/978-3-540-85984-0_129	text mining;simd;semantic grid;computer science;artificial intelligence;data science;data mining;information retrieval;grid computing	HPC	-35.59781385448261	-66.49694969681094	89173
ce52bdcce36117017f17623dfbe1370d0f753534	automatic generation of investigator bibliographies for institutional research networking systems	authorship;vocabulary controlled;social networking;medline;data mining;pattern recognition;abstracting and indexing as topic;artificial intelligence;algorithms;pattern recognition automated;bibliography as topic;automated;pubmed;natural language processing;biomedical research	OBJECTIVE Publications are a key data source for investigator profiles and research networking systems. We developed ReCiter, an algorithm that automatically extracts bibliographies from PubMed using institutional information about the target investigators.   METHODS ReCiter executes a broad query against PubMed, groups the results into clusters that appear to constitute distinct author identities and selects the cluster that best matches the target investigator. Using information about investigators from one of our institutions, we compared ReCiter results to queries based on author name and institution and to citations extracted manually from the Scopus database. Five judges created a gold standard using citations of a random sample of 200 investigators.   RESULTS About half of the 10,471 potential investigators had no matching citations in PubMed, and about 45% had fewer than 70 citations. Interrater agreement (Fleiss' kappa) for the gold standard was 0.81. Scopus achieved the best recall (sensitivity) of 0.81, while name-based queries had 0.78 and ReCiter had 0.69. ReCiter attained the best precision (positive predictive value) of 0.93 while Scopus had 0.85 and name-based queries had 0.31.   DISCUSSION ReCiter accesses the most current citation data, uses limited computational resources and minimizes manual entry by investigators. Generation of bibliographies using named-based queries will not yield high accuracy. Proprietary databases can perform well but requite manual effort. Automated generation with higher recall is possible but requires additional knowledge about investigators.		Stephen B. Johnson;Michael E. Bales;Daniel Dine;Suzanne Bakken;Paul J. Albert;Chunhua Weng	2014	Journal of biomedical informatics	10.1016/j.jbi.2014.03.013	computer science;artificial intelligence;data science;automation;data mining;database;information retrieval;social network	Web+IR	-47.16261583119915	-69.82876554652611	89596
21db6f578eb3e2665da29a8b7bac1ac84e7ead48	enhancing russian wordnets using the force of the crowd		The YARN (Yet Another RussNet) project aims at creating a large open machine-readable thesaurus for Russian using crowdsourcing. This paper describes the project itself along with its objectives and results of the pilot user study conducted at the end of 2013.		Dmitry Ustalov	2014		10.1007/978-3-319-12580-0_27	simulation;multimedia;advertising	Robotics	-33.96468678736332	-75.97427676574279	91411
a4ffca7760507690e9f05758899aebfd7e96075a	linguistic linked open data (llod). introduction and overview		The explosion of information technology has led to a substantial growth in quantity, diversity and complexity of linguistic data accessible over the internet. The lack of interoperability between linguistic and language resources represents a major challenge that needs to be addressed, in particular, if information from different sources is to be combined, like, say, machine-readable lexicons, corpus data and terminology repositories. For these types of resources, domainspecific standards have been proposed, yet, issues of interoperability between different types of resources persist, commonly accepted strategies to distribute, access and integrate their information have yet to be established, and technologies and infrastructures to address both aspects are still under development. The goal of the 2nd Workshop on Linked Data in Linguistics (LDL-2013) has been to bring together researchers from various fields of linguistics, natural language processing, and information technology to present and discuss principles, case studies, and best practices for representing, publishing and linking linguistic data collections, including corpora, dictionaries, lexical networks, translation memories, thesauri, etc., infrastructures developed on that basis, their use of existing standards, and the publication and distribution policies that were adopted. Background: Integrating Information from Different Sources In recent years, the limited interoperability between linguistic resources has been recognized as a major obstacle for data use and re-use within and across discipline boundaries. After half a century of computational linguistics [8], quantitative typology [12], empirical, corpus-based study of language [10], and computational lexicography [16], researchers in computational linguistics, natural language processing (NLP) or information technology, as well as in Digital Humanities, are confronted with an immense wealth of linguistic resources, that are not only growing in number, but also in their heterogeneity. Interoperability involves two aspects [14]: Structural (‘syntactic’) interoperability: Resources use comparable formalisms to represent and to access data (formats, protocols, query languages, etc.),	best practice;biological anthropology;computation;computational linguistics;dictionary;digital humanities;human-readable medium;interoperability;lexicography;lexicon;linguistic linked open data;linked data;natural language processing;query language;text corpus;thesaurus;translation memory	Christian Chiarcos;Philipp Cimiano;Thierry Declerck;John P. McCrae	2013			linked data;linguistics;computer science	NLP	-34.390045744017726	-71.63878264977956	92005
2f3a28603992dbcff2874136f0b19fa42cbd3f10	extending ontologies by finding siblings using set expansion techniques	data mining;medical subject headings;terminology as topic;internet;algorithms	MOTIVATION Ontologies are an everyday tool in biomedicine to capture and represent knowledge. However, many ontologies lack a high degree of coverage in their domain and need to improve their overall quality and maturity. Automatically extending sets of existing terms will enable ontology engineers to systematically improve text-based ontologies level by level.   RESULTS We developed an approach to extend ontologies by discovering new terms which are in a sibling relationship to existing terms of an ontology. For this purpose, we combined two approaches which retrieve new terms from the web. The first approach extracts siblings by exploiting the structure of HTML documents, whereas the second approach uses text mining techniques to extract siblings from unstructured text. Our evaluation against MeSH (Medical Subject Headings) shows that our method for sibling discovery is able to suggest first-class ontology terms and can be used as an initial step towards assessing the completeness of ontologies. The evaluation yields a recall of 80% at a precision of 61% where the two independent approaches are complementing each other. For MeSH in particular, we show that it can be considered complete in its medical focus area. We integrated the work into DOG4DAG, an ontology generation plugin for the editors OBO-Edit and Protégé, making it the first plugin that supports sibling discovery on-the-fly.   AVAILABILITY Sibling discovery for ontology is available as part of DOG4DAG (www.biotec.tu-dresden.de/research/schroeder/dog4dag) for both Protégé 4.1 and OBO-Edit 2.1.	bioinformatics;biomedicine;capability maturity model;conferences;contain (action);eighty;engineering;html;matching;medical subject headings;nomenclature;one thousand;ontology (information science);ontology learning;open biomedical ontologies;plug-in (computing);pontine structure;postediting;precision and recall;protégé;pubmed central;randomness;review aggregator;semiconductor industry;text retrieval conference;text mining;text-based (computing);format;trec	Götz Fabian;Thomas Wächter;Michael Schroeder	2012		10.1093/bioinformatics/bts215	the internet;ontology components;computer science;bioinformatics;data mining;world wide web;information retrieval;process ontology	Web+IR	-35.01499836604009	-67.74658359246612	92026
611c34f0ee78acca0ed9cb6201cb0b2b1b82347e	the unification of institutional addresses applying parametrized finite-state graphs (p-fsg)	bibliometrie;tratamiento automatico;web of science;evaluation performance;homogeneisation;homogenization;relation equivalence;performance evaluation;analyse linguistique;evaluacion prestacion;bibliometria;afiliacion;graphe fini;pairing;finite graph;matrice binaire;linguistic analysis;grafo finito;equivalence relation;automatic processing;ll automated language processing;affiliation;analisis linguistico;organisme recherche;unit of analysis;organismo investigacion;bibliometrics;research institution;transductor;homogeneizacion;emparejamiento;traitement automatique;transducer;appariement;relacion equivalencia;transducteur	We propose a semi-automatic method based on finite-state techniques for the unification of corporate source data, with potential applications for bibliometric purposes. Bibliographic and citation databases have a well-known problem of inconsistency in the data at micro-level and meso-level, affecting the quality of bibliometric searches and the evaluation of research performance. The unification method applies parametrized finite-state graphs (P-FSG) and involves three stages: (1) breaking of corporate source data in independent units of analysis; (2) creation of binary matrices; and (3) drawing finite-state graphs. This procedure was tested on university departmental addresses, downloaded from the ISI Web of Science. Evaluation was in terms of an adaptation of the measures of precision and recall. The results demonstrate the usefulness of this approach, though it requires some human processing.	bibliometrics;cluster analysis;database;experiment;hoc (programming language);information sciences institute;mesoscopic physics;online and offline;precision and recall;preprocessor;scientific literature;scientometrics;semiconductor industry;source data;the matrix;unification (computer science);web of science;word-sense disambiguation	Carmen Galvez;Félix de Moya Anegón	2006	Scientometrics	10.1007/s11192-006-0156-3	homogenization;transducer;bibliometrics;computer science;artificial intelligence;pairing;mathematics;equivalence relation;transductor;algorithm;unit of analysis	NLP	-34.12140126356697	-73.98252109684508	92066
0eca8d01f1360e4dde17deef037ce4ac270784ff	text-image linking of japanese historical documents: sharing and exchanging data by using text-embedded image file		This paper will demonstrate the effectiveness of the linkage between textual data and image data in the field of medieval Japanese history where the research primarily uses text-based documents instead of photocopies and digital images. The current state of the field of Japanese history requires that researchers (1) view more historical documents than in the past, (2) fully utilize the various types of information contained in these documents such as the form of a character, handwriting, type of paper etc., and (3) collaborate with others on inter-disciplinary projects instead of pursuing isolated projects in a single field. It is not possible to realize these goals through the individual efforts of researchers. A well-established digital environment becomes necessary. The most important and fundamental research tasks for a historian include: (1) looking up certain texts: what documents include these and where in the documents they can be found, and (2) examining the target documents. Given the nature of such research work, the utilization of computers is not as advanced as it should be. Without a fully computerized environment to enable easy access, the groundwork of identifying and reviewing documents would take considerable time and effort, and thus impede the advancement of research. If images and texts of documents are digitally connected, by searching keywords or sentences, the system will display the target texts directly as a part of the image, and users would also be able call up lists of entire images by inputting single characters 2. Semantic information and visual information of Historical Documents In the field of history, most commonly circulated materials are published in traditional printed media such as reprints of texts. Historical documents that have not been reprinted are used only when they are significant for specific research projects. For historical studies, much of the emphasis is on the interpretation and analysis of primary texts, and reprints of primary material, in plain text form, are often used for such purposes. Inevitably, certain information such as the original handwriting will be lost in the process of transforming the original into plain text. In addition, handwritten material published during the pre-modern period used numerous variants of Chinese characters (異体字) different from modern publishing standards. Many of the variations of old characters are not represented by computerized fonts and thus are often lost in the conversion into text files. Kunten (訓 点), the punctuation marks to read Chinese text (Chinese classics, …	accessibility;computer;digital environment;digital image;embedded system;handwriting recognition;historical document;image file formats;interpretation (logic);linkage (software);photocopier;printing;text corpus;text-based (computing)	Takaaki Okamoto	2010			world wide web;image file formats;computer science	Web+IR	-39.41156486820069	-68.42379586162485	93395
7968ffccadf370b3d29189c061a70b5320004af5	overview of the arpa human language technology workshop	darpa program;natural language workshops;darpa speech;written language understanding;defense advanced projects agency;technical progress;natural language;recent research;arpa human language technology;human language technology	The I-ILT workshop provides a forum where researchers can exchange information about very recent technical progress in an informal, highly interactive setting. The scope includes not just speech recognition, speech understanding, text understanding, and machine translation, but also all spoken and written language work (broadly interpreted to include ARPA's TIPSTER, MT, MUC, and TREC programs) with an emphasis on topics of mutual interest, such as statistical language modeling. This workshop no longer focuses on spoken language systems evaluation, as another workshop fills that need.	language model;language technology;machine translation;message understanding conference;speech recognition;text retrieval conference	Madeleine Bates	1993			natural language processing;speech recognition;computer science;language industry;linguistics;natural language;language technology	NLP	-34.88824914299196	-77.47185587057483	94094
79dbd39573042f4f576b832cb13e9d994442231f	improving the representation and conversion of mathematical formulae by considering their textual context		Mathematical formulae represent complex semantic information in a concise form. Especially in Science, Technology, Engineering, and Mathematics, mathematical formulae are crucial to communicate information, e.g., in scientific papers, and to perform computations using computer algebra systems. Enabling computers to access the information encoded in mathematical formulae requires machine-readable formats that can represent both the presentation and content, i.e., the semantics, of formulae. Exchanging such information between systems additionally requires conversion methods for mathematical representation formats. We analyze how the semantic enrichment of formulae improves the format conversion process and show that considering the textual context of formulae reduces the error rate of such conversions. Our main contributions are: (1) providing an openly available benchmark dataset for the mathematical format conversion task consisting of a newly created test collection, an extensive, manually curated gold standard and task-specific evaluation metrics; (2) performing a quantitative evaluation of state-of-the-art tools for mathematical format conversions; (3) presenting a new approach that considers the textual context of formulae to reduce the error rate for mathematical format conversions. Our benchmark dataset facilitates future research on mathematical format conversions as well as research on many problems in mathematical information retrieval. Because we annotated and linked all components of formulae, e.g., identifiers, operators and other entities, to Wikidata entries, the gold standard can, for instance, be used to train methods for formula concept discovery and recognition. Such methods can then be applied to improve mathematical information retrieval systems, e.g., for semantic formula search, recommendation of mathematical content, or detection of mathematical plagiarism.	benchmark (computing);bit error rate;complex systems;computation;computer algebra system;content format;entity;gene ontology term enrichment;human-readable medium;identifier;information retrieval;latex;logic programming;long tail;sanity check;scalability;scientific literature;selection rule;wikidata	Moritz Schubotz;André Greiner-Petter;Philipp Scharpf;Norman Meuschke;Howard S. Cohl;Bela Gipp	2018		10.1145/3197026.3197058	information retrieval;operator (computer programming);mathml;semantics;computation;word error rate;identifier;symbolic computation;computer science;representation (mathematics)	Web+IR	-34.556930035481315	-67.87115686459887	95048
0a283fb395343cd26984425306ca24c85b09ccdb	automatic indexing based on bayesian inference networks	bayesian inference;indexing terms;indexation;network model;automatic indexing	In this paper, a Bayesian inference network model for automatic indexing with index terms (descriptors) from a prescribed vocabulary is presented. It requires an indexing dictionary with rules mapping terms of the respective subject field onto descriptors and inverted lists for terms occuring in a set of documents of the subject field and descriptors manually assigned to these documents. The indexing dictionary can be derived automatically from a set of manually indexed documents. An application of the network model is described, followed by an indexing example and some experimental results about the indexing performance of the network model.	bayesian network;dspace;data dictionary;network model;time complexity;vocabulary	Kostas Tzeras;Stephan Hartmann	1993		10.1145/160688.160691	index term;computer science;network model;pattern recognition;data mining;bayesian inference;information retrieval;statistics	Web+IR	-36.61178543949129	-66.22841500366486	95605
96314e9183945dbda42636b2d879f4e1a565550b	supporting the cybercrime investigation process: effective discrimination of source code authors based on byte-level information	computer program;programming language;natural language;source code;similarity measure	Source code authorship analysis is the particular field that attempts to identify the author of a computer program by treating each program as a linguistically analyzable entity. This is usually based on other undisputed program samples from the same author. There are several cases where the application of such a method could be of a major benefit, such as tracing the source of code left in the system after a cyber attack, authorship disputes, proof of authorship in court, etc. In this paper, we present our approach which is based on byte-level n-gram profiles and is an extension of a method that has been successfully applied to natural language text authorship attribution. We propose a simplified profile and a new similarity measure which is less complicated than the algorithm followed in text authorship attribution and it seems more suitable for source code identification since is better able to deal with very small training sets. Experiments were performed on two different data sets, one with programs written in C++ and the second with programs written in Java. Unlike the traditional language-dependent metrics used by previous studies, our approach can be applied to any programming language with no additional cost. The presented accuracy rates are much better than the best reported results for the same data sets.	algorithm;byte;c++;comment (computer programming);computer program;cybercrime;experiment;java;n-gram;natural language;programming language;similarity measure;stylometry;test set;tracing (software)	Georgia Frantzeskou;Efstathios Stamatatos;Stefanos Gritzalis	2005		10.1007/978-3-540-75993-5_14	computer science;theoretical computer science;data mining;natural language;world wide web;computer security;source code	SE	-38.78835977561331	-73.77404186230177	97603
0d0d7c789949fd6b2cd29f8dc258615e36540c5a	ontology mapping: a way out of the medical tower of babel?	ontology mapping;information extraction;semantic interoperability	Integration of different information sources has been a problem that has been challenging (or perhaps better: plaguing) Computer Science throughout the decades. As soon as we had two computers, we wanted to exchange information between them, and as soon as we had two databases, we wanted to link them together. Fortunately, Computer Science has made much progress on different levels: Physical interoperability between systems has been all but solved: with the advent of hardware standards such as Ethernet, and with protocols such as TCP/IP and HTTP, we can nowadays walk into somebody house or office, and successfully plug our computer into the network, giving instant world-wide physical connectivity. Physical connectivity is not sufficient. We must also agree on the syntactic form of the messages we will exchange. Again, much progress has been made in recent years, with open standards such HTML and XML. Of course, even syntactic interoperability is not enough. We need not only agree on the form of the messages we exchange, but also no the meaning of these messages. This problem of semantic interoperability is still wide open, despite its importance in many application areas, and despite decades of work by different disciplines within Computer Science. It is clear that the problem of semantic interoperability is also plaguing Medical Informatics. Terminological confusion is plaguing the interoperability of data sources, and is hindering automatic support for document searches. [10] provides a study of synonymy and homonymy problems on gene-names in Medline. They established that genes have on the average 2-3 different names; cross-thesaurus homonymy is often up to 30%; and almost half of the of acronyms used to denote human genes also have another meaning in Medline entirely unrelated to human genes. The conclusion of a report [8] by the same research group states: “Information extraction and literature mining efforts will be strongly affected by this ambiguity, and solving this problem is essential for these research fields.”	computer science;database;html;hypertext transfer protocol;informatics;information extraction;internet protocol suite;medline;semantic integration;semantic interoperability;thesaurus;tower of babel;xml	Frank van Harmelen	2005		10.1007/11527770_1	semantic interoperability;semantic integration;computer science;artificial intelligence;machine learning;data mining;database;world wide web;information extraction	AI	-35.12238857916851	-70.65279875874617	97722
82916c19e3390e89e086e56fcda238179ede16ea	syntactic pattern recognition of the ecg	attribute grammars;semantic information attribute grammars ecg syntactic method electrocardiogram pattern recognition parameter measurement linguistic representation;linguistic representation;ecg;computerised pattern recognition;syntactic method;electrocardiographie;parameter measurement;attribute grammar;pattern recognition electrocardiography data mining power system modeling libraries;medical diagnostic computing computational linguistics computerised pattern recognition electrocardiography grammars;grammars;semantic information;electrocardiography;electrocardiografia;lenguaje descripcion;gramatica por atributo;syntactic pattern recognition;grammaire par attribut;pattern recognition;computational linguistics;reconnaissance forme;reconocimiento patron;medical diagnostic computing;langage description;electrocardiogram;description language	An application of the syntactic method to recognition of electrocardiogram (ECG) and to the measurement of ECG parameters is presented. Solutions to the subproblems of primitive pattern selection, primitive pattern extraction, linguistic representation, and pattern grammar formulation are given. Attribute grammars are used as the model for the pattern grammar because of their descriptive power, which is due to their ability to handle syntactic as well as semantic information. This approach has been implemented and the performance of the resultant system has been evaluated using an annotated standard ECG library. ; I T U segment r +--	attribute grammar;pattern grammar;resultant;syntactic pattern recognition	Panagiotis Trahanias;Emmanuel Skordalakis	1990	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.56207	natural language processing;speech recognition;feature;computer science;syntactic predicate;computational linguistics;pattern recognition;attribute grammar	Vision	-35.13966300942897	-79.03582266546728	97773
bfe3683953214c231f18fd3cd0ea89d20a6bcd88	tagging of name records for genealogical data browsing	history;hidden markov model;canonical form;information retrieval;visualization genealogy information retrieval tagging;new record;information retrieval hidden markov models history;visualization;hidden markov models;tagging information retrieval standardization data visualization cleaning couplings humans hidden markov models history documentation;genealogy;family history name record tagging genealogical data browsing tool historical archives record linkage corpora hidden markov model;record linkage;tagging	In this paper we present a method of parsing unstructured textual records briefly describing a person and their direct relatives, which we use in the construction of a browsing tool for genealogical data. The records have been created by researchers who are currently digitising a collection of historical archives stored at the Abbaye de Saint-Maurice, Switzerland. The string 'Beatrix, daughter of Johannes Trona, of Saillon' is a typical example of a record. We wish to annotate every term (word and symbol) in our records with a label which describes whether the term is a name (e.g. 'Beatrix'), a place (e.g. 'Saillon'), or a relationship (e.g. 'daughter'). Using this information, we are able to derive both a canonical form for each name (e.g. 'Beatrix Trona'), and the relationships between people. We build upon work developed for the cleaning and standardization of names for record linkage corpora, adding several enhancements to deal with our more difficult data, which contains common name structures of French, Italian and Latin, over hundreds of years. We present an approach to this problem that works interactively with a user to annotate the data set accurately, greatly reducing the human effort required. We do this by learning a Hidden Markov Model representing a record structure, and finding structural patterns in new records. Finally, we present a brief overview of a tool we are developing to help genealogical researchers browse and search the data.	ampersand;archive;browsing;hidden markov model;interactivity;linkage (software);markov chain;parsing;plasma cleaning;structural pattern;switzerland;tag (metadata);text corpus	Mike Perrow;David Barber	2006	Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '06)	10.1145/1141753.1141827	canonical form;record linkage;visualization;computer science;data mining;world wide web;hidden markov model	ML	-35.95260059116224	-71.9153210912893	97918
eb81e63510780060abebd8a299c6fbf6eeea3967	wordfinding problems and how to overcome them ultimately with the help of a computer		Our ultimate goal is to help authors to find an elusive word. Whenever we need a word, we look it up in the place where it is stored, the dictionary or the mental lexicon. The question is how do we manage to find the word, and how do we succeed to do this so quickly? While these are difficult questions, I believe to have some practical answers for them. Since it is unreasonable to perform search in the entire lexicon, I suggest to start by reducing this space (step-1) and to present then the remaining candidates in a clustered and labeled form, i.e. categorial tree (step-2). The goal of this second step is to support navigation. Search space is determined by considering words directly related to the input, i.e. direct neighbors (associations/co-occurrences). To this end many resources could be used. For example, one may consider an associative network like the Edinburgh Association Thesaurus (E.A.T.). As this will still yield too many hits, I suggest to cluster and label the outputs. This labeling is crucial for navigation, as we want users to find the target quickly, rather than drown them under a huge, unstructured list of words. Note, that in order to determine properly the initial search space (step-1), we must have already well understood the input [mouse1 / mouse2 (rodent/device)], as otherwise our list will contain a lot of noise, presenting ’cat, cheese’ together with ’computer, mouse pad’, which is not quite what we want, since some of these candidates are irrelevant, i.e. beyond the scope of the user’s goal.	dictionary;karp's 21 np-complete problems;lexicon;relevance;thesaurus	Michael Zock	2014		10.3115/v1/W14-4726	computer science;artificial intelligence;data mining;communication	HCI	-36.40710139857709	-73.27194861084205	97954
858c9ac960449ee1ec00b4b7126df83a111de9a8	1 million dutch newspaper images available for researchers: the kbk-1m dataset				Martijn Kleppe;Desmond Elliott	2016				ML	-39.34399134737532	-70.85659503783972	98365
47ca79b8a9930e6a3069af597731da4adf6dbb6f	concept based information retrieval for clinical case summaries		Objective: Query representation is a classic information retrieval (IR) problem. Forming appropriate query representations from clinical free-text adds additional complexity. We examined if external search engine mediated conceptualization based on expert knowledge, concept representation of the abstract, and application of machine learning improve the process of clinical information retrieval. Methods: Diagnosis concepts were derived through either using a Google Custom Search over a specific set of health-related websites or through manual, expert clinical diagnosis. We represented concepts both as text and UMLS concepts identified with MedTagger. Our approaches leverage Lucene indexing/searching of article full text, abstracts, titles and semantic representations. Additionally, we experimented with automatically generated diagnosis using Web search engines and the case summaries. Further, we utilized a variety of filters such as PubMed’s Clinical Query filters, which retrieve articles with high scientific quality, and UMLS semantic type filters for search terms. In our final submission for the TREC 2015 CDS challenge, we focused on three approaches: 1. DFML/DFMLB: Combined ranking scores by data fusion and relevance probabilities derived by a machine learning method to offset ranking and classification errors. 2. HAKT/HMKTB: Used an iterative hierarchical search approach that progressively relaxed filters until we reached 1000 retrieved documents. 3. MDRUN/MDRUB: Manually added a diagnosis to each case and matched UMLS concepts by manual annotations with UMLS concepts in the case summaries. Results: The concepts extracted from search results are similar to the diagnosis concepts extracted from manual annotation by clinicians, and similar to the extracted concepts from the given diagnosis in task B. Two out of our three approaches performed above the median performance by all participants for both Task A and B. Overall, the run by manual diagnosis worked the best. The similarity between manual annotation by clinicians and given diagnosis in task B partially explains the performance of our algorithms. There was statistically significant difference in performance among our runs with two measures (R-prec and Prec@10) for Task A, but we could not find difference with other two measures (infNDCG and infAP) for Task A and all measures for Task B. Discussion: Our concept based approach avoids the need to remove stop words or stemming and reduces the need to look for synonyms. Conclusions: Overall, our major innovations are query transformation using diagnosis information inferred from Google searching of health resources, concept based query and document representation, and pruning of concepts based on semantic types and groups.	algorithm;conceptualization (information science);information retrieval;information system;iterative method;live cd;machine learning;pubmed;query expansion;relevance;stemming;web search engine	Jakob Stöber;Bret S. E. Heale;Kelley Fulghum;Guilherme Del Fiol;Heejun Kim;Kalpana Raja;Siddhartha Jonnalagadda	2015			human–computer information retrieval;data mining;information retrieval;natural language processing;computer science;search engine indexing;search engine;stop words;unified medical language system;artificial intelligence;sensor fusion;annotation;ranking	Web+IR	-47.51897541003519	-68.20841025209755	99463
37b3fee9bb00261d42eb1d80a0c0952a4cefd2e4	csnet: constructing symptom network based on disease-symptom relationships		A symptom is the physical indication of an unstable state or the beginning of diseases. Symptom analysis is an essential factor in the medical area, where it is used for disease diagnosis, drug prescription, and the development of new pharmaceuticals. Commensurate with its importance, symptom analysis has been the subject of various studies in recent years. However, prior literature on this topic has been largely limited to studying symptoms for a specific disease. Our paper attempts to expand and build on previous studies by introducing a network-based symptom analysis. Symptom analysis that can provide a basis for analyzing symptoms related to various diseases. For a universal symptom analysis system, we proposed a network-based symptom analysis. In order to construct a symptom network, we utilized Medical Subject Heading (MeSH) terms and the PubMed search engine which are maintained and developed by the National Center for Biotechnology Information (NCBI) at the National Library of Medicine (NLM). We identified symptom-disease relationships with two measurements, the term frequency-inverse document frequency (TF-IDF) and frequent occurrence of two terms (co-occurrence) from PubMed articles. Symptom-symptom pairs, which is the outline for symptom network, were built up based on symptom-disease relationships. As a result, we constructed a symptom network with 223 nodes and 5313 edges. Evaluations were performed in two ways, compared with two symptom clusters and demonstrated with previous researches. Additionally, proposed method has shown possibility for a guideline of clinical demonstration and a discovery of potential symptoms pair.	basis (linear algebra);biomedical text mining;computation;control theory;netware loadable module;pubmed;tf–idf;web search engine	Sohee Hwang;Jungrim Kim;Jeongwoo Kim;Sanghyun Park	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122734	machine learning;search engine;computer science;artificial intelligence;data mining;guideline;disease;medical prescription	ML	-47.15488399380401	-68.00168988367595	99592
6450fadae50d3cb2efaa3ab67f3268f49ae55c18	guideline: multiple hierarchies	004;xml overlapping markup	As the title of the Dagstuhl Seminar Digital Historical Corpora Architecture, Annotation, and Retrieval already suggests, corpus architecture and corpus annotation is an important topic for representing (historical) texts. Especially the limitation of SGML-based markup languages to tree structured annotations raises a special problems when dealing with manuscripts: How is it possible to represent overlap. This problem was addressed by the Text Encoding Initiative (TEI) and by several scholars. This text gives an overview of several techniques for handling the overlap problem. This problem occurs especially when one wishes to digitize and to annotate richly structured historical texts, e.g. manuscripts. The overlap problem tends to arise in a variety of contexts such as:	standard generalized markup language;text encoding initiative;text corpus	Andreas Witt	2006			computer science;data mining;database;information retrieval	NLP	-34.906436844895	-71.4134429155157	99749
c7fc652a6636bfa2cb84e7703b71461b3577743b	evaluation of an alert system for selective dissemination of broadcast news	broadcast news;user needs;user interface;field trial;type of service;operating system;point of view	This paper describes the evaluation of the system for selective dissemination of Broadcast News that we developed in the context of the European project ALERT. Each component of the main processing block of our system was evaluated separately, using the ALERT corpus. Likewise, the user interface was also evaluated separately. Besides this modular evaluation which will be briefly mentioned here, as a reference, the system can also be evaluated as a whole, in a field trial from the point of view of a potential user. This is the main topic of this paper. The analysis of the main sources of problems hinted at a large number of issues that must be dealt with in order to improve the performance. In spite of these pending problems, we believe that having a fully operational system is a must for being able to address user needs in the future in this type of service.	operational system;type of service;user interface	Isabel Trancoso;João Paulo da Silva Neto;Hugo Meinedo;Rui Amaral	2003			computer science;type of service;multimedia;internet privacy;user interface;world wide web	Web+IR	-42.70451997293897	-78.03955428340628	99763
961b0902a085edb57f5e2f0c857b354220341fae	forensic analysis of print using digital image analysis	databases;developpement logiciel;analisis imagen;software;image numerique;text;digital image analysis;methodology and techniques;printmaking;character;texte;analyse medicolegal;reconnaissance caractere;desarrollo logicial;software development;imagen numerica;impression numerique;caractere;image analysis;caracter;digital image;texto;database search;analyse image;character recognition;reconocimiento caracter	The objective of this investigation is to establish whether it is possible to produce a practical forensic tool that can identify the production sources of printed text from different digital print engines. The identification of print using an automated machine system is important because although expert observers can be employed for this task, there are cases when they make mistakes or do not possess the required knowledge. Therefore the development of an automated print identification system is under consideration. It is envisaged that the system will be useful in solving criminal cases involving the analysis of fraudulent replication of official documents, threatening letters and the counterfeiting of consumer products. The methodology used in this investigation employed a digital image analysis system and specially developed software to measure the shape characteristics of text characters. The information about the shapes of the text characters can be stored in a database along with the corresponding data about the print engines that produced them. A database search engine can then be used to classify text characters of unknown origin. The paper will report on the methodology and techniques used in the investigation and the latest experimental results for the project.	digital image;image analysis;printing;web search engine	Jack Tchan	2003		10.1117/12.477372	computer science;multimedia;cartography;computer graphics (images)	SE	-39.206638675901445	-66.55518276335607	100723
59c9da928fbf10da0c46eef255f14aba2a8ed9a9	evaluation of machine-learning protocols for technology-assisted review in electronic discovery	electronic discovery;e discovery;technology assisted review;predictive coding	"""Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P<0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P<0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of """"stabilization"""" -- determining when training is adequate, and therefore may stop."""	active learning (machine learning);electronic discovery;machine learning;monte carlo method;randomness;relevance feedback;sampling (signal processing);search algorithm;software deployment	Gordon V. Cormack;Maura R. Grossman	2014		10.1145/2600428.2609601	computer science;data science;machine learning;data mining;world wide web;information retrieval	Web+IR	-44.90697840351023	-69.8941574845859	102073
5841c752157f81ae21e8106fa15b2243d692ed3c	personalized reading support for second-language web documents	reading support;web pages;logistic regression;glossing systems;item response theory	A novel intelligent interface eases the browsing of Web documents written in the second languages of users. It automatically predicts words unfamiliar to the user by a collective intelligence method and glosses them with their meaning in advance. If the prediction succeeds, the user does not need to consult a dictionary; even if it fails, the user can correct the prediction. The correction data are collected and used to improve the accuracy of further predictions. The prediction is personalized in that every user's language ability is estimated by a state-of-the-art language testing model, which is trained in a practical response time with only a small sacrifice of prediction accuracy. The system was evaluated in terms of prediction accuracy and reading simulation. The reading simulation results show that this system can reduce the number of clicks for most readers with insufficient vocabulary to read documents and can significantly reduce the remaining number of unfamiliar words after the prediction and glossing for all users.	collective intelligence;dictionary;gloss (annotation);lexical definition;personalization;response time (technology);simulation;vocabulary;web page	Yo Ehara;Nobuyuki Shimizu;Takashi Ninomiya;Hiroshi Nakagawa	2013	ACM TIST	10.1145/2438653.2438666	item response theory;speech recognition;computer science;artificial intelligence;machine learning;web page;data mining;logistic regression;world wide web;statistics	Web+IR	-39.48244762078574	-72.83141615421445	102215
7c3bb3dece9ea21345e6810a753e4b85ed2c84ba	manipulation of trees in information retrieval	information retrieval	"""/.} ,I n t r o d u c t i o n All inf~n'm:t{i~)l,., ret, ri( 'val system is (tesigned t() provide """"tli:-tvo!'s It) l'('(llIOSld, I'()F i i l f ( ) l ' l l la i io t l . I l l s ( ) n l e S VS{OInS~ l'('ful'('!i('c:< i l l thc f4)l'll l O[ d<)cimlel i t or i l l )ok t i t les al'o ( i l ) /a [ ] i ( : r i l 'al}i('l"""" l ha l i sf)e('ific i lenis ()f infor i l iat io l l ; tiles() io['e!'cile(,s. {l/ iurn, l(,~.t t,> (he act !lal r('(lttired infori l tat ioi i . If a-'.ttolli~tlic nu'lho([> ave i() }~e used for tit(' per[ornlance of iP.f()l ' l t latj() l i l ( ' t r i ( ' va l o[)(,l't/iiOliS, pl 'ot 'ed/l l 'es I l i t tst be !>r<)vide<l for fi)nr !)r lucipat '<ypes of lasks: the analysis of iilf(Jrln~itioii i'c.qiicsls ali(t of stored <l-ira; the gelieration of inf( i in ial i!)ii i(14'ili itlca~ i .ns which display Will(ted reiu)ions and associali<)ns i)(q:weeli ilenls of in format ion; ihe sinip}if icatiou, c(m(lensation, or tralisforritlation (if illf<)l'Hla!i()ll ideulifi<'uii(ms to (;onfol'ltl with sonic given ciassificalioii sysletn: aiid the ntatching of the modif ied request idei i l i f icai ions w i i h ihe data identif ications. .~[tlCtJ of the worl,, ill a l l tontal ic ii ifol'nlai;ion retr ieval i.-eoi l( 'entratcd iu i tw lasl area, and cons(sis, in parl imllai ', of pioced'u'i ' . < I()1' tim arraiigenlenl: of (reins of in let 'mat(on ill StOl'i/~o, ((lid for the exlra( ' l ion of tilose items which ('()l'respoll(l exactly to some givell input, ('riiori()lt. ~li('h pro~ralris (it) li(il cov('r lnaliV of the req~liven-l(,iiis for a tvon la l ic in format ion retr ieval, firsl; })(,catlse al l CXi.(('I l l l t i i ('h })e{w('( ' l l i n f ( i r n l a l i on re(lttests and ~lored data wi l l freq(lo|~tly i}()t reoov(,r t | ic desired it.eros, and second be<.ause ih(, in i t )or ial i t problenis COllliOcted wi th the autonlat ic almlvsis ()f infor lnat iou are ig, iored, fit fact, most ()f ~tle investigat()rs Cil'elinlvelit the <litl icultics inhereig iti autonmt ie aiialysis by assuming so(lie sort of i l lal i l ia! ly prl 'pm'('d (lip(it, or (?is("""" use relat ively rlldinlentary st(iris(toni te<'hniques, such as word fre(luency cotlnts..As tt res|ilt the practical results aellieved in this area are fully applicable only to simplified or artificial, semiformal languages, or are restricted to snmll, specific areas of discourse iri the natura l language. In either case, tit(, practical situation which requires the pr<)eessing of i)fformation expressed in the natural l anguage in a relatively unrestricted area of discourse is not included. Moreover, results <)btained for the siInplified st(nations cannot in general be extended to cover the desired problem area."""	act!;information retrieval;insertion sort;linear algebra;resolution enhancement technology	Gerard Salton	1962	Commun. ACM	10.1145/366792.366828	document retrieval;query expansion;visual word;computer science;concept search;adversarial information retrieval;information retrieval	AI	-36.43232059508317	-75.50694118445772	103045
a8d89d11dcf46643c37d93da07f135032505d6a1	essential summarizer: innovative automatic text summarization software in twenty languages	text mining;automatic text summarization;summarization;extraction of information;information language;computational linguistics;relevance;discourse analysis;multilingualism;text summarization tool	With the advent of electronic textual documents following the fulgurating development of data processing, there are now pressing needs to extract useful and reusable information from text. It is thus quite natural to address the problem of the overabundance of digital textual information. The technology of automatic text summarization, along with other solutions in the area of text mining, tries to remedy this by providing easier access to essential information, in condensed form and for better potential reuse. Through a specific process, this technology makes it possible to analyze a text in order to extract only efficient information for reuse in view of precise goals, saving time and enhancing productivity. We have developed an automatic summarization software called Essential Summarizer, with an approach based on linguistic techniques to perform semantic analysis of written text. This innovative application is very fast and produces summaries tailored to the user's needs in twenty languages.	automatic summarization	Abderrafih Lehmam	2010			natural language processing;text graph;text mining;multi-document summarization;computer science;automatic summarization;data mining;information retrieval	AI	-33.76600797875132	-68.31290641206084	103190
f35be0af7075d4ef6632ae41289ea79843c327ec	lemon-aid: using lemon to aid quantitative historical linguistic analysis		In this short paper, we describe how we converted dictionary and wordlist data made available by the QuantHistLing project into the Lexicon Model for Ontologies. By doing so, we leverage Linked Data to combine disparate lexical resources – more than fifty lexicons and dictionaries – by converting the lexical data into an RDF model that is specified by Lemon. The resulting new Linked Data resource, what we call the QHL dataset, provides researchers with a translation graph, which allows users to query across the underlying lexicons and dictionaries to extract semantically-aligned wordlists.	algorithm;dictionary;hellmann–feynman theorem;human-readable medium;interoperability;language code;lexicon;linguistic linked open data;linked data;natural language processing;neuroscience information framework;ontology (information science);qualitative comparative analysis	Steven Moran;Martin Brümmer	2013			humanities;art;literature	NLP	-34.709903521448936	-71.46917462353495	103667
d287152fda8715275c8087c9e56b24fe5653a24d	analysis of the communications aspects of an inquiry-response system	input-output terminal;customer representative;airlines industry;information retrieval need;requested information;input message;inquiry-response system;communications aspect;certain information;computer file;centralized computer file	In order to meet the information retrieval needs of various industries, inquiry-response systems are being implemented by storing large data bases in centralized computer files. In some systems, the files are accessed by personnel primarily as the result of telephone calls from customers. As an example, in the airlines industry, computer files are accessed by reservation clerks to determine the availability of reservations for a specific flight. In this example, and in similar applications involving queries or requests from customers, input messages requesting certain information are generated by a customer representative and then transmitted to a computer from an input-output terminal such as a visual display device. When the computer has obtained the requested information, a response message is transmitted back to the requesting terminal, and the representative continues her dialogue with the customer.	centralized computing;computer file;database;display device;information retrieval	J. S. Sykes	1969		10.1145/1478559.1478639	simulation;computer science;multimedia;world wide web	DB	-42.75421757208201	-78.15410289866647	104565
b702f4a48b5daf1b07ac22bbc8f17c860cc24150	digital documentation of oral discourse genres		This article presents the design of an interoperable database for digital documen- tation of oral discourse genres in multiple languages. Focussing on stylistic form and cultural specificity of artistic expression, the categories of study that serve as data fields build on contextual and functional approaches to verbal art per- formance. The database is part of a larger project known as VOVA (VOcal and Verbal Arts archives) that seeks to create digital tools for editing and annotating stylized oral discourse for purposes of comparative study of oral traditions and the preservation of endangered languages. Detailed descriptions of fields and numerous examples of the type of data solicited by VOVA, taken from leading scholarship in the field, help to clarify the scientific aims of the project. Search modes for consulting the database are also provided. Relations between the symbol-making and symbol-using activities of language use, text editing, and the digital humanities are discussed in light of the anthropological and linguistic research that will serve as a basis for a systematic study of stylistics in speech.	documentation	Catharine Mason	2010	LLC	10.1093/llc/fqq008	linguistics;multimedia	NLP	-35.46276491140423	-76.52589751389728	104730
84279ff1e8b847ec294d24a8ea7a0ce659598e9d	enriching entity grids and graphs with discourse relations: the impact in local coherence evaluation		This paper describes how discursive knowledge, given by the discursive theories RST (Rhetorical Structure Theory) and CST (Crossdocument Structure Theory), may improve the automatic evaluation of local coherence in multi-document summaries. Two of the main coherence models from literature were incremented with discursive information and obtained 91.3% of accuracy, with a gain of 53% in relation to the original results.	graph (discrete mathematics);theory	Márcio de Souza Dias;Thiago A. S. Pardo	2015			discrete mathematics;graph;coherence (physics);mathematics	NLP	-37.11963559758216	-79.95239645317324	104912
7677a1adc9823c42ad81cc13519591f2a4db7717	diseño y generación semi-automática de patrones adaptables para el reconocimiento de entidades	informatica;doctoralthesis;computacion informatica;diseno de patrones;information extraction;gestion de la informacion;peerreviewed;filologias;generacion automatica de patrones;info eu repo semantics article;informacion documentacion;reconocimiento de entidades nombradas;linguistica;named entity recognition;ciencias basicas y experimentales;extraccion de informacion;info eu repo semantics doctoralthesis;control automatico;grupo a;ciencias sociales;ner;grupo b;automatic pattern generation	PhD Thesis in Computer Science written by Mónica Marrero Llinares at the University Carlos III of Madrid under the supervision of Dra. Sonia Sánchez Cuadrado and Dr. Jorge Morato Lara. The author was examined on 21st May 2013 by a committee formed by the doctors Juan Lloréns Morillo (University Carlos III of Madrid), Rafael Valencia Garćıa (University of Murcia) and Roberto Carniel (University of Udine). It obtained the grade of Excellent Cum Laude unanimously and received the international mention.	computer science;roberto busa;semiconductor industry	Mónica Marrero	2014	Procesamiento del Lenguaje Natural		natural language processing;computer science;information extraction	Theory	-38.48367586076774	-77.0939328375552	105351
7550db75aa93dde23a0ebf1e40ef81f5e1f14690	machine vs. human translation of snomed ct terms		OBJECTIVE In the context of past and current SNOMED CT translation projects we compare three kinds of SNOMED CT translations from English to German by: (t1) professional medical translators; (t2) a free Web-based machine translation service; (t3) medical students.   METHODS 500 SNOMED CT fully specified names from the (English) International release were randomly selected. Based on this, German translations t1, t2, and t3 were generated. A German and an Austrian physician rated the translations for linguistic correctness and content fidelity.   RESULTS Kappa for inter-rater reliability was 0.4 for linguistic correctness and 0.23 for content fidelity. Average ratings of linguistic correctness did not differ significantly between human translation scenarios. Content fidelity was rated slightly better for student translators compared to professional translators. Comparing machine to human translation, the linguistic correctness differed about 0.5 scale units in favour of the human translation and about 0.25 regarding content fidelity, equally in favour of the human translation.   CONCLUSION The results demonstrate that low-cost translation solutions of medical terms may produce surprisingly good results. Although we would not recommend low-cost translation for producing standardized preferred terms, this approach can be useful for creating additional language-specific entry terms. This may serve several important use cases. We also recommend testing this method to bootstrap a crowdsourcing process, by which term translations are gathered, improved, maintained, and rated by the user community.	ct scan;correctness (computer science);crowdsourcing;genetic translation process;inter-rater reliability;language translations;linguistics;machine translation;name;pet/ct scan;randomness;rating (action);systematized nomenclature of medicine;test-retest reliability;virtual community;x-ray computed tomography	Stefan Schulz;Johannes Bernhardt-Melischnig;Markus Kreuzthaler;Philipp Daumke;Martin Boeker	2013	Studies in health technology and informatics	10.3233/978-1-61499-289-9-581	data mining;information retrieval;snomed ct;medicine	NLP	-46.60133161058443	-70.22583657598895	105400
08a705f1b1ec4c30f771e0a8f83d1247529913d4	language systems, inc.: description of the dbg system as used for muc-3		LSI's Data Base Generation (DBG) approach to natural language understanding is characterized by three main features: First, the DBG system is comprehensive. It performs full-scale lexical, syntactic, semantic, and discourse analyses of the entire message text being processed and produces a complete knowledge representation of the text. Second, the DBG system is modular and flexible. The modular and transparent system architecture ensures easy extension, maintenance, and upgrading of the system. And, third, the DBG system is generic but at the same time domain-sensitive. It applies domain modeling to text interpretation, which enables the extension of the system to any number of new domains. In addition, it provides a powerful capability for handling unknown data in familiar domains. DBG's development has been based on analysis of large volumes of message traffic (thousands of Air Force and Army messages) in five domains, as described below. The system can currently process a large number of messages in each of these domains and has been formally tested on previously unseen messages in three of these, with competitive tests against humans performing the same task in two domains. The functional flow of the DBG system is shown in Figure 1 (actually Figure 1 of our site report [Language Systems Inc: MUC-3 Test Results and Analysis] in this proceedings).	dbg	Christine A. Montgomery;Bonnie Glover Stalls;Robert S. Belvin;Robert E. Stumberger	1991		10.3115/1071958.1071987	natural language processing;computer science;artificial intelligence;communication	Robotics	-35.007780022864715	-77.64386284411782	105513
f081f24fdde13121514c86599c73c1fb36ad9b76	multimodal learning analytics: description of math data corpus for icmi grand challenge workshop	domain expertise;digital pen;multimodal learning analytics;speech;data resources;math data corpus;images	This paper provides documentation on dataset resources for establishing a new research area called multimodal learning analytics (MMLA). Research on this topic has the potential to transform the future of educational practice and technology, as well as computational techniques for advancing data analytics. The Math Data Corpus includes high-fidelity time-synchronized multimodal data recordings (speech, digital pen, images) on collaborating groups of students as they work together to solve mathematics problems that vary in difficulty level. The Math Data Corpus resources include initial coding of problem segmentation, problem-solving correctness, and representational content of students' writing. These resources are made available to participants in the data-driven grand challenge for the Second International Workshop on Multimodal Learning Analytics. The primary goal of this event is to analyze coherent signal, activity, and lexical patterns that can identify domain expertise and change in domain expertise early, reliably, and objectively, as well as learning-oriented precursors. An additional aim is to build an international research community in the emerging area of multimodal learning analytics by organizing a series of workshops that bring together multidisciplinary scientists to work on MMLA topics.	coherence (physics);correctness (computer science);digital pen;documentation;grand challenges;multimodal interaction;multimodal learning;organizing (structure);problem solving	Sharon L. Oviatt;Adrienne Cohen;Nadir Weibel	2013		10.1145/2522848.2533790	computer vision;analytics;speech recognition;human–computer interaction;computer science;speech;artificial intelligence;data science;machine learning;data mining;linguistics;multimedia;subject-matter expert;cultural analytics;semantic analytics	ML	-39.760738189565714	-77.50894731235742	106010
b38ac385eb3d2484be5963a221b58fd7dc0dfcc8	graph-based word sense disambiguation of biomedical documents	documento;word sense disambiguation;document;grafo;graph;graphe	MOTIVATION Word Sense Disambiguation (WSD), automatically identifying the meaning of ambiguous words in context, is an important stage of text processing. This article presents a graph-based approach to WSD in the biomedical domain. The method is unsupervised and does not require any labeled training data. It makes use of knowledge from the Unified Medical Language System (UMLS) Metathesaurus which is represented as a graph. A state-of-the-art algorithm, Personalized PageRank, is used to perform WSD.   RESULTS When evaluated on the NLM-WSD dataset, the algorithm outperforms other methods that rely on the UMLS Metathesaurus alone.   AVAILABILITY The WSD system is open source licensed and available from http://ixa2.si.ehu.es/ukb/. The UMLS, MetaMap program and NLM-WSD corpus are available from the National Library of Medicine https://www.nlm.nih.gov/research/umls/, http://mmtx.nlm.nih.gov and http://wsd.nlm.nih.gov. Software to convert the NLM-WSD corpus into a format that can be used by our WSD system is available from http://www.dcs.shef.ac.uk/∼marks/biomedical_wsd under open source license.	algorithm;body of uterus;data table;exercise;expectation propagation;experiment;graph - visual representation;national library of medicine (u.s.);netware loadable module;open-source license;open-source software;pagerank;science;sense organs;silo (dataset);text corpus;tracer;umls metathesaurus;unified medical language system;unsupervised learning;vocabulary;web services for devices;word sense;word-sense disambiguation	Eneko Agirre;Aitor Soroa;Mark Stevenson	2010	Bioinformatics	10.1093/bioinformatics/btq555	natural language processing;computer science;graph;world wide web;information retrieval	NLP	-34.34769621383178	-69.79061188827154	106382
8ddf28c92c479dc7d54180308e98380778c8f7a4	mesh(c): from a controlled vocabulary to a processable resource.	controlled vocabulary	Large repositories of life science data in the form of domain-specific literature, textual databases and other large specialised textual collections (corpora) in electronic form increase on a daily basis to a level beyond the human mind can grasp and interpret. As the volume of data continues to increase, substantial support from new information technologies and computational techniques grounded in the form of the ever increasing applications of the mining paradigm is becoming apparent. These emerging technologies play an increasingly critical role in aiding research productivity, and they provide the means for reducing the workload for information access and decision support and for speeding up and enhancing the knowledge discovery process. In order to accomplish these higher level goals and support the mining approach however, a fundamental and unavoidable starting point is the identification and mapping of terminology from the textual, unstructured data onto biomedical knowledge sources and concept hierarchies. In this paper, we provide a description of the work regarding terminology recognition using the Swedish MeSH® thesaurus and its corresponding English original source. We explain the various transformation and refinement steps applied to the original database tables into a fully-fledged processing oriented annotating resource. Particular attention has been given to a number of these steps in order to automatically map the extensive variability of lexical terms to structured MeSH® nodes. Issues on annotation and coverage are also discussed.	controlled vocabulary;database;decision support system;information access;mind;programming paradigm;refinement (computing);spatial variability;table (database);text corpus;thesaurus;unstructured grid	Dimitrios Kokkinakis	2008			controlled vocabulary;speech recognition;computer science;linguistics;multimedia;world wide web	Web+IR	-33.95234033244721	-68.19747101380698	106871
4bf6cdc7e3f8efea6831a24574f31b2569a18615	detecting an infant's developmental reactions in reviews on picture books		We extract the book reviews on picture books written on the Web site specialized in picture books, and found that those reviews reflect infants’ behavioral expressions as well as their parents’ reading activities in detail. Analysis of the reviews reveals that infants’ reactions written on the reviews are coincident with the findings of developmental psychology concerning infants’ behaviors. In order to examine how the stimuli of picture books induces varieties of infants’ reactions, this paper proposes to detect an infant’s developmental reactions in reviews on picture books and shows effectiveness of the proposed method through experimental evaluation.	book;sensor;world wide web	Hiroshi Uehara;Mizuho Baba;Takehito Utsuro	2015			coincident;developmental psychology;psychology	NLP	-44.89400843507093	-74.15417676927899	107302
4e1c41da744c603ddb3a0bbdf8d8334644fe1a13	correcting real-word spelling errors by restoring lexical cohesion	filologias;linguistica;grupo a	Spelling errors that happen to result in a real word in the lexicon cannot be detected by a conventional spelling checker. We present a method for detecting and correcting many such errors by identifying tokens that are semantically unrelated to their context and are spelling variations of words that would be related to the context. Relatedness to context is determined by a measure of semantic distance initially proposed by Jiang and Conrath (1997). We tested the method on an artificial corpus of errors; it achieved recall of 23–50% and precision of 18–25%. 1 Real-word spelling errors Conventional spelling checkers detect typing errors simply by comparing each token of a text against a dictionary of words that are known to be correctly spelled. Any token that matches an element of the dictionary, possibly after some minimal morphological analysis, is deemed to be correctly spelled; any token that matches no element is flagged as a possible error, with near-matches displayed as suggested corrections. Typing errors that happen to result in a token that is a correctly spelled word, albeit not the one that the user intended, cannot be detected by such systems. Such errors are not uncommon; Mitton (1987, 1996) found that “real-word errors account for about a quarter to a third of all spelling errors, perhaps more if you include word-division errors”. A fortiori, it is now common for real-word errors to be introduced by auto-correction mechanisms and by conventional spelling checkers 1 An auto-correction mechanism watches out for certain pre-defined “errors” as the user types, replacing them with a “correction” and giving no indication or warning of the change. Such mechanisms are intended for undoubted typing errors for which only one correction is plausible, such as correcting accomodate to accommodate; deliberate misspellings (as in this footnote) are precluded. However, the ‘AutoCorrect’ feature of Microsoft Word contains by default many “corrections” for which other possibilities are also plausible. For example (in Microsoft Word v.X for Macintosh, with all updates to December 2003), wierd is changed to weird, although wired and wield are also plausible; eyt is changed to yet, although eye is plausible; and Herat is changed to Heart, although Herat is plausibly correct as it stands. Thus, a typing error that could have been subsequently detected by a spelling-checker may be replaced by a real-word error that can’t be. 88 G. Hirst and A. Budanitsky when the user carelessly accepts an incorrect recommendation by the system or inadvertently chooses the wrong correction from a menu. By contrast, a human proofreader, using linguistic and world knowledge, will usually notice an error of this kind because it will cause the text to be set somehow awry. If the erroneous token is of a different part of speech from that intended, then the sentence that it is in might not be parsable: Example 1 The instrumental parts were recorded at different times and them [then] later combined on the master tape to produce special effects. If the erroneous token is semantically unrelated to the intended one, then the sentence might not make sense: Example 2 It is my sincere hole [hope] that you will recover swiftly. Some typing errors will cause both conditions to occur: Example 3 We all hole [hope] that you will recover swiftly. And, of course, some errors result in a perfectly well-formed text, even if they produce a meaning other than that intended, and hence cannot be detected without knowledge or inference of the writer’s intention: Example 4 The committee is now [not] prepared to grant your request. See Kukich (1992) or Mitton (1996) for an extensive survey of types of spelling errors and early approaches to the problem. In this paper, we will discuss the detection and correction of errors that result in semantic anomaly, as in example 2. To distinguish these errors from the other kinds, we will refer to them, somewhat loosely, as malapropisms. Strictly speaking, a malapropism is an amusing substitution, due to ignorance and pretentiousness on the part of the writer or speaker, of one word for another of similar spelling or sound: Example 5 She has reached the pinochle [pinnacle] of success. For our purposes in this paper, it is immaterial whether the cause of the error is ignorance, pretentiousness, poor typing, or careless use of a conventional spelling checker (and whether or not the error is cause for amusement). Our goal is thus 2 The term is sometimes used even more loosely; for example, many of the spoken “malapropisms” attributed to George W. Bush, while possibly both amusing and due to ignorance, are actually non-word errors (They misunderestimated me; This issue doesn’t seem to resignate with the people) or other kinds of linguistic or non-linguistic error (Families is . . .where wings take dream; Our nation must come together to unite); see http://slate.msn.com/Features/bushisms/bushisms.asp. Correcting real-word spelling errors by restoring lexical cohesion 89 considerably broader than that of other recent work on real-word errors that aims simply at detecting occurrences of any of a small, pre-defined set of common errors; see section 7 below for discussion of this work. 2 Malapropisms as perturbations of cohesion By their nature, naturally occurring coherent, meaningful texts contain many instances of the mechanisms of linguistic cohesion, such as word repetitions, coreference, and sets of semantically related words (Halliday and Hasan 1976; Hoey 1991). A coherent text will naturally refer to various concepts that are related to its topic or topics and hence are related to one another. The recurrence in a text of lexemes related to a particular concept is often characterized metaphorically as a ‘chain’ of words running through the text, linked by lexical and semantic relationships such as literal repetition, coreference, synonymy, and hyponymy (Halliday and Hasan 1976; Morris and Hirst 1991; Hoey 1991). A coherent text will have many such lexical chains, each running through part or all of the text and pertaining to some aspect of the topic or topics of the text; and, conversely, most content words of the text will be members of one or more chains. Because they are indicative of the structure and content of a text, lexical chains have been applied in computational linguistics for tasks such as text segmentation (Morris and Hirst 1991; Okumura and Honda 1994), lexical disambiguation (Okumura and Honda 1994), automatic creation of hypertext (Green 1999), and text summarization (Barzilay and Elhadad 1999; Silber and McCoy 2002); see Budanitsky (1999) for a detailed review. However, it remains an open research question as to just what kinds and degrees of semantic relationship should qualify as links for a lexical chain; we discuss this issue in a separate paper (Budanitsky and Hirst submitted). A malapropism is a perturbation of the cohesion (and coherence) of a text. By definition, it is semantically inappropriate in its immediate context, and is probably therefore also semantically inappropriate in the broader context of the text itself. It is therefore unlikely that a malapropism can be linked into any of the lexical chains of the nearby text; it will probably bear no semantic relationship to any other word in the text. On the other hand, it is likely (though not assured) that the intended word would fit into a lexical chain in the text. Thus the problem of detecting and correcting malapropisms can be cast as the problem of detecting tokens that fit into no lexical chain of the text and replacing them with words for which they are plausible mistypings that do fit into a lexical chain. This idea was first tried by Hirst and St-Onge (1998), who reported modest success; while the system’s performance in both detecting and correcting malapropisms was 3 There are two qualifications to this statement. First, the malapropisms that we are considering are primarily performance errors – slips in typing. So if the malapropism is instead a competence error and is repeated within the text – consistently typing pinochle for pinnacle, for example, in the belief that the former is the correct spelling of the latter – then the malapropisms will form a lexical chain by their repetition. Such a text would be incoherent but cohesive, and the methods to be discussed below will not apply. Second, there is actually a mild cognitive bias in performance errors to words that are indeed related to the intended word or its context (Fromkin 1980), but we ignore this effect here. 90 G. Hirst and A. Budanitsky well above baseline, it was nonetheless prone to far too many false alarms: for every true malapropism that it found, it would flag about ten other tokens that were not malapropisms at all. It was especially vulnerable to confusion by proper names and by common words of minimal topical content; for example, it suggested that year was a mistyping of pear in the context of Lotus Development Corporation, because lotus and pear are both hyponyms of fruit and hence could form a lexical chain. One of the serious problems underlying the system was an inadequate account of semantic relatedness in its construction of lexical chains. Two non-identical lexemes were considered to be related if and only if a short path of an allowable shape could be found between their synsets in the noun portion of WordNet. Paths could follow any of the WordNet relationships. (The details of ‘allowable shape’ and requisite shortness are not necessary here; the interested reader may see Hirst and St-Onge (1998).) 3 A new algorithm for detecting and correcting malapropisms We have developed a new algorithm for malapropism detection and correction that, like Hirst and St-Onge’s, is based on the idea of detecting and eliminating perturbations of cohesion in text. However, the new algorithm does not use lexical chains per se; rather, it treats a text as a bag of words (more	algorithm;anomaly detection;automatic summarization;bag-of-words model;baseline (configuration management);blender (software);coherence (physics);cohesion (computer science);commonsense knowledge (artificial intelligence);computational linguistics;dictionary;hypertext;lexical definition;lexical substitution;lexicon;literal (mathematical logic);lotus 1-2-3;markov chain;microsoft word for mac;open research;parsing;perturbation theory;precision and recall;prototype;semantic similarity;sensor;spell checker;synonym ring;syntactic methods;text segmentation;typing;usability;user interface;well-formed petri net;well-formed formula;word-sense disambiguation;wordnet	Graeme Hirst;Alexander Budanitsky	2005	Natural Language Engineering	10.1017/S1351324904003560	natural language processing;speech recognition;computer science;linguistics	NLP	-38.78235089034216	-75.97386714666389	108137
15ff7277f4334482086522dd901dbdf5d1cd6fd4	uncertainty modeling for ontology-based mammography annotation with intelligent bi-rads scoring	bayesian network;uncertainty;semantic web;sqwrl;mammography;ontology	This paper presents an ontology-based annotation system and BI-RADS (Breast Imaging Reporting and Data System) score reasoning with Semantic Web technologies in mammography. The annotation system is based on the Mammography Annotation Ontology (MAO) where the BI-RADS score reasoning works. However, ontologies are based on crisp logic and they cannot handle uncertainty. Consequently, we propose a Bayesian-based approach to model uncertainty in mammography ontology and make reasoning possible using BI-RADS scores with SQWRL (Semantic Query-enhanced Web Rule Language). First, we give general information about our system and present details of mammography annotation ontology, its main concepts and relationships. Then, we express uncertainty in mammography and present approaches to handle uncertainty issues. System is evaluated with a manually annotated dataset DEMS (Dokuz Eylul University Mammography Set) and DDSM (Digital Database for Screening Mammography). We give the result of experimentations in terms of accuracy, sensitivity, precision and uncertainty level measures.	annotation;bi-rads;monoamine oxidase;ontology (information science);reasoning;score;screening mammography;semantic web;semantic query;silo (dataset);web rule language	Hakan Bulu;Adil Alpkocak;Pinar Balci	2013	Computers in biology and medicine	10.1016/j.compbiomed.2013.01.001	uncertainty;computer science;data science;machine learning;semantic web;ontology;bayesian network;data mining;information retrieval;statistics	AI	-48.23671074046779	-66.50706515928178	109069
47af0492543cb70a4e9cdea7fe834bd4fab5e8a1	enrichissement des contenus par la réindexation des usagers : un état de l'art sur la problématique		Information retrieval (IR) is a user approach to obtain relevant information which meets needs with the help of a IR system (IRS). However, the IRS shows certain differences between user relevance and system relevance. These gaps are essentially related to the imperfection of the indexing process (as approach related to the IR), to problems related to the misunderstanding of the natural language and the non correspondence between the real needs of the user and the results of his query. As idea is to think about an ?intellectual? indexing that takes into account the point of view of the user. By consulting the document, user can build information as added-value on the existing content: new information which grows contents and allows the semantic visibility or facilitates the reading by the annotations, by links to other content, by new descriptors, specific new abstracts of users: it is the reindexing of the contents by the contribution or the vote of the uses	information retrieval;linear algebra;natural language;point of view (computer hardware company);relevance;search engine indexing	Azza Harbaoui;Malek Ghenima;Sahbi Sidhom	2009	CoRR		data mining;information retrieval;search engine indexing;natural language;computer science;indexation	Web+IR	-37.53930689981551	-66.32007422102818	109565
366bed5a435b3a15840791685f7a39c8f14002f8	quality control for terms and definitions in ontologies and taxonomies	biomedical ontologies;vocabulary controlled;ontology mapping;phylogeny;databases genetic;standardized formats;computational method;classification;computational biology bioinformatics;terminology as topic;proteins;molecular biology;artificial intelligence;algorithms;combinatorial libraries;quality control;computer appl in life sciences;information storage and retrieval;controlled vocabularies;natural language processing;documentation;microarrays;bioinformatics;gene ontology	Ontologies and taxonomies are among the most important computational resources for molecular biology and bioinformatics. A series of recent papers has shown that the Gene Ontology (GO), the most prominent taxonomic resource in these fields, is marked by flaws of certain characteristic types, which flow from a failure to address basic ontological principles. As yet, no methods have been proposed which would allow ontology curators to pinpoint flawed terms or definitions in ontologies in a systematic way. We present computational methods that automatically identify terms and definitions which are defined in a circular or unintelligible way. We further demonstrate the potential of these methods by applying them to isolate a subset of 6001 problematic GO terms. By automatically aligning GO with other ontologies and taxonomies we were able to propose alternative synonyms and definitions for some of these problematic terms. This allows us to demonstrate that these other resources do not contain definitions superior to those supplied by GO. Our methods provide reliable indications of the quality of terms and definitions in ontologies and taxonomies. Further, they are well suited to assist ontology curators in drawing their attention to those terms that are ill-defined. We have further shown the limitations of ontology mapping and alignment in assisting ontology curators in rectifying problems, thus pointing to the need for manual curation.	bioinformatics;computation;computational resource;curators of sweden;digital curation;gene ontology;molecular biology;ontology (information science);paper;providing (action);rectifier;semantic integration;subgroup;taxonomy (general);web ontology language	Jacob Köhler;Katherine Munn;Alexander Rüegg;Andre Skusa;Barry Smith	2005	BMC Bioinformatics	10.1186/1471-2105-7-212	biology;quality control;ontology components;documentation;computer science;bioinformatics;data mining;information retrieval;phylogenetics	Web+IR	-35.3032265946993	-69.88567189767132	109666
0109ec6fab8e2d487cb35006097e51c23b557263	research paper: using semantic and structural properties of the unified medical language system to discover potential terminological relationships	preferential attachment;unified medical language system;research paper;graph partitioning;classification accuracy;structural properties	OBJECTIVE To use the semantic and structural properties in the Unified Medical Language System (UMLS) Metathesaurus to characterize and discover potential relationships.   DESIGN The UMLS integrates knowledge from several biomedical terminologies. This knowledge can be used to discover implicit semantic relationships between concepts. In this paper, the authors propose a problem-independent approach for discovering potential terminological relationships that employs semantic abstraction of indirect relationship paths to perform classification and analysis of network theoretical measures such as topological overlap, preferential attachment, graph partitioning, and number of indirect paths. Using different versions of the UMLS, the authors evaluate the proposed approach's ability to predict newly added relationships.   MEASUREMENTS Classification accuracy, precision-recall.   RESULTS Strong discriminative characteristics were observed with a semantic abstraction based classifier (classification accuracy of 91%), the average number of indirect paths, preferential attachment, and graph partitioning to identify potential relationships. The proposed relationship prediction algorithm resulted in 56% recall in top 10 results for new relationships added to subsequent versions of the UMLS between 2005 and 2007.   CONCLUSIONS The UMLS has sufficient knowledge to enable discovery of potential terminological relationships.	attachments;congenital neurologic anomalies;controlled vocabulary;discriminative model;graph - visual representation;graph partition;programming languages;semantic data model;umls metathesaurus;unified medical language system;version;algorithm	Chintan Patel;James J. Cimino	2009	Journal of the American Medical Informatics Association : JAMIA	10.1197/jamia.M2931	natural language processing;medicine;computer science;graph partition;nursing;machine learning;data mining;database;unified medical language system;information retrieval	Web+IR	-47.538339256947225	-67.40809951990758	110577
b3133bfc330566a8c2c7c81368f53af196c4d10c	adaptive classifiers, topic drifts and go annotations	computational biology;genes;natural language processing	Gene annotations with Gene Ontology codes offer scientists important options in their study of genes and their functions. Automatic GO annotation methods have the potential to supplement the intensive manual annotation processes. Annotation approaches using MEDLINE documents are generally two-phased where the first is to annotate documents with GO codes and the second is to annotate gene products via the documents. In this paper we study document annotation with GO codes using a temporal perspective. Specifically, we build adaptive code-specific classifiers. We also study topic drift i.e., changes in the contextual characteristics of annotations over time. We show that topic drift is significant especially in the biological process GO hierarchy. This at least partially explains the particular challenges faced with codes of this hierarchy.	abstract summary;adaptive grammar;biological processes;code;gene annotation;gene ontology;largest;medline;support vector machine;version;cellular_component	Padmini Srinivasan	2007	AMIA ... Annual Symposium proceedings. AMIA Symposium		gene annotation;ontology;hierarchy;natural language processing;annotation;artificial intelligence;medline;computer science	Theory	-35.273558913027486	-69.8629122186244	110752
71fef916055871f84e6f02015a92ca30e025ea31	an ontology framework for recommendation about a crime scene investigation	ontologies forensics concrete standards owl laboratories logic gates;owl;figures of merit ontology framework crime scene investigation recommendation forensic science police center semantic analysis semantic structures case reports knowledge base concept thailand tablet android os;standards;recommender systems android operating system forensic science knowledge based systems notebook computers ontologies artificial intelligence police data processing;logic gates;crime scene investigation recommendation ontologyframework text tokenization;ontologies;forensics;concrete	Currently, process of recording the evidence at the scene of the forensic science of police center is facing a problem of long delay, information redundancy and disorganized structure. The semantic analysis found that words and phrases in case reports are semantic structures. Moreover, the content of a case report can be represented by the knowledge base concept (Ontology). This research developed ontology to recommend words to fill-in a case report of the crime scene investigation report about properties for the Forensic Science Police Center 4 of Thailand. The purpose is to suggest words or sentenceswhich areappropriate and have most possibilities to be used by users. The rules of ontology were defined based on theexisting documents. The ontology was bringing into acknowledge feeding unitof an application program on a tablet with Android OS to enhance the performance of the crime scene investigation process. The figures of merit of the ontology were measured by filling-in of 30 real crime cases. The results were 82.50%, 96.67% and 88.49% of precision, recall and F-measure, respectively.Thus, the suggestions based on the developed ontology can reduce the key stroke and get a shortening the time, higher accuracy and more standardized document.	android;chaos theory;f1 score;knowledge base;operating system;redundancy (information theory);semantic analysis (compilers);tablet computer	Boonyarin Onnoom;Sirapat Chiewchanwattana;Khamron Sunat;Nutcharee Wichiennit	2014	2014 14th International Symposium on Communications and Information Technologies (ISCIT)	10.1109/ISCIT.2014.7011895	upper ontology;bibliographic ontology;computer science;ontology;data mining;world wide web;computer security;process ontology	Web+IR	-38.48285284812731	-71.04231584672029	111776
2f318c5f6a934c82a3c41f0ef73bc7d7b164c26d	a distributed information extraction system integrating ontological knowledge and probabilistic classifiers	distributed approach information extraction markov random fields;bionlp 2013 distributed information extraction system ontological knowledge probabilistic classifiers concept extraction relation extraction search engine biomedical application distributed architecture;information extraction;markov random fields;distributed approach;random variables;computer architecture;probabilistic logic computer architecture indexing microorganisms computational modeling belief propagation random variables;computational modeling;indexing;belief propagation;search engines information retrieval medical computing ontologies artificial intelligence pattern classification probability;probabilistic logic;microorganisms	In this work we consider the problem of extracting concepts and relations between them from documents, aiming at constructing an index for a more semantically oriented search engine. While assessment is performed on a biomedical application, the proposed solutions can be also applied to different domains. With the distributed architecture proposed, we obtain an approach that can be applied also on large data sets. Experimental assessment has been performed on a standard data set, BioNLP 2013.	biomedical text mining;distributed computing;information extraction;probabilistic database;relationship extraction;statistical classification;web search engine	Anita Alicante;Massimo Benerecetti;Anna Corazza;Stefano Silvestri	2014	2014 Ninth International Conference on P2P, Parallel, Grid, Cloud and Internet Computing	10.1109/3PGCIC.2014.87	computer science;theoretical computer science;machine learning;data mining	DB	-41.78961108412674	-67.82574789803773	111916
24f73ab2828f9aecbc079a9d5f5028c392eaa5b0	automatic indexing of pathology data	databases;computer storage devices;diagrams;indexing;tables data;automatic indexing;diseases;pathology;subject index terms	A procedure for automated indexing of pathology diagnostic reports at the National Institutes of Health is described. Diagnostic statements in medical English are encoded by computer into the Systematized Nomenclature of Pathology (SNOP). SNOP is a structured indexing language constructed by pathologists for manual indexing. It is of interest that effective automatic encoding can be based upon an existing vocabulary and code designed for manual methods. Morphosyntactic analysis, a simple syntax analysis, matching of dictionary entries consisting of several words, and synonym substitutions are techniques utilized.	dictionary [publication type];indexes;matching;parsing;rodent nomenclature name;synonym ring;vocabulary	George S. Dunham;Milos Pacak;Arnold W. Pratt	1978	Journal of the American Society for Information Science. American Society for Information Science	10.1002/asi.4630290207	natural language processing;search engine indexing;computer science;diagram;computer data storage;database;information retrieval	NLP	-48.01046419377431	-68.660730067787	112033
2f9af2f895735cea8c8f191d1010a3abfb46b19b	annalist - annotation alignment and scoring tool	system architecture	In this paper we describe ANNALIST (Annotation, Alignment and Scoring Tool), a scoring system for the evaluation of the output of semantic annotation systems. ANNALIST has been designed as a system that is easily extensible and configurable for different domains, data formats, and evaluation tasks. The system architecture enables data input via the use of plugins and the users can access the system’s internal alignment and scoring mechanisms without the need to convert their data to a specified format. Although developed for evaluation tasks that involve the scoring of entity mentions and relations primarily, ANNALIST’s generic object representation and the availability of a range of criteria for the comparison of annotations enable the system to be tailored to a variety of scoring jobs. The paper reports on results from using ANNALIST in real-world situations in comparison to other scorers which are more established in the literature. ANNALIST has been used extensively for evaluation tasks within the VIKEF (EU FP6) and CLEF (UK MRC) projects.	plug-in (computing);systems architecture	George Demetriou;Robert J. Gaizauskas;Haotian Sun;Angus Roberts	2008			data mining;plug-in;information retrieval;computer science;extensibility;systems architecture;annotation	Web+IR	-34.450091321831145	-73.09772940870153	112068
38b7936f0bf33f7793ea4faaa16af4d60eb46490	employing named entities for semantic retrieval of news videos in turkish	semantic annotation;semantic indexing;named entities;named entity extraction;automatic semantic video annotation;semantic retrieval architecture;rule based;video retrieval;text analysis;speech;video retrieval natural language processing text analysis;satisfiability;data mining;genuine news video corpus;automatic semantic video annotation genuine news video corpus semantic retrieval architecture semantic annotation named entities semantic indexing news transcription texts rule based named entity recognizer retrieval interface boolean queries;indexing;videos data mining indexing automatic speech recognition ontologies employment multimedia communication natural languages tv power electronics;rule based named entity recognizer;named entity recognizer;news transcription texts;success rate;video annotation;speech recognition;organizations;text recognition;boolean queries;natural language processing;retrieval interface;named entity;videos;semantic retrieval	Named entities are known to be important means for semantic annotation of news texts. Considerable work has been carried out for semantic indexing of both textual news and news videos especially in English through the employment of named entities extracted from textual news or transcriptions of the news videos. In this paper, we present our semantic retrieval architecture for news videos in Turkish based on prior semantic annotation of the videos with the corresponding named entities in the news transcription texts. We employ a rule-based named entity recognizer for Turkish which makes use of handcrafted sets of lexical resources and pattern bases. We compiled a small corpus of Turkish news videos and the named entity recognizer in its current form achieves a success rate of about 75% on this corpus. A retrieval interface is implemented to access the video corpus through the boolean queries formed with the extracted named entities. The interface currently does not involve any ranking procedure, displaying all the videos, the transcription texts of which satisfy the boolean query posed through the interface, sorted by their broadcast date. The presented study is significant for its being the first study to perform automatic semantic video annotation on a genuine news video corpus in Turkish and demonstrating the utilization of the annotations through a retrieval interface.	compiler;finite-state machine;logic programming;named entity;text corpus;transcription (software)	Dilek Küçük;Adnan Yazici	2009	2009 24th International Symposium on Computer and Information Sciences	10.1109/ISCIS.2009.5291836	rule-based system;natural language processing;search engine indexing;text mining;computer science;organization;speech;world wide web;information retrieval;satisfiability	NLP	-35.0335508387082	-72.525685612371	112390
85782416f174124c3fab3cc0c9ca8e6bba64b74d	automatic text summarization in engineering information management	online customer reviews;automatic text summarization;text summarization;information management;engineering information management;ta engineering general civil engineering general	In today’s knowledge-intensive engineering environment, information management is an important and essential activity. However, existing researches of Engineering Information Management (EIM) mainly focused on numerical data such as computer models and process data. Textual data, especially the case of free texts, which constitute a significant part of engineering information, have been somewhat ignored, mainly due to their lack of structure and the noisy information contained in them. Since summarization is a process to distill important information from source documents and at the same time remove irrelevant and redundant information, it could address the obstacles for handling textual data in EIM. Moreover, text summarization could address the increasing demand to integrate information from multiple documents and reduce the time in acquiring useful information from massive textual data in the engineering domain. This paper discusses in detail the need to apply text summarization in EIM and introduces a case study in summarizing multiple online customer reviews.	automatic summarization;computer simulation;enterprise information management;level of measurement;numerical analysis;relevance;text corpus	Jiaming Zhan;Han Tong Loh;Ying Liu;Aixin Sun	2007		10.1007/978-3-540-77094-7_44	text graph;multi-document summarization;information engineering;computer science;data science;automatic summarization;data mining;information management;world wide web;information retrieval	AI	-36.458927860556045	-66.68430129654917	112411
bbdd1dee9c0537445980dc75c2772b09eb179a84	an xml representation for annotated handwriting datasets for online handwriting recognition		In this paper, we briefly descibe an XML representation for annotation of online handwriting data to support the development and evaluation of handwriting recognition algorithms, that is based on the emerging Digital Ink Markup Language (InkML) draft standard from W3C. In particular, we describe how the XML representation we have defined attempts to address issues of (i) support for different scripts, (ii) partial automation of labeling using recognition engines, (iii) planned as well as casual capture of handwriting data and (iv) semantic annotation of handwriting data at various levels such as character, word and phrase. The representation keeps the raw handwriting data (described by InkML) separate from its semantic interpretations. We also compare and contrast the XML representation with the extant UNIPEN representation for annotation of handwriting data.	algorithm;handwriting recognition;markup language;xml	Ajay S. Bhaskarabhatla;Sriganesh Madhvanath	2004			xml;inkwell;speech recognition;artificial intelligence;natural language processing;handwriting;inkml;handwriting recognition;markup language;intelligent character recognition;computer science;annotation	AI	-33.97658190682633	-75.48686326994014	113226
93bacaeb38657a9eac5c6895a9e90dd591baae96	détection d'évènements à partir de twitter	applied sciences computer science sciences appliques et technologie informatique umi 0984;taln;evenement;informatique;event;twitter;hashtags;nlp;these ou memoire numerique electronic thesis or dissertation	We present a system for finding, from Twitter data, events that raised the interest of users within a given time period and the important dates for each event. An event is represented by many terms whose frequency increases suddenly at one or more moments during the analysed period. In order to determine the terms (especially the hashtags) dealing with a topic, we propose methods to cluster similar terms: phonetic methods adapted to the writing mode used by users and some statistical methods. In order to select the set of events, we used three main criteria: frequency, variation and Tf·Idf. MOTS-CLÉS : Twitter, hashtags, évènement, similarité sémantique, DBscan.	dbscan;decision theory;hashtag	Houssem Eddine Dridi;Guy Lapalme	2013	TAL		art;art history;literature	Web+IR	-38.15003358914361	-73.79330720330819	113359
711ead6205db51aa0008da0c83c3c936bfb895f1	constructing a broad-coverage lexicon for text mining in the patent domain	intellectual property;information retrieval;text mining	For mining intellectual property texts (patents), a broad-coverage lexicon that covers general English words together with terminology from the patent domain is indispensable. The patent domain is very diffuse as it comprises a variety of technical domains (e.g. Human Necessities, Chemistry & Metallurgy and Physics in the International Patent Classification). As a result, collecting a lexicon that covers the language used in patent texts is not a straightforward task. In this paper we describe the approach that we have developed for the semi-automatic construction of a broad-coverage lexicon for classification and information retrieval in the patent domain and which combines information from multiple sources. Our contribution is twofold. First, we provide insight into the difficulties of developing lexical resources for information retrieval and text mining in the patent domain, a research and development field that is expanding quickly. Second, we create a broad coverage lexicon annotated with rich lexical information and containing both general English word forms and domain terminology for various technical domains.	information retrieval;lexicon;semiconductor industry;text mining	Nelleke Oostdijk;Suzan Verberne;Cornelis H. A. Koster	2010				NLP	-34.288266217648925	-70.85004211402367	113855
4a2c75b5e642bce1ef3b6ef717339e6d0e7c41c0	developments in the tiger annotation scheme and their realization in the corpus	noun	This paper presents the annotation of the German TIGER Treebank. First, issues concerning the annotation, representation as well as querying of the treebank are discussed. Within this context, the annotation tool ANNOTATE, the export and XML formats of the TIGER Treebank and the TIGER search tool are briefly introduced. Secondly, the developments of the TIGER annotation scheme and their realization in the corpus are introduced focussing on the differences between the underlying NEGRA annotation scheme and the further developed TIGER annotation scheme. The main differences are concerned with verb-subcategorization, coordination, appositions and parentheses as well as proper nouns. Thirdly, the annotation scheme is assessed through an evaluation and a problem discussion of the above mentioned changes. For this purpose, inter-annotator agreement in the TIGER project has been analyzed focussing on exactly these changes. This analysis shows where the annotators’ decision problems are. These difficulties are discussed in greater detail on the basis of annotation examples. The paper concludes with some suggestions for the improvement of the TIGER annotation scheme.	decision problem;inter-rater reliability;tiger team;treebank;xml	Sabine Brants;Silvia Hansen	2002			tiger;xml;natural language processing;artificial intelligence;noun;treebank;decision problem;proper noun;computer science;annotation	NLP	-37.50746044399625	-79.52356999916424	115004
bb1bf3cbf897293ff523cdd292a91b79c5920fb7	western classical music development: a statistical analysis of composers similarity, differentiation and evolution	classical composers;differentiation;evolution;imitation;influences network;similarity indices	This paper proposes a statistical analysis that captures similarities and differences between classical music composers with the eventual aim to understand why particular composers `soundu0027 different even if their `lineagesu0027 (influences network) are similar or why they `soundu0027 alike if their `lineagesu0027 are different. In order to do this we use statistical methods and measures of association or similarity (based on presence/absence of traits such as specific `ecologicalu0027 characteristics and personal musical influences) that have been developed in biosystematics, scientometrics, and bibliographic coupling. This paper also represents a first step towards a more ambitious goal of developing an evolutionary model of Western classical music.	advocate (person);algorithm;appendix;bid gene;bid protein;bibliographic coupling;bibliometrics;binary integer decimal;biologic preservation;biological evolution;biological anthropology;capture one;categories;catherine;cell differentiation process;central serous chorioretinopathy;centralisation;classical music;classification;composer;computation;conflict (psychology);congenital mesoblastic nephroma;cosine distance method;cosine similarity;database;databases;eighteen;erewhon;extraction;family tree;genealogical tree;genus (mathematics);iso 8601;index (publishing);information retrieval;inverted index;large;level of detail;license;mike lesser;models of dna evolution;numerous;phylogenetic tree;phylogenetics;quadratic function;roux-en-y anastomosis;scientometrics;trait	Patrick Georges	2017		10.1007/s11192-017-2387-x	musical;scientometrics;data mining;natural language processing;bibliographic coupling;correlation and dependence;computer science;classical music;imitation;artificial intelligence	ML	-42.52653216107252	-72.9621232691573	115208
1aa5841ec5542a866d8b0b7e75cb25d144af7f66	image mining of historical manuscripts to establish provenance	clustering;texture;classification	he recent digitization of more than twenty million books has been led by initiatives from countries wishing to preserve their cultural heritage and by commercial endeavors, such as the Google Print Library Project. Within a few years a significant fraction of the world’s books will be online. For millions of intact books and tens of millions of loose pages, the provenance of the manuscripts may be in doubt or completely unknown, thus denying historians an understanding of the context of the content. In some cases it may be possible for human experts to regain the provenance by examining linguistic, cultural and/or stylistic clues. However, such experts are rare and this investigation is clearly a time-consuming process. One technique used by experts to establish provenance is the examination of the ornate initial letters appearing in the questioned manuscript. By comparing the initial letters in the manuscript to annotated initial letters whose origin is known, the provenance can be determined. In this work we show for the first time that we can reproduce this ability with a computer algorithm. We leverage off a recently introduced technique to measure texture similarity and show that it can recognize initial letters with an accuracy that rivals or exceeds human performance. A brute force implementation of this measure would require several years to process a single large book; however, we introduce a novel lower bound that allows us to process the books in minutes.	algorithm;brute-force search;human reliability;million book project;self-similarity	Bing Hu;Thanawin Rakthanmanon;Bilson J. L. Campana;Abdullah Mueen;Eamonn J. Keogh	2012		10.1137/1.9781611972825.69	computer science;leverage (finance);data mining;digitization;provenance;cultural heritage	ML	-39.38206784536693	-69.72255045236302	115358
732ea0433c4673dd0255ffb67d2314c98bbe93b2	selection and aggregation techniques for crowdsourced semantic annotation task.		Crowdsourcing is an accessible and cost-effective alternative to traditional methods of collecting and annotating data. The application of crowdsourcing to simple tasks has been well investigated. However, complex tasks like semantic annotation transfer require workers to take simultaneous decisions on chunk segmentation and labeling while acquiring on-the-go domainspecific knowledge. The increased task complexity may generate low judgment agreement and/or poor performance. The goal of this paper is to cope with these crowdsourcing requirements with semantic priming and unsupervised quality control mechanisms. We aim at an automatic quality control that takes into account different levels of workers’ expertise and annotation task performance. We investigate the judgment selection and aggregation techniques on the task of cross-language semantic annotation transfer. We propose stochastic modeling techniques to estimate the task performance of a worker on a particular judgment with respect to the whole worker group. These estimates are used for the selection of the best judgments as well as weighted consensus-based annotation aggregation. We demonstrate that the technique is useful for increasing the quality of collected annotations.	control system;crowdsourcing;requirement;stochastic modelling (insurance);usb on-the-go	S. A. Chowdhury;Marcos Calvo Lafarga;Arindam Ghosh;Evgeny A. Stepanov;Ali Orkan Bayer;Giuseppe Riccardi;Fernando García;Emilio Sanchis Arnal	2015			bioinformatics	Web+IR	-36.86509138129865	-68.83138121175197	115552
0972e61d16876061c538028afec664f4a582ff51	issues of projectivity in the prague dependency treebank		In the present paper we discuss some issues connected with the condition of projectivity in a dependency based description of language (see Sgall, Hajičová, and Panevová (1986), Hajičová, Partee, and Sgall (1998)), with a special regard to the annotation scheme of the Prague Dependency Treebank (PDT, see Hajič (1998)). After a short Introduction (Section 1), the condition of projectivity is discussed in more detail in Section 2, presenting its formal definition and formulating an algorithm for testing this condition on a subtree (Section 2.1); the introduction of the condition of projectivity in a formal description of language is briefly substantiated in Section 2.2. and some problematic cases are discussed in Section 2.3. In Section 3, a preliminary classification into three main groups and several subgroups of Czech non-projective constructions on the analytical level is presented (Section 3.1), with illustrations of each subgroup in Section 3.2. A discussion of (surface) non-projectivities viewed from the perspectives of the underlying (tectogrammatical) structures is given in Section 4; the classification outlined in Section 4.1 reflects the types of deviations from projectivity caused by topic-focus articulation (TFA). In Section 4.2 we examine the motivation and factors of non-projective constructions. The treatment of non-projective constructions in the annotation scenario of PDT is presented in Section 5. In the Conclusion (Section 6) we summarize the results and outline some directions for further research in this domain. The present contribution is an enlarged and slightly modified version of the paper Veselá, Havelka, and Hajičová (2004). 1 Condition of projectivity The objective of the present paper is to analyze the property of projectivity, a condition formally defined by Marcus (1965) and postulated for dependency trees (see e.g., Kunze (1975); on projectivity in the tectogrammatical level of FGD, see e.g. Sgall, Hajičová, and Panevová (1986), pp. 238 ff.) in view of a complex multilevel account of language structure and, more specifically, as reflected in the multilayered annotation scenario of the Prague Dependency Treebank. The Prague Dependency Treebank is a subset of texts taken from the Czech National Corpus (CNC); each randomly chosen sample consisting of 50 sentences of a coherent text is annotated on three layers of annotation: (i) the morphemic (POS) layer with about 2000 tags for the highly inflectional Czech language; (ii) a layer of ‘analytic’ (“surface”) syntax (analytic representations, AR in the sequel): about 100,000 Czech sentences, i.e. 2000 samples of texts each consisting of 50 sentences of a continuous text have been assigned dependency tree structures; (iii) the tectogrammatical (underlying) syntactic layer: tectogrammatical tree structures (TGTSs) are assigned to a subset of the set tagged according to (ii); the current phase has resulted in 1000 samples of 50 sentences each; the TGTSs are again based on dependency syntax, and the following principles are observed: (a) only autosemantic (lexical) words have nodes of their own; function words, as far as semantically relevant, are reflected by parts of complex node labels (with the exception of coordinating conjunctions); (b) nodes are added in case of deletions on the surface level; (c) the condition of projectivity is met (i.e. no crossing of edges is allowed); (d) tectogrammatical functions (‘functors’) such as Actor/Bearer, Patient, Addressee, Origin, Effect, different kinds of Circumstantials are assigned; (e) basic features of topic-focus articulation (TFA) are introduced; (f) elementary coreference links (both grammatical and textual) are indicated. A TGTS node label consists of: (a) the lexical value of the word; (b) its ‘(morphological) grammatemes’ (i.e. the values of morphological categories); (c) its ‘functors’ (with a more subtle differentiation of syntactic relations by means of ‘syntactic grammatemes’ (e.g. ‘in’, ‘at’, ‘on’, ‘under’); (d) the attribute of Contextual Boundness (topic-focus articulation); (e) values concerning intersentential links. In Figure 1 we give a (rather simplified) illustrative example of a TGTS, which represents the preferred reading of the sentence 1.	algorithm;biconnected component;categorization;coherence (physics);contextual inquiry;decision problem;emoticon;meaning–text theory;php development tools;parsing;randomness;scott continuity;syntactic predicate;text corpus;tree (data structure);treebank	Eva Hajicová;Jirí Havelka;Petr Sgall;Katerina Vesela;Daniel Zeman	2004	Prague Bull. Math. Linguistics		artificial intelligence;linguistics;treebank;natural language processing;computer science	NLP	-37.22675600521179	-79.79808492106024	115696
90aafb4fb9b4021b6ed3b377758d744c1fc309be	beyond the concordance lotus and dbase as text analysis tools	text analysis;software package	Concordances are not the only computer tools available to literary scholars. This article looks at two general purpose software packages, Lotus 1-2-3 and dBase III. Several suggested approaches to organizing the texts are presented. Each program allows the words to have multiple levels something standard concordance programs lack. This encourages the researcher to view the text as texture rather than as words strung together linearly. Lotus is easier to use than dBase but lacks the ability to relate information between files. With programming skills, the researcher can develop complex queries in dBase.	bible concordance;concordance (publishing);lotus 1-2-3;organizing (structure);texture mapping;dbase	Louis Janus;Gregg Shadduck	1989	Computers and the Humanities	10.1007/BF02176643	natural language processing;text mining;computer science;data mining;database;world wide web	HCI	-39.931933754113516	-68.60025313054504	115837
ae1567f5bdc1aa19200b4fa802eb5d7889a69efc	urban problem lod for understanding the problem structure and detecting vicious cycles		Urban problems, such as littering, graffiti, and homelessness have various causes and are linked to each other; thus, understanding of the problem structure is required for detecting and solving root problems that generate vicious cycles of the problems. Moreover, in order to implement the action plans for solving these problems, local governments need to estimate the cost-effectiveness of the plans. Therefore, this paper proposes constructing Urban Problem Linked Open Data (UPLOD) that include urban problems' causality and the related cost information of their budget sheets. We first design an RDF schema that represents the urban problems' causality and then we extend the schema to include budget information based on RDF Data Cube Vocabulary. Next, we instantiate actual causes and effects using crowdsourcing supporting with natural language processing based techniques. In addition, we complement the UPLOD by inferring with Semantic Web Rule Language (SWRL) rules. Finally, we detect several root problems that lead to the cycle of the problems using SPARQL queries and then confirm the results with evidence manually extracted from articles of local governments.	causality;crowdsourcing;data cube;linked data;natural language processing;ontology (information science);rdf schema;sparql;semantic web rule language;sensor;source data;vocabulary	Shusaku Egami;Takahiro Kawamura;Kouji Kozaki;Akihiko Ohsuga	2018	2018 IEEE 12th International Conference on Semantic Computing (ICSC)	10.1109/ICSC.2018.00034	linked data;rdf;data mining;semantic web rule language;rdf schema;ontology (information science);sparql;schema (psychology);computer science;crowdsourcing	DB	-40.49700847331907	-70.06523897908325	115884
079af2c73e1aaf8c22c0285589f737bff0438b90	visual readability analysis: how to make your writings easier to read	journal_article;text analysis learning artificial intelligence;document processing visual readability analysis semiautomatic feature selection approach visual analysis tool visual representations visra draft version text processing;text processing;vocabulary;draft version;vocabulary correlation training data length measurement navigation visual analytics;text analysis;length measurement;visual readability analysis;feature evaluation and selection document and text processing;feature evaluation and selection;semiautomatic feature selection approach;training data;navigation;visual representations;visual analysis tool;visual representation;visual analysis;visra;feature selection;document processing;document and text processing;correlation;books comprehension computer graphics databases factual humans image processing computer assisted linguistics reading software writing;learning artificial intelligence;visual analytics	We present a tool that is specifically designed to support a writer in revising a draft version of a document. In addition to showing which paragraphs and sentences are difficult to read and understand, we assist the reader in understanding why this is the case. This requires features that are expressive predictors of readability, and are also semantically understandable. In the first part of the paper, we, therefore, discuss a semiautomatic feature selection approach that is used to choose appropriate measures from a collection of 141 candidate readability features. In the second part, we present the visual analysis tool VisRA, which allows the user to analyze the feature values across the text and within single sentences. Users can choose between different visual representations accounting for differences in the size of the documents and the availability of information about the physical and logical layout of the documents. We put special emphasis on providing as much transparency as possible to ensure that the user can purposefully improve the readability of a sentence. Several case studies are presented that show the wide range of applicability of our tool. Furthermore, an in-depth evaluation assesses the quality of the measure and investigates how well users do in revising a text with the help of the tool.	document;expressive power (computer science);feature selection;level of detail;natural language generation;spaces;paragraphs;sentence	Daniela Oelke;David Spretke;Andreas Stoffel;Daniel A. Keim	2010	2010 IEEE Symposium on Visual Analytics Science and Technology	10.1109/TVCG.2011.266	natural language processing;computer vision;training set;navigation;visual analytics;speech recognition;document processing;length measurement;computer science;data mining;correlation;information retrieval	SE	-38.55753677118143	-72.6999416384236	116332
f1463a1d4e1ae76a89db5f53830cc30fcdcd41fb	coding neuroradiology reports for the northern manhattan stroke study: a comparison of natural language processing and manual review		Automated systems using natural language processing may greatly speed chart review tasks for clinical research, but their accuracy in this setting is unknown. The objective of this study was to compare the accuracy of automated and manual coding in the data acquisition tasks of an ongoing clinical research study, the Northern Manhattan Stroke Study(NOMASS). We identified 471 neuroradiology reports of brain images used in the NOMASS study. Using both automated and manual coding, we completed a standardized NOMASS imaging form with the information contained in these reports. We then generated ROC curves for both manual and automated coding by comparing our results to the original NOMASS data, where study in investigators directly coded their interpretations of brain images. The areas under the ROC curves for both manual and automated coding were the main outcome measure. The overall predictive value of the automated system (ROC area 0.85, 95% CI 0.84-0.87) was not statistically different from the predictive value of the manual coding (ROC area 0.87, 95% CI 0.83-0.91). Measured in terms of accuracy, the automated system performed slightly worse than manual coding. The overall accuracy of the automated system was 84% (CI 83-85%). The overall accuracy of manual coding was 86% (CI 84-88%). The difference in accuracy between the two methods was small but statistically significant (P = 0.026). Errors in manual coding appeared to be due to differences between neurologists' and nueroradiologists' interpretation, different use of detailed anatomic terms, and lack of clinical information. Automated systems can use natural language processing to rapidly perform complex data acquisition tasks. Although there is a small decrease in the accuracy of the data as compared to traditional methods, automated systems may greatly expand the power of chart review in clinical research design and implementation.		Jacob S. Elkins;Carol Friedman;Bernadette Boden-Albala;Ralph L. Sacco;George Hripcsak	2000	Computers and biomedical research, an international journal	10.1006/cbmr.1999.1535		HCI	-46.503502405925055	-70.68657755416363	116515
968f1c708b8ca471c4196dc01d62ef60ad8af6e1	a methodology for reader's emotional state extraction to augment expressions in speech synthesis	document-to- audio;webpage genre identification;online newspaper;emotional state extraction;speech representation of text formatting;search engine;speech synthesis;augment expressions;important factor;information retrieval;great potential;personal homepages;emotional state extraction-annotation of documents;expressive speech synthesis;system architecture;meta data;real time;text analysis	This paper presents a methodology for the real-time extraction of readers' emotional state from documents as well as the representation of emotionally annotated documents into acoustic modality. Using the Self Assessment Manikin Test we performed preliminary experiments on the extraction of Pleasure - Arousal - Dominance (P.A.D.) annotation rules that the documents evoke to the readers. The rules are used in an automated procedure that assigns text's formatting and structure of documents (meta-data) to emotional state values. During the vocalization of documents, these values, and consequently the documents' meta-data, are carried by different expressions of speech using text-to-speech synthesis. The proposed system architecture is language independent and content-free.	acoustic cryptanalysis;document;experiment;factorization of polynomials;modality (human–computer interaction);real-time computing;real-time transcription;speech synthesis;systems architecture	Dimitrios Tsonos;Gerasimos Xydas;Georgios Kouroupetroglou	2007	19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)	10.1109/ICTAI.2007.127	natural language processing;speech recognition;computer science;artificial intelligence;metadata;speech synthesis	Robotics	-35.12895913642371	-72.59534371712193	116616
de3a315044b78210ac2be2e583f02d020f5cee68	visualizing endangered indigenous languages of french polynesia with lexus	visualization natural languages documentation dictionaries speech cultural differences multimedia databases video recording psychology global communication;natural language interfaces;global communication;multimedia lexicon;document handling;language use;documentation of endangered languages project;semantic network knowledge base;language documentation;documentation of endangered languages endangered indigenous language visualization french polynesia lexus web based lexicon tool documentation of endangered languages project digital multimedia encyclopedic lexicon marquesan tuamotuan language language documentation semantic network knowledge base speech user interface collaborative workspace functionality;user interface;endangered languages language documentation multimedia lexicon;encyclopaedias;semantic network;audio video;endangered languages;speech;endangered indigenous language visualization;natural languages;psychology;speech user interface;marquesan tuamotuan language;multimedia computing;data visualisation;visualization;dictionaries;multimedia databases;video recording;visual language;french polynesia;digital multimedia encyclopedic lexicon;speech communication;documentation of endangered languages;collaborative workspace functionality;lexus web based lexicon tool;natural languages data visualisation document handling encyclopaedias linguistics multimedia computing natural language interfaces;documentation;cultural differences;knowledge base;linguistics	This paper reports on the first results of the DOBES project 'Towards a multimedia dictionary of the Marquesan and Tuamotuan languages of French Polynesia'. Within the framework of this project we are building a digital multimedia encyclopedic lexicon of the endangered Marquesan and Tuamotuan languages using a new tool, LEXUS. LEXUS is a Web-based lexicon tool, targeted at linguists involved in language documentation. LEXUS offers the possibility to visualize language. It provides functionalities to include audio, video and still images to the lexical entries of the dictionary, as well as relational linking for the creation of a semantic network knowledge base. Further activities aim at the development of (1) an improved user interface in close cooperation with the speech community and (2) a collaborative workspace functionality which will allow the speech community to actively participate in the creation of lexica.	dictionary;directed graph;documentation;entity;google summer of code;ibm shoebox;knowledge base;knowledge level;lexicon;multi-user;semantic network;user interface;workspace	Gaby Cablitz;Jacquelijn Ringersma;Marc Kemps-Snijders	2007	2007 11th International Conference Information Visualization (IV '07)	10.1109/IV.2007.134	natural language processing;computer science;linguistics;programming language	DB	-35.152563022188424	-75.29276068478138	116758
19629ea1f85cf59adc189169b6b5b534c7e3125c	argumentation mining: how can a machine acquire common sense and world knowledge?		Argumentation mining is an advanced form of human language understanding by the machine. This is a challenging task for a machine. When sufficient explicit discourse markers are present in the language utterances, the argumentation can be interpreted by the machine with an acceptable degree of accuracy. However, in many real settings, the mining task is difficult due to the lack or ambiguity of the discourse markers, and the fact that a substantial amount of knowledge needed for the correct recognition of the argumentation, its composing elements and their relationships is not explicitly present in the text, but makes up the background knowledge that humans possess when interpreting language. In this article1 we focus on how the machine can automatically acquire the needed common sense and world knowledge. As very few research has been done in this respect, many of the ideas proposed in this article are tentative, but start being researched. We give an overview of the latest methods for human language understanding that map language to a formal knowledge representation that facilitates other tasks (for instance, a representation that is used to visualize the argumentation or that is easily shared in a decision or argumentation support system). Most current systems are trained on texts that are manually annotated. Then we go deeper into the new field of representation learning that nowadays is very much studied in computational linguistics. This field investigates methods for representing language as statistical concepts or as vectors, allowing straightforward methods of compositionality. The methods often use deep learning and its underlying neural network technologies to learn concepts from large text collections in an unsupervised way (i.e., without the need for manual annotations). We show how these methods can help the argumentation mining process, but also demonstrate that these methods need further research to automatically acquire the necessary background knowledge and more specifically common sense and world knowledge. We propose a number of ways to improve the learning of common sense and world knowledge by exploiting textual and visual data, and touch upon how we can integrate the learned knowledge in the argumentation mining process.		Marie-Francine Moens	2018	Argument & Computation	10.3233/AAC-170025	natural language processing;argumentation theory;natural language understanding;common sense;artificial intelligence;feature learning;computer science	NLP	-33.70883083873768	-80.04662944803106	116904
beeec64ae69ac30b06974399f190f230ca691255	summarization of multimodal information		Information Summarization is one of the key challenges for current and future information systems. In this paper, we will outline a system that comprises modules for summarizing texts and time series to study the link between the two. Summaries of texts are generated using a lexical analysis of cohesion in texts focusing on key sentences that provide cohesion: by implication, these are the sentences that comprise chief points of a given text. Time series summarization is accomplished using the so-called wavelet analysis to separate out the trend, cyclical fluctuations and autocorrelational effects and generating verbal signals to describe each phenomenon. Finally, we present a case study performed on the UK financial market with regards to multimodal information processing, namely textual and numerical summarization.	automatic summarization;information processing;information system;lexical analysis;multimodal interaction;numerical analysis;time series;wavelet	Saif Ahmad;Paulo C. F. de Oliveira;Khurshid Ahmad	2004			lexical analysis;wavelet;natural language processing;automatic summarization;artificial intelligence;information processing;computer science;information system;phenomenon	NLP	-37.88434197728725	-75.36862234501326	117133
11876680b4c1008e0f46d4dc1cab190be7e2ae1a	represented indicator measurement and corpus distillation on focus species detection	mice;iterative corpus distillation method;name disambiguation;information extraction;corpus distillation;focus species detection;training;interspecies gene name normalization;skeleton;genetics;iterative methods;represented indicator measurement;proteins;medical information systems;domain specific entities;molecular biophysics;iterative corpus distillation method represented indicator measurement corpus distillation focus species detection information extraction biomedical literature name disambiguation domain specific entities proteins interspecies gene name normalization;proteins genetics iterative methods medical information systems molecular biophysics;humans;training humans correlation proteins muscles mice skeleton;correlation;domain specificity;biomedical literature;muscles	In extraction of information from the biomedical literature, name disambiguation of domain-specific entities, such as proteins, is one of the most important issues. The entity ambiguity with the highest dimension is the species to which an entity is associated with. Furthermore, one of the bottlenecks in inter-species gene name normalization is species disambiguation. To enhance the performance of species disambiguation, the detection of focus species detection remains a substantial challenge. This study presents a method addressing this issue. The results present evaluations of all articles from the BioCreaTive I&II GN task. Our method is robust for all types of articles, particularly those without explicit species entity information. Since our method requires a training corpus to be the indicator vector, we developed an iterative corpus distillation method to extend the corpus. In the conducted experiments, the proposed method achieved a high accuracy of 85.64% and 84.32% without species entity information.	biocreative;entity;experiment;grid north;information extraction;iterative method;robustness (computer science);word-sense disambiguation	Chih-Hsuan Wei;Hung-Yu Kao	2010	2010 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2010.5706647	natural language processing;computer science;bioinformatics;machine learning;data mining;iterative method;skeleton;genetics;correlation;information extraction;molecular biophysics	Robotics	-35.73245782300657	-69.55394014982541	118087
d506e1a019cfb672450e0567198485e2c398ef97	mt quality estimation for computer-assisted translation: does it really help?		The usefulness of translation quality estimation (QE) to increase productivity in a computer-assisted translation (CAT) framework is a widely held assumption (Specia, 2011; Huang et al., 2014). So far, however, the validity of this assumption has not been yet demonstrated through sound evaluations in realistic settings. To this aim, we report on an evaluation involving professional translators operating with a CAT tool in controlled but natural conditions. Contrastive experiments are carried out by measuring post-editing time differences when: i) translation suggestions are presented together with binary quality estimates, and ii) the same suggestions are presented without quality indicators. Translators’ productivity in the two conditions is analysed in a principled way, accounting for the main factors (e.g. differences in translators’ behaviour, quality of the suggestions) that directly impact on time measurements. While the general assumption about the usefulness of QE is verified, significance testing results reveal that real productivity gains can be observed only under specific conditions.	computer-assisted translation;experiment;marginal model;polyethylene terephthalate;postediting;quadratic equation;query expansion;spatial variability	Marco Turchi;Matteo Negri;Marcello Federico	2015			artificial intelligence;natural language processing;computer-assisted translation;computer science	SE	-45.577250371907226	-72.00528716617646	118293
6d91f71f1e9019075241976136c71197264eae19	surprisal and satisfaction: towards an information-theoretic characterization of presuppositions with a diachronic application		The paper offers a pilot study concerned with presuppositions in historical data, which are identified and annotated on the basis of six triggers, viz. three for additives, and three for factives. It brings together information extraction and annotation on (A) the satisfaction/binding and (B) information-theoretic surprisal values of presuppositions. An initial (naive) hypothesis is that the two lines of investigation converge, but this only turned out to be the case for factives in the data inspected. The work conducted relates two strands of research: information theory (Shannon 1948, Fankhauser et al. 2014, DegaetanoOrtlieb et al. 2016) and the semantic theory of presuppositions (Stalnaker 1973, Heim 1983, Schwarz 2014, 2016). Furthermore, the study begins to connect two methodological points relevant for studies concerned with the diachronic evolution of meaning and structure but not approached jointly so far: syntactically parsed data and information-theoretically calculated predictors on semantic phenomena. Using such tools, the paper offers an initial description, a discussion of methodological issues, and some empirical results such as the existence of two crystallizing major classes of triggers during the Early Modern English period, which may be indicative of the distinction between informative and run-of-the-mill presuppositions. While the focus of the paper is on the early modern period (that is, roughly, the sixteenth and the seventeenth century), a short outlook on Late Modern English (the subsequent two centuries) is offered.	converge;information extraction;information theory;microsoft outlook for mac;parsing;self-information;shannon (unit);viz: the computer game	Remus Gergel;Martin Kopf-Giammanco;Julia Masloh	2017			linguistics;political science;presupposition	NLP	-36.97511608443418	-72.7419150473412	118772
0de4dcca2dfe43c8218e27426d5ce210eda2a655	making the voiceweb smarter - integrating intelligent component technologies and voicexml	natural language interfaces;hypermedia markup languages;information resources;speech synthesis automatic speech recognition natural languages application software speech recognition telephony switches natural language processing artificial intelligence network servers;ai research voiceweb intelligent component technologies voicexml voice interfaces commercial applications speech processing voice browser voice enabled web applications internet speech recognition natural language processing;component technology;natural language interfaces internet information resources hypermedia markup languages speech based user interfaces speech recognition;speech based user interfaces;natural language understanding;internet;natural language generation;speech recognition;natural language interface;natural language processing;web development	VoiceXML offers the prospect of a streamlined deployment process of voice interfaces for commercial applications, similar to the ease of Web development. This in turn promises increased dynamism in the field of speech and language research. We investigate opportunities and constraints for the integration of intelligent component technologies into VoiceXML-based systems: such components will solve tasks from both sides of natural language processing, natural language understanding/analysis as well as natural language generation. We ask what role these fields originating from AI research will play in the VoiceXML environment. For a more detailed report including also the issues of multilinguality and natural language interfaces see (Mittendorfer et al., 2001).		Markus Mittendorfer;Georg Niklfeld;Werner Winiwarter	2001		10.1109/WISE.2001.996736	natural language processing;natural language programming;web development;the internet;speech recognition;universal networking language;question answering;natural language user interface;computer science;multimedia;language technology;information extraction	Robotics	-41.30467687376564	-69.18754317706238	119164
fcc4ea12c9ef66ab8fc79d014ca5009555b027ea	browsing the chinese web pages using mandarin speech	web pages;keyword spotting;www browser;speech user interface;dynamic keyword lexicon	A speech interface that allows easy access to information on the WWW has the potential to make the browser more friendly and powerful. This paper, thus, presents a working Mandarin speech interface for using unconstrained Mandarin speech to control the WWW browser for conveniently browsing the Chinese Web pages. The interface currently provides speakable commands, bookmarks, and links. The experimental results show that our approach which specially consider the characteristics of the Chinese language is very effective, and very high accuracy can be achieved.	browsing;super robot monkey team hyperforce go!;web page	Hsin-Min Wang;Yu-Hsueh Chou;Berlin Chen	2000	Int. J. Comput. Proc. Oriental Lang.	10.1142/S0219427900000090	natural language processing;speech recognition;computer science;web page;world wide web	NLP	-35.29092768670352	-74.90257339943929	119660
3f325ffcae610ddc806cfa0d2cf6dcf109c6f4b5	using machine learning for extracting information from natural disaster news reports	databases;information extraction;bases de datos;aprendizaje automatico;natural disasters;machine learning;extraccion de informacion;desastres naturales;clasificacion tematica de textos;text categorization	The disasters caused by natural phenomena have been present all along human history; nevertheless, their consequences are greater each time. This tendency will not be reverted in the coming years; on the contrary, it is expected that natural phenomena will increase in number and intensity due to the global warming. Because of this situation it is of great interest to have sufficient data related to natural disasters, since these data are absolutely necessary to analyze their impact as well as to establish links between their occurrence and their effects. In accordance to this necessity, in this paper we describe a system based on Machine Learning methods that improves the acquisition of natural disaster data. This system automatically populates a natural disaster database by extracting information from online news reports. In particular, it allows extracting information about five different types of natural disasters: hurricanes, earthquakes, forest fires, inundations, and droughts. Experimental results on a collection of Spanish news show the effectiveness of the proposed system for detecting relevant documents about natural disasters (reaching an F-measure of 98%), as well as for extracting relevant facts to be inserted into a given database (reaching an F-measure of 76%).	binary classification;international standard serial number;machine learning;microsoft windows 98;natural language processing;population;relevance;sensor;test set	Alberto Téllez-Valero;Manuel Montes-y-Gómez;Luis Villaseñor Pineda	2009	Computación y Sistemas		geography;data mining;cartography	ML	-34.2522305739097	-72.36610388121463	119690
e5ad5703c81aa924a395500be16fc59133e24bbb	who wrote bacon? assessing the respective roles of francis bacon and his secretaries in the production of his english works	authorship;wordprint;thomas hobbes;francis bacon	In an earlier study that identified previously unrecognized writings of the young 15 Thomas Hobbes, questions were raised about the authorship of some of Francis Bacon's published works. This article reports a follow-up study in which two independent statistical analyses of Bacon's English works both conclude that, whereas Bacon's autographic writings show clearly that they are authored by the same person; almost none of his published works can be matched statistically 20 with the autographs. The most likely explanation for this dramatic finding is that Bacon's well-known reliance on secretaries may have been sufficiently extensive that his writing patterns are obscured or replaced by theirs. This finding suggests a far simpler explanation for a wide array of anomalies in Bacon's works than others have offered. The study further identifies some of Bacon's works written 25 during a period when Thomas Hobbes was his secretary, which match Hobbes's writing pattern.	autograph;bacon's cipher;francis	Noel B. Reynolds;G. Bruce Schaalje;John L. Hilton	2012	LLC	10.1093/llc/fqs020	baconian theory of shakespeare authorship;philosophy;baconian method	NLP	-41.71953002055001	-66.19617244441615	120371
287740468faa3057b4c018c8f75d623e0a67a4c1	lexus 3 - a collaborative environment for multimedia lexica	lexicon tool mutimedia visualization endangered languages collaborative work space		lexicon	Shakila Shayan;André Moreira;Menzo Windhouwer;Alexander Koenig;Sebastian Drude	2013			natural language processing;computer science;multimedia;world wide web	DB	-34.95587896164748	-75.35890654078823	120530
267918dccc16efc20f1612f661e9b4c6ea0a9815	a corpus-based study on mapping principles of metaphors in politics		This study proposes a corpus-based method to generate Mapping Principle of metaphors. In particular, Ahrens's (2002) Mapping Principle in the Conceptual Mapping Model (CM model) is simply based on the native speakers' intuition instead of analyzing it from huge linguistic data. In order to provide more convincing evidence to support the CM model, we adopt the corpus method to extract out the metaphorical expressions in politics from the Academic Sinica Balanced Corpus. We analyze the correspondences existing within the source-target domain pairings and generate Mapping Principle based on the salient meanings in these linguistic expressions. We adopt this method to examine the mapping principles of five metaphors: POLITICS IS BUILDING, POLITICS IS A JOURNEY, POLITICS IS A PLAY, POLITICS IS A COMPETITION and POLITICS IS SPORT. This corpus-based method can provide a more convincing way to generate Mapping Principle at the linguistic level than the original one (Ahrens 2002). 2	play;text corpus	Shu-Ping Gong	2003			salient;social science;natural language processing;politics;artificial intelligence;expression (mathematics);political science;intuition	NLP	-36.83628516703002	-74.24810269860505	120657
3074e1e87aafe5024da7176fb7fb89459796419e	multilingual mapping reconciliation between english-french biomedical ontologies	biomedical ontologies;linked data;bioportal;multilingual mapping;semantic web;mapping reconciliation;ontology repository;ontology localization;ontology alignment	Even if multilingual ontologies are now more common, for historical reasons, in the biomedical domain, many ontologies or terminologies have been translated from one natural language to another resulting in two potentially aligned ontologies but with their own specificity (e.g., format, developers, and versions). Most often, there is no formal representation of the translation links between translated ontologies and original ones and those mappings are not formally available as linked data. However, these mappings are very important for the interoperability and the integration of multilingual biomedical data. In this paper, we propose an approach to represent translation mappings between ontologies based on the NCBO BioPortal format. We have reconciled more than 228K mappings between ten English ontologies hosted on NCBO BioPortal and their French translations. Then, we have stored both the translated ontologies and mappings on a French customized version of the platform, called the SIFR BioPortal, making the whole thing available in RDF. Reconciling the mappings turned more complex than expected because the translations are rarely exactly the same than the original ontologies as discussed in this paper.	interoperability;linked data;national center for biomedical ontology;natural language;ontology (information science);resource description framework;scalable inman flash replacement;sensitivity and specificity	Amina Annane;Vincent Emonet;Faiçal Azouaou;Clement Jonquet	2016		10.1145/2912845.2912847	natural language processing;idef5;open biomedical ontologies;ontology components;computer science;data mining;information retrieval	Web+IR	-34.81900277929154	-70.6751580157905	120764
ca9bf5d928bcd3f8af9add856ecc4c135388be80	multiplicity and uncertainty: media coverage of autism causation		Employing the machine learning method, this study analyzes 6,504 articles from four major newspapers, New York Times, Washington Post, USA Today, and The Guardian, to examine how media cover the topic about causes of autism. A total of 14,305 causal sentences on the topic are extracted from media articles and subjected to analysis of causal entities and descriptions. Results show media have presented multiple factors (e.g. vaccination, genetics, and parenting) pertaining to the causes of autism, as well as multiple symptoms of autism. Most of those causal relationships are presented in a tentative or uncertain manner. The study also reveals significant differences in reportage of autism causation across time and media channels.	causal filter;causality;entity;machine learning;the new york times	Yujia Zhai;Shaojing Sun;Fang Wang;Ying Ding	2017	J. Informetrics	10.1016/j.joi.2017.07.005	data mining;causes of autism;autism;social psychology;newspaper;causation;computer science	Web+IR	-45.13113065294994	-73.98356377682748	121833
412d5a076cb54a8a872416a86453d282083ab677	chener: chemical named entity recognizer	software;databases chemical;article;information storage and retrieval	MOTIVATION Chemical named entity recognition is used to automatically identify mentions to chemical compounds in text and is the basis for more elaborate information extraction. However, only a small number of applications are freely available to identify such mentions. Particularly challenging and useful is the identification of International Union of Pure and Applied Chemistry (IUPAC) chemical compounds, which due to the complex morphology of IUPAC names requires more advanced techniques than that of brand names.   RESULTS We present CheNER, a tool for automated identification of systematic IUPAC chemical mentions. We evaluated different systems using an established literature corpus to show that CheNER has a superior performance in identifying IUPAC names specifically, and that it makes better use of computational resources.   AVAILABILITY AND IMPLEMENTATION http://metres.udl.cat/index.php/9-download/4-chener, http://chener.bioinfo.cnio.es/	body of uterus;chemicals;computational resource;finite-state machine;galaxy morphological classification;information extraction;named-entity recognition;proprietary name;text corpus	Anabel Usie;Rui Alves;Francesc Solsona;Miguel Vazquez;Alfonso Valencia	2014		10.1093/bioinformatics/btt639	natural language processing;computer science;data mining;information retrieval	NLP	-35.09506712844296	-69.81849799846835	123663
36888e41c2cc2091747ca13da998812ad0e4564f	visualization of language relations and families: multitree		MultiTree is an NFS-funded project collecting scholarly hypotheses about language relationships, and visualizing them on a web site in the form of trees or graphs. Two open online interfaces allow scholars, students, and the general public an easy access to search for language information or comparisons of competing hypotheses. One objective of the project was to facilitate research in historical linguistics. MultiTree has evolved to a much more powerful tool, it is not just a simple repository of scholarly information. In this paper we present the MultiTree interfaces and the impact of the project beyond the field of historical linguistics, including, among others, the use of standardized ISO language codes, and creating an interconnected database of language and dialect names, codes, publications, and authors. Further, we offer the dissemination of linguistic findings world-wide to both scholars and the general public, thus boosting the collaboration and accelerating the scientific exchange. We discuss also the ways MultiTree will develop beyond the time of the duration of the funding.	accessibility;graph (discrete mathematics);iso 639;language code	Damir Cavar;Malgorzata Cavar	2014			natural language processing;linguistics;programming language	NLP	-34.79609969706853	-73.78737317175045	124164
bbfd7924eb815a6a844b2cb18eddf8c545aae3bc	the bavarian archive for speech signals: resources for the speech community.	non profit organization;very large database;focal point;speech communication	This paper gives an overview of the activities at the Bavarian Archive of Speech Signals (BAS) that was founded as a non-pro t organization in 1995. The main purpose of BAS is the development of a Complete Phonetic Theory (CPT) of German based on the empirical exploitation of very large databases of spoken German. However, on our way to that goal BAS will act as a focal point for all computer readable speech resources in the German language and distribute these resources to the speech community. These resources are intended to cover the speech part of the German language, i.e. speech data, labeling and segmentations, knowledge about pronunciation. In the following we give a concise overview of what resources are presently available at BAS, how they were produced, how they can be obtained from BAS, how we use these resources in various scienti c activities and a brief summary of ongoing projects.	archive;broadcast auxiliary service;cpt (file format);database;focal (programming language);human-readable medium;machine-readable medium;sequence labeling	Florian Schiel;Christoph Draxler;Hans G. Tillmann	1997			natural language processing;speech recognition;computer science;communication;speech analytics	NLP	-33.98895453169808	-76.40104667490279	125857
974a7e644b63e7e95a99f3d8020b909e37298895	multilingual input system for the web - an open multimedia approach of keyboard and handwriting recognition for chinese and japanese	handwriting recognition;multilingual information retrieval;information extraction;neural nets;building block;neural nets character sets handwriting recognition user interfaces internet character recognition;conference paper;internet;pen multilingual input system web open multimedia keyboard handwriting recognition chinese japanese multilingual information retrieval system input methods java neural network mouse;multimedia systems keyboards java mice handwriting recognition information systems natural languages internet dictionaries communication standards;artificial intelligence;character sets;user interfaces;character recognition;neural network	The basic building block of a multilingual information retrieval system is the input system. Chinese and Japanese characters pose great challenges for the conventional 101 -key alphabet-based keyboard, because they are radical-based and number in the thousands. This paper reviews the development of various approaches and then presents a framework and working demonstrations of Chinese and Japanese input methods implemented in Java, which allow open deployment over the web to any platform, The demo includes both popular keyboard input methods and neural network handwriting recognition using a mouse or pen. This framework is able to accommodate future extension to other input mediums and languages of interest.	artificial neural network;handwriting recognition;information retrieval;input method;japanese input methods;java;plover;software deployment;world wide web	Marshall Ramsey;Thian-Huat Ong;Hsinchun Chen	1998		10.1109/ADL.1998.670394	natural language processing;speech recognition;intelligent character recognition;computer science;multimedia;handwriting recognition	HCI	-41.27815313944066	-69.18315451892092	126227
5d1e2cadec743c1b71764790f0220a172b2910a4	decoding the representation of code in the brain: an fmri study of code review and expertise		Subjective judgments in software engineering tasks are of critical importance but can be difficult to study with conventional means. Medical imaging techniques hold the promise of relating cognition to physical activities and brain structures. In a controlled experiment involving 29 participants, we examine code comprehension, code review and prose review using functional magnetic resonance imaging. We find that the neural representations of programming languages vs. natural languages are distinct. We can classify which task a participant is undertaking based solely on brain activity (balanced accuracy 79%, p	cognition;electroencephalography;medical imaging;natural language;programming language;resonance;software engineering	Benjamin Floyd;Tyler Santander;Westley Weimer	2017	2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)		natural language processing;medical imaging;computer science;artificial intelligence	SE	-43.919695454357225	-75.0395278561892	126572
8255f9a547568d4d73d053554825749cd6d70480	automatic analysis of author judgment in scientific articles based on semantic annotation	semantic annotation;judgment;bepress selected works;semantic annotation text mining scientific publication citation analysis;container index;indexation;bibliometrics;scientific research;text segmentation	In this paper we describe how the annotation methodology adopted in our approach allows us to explain the organization of indexed references in scientific research articles. We identify the semantic values of author judgments in the text segments containing indexed references. We use an automated semantic annotation platform to annotate our corpora. Exploiting this result, we obtain a representation of the annotation distribution on different scales. Finally, we present two evaluations of	indexed grammar;memory segmentation;text corpus	Marc Bertin;Iana Atanassova;Jean-Pierre Desclés	2009			text segmentation;judgment;scientific method;bibliometrics;image retrieval;computer science;data science;data mining;temporal annotation;information retrieval	NLP	-34.048271433891145	-69.83490176812671	126759
f2bd4ae77fe255f12b3be5e87052ae54345199c2	aplenty: annotation tool for creating high-quality datasets using active and proactive learning			proactive learning	Minh-Quoc Nghiem;Sophia Ananiadou	2018			artificial intelligence;natural language processing;computer science;proactive learning;annotation	NLP	-34.78707083964584	-75.80071557458456	126889
1c0f89a004628a20a273dfa496fdd5c3df329d5b	a hybrid approach to faceted classification based on analysis of descriptor suffixes	construction process;affixes;faceted classification;facet analysis;automatic taxonomy generation;ic index languages processes and schemes;hq web pages;ia cataloging bibliographic control;hybrid approach;ll automated language processing;terms;organizational structure	ions field, discipline, science art, process, or science of measuring -metry psychometry abstractions field, discipline, science fields, subjects -graphy geographyions field, discipline, science fields, subjects -graphy geography abstractions ways, courses, directions, manners -way highwayions ways, courses, directions, manners -way highway actions, processes -ence an emergence actions, processes actions -ing a blessing actions, processes processes results of an action -ion a rebellion actions, processes processes results of an action -ment the recruitment characteristics -ive coordinative characteristics an action -ative talkative characteristics full of, abounding in, having -ous glamorous characteristics relating to degrees -imum pessimum entities chemicals, chemical compounds -dehyde acetaldehyde entities chemicals, chemical compounds chemical compounds or complex anions derived from a specified compound or element -ylate carboxylate entities chemicals, chemical compounds salt or ester of a carboxylic acid -oate octanoate entities performing an action -or editor entities places towns, cities -boro hillsboro entities places towns, cities -burgh pittsburgh entities proper nouns companies or organizations -co arco entities things books -book handbook entities things devices for the manipulation of subatomic particles -tron cyclotron entities things drawings, writings, records -gram spectrogram states, qualities, conditions -ment amazement	book;context awareness;emergence;entity;faceted classification;heuristic (computer science);lexicon;semiconductor industry;spectrogram;subatomic particle;towns;tron;vocabulary	Aaron Loehrlein;Elin K. Jacob;Kiduk Yang;Seungmin Lee;Ning Yu	2005		10.1002/meet.14504201204	natural language processing;organizational structure;computer science;data science;data mining;world wide web;information retrieval	ML	-37.049001945196586	-71.39706357734455	126985
3b3146b8dbe38407bbde77d98bacaa45b0e907bc	compressed pattern matching for sequitur	decompression;program;data compression;software libraries;phrase hierarchy;compressed pattern matching;text analysis;size measurement;ordinal search compressed pattern matching sequitur program phrase hierarchy input text semi structured text compression compressed pattern matching algorithm decompression;semi structured text compression;sequitur;ordinal search;pattern matching;compressed pattern matching algorithm;informatics;pattern matching informatics software libraries data compression size measurement;input text;text analysis data compression pattern matching	Sequitur due to Nevill-Manning and Witten. [18] is a powerful program to infer a phrase hierarchy from the input text, that also provides extremely effective compression of large quantities of semi-structured text [17]. In this paper, we address the problem of searching in Sequitur compressed text directly. We show a compressed pattern matching algorithm that finds a pattern in compressed text without explicit decompression. We show that our algorithm is approximately 1.27 times faster than a decompression followed by an ordinal search.	compressed pattern matching;data compression;ordinal data;semiconductor industry;sequitur algorithm;structured text	Shuichi Mitarai;Masahiro Hirao;Tetsuya Matsumoto;Ayumi Shinohara;Masayuki Takeda;Setsuo Arikawa	2001		10.1109/DCC.2001.917178	data compression;text mining;computer science;pattern matching;pattern recognition;data mining;informatics;information retrieval;statistics	ML	-36.22243295818585	-66.4175171304119	127427
a6f3b180a91ef14cd7eb9f4b1d8da99f614fd553	soft search and stepwise selection: a new strategy for loinc mapping		AbstractrnThis article briefly describes the soft search and stepwise selection strategy used for Loinc mapping.	stepwise regression;top-down and bottom-up design	Xiaoping Zhang	1998			information retrieval;data mining;loinc;stepwise regression;computer science;text mining	NLP	-34.99662798171441	-68.0857324288182	127456
b8aaf32a360a49b9d412c73b98570303325f3393	data mining of text as a tool in authorship attribution	databases;test hypothese;tratamiento datos;text;base donnee;analisis datos;text mining;mining;test hipotesis;database;base dato;data processing;traitement donnee;texte;data mining;word order;data analysis;document database;fouille donnee;analyse donnee;texto;hypothesis test	It is common that text documents are characterized and classified by keywords that the authors use to give them. Visa et al. have developed a new methodology based on prototype matching. The prototype is an interesting document or a part of an extracted, interesting text. This prototype is matched with the document database of the monitored document flow. The new methodology is capable of extracting the meaning of the document in a certain degree. Our claim is that the new methodology is also capable of authenticating the authorship. To verify this claim two tests were designed. The test hypothesis was that the words and the word order in the sentences could authenticate the author. In the first test three authors were selected. The selected authors were William Shakespeare, Edgar Allan Poe, and George Bernard Shaw. Three texts from each author were examined. Every text was one by one used as a prototype. The two nearest matches with the prototype were noted. The second test uses the Reuters-21578 financial news database. A group of 25 short financial news reports from five different authors are examined. Our new methodology and the interesting results from the two tests are reported in this paper. In the first test, for Shakespeare and for Poe all cases were successful. For Shaw one text was confused with Poe. In the second test the Reuters-21578 financial news were identified by the author relatively well. The resolution is that our text mining methodology seems to be capable of authorship attribution.© (2001) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	data mining;stylometry	Ari Visa;Jarmo Toivonen;Sami Autio;Jarno Mäkinen;Barbro Back;Hannu Vanharanta	2001		10.1117/12.421068	word order;statistical hypothesis testing;text mining;mining;data processing;telecommunications;computer science;data mining;data analysis;world wide web;information retrieval	ML	-37.514141168224356	-66.4175589950544	128149
3b748de5bdca9a4fd09ef97cba1ff8333ddcba2a	extraction d'information non supervisée à partir de textes - extraction et regroupement de relations entre entités. (unsupervised information extraction from text - extraction and clustering of relations between entities)			entity;information extraction	Wei Wang	2013				NLP	-34.13890504739949	-72.24315374936069	129319
d1f7dd1f52453babb7a0fe6c542957d5e7e4b991	semantics-enhanced task-oriented dialogue translation: a case study on hotel booking		We showcase TODAY, a semanticsenhanced task-oriented dialogue translation system, whose novelties are: (i) taskoriented named entity (NE) definition and a hybrid strategy for NE recognition and translation; and (ii) a novel grounded semantic method for dialogue understanding and task-order management. TODAY is a case-study demo which can efficiently and accurately assist customers and agents in different languages to reach an agreement in a dialogue for the hotel booking.	game demo;machine translation;order management system	Longyue Wang;Jinhua Du;Liangyou Li;Zhaopeng Tu;Andy Way;Qun Liu	2017			artificial intelligence;natural language processing;computer science;semantics	NLP	-34.050019340685495	-76.0838362860915	129462
6788be77a07e8e90c8eacc02ea126d670564f431	a textual approach for the analysis of health practice guidelines		NLP is mainly focused on words and sentences even if most people agree that a better understanding of text structure could help extracting knowledge. In this article we show that, in certain domains, textual approaches are relevant for NLP, taking as an example a specific task related to the medical domain : the automatic modelling of health practice guidelines. Our system, GemFrame, is capable of automatically structuring Health Practice Guidelines. We propose a strategy based on the recognition of linguistic features. The system has been validated on three complementary aspects : usefulness, performances and relevance of the method. MOTS-CLÉS : analyse discursive, linguistique textuelle, modélisation des connaissances, Guides de Bonnes Pratiques Médicales.	bibliothèque de l'école des chartes;natural language processing;performance;relevance	Amanda Bouffier	2009	TAL		management science;knowledge management;medicine	NLP	-34.07534203857579	-73.50628276670597	129847
2b6b2775d5c3ddb14b25b5cce9881f7c65a15f20	mapping the umls semantic network into general ontologies	animals;vocabulary controlled;semantic network;semantics;fever;unified medical language system;humans	In this study, we analyzed the compatibility between an ontology of the biomedical domain (the UMLS Semantic Network) and two other ontologies: the Upper Cyc Ontology (UCO) and WordNet. 1) We manually mapped UMLS Semantic Types to UCO. One fifth of the UMLS Semantic Types had exact mapping to UCO types. UCO provides generic concepts and a structure that relies on a larger number of categories, despite its lack of depth in the biomedical domain. 2) We compared semantic classes in the UMLS and WordNet. 2% of the UMLS concepts from the Health Disorder class were present in WordNet, and compatibility between classes was 48%. WordNet, as a general language-oriented ontology is a source of lay knowledge, particularly important for consumer health applications.	categories;class;cyc;generic drugs;large;medical device incompatibility problem;ontology (information science);semantic network;unified medical language system;wordnet	Anita Burgun-Parenthoine;Olivier Bodenreider	2001	Proceedings. AMIA Symposium		natural language processing;computer science;database;information retrieval	AI	-35.0219934276659	-69.40332697239096	130580
656e0feed4e1d78d83700b8e707b1d3b416a54dc	supervised wrapper generation with lixto	visual interfaces	We illustrate basic features of the Lixto wrapper generator such as the user and system interaction, the capacious visual interface, the marking and selecting procedures, and the extraction tasks by describing the construction of a simple example program in the current Lixto prototype.	item unique identification;prototype	Robert Baumgartner;Sergio Flesca;Georg Gottlob	2001			computer science;machine learning;data mining;world wide web	HCI	-35.99790132769794	-74.51364622900365	131000
169e08c9340bb29478f2720f25835ff63eb6b22f	textual analysis for studying chinese historical documents and literary novels	textual analysis;history of concepts;name disambiguation;text mining;geographical analysis;keyword collocation;228 incident in taiwan;named entity recognition;digital humanities;temporal analysis;transliterated words in chinese historical documents;computational linguistics	We analyzed historical and literary documents in Chinese to gain insights into research issues, and overview our studies which utilized four different sources of text materials in this paper. We investigated the history of concepts and transliterated words in China with the Database for the Study of Modern China Thought and Literature, which contains historical documents about China between 1830 and 1930. We also attempted to disambiguate names that were shared by multiple government officers who served between 618 and 1912 and were recorded in Chinese local gazetteers (地方志 /di4 fang1 zhi4/). To showcase the potentials and challenges of computer-assisted analysis of Chinese literatures, we explored some interesting yet non-trivial questions about two of the Four Great Classical Novels of China: (1) Which monsters attempted to consume the Buddhist monk Xuanzang in the Journey to the West (西遊記 /xi1 you2 ji4/, JTTW), which was published in the 16 century, (2) Which was the most powerful monster in JTTW, and (3) Which major role smiled the most in the Dream of the Red Chamber (紅樓夢 /hong2 lou2 meng4/), which was published in the 18 century. Similar approaches can be applied to the analysis and study of modern documents, such as the newspaper articles published about the 228 incident that occurred in 1947 in Taiwan. CCS Concepts •Information systems➝Information retrieval •Information systems➝Retrieval tasks and goals •Information systems➝ Information extraction •Computing methodologies➝Natural language processing •Applied computin➝Arts and humanities	historical document;information extraction;information retrieval;natural language processing	Chao-Lin Liu;Guantao Jin;Hongsu Wang;Qingfeng Liu;Wen-Huei Cheng;Wei-Yun Chiu;Richard Tzong-Han Tsai;Yu-Chun Wang	2015	CoRR	10.1145/2818869.2818912	natural language processing;digital humanities;text mining;conceptual history;computer science;artificial intelligence;computational linguistics;linguistics	NLP	-37.265458527368274	-70.92402397246703	131580
ae50b71a17119c488561dd5136e9914dd8941209	midas - the morphological component of the ida system for efficient natural language interface design	front end;database system;canonical form;design and implementation;natural language;morphological analysis;natural language interface;production planning and control;natural language processing;deductive databases	The framework of our research originates from natural language processing and deductive database technology. Deductive databases possess uperior functionality relevant to the efficient solution of many problems in practical applications, yet there still exists no broad acquaintance. As main obstacle we identified the absence of any user-friendly interface. Natural language interfaces have been proposed a optimal candidate, however, in spite of the vast number of ambitious attempts to build natural language front ends, the achieved results were rather disappointing. In our opinion the main reason for this is missing integration, responsible for insufficient performance and wrong interpretation. In our Integrated Deductive Approach (IDA) the interface constitutes an integral part of the database system itself which guarantees the consistent mapping from the user query to the appropriate semantic application model. This paper focuses on MIDAS, the morphological analysis component. We adapt the lexical approach by storing only canonical forms as dictionary entries and attaching to them all morphological features. We do not only consider word endings but also cover prefixes, derivations, and compound words. We prove the feasibility of IDA by use of a case study, the design and implementation of a production planning and control system.	adaptive behavior;automated planning and scheduling;control system;deductive database;dictionary;floor and ceiling functions;loose coupling;natural language processing;natural language user interface;personalization;software portability;systems architecture;test data;usability	Werner Winiwarter	1995		10.1007/BFb0049155	natural language processing;interface description language;canonical form;first-generation programming language;natural language programming;speech recognition;universal networking language;object language;specification language;natural language user interface;data control language;morphological analysis;computer science;front and back ends;natural language;programming language	AI	-36.895356831699374	-77.16394537121677	131696
20fed2410f9dc868e367a38104e88f10c92bd15b	exploring similarity between academic paper and patent based on latent semantic analysis and vector space model	scientific information systems document image processing educational administrative data processing electronic publishing image matching optical sensors patents;latent semantic analysis lsa;similarity latent semantic analysis lsa vector space model vsm academic paper patent;patent;term based vsm latent semantic analysis vector space model academic paper patent similarity exploration network technology science and technology literature electronic version science and technology literature semantic similarity measurements lsa optical sensors;vector space model vsm;similarity;academic paper;patents semantics technological innovation couplings optical sensors analytical models correlation	With the development of network technology, the storage format of science and technology literature changes from paper to electronic version, and its size also is increasing. The academic papers and patents are important science and technology literature. To a certain extent, they represent the highest level of academic research and technical innovation. In this paper, we perform a study to measure the semantic similarity between academic papers and patents. The paper argues it's important to get similarity between single paper and single patent. To find linkage between them, four semantic similarity measurements are compared: Latent Semantic Analysis (LSA) based on words, LSA based on terms, Vector Space Model (VSM) based on words, VSM based on terms. A case study is conducted in the area of optical sensors. And result shows that the measurement method of terms based VSM is the best to find the similarity between single paper and single patent.	algorithm;computation;latent semantic analysis;linkage (software);scientific literature;semantic similarity;sensor;viable system model	Hongjiao Xu;Wen Zeng;Jie Gui;Peng Qu;Xiaohua Zhu;Lijun Wang	2015	2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2015.7382045	semantic similarity;similarity;computer science;artificial intelligence;data science;data mining;probabilistic latent semantic analysis;information retrieval	SE	-34.54402302168781	-66.24358307551381	131781
15b6ebacba7b28ff8a0e4326964fede341e492d7	self organizing maps for visualization of categories	categories visualization;wikipedia;self organizing maps;info eu repo semantics article;documents categorization	Visualization of Wikipedia categories using Self Organizing Maps shows an overview of categories and their relations, helping to narrow down search domains. Selecting particular neurons this approach enables retrieval of conceptually similar categories. Evaluation of neural activations indicates that they form coherent patterns that may be useful for building user interfaces for navigation over category structures.	coherence (physics);map;organizing (structure);user interface;wikipedia	Julian Szymanski;Wlodzislaw Duch	2012		10.1007/978-3-642-34475-6_20	self-organizing map;computer science;artificial intelligence;machine learning;data mining;brand;information retrieval	HCI	-38.070560687080224	-75.31980037381199	132100
88690a687ff066e878587984da747904ecad4e6a	predicting lexical relations between biomedical terms: towards a multilingual morphosemantics-based system		This paper addresses the issue of how semantic information can be automatically assigned to compound terms, i.e. both a definition and a set of semantic relations. This issue is particularly crucial when elaborating multilingual databases and when developing cross-language information retrieval systems. The paper shows how morpho-semantics can contribute in the constitution of multilingual lexical networks in biomedical corpora. It presents a system capable of labelling terms with morphologically related words, i.e. providing them with a definition, and grouping them according to synonymy, hyponymy and proximity relations. The approach requires the interaction of three techniques: (1) a la morphosemantic parser, (2) a multilingual table defining basic relations between word roots, and (3) a set of language-independant rules to draw up the list of related terms. This approach has been fully implemented for French, on an about 29,000 terms biomedical lexicon, resulting to more than 3,000 lexical families.	addresses (publication format);cross-language information retrieval;database;lexicon;linear algebra;multilingualism;parser;personnameuse - assigned;plant roots;rule (guideline);text corpus	Fiammetta Namer;Robert H. Baud	2005	Studies in health technology and informatics		multilingualism;labelling;data mining;synonym;natural language processing;parsing;semantics;lexicon;medicine;artificial intelligence	NLP	-33.80017558330937	-69.93433670629797	133477
9e374f06cfa039c66fb9baf00bb3754ae65f809f	detecting document types, plot twists, and humor	catastrophe theory;conceptual graphs;humor	Some humorous texts can be detected by stereotyped patterns and terminology. But a humorous story or situation is often an exaggeration of patterns that also occur in serious texts:  novelty, unusual plot twists, and situations that disrupt normal social conventions. The same methods for detecting novelty in serious texts can be adapted to detecting novelty in a humorous situation, but with additional tests for features that make it humorous. To interpret and reason about natural language texts, VivoMind Research has developed a cognitive architecture based on societies of heterogeneous intercommunicating agents that use conceptual graphs (CGs) as the knowledge representation. CGs are designed for representing semantics at the level of sentences and paragraphs, but they must be related to larger patterns that span an entire story, article, or book. For detecting and analyzing large-scale patterns, catastrophe theoretical semantics has proved to be surprisingly effective. This article discusses applications to both fictional and nonfictional documents of various kinds, both serious and humorous.	sensor	Arun K. Majumdar;John F. Sowa	2012			computer science;artificial intelligence;catastrophe theory	NLP	-37.73868704023964	-67.6402971145333	134000
1f8d7d6c887bb25a7b36aa5e116df7ef7718ca25	search words and geography	re used query words;geography	In this paper, we present a preliminary study of geographic query words, which users' tend to re-use. The categories of the words demonstrate that geographically related words take up the largest proportion of all repeated words. These geo-words refer to a range of spatial areas. In addition, it was found that different geo-word types are re-used in different ways by users.	microsoft word for mac	Mark Sanderson;Yu Han	2007		10.1145/1316948.1316952	computer science;data mining;information retrieval;stop words	HCI	-37.674806023193305	-71.58851949523326	134054
1f010b524247e9b49353a272a48cea958d7d3992	refref: a tool for viewing and exploring coreference space.	source code	We present RefRef, a tool for viewing and exploring coreference space, which is publicly available for research purposes. Unlike similar tools currently available whose main goal is to assist the annotation process of coreference links, RefRef is dedicated for viewing and exploring coreference-annotated data, whether manually tagged or automatically resolved. RefRef is also highly customizable, as the tool is being made available with the source code. In this paper we describe the main functionalities of RefRef as well as some possibilities for customization to meet the specific needs of the users of such coreference-annotated text.		Hisami Suzuki;Gary Kacmarcik	2006			personalization;information retrieval;source code;coreference;computer science;annotation	Logic	-33.91340954372605	-66.17769134330163	135512
7174d7263f38c25b7e4d45ebfd086f29827ba4e4	pipelines and size constraints	computacion informatica;multiple solution;filologias;grupo de excelencia;linguistica;ciencias basicas y experimentales;natural language generation;grupo a;smoking cessation	Some types of documents need to meet size constraints, such as fitting into a limited number of pages. This can be a difficult constraint to enforce in a pipelined natural language generation (NLG) system, because size is mostly determined by content decisions, which usually are made at the beginning of the pipeline, but size cannot be accurately measured until the document has been completely processed by the NLG system. I present experimental data on the performance of single-solution pipeline, multiple-solution pipeline, and revision-based variants of the STOP system (which produces personalized smoking-cessation leaflets) in meeting a size constraint. This shows that a multiple-solution pipeline does much better than a single-solution pipeline, and that a revision-based system does best of all.	graphics pipeline;natural language generation;personalization	Ehud Reiter	2000	Computational Linguistics	10.1162/089120100561692	simulation;algorithm	PL	-39.862530575480065	-73.06599663888713	135895
7250bcbc4f8e7464f10dfc1e71390746acd0597c	skeletons and semantic web descriptions to integrate parallel programming into ontology learning frameworks	biomedical ontology learning framework;machine learning algorithms;semantic web description;owl;owl s;skeleton based parallel programming;atomic clocks;mpi standard;skeleton semantic web parallel programming ontologies machine learning biomedical computing statistical analysis probes algorithm design and analysis machine learning algorithms;distributed programs;parallel programming;formal semantics;data mining;probes;ontologies artificial intelligence;skeleton;programming model;ontology learning frameworks;formal semantic description semantic web description skeleton based parallel programming biomedical ontology learning framework biomedical information overload distributed programming mpi standard high level programming model;ontology learning;statistical analysis;machine learning;high level programming model;medical information systems;semantic web learning artificial intelligence medical information systems ontologies artificial intelligence parallel programming;distributed programming;process control;semantic web;ontologies;biomedical information overload;learning artificial intelligence;parallel programs;programming;natural language processing;algorithm design and analysis;owl s skeleton based parallel programming ontology learning frameworks natural language processing machine learning semantic web ontologies owl;biomedical computing;formal semantic description	The current growth of biomedical knowledge is increasing the demand from the user community to automate the conversion of free text into a biomedical ontology. Thus ontology learning frameworks are gaining momentum as potential candidates to alleviate the current overload of biomedical information. Unfortunately the current problem at hand with these frameworks is scalability in terms of computing resources, processing power and the processing time required for biomedical experts and trained terminologists who use these frameworks. The current research study aims to tackle current difficulties in low-level parallel and distributed programming, e.g. the MPI standard, and probe the advantages for ontology learning frameworks in coupling high-level programming models together with formal semantic descriptions to enable a pay-back for the effort involved in skeleton-based parallel programming.	distributed computing;high- and low-level;high-level programming language;message passing interface;ontology learning;parallel computing;scalability;semantic web;semantics (computer science);virtual community	Mercedes Argüello Casteleiro;Ricardo Gacitua;J. Osborne;S. Peters;Pascal Ekin;Peter Sawyer	2009	2009 11th International Conference on Computer Modelling and Simulation	10.1109/UKSIM.2009.47	natural language processing;computer science;ontology;theoretical computer science;data mining	HPC	-41.79102271560233	-67.83393680763727	135965
60e3c9fc6f8736e5d376ce8033bda84a8edabf01	journalists and twitter: a multidimensional quantitative description of usage patterns		We conduct a large scale quantitative comparison of the usage pattern of a microblogging service by journalists, news organizations, and news consumers. Through two statistical tests of eighteen numerical features over 5,000 news producers and 1 million news consumers, we find that Arab journalists and English news organizations tend to broadcast their tweets to a large audience; that English journalists adopt a strategy of targeted and engaging communication; that journalists are more distinguishable in the Arab world than in the European English speaking countries; that print and radio journalists have a very dissimilar behavior while the television ones share some characteristics with each of them; and that British and Irish journalists are similar to a large extent. This paper is the first to provide a multidimensional bird’s-eye view on the usage pattern of journalists over Twitter.	bird's-eye view;information source;numerical analysis;television;welch's method	Mossaab Bagdouri	2016			multimedia;world wide web	Web+IR	-42.71737951419529	-71.3346581446081	136000
cb0e4d6110ae8558844934ec384f44cc404d50dd	de-identification of clinical narratives through writing complexity measures	electronic medical records;narration;cluster analysis;writing;natural language processing;privacy;electronic health records	PURPOSE Electronic health records contain a substantial quantity of clinical narrative, which is increasingly reused for research purposes. To share data on a large scale and respect privacy, it is critical to remove patient identifiers. De-identification tools based on machine learning have been proposed; however, model training is usually based on either a random group of documents or a pre-existing document type designation (e.g., discharge summary). This work investigates if inherent features, such as the writing complexity, can identify document subsets to enhance de-identification performance.   METHODS We applied an unsupervised clustering method to group two corpora based on writing complexity measures: a collection of over 4500 documents of varying document types (e.g., discharge summaries, history and physical reports, and radiology reports) from Vanderbilt University Medical Center (VUMC) and the publicly available i2b2 corpus of 889 discharge summaries. We compare the performance (via recall, precision, and F-measure) of de-identification models trained on such clusters with models trained on documents grouped randomly or VUMC document type.   RESULTS For the Vanderbilt dataset, it was observed that training and testing de-identification models on the same stylometric cluster (with the average F-measure of 0.917) tended to outperform models based on clusters of random documents (with an average F-measure of 0.881). It was further observed that increasing the size of a training subset sampled from a specific cluster could yield improved results (e.g., for subsets from a certain stylometric cluster, the F-measure raised from 0.743 to 0.841 when training size increased from 10 to 50 documents, and the F-measure reached 0.901 when the size of the training subset reached 200 documents). For the i2b2 dataset, training and testing on the same clusters based on complexity measures (average F-score 0.966) did not significantly surpass randomly selected clusters (average F-score 0.965).   CONCLUSIONS Our findings illustrate that, in environments consisting of a variety of clinical documentation, de-identification models trained on writing complexity measures are better than models trained on random groups and, in many instances, document types.		Muqun Li;David Carrell;John S. Aberdeen;Lynette Hirschman;Bradley A Malin	2014	International journal of medical informatics	10.1016/j.ijmedinf.2014.07.002	medicine;computer science;artificial intelligence;data science;data mining;narrative;cluster analysis;privacy;writing;information retrieval;statistics	ML	-46.93733464135945	-69.88076737894652	136328
d8b933092f01a0141a752ad5a1674053d27e7301	document analysis research in the year 2021	document analysis;algebraic factorization;table syntax;pattern recognition;table analysis;category trees;relational tables;table headers;experimental research;header paths	Despite tremendous advances in computer software and hardware, certain key aspects of experimental research in document analysis, and pattern recognition in general, have not changed much over the past 50 years. This paper describes a vision of the future where community-created and managed resources make possible fundamental changes in the way science is conducted in such fields. We also discuss current developments that are helping to lead us in this direction.	collective intelligence;crowdsourcing;experiment;machine perception;pattern recognition;programming paradigm;prototype;server (computing);wiki	Daniel P. Lopresti;Bart Lamiroy	2011		10.1007/978-3-642-21822-4_27	computer science;artificial intelligence;theoretical computer science;machine learning;data mining	Web+IR	-41.878430898598864	-74.4352744004172	137364
d1e8250f3306613d39f2fed435a44b0abb4a0936	automated text summarisation and evidence-based medicine: a survey of two domains		The practice of evidence-based medicine (EBM) urges medical practitioners to utilise the latest research evidence when making clinical decisions. Because of the massive and growing volume of published research on various medical topics, practitioners often find themselves overloaded with information. As such, natural language processing research has recently commenced exploring techniques for performing medical domain-specific automated text summarisation (ATS) techniques-- targeted towards the task of condensing large medical texts. However, the development of effective summarisation techniques for this task requires cross-domain knowledge. We present a survey of EBM, the domain-specific needs for EBM, automated summarisation techniques, and how they have been applied hitherto. We envision that this survey will serve as a first resource for the development of future operational text summarisation techniques for EBM.	automatic summarization;distributional semantics;domain-specific language;natural language processing;technical standard;text corpus	Abeed Sarker;Diego Mollá Aliod;Cécile Paris	2017	CoRR		artificial intelligence;evidence-based medicine;natural language processing;computer science	NLP	-35.122316353997604	-68.40443184651768	137368
e64891c88f41a51d45278e24d57c88ea96759773	separate handles from names on the internet	human meaning;domain name	The human meaning of domain names attracts conflict over their control, degrading their reliability as permanent handles. Solution: support handles separately from names.	internet	Michael J. O'Donnell	2005	Commun. ACM	10.1145/1101779.1101780	computer science;artificial intelligence	OS	-38.887560762205815	-75.95560164054037	137781
f5205f5881feabdb4e6e8b2b05ff81fd32aa743a	research and analysis of improved extraction based on information processing technology	analysis;natural language;seminar	This paper analyzes the problem of research topics. The key words are extracted from the academic papers published in the key journals based on information processing technology, and these words have the cooccurrence relationship between clustered. Research topics in the academic field analysis applications, researchers can provide a clear outline; in the information retrieval process, you can help clarify the information needs. We will apply the proposed method to ROCLING seminar papers on the data to extract the important field of computational linguistics research topics. The results show that this method can be applied to the special circumstances of domestic academic field, while taking out the key words in English and Chinese. the word has been said that the field of cluster results can also be an important research topic. These results in a preliminary validation of the method proposed in this paper the feasibility. It can also be found that application of computational linguistics research and practice are closely related, taking out many of the words in the cluster and machine translation, speech processing and information retrieval related to the calculation model in the language, grammar patterns and analysis, broken words and statistical language model type is the calculation is the subject of interest to linguists.	computation;computational linguistics;digital rights management;information management;information needs;information processing;information retrieval;information science;language model;library science;machine translation;natural language processing;relevance;speech processing	Yongqin Wei;Yinjing Guo	2012	JDIM		natural language processing;computer science;artificial intelligence;machine learning;data mining;database;world wide web;information retrieval	NLP	-37.83667161272972	-70.014293113096	137948
94fdd9e3900243c002a8cd1053529ea1b573dc68	a fast method for parallel document identification	parallel document identification;parallel document identification system;fast method;shared low-frequency word;efficient method;identical low-frequency word;homogeneous parallel document;distorted document;parliamentary proceeding;parallel document;low frequency	We present a fast method to identify homogeneous parallel documents. The method is based on collecting counts of identical low-frequency words between possibly parallel documents. The candidate with the most shared low-frequency words is selected as the parallel document. The method achieved 99.96% accuracy when tested on the EUROPARL corpus of parliamentary proceedings, failing only in anomalous cases of truncated or otherwise distorted documents. While other work has shown similar performance on this type of dataset, our approach presented here is faster and does not require training. Apart from proposing an efficient method for parallel document identification in a restricted domain, this paper furnishes evidence that parliamentary proceedings may be inappropriate for testing parallel document identification systems in general.	document;experiment;failure;identification scheme	Jessica Enright;Grzegorz Kondrak	2007			computer science;theoretical computer science;data mining;low frequency;algorithm	NLP	-34.61223247945426	-73.67935579568851	138146
10fed469ca8e187c9f7553a3f5aeedd33e162f14	human language technology for automatic annotation and indexing of digital library content	century oldbailey proceeding;automatic annotation;digital library content;indexing multimedia content;customisable human language technology	In this paper we show how we used robust human language technology, such as our domain-independent and customisable named entity recogniser, for automatic content annotation and indexing in two digital library applications. Each of these applications posed a unique challenge: one required adapting the language processing components to the non-standard written conventions of 18th century English, while the other presented the challenge of processing material in multiple modalities. This reusable technology could also form the basis for the creation of computational tools for the study of cultural heritage languages, such as Ancient Greek and Latin.	digital library;email;gate;html;information extraction;language technology;library (computing);named entity;performance evaluation;signal-to-noise ratio;unicode;usability;xml	Kalina Bontcheva;Hamish Cunningham	2002		10.1007/3-540-45747-X_51		NLP	-34.811731069061715	-75.19235305452554	138339
239021a4d18204212e654078fea30285e8620145	knowledge-based linguistic annotation of digital cultural heritage collections	artefacto;anotacion;vocabulaire;metadata schema;linguistique;art;base de connaissances;metadata;heritage;probability density function;digital libraries;cultural heritage;vocabulary;aria collection;text analysis;annotation;vocabulary digital libraries exhibitions humanities information retrieval systems text analysis;intelligence artificielle;vocabulario;data mining;structured vocabularies;intelligent web services;artefact;cultural heritage natural language processing intelligent web services semantic web machine learning;article letter to editor;patrimoine culturel;linguistica;rijksmuseum amsterdam;machine learning;digital cultural heritage collections;humanities;rijksmuseum amsterdam digital cultural heritage collections knowledge based linguistic annotation text description structured vocabularies metadata schema aria collection;patrimonio cultural;text description;exhibitions;metadonnee;information retrieval systems;cultural differences vocabulary art databases thesauri artificial intelligence intelligent structures humans automatic control licenses;semantic web;artificial intelligence;base conocimiento;knowledge based linguistic annotation;metadatos;inteligencia artificial;natural language processing;cultural differences;tagging;knowledge base;linguistics	A method for automatically annotating objects in digital cultural heritage collections uses structured vocabulary concepts and their metadata schema roles.	vocabulary	Tuukka Ruotsalo;Lora Aroyo;Guus Schreiber	2009	IEEE Intelligent Systems	10.1109/MIS.2009.32	natural language processing;knowledge base;probability density function;computer science;cultural heritage;artificial intelligence;semantic web;exhibition;metadata;world wide web;cultural diversity;information retrieval	Vision	-35.04838180275343	-66.7763067831604	138840
698d28bb20adc4cde096dd1045f09e1567d6d730	context : natural language full-text retrieval system.	natural language;text retrieval	H2-receptor antagonist ascorbate compounds, derived from the lactone form of 3-ketohexuronic acid of Formula where (X) may be 1, 2 or 3 and (Y) may be 1 or 2, R1 and R2 are both hydrogen or R1 may be hydroxyl, R2 hydrogen or the O-alkylidene, 5,6-diacyl, 6-acyl derivatives thereof having two to sixteen carbon atoms, or a 6-phosphate, R3 being an organic base or a salt thereof having one or more basic functional groups, having H2-receptor antagonist properties and capable of reacting with nitrous acid, and a process for the preparation of said compounds.	document retrieval;natural language	Zeev Menkes	1988			natural language processing;document retrieval;language identification;full text search;natural language programming;universal networking language;explicit semantic analysis;question answering;natural language user interface;concept search;temporal annotation;information extraction;human–computer information retrieval	NLP	-36.445854745681615	-75.61567873472936	139612
f3fc0a3954592f3630c916f1d31f50f4456f74b6	summarization by domain ontology navigation	simple taxonomy;domain ontology;wiley periodicals;generative domain ontology;text document;simple mean;meaningful natural language description;concise description;domain ontology navigation;conceptual summary;domain corpus	A summary is a concise description that reflects the essence of a subject. A text, a collection of text documents, or a query answer can be summarized by simple means such as an automatically generated list of the most frequent words or “advanced” by a meaningful natural language description of the subject. In between these two extremes, conceptual summaries encompass selected concepts derived using background knowledge. We address in this paper an approach where conceptual summaries are provided through a conceptualization as given by an ontology. The ontology guiding the summarization can be a simple taxonomy or a generative domain ontology. A domain ontology can be provided by a preanalysis of a domain corpus and can be used to condense improved summaries that better reflects the conceptualization of a given domain. C © 2012 Wiley Periodicals, Inc.	conceptualization (information science);document;john d. wiley;natural language;ontology (information science);taxonomy (general);web ontology language	Troels Andreasen;Henrik Bulskov	2013	Int. J. Intell. Syst.	10.1002/int.21575	natural language processing;upper ontology;conceptualization;bibliographic ontology;computer science;ontology;data mining;ontology-based data integration;information retrieval;process ontology;suggested upper merged ontology	AI	-34.04241036978276	-68.19928782226226	139660
014135a32321857e7a66f340cf293b7bb8489c3f	a visual ontology-driven interface for a web sign language dictionary	sign languages;user needs;sign language;user centred design methodology;ontology visualisation;language development;user centred design;querying task;visual interfaces	Sign languages are visual-gestural languages developed mainly in deaf communities; their tempo-spatial nature makes it difficult to write them, yet several transcription systems are available for them. Most sign languages dictionaries interact with their users via a transcriptionbased inteface; thus their users need to be expert of their specific transcription system. The e-LIS dictionary is the first web bidirectional dictionary for Italian sign language-Italian; using the current interface, the dictionary users can define a sign interacting with intuitive iconic images, without knowing the underlying transcription system. Nevertheless the users of the current e-LIS dictionary are assumed to be expert of Italian sign language. The e-LIS ontology, which specifies how to form a sign, was created to allow even the non-experts of Italian sign language to use the dictionary. Here we present a prototype of a visual interface based on the e-LIS ontology for the e-LIS dictionary; the prototype is a query-oriented navigation interface; it was designed following the User Centred Design Methodology, which focuses on the user during the design, development and testing of the system.	dictionary;interaction;prototype;transcription (software)	Mauro Felice;Tania Di Mascio;Rosella Gennari	2007		10.1007/978-3-540-77010-7_41	natural language processing;speech recognition;sign language;computer science;database;programming language;world wide web	HCI	-35.28221961510322	-74.93157070620617	140415
b5f23a00f784ea67deb596daf2a888247f550cba	a concordance to keil's latin grammarians	latin;latin grammarians;concordances;heinrich keil	The following is a description of a computerized version of the corpus of Latin grammarians published by Heinrich Keil in Leipzig between 1855 and 1880. The intent was to prepare an instrument which would serve both as a key to Keil's corpus and as the basis for a re-edition of the work itself. We discuss the corpus itself, the ways in which it was encoded, the pre-editing work, and how the material was organized for analysis by computer.	concordance (publishing)	Valeria Lomanto	1990	Computers and the Humanities	10.1007/BF00186487	literature	NLP	-38.245657757000885	-77.0011094918249	140818
770e7e167b2c29705c79177b49be2c1cfc84b4c0	identification of misspelled words without a comprehensive dictionary using prevalence analysis	software;medical records;roc curve;algorithms;humans;dictionaries as topic;natural language processing;antihypertensive agents	Misspellings are common in medical documents and can be an obstacle to information retrieval. We evaluated an algorithm to identify misspelled words through analysis of their prevalence in a representative body of text. We evaluated the algorithm's accuracy of identifying misspellings of 200 anti-hypertensive medication names on 2,000 potentially misspelled words randomly selected from narrative medical documents. Prevalence ratios (the frequency of the potentially misspelled word divided by the frequency of the non-misspelled word) in physician notes were computed by the software for each of the words. The software results were compared to the manual assessment by an independent reviewer. Area under the ROC curve for identification of misspelled words was 0.96. Sensitivity, specificity, and positive predictive value were 99.25%, 89.72% and 82.9% for the prevalence ratio threshold (0.32768) with the highest F-measure (0.903). Prevalence analysis can be used to identify and correct misspellings with high accuracy.	algorithm;area under curve;dictionary;hypertensive disease;information retrieval;name;note (document);positive predictive value of diagnostic test;prototype;randomness;receiver operating characteristic;sensitivity and specificity;source code;algorithm;spelling	Alexander Turchin;Julia T. Chu;Maria Shubina;Jonathan S. Einbinder	2007	AMIA ... Annual Symposium proceedings. AMIA Symposium		speech recognition;computer science;pattern recognition;data mining	Logic	-47.998066413238405	-69.6087630043189	141044
297f798fc8818edb1e89ce98835db88cd060754c	retrospective semi-automated software feature extraction from natural language user manuals		Mature software systems comprise a vast number of heterogeneous system capabilities which are usually requested by different groups of stakeholders and evolve over time. Software features describe and logically bundle low level capabilities on an abstract level and thus provide a structured and comprehensive overview of the entire capabilities of a software system. Software features are often not explicitly managed. Quite the contrary,software feature-relevant information is often spread across several software engineering artifacts (e.g., user manual, issue tracking systems). It requires huge manual effort to (1)rnidentify and extract software feature-relevant information from these artifacts in order to make software feature knowledge explicit and furthermore to (2) determine whichrnsoftware features the disclosed software feature-relevant information belongs to. This thesis presents a three-step-approach to semi-automatically enhance software features byrnsoftware feature-relevant information from a user manual: first, a domain terminology is semi-automatically extracted from a natural language user manual based on linguisticrnpatterns. Second, the extracted domain terminology, structural sentence information and natural language processing techniques are used to automatically identify andrnextract atomic software feature-relevant information with an F1-score of at least 92.00%.rnFinally, the determined atomic software feature-relevant information is semi-automatically assigned to existing and logically related software features. The approach is empirically evaluated by means of a user manual and corresponding gold standards of an industrial partner. This thesis provides tool support to identify and extract atomic software featurerelevant information from user manuals and furthermore recommend logically related software features.	feature extraction;natural language;semiconductor industry;software feature	Thomas Quirchmayr	2018			software system;terminology;software engineering;natural language;software feature;tracking system;software;computer science;sentence	AI	-34.875889459033765	-67.4718719587283	142037
d3571b8f069b2016da529d9ef78706acf4dd3f7d	improving the inter-corpora compatibility for protein annotations	named entity recognition;protein annotation;corpus	Although there are several corpora with protein annotation, incompatibility between the annotations in different corpora remains a problem that hinders the progress of automatic recognition of protein names in biomedical literature. Here, we report on our efforts to find a solution to the incompatibility issue, and to improve the compatibility between two representative protein-annotated corpora: the GENIA corpus and the GENETAG corpus. In a comparative study, we improve our insight into the two corpora, and a series of experimental results show that most of the incompatibility can be removed.	body of uterus;medical device incompatibility problem;name;protein annotation;software incompatibility;text corpus	Yue Wang;Jin-Dong Kim;Rune Sætre;Sampo Pyysalo;Tomoko Ohta;Jun'ichi Tsujii	2010	Journal of bioinformatics and computational biology	10.1142/S0219720010004999	natural language processing;biology;computer science;bioinformatics;text corpus;information retrieval	NLP	-35.11032412430141	-69.85040322808385	142608
ba5764ac04fdfba3cd3445ca8d59bba97c8a6292	semantic similarity estimation in the biomedical domain: an ontology-based information-theoretic perspective	semantic similarity;biomedical ontologies;information content;information theoretic;information theory	Semantic similarity estimation is an important component of analysing natural language resources like clinical records. Proper understanding of concept semantics allows for improved use and integration of heterogeneous clinical sources as well as higher information retrieval accuracy. Semantic similarity has been the focus of much research, which has led to the definition of heterogeneous measures using different theoretical principles and knowledge resources in a variety of contexts and application domains. In this paper, we study several of these measures, in addition to other similarity coefficients (not necessarily framed in a semantic context) that may be useful in determining the similarity of sets of terms. In order to make them easier to interpret and improve their applicability and accuracy, we propose a framework grounded in information theory that allows the measures studied to be uniformly redefined. Our framework is based on approximating concept semantics in terms of Information Content (IC). We also propose computing IC in a scalable and efficient manner from the taxonomical knowledge modelled in biomedical ontologies. As a result, new semantic similarity measures expressed in terms of concept Information Content are presented. These measures are evaluated and compared to related works using a benchmark of medical terms and a standard biomedical ontology. We found that an information-theoretical redefinition of well-known semantic measures and similarity coefficients, and an intrinsic estimation of concept IC result in noticeable improvements in their accuracy.		David Sánchez;Montserrat Batet	2011	Journal of biomedical informatics	10.1016/j.jbi.2011.03.013	natural language processing;open biomedical ontologies;semantic similarity;semantic computing;semantic integration;self-information;information theory;computer science;data mining;information retrieval;statistics;dishin	Web+IR	-43.325128596156105	-67.74230552264808	142994
fd5b4f8e2f2cc3be7fb704dc45f73f1e6ca83cab	document detection summary of results	comparative system performance;detection half;individual experiment;document detection summary;tipster phase;system overviews;tipster result	Four contractors were involved in the document detection half of TIPSTER. Two of the contractors worked in English only (Syracuse University and HNC Inc.), one contractor worked in Japanese only (TRW Systems Development Division), and one contractor worked in both languages (University of Massachusetts at Amherst). The four contractors had extremely varied approaches to the detection task. TRW transformed an operational English retrieval system (based on pattern matching using a fast hardware approach), into a Japanese version of the same operation, with a special interface designed to facilitate work in Japanese. The University of Massachusetts approach involved taking a relatively small experimental system using a probabilistic inference net methodology, scaling it up to handle the very large amounts of text and long topics in TIPSTER, and modifying the algorithms to handle Japanese. Both Syracuse University and HNC Inc. built completely new systems to handle the English collection. In the case of Syracuse University, their system is based heavily on a natural language approach to retrieval, with many of the techniques traditionally used in document understanding applied to the retrieval task. HNC Inc. took a totally different approach, applying statistical techniques based on robust mathematical models (including the use of neural networks).	algorithm;artificial neural network;experimental system;image scaling;mathematical model;natural language;pattern matching	Donna K. Harman	1993			engineering;data mining;world wide web;information retrieval	NLP	-35.1621253290597	-77.52811727507354	144052
2f047f0e96fcc35d316b289a9ecd56059b023a03	text clustering to help knowledge acquisition from documents	top down;text analysis;statistical method;hierarchical classification;statistical analysis;knowledge acquisition;empirical validation;text clustering;knowledge engineering	At the earlier stage of the knowledge acquisition process, interviews of experts produce a large amount of rich but ill-structured texts. Knowledge engineers need some tool to help them in the exploitation of all these texts. We propose the use of a statistical method, the top-down hierarchical classification and a new interpretation of its results. The initial statistical analysis proposed by M. Reinert [16, 17] gives two kinds of results: first a segmentation of texts that reflects their semantic contexts that we use to raise structures of texts, and second, classes of significant terms belonging to these contexts, which can be related to the experts or to these specialities. In this paper, we describe the method, its empirical validity and a comparison with similar approaches, its uses with examples and results. We conclude with some research directions to extend the exploitation of the analysis results.	knowledge acquisition	Stéphane Lapalut	1996		10.1007/3-540-61273-4_8	computer science;artificial intelligence;data science;knowledge engineering;pattern recognition;top-down and bottom-up design;data mining;cluster analysis	ML	-36.82962458763849	-67.47136863136417	144116
4ac90686fc9b5c17ca09c9441f45eef086e85eae	evaluation of lexical methods for detecting relationships between concepts from multiple ontologies	linguistics;string matching;natural language processing;computational biology;information retrieval;computer simulation	We used exact term matching, stemming, and inclusion of synonyms, implemented via the Lucene information retrieval library, to discover relationships between the Gene Ontology and three other OBO ontologies: ChEBI, Cell Type, and BRENDA Tissue. Proposed relationships were evaluated by domain experts. We discovered 91,385 relationships between the ontologies. Various methods had a wide range of correctness. Based on these results, we recommend careful evaluation of all matching strategies before use, including exact string matching. The full set of relationships is available at compbio.uchsc.edu/dependencies.	brenda;body tissue;chebi;correctness (computer science);gene ontology;information retrieval;matching;ontology (information science);open biomedical ontologies;stemming;string searching algorithm;subject-matter expert	Helen L. Johnson;K. Bretonnel Cohen;William A. Baumgartner;Zhiyong Lu;Michael Bada;Todd Kester;Hyunmin Kim;Lawrence Hunter	2006	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing		computer simulation;natural language processing;computer science;bioinformatics;data mining;information retrieval;string searching algorithm	Logic	-35.28610789567186	-68.97265455655064	145264
f6e37998194abffda15415f815cebe2e06b3fb75	ontology-based information extraction from the configuration command line of network routers	data mining;semantic relatedness;conference report;ontology based information extraction;configuration management;routers;knowledge discovery	Knowledge extraction is increasingly attracting the attention of researchers from different disciplines, as a means to automate complex tasks that rely on bulk textual resources. However, the configuration of many devices in the networking field continues to be a labor intensive task, based on the human interpretation and manual entry of commands through a text-based user interface. Typically, these Command-Line Interfaces (CLIs) are both device and vendor-specific, and thus, commands differ syntactically and semantically for each configuration space. Because of this heterogeneity, CLIs always provide a “help” feature—i.e., short command descriptions encoded in natural language—aimed to unveil the semantics of configuration commands for network administrators. In this paper, we exploit this feature with the aim of automating the abstraction of device configurations in heterogeneous settings. In particular, we introduce an Ontology-Based Information Extraction (OBIE) system from the Command-Line Interface of network routers. We also present ORCONF, a domain Ontology for the Router CONFiguration domain, and introduce a semantic relatedness measure that quantifies the degree of interrelation among candidate concepts. The results obtained over the configuration spaces of two widely used network routers demonstrate that this is a promising line of research, with overall percentages of precision and recall of 93%, and 91%, respectively.	command-line interface;information extraction;natural language;ontology (information science);precision and recall;router (computing);semantic similarity;text-based (computing);text-based user interface	Anny Martínez;Marcelo Yannuzzi;René Serral-Gracià;Wilson Ramírez	2014		10.1007/978-3-319-13817-6_30	computer science;data mining;database;world wide web	AI	-35.613138309008654	-67.49753880319732	145516
1caaa73e66adf348e62c26fdef922d34f63c09b5	application of computer-supported content analysis on twitter: a case study on swiss national media criticism		Media and media content are of considerable social relevance. However, the systematic and automated extraction of media content is hardly a subject in the communication and media studies. This also applies to the potential use of computer-assisted content analysis as an alternative or complement to manual coding in a media science context. My project aims to address this research gap and reduce labor-intensive processes by using machine learning methods and pooled computing resources to automatically extract topic related data from Twitter. As research topic the project intents to analyze the status quo of Swiss national media-criticism. This project combines the two research fields computer linguistics applied to media studies and examines the extent to which an automated process using a Naïve-Bayes algorithm can adequately identify content containing Swiss national media-criticism. To answer this question the results are validated by means of manual content analysis.	algorithm;computational linguistics;machine learning;naive bayes classifier;relevance;switzerland	Vincenzo Francolino	2017		10.1145/3102254.3102267	data mining;domain knowledge;data science;status quo;web intelligence;coding (social sciences);computer science;criticism;content analysis	NLP	-35.506693808658305	-68.52573887579287	145519
8c1b19664a1b15591e4a10014b023eaa28f33483	discovering causality in suicide notes using fuzzy cognitive maps	computer engineering;ethan p;computer science discovering causality in suicide notes using fuzzy cognitive maps university of cincinnati lawrence mazlack white	An important question is how to determine if a person is exhibiting suicidal tendencies in behavior, speech, or writing. This paper demonstrates a method of analyzing written material to determine whether or not a person is suicidal or not. The method involves an analysis of word frequencies that are then translated into a fuzzy cognitive map that will be able to determine if the word frequency patterns are showing signs of suicidal tendencies. The method could have significant potential in suicide prevention as well as in other forms of sociological behavior studies that might exhibit their own identifying patterns.	behavioral pattern;causal filter;causality;fuzzy cognitive map;word lists by frequency	Ethan White;Lawrence J. Mazlack	2011			psychology;computer science;artificial intelligence	NLP	-45.12804965534203	-73.9894463606387	145728
760bb6b1124b45cb4269f3d5f9f406292b9e1cd0	unsupervised medical subject heading assignment using output label co-occurrence statistics and semantic predications		Librarians at the National Library of Medicine tag each biomedical abstract to be indexed by their Pubmed information system with terms from the Medical Subject Headings (MeSH) terminology. The MeSH terminology has over 26,000 terms and indexers look at each article's full text to assign a set of most suitable terms for indexing it. Several recent automated attempts focused on using the article title and abstract text to identify MeSH terms for the corresponding article. Most of these approaches used supervised machine learning techniques that use already indexed articles and the corresponding MeSH terms. In this paper, we present a novel unsupervised approach using named entity recognition, relationship extraction, and output label co-occurrence frequencies of MeSH term pairs from the existing set of 22 million articles already indexed with MeSH terms by librarians at NLM. The main goal of our study is to gauge the potential of output label co-occurrence statistics and relationships extracted from free text in unsupervised indexing approaches. Especially, in biomedical domains, output label co-occurrences are generally easier to obtain than training data involving document and label set pairs owing to the sensitive nature of textual documents containing protected health information. Our methods achieve a micro F-score that is comparable to those obtained using supervised machine learning techniques with training data consisting of document label set pairs. Baseline comparisons reveal strong prospects for further research in exploiting label co-occurrences and relationships extracted from free text in recommending terms for indexing biomedical articles.	baseline (configuration management);bin;complement system proteins;index;indexes;information system;k-nearest neighbors algorithm;librarian;low-rank approximation;machine learning;medical specialities;medical subject headings;moving target indication;name;named entity;named-entity recognition;national library of medicine (u.s.);netware loadable module;nomenclature;protected health information;pubmed;relationship extraction;science;supervised learning;tracer;united states national institutes of health;unsupervised learning	Ramakanth Kavuluru;Zhenghao He	2013	Natural language processing and information systems : ... International Conference on Applications of Natural Language to Information Systems, NLDB ... revised papers. International Conference on Applications of Natural Language to Info...	10.1007/978-3-642-38824-8_15	computer science;pattern recognition;data mining;information retrieval	DB	-33.998365078142974	-69.47800175396557	146352
5a7dda8c06a03c03e6ea875b1fcacdcb9188a52e	user interfaces to the web of data based on natural language generation		The core idea of the Semantic Web vision is the evolution from a Web of hyperlinked human-readable web pages, the Web of Documents, to a machine-interpretable Web of Data. Since natural language text is a suitable knowledge representation for humans and not for machines, the knowledge representation formalism RDF was developed and large amounts of RDF data are published. Now that machine-interpretable data is available, the fact that RDF data is not human-readable poses challenges for humans intending to interact with the data and to exploit the wealth of data. In this work, Natural Language Generation is applied to bridge the gap from machine-interpretable data to human-readable text to improve user interfaces to the Web of Data. In the context of a concrete research practice, i.e., the qualitative and quantitative analysis of a large digital corpus of educational lexica, we1 explore how Virtual Research Environments based on Semantic Web technologies support research interactions with RDF data in various stages of corpus-based analysis. We analyze the human-readability of a large subset of the Web of Data in terms of the availability of human-readable labels of entities and propose label-related quality metrics. Since our analysis shows that labels are missing for a significant percentage of entities we explore an approach to derive labels from names of variables in queries formulated in SPARQL, which is an RDF query language. In the context of search interfaces to RDF data a class of SPARQL query-generating systems exists where users signify their information needs in the form of keywords or (controlled) natural language questions. For the purpose of enabling a user to observe a potential discrepancy between an intended question and the system-generated query we created a method to verbalize SPARQL queries. Here, the meaning of a query encoded in SPARQL is conveyed to the user via English text. The different syntaxes of RDF are not suitable for the presentation to casual users. We introduce a template-based approach to verbalize RDF graphs. Since manual creation of these templates is tedious work we developed a language-independent approach to induce RDF verbalization templates from a parallel corpus of text and data. 1I use the form we instead of I, since the work was published together with coauthors. In most publications, however, I provided the main contribution and was acknowledged as first author. Using the form we instead of I serves for the purpose of acknowledging my coauthors.		Basil Ell	2015			web page;web modeling;model–view–controller;world wide web;web service;natural language generation;human–computer interaction;natural language user interface;web navigation;web design;computer science	Web+IR	-34.153246333812085	-67.9214951411651	146424
1010d0a11ae1c15b77048af642909b530cf6c74e	the linguistic data consortium member survey: purpose, execution and results.	linguistic data consortium;quality of service	The Linguistic Data Consortium (LDC) seeks to provide its members with quality linguistic resources and services. In order to pursue these ideals and to remain current, LDC monitors the needs and sentiments of its communities. One mechanism LDC uses to generate feedback on consortium and resource issues is the LDC Member Survey. The survey allows LDC Members and nonmembers to provide LDC with valuable insight into their own unique circumstances, their current and future data needs and their views on LDC’s role in meeting them. When the 2006 Survey was found to be a useful tool for communicating with the Consortium membership, a 2007 Survey was organized and administered. As a result of the surveys, LDC has confirmed that it has made a positive impact on the community and has identified ways to improve the quality of service and the diversity of monthly offerings. Many respondents recommended ways to improve LDC’s functions, ordering mechanism and webpage. Some of these comments have inspired changes to LDC’s operation and strategy. 1. LDC Member Survey Goals The External Relations Group at the Linguistic Data Consortium (LDC) is primarily concerned with the needs and satisfaction of LDC’s user communities and extending LDC services to new organizations and communities. In order to best serve its members, LDC needs to remain aware of their changing needs and provide a mechanism for gathering and updating this information. The more LDC knows about its constituents communities, the better it can serve them and new communities not yet engaged. 1.1 LDC’s Target Market Language technology developers belong to a niche market, meaning that their needs are highly focused and cannot be satisfied by mainstream products. They include academic, government and commercial organizations whose research interests and performance goals vary widely within the sub-disciplines of text, speech, video and multimodal processing. As a result, there are few data collections that appeal to all of these communities. Finding a way to identify and address such highly differentiated needs is a challenge for the LDC. An ideal method for discovering market needs would have minimal administrative costs, be easily transmitted to a variety of end recipients and most importantly, be relevant to the communities involved. Before 2006, LDC had no regular mechanism in place to elicit feedback from its research communities. LDC’s management and External Relations group believed that members were satisfied with both the quality of data LDC released each year and the level of service provided, but there was no means to quantify members’ sentiments. In 2006, the External Relations group decided to administer a survey to active participants in the LDC community, including members and nonmember organizations, in order to probe the following issues: the groups’ awareness levels about LDC activities, their level of satisfaction with LDC data and services and their needs for the future. The remainder of this paper describes how the 2006 and 2007 surveys were created, administered, evaluated and incorporated into LDC policy. 2. External Relations LDC is a nonprofit, open, international consortium of universities, corporations, and government research organizations that creates and distributes linguistic resources, including data, tools and specifications. LDC also works with a variety of organizations in order to develop and disseminate best practices and standards for linguistic research. LDC was founded in 1992 with a seed grant from the Defense Advanced Research Projects Agency (DARPA). Ongoing operation costs are funded solely from membership and licensing fees while grants from commercial sponsors, other non-profits and the United States Departments of Commerce, Defense, Education and the Interior fund new resource creation. The University of Pennsylvania is LDC’s host institution. LDC’s External Relations group consists of the following: the Communications Coordinator, who develops and executes new initiatives; the Membership Coordinator, who handles LDC’s member and nonmember requests and administers data distributions for research projects and evaluations; the Publications Programmer, who produces LDC’s general publications and limited releases for ongoing projects and evaluations; and the External Relations Manager, who oversees these functions and reports directly to the Executive Director. The Communications Coordinator position was created in 2005 to demonstrate LDC’s commitment to member satisfaction and to focus on identifying new areas for growth. In 2006, these efforts were still in the early stages of development and the Member Survey was an important tool in setting the groundwork for future activities. 2.1 External Communications Generally, LDC focuses on strategies that solidify its membership base, create a strong sense of community within the Consortium and expand LDC awareness in and of related communities. Understanding the Human	best practice;language technology;linguistic data consortium;multimodal interaction;niche blogging;operation payback;programmer;quality of service;video;web page	Marian Reed;Denise DiPersio;Christopher Cieri	2008			quality of service;computer science;data mining;database;world wide web	HCI	-34.304487189438845	-76.44421368367765	146801
379214596cc3a0a3ede448fd2ce665778972a8c8	markup of korean dictionary entries	conference paper;text encoding initiative	Dictionary markup (encoding) is one of the concerns of TEI (Text Encoding Initiative), an international project for text encoding. In this paper, we investigate ways to use and extend TEI encoding scheme for the markup of Korean dictionary entries. Since TEI suggestions for dictionary markup are mainly for western language dictionaries, we need to cope with problems to be encountered in encoding Korean dictionary entries. We try to extend and modify the TEI encoding scheme in the way suggested by TEI. Also, we restrict the content model so that the encoded dictionary might be viewed more as a database than as a simple computerized, originally printed, dictionary.	database;dictionary;line code;printing;standard generalized markup language;text encoding initiative	Beom-mo Kang	1996			natural language processing;speech recognition;machine-readable dictionary;computer science;information retrieval	DB	-33.76715803057523	-74.71989416645482	146962
03865704596ad9790fc634825b9acec0fc05a309	system evaluation on a named entity corpus from clinical notes.	information extraction;system evaluation;snomed ct;machine learning;named entity	This paper presents the evaluation of the dictionary look-up component of Mayo Clinic’s Information Extraction system. The component was tested on a corpus of 160 free-text clinical notes which were manually annotated with the named entity disease. This kind of clinical text presents many language challenges such as fragmented sentences and heavy use of abbreviations and acronyms. The dictionary used for this evaluation was a subset of SNOMED-CT with semantic types corresponding to diseases/disorders without any augmentation. The algorithm achieves an F-score of 0.56 for exact matches and F-scores of 0.76 and 0.62 for right and left-partial matches respectively. Machine learning techniques are currently under investigation to improve this task.	algorithm;ct scan;dictionary;f1 score;information extraction;lookup table;machine learning;named entity;systematized nomenclature of medicine;text corpus	Karin Kipper Schuler;Vinod Kaggal;James J. Masanz;Philip V. Ogren;Guergana K. Savova	2008			natural language processing;computer science;snomed ct;data mining;information extraction;information retrieval	NLP	-33.98705896203126	-70.35873789606632	147451
b73ad6f0efac6a0f5aba01794dc066cf80a4e998	an experimental evaluation of de-identification tools for electronic health records		The robust development of Electronic Health Records (EHRs) causes a significant growth in sharing EHRs for clinical research. However, such a sharing makes it difficult to protect patients’ privacy. A number of automated de-identification tools have been developed to reduce the re-identification risk of published data, while preserving its statistical meanings. In this paper, we focus on the experimental evaluation of existing automated de-identification tools, as applied to our EHR database, to assess which tool performs better with each quasi-identifiers defined in our paper. Performance of each tool is analyzed wrt. two aspects: individual disclosure risk and information loss. Through this experiment, the generalization method has better performance on reducing risk and lower degree of information loss than suppression, which validates it as more appropriate de-identification technique for EHR databases.	database;de-identification;identifier;privacy;quasi-identifier;zero suppression	Jie Qian;Nafees Qamar	2012	CoRR		computer science;data mining;world wide web;information retrieval	DB	-45.63671167341342	-66.52199413426433	147857
84c29398731d089c454be3bb0f80503005c2ca49	a multilingual patent text-mining approach for computing relatedness evaluation of patent documents	multilingual patent retrieval;pattern clustering;patent retrieval;information sources;neural networks;text mining;text analysis data mining indexing information retrieval patents pattern clustering;system modeling;information retrieval;text clustering techniques multilingual patent text mining approach language independent technique multilingual patent information sources multilingual patent documents multilingual vector space latent semantic indexing model;multilingual patent information sources;vector space;text analysis;text clustering techniques;data mining;multilingual patent text mining approach;text clustering neural networks multilingual patent retrieval text mining latent semantic indexing;large scale integration;indexing;patents;latent semantic indexing;information processing;text clustering;multilingual vector space;latent semantic indexing model;language independent technique;information system;multilingual patent documents;parallel corpora;indexing information retrieval large scale integration information systems text mining information processing dictionaries multimedia computing signal processing information management;conferences;neural network	This paper describes our work on developing a language-independent technique for discovery of implicit knowledge about patents from multilingual patent information ources. Traditional techniques of multi- and cross-language patent retrieval are mostly based on the process of translation. One major problem of those is that it is difficult to find related patents produced from other countries in a stand-alone patent information system. In this paper, we present a novel system platform to support locating similar and relevant multilingual patent documents. The platform is developed using a multilingual vector space based on the latent semantic indexing (LSI) model, and utilizing collected professional Chinese-English parallel corpora for training the system model. These multilingual patent documents can then be mapped into the semantic vector space for evaluating their similarity by means of text clustering techniques. The preliminary results show that our platform framework has potential for retrieval and relatedness evaluation of multilingual patent documents.	cluster analysis;information system;language-independent specification;latent semantic analysis;parallel text;text corpus;text mining	Chung-Hong Lee;Hsin-Chang Yang;Chih-Hong Wu;Yi-Ju Li	2009	2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2009.162	natural language processing;patent visualisation;search engine indexing;text mining;latent semantic indexing;systems modeling;vector space;computer science;machine learning;data mining;information retrieval;information system;artificial neural network	DB	-34.455406074800514	-66.27518313881434	149229
8bc3d6f6a8953c9194e2c0104e4a73e2fa9c58fa	development of a multilingual text mining approach for knowledge discovery in patents	relatedness evaluation multilingual text mining knowledge discovery patents multilingual patent information sources multilingual vector space latent semantic indexing professional chinese english parallel corpora text clustering document retrieval;document clustering;multilingual patent retrieval;patent retrieval;information sources;text mining;system modeling;information retrieval;document clustering text mining multilingual patent retrieval patent retrieval latent semantic indexing;multilingual patent information sources;vector space;text analysis;text analysis data mining indexing information retrieval patents;multilingual text mining;data mining;indexing;text mining indexing information analysis large scale integration dictionaries information systems cybernetics usa councils information management terminology;patents;latent semantic indexing;classification algorithms;text clustering;multilingual vector space;support vector machine classification;document retrieval;relatedness evaluation;parallel corpora;professional chinese english parallel corpora;conferences;knowledge discovery	In this paper we describe our work on developing a novel technique for discovery of implicit knowledge about patents from multilingual patent information sources. In this work we developed a system platform to support locating similar and relevant multilingual patent documents. The platform was implemented using a multilingual vector space based on the latent semantic indexing (LSI) model, and utilizing collected professional Chinese-English parallel corpora for training the system model. These multilingual patent documents could then be mapped into the semantic vector space for evaluating their similarity by means of text clustering techniques. The preliminary results show that our platform framework has potential for retrieval and relatedness evaluation of multilingual patent documents.	cluster analysis;latent semantic analysis;parallel text;precision and recall;semiconductor;sparse matrix;text corpus;text mining	Chung-Hong Lee;Hsin-Chang Yang;Yi-Ju Li	2009	2009 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2009.5345953	natural language processing;search engine indexing;text mining;latent semantic indexing;systems modeling;document clustering;vector space;computer science;data mining;information retrieval	DB	-34.4666137359876	-66.29730864092691	149587
e1a20b52cf6ad77e2fe8569e43407648108e7e58	extracting implicit information from free text technical reports	historia clinica;language comprehension;extraction information;analisis contenido;representation;evaluation systeme;text;implicit;medical record;information extraction;frase;medicina;connaissance;evaluacion sistema;conocimiento;texte;test;analisis automatico;medecine;ensayo;feasibility;prototipo;knowledge;essai;sentence;content analysis;system evaluation;automatic analysis;dossier medical;cognition;analyse automatique;cognicion;medicine;comprension lenguaje;phrase;comprehension langage;implicite;analyse contenu;analisis semantico;analyse semantique;texto;prototype;practicabilidad;faisabilite;semantic analysis;representacion;extraction informacion	We explore the issue of extracting both explicit and implicit information from narrative technical reports through knowledge-based free text understanding. We rely on the assumption that whereas technical texts convey much implicit information, such information can be recovered through natural language analysis by building and reasoning on a model of the situation described, if both linguistic and detailed world knowledge are provided to the system. We evaluated the feasibility of this approach by designing and testing a prototype performing information extraction from clinical record sentences in a restricted medical domain: thyroid cancer care. This prototype was fully implemented and was tested on actual sentences. This prototype was fully implmented and was tested on actual sentences. We present the natural language processing strategy adopted in our system with emphasis on knowledge use, as well as the preliminary results obtained		Marc Cavazza;Pierre Zweigenbaum	1991		10.1016/0306-4573(92)90030-4	natural language processing;cognition;content analysis;computer science;artificial intelligence;prototype;linguistics;knowledge;representation;information extraction;medical record	NLP	-36.30311590136053	-78.45940605989125	149857
e4d7ecac842950da07ad3b338f22f40d85124899	experiences with bilingual publishing: surveys of authors and editors		ABSTRACT#R##N##R##N#Bilingual publishing has become a strategy employed by journals from the non-Anglophone world to gain wider recognition. Beyond anecdotal evidence, however, there are no published accounts of the experiences of editors and authors of bilingual journals with the process of bilingual publication. It is also unclear how authors writing in bilingual journals judge the quality of the translations and whether they consider this sort of publishing as beneficial for their aims. Consequently, we carried out two surveys: one among editors of bilingual journals and one anonymous survey among authors and translators of articles published in Deutsches Arzteblatt International, the bilingual journal of the German Medical Association. Eight of nine journals as well as 233 of 353 authors and 4 of 6 translators took part. Most journals reported that bilingual publication helped in becoming indexed in important databases (e.g. Medline), receiving or improving an Impact Factor, and in attracting authors. All journals plan to continue publishing bilingually. Authors were ‘satisfied’ (40.8%) or ‘very satisfied’ (57.8%) with translations. Almost all (96.7%) were in favour of bilingual publication of their work. They did not view an English translation as an obstacle to another related English language paper. Translators highlighted challenges relating to specialized terminology and to terms specific to the regional healthcare system.		Elke Bartholomäus;Sandy Goldbeck-Wood;Meike Sewering;Christopher Baethge	2015	Learned Publishing	10.1087/20150407	computer science;linguistics;world wide web	HCI	-39.02974781204294	-67.82829937850512	149937
3aab8be232374f0671171e736e75a6fab1658597	implementing mt in the greek public sector: a users' survey	traitement automatique des langues naturelles;grece;secteur public;utilisateur;inquiry;public sector;user profile;enquete;traduction automatique;mechanical translation;evaluation;computational linguistics;linguistique informatique;natural language processing;assessment;machine translation	This paper presents the activities of Euromat (European Machine Translation) office in Greece, which has been functioning as a centre for Machine Translation Services for the Greek Public Sector since 1994. It describes the user profile, his/her attitude towards MT, strategies of promotion and the collected corpus for the first three years. User data were collected by questionnaires, interviews and corpus statistics. The general conclusions which have come out from our surveys are discussed.		Athanassia Fourla;Olga Yannoutsou	1998		10.1007/3-540-49478-2_27	natural language processing;computer science;inquiry;artificial intelligence;evaluation;computational linguistics;public sector;machine translation;operations research;educational assessment	HCI	-38.32181403097078	-77.19250363369827	150594
9596afe0cd5d180403adae464b0222111439ceb1	closed domain question answering for cultural heritage		In this paper I present my research goals and what I have obtained so far into my first year of PhD. In particular this paper is about a novel architecture for closed domain question answering and a possible application in the cultural heritage context. Unlike open domain question answering, which makes intensive use of Information Retrieval (IR) techniques, closed domain question answering systems might be built on top of a formal model with the possibility to apply formal logics and reasoning. Natural language question answering poses some nontrivial problems to tackle. We investigate such problems and propose some solutions based on AI techniques, picking the Cultural Heritage domain as a target application.	formal language;information retrieval;natural language;question answering	Bernardo Cuteri	2016			natural language processing;cultural heritage;question answering;computer science;artificial intelligence	AI	-37.247383353471584	-77.18423455804943	150942
d975c757c3f471fd1274778a27ceac805bd1de5c	nlp and online health reports: what do we say and what do we mean?		Social media sites such as microblogs and discussion board forums have the potential to be rich source of information about human health. Going beyond simple keyword search and harnessing the data for insights that can benefit public health presents both opportunities and challenges to natural language processing (NLP). In this talk I survey the progress made using NLP methods, e.g. for adverse drug reaction profiling, flu surveillance and the study of depressive disorders. I will then look at the technical challenges in understanding such messages, in particular how NLP can automatically encode/normalise laymen’s language to the formal terminologies of healthcare professionals. To this end I present state of the art results from our recent work on using deep neural networks to de-conflate word senses as well as ‘translating’ from social media messages to SNOMED-CT. I will finish by briefly reflecting on the practical and ethical challenges that lie ahead.	artificial neural network;deep learning;encode;information source;natural language processing;search algorithm;social media;systematized nomenclature of medicine	Nigel Collier	2016		10.18653/v1/W16-6111	computer science;artificial intelligence;natural language processing	NLP	-35.871324389513205	-68.62437298017498	150986
ecf9405413466b81ff709f1b147703e71b436236	web-based mass argumentation in natural language		We provide a novel framework and implementation which integrates tools to support the acquisition of mass, distributive, incremental, dynamic, argumentative knowledge in natural language. With the Attempto Controlled English (ACE) system, natural language statements are automatically translated to a machine processable form. A discussion forum allows the specification of argumentation theoretic relationships among statements. Statements and their relationships are input to a formal, implemented argumentation system, which calculates inferences from asserted premises.	ace;argumentation framework;attempto controlled english;natural language;theory	Adam Zachary Wyner;Tom M. van Engers	2010			argumentation theory;web application;natural language;natural language processing;multimedia;computer science;artificial intelligence	NLP	-36.50989387755845	-76.50548748757006	151512
d44294c552f43c8bf379621d251930081bab7d28	conceptual modeling of online entertainment programming guide for natural language interface	electronic program guide;information retrieval system;conceptual model;electronic programming guide;low complexity;metadata harvesting;specification language;natural language;natural language modeling;natural language interface;language model;linguistic properties of entertainment language	This paper describes a new novel approach to the conceptual modeling of text-based electronic programming guide (EPG) for broadcast TV programs by using a large text corpus constructed from the EPG metadata source. Two empirical experiments are carried out to evaluate the EPG-specific language models created using the new algorithm in context of natural language (NL) based information retrieval systems. The experimental results show the effectiveness of the algorithm for developing low-complexity concept models with high coverage for the user's language models associated with both typed and spoken queries when interacting with a NL based EPG search interface.	natural language user interface	Harry Chang	2010		10.1007/978-3-642-13881-2_19	natural language processing;language identification;interface description language;first-generation programming language;natural language programming;speech recognition;universal networking language;language primitive;question answering;data manipulation language;object language;specification language;natural language user interface;data control language;computer science;programming language implementation;conceptual model;linguistics;low-level programming language;modeling language;natural language;programming language;language technology;programming language specification;high-level programming language;language model	HCI	-33.80343320660477	-73.26531428839063	151638
2e709b0e81fae2ab7775d2b2c2b4fd520ea59d47	use of the chi-squared test to examine vocabulary differences in english language corpora representing seven different countries	analyse de corpus;quantitative linguistics;english language;linguistique quantitative;eprints newcastle university;language variety;lexicometry;corpus analysis;anglais;open access;lexicometrie;chi square test;english;variete linguistique;dr malcolm farrow	The chi-squared test is used to find the vocabulary most typical of seven different ICAME corpora, each representing the English used in a particular country. In a closely related study, Leech and Fallon (1992, Computer corpora - what do they tell us about culture? ICAME Journal, 16: 29-50) found differences in the vocabulary used in the Brown Corpus of American English and that the Lancaster-Oslo-Bergen Corpus of British English. They were mainly interested in those vocabulary differences which they assumed to be due to cultural differences between the United States and Britain, but we are equally interested in vocabulary differences which reveal linguistic preferences in the various countries in which English is spoken. Whether vocabulary differences are cultural or linguistic in nature, they can be used for the automatic classification according to variety of English of texts of unknown provenance. The extent to which the vocabulary differences between the corpora represent vocabulary differences between the varieties of English as a whole depends on the extent to which the corpora represent the full range of topics typical of their associated cultures, and thus there is a need for corpora designed to represent the topics and vocabulary of cultures or dialects, rather than stratified across a set range of topics and genres. This will require methods to determine the range of topics addressed in each culture, then methods to sample adequately from each topical domain.	chi;text corpus;vocabulary	Michael P. Oakes;Malcolm Farrow	2007	LLC	10.1093/llc/fql044	natural language processing;speech recognition;quantitative linguistics;computer science;english;corpus linguistics;linguistics	NLP	-36.87591254778318	-71.77784801323612	151791
7456ab9a7e2ef8752fe53f30442553c8412b9f21	natural language processing in information fusion terminology management	databases;text analysis information retrieval natural language processing;terminology organization;compounds;information extraction;information retrieval;technology;data mining terminology natural language processing dictionaries bioinformatics compounds databases;text analysis;data mining;teknik;corpus technology tools;soft data;natural language processing text databases information extraction term extraction soft data;dictionaries;text retrieval;biomedicine;term extraction;terminology;domain specific ontology;text databases;information extraction service;information extraction service natural language processing information fusion terminology management dynamic development terminology organization domain specific ontology corpus technology tools term extraction biomedicine text retrieval;natural language processing;dynamic development;information fusion terminology management;bioinformatics	The dynamic development of information fusion research implies introduction of new terms and concepts, which in turn requires tools and methods for terminology organization and standardization, as well as tools for creating domain-specific ontology. In this paper, we show how natural language processing and corpus technology tools applied for term extraction from texts in biomedicine can successfully be used for the field of information fusion. We demonstrate term and information extraction from a corpus of research articles in information fusion, showing how a vision of a combined text retrieval and information extraction service can be made real.	computer-assisted translation;document retrieval;domain-specific language;information extraction;natural language processing;terminology extraction	Elzbieta Dura;Barbara Gawronska	2008	2008 11th International Conference on Information Fusion		natural language processing;computer science;data mining;information extraction;information retrieval	DB	-34.422975938990916	-68.25109132327954	151859
7262c2fa40e6563b414d76a0708453d98426ccdc	english generation from a knowledge base and overly general classes	intelligent textbook;knowledge representation;ontology verbalization;knowledge based systems	In this paper, we describe the range of sentences that we synthesize from a knowledge base (KB) using a natural language generation (NLG) system. We give an overview of our NLG system, and then focus on the specific challenge in handling overly general classes such as Tangible-Entity and Physical-Object. Such classes are unavoidable in the KB, and it is undesirable to show them in an output for end users. Our approach has been implemented in an intelligent textbook application that helps students learn better.	computation;entity;knowledge base;knowledge representation and reasoning;natural language generation;ontology (information science);question answering;stepping level	Vinay K. Chaudhri;Nikhil Dinesh;Eva Banik;Umangi Oza	2015			natural language processing;knowledge representation and reasoning;computer science;knowledge management;artificial intelligence;knowledge-based systems;machine learning	AI	-36.751459703591614	-77.69936889053659	152757
c9c41713415673509ca5ef79c3ac691c48ad76e7	distillation of knowledge from the research literature on alzheimer's dementia	quality of life;aging society;alzheimer intervention;named entity recognition;pubmed	Many countries are aging societies. Since abilities generally deteriorate with age, technologies can assist older adults in their daily life. Loss of cognitive status is particularly severe in cases of dementia, with around 70% (according to Alzheimers.net) of dementia cases involving Alzheimer’s Dementia (AD), a progressive and currently incurable disease. There is considerable research on AD with thousands of relevant publications being added to the PubMed online database every year. The knowledge incorporated in this large body of work is spread across hundreds of thousands of pages of text, making it difficult to distill and mobilize that knowledge in terms of treatments and guidelines. Text mining technology may assist in distilling knowledge from the vast corpus of research literature on Alzheimer’s dementia. In this paper, we apply the Named Entity Recognition (NER) system, a text mining (TM) method used to group words into classes, in order to extract useful information from free texts. We present findings concerning how well NER can extract information from a corpus of AD research publications.	alzheimer's disease neuroimaging initiative;named entity;named-entity recognition;pubmed;scientific literature;text corpus;text mining	Wutthipong Kongburan;Mark H. Chignell;Jonathan H. Chan	2017		10.1145/3041021.3054934	natural language processing;quality of life	NLP	-47.00981783997055	-68.08556112296979	152877
7ade065dcfc1c1771d7c4110bebdb5c05f28baba	synchronization and combination techniques for audio-video based handwritten mathematical content recognition in classroom videos	audio video classifier combination;classifier combination;audio signal processing;handwriting recognition;video signal processing;computer aided instruction;videos synchronization character recognition accuracy video recording handwriting recognition speech recognition;audio video;audio video classifier combination handwriting recognition speech recognition;video recording;open source text recognizer combination techniques synchronization techniques audio video based handwritten mathematical content recognition classroom videos handwritten text audio based recognizer video based recognizer spoken content;speech recognition;video signal processing audio signal processing computer aided instruction handwriting recognition;character recognition;open source	Recognizing handwritten mathematical content is a challenging problem, and more so when such content appears in classroom videos. However, given the fact that in such videos the handwritten text and the accompanying audio refer to the same content, a combination of a video and an audio based recognizer has the potential to significantly improve the content recognition accuracy. In this paper, using a combination of video and audio based recognizers, we focus on improving the character recognition accuracy in such videos and propose: (1) synchronization techniques for establishing a correspondence between the handwritten and the spoken content, and (2) combination techniques for combining the outputs of the video and audio based recognizers. The current implementation of the system makes use of a modified open source text recognizer and a commercially available phonetic word-spotter. For evaluation purposes, we use videos recorded in a classroom-like environment and our experiments demonstrate the significant improvements (≈ 24% relative increase as compared to the baseline video based recognizer) in character recognition accuracy that can be achieved using our techniques.	baseline (configuration management);experiment;finite-state machine;open-source software;optical character recognition;word-sense disambiguation	Smita Vemulapalli;Monson H. Hayes	2011	2011 11th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2011.6121779	audio mining;speech recognition;audio signal processing;intelligent character recognition;computer science;video tracking;pattern recognition;multimedia;video processing;handwriting recognition	Vision	-35.29048601879915	-73.15304361695226	152958
474933e9de62ca8b5654b73ab45bb9d0e8ea8c61	research on natural-language processing: sri international	commonsense reasoning;natural language semantics;spectrum;language processing;natural language;defense advanced research project agency;basic research;natural language processing	Research on natural-language processing at SRI spans a broad spectrum of activity. Two of our major current efforts are a pair of research projects under the sponsorship of the Defense Advanced Research Projects Agency. The TEAM project is intended to provide natural-language access to large databases via systems that are easily adaptable to a wide range of new application domains. The KLAUS project is a longer-range effort to address basic research problems in natural-language semantics, commonsense reasoning, and the pragmatics of natural-language communication. These two projects share a common core-language-processing system called DIALOGIC.	application domain;commonsense reasoning;database;natural language processing	Barbara J. Grosz	1982	SIGART Newsletter	10.1145/1056663.1056723	natural language processing;spectrum;commonsense reasoning;computer science;artificial intelligence;linguistics;natural language;programming language	SE	-34.343552806875486	-77.20045868852458	153384
5bcdac6e4894647ea4a2990e1462cc36f2c9dab9	toward algorithmic discovery of biographical information in local gazetteers of ancient china		Difangzhi (地方志) is a large collection of local gazetteers complied by local governments of China, and the documents provide invaluable information about the host locality. This paper reports the current status of using natural language processing and text mining methods to identify biographical information of government officers so that we can add the information into the China Biographical Database (CBDB), which is hosted by Harvard University. Information offered by CBDB is instrumental for human historians, and serves as a core foundation for automatic tagging systems, like MARKUS of the Leiden University. Mining texts in Difangzhi is not easy partially because there is litter knowledge about the grammars of literary Chinese so far. We employed techniques of language modeling and conditional random fields to find person and location names and their relationships. The methods were evaluated with realistic Difangzhi data of more than 2 million Chinese characters written in literary Chinese. Experimental results indicate that useful information was discovered from the current dataset.	china biographical database (cbdb);conditional random field;language model;locality of reference;natural language processing;text mining	Chao-Lin Liu;Chih-Kai Huang;Hongsu Wang;Peter K. Bol	2015			data science;data mining;locality;china;chinese characters;conditional random field;government;language model;text mining;computer science;rule-based machine translation	NLP	-37.23495435056973	-70.7608363009934	154021
7304afb491930d520e194f4ca8b91a9652f0e658	medsts: a resource for clinical semantic textual similarity		The wide adoption of electronic health records (EHRs) has enabled a wide range of applications leveraging EHR data. However, the meaningful use of EHR data largely depends on our ability to efficiently extract and consolidate information embedded in clinical text where natural language processing (NLP) techniques are essential. Semantic textual similarity (STS) that measures the semantic similarity between text snippets plays a significant role in many NLP applications. In the general NLP domain, STS shared tasks have made available a huge collection of text snippet pairs with manual annotations in various domains. In the clinical domain, STS can enable us to detect and eliminate redundant information that may lead to a reduction in cognitive burden and an improvement in the clinical decision-making process. This paper elaborates our efforts to assemble a resource for STS in the medical domain, MedSTS. It consists of a total of 174,629 sentence pairs gathered from a clinical corpus at Mayo Clinic. A subset of MedSTS (MedSTS_ann) containing 1,068 sentence pairs was annotated by two medical experts with semantic similarity scores of 0-5 (low to high similarity). We further analyzed the medical concepts in the MedSTS corpus, and tested four STS systems on the MedSTS_ann corpus. In the future, we will organize a shared task by releasing the MedSTS_ann corpus to motivate the community to tackle the real world clinical problems.	embedded system;natural language processing;semantic similarity	Yanshan Wang;Naveed Afzal;Sunyang Fu;Liwei Wang;Feichen Shen;Majid Rastegar-Mojarad;Hongfang Liu	2018	CoRR		natural language processing;semantic similarity;snippet;artificial intelligence;cognition;sentence;computer science	NLP	-34.258666119018415	-68.7704281578951	154104
0d4c97305019a796b1bd885971518bb53e0e787b	invited talk: lessons from the malach project: applying new technologies to improve intellectual access to large oral history collections		In this talk I will describe the goals of the MALACH project (Multilingual Access to Large Spoken Archives) and our research results. I'll begin by describing the unique characteristics of the oral history collection that we used, in which Holocaust survivors, witnesses and rescuers were interviewed in several languages. Each interview has been digitized and extensively catalogued by subject matter experts, thus producing a remarkably rich collection for the application of machine learning techniques. Automatic speech recognition techniques originally developed for the domain of conversational telephone speech were adapted to process these materials with word error rates that are adequate to provide useful features to support interactive search and automated clustering, boundary detection, and topic classification tasks. As I describe our results, I will focus particularly on the evaluation methods that that we have used to assess the potential utility of this technology. I'll conclude with some remarks about possible future directions for research on applying new technologies to improve intellectual access to oral history and other spoken word collections. Maryland, and his research interests center around the use of emerging technologies to support information seeking by end users. Dr. Oard's recent work has focused on interactive techniques for cross­language information retrieval, searching conversational media, and leveraging observable behavior to improve user modeling. Additional information is available at	cluster analysis;information retrieval;information seeking;machine learning;observable;speech recognition;subject matter expert turing test;subject-matter expert;usability;user modeling	Douglas W. Oard	2007			library science;engineering;management science;world wide web	Web+IR	-34.92482804682245	-77.44834736815731	154149
062654fcb17f98ef19033d3a125609b5e7449fb5	finding difficult-to-disambiguate words: towards an efficient workflow to implement word sense disambiguation	medical informatics word sense disambiguation natural language processing;medical informatics;training;vocabulary;data mining;word sense disambiguation;natural language processing vocabulary data mining training informatics benchmark testing bioinformatics;clinical domain difficult to disambiguate words biomedical domain easy to disambiguate words wsd word sense disambiguation structured information extraction nlp natural language processing free text documents;informatics;text analysis information retrieval medical information systems natural language processing;natural language processing;benchmark testing;bioinformatics	In the biomedical and clinical domain, valuable information is frequently represented in free-text documents. Natural language processing (NLP) is a powerful tool that can extract structured information from theses documents. Word sense disambiguation (WSD) is a critical component in an NLP pipeline that increases the accuracy of the extracted information. However, WSD is expensive to apply for all known ambiguous words. Given limited time and resources, one practical strategy is to prioritize easy-to-disambiguate words and efficiently maximize the coverage of disambiguation. To aid prioritization efforts, we studied two quantitative indicators that are associated with how easy/difficult it is to disambiguate any given word.	apply;natural language processing;web services for devices;word sense;word-sense disambiguation	Manabu Torii;Jung-Wei Fan;Daniel Zisook	2015	2015 International Conference on Healthcare Informatics	10.1109/ICHI.2015.66	natural language processing;speech recognition;semeval;computer science;information retrieval	NLP	-34.132049746408235	-69.03426065941736	154381
b0f21f2d5683649fc49dbe0b29a744db080cb242	semantic descriptions of 24 evaluational adjectives, for application in sentiment analysis		We apply the Natural Semantic Metalanguage (NSM) approach (Goddard & Wierzbicka 2014) to the lexical-semantic analysis of English evaluational adjectives and compare the results with the picture developed in the Appraisal Framework (Martin & White 2005). The analysis is corpus-assisted, with examples mainly drawn from film and book reviews, and supported by collocational and statistical information from WordBanks Online. We propose NSM explications for 24 evaluational adjectives, arguing that they fall into five groups, each of which corresponds to a distinct semantic template. The groups can be sketched as follows: “First-person thought-plusaffect”, e.g. wonderful; “Experiential”, e.g. entertaining; “Experiential with bodily reaction”, e.g. gripping; “Lasting impact”, e.g. memorable; “Cognitive evaluation”, e.g. complex, excellent. These groupings and semantic templates are compared with the classifications in the Appraisal Framework’s system of Appreciation. In addition, we are particularly interested in sentiment analysis, the automatic identification of evaluation and subjectivity in text. We discuss the relevance of the two frameworks for sentiment analysis and other language technology applications.	automatic identification and data capture;language technology;relevance;sentiment analysis	Cliff Goddard;Maite Taboada;Radoslava Trnavac	2016	CoRR		natural language processing;semantic role labeling;semantic similarity;computer science;artificial intelligence;machine learning;linguistics	NLP	-34.148270620056955	-78.6624197563938	154627
271373efd91776887bf885fafe993b12e4e7ecd2	semantic components and metaphorization		A finite set of semantic components is assumed to underlie the surface level of a particular language, which amounts to its being independent of the particular semantic structure of any given language (Lyons 1969: 472). The assumption of the existence of a universal set of semantic components is frequently attacked by some linguists, but the description of semantic structure of a particular language is not necessarily impaired if the semantic components as such turn out not to be universal. Criticism often aimed at other weak points of the method of componential analysis as well, namely at its atomism, at the questionable psychological or cognitive relevance of the established sets of distinctive features, and at a certain lack of systematicity in the organization of meaning structure (cf. Nida 1975). However, the fuzziness of meaning is not a defect of language, but rather its necessary and useful property that makes vocabulary flexible and its expansion possible. It is not without significance that while the componential analysis of meaning is usually criticized on theoretical grounds, those who concentrate upon primary data – like for example ethnolinguists – do appreciate its advantages and feel that the method, instead of being abandoned, deserves elaboration and refinement. The subsequent microanalysis of lexical metaphors based mainly upon the vocabulary of Indonesian (Echols & Shadily 1963; Korigodskij 1990) and of Maori (e.g. Williams 1957), with a few occasional examples from other languages, confirms at least a partial psychological reality of semantic components indicating at the same time that the latter are not clearly delimited atomic entities. Instead, there is a good deal of overlapping and transition between them. The metaphorical transfer exceeds the limits of particularism; lexical metaphors are applied in clusters, as whole models (cf. Lakoff & Johnson 1980). Solitary, isolated metaphorizations are rare – pragmatically based semantic models like human body, family, natural elements, flora, etc. are the rule. Our cognitive basis is to be sought in the interface between the human being and his/her immediate surroundings. It is here that our first cognitive hypotheses arise and are subsequently applied to less accessible phenomena	delimiter;entity;korea internet & security agency;lexicon;refinement (computing);relevance;software bug;vocabulary	Viktor Krupa	2007		10.1515/9783110894219.353		NLP	-34.71243356242765	-79.97080806218169	155782
4679e44e7c7932f2572f39e1a74b23838271baf9	a syntactic and lexicon analyzer for the geography markup language (gml)	extensible markup language;xml schema;geography markup languages xml geographic information systems solid modeling computational geometry topology coordinate measuring machines measurement units time measurement;geographic information system;geographic information;user interface;visual basic syntactic analyzer lexicon analyzer geography markup language open gis consortium geographic information systems extensible markup language grammar xml schema geographic information modeling geographic information transport geographic information storage geographic phenomenon features coordinate reference systems geometry topology rivers lakes roads streets buildings city states shapes mountains geographic objects written documents user interface text editor spanish french portuguese english italian web searching executable software syntactic analysis lexicon analysis;geography markup language;natural languages;reference systems;visual basic;geographic information systems;xml;system development;computational linguistics;visual basic geographic information systems geography computational linguistics natural languages xml;geography	The geography markup language (GML), developed by Open GIS (Geographic Information Systems) Consortium, is currently in version 3.1. GML is an XML (Extensible Markup Language) grammar written in XML Schema for the modeling, transport, and storage of geographic information. The modeling of geographic phenomenon is based on different kinds of objects such as: features, coordinate reference systems, geometry, topology, time, units of measure and generalized values. With the feature objects we can model all geographic objects, like rivers, lakes, roads, streets, buildings, cities and state shapes, mountains, etc. These objects are represented through documents written in GML, and their properties are registered by parameters, like name, coordinates, dimension, extension, location, etc. The proposal of this work is to develop a syntactic and lexicon analyzer for GML, aimed to assist environmental professionals in the verification of the coherence of their written documents. The user interface of the system is a text editor, presenting all GML tags, as well as syntactic and lexicon analysis functions. The program also has an option for exhibit the GML tags in the following languages: Spanish, French, Portuguese, English and Italian. Searching the Web, we found only one analyzer for GML documents, which directly executed the GML interpretation in the site, not supplying an executable software for solving that task. Given the GML relevance, we thought about creating an executable software that provided GML syntactic and lexicon analysis. We have chosen for system development the Visual Basic, in its seventh version, due its simplicity and popularity (Cortez and Scherr, 2001; Petroutsos, 2001).	consortium;executable;geographic information system;geography markup language;lexicon;relevance;text editor;user interface;visual basic;world wide web;xml schema	João Araújo Ribeiro;Oscar Luiz Monteiro de Farias;Luiz Alberto Oliveira Lima Roque	2004	IGARSS 2004. 2004 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2004.1370299	natural language processing;xml;geography markup language;computer science;database;geographic information system;world wide web;web feature service	SE	-35.19029222350213	-78.71359519854562	155856
d53d732c6312a18b748c90185153028336d5f5f4	annotating preferences in chats for strategic games	online version;competitive game settlers;annotating preference;annotation scheme;strategic game;bargaining negotiation;on-line chat	This paper describes an annotation scheme for expressions of preferences in on-line chats concerning bargaining negotiations in the online version of the competitive game Settlers of Catan.	game theory;natural language processing;online and offline;parsing;real life;visual instruction set	Anaïs Cadilhac;Nicholas Asher;Farah Benamara	2012			simulation;computer science;multimedia;world wide web	NLP	-40.91747738436456	-75.27352451501052	156124
d38ecb68af68d377a637b2f0af74d3d90a9025a2	designing a system for semi-automatic population of knowledge bases from unstructured text	knowledge base	Important information from unstructured text is typically entered manually into knowledge bases, resulting in limited quantities of data. Automated information extraction from the text could assist with this process, but the technology is still at unacceptable accuracies. This task therefore requires a suitable user interface to allow for correction of the frequent extraction errors and validation of proposed assertions that a user wants to enter into a knowledge base. In this paper, we discuss our system for semi-automatic database population and how it handles the issues arising in content extraction and populating a knowledge base. The main contributions of this work are identifying the challenges in building such a semi-automated tool, the categorization of extraction errors, addressing the gaps in current extraction technology required for databasing, and the design and development of a usable interface and system, FEEDE, to support correcting content extraction output and speeding up the data entry time into knowledge bases. To our knowledge, this is the first effort to populate knowledge bases using content extraction from unstructured text.	categorization;information extraction;knowledge base;population;semiconductor industry;user interface	Jade Goldstein-Stewart;Ransom K. Winder	2009			knowledge base;computer science;knowledge management;artificial intelligence;data science;knowledge-based systems;open knowledge base connectivity;data mining;knowledge extraction;domain knowledge	AI	-36.45314604568197	-68.16099674331106	156927
48be7b9028ff0fe334c9af69827d8d097ef19730	a guided template-based question answering system over knowledge graphs		Question answering systems provide easy access to structured data, in particular RDF data. However, the user experience is often negatively affected by questions that are not interpreted correctly. To remedy this, we present a new guided approach to QA that ensures that all questions that can be entered into the system also return a corresponding answer. For this, a template-based approach is used to generate all possible questions from a given RDF dataset using a number of templates. The question/answer pairs can then be indexed to provide autocompletion functionality at querying time. We describe the architecture and approach and present preliminary evaluation results.	accessibility;clinical use template;graph - visual representation;index;knowledge graph;question answering;resource description framework;silo (dataset);user experience	Lukas Biermann;Sebastian Walter;Philipp Cimiano	2018				Web+IR	-47.08016087391496	-66.643012186411	158421
f42ed1f27a087d4563cd4810928ebab546e473ca	encoding historical dates correctly: is it practical, and is it worth it?				Martin Holmes;Janelle Jenstad;Cameron Butt	2013			encoding (memory);pattern recognition;artificial intelligence;computer science	Crypto	-37.48247803496917	-74.22962891182961	158585
3558eef0d3cf805f4c5972d599ef6f2d68c30f3e	question bank calibration using unsupervised learning of assessment performance metrics		In traditional e-assessments as well as in personalized learning, question bank calibration plays an important role in ensuring high-quality assessment outcomes. We propose an unsupervised learning approach that uses performance metrics derived from test-taker responses for precise calibration of question banks. We show partitioning test-takers into three groups using their scores is an effective feature extraction method. This approach, enables us to accurately capture performance variations in test-taker sub-groups thereby effectively capturing inter-item correlations. Integration of such a Machine Learning (ML) approach into the Question Bank module of a cloud-based e-assessment system enables seamless automatic question calibration. We validate the effectiveness of our approach, using a large dataset, collected from a two thousand student university-level proctored assessment.	association rule learning;baseline (configuration management);cloud computing;cluster analysis;disk partitioning;feature extraction;gene ontology term enrichment;machine learning;massive open online course;overhead (computing);performance;personalization;refinement (computing);seamless3d;unsupervised learning	Sankaran Narayanan;Vamsi Sai Kommuri;N. Sethu Subramanian;Kamal Bijlani	2017	2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2017.8125810	control engineering;computer science;supervised learning;calibration;machine learning;feature extraction;cloud computing;unsupervised learning;personalized learning;artificial intelligence	SE	-44.44195228239201	-70.98693474346673	158774
adfa593a12bf10a4a81e99e3b70d155709473b68	discovering characteristic patterns from collections of classical japanese poems	hardware and architecture;theoretical computer science;characteristic patterns;computational theory and mathematics;pattern analysis;rhetorical devices;classical japanese poems;mdl principle;machine discovery	Waka is a form of traditional Japanese poetry with a 1300-year history. In this paper, we attempt to discover characteristics common to a collection ofwaka poems. As a schema for characteristics, we use regular patterns where the constant parts are limited to sequences of auxiliary verbs and postpostional particles. We call such patternsfushi. The problem is to automate the process of finding significantfushi patterns that characterize the poems. Solving this problem requires a reliable significance measure for the patterns. Brāzma et al. (1996) proposed such a measure according to the MDL principle. Using this method, we report successful results in finding patterns from five anthologies. Some of the results are quite stimulating, and we hope that they will lead to new discoveries.	database schema;mdl (programming language)	Mayumi Yamasaki;Masayuki Takeda;Tomoko Fukuda;Ichiro Nanri	2000	New Generation Computing	10.1007/BF03037569	computer science;artificial intelligence;rhetorical device;algorithm	Web+IR	-38.836077296825835	-71.88437013081136	159338
1385e12d64912f501f1a71d7c63338e0d1320b74	orchive: digitizing and analyzing orca vocalizations	digital archive;development tool;region of interest;killer whale	This paper describes the process of creating a large digital archive of killer whale or orca vocalizations. The goal of the project is to digitize approximately 20000 hours of existing analog recordings of these vocalizations in order to facilitate access to researchers internationally. We are also developing tools to assist content-based access and retrieval over this large digital audio archive. After describing the logistics of the digitization process we describe algorithms for denoising the vocalizations and for segmenting the recordings into regions of interest. It is our hope that the creation of this archive and the associated tools will lead to better understanding of the acoustic communications of Orca communities worldwide.	acoustic cryptanalysis;algorithm;archive;logistics;noise reduction;orca;region of interest	George Tzanetakis;Mathieu Lagrange;Paul Spong;Helena Symonds	2007			speech recognition;engineering;communication;cartography	HCI	-45.61091154428178	-77.3456547150897	159736
052aff8a24b6c0e515d9299ebf0f5594cbb91e01	what's yours and what's mine: determining intellectual attribution in scientific text	global argumentative structure;scientific text;intellectual attribution;prototypical agent;citation index;particular feature;important aspect;automatic summarization;scientific argumentation;automated construction;scientific statement;indexation;information technology;computer science	"""We believe that identifying the structure of scientific argumentation in articles can help in tasks such as automatic summarization or the automated construction of citation indexes. One particularly important aspect of this structure is the question of who a given scientific statement is attributed to: other researchers, the field in general, or the authors themselves. We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text. In this paper we concentrate on two particular features, namely the occurrences of prototypical agents and their actions in scientific text. 1 I n t r o d u c t i o n When writing an article, one does not normally go straight to presenting the innovative scientific claim. Insteacl, one establishes other, wellknown scientific facts first, which are contributed by other researchers. Attribution of ownership often happens explicitly, by phrases such as """"Chomsky (1965) claims that"""". The question of intellectual attribution is important for researchers: not understanding the argumentative status of part of the text is a common problem for nonexperts reading highly specific texts aimed at experts (Rowley, 1982). In particular, after reading an article, researchers need to know who holds the """"knowledge claim"""" for a certain fact that interests them. We propose that segmentation according to intellectual ownership can be done automatically, and that such a segmentation has advantages for various shallow text understanding tasks. At the heart of our classification scheme is the following trisection: * BACKGROUND (generally known work)"""	algorithm;automatic summarization;cellular automaton;citation index;need to know	Simone Teufel;Marc Moens	2000			natural language processing;computer science;data science;automatic summarization;machine learning;data mining;information technology;information retrieval	NLP	-37.53477168547197	-67.58051070843328	161851
d22ac65af3598942dc9ee5b2849e889dbbde1042	automatic detection and correction of spelling errors in a large data base	automatic detection;large data	Abstract#R##N##R##N#On-line bibliographic search systems tend to increase the visibility of spelling errors through the use of indexes of unique terms; even low error rates in a data base can result in large numbers of misspelled terms in these indexes. This article describes the techniques used to detect and correct spelling errors in the data base of Chemical Abstracts Service. A computer program for spelling error detection achieves a high level of performance using hashing techniques for dictionary look-up and compression. Heuristic procedures extend the dictionary and increase the proportion of misspelled words in the words flagged. Automatic correction procedures are applied only to words which are known to be misspelled; other corrections are performed manually during the normal editorial cycle. The constraints imposed on the selection of a spelling error detection technique by a complex data base, human factors, and high-volume production are discussed.	database	Antonio Zamora	1980	JASIS	10.1002/asi.4630310106	speech recognition;computer science;data mining;database;information retrieval	NLP	-39.87443288914856	-74.22503867793935	162494
8cb221ec653ff1eed087bf0426d6a5390d5e5ecc	question answering from lecture videos based on automatically-generated learning objects	lecture recording;video segmentation;automatic generation;semantic metadata;question answering system;learning object;video recording;speech recognition;question answering	In the past decade, we have witnessed a dramatic increase in the availability of online academic lecture videos. There are technical problems in the use of recorded lectures for learning: the problem of easy access to the multimedia lecture video content and the problem of finding the semantically appropriate information very quickly. The retrieval of audiovisual lecture recordings is a complex task comprising many objects. In our solution, speech recognition is applied to create a tentative and deficient transcription of the lecture video recordings. The transcription and the words from the power point slides are sufficient to generate semantic metadata serialized in an OWL file. Each video segment (the lecturer is speaking about one power point slide) represent a learning object. A question-answering system based on these learning objects is presented. The annotation process is discussed, evaluated and compared to a perfectly annotated OWL file and, further, to an annotation based on a corrected transcript of the lecture. Furthermore, the consideration of the chronological order of the learning objects leads to a better  MRR value. Our approach out-performs the Google Desktop Search based on the question keywords.	question answering	Stephan Repp;Serge Linckels;Christoph Meinel	2008		10.1007/978-3-540-85033-5_50	question answering;computer science;artificial intelligence;database;multimedia;world wide web;information retrieval	AI	-35.41627782792977	-73.07171659405935	162694
92907c2a3db3aaec0ae6a1aeab8302dd7ab55e91	transportable english-language processing for office environments	english language;system design;early development;natural language processor	This article describes the Layered Domain Class system (LDC), a state-of-the-art natural language processor whose major goals are (1) to provide English-language retrieval capabilities for medium-sized office domains that have been stored on the computer as text-edited files, rather than more restrictive database structures and (2) to eliminate the need to call in the system designer when extensions into new domains are desired, without sacrificing the depth or reliability of the interface. Early developments in the design of portions of LDC were presented at NCC-83, and the entire system became operational in July 1983. The article gives an overview of the construction of the system, gives examples of the English structures provided for, briefly describes the most recently completed portions of the system, and mentions current directions the project is taking.	linguistic data consortium;natural language processing;systems design	Bruce W. Ballard;John C. Lusth;Nancy L. Tinkham	1984		10.1145/1499310.1499393	simulation;engineering;engineering drawing	SE	-35.28260174513745	-77.68211924544127	162869
24213cb70f32516a6e9f06acabd2946dc6f99f90	the eighth annual mlsp competition: overview	recommender systems eighth annual mlsp competition machine learning for signal processing technical committee data analysis competition annual mlsp workshop amazon corporation employee access company resources real training data disjoint test data conference proceedings automatic access provisioning collaborative filtering;competition;authorisation;machine learning competition automatic access provisioning recommender systems collaborative filtering;data analysis;machine learning;collaborative filtering;companies signal processing algorithms error analysis machine learning conferences manuals signal processing;business data processing;signal processing;learning artificial intelligence;recommender systems;signal processing authorisation business data processing collaborative filtering data analysis learning artificial intelligence recommender systems;automatic access provisioning	This marks the eighth year the Machine Learning for Signal Processing (MLSP) Technical Committee has hosted a data analysis competition, which is held in conjunction with the annual MLSP workshop. For this year's competition, which was sponsored by Amazon Corporation, entrants were asked to write an algorithm that attempts to automatically provision an employee's access to company resources in an optimal manner. In this paper, we (the organizers of the competition) briefly describe the application, the data, the rules, and the outcomes of the competition. A total of 4 teams entered the contest. We provided real (declassified) training data to the entrants and tested the algorithms using disjoint test data. The two teams with the best performing entries describe the approach they used in two separate companion papers, both of which appear in this year's conference proceedings.	algorithm;machine learning;signal processing;test data	Ken Montanez;Weifeng Liu;Vince D. Calhoun;Catherine Huang;Kenneth E. Hild	2012	2012 IEEE International Workshop on Machine Learning for Signal Processing	10.1109/MLSP.2012.6349769	computer science;data science;data mining;world wide web	ML	-41.68377949070486	-70.7568693651129	162999
b64a70165f342c13ee4d9d33be7c9a8b863811e2	deduce: a pattern matching method for automatic de-identification of dutch medical text		In order to use medical text for research purposes, it is necessary to de-identify the text for legal and privacy reasons. We report on a pattern matching method to automatically de-identify medical text written in Dutch, which requires a low amount of effort to be hand tailored. First, a selection of Protected Health Information (PHI) categories is determined in cooperation with medical staff. Then, we devise a method for de-identifying all information in one of these PHI categories, that relies on lookup tables, decision rules and fuzzy string matching. Our de-identification method DEDUCE is validated on a test corpus of 200 nursing notes and 200 treatment plans obtained from the University Medical Center Utrecht (UMCU) in the Netherlands, achieving a total micro-averaged precision of 0.814, a recall of 0.916 and a F1-score of 0.862. For person names, a recall of 0.964 was achieved, while no names of patients were missed.	de-identification;pattern matching	Vincent Menger;Floor Scheepers;Lisette Maria van Wijk;Marco Spruit	2018	Telematics and Informatics	10.1016/j.tele.2017.08.002	decision rule;approximate string matching;nursing notes;information retrieval;pattern matching;computer science;recall;protected health information;de-identification;lookup table	NLP	-48.173402384320845	-68.76619028995357	163052
4efc1f68ed90d1d76fdab62039a6bddaefee9ad5	evaluation of wordnet as a source of lay knowledge for molecular biology and genetic diseases: a feasibility study	genetic diseases inborn;databases genetic;health education;molecular biology;humans;feasibility studies	OBJECTIVES While several sources of biomedical knowledge are available, these resources are often highly specialized and usually not suitable for a lay audience. This paper evaluates whether concepts needed for molecular biology and genetic diseases are present in WordNet, the electronic lexical database.   METHODS Terms for four broad categories of concepts (phenotype, molecular function, biological process, and cellular component) were extracted from LocusLink and mapped to WordNet. All terms from the Gene Ontology database (gene products and ontology concepts) were also mapped to WordNet in order to evaluate its global coverage of the domain. Additionally, we tested two methods for improving the mapping of genetic disease names to WordNet.   RESULTS The coverage of concepts ranged from 0% (gene product symbols) to 2.8% (cellular components). Removing specialization markers from the terms and using synonyms significantly increased the rate of mapping of genetic disease names to WordNet.   CONCLUSIONS Many of the most common single gene disorders are present in WordNet, as well as many high-level concepts in Gene Ontology. Therefore, WordNet is likely to be a useful source of lay knowledge in the framework of a consumer health information system on genetic diseases.	categories;extraction;gene ontology;hereditary diseases;high- and low-level;information system;lexical database;molecular biology;name;partial template specialization;proteins;wordnet;cellular_component	Olivier Bodenreider;Anita Burgun-Parenthoine;Joyce A. Mitchell	2003	Studies in health technology and informatics	10.3233/978-1-60750-939-4-379	natural language processing;computer science;bioinformatics;data mining	Comp.	-34.97351910897028	-69.39961356522251	163215
798f143d0dc50c4c1b02a3d7b61d20c9b70e7025	automated correction for syntax errors in programming assignments using recurrent neural networks		We present a method for automatically generating repair feedback for syntax errors for introductory programming problems. Syntax errors constitute one of the largest classes of errors (34%) in our dataset of student submissions obtained from a MOOC course on edX. The previous techniques for generating automated feedback on programming assignments have focused on functional correctness and style considerations of student programs. These techniques analyze the program AST of the program and then perform some dynamic and symbolic analyses to compute repair feedback. Unfortunately, it is not possible to generate ASTs for student programs with syntax errors and therefore the previous feedback techniques are not applicable in repairing syntax errors. We present a technique for providing feedback on syntax errors that uses Recurrent neural networks (RNNs) to model syntactically valid token sequences. Our approach is inspired from the recent work on learning language models from Big Code (large code corpus). For a given programming assignment, we first learn an RNN to model all valid token sequences using the set of syntactically correct student submissions. Then, for a student submission with syntax errors, we query the learnt RNN model with the prefix token sequence to predict token sequences that can fix the error by either replacing or inserting the predicted token sequence at the error location. We evaluate our technique on over 14, 000 student submissions with syntax errors. Our technique can completely repair 31.69% (4501/14203) of submissions with syntax errors and in addition partially correct 6.39% (908/14203) of the submissions.	artificial neural network;correctness (computer science);language model;massive open online course;random neural network;recurrent neural network;syntax error;edx	Sahil Bhatia;Rishabh Singh	2016	CoRR		computer science;theoretical computer science;programming language;homoiconicity;algorithm;abstract syntax tree;syntax error	PL	-40.61524786884891	-76.7600961887238	163829
27807e830e72645113328b9fa3f81659b3403d90	orthographic structuring of human speech and texts: linguistic application of recurrence quantification analysis		A methodology based upon recurrence quantification analysis is proposed for the study of orthographic structure of written texts. Five different orthographic data sets (20th century Italian poems, 20th century American poems, contemporary Swedish poems with their corresponding Italian translations, Italian speech samples, and American speech samples) were subjected to recurrence quantification analysis, a procedure which has been found to be diagnostically useful in the quantitative assessment of ordered series in fields such as physics, molecular dynamics, physiology, and general signal processing. Recurrence quantification was developed from recurrence plots as applied to the analysis of nonlinear, complex systems in the physical sciences, and is based on the computation of a distance matrix of the elements of an ordered series (in this case the letters consituting selected speech and poetic texts). From a strictly mathematical view, the results show the possibility of demonstrating invariance between different language exemplars despite the apparent low-level of coding (orthography). Comparison with the actual texts confirms the ability of the method to reveal recurrent structures, and their complexity. Using poems as a reference standard for judging speech complexity, the technique exhibits language independence, order dependence and freedom from pure statistical characteristics of studied sequences, as well as consistency with easily identifiable texts. Such studies may provide phenomenological markers of hidden structure as coded by the purely orthographic level. Institute for Complexity Studies,American University, Rome, Italy TCE Laboratory,Istituto Superiore di Sanita, V.le Regina Elena 299 Rome, Italy Department of Physiology, Loyola University, 2160 S. First Ave., Maywood, IL 60153 USA Department of Molecular Biophysics and Physiology, Rush University, 1653 W. Congress, Chicago, IL 60612 USA	complex systems;computation;distance matrix;documentation;high- and low-level;ibm translation control entry;molecular dynamics;nonlinear system;orthographic projection;recurrence plot;recurrence quantification analysis;recurrence relation;regina;signal processing	Franco Orsucci;Kimberly Walter;Alessandro Giuliani;Charles L. Webber;Joseph P. Zbilut	1997	CoRR		natural language processing;speech recognition;computer science;artificial intelligence;machine learning;linguistics	NLP	-35.77688968684163	-77.29452659431553	163830
f2e43619e3269c5e537e0b49614fab0fa5391690	development of interactive virtual voice portals to provide municipal information		In this paper, we describe a Voice Portal designed to provide municipal information by phone. It includes the set of modules required to automatically recognize users’ utterances, understand their meaning, decide the following response and generate a speech response. The different functionalities include to consult information about the City Council, access city information, carry out several steps and procedures, complete surveys, access citizen’s mailbox to leave messages for suggestions and complaints, and be transferred to the City Council to be attended by a teleoperator. The voice portal is, therefore, pioneer in offering an extensive and comprehensive range of public services accessible through speech, creating a new communication channel which is useful, efficient, and easy to use. In addition, the voice portal improves the support of public services by increasing the availability, flexibility, control and reducing costs and missed calls. The paper describes the application software, infrastructures required for its operation 24 hours a day, and preliminary results of a quality assessment.	portals	David Griol;María García-Jiménez	2012		10.1007/978-3-642-28765-7_20	speech recognition	HPC	-42.77466623241871	-78.17673160023396	164237
c49706089c55984e20c4b05a0293226cd0361d47	implementation and evaluation of a negation tagger in a pipeline-based system for information extraction from pathology reports	information extraction;natural language processing	We have developed a pipeline-based system for automated annotation of Surgical Pathology Reports with UMLS terms that builds on GATE – an open-source architecture for language engineering. The system includes a module for detecting and annotating negated concepts, which implements the NegEx algorithm – an algorithm originally described for use in discharge summaries and radiology reports. We describe the implementation of the system, and early evaluation of the Negation Tagger. Our results are encouraging. In the key Final Diagnosis section, with almost no modification of the algorithm or phrase lists, the system performs with precision of 0.84 and recall of 0.80 against a gold-standard corpus of negation annotations, created by modified Delphi technique by a panel of pathologists. Further work will focus on refining the Negation Tagger and UMLS Tagger and adding additional processing resources for annotating freetext pathology reports.	algorithm;brill tagger;delphi method;discharger;gate;information extraction;open-source software;pipeline (computing);radiology;sensor	Kevin J. Mitchell;Michael J. Becich;Jules J. Berman;Wendy W. Chapman;John R. Gilbertson;Dilip Gupta;James Harrison;Elizabeth Legowski;Rebecca S. Crowley	2004		10.3233/978-1-60750-949-3-663	natural language processing;speech recognition;computer science;information extraction;information retrieval	NLP	-33.98793078155065	-74.11009527683517	164274
025167a18aaa035ed7b5cefe95d82eed2b29618f	multimedia presentation of grammatical description: design issues		In this paper, I argue that grammatical description of language is a type of information which is ideally suited to presentation as a multimedia object structured with hypertext. I examine three existing language resources, constructed for different audiences, and discuss various features of each which bear on the design issues relevant to grammatical description. From my examination of these exemplars, I argue for four guidelines in the design of a multimedia grammar: data centricity, multiple linking, exhaustive coding in data structures, and user control of the amount of information accessed.	data structure;hypertext;user interface	Simon Musgrave	2005			natural language processing;computer science;multimedia;communication	HCI	-35.60121905551931	-76.49579727882121	164282
1d9e207444667851622388654b053dd426c5aa42	meeting medical terminology needs-the ontology-enhanced medical concept mapper	medical libraries;digital signal processing;natural language interfaces;information resources;information sources;parsing computer grammar;noun;software libraries;information retrieval;medical tests;user study;information technology;vocabulary;semantic parsing;umls;natural language interfaces nomenclature vocabulary medical information systems information resources information retrieval;indexing terms;terminology mapping;metathesaurus medical terminology needs ontology enhanced medical concept mapper online medical information source access medical search terms patients medical vocabularies medical experts synonyms semantically related concepts personal queries natural language processing tool arizona noun phraser human created ontologies unified medical language system wordnet computer generated concept space semantic net deep semantic parsing algorithm;unified medical language system;algorithms health knowledge attitudes practice humans information systems internet natural language processing neoplasms software terminology as topic unified medical language system user computer interface;parsing;nomenclature;semantic net;internet;journal article paginated;medical information systems;unified modeling language;terminology ontologies vocabulary unified modeling language software libraries digital signal processing humans medical tests natural language processing internet;artificial intelligence;terminology;ontologies;humans;unified medical language system umls;semantic relations;query expansion;ontologies information retrieval;natural language processing;querying computer science;concept space	This paper describes the development and testing of the Medical Concept Mapper, a tool designed to facilitate access to online medical information sources by providing users with appropriate medical search terms for their personal queries. Our system is valuable for patients whose knowledge of medical vocabularies is inadequate to find the desired information, and for medical experts who search for information outside their field of expertise. The Medical Concept Mapper maps synonyms and semantically related concepts to a user's query. The system is unique because it integrates our natural language processing tool, i.e., the Arizona (AZ) Noun Phraser, with human-created ontologies, the Unified Medical Language System (UMLS) and WordNet, and our computer generated Concept Space, into one system. Our unique contribution results from combining the UMLS Semantic Net with Concept Space in our deep semantic parsing (DSP) algorithm. This algorithm establishes a medical query con-text based on the UMLS Semantic Net, which allows Concept Space terms to be filtered so as to isolate related terms relevant to the query. We performed two user studies in which Medical Concept Mapper terms were compared against human experts' terms. We conclude that the AZ Noun Phraser is well suited to extract medical phrases from user queries, that WordNet is not well suited to provide strictly medical synonyms, that the UMLS Metathesaurus is well suited to provide medical synonyms, and that Concept Space is well suited to provide related medical terms, especially when these terms are limited by our DSP algorithm.	algorithm;assignment zero;ephrin type-b receptor 1, human;map;naruto shippuden: clash of ninja revolution 3;natural language processing;nomenclature;ontology (information science);parsing;patients;phrases;question (inquiry);semantic network;text-based (computing);unified medical language system;vocabulary;wordnet	Gondy Leroy;Hsinchun Chen	2001	IEEE Transactions on Information Technology in Biomedicine	10.1109/4233.966101	natural language processing;computer science;database;unified medical language system;information technology;information retrieval	ML	-47.96716899192597	-68.1255513202552	164548
9c62ae4dbc68a4df2b8c5752d435bd59ad8b36a4	answering why and how questions with respect to a frame-based knowledge base: a preliminary report	answer set programming frame based knowledge representation question answering;004;answer set programming;frame based knowledge representation;question answering	Being able to answer questions with respect to a given text is the cornerstone of language understanding and at the primary school level students are taught how to answer various kinds of questions including why and how questions. In the building of automated question answering systems the focus so far has been more on factoid questions and comparatively little attention has been devoted to answering why and how questions. In this paper we explore answering why and how questions with respect to a frame-based knowledge base and give algorithms and ASP (answer set programming) implementation to answer two classes of questions in the Biology domain. They are of the form: “How are X and Y related in the process Z?” and “Why is X important to Y?” 1998 ACM Subject Classification D.1.6 Logic Programming, H.3.4 Question-answering (fact retrieval) systems, I.2.4 Frames and scripts	algorithm;answer set programming;knowledge base;logic programming;natural language understanding;question answering;stable model semantics	Chitta Baral;Nguyen Ha Vo;Shanshan Liang	2012		10.4230/LIPIcs.ICLP.2012.26	question answering;computer science;knowledge management;artificial intelligence;answer set programming;data mining;information retrieval	AI	-37.15186620594733	-77.20994708413754	164778
a268e5e1964aea6a71ee6b84f482e3cdad6df8c4	automated syndrome classification using early phase emergency department data	emergency department;vector space model;information retrieval;disease syndromes;triage notes;disease syndrome;near real time;biosurveillance;public health;cosine similarity	The primary motivation behind automated syndrome classification is to shorten the time it may take to detect an outbreak or a community-wide public health issue. State-of-the-art syndrome classification techniques have primarily used Emergency Department (ED) chief complaints (CCs) or other short free text descriptions of patient symptoms for near real-time surveillance purposes. We propose a system that can automatically classify an ED record with initial temperature, chief complaint and triage nurse's notes into one or more syndromes using the vector space model. Terms from syndrome definitions and a small tf-idf weighted syndrome-positive training set are used to create a reference dictionary for generating the syndrome and triage note vectors. Subsequently, cosine similarity between the vectors is used to establish the particular syndrome category. We tested the system on a manually classified dataset of 485 ED records for the gastro-intestinal (GI) syndrome and measured performance in terms of sensitivity (~93%) and specificity (~82%). Initial results show that this system is capable of producing a better balance between these measures compared to existing automatic syndrome categorization systems which use only chief complaints and/or diagnosis codes.	categorization;code;cosine similarity;dictionary;real-time computing;real-time locating system;sensitivity and specificity;test set;tf–idf	Deepika Mahalingam;Javed Mostafa;Debbie A. Travers;Stephanie W. Haas;Anna E. Waller	2012		10.1145/2110363.2110406	medicine;data mining;medical emergency	NLP	-48.233248790783975	-69.73187833553798	165087
960cb5ab192131cc05deca05248305d8fae85757	indowordnet visualizer: a graphical user interface for browsing and exploring wordnets of indian languages		In this paper, we are presenting a graphical user interface to browse and explore the IndoWordnet lexical database for various Indian languages. IndoWordnet visualizer extracts the related concepts for a given word and displays a sub graph containing those concepts. The interface is enhanced with different features in order to provide flexibility to the user. IndoWordnet visualizer is made publically available. Though it was initially constructed for making the wordnet validation process easier, it is proving to be very useful in analyzing various Natural Language Processing tasks, viz., Semantic relatedness, Word Sense Disambiguation, Information Retrieval, Textual Entailment, etc.	browsing;graphical user interface;information retrieval;lexical database;natural language processing;semantic similarity;textual entailment;the advanced visualizer;viz: the computer game;word sense;word-sense disambiguation;wordnet	Devendra Singh Chaplot;Sudha Bhingardive;Pushpak Bhattacharyya	2014			communication	AI	-35.32572599619168	-74.78610415112243	165190
27605dedfb26011b61af1a44cea67bd0da3482d1	editorial for the first workshop on mining scientific papers: computational linguistics and bibliometrics		The open access movement in scientific publishing and search engines like Google Scholar has made scientific articles more broadly accessible. During the last decade, the availability of scientific papers in full text has become more and more widespread thanks to the growing number of publications on online platforms such as ArXiv and CiteSeer [1]. The efforts to provide articles in machine-readable formats and the rise of Open Access publishing have resulted in a number of standardized formats for scientific papers (such as NLM-JATS, TEI, DocBook), full text datasets for research experiments (PubMed, JSTOR, etc.) and corpora (iSearch, etc.). At the same time, research in the field of Natural Language Processing have provided a number of open source tools for versatile text processing (e.g. NLTK, Mallet, OpenNLP, CoreNLP, Gate [2], CiteSpace [3]). Scientific papers are highly structured texts and display specific properties related to their references but also argumentative and rhetorical structure. Recent research in this field has concentrated on the construction of ontologies for citations and scientific articles (e.g. CiTO [4], Linked Science [5]) and studies of the distribution of references [6]. However, up to now full text mining efforts are rarely used to provide data for bibliometric analyses. While Bibliometrics traditionally relies on the analysis of metadata of scientific papers (see e.g. a recent special issue on Combining Bibliometrics and Information Retrieval edited by Mayr & Scharnhorst, 2015 [7]), we will explore the ways full-text processing of scientific papers and linguistic analyses can play. With this workshop we like to discuss novel approaches and provide insights into scientific writing that can bring new perspectives to understand both the nature of citations and the nature of scientific articles. The possibility to enrich metadata by the full text processing of papers offers new fields of application to Bibliometrics studies. Working with full text allows us to go beyond metadata used in Bibliometrics. Full text offers a new field of investigation, where the major problems arise around the organization and structure of text, the extraction of information and its representation on the level of metadata. Furthermore, the study of contexts around in-text citations offers new perspectives related to the semantic dimension of citations. The analyses of citation contexts and the semantic categorization of publications will allow us to rethink co-citation networks, bibliographic coupling and other bibliometric techniques.	bibliographic coupling;bibliometrics;categorization;citeseerx;co-citation;computational linguistics;docbook;experiment;google scholar;human-readable medium;information extraction;information retrieval;natural language toolkit;natural language processing;netware loadable module;ontology (information science);open-source software;pubmed;scientific literature;text encoding initiative;text corpus;text mining;web search engine	Iana Atanassova;Marc Bertin;Philipp Mayr	2015			bibliometrics;computer science;data science;data mining;information retrieval	ML	-35.29025919830363	-71.35781678741533	165463
1becd354a312e91c1bb0bedd9c7a0fcd4113f878	optimizing feature representation for automated systematic review work prioritization	workload;evidence based medicine;oregon;abstracting and indexing as topic;artificial intelligence;pattern recognition automated;natural language processing;documentation;health priorities	Automated document classification can be a valuable tool for enhancing the efficiency of creating and updating systematic reviews (SRs) for evidence-based medicine. One way document classification can help is in performing work prioritization: given a set of documents, order them such that the most likely useful documents appear first. We evaluated several alternate classification feature systems including unigram, n-gram, MeSH, and natural language processing (NLP) feature sets for their usefulness on 15 SR tasks, using the area under the receiver operating curve as a measure of goodness. We also examined the impact of topic-specific training data compared to general SR inclusion data. The best feature set used a combination of n-gram and MeSH features. NLP-based features were not found to improve performance. Furthermore, topic-specific training data usually provides a significant performance gain over more general SR training.	document classification;http 404;mesh networking;n-gram;natural language processing;optimizing compiler;rss gene;receiver operating characteristic;review [publication type];systematic review;gram	Aaron M. Cohen	2008	AMIA ... Annual Symposium proceedings. AMIA Symposium		computer science;data science;data mining;information retrieval	SE	-47.06815414926048	-69.85371938386861	166128
3490937ff75ab2105e4ad82ac810cafb0f7ef1af	on the provenance of linked data statistics.		As the amount of linked data published on the web grows, attempts are being made to describe and measure it. However even basic statistics about a graph, such as its size, are difficult to express in a uniform and predictable way. In order to be able to sensibly interpret a statistic it is necessary to know how it was calculate. In this paper we survey the nature of the problem and outline a strategy for addressing it. 1 Background and Motivation For the past several years datasets of Linked Open Data on the web have been catalogued and made into a diagram [6] to illustrate their proliferation and interconnectedness. More recently some statistics about these datasets have been calculated [3]. Amongst the published statistics are, for example the number of triples in various graphs or unions of graphs across a particular domain of interest. Some more sophisticated statistics are also given in absolute terms, e.g. the absolute number of links outgoing from a particular dataset. In conjunction with this work, a vocabulary [1] has been developed for describing RDF datasets. This vocabulary contains predicates for describing common statistics, for example the number of triples or number of distinct subjects, as well as some more generic facilities for annotating a dataset description with other types of statistical information. Inasmuch as these statistics help to understand some of the properties of these data at a coarse grained level and get a rough idea of their dimensions they are quite useful and indeed valuable contributions. However as always we must ask what they mean. As, for example, void:triples denotes the size of the dataset, intuitively we might think that this gives some idea of the amount of information contained in it. But, most datasets carry a greater or lesser amount of redundant information. It might be included to make querying easier or to make extracts more easily readable by a human. In some sense it could be argued that when counting triples that this redundant information	diagram;graph (discrete mathematics);human-readable medium;interconnectedness;linked data;mike lesser;predicate (mathematical logic);resource description framework;vocabulary	William Waites	2014	CoRR		computer science;data mining;linked data;data science;database;statistics;statistic;know-how;graph	ML	-34.607049748555234	-67.7067716555525	166154
796e9724d51837495751a8ac3d0adcf43bc2eefe	digital pen in mammography patient forms	pen based interface;text entry;automatic generation;medical healthcare;design;pen ink interface	"""We present a digital pen based interface for clinical radiology reports in the field of mammography. It is of utmost importance in future radiology practices that the radiology reports be uniform, comprehensive, and easily managed. This means that reports must be """"readable"""" to humans and machines alike. In order to improve reporting practices in mammography, we allow the radiologist to write structured reports with a special pen on paper with an invisible dot pattern. A handwriting software takes care of the interpretation of the written report which is transferred into an ontological representation. In addition, a gesture recogniser allows radiologists to encircle predefined annotation suggestions which turns out to be the most beneficial feature. The radiologist can (1) provide the image and image region annotations mapped to a FMA, RadLex, or ICD10 code, (2) provide free text entries, and (3) correct/select annotations while using multiple gestures on the forms and sketch regions. The resulting, automatically generated PDF report is then stored in a semantic backend system for further use and contains all transcribed annotations as well as all free form sketches."""	care-of address;digital pen;fma instruction set;human-readable medium;portable document format;radiology	Daniel Sonntag;Marcus Liwicki;Markus Weber	2011		10.1145/2070481.2070537	computer vision;design;speech recognition;computer science;multimedia;world wide web;computer graphics (images)	HCI	-46.62347366864421	-67.16824186740506	166350
9a4e4584d2c104d7c327e76b78f8e2a991196339	new york university: description of the proteus system as used for muc-4	semantic analyzer;syntactic analyzer;proteus system;main component;template generator;new york university	The PROTEUS system which we have used for MUC-4 is largely unchanged from that used for MUC-3. It has three main components: a syntactic analyzer, a semantic analyzer, and a template generator.		Ralph Grishman;Catherine Macleod;John Sterling	1992		10.3115/1072064.1072099		Robotics	-34.75941322993159	-78.59375099916787	166677
bf3c6410ef6420985b2dcbde31a9d6631bf934fc	on the efficacy of per-relation basis performance evaluation for ppi extraction and a high-precision rule-based approach	health informatics;information systems and communication service;decision support techniques;management of computing and information systems;pattern recognition automated;humans;protein interaction mapping;computational biology;information storage and retrieval;natural language processing	"""BACKGROUND Most previous Protein Protein Interaction (PPI) studies evaluated their algorithms' performance based on """"per-instance"""" precision and recall, in which the instances of an interaction relation were evaluated independently. However, we argue that this standard evaluation method should be revisited. In a large corpus, the same relation can be described in various different forms and, in practice, correctly identifying not all but a small subset of them would often suffice to detect the given interaction.   METHODS In this regard, we propose a more pragmatic """"per-relation"""" basis performance evaluation method instead of the conventional per-instance basis method. In the per-relation basis method, only a subset of a relation's instances needs to be correctly identified to make the relation positive. In this work, we also introduce a new high-precision rule-based PPI extraction algorithm. While virtually all current PPI extraction studies focus on improving F-score, aiming to balance the performance on both precision and recall, in many realistic scenarios involving large corpora, one can benefit more from a high-precision algorithm than a high-recall counterpart.   RESULTS We show that our algorithm not only achieves better per-relation performance than previous solutions but also serves as a good complement to the existing PPI extraction tools. Our algorithm improves the performance of the existing tools through simple pipelining.   CONCLUSION The significance of this research can be found in that this research brought new perspective to the performance evaluation of PPI extraction studies, which we believe is more important in practice than existing evaluation criteria. Given the new evaluation perspective, we also showed the importance of a high-precision extraction tool and validated the efficacy of our rule-based system as the high-precision tool candidate."""	algorithm;body of uterus;complement system proteins;f1 score;gröbner basis;information extraction;lazy evaluation;logic programming;performance evaluation;pipeline (computing);pixel density;precision and recall;proton pump inhibitors;rule-based system;solutions;subgroup;text corpus;protein protein interaction	Junkyu Lee;Seongsoon Kim;Sunwon Lee;Jaewoo Kang	2013		10.1186/1472-6947-13-S1-S7	health informatics;medicine;computer science;nursing;machine learning;data mining;information retrieval	NLP	-46.550922080694576	-69.65169745829897	167393
d3ce0e4812de888e8f86868273a2bd2124fe6890	natural language interfaces: what is the problem? - a data-driven quantitative analysis	computer and information science;datavetenskap datalogi;quantitative analysis;natural language interface;computer science;data och informationsvetenskap	While qualitative analyses of the problems involved in building natural language interfaces (NLIs) have been available, a quantitative grounding in empirical data has been missing. We fill this gap by providing a quantitative analysis on the basis of the Geobase dataset. We hope that this analysis can guide further research in NLIs.	geobase;natural language	Philipp Cimiano;Michael Minock	2009		10.1007/978-3-642-12550-8_16	natural language processing;human–computer interaction;natural language user interface;computer science;quantitative analysis	PL	-33.840605201781194	-79.851980967449	167399
7ca9fa4bbf66a3d8a4c91bced93f882a220794ec	apport du style linguistique à la modélisation cognitive d'un elève	la mod;un el;lisation cognitive	In the context of an intelligent tutoring system dedicated to definition control (these definitions had been already learnt by students from lectures), this paper addresses more particularly the building of a cognitive profile, on which a dialogue strategy between the system and the student will be based. Among the elements involved in the cognitive profile building, we focus on the information emerging from the linguistic form spontaneously adopted by the student, while giving his/her definition in unconstrained natural language. From two studies on real collected corpora, this contribution draws conclusions aboutthe relevant clues that will allow the system to choose a strategy adapted to the student.	crash reporter;linear algebra	Marie-Paule Daniel;Lydia Nicaud;Violaine Prince;Marie-Paule Péry-Woodley	1992		10.1007/3-540-55606-0_32	knowledge management;natural language;computer science;intelligent tutoring system;cognition	HCI	-34.22709961699431	-79.63512841070973	167485
2c4d2fc96bd794d97d128197484447b705475d17	automated identification of synonyms in biomedical acronym sense inventories	synonymous long form;acronym long form expansion;synonymous biomedical acronym;biomedical natural language processing;biomedical text;biomedical literature;automated identification;biomedical acronym sense inventory;long form;sense inventory;long form expansion	Acronyms are increasingly prevalent in biomedical text, and the task of acronym disambiguation is fundamentally important for biomedical natural language processing systems. Several groups have generated sense inventories of acronym long form expansions from the biomedical literature. Long form sense inventories, however, may contain conceptually redundant expansions that negatively affect their quality. Our approach to improving sense inventories consists of mapping long form expansions to concepts in the Unified Medical Language System (UMLS) with subsequent application of a semantic similarity algorithm based upon conceptual overlap. We evaluated this approach on a reference standard developed for ten acronyms. A total of 119 of 155 (78%) long forms mapped to concepts in the UMLS. Our approach identified synonymous long forms with a sensitivity of 70.2% and a positive predictive value of 96.3%. Although further refinements are needed, this study demonstrates the potential value of using automated techniques to merge synonymous biomedical acronym long forms to improve the quality of biomedical acronym sense inventories.	algorithm;inventory;natural language processing;semantic similarity;technical standard;word-sense disambiguation	Genevieve B. Melton;Sungrim Moon;Bridget T. McInnes;Serguei V. S. Pakhomov	2010			natural language processing;computer science;data mining;information retrieval	NLP	-34.14830543410059	-68.9566186410505	168157
6d868b696470a929df40fdb8a3fea82f1be89e7f	a hybrid approach to gene ranking using gene relation networks derived from literature for the identification of disease gene markers	gene relation extraction;text mining;gene ranking;microarray data analysis;disease gene markers;generank	For the identification of gene markers involved in diseases, microarray expression profiles have been widely used to prioritize genes. In this paper, we propose a novel approach to gene ranking that employs gene relation network derived from literature along with microarray expression scores to calculate ranking statistics of individual genes. In particular, the gene relation network is constructed from literature by applying syntactic analysis and co-occurrence method in a hybrid manner. For evaluation, the proposed method was tested with publicly available prostate cancer data. The result shows that our method is superior to other existing approaches.	attempt;baseline (configuration management);benign mixed epithelial and stromal tumor of kidney;british informatics olympiad;cell hybridization;endocrine system diseases;extractor device component;meltwater entrepreneurial school of technology;microarray;parsing;precision and recall;predicate (mathematical logic);prostatic neoplasms;randomness extractor;relationship extraction;rule (guideline);text mining	Miyoung Shin;Hyungmin Lee;Munpyo Hong	2012	International journal of data mining and bioinformatics	10.1504/IJDMB.2012.049250	biology;microarray analysis techniques;text mining;computer science;bioinformatics;data mining;gene expression profiling;microarray databases;genetics	Comp.	-34.83993656734447	-69.76011125346872	168574
d50dcb04bd71fc35f21e3aec8ed2bd8ecf88e3ed	automatically analyzing large texts in a gis environment: the registrar general's reports and cholera in the 19th century	history;gis;digital humanities;corpus linguistics;article;natural language processing;19th century	The aim of this article is to present new research showcasing how Geographic Information Systems in combination with Natural Language Processing and Corpus Linguistics methods can offer innovative venues of research to analyze large textual collections in the Humanities, particularly in historical research. Using as examples parts of the collection of the Registrar General’s Reports that contain more than 200,000 pages of descriptions, census data and vital statistics for the UK, we introduce newly developed automated textual tools and well known spatial analyses used in combination to investigate a case study of the references made to cholera and other diseases in these historical sources, and their relationship to place-names during Victorian times. The integration of such techniques has allowed us to explore, in an automatic way, this historical source containing millions of words, to examine the geographies depicted in it, and to identify textual and geographic patterns in the corpus.	collocation;corpus linguistics;geographic information system;natural language processing;semiconductor industry;sensor;spatial analysis;text corpus	Patricia Murrieta-Flores;Alistair Baron;Ian N. Gregory;Andrew Hardie;Paul Rayson	2015	Trans. GIS	10.1111/tgis.12106	digital humanities;geography;computer science;artificial intelligence;data science;corpus linguistics;data mining;database;cartography	NLP	-37.235483582808925	-71.0041703422314	168733
6425731a497bcd9e2458c022977bb9a1053d1d20	active learning reduces annotation time for clinical concept extraction	active learning;annotation time;clinical free text;concept extraction;machine-assisted pre-annotation	OBJECTIVE To investigate: (1) the annotation time savings by various active learning query strategies compared to supervised learning and a random sampling baseline, and (2) the benefits of active learning-assisted pre-annotations in accelerating the manual annotation process compared to de novo annotation.   MATERIALS AND METHODS There are 73 and 120 discharge summary reports provided by Beth Israel institute in the train and test sets of the concept extraction task in the i2b2/VA 2010 challenge, respectively. The 73 reports were used in user study experiments for manual annotation. First, all sequences within the 73 reports were manually annotated from scratch. Next, active learning models were built to generate pre-annotations for the sequences selected by a query strategy. The annotation/reviewing time per sequence was recorded. The 120 test reports were used to measure the effectiveness of the active learning models.   RESULTS When annotating from scratch, active learning reduced the annotation time up to 35% and 28% compared to a fully supervised approach and a random sampling baseline, respectively. Reviewing active learning-assisted pre-annotations resulted in 20% further reduction of the annotation time when compared to de novo annotation.   DISCUSSION The number of concepts that require manual annotation is a good indicator of the annotation time for various active learning approaches as demonstrated by high correlation between time rate and concept annotation rate.   CONCLUSION Active learning has a key role in reducing the time required to manually annotate domain concepts from clinical free text, either when annotating from scratch or reviewing active learning-assisted pre-annotations.		Mahnoosh Kholghi;Laurianne Sitbon;Guido Zuccon;Anthony N. Nguyen	2017	International journal of medical informatics	10.1016/j.ijmedinf.2017.08.001	sampling (statistics);supervised learning;active learning;data mining;discharge summary;information retrieval;annotation;computer science	NLP	-45.41343614994226	-69.8851048640085	169647
14e78160930f50b265e5900d2e8a4452fd7740e9	finding conflicting statements in the biomedical literature	literature based discovery;information extraction;text mining;contradiction;negation;molecular event extraction;contrast;biomedical text mining;natural language processing;bioinformatics	.................................................................................................................13 Declaration.............................................................................................................15 Copyright................................................................................................................17 Acknowledgements................................................................................................19	declaration (computer programming)	Farzaneh Sarafraz	2012			computer science;data science;data mining;information retrieval	Crypto	-34.385125762808386	-69.9053918789275	170453
d9803565dc8d1eeee372058a2e4cd29c82d99c90	computational models for incongruity detection in humour	incongruity resolutions;computer model;semantic relatedness;sentiment analysis;humor recognition	Incongruity resolution is one of the most widely accepted theories of humour, suggesting that humour is due to the mixing of two disparate interpretation frames in one statement. In this paper, we explore several computational models for incongruity resolution. We introduce a new data set, consisting of a series of ‘set-ups’ (preparations for a punch line), each of them followed by four possible coherent continuations out of which only one has a comic effect. Using this data set, we redefine the task as the automatic identification of the humorous punch line among all the plausible endings. We explore several measures of semantic relatedness, along with a number of joke-specific features, and try to understand their appropriateness as computational models for incongruity detection.	automatic identification and data capture;baseline (configuration management);coherence (physics);computation;computational model;computer simulation;continuation;experiment;norm (social);semantic similarity;text corpus;theory	Rada Mihalcea;Carlo Strapparava;Stephen G. Pulman	2010		10.1007/978-3-642-12116-6_30	natural language processing;semantic similarity;computer science;linguistics;sentiment analysis	NLP	-34.6642515835876	-80.13639017336389	171402
42c32d82b8322f259f530fcb0f95a4edc40835e5	natural language processing for solving simple word problems		This paper describes our system which solves simple arithmetic word problems. The system takes a word problem described in natural language, extracts information required for representation, orders the facts presented, applies procedures and derives the answer. It then displays this answer in natural language. The emphasis of this paper is on the natural language processing(NLP) techniques used to retrieve the relevant information from the English word problem. The system shows improvements over existing systems.	entity;event calculus;heuristic;java;level of detail;natural language processing;pier solar and the great architects;question answering	Sowmya S. Sundaram;Deepak Khemani	2015			natural language processing;word problem (mathematics education);artificial intelligence;computer science	NLP	-36.92838690729932	-77.46074093608942	171693
d7f70e3557139663df8d8db4e5edada970425748	emergency management training: making sense of rich qualitative data	text analysis computer based training emergency management software packages;qualitative data;crisis communication;emergency management;nvivo software package emergency management training rich qualitative data content analysis unstructured text analysis crisis management training tool emergency response personnel norway sweden;content analysis;training software;law enforcement training context companies emergency services fires tag clouds;content analysis emergency management crisis communication training software qualitative data	Content analyses with the assistance of computers date back over 50 years and allow analysis of unstructured text by looking at concepts and expressions used. Emergency management training also has a long history of using computerized tools to enhance the training process. This paper presents an analysis of rich qualitative data from a web based crisis management training tool used with emergency response personnel in the mountainous border region between Norway and Sweden. Using the NVivo software package the paper demonstrates the efficiency of handling rich sources of textual communications between police, fire and ambulance teams communicating with representatives of power companies and county and municipality officials in responding to emergencies. The analysis shows distinct patterns of concepts and terms used by the various emergency response agencies in the two native languages. In order to compare the concepts and terms used on agencies on both sides of the border it was necessary to provide a translation of the transcripts to English of the two police agencies to illustrate differences across the border.	computer	Knut Ekker	2015	2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2015.7382147	qualitative property;simulation;content analysis;knowledge management;data mining;computer security;emergency management	SE	-42.70687622169133	-71.42688783790295	172382
46c7f8451485e2e0512428936db326a51427a5b1	exploring the clinical notes of pathology ordering by australian general practitioners: a text mining perspective	pattern clustering;pattern clustering data mining medical computing;text mining;data mining;medical computing;knowledge extraction clinical note exploration pathology ordering australian general practitioners unsupervised text mining perspective;general practitioner;pathology australia text mining data mining government pattern analysis laboratories logic testing information analysis system testing	A massive rise in the number and expenditure of pathology ordering by general practitioners (GPs) concerns the government and attracts various studies with the aim to understand and improve the ordering behavior. In this paper we attempt to understand the reasons for and implications of pathology ordering by general practitioners by applying an unsupervised text mining technique on the clinical notes of the pathology requests obtained from a pathology company in Australia. Pathology requests are clustered into different groups based on the information that is included by the doctors in clinical notes accompanying the requests. Features and patterns of the groups are investigated and analyzed. The novelty of the paper is in using text mining techniques to extract knowledge from unstructured text data in the area of pathology ordering and to understand the reasons for pathology ordering from a doctors' perspective	text corpus;text mining;unsupervised learning	Zoe Yan Zhuang;Rasika Amarasiri;Leonid Churilov;Damminda Alahakoon;Ken Sikaris	2007	2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)	10.1109/HICSS.2007.220	text mining;computer science;bioinformatics;data science;data mining;database;world wide web	HPC	-39.295131020722444	-69.13343321254692	172702
62e26463fbdddf6d09572e5e4a5002e72df39e86	access to relational data bases for a casual user	relational data;english language;natural language	The goal of the RENDEZVOUS Project is to enable a casual user to access relational data bases in natural language without a preliminary training period. The system will, if necessary, engage in an English dialogue with the user in order to arrive at an unambiguous statement of his query. This dialogue serves the triple purpose of aiding the user in his problem specification, of clarifying ambiguities inherent in the English language and of enabling the system to overcome incomprehensible features of the users query. The conversation must constantly narrow the scope of inquiry, resorting to menu selection if necessary until the user and system come to an agreement as to what information has been requested. At any moment during the dialogue, the system must be capable of expressing its current understanding of the user's query in clear and unambiguous English. In particular, the system must be able to rephrase the user's query for approval before execution.		E. F. Codd	1977	SIGART Newsletter	10.1145/1045283.1045298	relational database;computer science;english;data mining;database;natural language;programming language	AI	-42.316512942792265	-77.56554657900881	173784
1972621697667e4eac23f5dbfdb5fb440bb68ff6	fastdocode: finding approximated segments of n-grams for document copy detection - lab report for pan at clef 2010		Nowadays, plagiarism has been presented as one of the main distresses that the information technology revolution has lead into our society for which using pattern matching algorithms and intelligent data analysis approaches, these practices could be identified. Furthermore, a fast document copy detection algorithm could be used in large scale applications for plagiarism detection in academia, scientific research, patents, knowledge management, among others. Notwithstanding the fact that plagiarism detection has been tackled by exhaustive comparison of source and suspicious documents, approximated algorithms could lead to interesting results. In this paper, an approach for plagiarism detection is presented. Results in a learning dataset of plagiarized documents from the PAN’09, and its further evaluation in the PAN’10 plagiarism detection challenge, showed that the trade-off between speed and performance could improve other plagiarism detection algorithms.	approximation algorithm;grams;knowledge management;n-gram;pattern matching	Gabriel Oberreuter;Gaston L'Huillier;Sebastián A. Ríos;Juan D. Velásquez	2010			information retrieval;clef;computer science;speech recognition	ML	-39.29943099794402	-69.71074573800351	173986
0d99c95343d97074df09d7346812e4d4ed3a8e5a	automatic identification of ineffective online student questions in computing education		This Research Full Paper explores automatic identification of ineffective learning questions in the context of large-scale computer science classes. The immediate and accurate identification of ineffective learning questions opens the door to possible automated facilitation on a large scale, such as alerting learners to revise questions and providing adaptive question revision suggestions. To achieve this, 983 questions were collected from a question & answer platform implemented by an introductory programming course over three semesters in a large research university in the Southeastern United States. Questions were firstly manually classified into three hierarchical categories: 1) learning-irrelevant questions, 2) effective learning-relevant questions, 3) ineffective learning-relevant questions. The inter-rater reliability of the manual classification (Cohen’s Kappa) was .88. Four different machine learning algorithms were then used to automatically classify the questions, including Naive Bayes Multinomial, Logistic Regression, Support Vector Machines, and Boosted Decision Tree. Both flat and single path strategies were explored, and the most effective algorithms under both strategies were identified and discussed. This study contributes to the automatic determination of learning question quality in computer science, and provides evidence for the feasibility of automated facilitation of online question & answer in large scale computer science classes.	algorithm;automatic identification and data capture;computer science;decision tree;inter-rater reliability;machine learning;multinomial logistic regression;naive bayes classifier;relevance;scalability;support vector machine	Qiang Hao;April Galyardt;Bradley Barnes;Robert Maribe Branch;Ewan Wright	2018	CoRR		gradient boosting;data science;human–computer interaction;support vector machine;naive bayes classifier;computer science;decision tree;statistical classification	NLP	-45.09822634079796	-69.98009969311198	174222
0e7dc957d8e6b028d81611154d313048f40fbd32	word grader and powerpoint grader	microsoft word;grader;microsoft powerpoint;grading;checker;automatic grading;marker	"""Word Grader is a document checker for use by instructors in Microsoft Word or Microsoft Office application courses. Word Grader uses Microsoft Word's Compare and Combine feature to merge a student document with the correct version of the assignment. Word Grader counts errors and embeds a grade report in the marked-up document. """"PowerPoint Grader"""" is a companion product that grades Microsoft PowerPoint assignments. PowerPoint Grader extracts the text from a presentation and grades it. It also grades themes and slide layouts. Instructors have downloaded these graders hundreds of times and successfully integrated them into computer science curricula."""	computer science;microsoft word for mac;powerpoint animation;slide rule	Thomas G. Hill	2011	Inroads	10.1145/1963533.1963546	speech recognition;computer science;multimedia;computer graphics (images)	NLP	-39.67100958176484	-75.01612034077354	174627
922a36a65110881dbe82da1cb335365927571a42	granular causality applications: using part-of relations for discovering causality	part of relations;granularity;theory of granular causality;semantic relations;causality	Causal markers, syntactic structures and connectives have been the sole identifying features for automatically extracting causal relations in natural language discourse. However, various connectives such as “and”, prepositions such as “as”, and other syntactic structures are highly ambiguous in nature, as they have multiple meanings besides causality. As a result, one cannot solely rely on lexico-syntactic markers for detection of causal phenomenon in discourse. This paper introduces the Theory of Granular Causality and describes a new approach to identify causality in natural language. Causality is often granular in nature (Mulkar-Mehta, 2011; Mazlack, 2004), and this property of causality is used to discover and infer the presence of causal relations in text. This is compared with causal relations identified using just causal markers. A precision of 0.91 and a recall of 0.79 is achieved using granularity for causal relation detection, as compared to a precision of 0.79 and a recall of 0.44 using text-based causal words for causality detection. Next, the author presents the findings for discovering causal relations between two sentences in an article. The system achieves a precision of 0.60 for discovering causality between two sentences using granular causality markers as features. The results are encouraging, and show that the granular causality is an important phenomenon in natural language DOI: 10.4018/jcini.2012070105 International Journal of Cognitive Informatics and Natural Intelligence, 6(3), 88-108, July-September 2012 89 Copyright © 2012, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. tions using Elementary Discourse Units (EDUs). Neilson (1996) describes the way in which the semantic and pragmatic functions of causal markers can be accounted for in terms of linguistic and rhetorical theories of argumentation. All these works consider causality as a sequential set of events at the `same’ level of descriptive specificity. However, causality is often described using a granular structure, where the coarse grained event is described as happening because of a fine grained event. For instance, in a building collapses because the roof caved in, the roof is in integral part of the building, and is a sub-event of the entire building collapsing. This paper focuses on granular causality, and how such granular causality structures can be used to identify causal relations in text. We use the phenomenon of granularity on a regular basis in our everyday life. For planning and scheduling of important tasks, we often divide or split our tasks into smaller pieces, until each task is easily manageable. For instance, the day-to-day activity of shopping for groceries involves some finer grained events such as driving to the grocery store, carrying a list, picking out required items, and paying the cashier. Each of these events in turn involves some finer level events; e.g., driving to the grocery store involves sub-events like opening the car door, starting the engine, planning the route, and driving to the destination. The sequence of fine grained events make up a coarser grained event. When the fine grained events are completed successfully, the coarse grained event is completed successfully. In this sense, granularity decomposition is script or plan decomposition. Historically, to discover causality in discourse, certain words are used as causal markers. Certain markers are domain-specific; others (the most frequently occurring causal markers) are generic; but there is a long tail of high precision but low frequency causal marker words. For instance, the words ‘because’ and ‘cause’ are considered causal markers, but ‘behind’ is not generally considered a causal marker, although it often refers in causality in football newspaper articles, as in The Miami Dolphins went ahead 21-6 at halftime behind three touchdown throws by Dan Marino. Word-based causality detection techniques almost always miss the less frequently occurring but high precision causal markers in discourse. This paper describes the application and implementation of the Theory of Granular Causality for discovering causality and causal markers in various domains. Three experiments are performed: (i) Apply the Theory of Granular Causality for discovering causality within a sentence, (ii) Use the Theory of Granular Causality to extract low frequency causal markers from sentences containing causality, and (iii) Use the Theory of Granular Causality to discover causality between two sentences.	automated planning and scheduling;causality;cognitive science;experiment;floor and ceiling functions;granular computing;informatics;lexico;logical connective;long tail;natural language;scheduling (computing);sensitivity and specificity;text-based (computing);theory	Rutu Mulkar-Mehta	2012	IJCINI	10.4018/jcini.2012070105	natural language processing;granularity;causality;computer science;algorithm	NLP	-37.730084537987864	-67.7119376333722	175884
6c64b75276765cba15064c0d10a76f54c68a58f0	continuous improvement of a document treatment chain using reinforcement learning	osint open source intelligence;man machine interaction;reinforcement learning;apprentissage par renforcement;intelligence artificielle;extraction et gestion des connaissances;extraction and knowledge management;renseignement d origine sources ouvertes roso;interaction homme machine;artificial intelligence	We tackle the problem of continuous improvement of a treatment chain which extracts events from open-source documents.  We use the human operators' corrections to allow the treatment chain to learn from its errors, and self-improve generally.  We apply reinforcement learning (specifically Q-learning) to this problem, where the actions are the services of a treatment chain for the extraction of information.  The objective is to use the user feedback to allow the system to learn the ideal configuration of the services (order, gazetteers, and extraction rules) based on the characteristics of the documents treated (language, type, etc.).  We carry out the first experiments with automatically generated feedback data, and the results are encouraging.	reinforcement learning	Esther Nicart;Bruno Zanuttini;Bruno Grilhères;Patrick Giroux	2015			simulation;computer science;artificial intelligence;machine learning	NLP	-36.5560157423535	-78.47803563301756	178175
5021e63b96ec5f7d4d4bab7e075b21268bce0f38	pegasus: a spoken language interface for on-line air travel planning i	semantic representation;language technology;spoken language interface;speech understanding;system management	This paper describes PEGASUS, a spoken language interface for on-line air travel planning that we have recently developed. PEGASUS leverages off our spoken language technology development in the ATIS domain, and enables users to book flights using the American Airlines EAASY SABRE system. The input query is transformed by the speech understanding system to a frame representation that captures its meaning. The tasks of the System Manager include transforming the semantic representation into an EAASY SABRE command, transmitting it to the application backend, formatting and interpreting the xesulting information, and managing the dialogue. Preliminary evaluation results suggest that users can learn to make productive use of PEGASUS for travel planning, although much work remains to be done. I N T R O D U C T I O N Over the past few years, our group has part icipated, as a member of the ARPA Human Language Technology (HLT) research community, in the development of spoken language technology in the common domain called Air Travel Information Service, or ATIS [i]. ATIS permits users to verbally query for air travel information, such as flight schedules from one city to another, obtained from a small relational database excised from the Official Airline Guide. By requiring that all system developers use the same database, it has been possible to compare the performance of various spoken language systems based on their ability to extract the correct information from the database, using a set of prescribed training and test data, and a set of interpretation guidelines. Indeed, periodic common evaluations have occurred at regular intervals, and steady performance improvements have been observed for all systems [2, 3, 4]. While the ATIS task has been instrumental in the development of technologies that can understand spontaneously generated verbal queries in a limited domain, it 1This research was supported by ARPA under Contract N0001489-J-1332, monitored through the Office of Naval Research. 2The authors are listed in reversed alphabetical order. does have some shortcomings. First , the current common evaluation focuses on the correctness of the information extracted from the database without any regard to the system's side of the interchange (e.g., clarification queries and helpful suggestions). Thus it has the effect of discouraging research on dialogue-based systems which, we believe, is a crucial aspect of human computer interactions. Second, ATIS makes use of a mock-up, static database containing flight and fare information for a small set of cities within the United States and Canada. It is not a realistic model of the databases actually being used by travel agents and travellers. In particular, operational flight information systems are much larger and more complex, and, most impoi'tantly, they contain information which is dynamic in nature. The rapid technological progress that we are witnessing gives us hope that spoken language systems capable of performing real tasks will begin to emerge within the decade. To realize this potential , however, it is important that we begin to develop the technology using real databases, so that we can uncover limitations and gaps in our present research paradigm. To this end, we star ted in 1992 to investigate the feasibility of at taching a spoken language interface to an available on-line database. We selected the American Airlines EAASY SABRE system, which allows subscribers to obtain flight information and make flight reservations via a large dynamic database, accessed through their personal computers over the telephone line. This system currently has over 700,000 active subscribers, most of whom are travellers, not travel agents. We selected this database mainly because we believe we can leverage off of our existing ATIS system to build an appropriate user-friendly interface. To communicate with EAASY SABRE in its normal mode of operation, users issue coded queries specifying restrictions such as source, date, and fare code. If the necessary pieces of information are omitted from the query, the system enters a t ightly controlled menu protocol to fill them in. What we have a t tempted to accomplish is a replacement of this cumbersome interface with something that permits a more natural dialogue with the computer. Our system, called PEGASUS, acts as a mediator between	automatic transmitter identification system (television);block cipher mode of operation;common criteria;correctness (computer science);human computer;information system;interaction;interchange fee;language technology;matchware mediator;mock object;normal mode;online and offline;pegasus;personal computer;programming paradigm;relational database;sabre (computer system);telephone line;test data;usability	Victor Zue;Stephanie Seneff;Joseph Polifroni;Michael S. Phillips;Christine Pao;David Goddeau;James R. Glass;Eric Brill	1994			natural language processing;systems management;speech recognition;universal networking language;natural language user interface;computer science;linguistics;natural language;language technology;comprehension approach	NLP	-42.723041571531915	-78.17862567836342	178573
cf04050f7837562e952a26206dd7742e4be6fa82	inférence semi-automatique et interactive de règles sans vérité terrain		Dealing with non annotated documents for the design of a document recognition system is not an easy task. In general, statistical methods cannot learn without an annotated ground truth, unlike syntactical methods. However their ability to deal with non annotated data comes from the fact that the description is manually made by a user. The adaptation to a new kind of document is then tedious as the whole manual process of extraction of knowledge has to be redone. In this paper, we propose a method to extract knowledge and generate rules without any ground truth. Using large volume of non annotated documents, it is possible to study redundancies of some extracted elements in the document images. The redundancy is exploited through an automatic clustering algorithm. An interaction with the user brings semantic to the detected clusters. In this work, the extracted elements are some keywords extracted with word spotting. This approach has been applied to old marriage record field detection on the FamilySearch HIP2013 competition database. The results demonstrate that we successfully automatically infer rules from non annotated documents using the redundancy of extracted elements of the documents. MOTS-CLÉS : Reconnaissance de documents structurés, Inférence de règles, Extraction de connaissances, Partitionnement de données, Données non annotées.	algorithm;cluster analysis;document;familysearch indexing;ground truth;redundancy (engineering);sans institute;semiconductor industry	Cérès Carton;Aurélie Lemaitre;Bertrand Coüasnon	2016		10.24348/sdnri.2016.CIFED-15	cartography;philosophy	NLP	-34.06129802425326	-72.20839178023263	178934
5e904bb9070da0a45cfb0c138819882753d78993	using genetic algorithms to create meaningful poetic text	poetry;quality metric;semantic representation;creative language;journal article;automatic generation;qa75 electronic computers computer science;natural language generation;genetic algorithm;genetic algorithms	This paper presents a series of experiments in automatically generating poetic texts. We confined our attention to the generation of texts which are syntactically well-formed, meet certain pre-specified patterns of metre, and broadly convey some given meaning. Such aspects can be formally defined, thus avoiding the complications of imagery and interpretation that are central to assessing more free forms of verse. Our implemented system, McGonagall, applies the genetic algorithm to construct such texts. It uses a sophisticated linguistic formalism to represent its genomic information, from which can be computed the phenotypic information of both semantic representations and patterns of stress. The conducted experiments broadly indicated that relatively meaningful text could be produced if the constraints on metre were relaxed, and precise metric text was possible with loose semantic constraints, but it was difficult to produce text which was both semantically coherent and of high quality metrically.	coherence (physics);display resolution;evaluation function;experiment;formal grammar;genetic algorithm;informatics;natural language generation;relaxation (approximation);relevance;robustness (computer science);semantics (computer science);stochastic optimization;tree-adjoining grammar;verse protocol;well-formed document;well-formed element;while	Ruli Manurung;Graeme Ritchie;Henry Thompson	2012	J. Exp. Theor. Artif. Intell.	10.1080/0952813X.2010.539029	natural language processing;genetic algorithm;computer science;artificial intelligence;machine learning	NLP	-35.70027095310908	-70.23232801052258	179054
8a64e801cb6395e05f3b8d29af1980643dba2e4e	encoding forensic multimedia evidence from marf applications as forensic lucid expressions		In this work we summarize biometric evidence as well as file type evidence extraction “exported” as formal Forensic Lucid language expression in the form of higher-order intensional contexts for further case analysis by a system that interprets Forensic Lucid expressions for claim verification and event reconstruction. The digital evidence is exported from the Modular Audio Recognition Framework (MARF)’s applications runs on a set of data comprising biometric voice recordings for speaker, gender, spoken accent, etc. as well as more general file type analysis using signal and pattern recognition processing techniques. The focus is in translation aspect of the extracted evidence into formal Forensic Lucid expressions for further analysis.	biometrics;intensional logic;lucid;modular audio recognition framework (marf);pattern recognition;signal processing	Serguei A. Mokhov	2008		10.1007/978-90-481-3662-9_71	natural language processing;speech recognition;computer science	PL	-35.35704951985772	-76.91609300583848	180688
2b2f7aa881d45c7a96c2377beb5fe2d4ff88fa1b	a corpus of tables in full-text biomedical research publications		The development of text mining techniques for biomedical research literature has received increased attention in recent times. However, most of these techniques focus on prose, while much important biomedical data reside in tables. In this paper, we present a corpus created to serve as a gold standard for the development and evaluation of techniques for the automatic extraction of information from biomedical tables. We describe the guidelines used for corpus annotation and the manner in which they were developed. The high inter-annotator agreement achieved on the corpus, and the generic nature of our annotation approach, suggest that the developed guidelines can serve as a general framework for table annotation in biomedical and other scientific domains. The annotated corpus and the guidelines are available at http://www.csse.monash.edu.au/research/umnl/data/index.shtml.	information extraction;inter-rater reliability;scientific literature;text corpus;text mining	Tatyana Shmanina;Ingrid Zukerman;Ai Lee Cheam;Thomas Bochynek;Lawrence Cavedon	2016			natural language processing;artificial intelligence;computer science	NLP	-34.604492257092446	-69.43800641606494	181060
42c83da6b2ea1af79e08776bad37fe7435f87fa2	in codice ratio: scalable transcription of historical handwritten documents		Huge amounts of handwritten historical documents are being published by digital libraries world wide. However, for these raw digital images to be really useful, they need to be annotated with informative content. State-of-the-art Handwritten Text Recognition (HTR) approaches require an impressive training effort by expert paleographers. Our contribution is a scalable, end-to-end transcription work-flow – that we call In Codice Ratio – based on fine-grain segmentation of text elements into characters and symbols, with limited training effort. We provide a preliminary evaluation of In Codice Ratio over a corpus of letters by pope Honorii III, stored in the Vatican Secret Archive.	archive;digital image;digital library;end-to-end principle;historical document;information;library (computing);medical transcription;optical character recognition;scalability;transcription (software)	Serena Ammirati;Donatella Firmani;Marco Maiorino;Paolo Merialdo;Elena Nieddu;Andrea Rossi	2017			database;scalability;transcription (biology);computer science	ML	-33.74838017286847	-72.58945765410844	181788
b6b24e40f83b2ba43fdf27a7d38771bf3aedbc98	jasist special issue on biomedical information retrieval		Information Retrieval (IR) and text analytics in the biomedical and healthcare domain has been attracting an abundant amount of research in the past decades. Similar to other domains, the main purpose used to be accurate retrieval of documents. However, in recent years, with the increased access to Electronic Health Records (EHRs), the interests and tasks are expanding (Hersh 2002). Retrieval of biomedical literature always had its unique methods due to the rich knowledge bases, such as the Unified Medical Language System (UMLS), Medical Subject Headings (MeSH), and the Systematized Nomenclature of Medicine (SNOMED) that enable the indexing of documents into concepts, for various purposes, such as retrieval (Hersh & Greens, 1989; Hersh & Hickam, 1992, 1993; Moskovitch et al., 2004; Lin & Demner-Fushman, 2006) and more (Moskovitch et al., 2006; Moskovitch & Shahar 2009; Ruch, 2006). The use of these domain-specific terminologies and vocabularies boosted the development of a large number of domainoriented retrieval methods. An important related thread of research has been the application of Natural Language Processing techniques to extract named entity concepts from data (Ruch, 2006), as well as other important pieces of information. In addition, to evaluate methods in biomedical text analytics and retrieval, several critical test data collections are now available, including TREC (Roberts et al., 2016) and more. In recent years, with the increased access to patients’ data in the form of EHRs, there are new problems and challenges related to the analysis and extraction from clinical notes and discharge summaries, particularly because the notes typically are telegraphic and include abbreviations, meta-data, semistructured text such as tables and billing codes, and use new lines to signal the end of a sentence. Another important retrieval task in the biomedical domain is from images that require image processing and retrieval. In addition to the data accumulated at hospital systems, and to the traditional clinical literature published in scientific journals and conferences and indexed in PubMed, there is an increase in healthrelated discussions in various relevant online forums and social medical sites. These forums span over multiple topics in the medical domain, in which patients share and discuss their experiences and questions. Consequently, the users of medical information retrieval may vary in their clinical knowledge, from physicians, medical students and related experts, to patients, or their relatives. They may also vary in their information needs, from scientific literature for experts to patients’ discussions on the internet. All these characteristics bring many challenges and opportunities to the scientific community. Finally, in the biomedical domain, in addition to improving the access to information through better and more efficient retrieval methodologies, there is the potential to improve the quality of care for patients. Thus, in this special issue, we encouraged participation from researchers in all fields related to medical information research, including mainstream information retrieval, but also natural language processing, multilingual text processing, and medical image analysis and retrieval.	code;content-based image retrieval;discharger;electronic billing;experience;freedom of information laws by country;image analysis;image processing;information needs;information retrieval;internet;journal of the association for information science and technology;medical image computing;natural language processing;pubmed;scientific literature;systematized nomenclature of medicine;test data;text retrieval conference;text mining;vocabulary;word lists by frequency	Robert Moskovitch;Fei Wang;Jian Pei;Carol Friedman	2017	JASIST	10.1002/asi.23972	computer science;information retrieval	Web+IR	-38.827674172400684	-67.80487962363877	183071
37bbb7a109f821b9b8baa0e461f9631855943606	unsupervised event extraction from biomedical text based on event and pattern information	busqueda informacion;tratamiento automatico;extraction information;information biomedicale;lenguaje natural;information extraction;information retrieval;medicina;langage naturel;systeme recherche;medecine;biomedical information;search system;automatic processing;sistema investigacion;recherche information;informacion biomedical;natural language;medicine;evenement;event;traitement automatique;extraccion informacion;extraction method	In this paper, we proposed a new event extraction method from biomedical texts. It can extend patterns by unsupervised way based on event and pattern information. Evaluation of our system on GENIA corpus achieves 90.1% precision and 70.0% recall.	text-based (computing);unsupervised learning	Hong-Woo Chun;Young-Sook Hwang;Hae-Chang Rim	2004		10.1007/978-3-540-24630-5_66	speech recognition;event;computer science;artificial intelligence;linguistics;natural language;information extraction	NLP	-33.89598685180336	-72.26614729000026	183526
bcae56bb409edbd0d8897f3f17ab63aa0d1d5c14	automated analysis of e-participation data by utilizing associative networks, spreading activation and unsupervised learning	unsupervised learning;unsupervised clustering;artificial intelligent;domain knowledge;public opinion;information and communication technology;machine learning;spreading activation;political participation	"""According to [1], the term e-participation is defined as """"the use of information and communication technologies to broaden and deepen political participation by enabling citizens to connect with one another and with their elected representatives"""". This definition sounds quite simple and logical, but when considering the implementation of such a service in a real world scenario, it is obvious that it is not possible to evaluate messages, which are generated by thousands of citizens, by hand. Such documents need to be read and analyzed by experts with the required in-depth domain knowledge. In order to enable this analysis process and thereby to increase the number of possible e-particpation applications, we need to provide these experts with automated analysis tools that cluster, pre-screen and pre-evaluate public opinions and public contributions. In this paper we present a framework based on Machine Learning-(ML) and Artificial Intelligence-(AI) techniques that are capable of various analysis mechanisms such as unsupervised clustering of yet unread documents, searching for related concepts within documents and the description of relations between terms. To finish, we show how the proposed framework can be applied to real world data taken from the Austrian e-participation platform mitmachen.at."""	spreading activation;unsupervised learning	Peter Teufl;Udo Payer;Peter Parycek	2009		10.1007/978-3-642-03781-8_13	computer science;artificial intelligence;machine learning;data mining	ML	-37.957564176542526	-68.45183280434158	183793
d3077a3461b53c129abe73ba2aa1694966be6bfc	classifying biomedical abstracts using committees of classifiers and collective ranking techniques	early experience;systematic reviews;systematic review;evidence based medicine;text classification;machine learning;automatic text classification;ranking algorithms;ranking algorithm;article	The purpose of this work is to reduce the workload of human experts in building systematic reviews from published articles, used in evidence-based medicine. We propose to use a committee of classifiers to rank biomedical abstracts based on the predicted relevance to the topic under review. In our approach, we identify two subsets of abstracts: one that represents the top, and another that represents the bottom of the ranked list. These subsets, identified using machine learning (ML) techniques, are considered zones where abstracts are labeled with high confidence as relevant or irrelevant to the topic of the review. Early experiments with this approach using different classifiers and different representation techniques show significant workload reduction.	experiment;machine learning;relevance;systematic review	Alexandre Kouznetsov;Stan Matwin;Diana Inkpen;Amir Hossein Razavi;Oana Frunza;Morvarid Sehatkar;Leanne Seaward;Peter O'Blenis	2009		10.1007/978-3-642-01818-3_29	evidence-based medicine;systematic review;computer science;machine learning;pattern recognition;data mining;information retrieval;learning to rank	NLP	-46.29173660187492	-69.7696530968091	183835
f74f2cd7bbfa2b918fb46b00a41f04fa659410a1	problems of automatic processing and analysis of information from legal texts		In the paper, problems of legal information digitalization are investigated. Conditions for extraction information from legal texts related to the common ones processing (non-legal terms) are outlined. Sample results of similarity analysis are presented. Further research aimed at semantic analysis of legal texts are outlined.	semantic analysis (compilers)	Tomasz Pelech-Pilichowski;Piotr Potiopa;Wojciech Cyrul	2012				NLP	-37.51273953991621	-66.3796240203476	184313
f0a5a283510b455c53fcbc9be881fc698a19f654	the life of lazarillo de tormes and of his machine learning adversities		Summit work of the Spanish Golden Age and forefather of the so-called picaresque novel, The Life of Lazarillo de Tormes and of His Fortunes and Adversities still remains an anonymous text. Although distinguished scholars have tried to attribute it to different authors based on a variety of criteria, a consensus has yet to be reached. The list of candidates is long and not all of them enjoy the same support within the scholarly community. Analyzing their works from a data-driven perspective and applying machine learning techniques for style and text fingerprinting, we shed light on the authorship of the Lazarillo. As in a state-of-the-art survey, we discuss the methods used and how they perform in our specific case. According to our methodology, the most likely author seems to be Juan Arce de Otálora, closely followed by Alfonso de Valdés. The method states that not certain attribution can be made with the given corpus.	fingerprint (computing);machine learning;text corpus	Javier de la Rosa Pérez;Juan-Luis Suárez	2016	CoRR		computer science;artificial intelligence	NLP	-37.1603115911751	-72.94261958948661	184424
7fcf0f7ed4b84ccf6714a8a1dd2ef961db9f15cb	fuzzy set and semantic similarity in ontology alignment	semantic similarity;fuzzy set similarity;semantic alignment quality measure ontology alignment semantic web information interoperability heterogeneous ontologies gold standard reference alignment semantic similarity measure fuzzy set similarity measure;biomedical measurements;atmospheric measurements;standards;ontologies semantics integrated circuits biomedical measurements standards atmospheric measurements particle measurements;ontology alignment evaluation initiative oaei;particle measurements;semantics;fuzzy set theory;ontologies artificial intelligence;ontology alignment evaluation initiative oaei ontology alignment semantic similarity fuzzy set similarity;semantic web fuzzy set theory ontologies artificial intelligence;semantic web;ontologies;integrated circuits;ontology alignment	A challenge for the Semantic Web is enabling information interoperability between related but heterogeneous ontologies. Ontology alignment (OA) addresses this challenge by identifying correspondences between entities in different ontologies. The traditional OA evaluation strategy uses a gold standard reference alignment created by a domain expert. The problem is that often a reference alignment may not exist. The current use of semantic similarity measures in the OA process and proposals for their use in the OA evaluation task are presented. Many semantic similarity measures are derivative of fuzzy set similarity measures. A general semantic alignment quality (SAQ) measure is developed and used on the alignment results of 10 different OA systems produced on the anatomy track of the 2010 OA evaluation initiative. The SAQ results indicate much variation in performance depending on the selected semantic similarity measure. Problems with using several semantic similarity measures in SAQ are further investigated and the findings are discussed.	computational anatomy;discrepancy function;entity;f1 score;fuzzy set;integer factorization;interoperability;ontology (information science);ontology alignment;performance;semantic web;semantic similarity;similarity measure;subject-matter expert	Valerie V. Cross;Xueheng Hu	2012	2012 IEEE International Conference on Fuzzy Systems	10.1109/FUZZ-IEEE.2012.6251265	ontology alignment;semantic similarity;computer science;ontology;artificial intelligence;semantic web;data mining;database;semantics;fuzzy set;information retrieval	NLP	-44.47240654454045	-67.95898477091384	185032
5d1b04a6c0fc9661f6046ad9e606db040e124ead	authorship attribution of texts: a review	nineteenth century;naive bayes;word length	We study the authorship attribution of documents given some prior stylistic characteristics of the author’s writing extracted from a corpus of known works, e.g., authentication of disputed documents or literary works. Although the pioneering paper based on word length histograms appeared at the very end of the nineteenth century, the resolution power of this and other stylometry approaches is yet to be studied both theoretically and on case studies such that additional information can assist finding the correct attribution. We survey several theoretical approaches including ones approximating the apparently nearly optimal one based on Kolmogorov conditional complexity and some case studies: attributing Shakespeare canon and newly discovered works as well as allegedly M. Twain’s newly-discovered works, Federalist papers binary (Madison vs. Hamilton) discrimination using Naive Bayes and other classifiers, and steganography presence testing. The latter topic is complemented by a sketch of an anagrams ambiguity study based on the Shannon cryptography theory. 1. Micro-style analysis	authentication;cryptography;cyclomatic complexity;document;emergence;kolmogorov complexity;naive bayes classifier;shannon (unit);steganography;stylometry;twain;text corpus;zero insertion force	Mikhail B. Malyutov	2005	Electronic Notes in Discrete Mathematics	10.1016/j.endm.2005.07.064	naive bayes classifier;computer science;artificial intelligence;mathematics;algorithm	Crypto	-41.678580665499446	-66.75443231994053	185514
04153dfe5e7285cafc6260056e073d970445824e	retrieving the vital status of patients with cancer using online obituaries	digital epidemiology;medical record linkage;vital status;web mining	The aim of this study was to develop a methodology to link mortality data from Internet sources with administrative data from electronic health records and to assess the performance of different record linkage methods. We extracted the electronic health records of all adult patients hospitalized at Rennes comprehensive cancer center between January 1, 2010 and December 31, 2015 and separated them in two groups (training and test set). We also extracted all available online obituaries from the most exhaustive French funeral home website using web scraping techniques. We used and evaluated three different algorithms (deterministic, approximate deterministic and probabilistic) to link the patients' records with online obituaries. We optimized the algorithms using the training set and then evaluated them in the test set. The overall precision was between 98 and 100%. The three classification algorithms performed better for men than women. The probabilistic classification decreased the number of manual reviews, but slightly increased the number of false negatives. To address the problem of long delays in the publication or sharing of mortality data, online obituary data could be considered for real-time surveillance of mortality in patients with cancer because they are easily available and time-efficient.	approximation algorithm;electronic health records;extraction;linkage (software);nci-designated cancer center;nci-designated comprehensive cancer center;neoplasms;obituary;patients;real-time transcription;review [publication type];test set;web site;web scraping;genetic linkage	Emmanuelle Sylvestre;Guillaume Bouzillé;Mathias Breton;Marc Cuggia;Boris Campillo-Gimenez	2018	Studies in health technology and informatics	10.3233/978-1-61499-852-5-571	cancer;knowledge management;medical emergency;vital status;medicine	Web+IR	-48.15989261517659	-70.16375125121102	185821
61165f6fdf445a2706578ba950202cde9a03cfb6	how short is a piece of string? : the impact of text length and text augmentation on short-text classification		Recent increases in the use and availability of short messages have created opportunities to harvest vast amounts of information through machine-based classification. However, traditional classification methods have failed to yield accuracies comparable to classification accuracies on longer texts. Several approaches have previously been employed to extend traditional methods to overcome this problem, including the enhancement of the original texts through the construction of associations with external data supplementation sources. Existing literature does not precisely describe the impact of text length on classification performance. This work quantitatively examines the changes in accuracy of a small selection of classifiers using a variety of enhancement methods, as text length progressively decreases. Findings, based on ANOVA testing at a 95% confidence interval, suggest that the performance of classifiers using simple enhancements decreases with decreasing text length, but that the use of more sophisticated enhancements risks over-supplementation of the text and consequent concept drift and classification performance decrease as text length increases.	concept drift;document classification	Austin Mccartney;Svetlana Hensman;Luca Longo	2017				NLP	-36.02008086870591	-68.25459300277036	186013
2e3bb403bdd7dcccbc5495be4084c64f88ed811f	automated geocoding of textual documents: a survey of current approaches		This survey article describes previous research addressing text-based document geocoding, i.e. the task of predicting the geospatial coordinates of latitude and longitude, that best correspond to an entire document, based on its textual contents. We describe (1) early document geocoding systems that use heuristics over place names mentioned in the text (e.g. names of cities and states), (2) probabilistic language modeling approaches, where generative models are built for different regions in the world (usually considering a discretization based on a rectangular grid) from the words occurring in a set of georeferenced training documents, which are then used to predict per-region probabilities for previously unseen test documents, (3) combinations of different models and heuristics, including clustering procedures, feature selection approaches, and/or language models built from different sources, and (4) recent approaches based on discriminative classification models.	cluster analysis;discretization;feature selection;floor and ceiling functions;geocoding;geographic coordinate system;geographic information system;heuristic (computer science);information retrieval;john d. wiley;language model;machine learning;regular grid;statistical classification;supervised learning;text-based (computing);wikipedia	Fernando Melo;Bruno Martins	2017	Trans. GIS	10.1111/tgis.12212	geography;data science;data mining;database;information retrieval	NLP	-37.25645629818828	-70.99229585465883	186588
eac9534de91f7ba87049ad108a84b63a94b0ab1a	performance and trends in recent opinion retrieval techniques		This paper presents trends and performance of opinion retrieval techniques proposed within the last 8 years. We identify major techniques in opinion retrieval and group them into four popular categories. We describe the state-of-the-art techniques for each category and emphasize on their performance and limitations. We then summarize with a performance comparison table for the techniques on different datasets. Finally, we highlight possible future research directions that can help solve existing challenges in opinion retrieval.	baseline (configuration management);blog;carroll morgan (computer scientist);comparison of raster-to-vector conversion software;context-sensitive language;digital library;document classification;google scholar;les racines du mal;lexicon;machine learning;os-tan;performance evaluation;relevance;scalability;sunway;text retrieval conference;word lists by frequency;yang	Sylvester Olubolu Orimaye;Saadat M. Alhashmi;Eu-Gene Siew	2015	Knowledge Eng. Review	10.1017/S0269888913000167	data science;data mining	Web+IR	-35.41277451582472	-66.81318154110612	187075
56e9261af089e3b603a15648b16badbcc6dc1661	algorithmic detection of inconsistent modeling among snomed ct concepts by combining lexical and structural indicators	quality assurance methods international health terminology standards development organization systematized nomenclature of medicine clinical terms algorithmic detection snomed ct concepts lexical indicator structural indicator electronic health record encoding clinical use positional similarity sets statistical significance;snomed ct;modeling inconsistency snomed ct lexical analysis terminology auditing terminology quality assurance;terminology quality assurance;modeling inconsistency;terminology auditing;lexical analysis;quality assurance biomedical engineering electronic health records encoding health care	SNOMED CT is important for clinical applications, such as Electronic Health Record (EHR) encoding. However, inconsistency in modeling its concepts may prevent SNOMED CT from providing proper support for clinical use. This study provides an effective methodology for locating inconsistently modeled SNOMED CT concepts. One can expect lexically similar concepts to be modeled similarly. Positional similarity sets, sets of lexically similar concepts having only one different word at the same position of their names, are introduced. Concepts in such sets have a higher likelihood of being unjustifiably inconsistently modeled. A technique to incorporate three structural indicators into the selected sets is provided to further improve the likelihood of finding inconsistently modeled concepts. An analysis of a sample of 50 such sets and for each of these three indicators is performed. The sample of positional similarity sets is found to have 18.6% inconsistent concepts. The use of structural indicators is shown to further improve the likelihood of finding inconsistently modeled concepts up to 41.6% with high statistical significance when compared to the previous sample of positional similarity sets. Positional similarity sets with different structural indicators are shown to help identify inconsistencies in concept modeling with high likelihood. Furthermore, such sets enable the comparison of concept modeling in the context of other lexically similar concepts, which enhances the effectiveness of corrections by auditors. Such quality assurance methods can be used to supplement IHTSDO's own efforts in order to improve the quality of SNOMED CT.	ct scan;systematized nomenclature of medicine	Ankur Agrawal;Yehoshua Perl;Christopher Ochs;Gai Elhanan	2015	2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2015.7359731	lexical analysis;computer science;snomed ct;data mining;database;information retrieval	DB	-44.67335874026654	-67.93154821109252	187295
3598d38ed6c7aac3f4e31b4a60945a45e349476b	crosslingual opinion extraction from author and authority viewpoints at ntcir-6	information retrieval	Opinion research has been paid much attention by the Information Retrieval community, and opinion holder extraction research is important for discriminating between opinions that are viewed from different perspectives. In this paper, we describe our experience of participation in the NTCIR-6 Opinion Analysis Pilot Task by focusing on opinion extraction results in Japanese and English. Our approach to opinion holder extraction was based on the discrimination between author and authority viewpoints in opinionated sentences, and the evaluation results were fair with respect to the Japanese documents.	information extraction;information retrieval	Yohei Seki	2007			public relations;political science;opinion;data mining;information retrieval	NLP	-37.26827824879184	-69.24000078584062	187728
213badcdfcaebc218ed340517a3c853b4036954d	anatomical entity mention recognition at literature scale	animals;unified medical language system;artificial intelligence;algorithms;humans;databases factual;anatomy	MOTIVATION Anatomical entities ranging from subcellular structures to organ systems are central to biomedical science, and mentions of these entities are essential to understanding the scientific literature. Despite extensive efforts to automatically analyze various aspects of biomedical text, there have been only few studies focusing on anatomical entities, and no dedicated methods for learning to automatically recognize anatomical entity mentions in free-form text have been introduced.   RESULTS We present AnatomyTagger, a machine learning-based system for anatomical entity mention recognition. The system incorporates a broad array of approaches proposed to benefit tagging, including the use of Unified Medical Language System (UMLS)- and Open Biomedical Ontologies (OBO)-based lexical resources, word representations induced from unlabeled text, statistical truecasing and non-local features. We train and evaluate the system on a newly introduced corpus that substantially extends on previously available resources, and apply the resulting tagger to automatically annotate the entire open access scientific domain literature. The resulting analyses have been applied to extend services provided by the Europe PubMed Central literature database.   AVAILABILITY AND IMPLEMENTATION All tools and resources introduced in this work are available from http://nactem.ac.uk/anatomytagger.   CONTACT sophia.ananiadou@manchester.ac.uk   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	anatomical entity;bibliographic database;bioinformatics;biological system;body of uterus;europe pubmed central (europe pmc);machine learning;national centre for text mining;ontology (information science);open biomedical ontologies;scientific literature;text corpus;truecasing;unified medical language system;body system	Sampo Pyysalo;Sophia Ananiadou	2014		10.1093/bioinformatics/btt580	natural language processing;computer science;bioinformatics;data mining;unified medical language system;information retrieval	Comp.	-35.0714045022908	-69.72227859558922	188476
635fa1876722df5a52e41c237ec4b3aaaf2abbf6	acquiring meaning for french medical terminology: contribution of morphosemantics	semantics;biomedical lexical database.;morphosemantics;neoclassical compounds;language;semantic rela- tions;france;natural language processing	Morphologically complex words, and particularly neoclassical compounds, form more than 60% of the neologisms in the biomedical field. Guessing their definitions and grouping them into semantic classes by means of lexical relations are thus two crucial improvements for handling these words, e.g., for information retrieval, indexing and text understanding applications. This paper describes a morphosemantic linguistic-based parser called DériF, currently developed in the framework of two projects, UMLF and VUMeF, and its application to French biomedical derived and compound words. It shows how the resulting morphologically tagged lexicon is enriched by semantic relations leading both to the synthesis of pseudo-definitions and to the constitution of classes of synonyms, hypo- and hypernyms.	class;handling (psychology);indexes;information retrieval;lexical substitution;lexicon;linguistics;neologism;nomenclature;parser;pseudo brand of pseudoephedrine;tracer	Fiammetta Namer;Pierre Zweigenbaum	2004	Studies in health technology and informatics	10.3233/978-1-60750-949-3-535	natural language processing;medical terminology;data mining;compound;artificial intelligence;parsing;search engine indexing;lexicon;neologism;synonym;medicine	NLP	-33.98072642921491	-70.25052156036412	189521
91fc24716e5fd503014e2a3b7f84b21710fd2f80	a scalable problem-solver for large knowledge-bases	cells biology libraries physics computing chemistry biological cells artificial intelligence equations vents information retrieval logic;knowledge based system;project halo;biology;data mining;acceleration;reasoning knowledge base systems project halo problem solving question answering;cognition;advanced placement exams;large knowledge bases;reasoning;knowledge base systems;scalable problem solver;advanced placement;knowledge based systems;advanced placement exams scalable problem solver large knowledge bases;problem solving;question answering;problem solving knowledge based systems;knowledge base	We describe a problem solver built to answer questions like those on Advanced Placement exams using knowledge bases authored by domain experts. The problem solver is designed to work independently of any particular knowledge base or domain. Given a question, the problem solver identifies those portions of the knowledge base that are relevant to the question. We found that simple heuristics for judging relevance significantly improved performance, with no drop in coverage.	heuristic (computer science);knowledge base;relevance;solver	Shaw Yi Chaw;Ken Barker;Bruce W. Porter;Dan Tecuci;Peter Z. Yeh	2009	2009 21st IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2009.108	acceleration;cognition;advanced placement exams;computer science;knowledge management;artificial intelligence;data science;theoretical computer science;knowledge-based systems;machine learning;reason;advanced placement	Robotics	-42.95626989501021	-66.96630452850566	189571
0b1cf029eaf97448cb8f0b6999a55db8cb55f598	a study in rule-specific issue categorization for e-rulemaking	erulemaking issue categorization egovernment;issue categorization;bepress selected works;e rulemaking;notice and comment process;ceri;human interannotator agreement;public comment;machine learning;automatic annotation;flat text categorization;rule specific issue categorization;erulemaking;hierarchical text categorization;text categorization;egovernment;electronic rulemaking;cornell e rulemaking initiative	We address the e-rulemaking problem of categorizing public comments according to the issues that they address. In contrast to previous text categorization research in e-rulemaking [5, 6], and in an attempt to more closely duplicate the comment analysis process in federal agencies, we employ a set of rule-specific categories, each of which corresponds to a significant issue raised in the comments. We describe the creation of a corpus to support this text categorization task and report interannotator agreement results for a group of six annotators. We outline those features of the task and of the e-rulemaking context that engender both a non-traditional text categorization corpus and a correspondingly difficult machine learning problem. Finally, we investigate the application of standard and hierarchical text categorization techniques to the e-rulemaking data sets and find that automatic categorization methods show promise as a means of reducing the manual labor required to analyze large comment sets: the automatic annotation methods approach the performance of human annotators for both flat and hierarchical issue categorization. 1. BACKGROUND AND INTRODUCTION Every year federal agencies publish in the Federal Register [on-line version: http://www.gpoaccess.gov/fr/index.html] several thousand documents on which they seek public comment. Most of these are proposed rules in areas including: environmental protection; agriculture standards; drug, workplace, and consumer safety; import and export controls; air, highway, and water-based transportation safety; communications; and various federal grant and aid programs. Federal statutes, especially the Administrative Procedure Act [5 U.S. C. §§551 et seq.], generally require such regulations to go through this “notice and comment” process before they can become final and binding on the public. In addition, agencies may request comments on a category of documents known as “guidance.” These documents, which often closely resemble proposed rules in form, have different names but share the characteristic that they are advice, or warning, about how the agency will exercise its power or interpret the law, rather than binding rules themselves. Sometimes, the agency is legally required to seek public comment before it finalizes guidance; other times, it simply chooses to do so. Finally, there is a third, miscellaneous category of documents in the Federal Register on which comments are solicited. These include such things as draft statements of environmental or other impacts of proposed agency action. Again, the agency may be required by one of its statutes to allow the public to comment, or it may be doing so as a matter of policy. Until the 1990s, all comments came to the agency in hard copy — through hand delivery, conventional, or express mail. As electronic transmission and then the Internet became more generally available, agencies began to receive comments first by fax and then by e-mail. Most recently, agencies have provided web portals for comment submission. Although a few agency-specific sites remain, most have been superseded by a central portal, www.regulations.gov, which now provides access to all agencies’ proposed rules and guidance, as well as to some of the documents in the third, miscellaneous category. Comments can be submitted to all agencies through this portal. (Commentors can continue to use the older submission methods as well.) Transfer of the notice-and-comment process to the web — and, more broadly, the use of information technology to support any step in the rulemaking process — is known as electronic rulemaking (e-rulemaking). At the close of the public comment period (typically, 30–60 days) the comments must be reviewed to determine what issues they contain. The comment process is not a vote; its purpose is not to tally the commentors’ preferences for or against the proposal. By law, agencies must act based on factors, and to further objectives, specified by their authoThese include such widely used names as “statements of policy” and “interpretive rule,” as well as more agencyspecific names as “circular” and “bulletin.” rizing statutes. Along with the proposed rule, guidance, or other document on which it seeks comment, the agency is supposed to reveal its underlying data, as well as its legal and policy rationale. Ideally, the comments will address the substance of the proposal, and discuss how well the agency has met the statutory factors and objectives. Ultimately, the agency’s responsibility (enforceable by the courts in many cases) is to issue a statement accompanying any final action it takes; this statement demonstrates its attention to the comments by responding to significant criticisms they contain, and explaining why it rejected alternatives they suggest [10]. Reviewing the comments to determine what relevant issues they raise can present substantial challenges for agencies. Sometimes they are working under a deadline for final decision, set by their statute or a court order. Even with no formal time limits, the process is often intense and laborious. As we observed working with rulewriters and analysts in two units within the Department of Transportation, analysts read the comments and manually mark, code, summarize or partially re-type portions. These “annotations” identify the relevant issues raised by commentors, and organize the various references to each in a fashion that facilitates analysis by the entire group working on the rule (or other proposal). This process ultimately leads to preparation of the accompanying final statement. As the number, or number-pluscomplexity, of comments increases, the process of finding, extracting, and organizing material raising relevant issues becomes proportionately more challenging. Indeed, agencies that have the resources to do so frequently hire outside contractors to read and summarize large comment sets. The current paper.This paper reports results in a project to determine the degree to which automatic issue categorization can facilitate reviewing public comments: given a comment set, the automated system should determine for each sentence in each comment, which of a group of pre-defined issues it raises, if any. We build on the work of Kwon & Hovy [5] and Kwon et al.[6], which applies machine learningbased text categorization techniques (see Sebastiani [8] for an overview) to automate the comment sorting process. In particular, Kwon et al. [5, 6] first develop a set of eight general topic codes — economic, environment, government responsibility, health, legal, policy, pollution, and technology and train a machine learning algorithm (they use a support vector machines (SVMs) [11]) to classify individual sentences according to the topics they address. Using a set of 160 comments divided appropriately into training, development, and test sets, they report F-measure scores of 0.30 to 0.83 depending on the topic, with an average Fmeasure score of 0.67. SVMs significantly outperforms three baselines that assign to each sentence (a) all topics, (b) the most common topic, and (c) any topic with a morphological variant of its name in the sentence. In addition, the system performance approaches that of human annotator agreement (0.72 F-measure). Rather than use a small, closed set of general topic codes, The Federal Transit Authority and the Office of Civil Rights. however, we investigate the possibility of categorizing sentences according to the usually much larger, and possibly hierarchical, set of rule-specific issues employed by rulewriters as they sort and analyze the comments. In this manner, we aim to replicate more closely what agency personnel now do manually. The longer range goal is to employ automatic issue categorization to speed up the (required) manual review of public comments by grouping similar comment snippets so that rulewriters can read and respond to them as a whole. Another application would facilitate reply comment periods by allowing agencies to rapidly provide the public with first-round comments sorted by issue, to aid and channel responsive submissions. In the sections below, we begin by presenting the first in a series of sentence-level text categorization corpora to be developed in this project by the Cornell e-Rulemaking Initiative (CeRI). We describe the creation and annotation of the corpus, focusing on characteristics of the notice-and-comment domain that engender a nontraditional text categorization corpus and a correspondingly difficult machine learning task. Interannotator results are presented for a group of six annotators. We next investigate the application of both standard and hierarchical text categorization techniques to the e-rulemaking data sets. We find that automatic text categorization methods show promise as a means of reducing the manual labor required to analyze public comment sets: the sentencelevel issue annotation techniques approach the performance of human annotators for both flat and hierarchical issue categorization and outperform a baseline that selects the most common category for each sentence. The categorization scheme includes 17 issues, some of which can be further divided to create a set of 39 fine-grained issues. Using an overlap measure of agreement, human annotators achieve interannotator agreement scores of 64.7% and 46.4% for the 17 and 39 issues, respectively. Measured across three issue categorization data sets, the best-performing automatic categorization technique is competitive with the interannotator agreement results, reaching levels of 59-66% and 42-56% accuracy for the 17 and 39 issues, respectively.	algorithm;baseline (configuration management);categorization;code;comment (computer programming);design rationale;document classification;email;emoticon;f1 score;fax;federal standard 1037c;internet;machine learning;name binding;norm (social);online and offline;organizing (structure);portals;privacy policy;self-replicating machine;sorting;support vector machine;test set;text corpus;turing machine	Claire Cardie;Cynthia Farina;Adil Aijaz;Matt Rawding;Stephen Purpura	2008			computer science;data science;data mining;information retrieval;categorization	NLP	-36.813369649872115	-69.82230284710892	189846
833230f1396f9a7093455a517e4a56472d596fab	comparing programming language comprehension between novice and expert programmers using eeg analysis	empirical study;program comprehension;human factors;neuroscientific;electroencephalogram	For programming language comprehension, high cognitive skills (e.g., reading, writing, working memory, etc.) and information processing are required. However, there are few papers that approach this from a neuroscientific perspective. In this paper, we examine program comprehension neuroscientifically and also observe the differences between novice and expert programmers. We designed an EEG (electroencephalogram) experiment and observed 18 participants during a series of program comprehension tasks. We found clear differences in program comprehension ability between novice and expert programmers. Experts exhibited higher brainwave activation than novices in electrodes F3 and P8. These results indicate that experts have outstanding program comprehension-associated abilities such as digit encoding, coarse coding, short-term memory, and subsequent memory effect. Our findings can serve as a foundation for future research in this pioneering field.	electroencephalography;information processing;list comprehension;long short-term memory;memory effect;neural oscillation;program comprehension;programmer;programming language	SeolHwa Lee;Andrew Matteson;Danial Hooshyar;SongHyun Kim;JaeBum Jung;GiChun Nam;Heuiseok Lim	2016	2016 IEEE 16th International Conference on Bioinformatics and Bioengineering (BIBE)	10.1109/BIBE.2016.30	natural language processing;computer science;artificial intelligence;human factors and ergonomics;empirical research	SE	-43.912570355648796	-75.02839897265059	189997
65bc32f069e502f288b063b64864910ee92ce403	effectiveness of text processing in patent documents visualization	high dimensional data patent documents visualization text processing algorithm software package business intelligence purposes patents bibliographic parameters text mining software tool;text analysis;data mining;patents data visualization text processing technological innovation abstracts tv mobile handsets;software tools;text analysis data mining software packages software tools;software packages	This paper analyzes effectiveness of text processing algorithm applied to different document parts for a data set consisting of patent documents. The algorithm is part of the software package which is used as a tool for business intelligence purposes. The tool assembles patent data from publicly available data bases, collects and analyzes patents bibliographic parameters and performs text mining. High-dimensional data contained in the patent documents are transformed into lower dimensionality space (2D or 3D), clustered and visualized. These features of the software tool enabled estimation of the effectiveness of text processing algorithm when run on different parts of the patent, such as abstract, claim, international patent code description and detailed patent description.	algorithm;database;programming tool;text mining	Miroslava Drazic;Dragan Kukolj;Milana Vitas;Maja Pokric;Sanja Manojlovic;Zeljko Tekic	2013	2013 IEEE 11th International Symposium on Intelligent Systems and Informatics (SISY)	10.1109/SISY.2013.6662588	patent visualisation;text mining;computer science;package development process;data science;data mining;information retrieval	SE	-40.37642059425172	-67.74763539679252	190222
8b2ed97cd73602353c010cc01ebe65914aa668a1	exploring language variation across europe - a web-based tool for computational sociolinguistics.		Language varies not only between countries, but also along regional and socio-demographic lines. This variation is one of the driving factors behind language change. However, investigating language variation is a complex undertaking: the more factors we want to consider, the more data we need. Traditional qualitative methods are not well-suited to do this, and therefore restricted to isolated factors. This reduction limits the potential insights, and risks attributing undue importance to easily observed factors. While there is a large interest in linguistics to increase the quantitative aspect of such studies, it requires training in both variational linguistics and computational methods, a combination that is still not common. We take a first step here to alleviating the problem by providing an interface, www.languagevariation.com, to explore large-scale language variation along multiple socio-demographic factors – without programming knowledge. It makes use of large amounts of data and provides statistical analyses, maps, and interactive features that will enable scholars to explore language variation in a data-driven way.	computation;interactivity;map;variational principle	Dirk Hovy;Anders Johannsen	2016			natural language processing;computer science;linguistics	NLP	-34.807171101172045	-76.42169703587363	190393
d23eefa40a079a97341e0d3a27171022d83a962a	research and development in natural language understanding	inference system;new generation natural language;knowledge base;natural language understanding;linguistic knowledge acquisition;spoken language systems project;system component;darpa application;natural language system;knowledge acquisition system;kl-two knowledge representation;software component;system integration;natural language;natural language interface	Brief Summary of Objectives: There are three objectives of the contract: to perform research and development in parallel parsing, semantic representation, ill-formed input, discourse, and tools for linguistic knowledge acquisition, and to integrate software components from BBN and elsewhere to produce Janus, DARPA's New Generation Natural Language Interface, and to demonstrate state-of-theart natural language technology in DARPA applications. The following software has been distributed: natural language system; IRACQ, knowledge acquisition system; System components and knowledge bases of Janus; KL-TWO knowledge representation and inference system integrated with Janus; various components for DARPA's Spoken Language Systems Project at BBN.	component-based software engineering;glr parser;inference engine;janus;kl-one;knowledge acquisition;knowledge representation and reasoning;language technology;natural language understanding;natural language user interface;parsing	Ralph M. Weischedel	1989			natural language processing;language identification;knowledge base;natural language programming;multinet;universal networking language;question answering;object language;natural language user interface;computer science;component-based software engineering;open knowledge base connectivity;linguistics;modeling language;natural language;programming language;language technology;system integration	NLP	-33.946821295576484	-77.4621762292425	190518
b786b9ef56e7de7c1202e9d7c761f48de102ce89	geographical analysis of the vernacular	language analysis;gis;bbc voices;clustering;language mapping;geolinguistics	The BBC Voices project of 2005 resulted in a large repository of lexical, phonological and grammatical data from the UK, which included geographical references. In order to investigate the relationship between language and geography, various clustering algorithms have been applied to the BBC Voices data. Results show a clear spatial relationship, with well-defined, contiguous regions of UK language being identified. In order to prove the clustering methodology, Bayesian models have been generated for each region, and these have been tested using a set of non-standard expressions contributed by a small number of participants. Results of this second stage indicate that the models are, in most cases, able to identify the geographical region of each test participant based on the linguistic items they use.	geospatial analysis	John Holliday;Clive Upton;Ann Thompson;Jonathan Robinson;Jon Herring;Holly Gilbert;Paul Norman	2013	J. Information Science	10.1177/0165551512470049	natural language processing;geomatics;computer science;database;linguistics;cluster analysis	Theory	-37.68497942555713	-71.68964167269553	190881
94c74910e8716f5d2db536efcbf4456c7baf410f	clinical text retrieval - an overview of basic building blocks and applications	computer and systems sciences;systemvetenskap informationssystem och informatik;information systems;data och systemvetenskap	This article describes information retrieval, natural language processing and text mining of electronic patient record text, also called clinical text. Clinical text is written by physicians and nurses to docu- ment the health care process of the patient. First we describe some char- acteristics of clinical text, followed by the automatic preprocessing of the text that is necessary for making it usable for some applications. We also describe some applications for clinicians including spelling and grammar checking, ICD-10 diagnosis code assignment, as well as other applications for hospital management such as ICD-10 diagnosis code validation and detection of adverse events such as hospital acquired infections. Part of the preprocessing makes the clinical text useful for faceted search, al- though clinical text already has some keys for performing faceted search such as gender, age, ICD-10 diagnosis codes, ATC drug codes, etc. Pre- processing makes use of ICD-10 codes and the SNOMED-CT textual descriptions. ICD-10 codes and SNOMED-CT are available in several languages and can be considered the modern Greek or Latin of medical language. The basic research presented here has its roots in the chal- lenges described by the health care sector. These challenges have been partially solved in academia, and we believe the solutions will be adapted to the health care sector in real world applications.		Hercules Dalianis	2014		10.1007/978-3-319-12511-4_8	text mining;speech recognition;computer science;data science;noisy text analytics;data mining	Web+IR	-48.21808390749313	-68.86570322393302	191198
e1dd19957934516aa948cf7ca6017ab3b31b5a1b	a lightweight approach for extracting disease-symptom relation with metamap toward automated generation of disease knowledge base	necessitate disease knowledge base;metamap output;simple approach;low cost;automated generation;disease-symptom relation;lightweight approach;disease knowledge base;efficient extraction;straightforward approach;symptomatic expression;symptomatic term;typical symptomatic expression	Diagnostic decision support systems necessitate disease knowledge base, and this part may occupy dominant portion in the total development cost of such systems. Accordingly, toward automated generation of disease knowledge base, we conducted a preliminary study for efficient extraction of symptomatic expressions, utilizing MetaMap, a tool for assigning UMLS (Unified Medical Language System) semantic tags onto phrases in a given medical literature text.#R##N##R##N#We first utilized several tags in the MetaMap output, related to symptoms and findings, for extraction of symptomatic terms. This straightforward approach resulted in Recall 82% and Precision 64%. Then, we applied a heuristics that exploits certain patterns of tag sequences that frequently appear in typical symptomatic expressions. This simple approach achieved 7% recall gain, without sacrificing precision.#R##N##R##N#Although the extracted information requires manual inspection, the study suggested that the simple approach can extract symptomatic expressions, at very low cost. Failure analysis of the output was also performed to further improve the performance.		Takashi Okumura;Yuka Tateisi	2012		10.1007/978-3-642-29361-0_20	computer science;artificial intelligence;data mining;communication	NLP	-34.19373695993454	-69.01613490307999	191384
aa2aac58938ee1277aeef4377680da61b24db345	tell me what you do and i'll tell you what you are: learning occupation-related activities for biographies	information technology;general population;specific activity;statistical techniques;computer science	Biography creation requires the identification of important events in the life of the individual in question. While there are events such as birth and death that apply to everyone, most of the other activities tend to be occupation-specific. Hence, occupation gives important clues as to which activities should be included in the biography. We present techniques for automatically identifying which important events apply to the general population, which ones are occupation-specific, and which ones are person-specific. We use the extracted information as features for a multi-class SVM classifier, which is then used to automatically identify the occupation of a previously unseen individual. We present experiments involving 189 individuals from ten occupations, and we show that our approach accurately identifies general and occupation-specific activities and assigns unseen individuals to the correct occupations. Finally, we present evidence that our technique can lead to efficient and effective biography generation relying only on statistical techniques.	experiment;natural language processing;support vector machine	Elena Filatova;John M. Prager	2005		10.3115/1220575.1220590	computer science;artificial intelligence;specific activity;information technology	NLP	-45.122211775813895	-73.95425907491231	191853
b147ce1736cc9f781fd4190835708ae1aaf4cdb1	cat: computer aided triage improving upon the bayes risk through ε-refusal triage rules	classification;machine learning	Manual extraction of information from electronic pathology (epath) reports to populate the Surveillance, Epidemiology, and End Result (SEER) database is labor intensive. Systematizing the data extraction automatically using machine-learning (ML) and natural language processing (NLP) is desirable to reduce the human labor required to populate the SEER database and to improve the timeliness of the data. This enables scaling up registry efficiency and collection of new data elements. To ensure the integrity, quality, and continuity of the SEER data, the misclassification error of ML and NPL algorithms needs to be negligible. Current algorithms fail to achieve the precision of human experts who can bring additional information in their assessments. Differences in registry format and the desire to develop a common information extraction platform further complicate the ML/NLP tasks. The purpose of our study is to develop triage rules to partially automate registry workflow to improve the precision of the auto-extracted information. This paper presents a mathematical framework to improve the precision of a classifier beyond that of the Bayes classifier by selectively classifying item that are most likely to be correct. This results in a triage rule that only classifies a subset of the item. We characterize the optimal triage rule and demonstrate its usefulness in the problem of classifying cancer site from electronic pathology reports to achieve a desired precision. From the mathematical formalism, we propose a heuristic estimate for triage rule based on post-processing the soft-max output from standard machine learning algorithms. We show, in test cases, that the triage rule significantly improve the classification accuracy.		Nicolas W. Hengartner;Leticia Cuellar;Xiao-Cheng Wu;Georgia D. Tourassi;John X. Qiu;J. Blair Christian;Tanmoy Bhattacharya	2018		10.1186/s12859-018-2503-9		DB	-47.005695299827345	-69.02237766365288	192264
e899f747dd1e0bd626117e07739926fd04183d29	computer assistance for digital libraries: contributions to middle-ages and authors' manuscripts exploitation and enrichment	information retrieval digital libraries document image processing indexing;image recognition;software libraries information retrieval writing information analysis authentication image color analysis shape document image processing history image recognition;writing styles;manuscripts analysis;history;software libraries;information retrieval;digital library;digital libraries;authentication;handwriting documents computer assistance digital libraries middle ages manuscripts manuscripts analysis information retrieval ancient manuscripts writing styles shapes analysis writing indexation document image processing;computer assistance;shape analysis;shapes analysis;ancient manuscripts;indexes;shape;indexing;middle ages manuscripts;image color analysis;indexation;handwriting documents;document image processing;writing;digital image;writing indexation;information analysis	In this paper, we are interested in digitized middle-ages and 18th century authors' manuscripts analysis for the realization of suitable assistance tools dedicated to humanists and historians. The purpose is to help them in their intuitive and empirical work of information retrieval (word retrieval, identification of writers, writing styles classification...) and to enrich ancient manuscripts with additional descriptions which are based on writing styles and shapes analysis. In this project, we intend to create efficient software that analyses images contents and automatically extracts necessary information for writing indexation. This work can be done by adapting our expertise in document image processing to the domain of middle-ages and authors' manuscript. The development of such dedicated solutions is a complex task which reaches today technological limits due to the difficulty to automatically process a growing mass of digitized images of handwriting documents from different origins	digital library;gene ontology term enrichment;image processing;information retrieval;library (computing)	Véronique Eglin;Frank Lebourgeois;Stéphane Bres;Hubert Emptoz;Yann Leydier;Ikram Moalla;Fadoua Drira	2006	Second International Conference on Document Image Analysis for Libraries (DIAL'06)	10.1109/DIAL.2006.9	computer science;multimedia;world wide web;information retrieval	SE	-39.6184341827223	-66.85102895414589	193647
4571ca5cd6ed29bebfdaf44cf4c0ca1ab79b9a28	the bioscope corpus: annotation for negation, uncertainty and their scope in biomedical texts	corpus size;annotation guideline;sentence level;biomedical text;biological scientific abstract;biological full paper;annotation process;bioscope corpus;independent linguist annotators;ambiguity level;corpus annotation project	This article reports on a corpus annotation project that has produced a freely available resource for research on handling negation and uncertainty in biomedical texts (we call this corpus the BioScope corpus). The corpus consists of three parts, namely medical free texts, biological full papers and biological scientific abstracts. The dataset contains annotations at the token level for negative and speculative keywords and at the sentence level for their linguistic scope. The annotation process was carried out by two independent linguist annotators and a chief annotator – also responsible for setting up the annotation guidelines – who resolved cases where the annotators disagreed. We will report our statistics on corpus size, ambiguity levels and the consistency of annotations.	speculative execution;text corpus	György Szarvas;Veronika Vincze;Richárd Farkas;János Csirik	2008			natural language processing;computer science;corpus linguistics;text corpus;linguistics;information retrieval	NLP	-34.42758397394108	-69.9698517247335	194110
77808a0030cbcf9e88b0608cb4b382e998a8fe44	incidence rate of canonical vs. derived medical terminology in natural language		Medical terminology appears in the natural language in multiple forms: canonical, derived or inflected form. This research presents an analysis of the form in which medical terminology appears in Romanian and English language. The sources of medical language used for the study are web pages presenting medical information for patients and other lay users. The results show that, in English, medical terminology tends to appear more in canonical form while, in the case of Romanian, it is the opposite. This paper also presents the service that was created to perform this analysis. This tool is available for the general public, and it is designed to be easily extensible, allowing the addition of other languages.	incidence matrix;natural language;nomenclature;page (document);patients;programming languages;web page	Vasile Topac;Daniel-Alexandru Jurcau;Vasile Stoicu-Tivadar	2015	Studies in health technology and informatics	10.3233/978-1-61499-512-8-5	incidence (epidemiology);data mining;web page;medical terminology;canonical form;romanian;of the form;natural language processing;natural language;artificial intelligence;unified medical language system;computer science	NLP	-47.94279915525357	-68.17547803377832	194141
63af018720d669571514a4afcc6b7c33fc3ddef6	a new alignment algorithm to identify definitions corresponding to abbreviations in biomedical text	machine learning data mining text mining dynamic programming heuristic algorithms laboratories high performance computing biology text analysis training data;information retrieval;pairwise sequence alignment;text analysis;text alignment algorithm;pairwise sequence alignment algorithm text alignment algorithm biomedical text analysis biomedical literature retrieval abbreviation extraction;abbreviation extraction;biomedical literature retrieval;medical information systems;text analysis information retrieval medical information systems;pairwise sequence alignment algorithm;biomedical text analysis	The exploding growth of the biomedical literature presents many challenges for biological researchers. One such challenge is from the use of a great deal of abbreviations. Extracting abbreviations and their definitions accurately is very helpful to biologists and also facilitates biomedical text analysis. Among existing approaches, text alignment algorithms are simple, effective and require no training data. However, state of the art alignment algorithms could not identify the definitions of irregular abbreviations (e.g., <CNS1, cyclophilin seven suppressor>). We propose an algorithm analogous to pairwise sequence alignment, in which it is given a penalty score if there are two unmatched characters separately from the abbreviation and definition, and in this way some irregular abbreviations are found.	algorithm;sequence alignment;text mining	Yun Xu;Zhihao Wang;YuZhong Zhao	2008	First International Workshop on Knowledge Discovery and Data Mining (WKDD 2008)	10.1109/WKDD.2008.53	text mining;computer science;data science;data mining;information retrieval	Comp.	-35.69736150244916	-69.5468175537023	194564
33359c89a8869c1f4fca3c9645bf17b3163deeba	entailment-based text exploration with application to the health-care domain	textual entailment relation;exploration process;novel text exploration model;health-care domain;optional concept taxonomy;standard concept-based exploration;statement-based exploration;entailment graph;exploration system;statement level;entailment-based text exploration	We present a novel text exploration model, which extends the scope of state-of-the-art technologies by moving from standard concept-based exploration to statement-based exploration. The proposed scheme utilizes the textual entailment relation between statements as the basis of the exploration process. A user of our system can explore the result space of a query by drilling down/up from one statement to another, according to entailment relations specified by an entailment graph and an optional concept taxonomy. As a prominent use case, we apply our exploration system and illustrate its benefit on the health-care domain. To the best of our knowledge this is the first implementation of an exploration system at the statement level that is based on the textual entailment relation.	textual entailment	Meni Adler;Jonathan Berant;Ido Dagan	2012			natural language processing;textual entailment;computer science;data mining;algorithm	NLP	-40.19391985560372	-69.87257117436386	196056
76d8e17cbf620d23a55f1fa924b465eb349ac566	extracting formulaic and free text clinical research articles metadata using conditional random fields	conditional random field;formulaic metadata;body text;metadata field;free text field;free text;formulaic meta-data;extracting formulaic;macro level trend;acceptable level;important metadata;clinical research article	We explore the use of conditional random fields (CRFs) to automatically extract important metadata from clinical research articles. These metadata fields include formulaic metadata about the authors, extracted from the title page, as well as free text fields concerning the study’s critical parameters, such as longitudinal variables and medical intervention methods, extracted from the body text of the article. Extracting such information can help both readers conduct deep semantic search of articles and policy makers and sociologists track macro level trends in research. Preliminary results show an acceptable level of performance for formulaic metadata and a high precision for those found in the free text.	conditional random field;feature engineering;information extraction;semantic search	Sein Lin;Jun-Ping Ng;Shreyasee S. Pradhan;Jatin Shah;Ricardo Pietrobon;Min-Yen Kan	2010			natural language processing;speech recognition;computer science;data mining;world wide web;information retrieval	NLP	-33.93366688174753	-69.21587856880166	196592
52679f4ab520b642d996c4fe7da2f6450b316dd2	information extraction a multidisciplinary approach to an emerging information technology		1 I n t r o d u c t i o n Informat ion extraction (IE) [4, 12] is the mapping of natural language texts (such as newswire reports, newspaper and journal articles, patents, electronic mail, World Wide Web pages, etc.) into predefined, structured representations, or templates, which, when filled, represent an extract of key information from the original text. The information pertains to entities of interest in the application domain (e.g. companies or persons), or to relations between such entities, usually in the form of events in which the entities take part (e.g. company takeovers, management successions). Once extracted, the information can then be stored in databases to be queried, da ta mined, summarised in natural language, etc. To date most work on IE has been done in English, s t imulated to a large degree by the DARPA Message Understanding Conferences [6, 13]. However,	information extraction	Maria Teresa Pazienza	1997		10.1007/3-540-63438-X	systems engineering;knowledge management;management science	NLP	-36.38429960093214	-66.85833596346424	198027
9087600d1d9af4b33de7b636bb3e03983841fb98	ontology extension and population: an approach for the pharmacotherapeutic domain	ontology expansion;ontology population;ontology learning;ontology extension;pharmacotherapeutic	For several years ontologies have been seen as a solution to share and reuse knowledge between humans and machines. An ontology is a knowledge photography at the moment of its creation. Nevertheless, in order to keep an ontology useful throughout time, it must be expanded and maintained regularly, mainly in the pharmacotherapeutic domain. Drug-therapy needs up-to-date and reliable information. Unfortunately, achieving a systematic ontology updating has is an arduous and a tedious task that becomes a bottleneck. To limit this obstacle we need methods that expedite the process of extension and population.This proposal aims the designing and validating method able to extract, from a corpus of summary of product characteristics and a pharmacotherapeutic ontology, the relevant knowledge to be added to the ontology.	population	Jorge Cruanes	2011		10.1007/978-3-642-22327-3_51	upper ontology;ontology alignment;bibliographic ontology;computer science;knowledge management;ontology;data science;data mining;ontology-based data integration;owl-s;process ontology;suggested upper merged ontology	AI	-36.44785161357889	-68.11278327013373	198834
9cb2e6b5feab2267f6c31a96b0824d7bd354e12e	a study of biomedical concept identification: metamap vs. people	natural language processing;unified medical language system	Although huge amounts of unstructured text are available as a rich source of biomedical knowledge, to process this unstructured knowledge requires tools that identify concepts from free-form text. MetaMap is one tool that system developers in biomedicine have commonly used for such a task, but few have studied how well it accomplishes this task in general. In this paper, we report on a study that compares MetaMap's performance against that of six people. Such studies are challenging because the task is inherently subjective and establishing consensus is difficult. Nonetheless, for those concepts that subjects generally agreed on, MetaMap was able to identify most concepts, if they were represented in the UMLS. However, MetaMap identified many other concepts that peo-ple did not. We also report on our analysis of the types of failures that MetaMap exhibited as well as trends in the way people chose to identify concepts.	biomedicine;c10orf2 gene;checking (action);choose (action);consensus (computer science);lelia decempunctata;medline;phrases;reference standards;unified medical language system;vocabulary	Wanda Pratt;Meliha Yetisgen-Yildiz	2003	AMIA ... Annual Symposium proceedings. AMIA Symposium		data science;natural language processing;unified medical language system;medline;chose;computer science;artificial intelligence	SE	-36.42217783247869	-68.34744421725567	198836
d3c402379d592fd541f9b070a39c747c5a7f039c	tools for rapid customization of s2s systems for emergent domains.		Component models of speech-to-speech translation (S2S) systems need to be customized to emerging needs. In this demonstration, we will showcase the technical functionality of BBN’s domain customization tools for S2S systems that allow subject matter experts to augment an existing S2S system with new vocabulary items and translation rules using a web-based user interface. To reduce the user effort and time, our tools leverage Wikipedia as a linguistic resource for enrichment of domain profiles by finding lexical items and translations related to the domain. In a recent evaluation of BBN S2S system customized for using these tools, we found 15% (relative) reduction in word error rate as well as 30% (relative) reduction in untranslatable words when used within customized conversational domains.	emergence;gene ontology term enrichment;subject matter expert turing test;user interface;vocabulary;web application;wikipedia;word error rate	Rohit Kumar;Matthew E. Roy;Sanjika Hewavitharana;Dennis Mehay;Nina Zinovieva	2015			personalization;human–computer interaction;speech recognition;computer science	HCI	-34.71733227016398	-75.05189640975607	199209
6a87403d69ffa51a2288cc7b7bdc940a9ddeeb18	a joint local-global approach for medical terminology assignment		In community-based health services, vocabulary gap between health seekers and community generated knowledge has hindered data access. To bridge this gap, this paper presents a scheme to label question answer(QA) pairs by jointly utilizing local mining and global learning approaches. Local mining attempts to label individual QA pair by independently extracting medical concepts from the QA pair itself and mapping them to authenticated terminologies. However, it may suffer from information loss and lower precision, which are caused by the absence of key medical concepts and presence of irrelevant medical concepts. Global learning, on the other hand, works towards enhancing the local mining via collaboratively discovering missing key terminologies and keeping off the irrelevant terminologies by analyzing the social neighbors. Practically, this unsupervised scheme holds potential to large-scale data.	authentication;data access;relevance;software quality assurance;unsupervised learning;vocabulary	Liqiang Nie;Mohammad Akbari;Tao Li;Tat-Seng Chua	2014			computer science;data mining;world wide web;information retrieval	ML	-47.84914453356827	-67.04133830277698	199841
