id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
c3f4c44a5310ddd6259071ba3fa70559c48a27b5	metacube xtm: a multidimensional metadata approach for semantic web warehousing systems	xml topic maps;information model;red www;metadata;interoperabilite;interoperabilidad;integration information;xml language;interrogation base donnee;reseau web;interrogacion base datos;semantics;schema integration;almacen dato;semantica;semantique;semantic heterogeneity;information integration;specification donnee;internet;intercambio electronico de datos;heterogeneidad;integracion informacion;metadonnee;especificacion datos;semantic web;multidimensional data base;world wide web;base donnee multidimensionnelle;echange donnee informatise;metadatos;interoperability;entrepot donnee;data warehouse;database query;langage xml;lenguaje xml;electronic data interchange;data specification;heterogeneity;heterogeneite	Providing access and search among multiple, heterogeneous, distributed and autonomous data warehouses has become one of the main issues in the current research. In this paper, we propose to inte- grate data warehouse schema information by using metadata represented in XTM (XML Topic Maps) to bridge possible semantic heterogeneity. A detailed description of an architecture that enables the efficient proc- essing of user queries involving data from heterogeneous is presented. As a result, the interoperability is accomplished by a schema integration approach based on XTM. Furthermore, important implementation as- pects of the MetaCube-XTM prototype, which makes use of the Meta Data Interchange Specification (MDIS), and the Open Information Model, complete the presentation of our approach.	semantic web	Binh Thanh Nguyen;A Min Tjoa;Oscar Mangisengi	2003		10.1007/978-3-540-45228-7_9	interoperability;the internet;xml;information model;computer science;information integration;heterogeneity;data warehouse;semantic web;electronic data interchange;data mining;database;semantics;metadata;world wide web	DB	-36.59421239374832	11.577588306706364	43462
f97a17bdbfe7419cbf5935ce73a71a3c86eee9c1	an approach to analyzing user preference based dynamic service composition	composability;service composition;transfer matrix;user's preference;web service	As a way to compose independent service together to fulfill a function, service composition is an important means for flexible and rapid information integration of complex distribution application in open and heterogeneous environment. However, the diversity of requirements makes it difficult to guarantee the correctness of service composition. This paper proposes a hierarchical dynamic service composition net (HDSC-net), and user's preference based mechanism for service composition. HDSC-net is used to model operation, the relationships between operations, operation mapping, and user's preference. Transfer matrix is constructed to express the relationships between states, while theories of Petri nets help prove the composability of service. The strategy for dynamic service composition and its corresponding enforcement method are also proposed. A case study of Travel Service demonstrates the applicability of proposed method and its effectiveness.	correctness (computer science);petri net;requirement;service composability principle;theory;transfer matrix	Guisheng Fan;Huiqun Yu;Liqiong Chen;Caizhu Yu	2010	JSW		web service;transfer matrix;differentiated service;computer science;knowledge management;database;data as a service;world wide web	HCI	-45.181187215589176	15.661533159472071	43496
453c11462c01a6797c1b5fb2e6623c274ea2d73c	resource discovery in federated systems with voluntary sharing	management system;resource discovery;resource manager;dht networks;multicasting;resource sharing;resource record;bandwidth;middleware;p2p networks	Federated system is a popular paradigm for multiple organizations to collaborate and share resources for common benefits. In practice, however, these organizations are often within different administrative domains and thus demand autonomous resource management, as opposed to blindly exporting their resources for efficient search. To address these challenges, we present a new resource discovery middleware, called ROADS, that can facilitate voluntary sharing. In ROADS, the participants can associate with each other at their own will and dictate the extent of sharing by properly exporting summaries, a condensed representation of their resource records. To enable efficient search, these summaries are aggregated and replicated along an overlay-assisted server hierarchy, and the queries are routed to those servers that are likely to hold the desired resources. Our experimental results show that ROADS provides not only flexible resource sharing for federated systems but also efficient resource discovery, with performance comparable to a centrally managed system.	autonomous robot;canonical account;federated database system;file spanning;middleware;programming paradigm;replay attack;routing;server (computing)	Hao Yang;Fan Ye;Zhen Liu	2007		10.1145/1377943.1377964	resource allocation;knowledge management;database;business;world wide web;human resource management system	OS	-37.22372250903079	15.44870900794673	44132
4ca50053b53b395f625f231a63f91f123897bc92	a plan-based service composition for work process agent in ubiquitous computing	process agent plan based service composition service oriented computing;service composition;process agent;computer model;planning artificial intelligence;semantics;plan based service composition;computer architecture;computational modeling;service oriented computing;bdi based coordinator agent plan based service composition work process agent ubiquitous computing wpa smart assistant application information service user model user preferences;schedules;ubiquitous computing;computational modeling conferences semantics adaptation models schedules computer architecture;information service;ubiquitous computing planning artificial intelligence service oriented architecture;service oriented architecture;adaptation models;conferences;user model;ubiquitous computing environment	In this paper, we discuss a Work Process Agent (WPA) that enables users to access a range of services for achieving their tasks with service composition based on planning in ubiquitous computing environment. WPA is a smart assistant application specializing in information service which needs to provide each user's own user model and make the model be adapted to the user's preferences dynamically. With WPA, users can achieve their task in a simple way. WPA employs a special BDI-based coordinator agent with planning for coordinating the individual WPA and external services. The plan-based service composition model works at abstraction level of process. This model makes it possible to reuse the frames of plans by making it independent from domain.	abstraction layer;ontology (information science);precondition;service composability principle;service discovery;ubiquitous computing;wi-fi protected access	Seheon Song;Minkoo Kim	2011	2011 IEEE Asia-Pacific Services Computing Conference	10.1109/APSCC.2011.28	computer science;knowledge management;service-oriented architecture;database;semantics;world wide web;computer security;ubiquitous computing	HCI	-43.65658016490351	15.614579965780258	44393
bb71e5bd7c0c87c15a22abe1fcfa3869b215ea9c	contract based behavior model for services coordination	service composition;behavior modeling;functional properties;web service;advanced transaction model	A key step towards consistent services coordination to provide non functional properties. In this sense, transactional properties  are particularly relevant because of the business nature of current applications. While services composition has been successfully  addressed, transactional properties have been main- ly provided by ad-hoc and limited solutions at systems’ back end. This paper proposes a flexible transactional behavior model for services coordination.  We assume that given a flow describing the application logic of a service based application, it is possible to associate to  it a personalized transactional behavior in an orthogonal way. This behavior is defined by specifying contracts and associating  a well defined behavior to the activities participating in the coordination. Such contracts ensure transactional properties  at execution time in the presence of exceptions.  		Edgar Alberto Portilla-Flores;Genoveva Vargas-Solar;Christine Collet;José-Luis Zechinelli-Martini;Luciano García-Bañuelos	2007		10.1007/978-3-540-68262-2_9	behavioral modeling;web service;computer science;knowledge management;service delivery framework;law;world wide web	Robotics	-45.87644112323874	17.654825271665704	44818
88b7c78b50a2c50309c9e5aad1ce74e5093113d5	the semantic web: eswc 2011 workshops			eswc;semantic web		2011		10.1007/978-3-642-25953-1	semantic web stack;database;world wide web	Vision	-39.92822987030474	6.009481387228753	44904
93548605e1adf9acc087d4611c6454e3dd9a940e	aggregate quality of service computation for composite services	business process model;quality of service	This paper addresses the problem of computing the aggre- gate QoS of a composite service given the QoS of the services participat- ing in the composition. Previous solutions to this problem are restricted to composite services with well-structured orchestration models. Yet, in existing languages such as WS-BPEL and BPMN, orchestration mod- els may be unstructured. This paper lifts this limitation by providing equations to compute the aggregate QoS for general types of irreducible unstructured regions in orchestration models. In conjunction with ex- isting algorithms for decomposing business process models into single- entry-single-exit regions, these functions allow us to cover a larger set of orchestration models than existing QoS aggregation techniques.	aggregate function;computation;quality of service	Marlon Dumas;Luciano García-Bañuelos;Artem Polyvyanyy;Yong Yang;Liang Zhang	2010		10.1007/978-3-642-17358-5_15	real-time computing;quality of service;computer science;operating system;database;distributed computing;world wide web;business process modeling	DB	-45.9372722293437	17.057386737584125	45057
2767d35a8e1512f7f03984bd949d086158bd3347	ensuring semantic interoperability on lexical resources	semantic interoperability;lexical markup framework	In this paper, we describe a unifying approach to tackle data heterogeneity issues for lexica and related resources. We present LEXUS, our software that implements the Lexical Markup Framework (LMF) to uniformly describe and manage lexica of different structures. LEXUS also makes use of a central Data Category Registry (DCR) to address terminological issues with regard to linguistic concepts as well as the handling of working and object languages. Finally, we report on ViCoS, a LEXUS extension, providing support for the definition of arbitrary semantic relations between lexical entries or parts thereof.	iso 12620;lexical markup framework;lexicon;semantic interoperability	Marc Kemps-Snijders;Claus Zinn;Jacquelijn Ringersma;Menzo Windhouwer	2008			natural language processing;semantic interoperability;artificial intelligence;semantic computing;semantic technology;semantic grid;semantic equivalence;semantic analytics;semantic web rule language;semantic web stack;computer science	NLP	-42.1147273874768	4.516718448707118	45073
8c2b77142bb0801b399c8365a5c6ec10a92ad339	evaluation of the opportunities and limitations of using ifc models as source of building metadata		Structured information about the available components in each building is critical to build scalable and maintainable smart building applications. Recent work has proposed a solid knowledge base for what information is needed and how to represent the information as metadata models. However, populating metadata models with information is still an extensive process. Therefore, it is important to consider how other digital sources of information about a building can aid this process. Building information models, such as IFC, are used as a common digital representation throughout the design and construction of buildings. Therefore, IFC models hold the potential to provide many of the elements needed by a metadata model for a building. In this paper, we document to which extend this premise holds by quantifying the IFC model elements that can be used to populate a Brick metadata model, both according to the specification and the actual IFC models implementing that specification. Our results show that considering the specification 4.5% of the elements in an IFC model can map to an element of the Brick metadata model. However, when considering the data in actual IFC models of 20 buildings our results show that only 0.2% of the content can be mapped to Brick. We interview building industry professionals to put a context to those numbers.	building information modeling;industry foundation classes;information model;knowledge base;metadata modeling;population;scalability	Henrik Lange;Aslak Johansen;Mikkel Baun Kjærgaard	2018		10.1145/3276774.3276790	information model;premise;software engineering;metadata;knowledge base;building automation;scalability;metadata modeling;brick;computer science	SE	-45.131540565192495	4.476845160713806	45156
188663e57ca6a3f74481789023521f1be1983fd8	a feature-based model for lexical databases	multiple way;widespread use;lexical resource;computational lexicon;suitable data model;multiple format;feature-based model;classical database model;lexical databases;feature structure;linguistic information;data model	To date, no fully suitable data model for lexical databases has been proposed. As lexical databases have proliferated in multiple formats, there has been growing concern over the reusability of lexical resources. In this paper, we propose a model based on feature structures which overcomes most of the problems inherent in classical database models, and in particular enables accessing, manipulating or merging information structured in multiple ways. Because of their widespread use in the representation of linguistic information, the applicability of feature structures to lexical databases seems natural, although to our knowledge this has not yet been implemented. The use of feature structures in lexical databases also opens up the possibility of compatibility with computational lexicons.	data model;database model;lexicon;relational database	Jean Véronis;Nancy Ide	1992			natural language processing;data model;computer science;data mining;lexical choice;information retrieval	NLP	-36.45602894246902	7.574266151332284	45166
4f4e8f553cf1d60939415adf08f764c3dd767079	a technique for information system integration	information system integration	Nowadays, a central topic in database science is the need of an integrated access to large amounts of data provided by various information sources whose contents are strictly related. Often information sources have been designed independently for autonomous applications, so they may present several kinds of heterogeneity. Particularly hard to manage is the semantic heterogeneity, which is due to schema and value inconsistencies. In this paper, we focus our attention mainly on the inconsistency which arises when conflicting instances related to the same concept and possibly coming from different sources are integrated. First, we introduce an operator, called Merge Operator, which allows us to combine data coming from different sources, preserving the information contained in each of them. Then, we present a variant of this operator, the Extended Merge Operator, which associates the integrated data with some information about the process by which they have been obtained. Finally, in order to manage conflicts among integrated data, we briefly present a technique for computing consistent answers over inconsistent databases.	autonomous robot;database;information system;semantic heterogeneity;system integration	Sergio Greco;Luigi Pontieri;Ester Zumpano	2001			computer science;artificial intelligence;data mining;database;information retrieval	DB	-37.38938893326742	5.324162077392266	45173
15b3454d681738f4190709a8c3025e7d25adb8b7	improving personal information management by integrating activities in the physical world with the semantic desktop	personal information management;activities;events;semantic web;ontologies;semantic desktop	Semantic desktops are a novel approach to improve user interfaces by recording, semantically annotating, and learning from the user's activities to create a personalized user experience and improve search. Such activities, however, are restricted to the information universe, i.e., they only cover events on the local desktop. A next step towards smart mobile devices is the integration of those desktop events with the user's activities in the physical world. Establishing such mappings enables the device to draw conclusions from the recorded desktop events to those that the user is likely performing in the physical world. A Personal Information Management (PIM) system can then better assist the user in task planning and routing. In this work, we propose activity ontologies as blueprints to model the user's activities in the physical world, and use these ontologies to link the Semantic Desktop and the information available on the Web of Linked Data. We discuss the principles of designing the activity ontologies and how to employ them to associate local files and applications with complementary information from the Web. We design a specific activity ontology for a conference use case and present a user interface that extends the Zeitgeist Semantic Desktop to evaluate our approach.	blueprint;desktop computer;linked data;mobile device;ontology (information science);personal information management;personalization;routing;semantic desktop;smart device;user experience;user interface;web ontology language;world wide web;zeitgeist	Yingjie Hu;Krzysztof Janowicz	2012		10.1145/2424321.2424420	desktop management interface;computer science;ontology;semantic web;personal information management;data mining;semantic web stack;database;multimedia;world wide web	HCI	-41.737134439862814	9.228542313815588	45194
a860fcdf91802ccb9a6340f5345e23de0c96cf97	extending sd-core for ontology-based data integration	data integrity	This paper describes the main elements of a functional platform for building Semantic Web Applications, the Semantic Directory (additional information can be found at http://khaos.uma.es/SD-Core). A Semantic Directory provides a resource directory, in which web resources are registered and their semantics published. Using the Semantic Directories we provide a solution for publishing the semantics of resources, and interoperating them with some other applications in the same or different domains. The main idea behind this proposal is to help developers build Semantic Web applications by providing them with functional components for this task. This paper also describes some applications that have been developed using an SD-Core extension: SD-Data. Then, we describe the instantiation of the Khaos Ontology-based Mediation Framework (KOMF) in the Systems Biology domain. This framework provides an architecture that enables the research on the development of ontology-based mediators. Thus, an ontology-based mediator has been produced that has demonstrated its utility in two applications developed in the Amine System Project using the SD-Data for registering semantics: AMMO-Prot and SBMM Assistant. The use of ontologies is limited in the current version of the mediator, but its development as a framework enables the implementation of improvements based on the use of reasoning.	bioinformatics;data compression;database;directory service;homology modeling;matchware mediator;ontology (information science);ontology-based data integration;open-source hardware;resource directory description language;secure digital;semantic web;systems biology;universal instantiation;usability;web application;web resource	Ismael Navas Delgado;José Francisco Aldana Montes	2009	J. UCS	10.3217/jucs-015-17-3201	ontology-based data integration;data mining;computer science;data integrity	Web+IR	-41.53580673760015	4.717619794191727	45205
59ae7e9f03ad2dbef44a450ef35d88e281ea12b8	workload characterization of selected jee-based web 2.0 applications	workload characterization;mashups;rich internet application;social networking;portal based technologies workload characterization jee based web 2 0 applications rich internet applications web based communities social networking web 2 0 development content aggregation;blogs servers java benchmark testing positron emission tomography mashups internet;positron emission tomography;social network;servers;java internet;col;content aggregation;internet;portal based technologies;rich internet applications;blogs;benchmark testing;web 2 0 development;jee based web 2 0 applications;web based communities;java	Web 2.0 represents the evolution of the web from a source of information to a platform. Network advances have permitted users to migrate from desktop applications to so-called Rich Internet Applications (RIAs) characterized by thin clients, which are browser-based and store their state on managed servers. Other Web 2.0 technologies have enabled users to more easily participate, collaborate, and share in web-based communities. With the emergence of wikis, blogs, and social networking, users are no longer only consumers, they become contributors to the collective knowledge accessible on the web. In another Web 2.0 development, content aggregation is moving from portal-based technologies to more sophisticated so-called mashups where aggregation capabilities are greatly expanded. While Web 2.0 has generated a great deal of interest and discussion, there has not been much work on analyzing these emerging workloads. This paper presents a detailed characterization of several applications that exploit Web 2.0 technologies, running on an IBM Power5 system, with the goal of establishing, whether the server-side workloads generated by Web 2.0 applications are significantly different from traditional web workloads, and whether they present new challenges to underlying systems. In this paper, we present a detailed characterization of three Web 2.0 workloads, and a synthetic benchmark representing commercial workloads that do not exploit Web 2.0, for comparison.	benchmark (computing);blog;desktop computer;dhrystone;emergence;information source;java platform, enterprise edition;mashup (web application hybrid);power5;rich internet application;server (computing);server-side;thin client;virtual community;web 2.0;web application;wiki	Priya Nagpurkar;William P. Horn;U. Gopalakrishnan;Niteesh Dubey;Joefon Jann;Pratap Pattnaik	2008	2008 IEEE International Symposium on Workload Characterization	10.1109/IISWC.2008.4636096	web service;web application security;web development;web modeling;rich internet application;data web;web analytics;web mapping;web design;web accessibility initiative;web standards;computer science;operating system;web navigation;social semantic web;multimedia;internet privacy;web intelligence;web engineering;web 2.0;world wide web;mashup;social network	Metrics	-46.84474843173541	11.531143914648158	45228
b5dd2e8f46541abab7d98d149ca23f1268c6e954	agent-based digital libraries: driving the information economy	libraries;software tool;spine;multi agent system;software libraries;agent based;web and internet services;information retrieval;digital library;digital libraries;emerging technology;appropriate technology;software agents;multi agent systems;intelligent agents;internet;informal economy;zuno digital library;information management;information retrieval systems internet software agents libraries information retrieval;intelligent agent;information economy;information retrieval systems;zunodl digital libraries information economy internet multi agent systems intelligent agents zuno digital library;world wide web;software tools;software libraries world wide web software tools web and internet services appropriate technology information retrieval information management spine floors multiagent systems;floors;zunodl;multiagent systems	Exploiting the enormous potential of the Internet will require distributed, scaleable, self-optimising software tools that are capable of pro-actively adapting their behaviour to meet the requirements of users with differing skills, interests, and objectives. We argue that an appropriate framework for building such software tools is that of an information economy, in which computational agents carry out the roles of information producer consumer and broker. We further argue that the emerging technology of intelligent agents and multi-agent systems provides the technological foundation upon which to implement such information economies, and provide the rich personalised information retrieval, management, and sharing services required to exploit the Internet. We illustrate our argument by describing, as a case study, the Zuno Digital Library (ZUNODL), a commercial framework for building commercial digital libraries.	digital library;library (computing)	David Derbyshire;Innes A. Ferguson;Jörg P. Müller;Markus Pischel;Michael Wooldridge	1997		10.1109/ENABL.1997.630795	the internet;spine;informal sector;computer science;knowledge management;artificial intelligence;appropriate technology;software agent;multi-agent system;database;multimedia;emerging technologies;information economy;world wide web	HCI	-46.374515729244834	11.008683617787575	45273
146ed52321e84c4b29779e4b82f6f74a5845a751	mining and reasoning on workflows	extraction information;graph formalization;control systems;groupware;workflow management;computer society;red www;data mining resource management humans technology management large scale systems computer society workflow management software application software control systems automation;analisis datos;information extraction;application software;web;resource management;reseau web;very large databases workflow management software data mining internet inference mechanisms;inference mechanisms;data mining;technology management;pattern discovery;artificial intelligence reasoning;data analysis;artificial intelligence reasoning workflow management systems web pattern discovery data mining graph formalization;systeme gestion electronique processus;internet;fouille donnee;decouverte connaissance;security key;traitement exception;workflow management software;workflow;world wide web;descubrimiento conocimiento;analyse donnee;exception handling;workflow management systems;humans;index terms data mining;very large databases;cle securite;collecticiel;busca dato;extraccion informacion;large scale systems;knowledge discovery;workflow management index terms data mining;llave seguridad;automation	"""Today's workflow management systems represent a key technological infrastructure for advanced applications that is attracting a growing body of research, mainly focused in developing tools for workflow management, that allow users both to specify the """"static"""" aspects, like preconditions, precedences among activities, and rules for exception handling, and to control its execution by scheduling the activities on the available resources. This paper deals with an aspect of workflows which has so far not received much attention even though it is crucial for the forthcoming scenarios of large scale applications on the Web: providing facilities for the human system administrator for identifying the choices performed more frequently in the past that had lead to a desired final configuration. In this context, we formalize the problem of discovering the most frequent patterns of executions, i.e., the workflow substructures that have been scheduled more frequently by the system. We attacked the problem by developing two data mining algorithms on the basis of an intuitive and original graph formalization of a workflow schema and its occurrences. The model is used both to prove some intractability results that strongly motivate the use of data mining techniques and to derive interesting structural properties for reducing the search space for frequent patterns. Indeed, the experiments we have carried out show that our algorithms outperform standard data mining algorithms adapted to discover frequent patterns of workflow executions."""	algorithm;data mining;exception handling;experiment;precondition;reachability problem;scheduling (computing);structure mining;system administrator;world wide web	Gianluigi Greco;Antonella Guzzo;Giuseppe Manco;Domenico Saccà	2005	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2005.63	workflow;computer science;artificial intelligence;data science;technology management;resource management;data mining;database;world wide web;information extraction;workflow technology	DB	-37.42670438994007	13.552553764290357	45338
1c1c168a992cff1895ea666cc2e57a4ad6867548	swrl rule based precondition and effects service matching	web services geophysics computing input output programs knowledge based systems;rule based;input output programs;io match;iope match rule based service match io match;geophysics computing;web services;web services semantics cognition ontologies semantic web algorithm design and analysis knowledge based systems;iope match;io based matching service discovery algorithm swrl rule based precondition effects service matching web service technologies rule based semantic web service discovery io based matching services authorization service execution;knowledge based systems;service match	The rapid development of Web service technologies has dramatically increased the quantity of available web services. Hence, it becomes a challenging task to efficiently discover services that exactly match user requirements from the large and growing service library. The rule-based semantic web service discovery strategy proposed in this paper documents the formal description of semantic web service, rule, and service matching etc. Service matching which is regarded as rule-based reasoning problem in this paper achieved by knowledge-base and reasoning technologies. IO-based matching services is to discovery the user required service via matching the service's input/output and user-desired input/output. In real life, the implementation of many services need to meet the more complicated pre-conditions (Preconditions) and effects, such as the implementation of some services need to have authorization (the Authorized), also have some effect (Effects) after the service execution. This paper use the IO-based matching service discovery algorithm as the basis and improve the service discovery's accuracy by adding the matching of the precondition and effect of service. In this paper, a rule-based reasoning mechanism is proposed to implement IOPE services matching algorithm, some experiment are conducted to verify the feasibility and effectiveness of the algorithm.	algorithm;authorization;input/output;knowledge representation and reasoning;logic programming;precondition;real life;reasoning system;requirement;semantic web rule language;semantic web service;service discovery;user requirements document	Hong Fan;Zhihua Wang;Wu Du	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6351376	rule-based system;web service;service level requirement;computer science;ws-policy;knowledge-based systems;data mining;database;service discovery;world wide web;universal description discovery and integration	Mobile	-44.140502707315974	13.91678850431531	45406
b2e177749b837ce4460319d6a20f4b2f0b9bd68c	a self-configuring schema matching system	libraries;model management domain self configuring schema matching system complex metadata structure mapping data integration ontology alignment mapping generation automatic matching system generated mapping evaluation;data integrity;noise robustness;ontologies artificial intelligence;schema matching;tuning;adaptive systems;data structures;pattern matching;adaptive system;robustness;meta data;model management;ontologies;tuning libraries pattern matching ontologies adaptive systems noise robustness;pattern matching data integration data structures meta data ontologies artificial intelligence;data integration;noise;ontology alignment	Mapping complex metadata structures is crucial in a number of domains such as data integration, ontology alignment or model management. To speed up the generation of such mappings, automatic matching systems were developed to compute mapping suggestions that can be corrected by a user. However, constructing and tuning match strategies still requires a high manual effort by matching experts as well as correct mappings to evaluate generated mappings. We therefore propose a self-configuring schema matching system that is able to automatically adapt to the given mapping problem at hand. Our approach is based on analyzing the input schemas as well as intermediate matching results. A variety of matching rules use the analysis results to automatically construct and adapt an underlying matching process for a given match task. We comprehensively evaluate our approach on different mapping problems from the schema, ontology and model management domains. The evaluation shows that our system is able to robustly return good quality mappings across different mapping problems and domains.	control flow;database schema;microsoft outlook for mac;ontology alignment;relevance	Eric Peukert;Julian Eberius;Erhard Rahm	2012	2012 IEEE 28th International Conference on Data Engineering	10.1109/ICDE.2012.21	ontology alignment;computer science;noise;ontology;adaptive system;data integration;pattern matching;data integrity;data mining;database;programming language;metadata;information retrieval;robustness	DB	-36.041213158084176	5.789041109083904	45451
f6271377a9c96fc1f9f46eecb0e57083042861db	amazing of using isg on implementing a web-based system	e commerce generator;object serialization information system generator web based generator e commerce generator;information system generator;arrays java buildings html electronic mail databases information systems;internet information systems;data entry isg dwl information systems customized web based system ubiquitous home conference uhc2013 object serialization mechanism cpu time;object serialization;web based generator	We developed two tools previously called ISG and DWL and ISG is for generating information systems and DWL is for generating web systems. We have used ISG and DWL to develop a customized web-based system for the 7th Ubiquitous-Home Conference UHC2013 and other web-based systems. The advantage of these web-based systems is that it uses object serialization mechanism to fill objects with data which saves CPU time. We used building files of ISG to build the file system of a web-based system and each attribute of an object to be specified for translating these settings to Java. We wrote Input Output programs to read data and write data and these objects with data entry thus created can be stored and retrieved efficiently. Our web-based systems avoid running cooperating processes that share data and resulting in inconsistencies in the shared data. Company produce new products frequently and the web pages of new products need to be updated shortly and using ISG can solve this problem.	central processing unit;information system;java;serialization;web application;web page	Ling-Hua Chang;Sanjiv Behl;Tung-Ho Shieh	2013	2013 International Conference on Parallel and Distributed Computing, Applications and Technologies	10.1109/PDCAT.2013.14	parallel computing;real-time computing;serialization;computer science;operating system;database;distributed computing;programming language;world wide web;computer security	DB	-35.258035706048126	16.40480487901189	45706
53d84a4a2c7ed575e44455369a5f3dd00a7cc985	a constructive framework for legal ontologies	ontology development;decision support system;expert system	The increasing development of legal ontologies seems to offer interesting solutions to legal knowledge formalization, which in past experiences lead to a limited exploitation of legal expert systems for practical use. The paper describes how a constructive approach to ontology can provide useful components to create newly designed legal decision support systems either as local or Web-based semantic services. We describe the relation of our research to AI&Law and legal philosophy, the components of our Core Legal Ontology, the JurWordNet semantic lexicon, and some examples of use of legal ontologies for both norm conformity and compatibility. Our legal ontologies are based on DOLCE+, an extension of the DOLCE foundational ontology developed in the WonderWeb and Metokis EU projects.	color light output;conformity;decision support system;experience;formal ontology;information retrieval;legal expert system;lexicon;ontology (information science);upper ontology	Aldo Gangemi;Maria-Teresa Sagri;Daniela Tiscornia	2003		10.1007/978-3-540-32253-5_7	upper ontology;legal expert system;ontology components;computer science;knowledge management;ontology;data mining;management science;process ontology	AI	-44.29206390973098	5.064698364911594	45743
37babd860e5b14a42dfe5ddae7107be6c3e9b7e5	active and adaptive services resource provisioning with personalized customization	personalized customization services resource provisioning requirements engineering atom;software maintenance;resource allocation;semantics production service oriented architecture algorithm design and analysis business publishing;requirements engineering;computer aided software engineering;atom;software maintenance cloud computing computer aided software engineering resource allocation service oriented architecture;case tools active services resource provisioning adaptive services resource provisioning personalized customization software as a service saas service oriented software engineering sose services computing on demand service dynamic services aggregation single provision structure passive selection mode service requesters software architecture service composition sliced method segmentation method legacy software servicelization on demand active provisioning production framework process guidance;personalized customization;service oriented architecture;services resource provisioning;cloud computing	Software as a service(SaaS), we are moving to the age of service-oriented software engineering(SOSE). But for the goal of services computing, namely on-demand service, it has not been able to achieved by far, especially the active provisioning approach for services resource. In view of these facts of services aggregation, i.e. The relative deficient services resource, single provision structure and passive selection mode for service requesters. Active provisioning of services resource with personalized customization will be focused. In this paper, we have established software architecture with personalized active custom for services resource. Customization of the active provisioning of services resource mainly includes two aspects: Firstly, for unmatched services resource of service composition, sliced or segmentation method should be chosen to acquire the individual needs of services resource according to the overall requirements. Secondly, a huge amount of legacy software will be comprehensively reused, namely servicelization. Through personalized customization of services resource, it will achieve on-demand active provisioning to furnish adequate material for dynamic services aggregation. It will also provide the production framework, process guidance and engineering support of CASE tools for services resource provisioning in runtime based on personalized customization. The feasibility and efficiency of the proposed approach are verified by a series of experiments.	computer-aided software engineering;e-services;experiment;just-in-time compilation;legacy system;personalization;provisioning;requirement;service composability principle;service-oriented device architecture;services computing;software architecture;software as a service	Bin Wen;Ziqiang Luo;Peng Liang	2013	2013 10th Web Information System and Application Conference	10.1109/WISA.2013.28	systems engineering;software as a service;database;business;services computing;world wide web	HPC	-47.343625954797446	16.05241219167816	45786
58f4347330606a5db5adc57ef3b6cd52af9928fc	a semantic datagrid for combinatorial chemistry	chemistry computing;grid computing;semantic web;combechem project;chemical structure;combinatorial chemistry;e-science infrastructure;scientific queries;semantic web;semantic datagrid	The CombeChem project has designed and deployed an e-science infrastructure using a combination of grid and semantic Web technologies. In this paper we describe the datagrid element of the project, which provides a platform for sophisticated scientific queries and a rich record of experimental data and its provenance. This datagrid constitutes a significant deployment of semantic Web technologies and we propose it as an example of a 'semantic datagrid'.	cheminformatics;combinatorial chemistry;digital curation;e-science;egi;information processing;level of detail;population;semantic web;software deployment;thread-local storage;triple des;triplestore;web developer;world wide web	Kieron R. Taylor;Robert J. Gledhill;Jonathan W. Essex;Jeremy G. Frey;Stephen W. Harris;David De Roure	2005	The 6th IEEE/ACM International Workshop on Grid Computing, 2005.		semantic computing;data web;semantic search;semantic grid;computer science;semantic web;social semantic web;semantic web stack;database;semantic technology;world wide web;information retrieval;semantic analytics;grid computing	HPC	-40.25554388560374	4.554131734973277	45848
239a4f3acb40d86d91bf88d96b15b5f12f536a18	a scalable architecture for autonomous heterogeneous database interactions	scalable architecture;autonomous heterogeneous database interactions	With t,ha exponential proliferation of databases and advances in wide area networking, interest, in worldwide dat,abase interoperability has gained momentum. Scalability and language support for this new environment remain open questSions. We propose a scheme where database nodes are dynamically clustered around current, areas of ibresl. Data sharing is then pursued, with any relationship informat8ion discovered being fed bac,k for reclustering. In order to achieve scalability, t,he proposed architect(ure sub-divides both the rrlntionship and illformntiorl spaces.	heterogeneous database system;interoperability;scalability;time complexity	Stephen Milliner;Athman Bouguettaya;Mike P. Papazoglou	1995			applications architecture;operating system;database;distributed computing	DB	-40.440050229463445	8.37906414069552	45880
191405ea086169a67654b484898860b95d581afa	omnivoke: a framework for automating the invocation of web apis	web services omnivoke web api innovation nonintrusive semantic annotations html pages;web services application program interfaces hypermedia markup languages;hypermedia markup languages;grounding;vocabulary;semantics;resource description framework;grounding semantic web services web apis service invocation;html;semantic web services;application program interfaces;web services;semantics resource description framework html ontologies vocabulary grounding web services;web apis;ontologies;service invocation	Web APIs, characterized by their relative simplicity and their natural suitability for the Web, have become increasingly dominant in the world of services on the Web. Despite their popularity, Web APIs are so heterogeneous in terms of the underlying principles adopted and the means used for publishing them that discovering, understanding and notably invoking Web APIs is nowadays more an art than a science. In this paper, we present our work towards supporting the automated invocation of Web APIs. In particular, we describe a framework that provides a unique entry point for the invocation of most Web APIs that can be found on the Web, by exploiting non-intrusive semantic annotations of HTML pages describing Web APIs in order to capture both their semantics as well as information necessary to carry out their invocation.	entry point;html;world wide web	Ning Li;Carlos Pedrinaci;Maria Maleshkova;Jacek Kopecký;John Domingue	2011	2011 IEEE Fifth International Conference on Semantic Computing	10.1109/ICSC.2011.72	ground;web service;web application security;web development;web modeling;data web;web mapping;web-based simulation;html;web design;web standards;computer science;ontology;web api;ws-policy;semantic web;web navigation;rdf;social semantic web;semantic web stack;database;semantics;web intelligence;web engineering;web 2.0;world wide web;information retrieval;mashup	DB	-40.95193438932082	10.740828815390628	45926
ca6ef234c35d0872e7d1e9f1da28146d1d4d676b	language service infrastructure on the web: the language grid		Globalization increasingly demands multilingual communication on the Internet, as well as in local communities. To create customized collaboration tools to support multilingual communities, the authors’ Language Grid, established ten years ago, has been improving Web-based services to communities throughout the world by providing highly adaptable infrastructure and access to a wide variety of language resources.	internet;world wide web	Toru Ishida;Yohei Murakami;Donghui Lin;Takao Nakaguchi;Masayuki Otani	2018	Computer	10.1109/MC.2018.2701643	computer science;web science;globalization;world wide web;language grid;maintenance engineering;the internet;services computing;usability	ML	-46.708832177593706	10.747877696704485	45943
ef908391380d142769fdb94d675ccdbcf6bf8d4d	wisdom2013: a large-scale web information analysis system		We demonstrate our large-scale web information analysis system called WISDOM2013, which consists of several deep semantic analysis systems such as a factoid QA, a non-factoid QA and a sentiment analyzer, and a software platform on which its semantic analysis systems can be applied to a billion-page-scale web archive. The software platform has an extendable architecture, and we are planning to enhance WISDOM2013 in the future by adding more semantic analysis systems and inference mechanisms.	archive;extensibility;sentiment analysis;software quality assurance;web archiving;word lists by frequency	Masahiro Tanaka;Stijn De Saeger;Kiyonori Ohtake;Chikara Hashimoto;Makoto Hijiya;Hideaki Fujii;Kentaro Torisawa	2013			semantic computing;data web;computer science;semantic web;social semantic web;semantic web stack;database;semantic technology;world wide web;information retrieval	Security	-40.063551500904545	6.714374951820177	45981
197e2e4d2992484435b5323649668b10a9cce0a2	using dynamic asynchronous aggregate search for quality guarantees of multiple web services compositions	offre service;distributed system;composite web service;systeme reparti;red www;service provider;empresa numerica;qos guarantee;reseau web;service web;swinburne;web service;constraint satisfaction;orientado servicio;qualite service;web service composition;feasibility;satisfaction contrainte;digital enterprise;sistema repartido;distributed constraint satisfaction problem;world wide web;oriente service;satisfaccion restriccion;entreprise numerique;quality of service;enterprise application integration;proposals;practicabilidad;faisabilite;service quality;servicio web;service oriented;calidad servicio	With the increasing impact and popularity of Web service technologies in today’s World Wide Web, composition of Web services has received much interest to support enterprise-to-enterprise application integrations. As for service providers and their partners, the Quality of service (QoS) offered by a composite Web service is important. The QoS guarantee for composite services has been investigated in a number of works. However, those works consider only an individual composition or take the viewpoint of a single provider. In this paper, we focus on the problem of QoS guarantees for multiple inter-related compositions and consider the global viewpoints of all providers engaged in the compositions. The contributions of this paper are two folds. We first formalize the problem of QoS guarantees for multi-compositions and show that it can be modelled as a Distributed Constraint Satisfaction Problem (DisCSP). We also take into account the dynamic nature of the Web service environment of which compositions may be formed or dissolved any time. Secondly, we present a dynamic DisCSP algorithm to solve the problem and discuss our initial experiment to show the feasibility of our approach for multiple Web service compositions with QoS guarantees.	aggregate function;algorithm;constraint satisfaction problem;distributed constraint optimization;enterprise software;mathematical optimization;quality of service;risk management;web service;world wide web	Xuan Thang Nguyen;Ryszard Kowalczyk;Jun Han	2006		10.1007/11948148_11	service provider;web service;feasibility study;mobile qos;simulation;enterprise application integration;quality of service;constraint satisfaction;computer science;ws-policy;database;law;world wide web;service quality	Web+IR	-39.384453033312184	15.534349664461363	46026
1500cb49043585407446c98c5d33d3aa9ef424d4	planning based integration of web services	automated planning;resource selection;automated planning techniques web services goal directed integration;web services planning artificial intelligence;web services satellites intelligent agent mathematics computer science proposals application software cities and towns information retrieval drugs;planning artificial intelligence;web service;web service composition;semantic web services;general solution;automated planning techniques;goal directed integration;web services;dynamic web service composition;constraint satisfaction problem	In this paper a planning system for goal directed integration of web services is presented. The presented model extends classical planning to manage some forms of non determinism in service execution and to manage collections of objects. The dynamical evolving domain of available web services require flexible tools for composing the given resources in order to fulfil user goals which are also unpredictable. Automated planning techniques can be efficiently used to realise dynamical and adaptive web service composition. The domain of available services is represented by a set of planning operators, the behaviour of the composed service is described by a final goal and the generated solution plans are used for synthesis of web service scripts.	automated planning and scheduling;service composability principle;web service	Alfredo Milani;Fabio Rossi;Simonetta Pallottelli	2006	2006 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology Workshops	10.1109/WI-IATW.2006.104	web service;web application security;web development;web modeling;data web;web mapping;web design;web standards;computer science;knowledge management;ws-policy;service-oriented architecture;web navigation;social semantic web;data mining;ws-addressing;database;services computing;web intelligence;ws-i basic profile;web 2.0;law;world wide web	Robotics	-44.151915049780584	14.33286847859225	46268
ecbe70b4d939d1a9108bf0d01ab178fffc29697d	large multidatabases: beyond federation and global schema integration	database system	Advances in wide area networking is making the goal of achieving a worldwide interoperable database system closer than ever. Because of the large environment , new problems have surfaced that need to be addressed. We distinguish several reasons for a new design approach. Traditional approaches like global integration and federated approaches are better suited to small and medium scale heterogeneous databases. In a large network of interoperable autonomous heterogeneous databases, the architecture should be exible enough to allow negotiation to take place to establish grouping of databases. In such a large environment, it is important that databases be made aware of other participating databases in an incremental fashion. Further, users should be educated about the space of information in an incremental and dynamic fashion. These goals are to be achieved under the major assumptions that participating databases keep their autonomy intact as much as possible and that the overhead in bridging heterogeneity be proportional to the level of sharing. In this paper, we describe a two-level approach that addresses these issues.	autonomous robot;bridging (networking);database;interoperability;overhead (computing)	Athman Bouguettaya	1994			data mining;computer science;database;three schema approach;database schema;schema (psychology);wide area network	DB	-37.186387077118056	15.375278539416158	46270
c69d4d7bf1a8b616b4dffb8b5dd879df3cbee4e9	multilingual collection retrieving via ontology alignment	cross lingual information retrieval;busqueda informacion;documento electronico;alignement;optimisation;infrastructure information;informacion numerica;ontologie;ontology mapping;optimizacion;global information infrastructure;routing;information infrastructure;information retrieval;digital library;interrogation base donnee;routage;interrogacion base datos;document electronique;digital information;biblioteca electronica;heterogeneidad;recherche information;alineamiento;ontologia;information numerique;optimization;electronic library;multilinguisme;ontology;database query;alignment;multilingualism;bibliotheque electronique;infraestructura informacion;electronic document;heterogeneity;heterogeneite;multilinguismo;ontology alignment;enrutamiento	As parts of global information infrastructure, digital libraries will likely be accessed by people all over world. Ontologies and their association can alleviate the heterogeneity and particularly the diversity of languages. This paper proposes a solution to cross-lingual information retrieval problem via ontology alignment. We elaborate two original techniques, i.e. primitives' association based on CL-LSI and mapping configuration optimization, to augment existing ontology mapping technology. As a result, multilingual collections can be bridged by this mapping, and searching across them can be achieved by three tractable steps: querying against local ontology, routing to target ontologies, and harvesting contents there online.	ontology alignment	Liang Zhang;Guowen Wu;Yanfei Xu;Wei Li;Yang Zhong	2004		10.1007/978-3-540-30544-6_57	upper ontology;information infrastructure;routing;ontology alignment;digital library;semantic integration;bibliographic ontology;computer science;ontology;heterogeneity;ontology;data mining;database;ontology-based data integration;world wide web;information retrieval;process ontology;suggested upper merged ontology	NLP	-37.353825336659796	11.02946394832401	46278
e74619879294f0cf8256e06570f1468abb71fdbe	concept learning algorithm for semantic web based on the automatically searched refinement condition	decision tree;semantic web;concept learning;ontology	Today, the web is the huge data repository which contains excessively growing with uncountable size of data. From the view point of data, Semantic Web is the advanced version of World Wide Web, which aims machine understandable web based on the structured data. For the advent of Semantic Web, its data has been rapidly increased with various areas. In this paper, we proposed novel decision tree algorithm, which called Semantic Decision Tree, to learning the covered knowledge beyond the Semantic Web based ontology. For this purpose, we newly defined six different refinements based on the description logic constructors. Refinements are replaced the features of traditional decision tree algorithms, and these refinements are automatically searched by our proposed decision tree algorithm based on the structure information of ontology. Additional information from the ontology is also used to enhance the quality of decision tree results. Finally, we test our algorithm by solving the famous rule induction problems, and we can get perfect answers with useful decision tree results. In addition, we expect that our proposed algorithm has strong advantage to learn decision tree algorithm on complex and huge size of ontology.	algorithm;concept learning;semantic web	Dongkyu Jeon;Wooju Kim	2013		10.1007/978-3-319-06826-8_30	web modeling;data web;bibliographic ontology;ontology inference layer;decision tree learning;computer science;decision tree;social semantic web;incremental decision tree;data mining;semantic web stack;database;ontology-based data integration;owl-s;id3 algorithm;information retrieval	AI	-37.20341561876228	4.663687620058418	46435
296661ce9da0e2adf38a08d4203a69d70e39cf93	a reference model for data mining web services	semantic web service;semantic annotation;interfaces format;reference model;semantics;heterogeneous data;data type;web service;data mining;ontologies artificial intelligence;data mining engines quality of service data models ontologies web services semantics;wsdl port types;engines;data mining grid services reference model data mining web services heterogeneous data mining application interfaces format data interchange format semantic web service wsdl port types ontology;data interchange format;software development;web services;grid service;interchange format;semantic web;heterogeneous data mining application;ontologies;data mining web services;quality of service;grid computing;data mining grid services;ontology;electronic data interchange;web services data mining electronic data interchange grid computing ontologies artificial intelligence semantic web;web services data mining;data models	Web services enable to achieve an interoperability between heterogeneous data mining applications through platform and language independent interfaces. As the number of available data mining services increases, it becomes more difficult to find a service that can perform a specific task at hand. Moreover, there may also be no such single data mining service capable of performing the specific task, but a combination of other existing services may provide this capability. Hence, a standardized set of interfaces and data interchange formats between services is necessary to discover suitable services as well as to enable composition of the services into complex workflows. Semantic Web services aiming to tackle the problem of discovering of demanded functionality by grounding semantic Web services to particular WSDL port types or by an extension of the interface description using references to an ontology. In both cases it requires semantically annotated interfaces with well described input parameters and results produced by the services. Hence, a data minig ontology is required to describe semantic Web services performing data mining tasks. The goal of this paper is not to provide an exhaustive specification of the interface defining all kinds of data types and data mining primitives used in communication with the data mining services, but rather propose a model, which can be used by software developers to unify development of the services using common Web standards and technologies. Additionally, the reference model can serve as a template for implementation of WSRF-compliant data mining grid services.	composability;data mining;discoverability;interoperability;reference model;semantic web service;software developer;surface web;web services description language;web standards	Ivan Janciak;Peter Brezany	2010	2010 Sixth International Conference on Semantics, Knowledge and Grids	10.1109/SKG.2010.36	web service;computer science;artificial intelligence;ws-policy;ontology;data mining;ws-addressing;database;semantics;services computing;data stream mining;ws-i basic profile;law;world wide web;owl-s	ML	-43.307621055243466	12.981605586725204	46605
456c776497bec571601808fbd2c8974543d1055d	djess - a knowledge-sharing middleware to deploy distributed inference systems		In this paper DJess is presented, a novel distributed production system that provides an infrastructure for factual and procedural knowledge sharing. DJess is a Java package that provides programmers with a lightweight middleware by which inference systems implemented in Jess and running on different nodes of a network can communicate. Communication and coordination among inference systems (agents) is achieved through the ability of each agent to transparently and asynchronously reason on inferred knowledge (facts) that might be collected and asserted by other agents on the basis of inference code (rules) that might be either local or transmitted by any node to any other node. Keywords— Knowledge-Based Systems, Expert Systems, Distributed Inference Systems, Parallel Production Systems, Ambient Intelligence, Mobile Agents.	ambient intelligence;distributed manufacturing;expert system;java package;jess;knowledge-based systems;middleware;mobile agent;production system (computer science);programmer	Federico Cabitza;Bernardo Dal Seno	2005				AI	-41.47722145322789	17.721086194804354	46693
8506d48df97a2f8d8e26bfa7f25d47a3a45bbe69	soa4all: an innovative integrated approach to services composition	integrated approach;optimisation;generators;optimization process innovative integrated approach services composition automated web service composition end user objectives service based components service oriented computing automatic template process generator abstract process templates ai parametric design techniques multi agent approach services processes;service composition;abstracting;generators adaptation model semantics concrete context web services context modeling;optimization process;automated web service composition;semantics;automated reasoning;automatic template process generator;satisfiability;parametric design;web service composition;multi agent systems;adaptation model;innovative integrated approach;service oriented computing;web services;service based components;web services abstracting multi agent systems optimisation;semantic web;abstract process templates;end user objectives;services processes;context modeling;context;automated reasoning service composition semantic web integrated architecture;integrated architecture;services composition;concrete;ai parametric design techniques;multi agent approach	Automated web service composition has been tackled from different directions and to different purposes. In addition, most of the approaches address the composition problem with under specified requirements, returning compositions models that do not necessarily satisfy and fulfill end-users objectives. Satisfying the latter objectives is a difficult problem, especially from scratch, which requires stronger requirements and a further step of integration with service-based components in order to make service oriented computing and service composition a reality. In this work, we address this issue by presenting an innovative and integrated approach to service composition which consists of i) an automatic template process generator, that is able to generate abstract process templates and their hierarchy from past executions; ii) a novel and scalable approach to AI parametric-design techniques using a multi agent approach to configure and adapt services processes, heavily relying on the latter set of abstract process templates; iii) an optimization process that maximizes the overall quality of final compositions. Finally, we compare the scalability of these components with some experiments.	experiment;mathematical optimization;requirement;scalability;service composability principle;service-oriented architecture;web service	Freddy Lécué;Yosu Gorronogoitia;Rafael Gonzalez;Mateusz Radzimski;Matteo Villa	2010	2010 IEEE International Conference on Web Services	10.1109/ICWS.2010.68	web service;concrete;computer science;service-oriented architecture;semantic web;database;semantics;context model;automated reasoning;world wide web;satisfiability	SE	-46.35352332484188	15.78292322363487	46941
28998c2a010026d7b56ae7c1bd498c349e8d0ed1	supporting collaborative authoring of web content by customizable resource repositories	value added services;collaborative authoring	The process of collaborative authoring of Web content may be supported by resource repositories which provide storage facilities as well as certain value-added services. However, since specific demands posed by the authoring process may vary according to special characteristics of such a process, resource repositories with predefined services usually address these demands only to a limited extent. To overcome this problem, we propose a way of generating customized repositories for collaborative authoring of Web content. Our approach involves a set of frameworks that can be adapted and applied to the specification of repository services in order to generate customized repositories with services tailored to the requirements of the authoring process.	requirement;web content	Jernej Kovse;Theo Härder;Norbert Ritter;Hans-Peter Steiert;Wolfgang Mahnke	2001			computer science;database;multimedia;world wide web	Web+IR	-43.48579904234899	10.578774501360353	47383
f18acaf9f11d4f5c7e3c4696389e069c364bc4a4	ipsi-pf: a business process matchmaking engine	electronic commerce;information systems;availability;web and internet services;search engines;ipsi process finder business process matchmaking engine web services b2b integration problems service discovery uddi querying functionality;companies;business process matchmaking engine;querying functionality;internet;logistics;web services;uddi;ip networks;b2b integration problems;knowledge based systems electronic commerce internet;service discovery;web services logistics search engines availability companies information systems service oriented architecture quality of service ip networks web and internet services;quality of service;service oriented architecture;ipsi process finder;knowledge based systems	Success of Web services mainly depends on the availability of tools facilitating usage of technology within the addressed B2B integration problems. One severe problem in loosely coupled systems is service discovery including a sufficient matchmaking definition. The concept for service discovery in Web service architecture is UDDI providing limited querying functionality and not being capable to deal with the multiple dimensions of a service, like for example semantic, workflow, or quality of service aspects. The IPSI process finder (IPSI-PF) provides a matchmaking definition and an engine focusing on process aspects. In particular, the matchmaking engine realizes service discovery by extending the capabilities of UDDI to ease the integration with additional UDDI extensions addressing other service description dimensions.	business process;e-commerce payment system;fits;loose coupling;pf (firewall);proceedings of the ieee;quality of service;service discovery;web services discovery;web framework;web service	Andreas Wombacher;Bendick Mahleko;Erich J. Neuhold	2004	Proceedings. IEEE International Conference on e-Commerce Technology, 2004. CEC 2004.	10.1109/ICECT.2004.1319727	e-commerce;web service;logistics;availability;the internet;quality of service;computer science;knowledge-based systems;service-oriented architecture;data mining;database;service discovery;world wide web;universal description discovery and integration;information system	HPC	-45.35442723892335	13.199874064822179	48142
844aca28fb84054394af7206510efc616aaa8ed0	is participation in the semantic web too difficult	ontologie;red www;critical mass;reseau web;semantics;resource description framework;semantica;semantique;internet;specification rdf;semantic web;world wide web;ontology	As long as there is not a sufficient base of RDF-annotated pages, the benefits of participating in the SemanticWeb are barely visible. This is true in particular for content providers like individuals or small institutions. These potential participants can’t afford the additional work necessary for the Semantic Web, yet they’re needed for the Semantic Web to reach the critical mass that will make it a success. This paper discusses problems that may prevent small content providers from participating in the Semantic Web, as well as a possible way to lower the barrier for entry using tools like our own Information Layer system.	daml+oil;microsoft outlook for mac;ontology engineering;semantic web	Stefan Haustein;Jörg Pleumann	2002		10.1007/3-540-48005-6_39	web service;web development;web modeling;the internet;semantic web rule language;data web;semantic search;semantic grid;web standards;computer science;semantic web;web navigation;rdf;ontology;social semantic web;linked data;semantic web stack;database;semantics;critical mass;web intelligence;web 2.0;world wide web;owl-s;website parse template;information retrieval;semantic analytics	Web+IR	-40.938934973667074	8.373135908087571	48188
33235fecd657163af9c8d85ad608a92111a5638c	recording and reasoning over data provenance in web and grid services	workflow management;web service;large scale;complex data;grid service;information system;service oriented architecture;geographic distribution	Large-scale, dynamic and open environments such as the Grid and Web Services build upon existing computing infrastructures to supply dependable and consistent large-scale computational systems. This kind of architecture has been adopted by those working with business and scientific information systems allowing them to exploit extensive and diverse computing resources to perform complex data processing tasks. In such systems, results are often derived by composing multiple, geographically distributed, heterogeneous services as specified by intricate workflow management. This leads to the undesirable situation where the results are known, but the means by which they were achieved is not. With both scientific experiments and business transactions, the notion of lineage and dataset derivation is of paramount importance since without it, information is potentially worthless. We address the issue of data provenance, the description of the origin of a piece of data, in these environments showing the requirements, uses and implementation difficulties. We propose an infrastructure level support for a provenance recording capability for service-oriented architectures such as the Grid and Web Services. We also offer services to view and retrieve provenance and we provide a mechanism by which provenance is used to determine whether previous computed results are still up to date.	experiment;information system;lineage (evolution);requirement;service-oriented architecture;web service	Martin Szomszor;Luc Moreau	2003		10.1007/978-3-540-39964-3_39	web service;workflow;computer science;operating system;service-oriented architecture;data mining;database;distributed computing;services computing;law;world wide web;information system;complex data type	HPC	-36.59163262662336	14.398980744632826	48228
5bc876d143f03c83f6c601eee2fd83f5e98ed0d0	driving large call center simulations using olap data cubes	data cube;call center	Simulating large call centers can require weeks or months. Additional time is spent on detailed analysis of input data to drive the models. Once a model is complete, the input data can be obsolete, requiring additional analysis and re-validation. This hinders the ability to provide accurate, timely models. This process is greatly improved by the use of OLAP data cubes to provide input data. Most simulation software packages have the functionality to read data from Excel workbooks. Data cubes provide data in a standard Excel pivot table format that can be easily updated to current data. Using OLAP cubes can significantly reduce the time to bring a simulation model to market, by streamlining the access to input data. Cubes also provide a quick, easy method to update data to the most current information. This ultimately provides a simulation analyst with a means to produce accurate, relevant and timely simulation solutions.	cubes;data cube;data validation;online analytical processing;pivot table;simulation software	Pam Laney Markt	2005			computer science;operating system;data mining;database;data cube	HPC	-34.117057953464	16.224278517883267	48250
d04ef8d8bc40dc590f9efe4dddcb6641c5aa34d7	description logics for interoperability	data model;semantic gap;description logic	Description Logics (DL) [5] are a very promising research area in Knowledge Representation (KR) with applications in databases (DBs). The main effort of the research in DL is in providing both theories and systems for expressing structured knowledge and for accessing and reasoning with it in a principled way. Recently, basic progress has been made by establishing the theoretical foundations for the effective use of DL in information systems [8]. DL offer promising formalisms for solving several problems concerning Conceptual Data Modelling and Ontology Design (see, e.g., [7], or the DAML+OIL and OWL efforts [20]), Intelligent Information Access and Query processing, and Information Integration and Interoperability. I want to argue that good Conceptual Modelling and Ontology Design is required to support powerful Query Management and to allow for semantic based Information Integration and Interoperability. Therefore, this short survey has been structured into three parts. In the first part, the notions of ontology language and of methodology for conceptual and ontology design will be introduced. In the second part, the query management problem in the presence of the previously devised conceptual model will be considered: a global framework will be introduced, together with various basic tasks involved in information access. In the last part, general issues about integration and interoperability will be presented. The most relevant research work carried out in our group will be cited in the paper, while for all the other relevant citations we refer to [4]. This work has been partially supported by the EU projects Sewasie, KnowledgeWeb, and Interop.	daml+oil;data modeling;database;description logic;information access;information system;interoperability;knowledge representation and reasoning;ontology (information science);theory	Enrico Franconi	2004			knowledge representation and reasoning;knowledge management;semantic interoperability;systems engineering;information integration;data modeling;interoperability;natural language processing;information system;computer science;data model;artificial intelligence;ontology language	AI	-43.25747093409878	5.280325762697877	48257
4e9697b9c93f992f07aa797686d8cc0359ca968d	procedures of integration of fragmented data in a p2p data grid virtual repository,	distributed system;query language;base donnee;systeme reparti;data integrity;integration information;par a par;database;base dato;semantics;p2p;almacen dato;fragmentacion;semantica;semantique;orientado servicio;lenguaje interrogacion;grid;virtual view;information integration;sistema repartido;poste a poste;vue virtuelle;rejilla;integrator;integrador;integracion informacion;grille;langage interrogation;oriente service;vista virtual;entrepot donnee;data warehouse;peer to peer;fragmentation;data grid;integrateur;service oriented	The paper deals with integration of distributed fragmented collections of data, being the basis for virtual repositories in the data grid or P2P architecture. The core of the described architecture is based on the Stack-Based Query Language (SBQL) and virtual updateable SBQL views. Our virtual repository transparently integrates distributed, heterogeneous and fragmented data producing conceptually and semantically coherent result. In the background the system is based on the P2P architecture. We provide three examples of data integration procedures, for either horizontal and vertical fragmentation. The procedures are implemented under the integrator prototype.		Kamil Kuliberda;Jacek Wislicki;Tomasz Marek Kowalski;Radoslaw Adamus;Krzysztof Kaczmarski;Kazimierz Subieta	2006		10.1007/11948148_54	integrator;computer science;information integration;operating system;data warehouse;peer-to-peer;data integrity;data grid;data mining;database;semantics;fragmentation;grid;world wide web;query language	HPC	-36.542576482579896	11.631709603441418	48259
23f08b3a68009e3b8472f0d064fc759007f5b289	on transforming a knowledge base from topic maps to owl	ontology transformation;topic maps;decision support systems;ontology;knowledge base	In this contribution we show, how we could overcome the shortcomings of the Topic-Map representation by applying our developed transformation concept for the particular system VCEDECIS. At a passive decision support system using Topic Maps as a semantic technology for representing its knowledge, it was decided to transform the knowledge representation to Web Ontology Language. We introduce a transformation concept that starts with a sound analysis of the source system. For typical patterns in the topic map the best corresponding patterns in OWL-DL were defined. A combination of general considerations and real examples act as a proof of concept.	knowledge base;map;topic maps;web ontology language	Kamil Matousek;Petr Kremen;Josef Küng;Reinhard Stumptner;Stefan Anderlik;Bernhard Freudenthaler	2011		10.1007/978-3-642-27549-4_27	natural language processing;topic maps;knowledge base;computer science;knowledge management;ontology;artificial intelligence;knowledge-based systems;ontology;open knowledge base connectivity;data mining	NLP	-42.94387519476329	6.113049605830638	48270
282babb5fea62a38f28f03c8fd0b36544b309c01	semantic management of middleware	distributed application;system configuration;application server;semantic technologies;conceptual model;semantic technology;web service;formal logic;middleware;service oriented architecture;ontology	The Ph.D. proposal addresses the complexity of building distributed applications and systems with Application Servers and Web Services middleware, respectively. Despite their flexible XML-based configuration, taming the ever growing complexity remains all but an easy task. To remedy such problems, the thesis proposes an ontology-based approach to support the management (i.e. development and administration) of Application Server and Web Services based applications. The ontology captures properties of, relationships between and behaviors of the components and services that are required for management purposes. The ontology is an explicit conceptual model with formal logic-based semantics. Therefore its descriptions of components and services may be queried, may foresight required actions, or may be checked to avoid inconsistent system configurations --- during development as well as during run time. Thus, the ontology-based approach retains the original flexibility, but it adds new capabilities for the developer and user of the system.	application server;artificial intelligence;distributed computing;middleware;run time (program lifecycle phase);web service;xml	Daniel Oberle	2004	KI	10.1145/1028480.1028484	upper ontology;middleware;bibliographic ontology;ontology inference layer;computer science;knowledge management;ontology;middleware;database;ontology-based data integration;world wide web;owl-s;process ontology;application server	SE	-43.94497193329758	11.82812043068852	48351
005d86e8420a3d765ba60d6b47910aae5d107470	web 2.0 and semantic web portal for annotation and discovery of web services in geosciences	web service;semantic web	The present invention provides a measurement device facilitating accurate quantitation of a substrate from a trace amount of sample and a highly reliable quantitating method of a substrate. The method uses an analysis element comprising a pair of electrodes for electrochemically quantitating reaction between a substrate in a sample and an oxidoreductase. One of the electrodes of the analysis element is a membrane formed on an inner wall of a cylindrical hollow space having an opening and the other is a needle. The needle electrode is projected temporarily external to the hollow space to puncture a subject and the resultant sample is collected from the subject.	semantic web;web 2.0;web service	Pearl Brazier;Artem Chebotko;Anthony Piazza;Ann Q. Gates;Leonardo Salayandia	2009			web modeling;social semantic web;web api;computer vision;information retrieval;data web;web service;web mapping;web page;artificial intelligence;semantic web stack;computer science	Web+IR	-40.21842282638709	5.216695739840975	48393
0a3771813d10b3488d446b23606690696e6f15ef	sync your data: update propagation for heterogeneous protein databases	modelizacion;base donnee;mise a jour;proteine;view maintenance;analisis datos;algoritmo adaptativo;software maintenance;base donnee tres grande;heterogeneous databases;database;base dato;bioinformatique;cache memory;data model;antememoria;actualizacion;scenario;data translation;modelisation;transformation donnee;maintenance logiciel;adaptive algorithm;data analysis;antememoire;transformacion dato;algorithme adaptatif;federated database;argumento;base donnee federee;update propagation;script;data transformation;life sciences;analyse donnee;scientific communication;modele donnee;base dato federada;proteina;schema evolution view maintenance;bioinformatica;very large databases;acido nucleico;acide nucleique;protein;modeling;nucleic acid;schema evolution;updating;data models;bioinformatics	The traditional model of bench (wet) chemistry in many life sciences domain is today actively complimented by computer-based discoveries utilizing the growing number of online data sources. A typical computer-based discovery scenario for many life scientists includes the creation of local caches of pertinent information from multiple online resources such as Swissprot [Nucleic Acid Res. 1(28), 45–48 (2000)], PIR [Nucleic Acids Res. 28(1), 41–44 (2000)], PDB [The Protein DataBank. Wiley, New York (2003)], to enable efficient data analysis. This local caching of data, however, exposes their research and eventual results to the problems of data staleness, that is, cached data may quickly be obsolete or incorrect, dependent on the updates that are made to the source data. This represents a significant challenge to the scientific community, forcing scientists to be continuously aware of the frequent changes made to public data sources, and more importantly aware of the potential effects on their own derived data sets during the course of their research. To address this significant challenge, in this paper we present an approach for handling update propagation between heterogeneous databases, guaranteeing data freshness for scientists irrespective of their choice of data source and its underlying data model or interface. We propose a middle-layer–based solution wherein first the change in the online data source is translated to a sequence of changes in the middle-layer; next each change in the middle-layer is propagated through an algebraic representation of the translation between the source and the target; and finally the net-change is translated to a set of changes that are then applied to the local cache. In this paper, we present our algebraic model that represents the mapping of the online resource to the local cache, as well as our adaptive propagation algorithm that can incrementally propagate both schema and data changes from the source to the cache in a data model independent manner. We present a case study based on a joint ongoing project with our collaborators in the Chemistry Department at UMass-Lowell to explicate our approach.	abstract syntax notation one;algorithm;blast;cache (computing);data model;download;drag and drop;emoticon;expressive power (computer science);fasta format;graph (discrete mathematics);graphical user interface;incremental backup;information privacy;john d. wiley;linear algebra;parsing;protein data bank;protein information resource;relational database;relevance;replay attack;software propagation;source data;spatial variability;testament;uniprot;web service;xml	Kajal T. Claypool;Elke A. Rundensteiner	2005	The VLDB Journal	10.1007/s00778-005-0155-7	data modeling;nucleic acid;systems modeling;cpu cache;data model;computer science;scenario;data mining;database;data analysis;software maintenance;data transformation;algorithm	DB	-33.80791119819034	14.972147572885255	48453
aafc1c668dc58238245fe9ab4d908ea079b51d00	service composition framework for big data service	service composition;service specification;big data transportation data visualization service oriented architecture computers educational institutions;workflow big data analysis service big data service development metamodel composition check transportation service composition framework transportation big data service expense problem human resource problem;framework service composition workflow big data service specification;big data;workflow;transportation big data data analysis traffic engineering computing;framework	"""Demand for big data analysis service has been increasing recently. In addition, the related business and market are also growing. However, big data service development requires a substantial amount of time and human resources. In this paper, we suggest a """"Service Composition Framework"""" for the development of a new big data service that easily combines a various big data services. The framework provides an environment with which the user can connect, and thus execute a specific service based on the desired Meta-model. We define the """"Workflow"""" to which the user links each service in the framework. The workflow can be executed after a """"Composition Check"""". In this paper, we apply the framework to the Transportation domain, i.e., the """"Transportation Service Composition Framework"""". Users can develop a transportation big data service easily through the transportation service composition framework. Such framework will reduce problems with expenses and increase reusability when developing a transportation big data service. In the future, we can solve problems with expenses and human resources when developing a big data service by applying the service composition framework to various domains."""	big data;logistics;metamodeling;metaobject;real life;service composability principle	Taewoo Nam;Kyungsuk Choi;Cheolmin Ok;Keunhyuk Yeom	2014	2014 International Conference on Future Internet of Things and Cloud	10.1109/FiCloud.2014.58	service level requirement;workflow;big data;differentiated service;computer science;service delivery framework;software framework;service design;data mining;database;service desk;data as a service;world wide web	Mobile	-47.97646833412906	16.243952777742084	48505
698e76cbe5e9fb49f7ca1d19141a7cddf793c816	representing and aligning similar relations: parts and wholes in isizulu vs. english		Ontology-enabled medical information systems are used in Sub-Saharan Africa, which require localisation of Semantic Web technologies, such as ontology verbalisation, yet keeping a link with the English language-based systems. In realising this, we zoom in on the partwhole relations that are ubiquitous in medical ontologies, and the isiZulu language. The analysis of part-whole relations in isiZulu revealed both ‘underspecification’—therewith also challenging the transitivity claim— and three refinements cf. the list of common part-whole relations. This was first implemented for the monolingual scenario so that it generates structured natural language from an ontology in isiZulu. Two new natural language-independent correspondence patterns are proposed to solve non-1:1 object property alignments, which are subsequently used to align the part-whole taxonomies informed by the two languages.	align (company);information system;language-independent specification;natural language;ontology (information science);semantic web;vertex-transitive graph;vocabulary	C. Maria Keet	2017		10.1007/978-3-319-59888-8_5	natural language processing;information system;ontology (information science);ontology;noun class;semantic web;natural language;natural language generation;artificial intelligence;transitive relation;computer science	AI	-42.887180466328914	7.151455423610654	48623
7e35ec9abb85b4400f9effb335dc5982b6851994	the sirup ontology query api in action	query language;ontologie;base donnee;navegacion informacion;metadata;sql;navigation information;integration information;reutilizacion;interrogation base donnee;information browsing;database;interrogacion base datos;langage declaratif;base dato;semantics;interface programme application;langage java;information access;semantica;semantique;lenguaje interrogacion;reuse;information integration;heterogeneidad;application program interfaces;integracion informacion;declarative language;metadonnee;acces information;ontologia;lenguaje java;acceso informacion;langage interrogation;metadatos;lenguaje declarativo;ontology;database query;reutilisation;heterogeneity;heterogeneite;java language	Ontology languages to represent ontologies exist in large numbers, and users who want to access or reuse ontologies can often be confronted with a language they do not know. Therefore, ontology languages are nowadays themselves a source of heterogeneity. In this demo, we present the SIRUP Ontology Query API (SOQA) [5] that has been developed for the SIRUP approach to semantic data integration [4]. SOQA is an ontology language independent Java API for query access to ontological metadata and data that can be represented in a variety of ontology languages. In addition, we demonstrate two applications that are based on SOQA: The SOQA Browser, a tool to graphically inspect all ontology information that can be accessed through SOQA, and SOQA-QL, an SQL-like query language that supports declarative queries against ontological metadata and data.	application programming interface;list of java apis;ontology (information science);query language;sql	Patrick Ziegler;Christoph Sturm;Klaus R. Dittrich	2006		10.1007/11687238_85	upper ontology;sql;ontology alignment;declarative programming;ontology components;bibliographic ontology;ontology inference layer;computer science;ontology;information integration;heterogeneity;ontology;data mining;reuse;database;semantics;ontology language;ontology-based data integration;programming language;metadata;world wide web;process ontology;query language;suggested upper merged ontology	AI	-35.791878211065175	11.288501670382852	48788
33dbc9321347a4930203d5fc7c63903103e65e7e	openscout: harvesting business and management learning objects from the web of data	linked data;metadata;sharing;open content	Already existing open educational resources in the field of Business and Management have a high potential for enterprises to address the increasing training needs of their employees. However, it is difficult to act on OERs as some data is hidden. In the meanwhile, numerous repositories provide Linked Open Data on this field. Though, users have to search a number of repositories with heterogeneous interfaces in order to retrieve the desired content. In this paper, we present the strategies to gather heterogeneous learning objects from the Web of Data, and we provide an overview of the benefits of the OpenScout platform. Despite the fact that not all data repositories strictly follow Linked Data principles, OpenScout addressed individual variations in order to harvest, align, and provide a single end-point. In the end, OpenScout provides a full-fledged environment that leverages on the Linked Open Data available on the Web and additionally exposes it in an homogeneous format.	align (company);linked data;semantic web;world wide web	Ricardo Kawase;Marco Fisichella;Katja Niemann;Vassilis Pitsilis;Ares Vidalis;Philipp Holtkamp;Bernardo Pereira Nunes	2013		10.1145/2487788.2487962	open content;computer science;linked data;data mining;database;metadata;world wide web	Web+IR	-43.164656113583476	8.957469924801524	48835
2a17d01a185b71fef2c1d28cb010c24ba01d2887	apecks: using and evaluating a tool for ontology construction with internal and external ka support	life cycle;web accessibility;knowledge structure;system architecture;ontology construction;domain ontology;knowledge engineering	Abstract   This paper describes Adaptive Presentation Environment for Collaboration Knowledge Structuring (APECKS), an experimental tool for collaborative ontology construction. APECKS takes a different line to most ontology servers, in that it is designed for use by domain experts, possibly in the absence of a knowledge engineer, and its aim is to foster and support debate about domain ontologies. To that end, it does not enforce ideals of consistency or correctness, and instead allows different conceptualizations of a domain to coexist. The system architecture and life cycle are introduced, and three extensive scenarios are outlined, showing how APECKS supports ontology construction, learning, ontology comparison and discussion. APECKS has also been used by several subjects during an evaluation experiment, and the results of this experiment are described. A particular factor about APECKS is that, as well as providing internal KA support, it is designed to interface with web-accessible KA tools, thereby allowing theoretically unlimited KA support for users. The prototype used WebGrid-II as external KA support, and the issues involved in integrating APECKS and WebGrid are discussed in detail.		Jeni Tennison;Kieron O'Hara;Nigel Shadbolt	2002	Int. J. Hum.-Comput. Stud.	10.1006/ijhc.2002.1000	upper ontology;biological life cycle;computer science;knowledge management;ontology;artificial intelligence;knowledge engineering;web accessibility;data mining;ontology-based data integration;process ontology;suggested upper merged ontology	Logic	-45.077897304830984	6.131445239251857	48932
39a5a51c32ed7eb4a0e7ced8372ec498a6cf3565	determining the trustworthiness of new electronic contracts	business to business;agent based system	Expressing contractual agreements electronically potentially allows agents to automatically perform functions surrounding contract use: establishment, fulfilment, renegotiation etc. For such automation to be used for real business concerns, there needs to be a high level of trust in the agent-based system. While there has been much research on simulating trust between agents, there are areas where such trust is harder to establish. In particular, contract proposals may come from parties that an agent has had no prior interaction with and, in competitive business-to-business environments, little reputation information may be available. In human practice, trust in a proposed contract is determined in part from the content of the proposal itself, and the similarity of the content to that of prior contracts, executed to varying degrees of success. In this paper, we argue that such analysis is also appropriate in automated systems, and to provide it we need systems to record salient details of prior contract use and algorithms for assessing proposals on their content. We use provenance technology to provide the former and detail algorithms for measuring contract success and similarity for the latter, applying them to an aerospace case study.	agent-based model;algorithm;approximation algorithm;data (computing);high-level programming language;k-nearest neighbors algorithm;machine learning;mobile phone;precomputation;similarity measure;simulation;trust (emotion);trust metric;weight function	Paul T. Groth;Simon Miles;Sanjay Modgil;Nir Oren;Michael Luck;Yolanda Gil	2009		10.1007/978-3-642-10203-5_12	public relations;contract management;knowledge management;business;commerce	AI	-43.92835997526778	17.20551600146958	49092
fa7599dd5c988f270d5326ab96fb294aa5e6ea64	a document classification and retrieval system for r&d in semiconductor industry - a hybrid approach	vector space model;hybrid approach;document classification and retrieval;document management system;document retrieval;document classification;document management	In this paper, a hybrid methodology with a vector space model (VSM) and process-oriented attributes for document management is proposed. The VSM is fine-tuned for classifying documents generated during R&D processes. The document correlation values are computed with the VSM for efficient retrieval. Only documents with high correlation values are presented to meet the specific retrieval purpose, which results in efficient and effective document retrieval. We further design a document classification and retrieval prototype system. The prototype is implemented to facilitate R&D document management in semiconductor industries. 2008 Elsevier Ltd. All rights reserved.	business process;continuation;document classification;document retrieval;extensibility;prototype;semiconductor industry;viable system model	Shui-Shun Lin	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.06.024	document retrieval;document clustering;computer science;document management system;data mining;database;vector space model;information retrieval	Web+IR	-41.091297476048155	6.0981952013764165	49183
111ecb54313d03f1c7bd2fc895b39ed87dc811fe	a web services orchestration solution for semantic multimedia indexing and retrieval	database indexing;semantic web service;multimedia metadata;multimedia retrieval;information retrieval;user complex queries;generic structure;vocabulary;metadata matching process;software systems;web services database indexing meta data multimedia databases ontologies artificial intelligence semantic web;web service;data mining;algorithm generic interface multimedia indexing semantic web services;multimedia systems;ontologies artificial intelligence;web services indexing multimedia systems vocabulary competitive intelligence software systems information retrieval content based retrieval software algorithms ontologies;semantic web services;multimedia retrieval task;algorithm generic interface;indexing;streaming media;web service modeling ontology;web services orchestration solution;semantic multimedia retrieval;indexation;multimedia communication;web services;semantic description;multimedia databases;semantic web;software algorithms;competitive intelligence;multimedia indexing;meta data;ontologies;multimedia indexation algorithms;content based retrieval;semantic multimedia indexing;algorithm design and analysis;metadata matching process web services orchestration solution semantic multimedia indexing semantic multimedia retrieval user complex queries multimedia indexation algorithms semantic description web service modeling ontology multimedia metadata generic structure multimedia retrieval task	In this article we are presenting a solution for the problem of combining various indexation algorithms in order to acquire a semantic multimedia indexation and to provide responses to the user complex queries. The challenge of this problem concerns the big heterogeneity of the multimedia indexation algorithms and the weak semantic aspect they address. Our solution considers a generic interface for the indexation algorithms, an implementation as Web services, as well as a semantic description in terms of WSMO (Web Service Modeling Ontology) of their functionality and orchestration. Original contribution of the article concerns the idea of organizing the various multimedia metadata types into a generic structure,used to express the user queries, the algorithms' generic interface, as well as the algorithms' WSMO metadata. This approach facilitates the definition of algorithm combination rules, and enables the reduction of the multimedia retrieval task to a metadata matching process.	algorithm;lindo;organizing (structure);server (computing);service-oriented architecture;wsmo;web service	Mihaela Brut;Florence Sèdes;Ana-Maria Manzat	2009	2009 International Conference on Complex, Intelligent and Software Intensive Systems	10.1109/CISIS.2009.180	semantic grid;computer science;semantic web stack;database;world wide web;information retrieval	Web+IR	-44.255974169864025	12.81448565901252	49239
7e69e7f379e2414e7b9ed939650eb20fd6d7440c	evolution of ubi-autonomous entities	modelizacion;topology;red www;food web;pervasive computing;cellular radio;food chain;industrie alimentaire;topologie;reseau web;semantics;embedded agents;intelligence artificielle;semantica;semantique;industria alimenticia;natural resources;topologia;informatica difusa;modelisation;autonomous agent;internet;food industry;ecosysteme;ecosistema;informatique diffuse;natural resource;intelligent network;recurso natural;ressource naturelle;artificial intelligence;world wide web;coordinacion;intelligent networks;inteligencia artificial;radiotelephonie cellulaire;modeling;world modeling and semantics;evolution computing;ecosystem;computational topology;coordination	Ubiquitous power includes one important factor that is the coordination among small instances of individualized smart programs. With the coordination, intelligent strategies can be established to realize computing topology and to serve different purposes of computing goals. With the development of cellular phones and internet, these individualized programs exist in the digital world where a new ecosystem can be formed. These smart entities create groups and consume natural resources in the digital world. Competition and collaboration thus exists among different groups of autonomous entities. We propose a model, based on a concept called food web in ecosystem. Food web exists in natural words for minions of years and serves as a balancing algorithm among different types of species. We define the food web model for autonomous entities and propose algorithms for formation and communication. As a consequence, the proposed model can be used to consider the development of intelligent strategies and the underlying communication topology such that ubiquitous intelligences can be evolved on the digital food web.	agent-based model;algorithm;autonomous robot;computer data storage;ecosystem;embedded system;entity;holography;information retrieval;intelligent agent;internet;mainframe computer;mobile agent;mobile phone;model of computation;numerical analysis;software engineering;state diagram;ubiquitous computing	Jason C. Hung;Kuan-Ching Li;Wonjun Lee;Timothy K. Shih	2006		10.1007/11833529_112	embedded system;intelligent network;computational topology;simulation;computer science;artificial intelligence;database;natural resource;semantics;world wide web;computer security;ubiquitous computing	Web+IR	-37.62510062447415	17.190409435215784	49243
12b899778cbe7ee280c65aea985951144f2ac65c	web ecology: recycling html pages as xml documents using w4f	xml document;world wide web	In this paper we present the World-Wide Web Wrapper Factory (W4F), a Java toolkit to generate wrappers for Web data sources. Some key features of W4F are an expressive language to extract information from HTML pages in a structured way, a mapping to export it as XML documents and some visual tools to assist the user during wrapper creation. Moreover, the entire description of wrappers is fully declarative. As an illustration, we demonstrate how to use W4F to create XML gateways, that serve transparently and on-they HTML pages as XML documents with their DTDs.	declarative programming;ecology;html;java;world wide web;xml	Arnaud Sahuguet;Fabien Azavant	1999			well-formed document;xml catalog;binary xml;xml base;site map;xml;streaming xml;web standards;computer science;document type definition;xml framework;soap;web page;database;web 2.0;world wide web;website parse template;xml schema editor;information retrieval;efficient xml interchange	DB	-38.86488011520322	8.604428609719312	49329
d4fc92d2ef0bbf2d665cc52ee021ddb3d796d582	combining automatic service composition with adaptive service recommendation for dynamic markets of services	software;service composition;image processing;reinforcement learning;learning artificial intelligence smoothing methods markov processes software image processing decision making transform coding;transform coding;on the fly computing service composition service recommendation reinforcement learning service markets;smoothing methods;service recommendation;web services decision making learning artificial intelligence recommender systems search problems;image processing domain automatic service composition dynamic market of services end users ambiguous user requests adaptive service recommendation best first backward search algorithm adaptive recommendation system decision making reinforcement learning techniques recommendation strategy user ratings;on the fly computing;markov processes;service markets;learning artificial intelligence	Automatic service composition is still a challenging task. It is even more challenging when dealing with a dynamic market of services for end users. New services may enter the market while other services are completely removed. Furthermore, end users are typically no experts in the domain in which they formulate a request. As a consequence, ambiguous user requests will inevitably emerge and have to be taken into account. To meet these challenges, we propose a new approach that combines automatic service composition with adaptive service recommendation. A best first backward search algorithm produces solutions that are functional correct with respect to user requests. An adaptive recommendation system supports the search algorithm in decision-making. Reinforcement Learning techniques enable the system to adjust its recommendation strategy over time based on user ratings. The integrated approach is described on a conceptional level and demonstrated by means of an illustrative example from the image processing domain.	backward induction;image processing;markov chain;markov decision process;recommender system;reinforcement learning;search algorithm;service composability principle;service discovery	Alexander Jungmann;Felix Mohr;Bernd Kleinjohann	2014	2014 IEEE World Congress on Services	10.1109/SERVICES.2014.68	computer science;data mining;multimedia;world wide web	Web+IR	-45.98323241690157	15.906939572169401	49378
8357cc381e7dc8fc07bac10076714e84c6f61426	fasta: a folksonomy-based automatic metadata generator	semantic metadata	Folksonomies provide a free source of keywords describing web resources, however, these keywords are free form and unstructured. In this paper, we describe a novel tool that converts folksonomy tags into semantic metadata, and present a case study consisting of a framework for evaluating the usefulness of this metadata within the context of a particular eLearning application. The evaluation shows the number of ways in which the generated semantic metadata adds value to the raw folksonomy tags.	folksonomy;web resource	Hend Suliman Al-Khalifa;Hugh C. Davis	2007		10.1007/978-3-540-75195-3_30	synonym ring;semantic grid;computer science;database;metadata;world wide web;information retrieval;metadata repository	NLP	-40.28441515499973	5.41437389234158	49388
7a08b11a444b04c4acefb68e6392d0c655f79276	bioinformatics data access service in the progengrid system	economie;site web;distributed system;economia;largeur bande;groupware;base donnee;systeme reparti;computational grid;systeme grande taille;maintenance;data management;distributed computing;database;service web;base dato;bioinformatique;heterogeneous data;large scale system;process integration;web service;sistema repartido;internet;analyse tâche;task analysis;virtual organization;anchura banda;data access;mantenimiento;calculo repartido;bandwidth;workflow;economy;bioinformatica;biological data;sitio web;economies of scale;high throughput;biological database;collecticiel;service integration;calcul reparti;web site;sistema gran escala;servicio web;bioinformatics	Current bioinformatics workflows require the collection of results coming from different tools on several Web sites. High-throughput services integrated through Web Services allow researchers to access a virtual organization by providing large computational and storage resources. There are considerable costs associated with running a high-throughput application including hardware, storage, maintenance, and bandwidth. Moreover, often such tools use biological data banks heterogeneous in the format and semantic, so the task of enabling their composition and cooperation is even more difficult. Researchers are now taking advantage of economies of scale by building large shared systems for bioinformatics processing. Integrating Computational Grids and Web Services technologies can be a key solution to simplify interaction between bioinformatics tools and biological databases. This paper presents a data access service for retrieving and transferring input data coming from heterogeneous data banks to high throughput applications, wrapped as Web Services.	bioinformatics;data access	Giovanni Aloisio;Massimo Cafaro;Sandro Fiore;Maria Mirto	2004		10.1007/978-3-540-30470-8_38	web service;high-throughput screening;data access;workflow;the internet;biological database;biological data;data management;computer science;economies of scale;operating system;data mining;task analysis;database;distributed computing;world wide web;computer security;bandwidth;process integration	HPC	-36.995119858373	12.7575289571222	49464
4729bab207325c031d31c5357f58a1205d0e348b	yasa-m: a semantic web service matchmaker	semantic web service;semantic matching degree aggregations;wsdl;semantics;resource description framework;web service;yasa descriptions;semantic matching degree aggregations semantic web service matchmaker wsdl yasa descriptions;syntactics;pattern matching;impedance matching;web services;semantic description;web services pattern matching semantic web;semantic web;ontologies;semantic web service matchmaker;semantic matching;semantic web web services ontologies telecommunications delay scalability simple object access protocol xml subspace constraints impedance matching	In this paper, we present new algorithms for matching Web services described in YASA4WSDL (YASA for short). We have already defined YASA, a semantic description of services that overcomes some issues in WSDL or SAWSDL. In this paper, we continue on our contribution and show how YASA Web services are matched based on the specificities of YASA descriptions. Our matching algorithm consists of three variants based on three different semantic matching degree aggregations. This algorithm was implemented in YASA-M, a new Web service matchmaker. YASA-M is evaluated and compared to well known approaches for service matching. Experiments show that YASA-M provides better results, in terms of precision, response time, and scalability, than a well known matchmaker.	algorithm;ambiguous name resolution;context-sensitive help;dependability;experiment;fire-control system;quality of service;requirement;response time (technology);sawsdl;scalability;semantic web service;semantic matching;service composability principle;web services description language	Yassin Chabeb;Samir Tata;Alain Ozanne	2010	2010 24th IEEE International Conference on Advanced Information Networking and Applications	10.1109/AINA.2010.122	web service;computer science;database;semantics;law;world wide web;information retrieval	DB	-44.26196067404964	13.628654382341324	49621
362fe6f70f5ee9af2077e6de9b9f18009f5c6f74	a formal general framework and service access model for service grid	protocols;electronic commerce;formal specification;quality of service resource management computer architecture grid computing web services application software protocols problem solving large scale systems programming;e business;application software;abstract state machine;howu service access model constituent resources quality of service e science e business web services interoperability formal specification virtual organization abstract state machines qos user request service grid system;e science;resource management;service access model;quality of service internet grid computing open systems formal specification natural sciences computing electronic commerce finite state machines;howu;user request;web service;qos;computer architecture;finite state machines;internet;constituent resources;abstract state machines;virtual organization;web services;interoperability;service grid system;quality of service;natural sciences computing;open systems;grid computing;programming;use case;grid system;problem solving;large scale systems	Constituent resources in a grid system need to be used in a coordinated fashion to deliver non trivial qualities of service. Various e-science and e-business use cases are investigated to guide how to create grid systems, and determine which functions grid systems should have. Web services emerge as a standard interoperable technology for grid systems. Although the motivations and goals for service grids are obvious, there is no clear definition for service grids to define and describe the general framework and service access model. In this paper, the general framework for service grids is defined in a formal approach, and the virtual organization based service access mechanism is modeled based on abstract state machines (ASM). In the service access model we proposed, the quality of service (QoS) issue is considered for the user request. This resulting serves as a theoretical base for our service grid system, HowU.	abstract state machines;e-science;electronic business;grid systems corporation;interoperability;open grid services infrastructure;quality of service;recursion;virtual organization (grid computing);web service	Deqing Zou;Weizhong Qiang;Xuanhua Shi	2005	10th IEEE International Conference on Engineering of Complex Computer Systems (ICECCS'05)	10.1109/ICECCS.2005.9	web service;service level requirement;service level objective;mobile qos;quality of service;service product management;differentiated service;computer science;basic service;service delivery framework;resource management;operating system;service design;database;distributed computing;service desk;programming language;data as a service;world wide web;service system;abstract state machines	HPC	-46.93129035699349	16.84033658604378	49987
d03d6aff14b1821d45c9a67bf5c4ff952b326bd9	a flexible approach for visualization development	design model;relational data model;implementation;prototypes;reference model;information analysis data visualisation;data type;information presentation;proof of concept;data visualisation;visualization;visualization techniques;visualization technique;visual development;information presentation visualization development visualization problems data volume data types information generation information representation;information representation;data visualization;ip networks;visualizaiton reference model;implementation visualization techniques visualizaiton reference model information generation information representation information presentation framework architecture;architecture;information analysis;data visualization data models visualization ip networks context information representation prototypes;context;information generation;framework;data models	Visualization problems nowadays often incorporate increasingly high complexities that may emerge from tremendous data volume, combined data types, the need of integrating multiple visualizations, visualization evaluation, and/or multiple paradigms/domains covered. These issues significantly exacerbate the difficulties involved in the development of flexible and effective visualization solutions. Dealing with such complex problems often requires visualizations to be developed in a manner that they can be flexibly created, instantiated, manipulated, customized, integrated with other visualizations, executed and modified. However, while existing visualization techniques and systems tend to provide reasonable support for particular paradigms, domains and data types, they are quite weak when it comes to addressing such flexibility requirements. To address these issues and requirements, we propose an information generation, representation and presentation (IGRP) approach to support visualization to be designed/modeled/implemented in a flexible manner. This approach is specified and demonstrated through IGRP reference model, framework and architecture that highlight the idea of developing visualizations through manipulating, creating, modifying, enhancing, mapping and integrating the IGRP related data, models, solvers and scenarios. A prototypical system is also implemented and discussed as a proof of concept.	application domain;domain theory;norm (social);programming paradigm;prototype;reference model;requirement;solver	Xiaoyan Bai;David C. White;David Sundaram	2010	2010 Sixth International Conference on Signal-Image Technology and Internet Based Systems	10.1109/SITIS.2010.58	reference model;visualization;data type;computer science;data science;theoretical computer science;architecture;data mining;database;programming language;implementation;proof of concept;data visualization	Visualization	-34.49121279761483	13.36734537677833	50122
155eeede0c070f1f017ba5c9f6cacd7ae0b098aa	efficient authorization of graph database queries in an attribute-supporting rebac model		Neo4j is a popular graph database that offers two versions; a paid enterprise edition and a free community edition. The enterprise edition offers customizable Role-Based Access Control (RBAC) features through custom developed procedures, while the community edition does not offer any access control support. Being a graph database, Neo4j is a natural application for Relationship-Based Access Control (ReBAC), an access control paradigm where authorization decisions are based on relationships between subjects and resources in the system. In this paper we present AReBAC, an attribute-supporting ReBAC model for Neo4j (applicable to both editions) that provides finer grained access control. AReBAC employs Nano-Cypher, a declarative policy language based on Neo4j»s Cypher query language, the result of which allows us to weave database queries with access control policies and evaluate both simultaneously. Evaluating the combined query and policy produces a result that i) matches the search criteria, and ii) the requesting subject has access to. Our experiments show that our evaluation algorithm performs faster than Neo4j»s query evaluation engine when evaluating queries that are expressible using Nano-Cypher.	algorithm;authorization;constraint satisfaction problem;cypher query language;eval;experiment;gnu nano;graph database;neo4j;performance evaluation;programming paradigm;role-based access control;usability;vertex (graph theory);web search engine	Syed Zain R. Rizvi;Philip W. L. Fong	2018		10.1145/3176258.3176331	computer security;information retrieval;access control;query language;role-based access control;graph database;authorization;computer science	DB	-41.058788777320004	12.005059963803193	50162
93aef6553116b395a1bce7f827701e5b4c324ddd	discovering interdatabase resemblance of classes for interoperable databases	canonical model;databases phase detection object detection context modeling knowledge acquisition data models data mining large hadron collider merging;knowledge acquisition;distributed databases;relational databases;aggregation graphs bloom model rdbms interdatabase resemblance classes interoperable databases database schemas semantic relationships deep knowledge generalization specialization semilattices;open systems;relational databases distributed databases knowledge acquisition open systems	1 The integration of database schemas into a federated one involves a detection phase where the similarities that exist among the classes of the different databases must be discovered in order to determine their semantic relationships. This phase is typically characterized by the complexity of the comparison task and the deep knowledge about the semantics of the databases that is required to perform it. Here we present our approach to handle this problem by upgrading the semantic level of the local schemas and guiding the search of the comparison process by the structure of the generalization/specialization semilattices and aggregation graphs of the resulting rich schemas. Unnecessary as well as most promising comparisons are identified and a systematic procedure to analyze the rich component schemas in a meaningful way is given. A reduction of the complexity is achieved. The methodology serves as the basis for a tool that semiautomatizes the process. The expressiveness of our canonical model plays a central role.	canonical model;complexity;database schema;interoperability;partial template specialization	Manuel García-Solaco;Malú Castellanos;Fèlix Saltor	1993		10.1109/RIDE.1993.281949	computer science;theoretical computer science;data mining;database	DB	-38.22031577056088	4.674892980589313	50292
6965b020a9e5e727265a0fa77e5033b237b4594c	sematch: semantic entity search from knowledge graph	informatica;telecomunicaciones	As an increasing amount of the knowledge graph is published as Linked Open Data, semantic entity search is required to develop new applications. However, the use of structured query languages such as SPARQL is challenging for non-skilled users who need to master the query language as well as acquiring knowledge of the underlying ontology of Linked Data knowledge bases. In this article, we propose the Sematch framework for entity search in the knowledge graph that combines natural language query processing, entity linking, entity type linking and semantic similarity based query expansion. The system has been validated in a dataset and a prototype has been developed that translates natural language queries into SPARQL.	algorithm;database;entity linking;entity–relationship model;knowledge graph;linked data;natural language processing;natural language user interface;prototype;query expansion;query language;run time (program lifecycle phase);sparql;semantic search;semantic similarity;web services for devices;word-sense disambiguation	Ganggao Zhu;Carlos Angel Iglesias	2015			query expansion;computer science;sparql;data mining;entity linking;database;rdf query language;web search query;information retrieval;query language	NLP	-36.798886810863145	5.955618344273281	50349
6e652db98e9f5d4c7cebaa2abcd3a929bc1dc3d5	an hl7-aware multi-agent system for efficiently handling query answering in an e-health context	servicio sanidad;health service;distributed system;gestion informacion;health care services;multiagent system;systeme reparti;multi agent system;aplicacion medical;service information;salud publica;interrogation base donnee;interrogacion base datos;intelligence artificielle;sistema repartido;internet;service sante;information management;servicio informacion;sante publique;artificial intelligence;medical application;inteligencia artificial;information service;information system;gestion information;query answering;sistema multiagente;database query;systeme information;public health;systeme multiagent;application medicale;sistema informacion	In this paper we present a multi-agent system aiming at supporting patients to search health care services of their interest in an e-health scenario. The proposed system is HL7-aware in that it represents both patient and service information according to the directives of HL7, the information management standard adopted in medical context. In this paper we illustrate the technical characteristics of our system and we present a comparison between it and other related systems already proposed in the literature.	data mining;digital library;health level 7;information management;joint conference on digital libraries;multi-agent system;nick mckeown;personalization;personalized search;query expansion;symposium on applied computing;user interface	Pasquale De Meo;Gabriele Di Quarto;Giovanni Quattrone;Domenico Ursino	2006		10.1007/11914853_61	the internet;public health;computer science;artificial intelligence;data mining;information management;world wide web;computer security;information system	AI	-38.42009642201154	13.977277570292681	50512
c3d995823caca1f14fac3f1e025aa4e2ddfb65cd	composing owl-s web services	semantic similarity;semantic web service;semantic similarity computation;owl s;numerical method;planning artificial intelligence;ontology matching threshold owl s semantic web service composition ai planning like algorithm name matching semantic similarity computation cycle detection;web services semantic web ontologies web and internet services credit cards computer aided manufacturing collaborative work process planning technology planning technology management;web service;ai planning like algorithm;ontologies artificial intelligence;web service composition;knowledge representation languages;ontology matching threshold;pattern matching;cycle detection;state space;web services;semantic web;on the fly;web services knowledge representation languages ontologies artificial intelligence pattern matching planning artificial intelligence semantic web;semantic web service composition;ai planning;ontology matching;name matching	There are numerous methods to achieve Web service composition, ranging from semi-automatic to on-the-fly automated processes. This paper proposes an approach to semantic Web service composition based on an AI-planning-like algorithm, which proceeds in a depth-first state-space search manner. Features include name matching, semantic similarity computation, cycle detection and optimization. The system gives users the flexibility of altering various parameters relating to the depth of composition, ontology matching threshold and other factors, prior to launching the composition process. A typical test scenario is also presented.	algorithm;automated planning and scheduling;business process;computation;cycle detection;depth-first search;dictionary;mathematical optimization;owl-s;online and offline;ontology alignment;scenario testing;semantic web service;semantic similarity;semiconductor industry;service composability principle;state space search;test data	Bao Duy Tran;Puay Siew Tan;Angela Goh	2007	2007 IEEE Conference on Emerging Technologies and Factory Automation (EFTA 2007)	10.1109/EFTA.2007.4416785	automated planning and scheduling;web service;computer science;artificial intelligence;social semantic web;semantic web stack;database;world wide web;owl-s;information retrieval	AI	-43.94129588677037	13.303223612549251	50521
9836eef80196f6c2760eecd9cadad504d1d87eee	research on data exchange push technology based on message-driven	design model;topology;electronic mail;web and internet services;recommended find;server push;publishing;data exchange;message driven;data engineering;web service;service model;recommending system;semantic web data exchange push technology message driven model web service recommending system publish subscribe technology;servers;computational modeling;web services simple object access protocol concrete web and internet services mobile communication artificial intelligence data engineering electronic mail service oriented architecture topology;data exchange recommended find message driven server push publish subscribe;monitoring;registers;publish subscribe;web services;mobile communication;message driven model;publish subscribe technology;message passing;semantic web;data exchange push technology;artificial intelligence;middleware;service oriented architecture;simple object access protocol;electronic data interchange;concrete;web services electronic data interchange message passing middleware semantic web	the aims system of message-driven model changes the way of data exchange based on Web Services from pull passively to push actively. The message-driven publish/ subscribe gives recommended find Web Services model, it changes the way users find Web Services. The key technologies of the platform model are discussed Last verifies the feasibility of the design model combined with concrete examples. The target system has a good application prospect.	centralized computing;distributed computing;interoperation;publish–subscribe pattern;push technology;semantic web;sensitivity and specificity;web service;world wide web	Xiwei Feng;Feng Xue;Tongwei Zhang	2009	2009 International Joint Conference on Artificial Intelligence	10.1109/JCAI.2009.185	web service;information engineering;computer science;database;internet privacy;world wide web	SE	-41.95030317057805	14.70787488166812	50543
100740b375c92891f07ff22759ce48190f3befed	putting similarity assessments into context: matching functions with the user's intended operations	semantic similarity;systeme intelligent;formal specification;systeme information geographique;geographic information system;information retrieval;integration information;sistema inteligente;contextual information;matching function;satisfiability;word sense disambiguation;specification formelle;especificacion formal;information integration;recherche information;integracion informacion;intelligent system;recuperacion informacion;analyse contextuelle;semantic relations;sistema informacion geografica	This paper presents a practical application of context for the evaluation of semantic similarity. The work is based on a new model for the assessment of semantic similarity among entity classes that satisfies cognitive properties of similarity and integrates contextual information. The semantic similarity model represents entity classes by their semantic relations (is-a and part-whole) and their distinguishing features (parts, functions, and attributes). Context describes the domain of an application that is determined by the user’s intended operations. Contextual information is specified by a set of tuples over operations associated with their respective entity-class arguments. Based on the contextual information, a partial word-sense disambiguation can be achieved and the relevance of distinguishing features for the similarity assessment is calculated in terms of the features’ contribution to the characterization of the application domain.	application domain;database;domain of discourse;entity–relationship model;heart rate variability;information system;information theory;interoperability;is-a;partial word;relevance;self-information;semantic similarity;word-sense disambiguation	M. Andrea Rodríguez;Max J. Egenhofer	1999		10.1007/3-540-48315-2_24	semantic similarity;computer science;information integration;data mining;formal specification;database;geographic information system;linguistics;information retrieval;satisfiability;dishin	Web+IR	-36.08522849275694	11.90031699536814	50707
f24010bdcd8a874bf5a7240c2fc66d848bcaf82e	an effective ontology matching technique	ontology matching	In this paper, we study the ontology matching problem and propose an algorithm, which uses as a backbone a multi-level matching technique and performs a neighbor search to find the correspondences between the entities in the given ontologies. A main feature of this algorithm is the high quality of the matches it finds. Besides, as the result of the initial search introduced, our algorithm converges fast, making it comparable to existing techniques.	algorithm;display resolution;entity;fast fourier transform;internet backbone;ontology (information science);ontology alignment	Ahmed Alasoud;Volker Haarslev;Nematollaah Shiri	2008		10.1007/978-3-540-68123-6_63	ontology alignment;computer science;3-dimensional matching;machine learning;optimal matching;data mining;information retrieval	AI	-39.015716560376404	5.812594293485663	50733
83115bf247ac2b2ce646870c1c43b6d2d7ceeb2e	sisels: semantic integration system for exploitation of biological resources	biology computing;data integration knowledge representation bioinformatics;semantic integration system;virtual instrumentation;data integrity;metadata;query processing;semantic integration;vocabulary;sisels;biological system modeling;virtual reality;semantics;mediation system;satisfiability;virtual laboratory;visualization;biology virtual laboratory;laboratory techniques;metadata sisels semantic integration system biology virtual laboratory mediation system distributed biological resources;ontologies collaboration visualization mexico council computer science electronic mail computer architecture mediation vocabulary biological system modeling;virtual reality biology computing laboratories laboratory techniques virtual instrumentation;ontologies;knowledge representation;data integration;bioinformatics;distributed biological resources	This article describes general architecture and construction principles to develop a Biology virtual laboratory through SISELS. SISELS is a mediation system that enables the configuration of virtual laboratories to provide transparent access to distributed biological resources (data or services). SISELS exploits the metadata associated to the subscribed resources to classify and organize them respect to their structure and content. This way, it is possible to generate subspaces of resources, denominated views respect to the requirements of a group of experts to study a biological problem. A view represents the semantic requirements of a group of experts and a subset of relevant resources satisfying partially or totally these requirements. SISELS uses three main levels of metadata to model the knowledge domain of the virtual laboratory based on the resources’ metadata and the semantic correspondences between the resources and the domain.	requirement;semantic integration	Gabriela Montiel-Moreno;José-Luis Zechinelli-Martini;Genoveva Vargas-Solar	2009	2009 Mexican International Conference on Computer Science	10.1109/ENC.2009.27	semantic integration;visualization;semantic grid;computer science;ontology;artificial intelligence;data integration;data integrity;database;semantics;virtual reality;programming language;metadata;world wide web;information retrieval;satisfiability	Visualization	-42.875456868655945	9.294872286397121	50794
fab6c3f2c0253f86ef6dafdbc4868fe5707ffc4e	interactive visual analysis of multi-dimensional metamodels			interactive visual analysis;metamodeling	Sascha Gebhardt;Sebastian Pick;Bernd Hentschel;Torsten Wolfgang Kuhlen	2018		10.2312/pgv.20181098	interpolation;interactive visual analysis;computer science;theoretical computer science	HCI	-35.140912993147545	9.804963734613144	50799
ee6ec3a0342d7083bd9d866ee1096e09667019e5	a schema and ontology-aided intelligent information integration	multiple intelligences;extensible markup language;local as view;syntactic and semantic interoperability;information sources;information technology;semantic interoperability;data model;semantic heterogeneity;information integration;intelligent information integration;xml;cost effectiveness;global as view;ontology	The research issues of intelligent information integration have become ubiquitous and critically important in e-business (EB) with the increasing dependence on Internet/Intranet and information technology (IT). Accessing the intelligent information sources separately without integration may lead to the chaos of information requested. It is also not cost-effective in EB settings. A common general way to deal with heterogeneity problems in traditional III is to create a common data model. The eXtensible Markup Language (XML) has been the standard data document format for exchanging information on the Web. XML only deals with the structural heterogeneity; it can barely handle the semantic heterogeneity. Ontologies are regarded as an important and natural means to represent the implicit semantics and relationships in the real world. And they are used to assist to reach semantic interoperability in III in this research. In this paper, we provide a generic construct orientation no ad hoc method to generate the global schema to enable the web-based alternative to traditional III. We provide a wiser query method over multiple intelligent information sources by applying global-as-view (GAV) and local-as-view (LAV) approach with the use of ontology to enhance both structural and semantic interoperability of the underlying intelligent information sources. We construct a prototype implementing the method to provide a proof on the validity and feasibility.	database schema	Jia-Lang Seng;I. L. Kong	2009	Expert Syst. Appl.	10.1016/j.eswa.2009.02.067	idef1x;semantic interoperability;xml;semantic integration;computer science;information integration;ontology;data mining;database;ontology-based data integration;information technology;information retrieval	Robotics	-37.661016756156364	9.57579853269344	50804
6262059c78cd61b1096e1931ca92c3fb0e3664d8	gaining knowledge information and delivery of services via the web	web based service delivery space time research data management services intelligent service delivery decision making knowledge template knowledge substantiation knowledge management;information resources;knowledge management;data management;web based service delivery;work environment;space time;data analysis;demography information resources data warehouses decision support systems;data management services;decision making environmental management knowledge management simple object access protocol spatial databases xml australia data analysis demography statistics;spatial databases;decision support systems;knowledge template;xml;statistics;data warehouses;simple object access protocol;demography;environmental management;knowledge substantiation;intelligent service delivery;australia;space time research	Our interest is in exploring how to gradually transfer traditional data management services to an integrated solution environment where intelligent services can be delivered. Data managed and delivered by Space-Time Research contains valuable information such as business trend and consumer patterns. Once the associated knowledge is elicited, available data can be effectively utilised to serve various business needs such as decision making. We deal with the concept of a knowledge template-an intermediate step towards eventual knowledge substantiation and knowledge management. In particular we are examining how knowledge templates may be produced and delivered via the Web to users' work environments leading to subsequent knowledge development and management activities. The diagram describes a technical framework that would allow the creation and delivery of such templates.	world wide web	Wei Dai;Donald McIntosh	2001		10.1109/ICSMC.2001.972947	knowledge base;xml;data management;computer science;knowledge management;space time;soap;data mining;database;personal knowledge management;data analysis;knowledge value chain;domain knowledge;statistics	Web+IR	-48.01441241879696	8.430851707259555	50831
cedf94f268a240607fb0724bfa817c908fe4bb96	enhance the interoperability of the library search systems with zsapn	access point;information retrieval;z39 50;distributed environment;interoperability;semantic access point substitution	The wide adoption of the Z39.50 protocol from the Libraries exposes their abilities to participate in a distributed environment. In spite of the specification of a unified global access mechanism from the Z39.50 protocol, unsupported Access Points result to query failures and/or inconsistent answers. A challenge to this issue is to substitute an unsupported Access Point with others, so that the most similar semantics to the original Access Point can be obtained. In this paper we present the zSAPN (Z39.50 Semantic Access Point Network), a system which enhance the interoperability of the library search systems, by exploiting the semantics from the Bib-1 Access Point official specification of the Z39.50 information retrieval protocol. zSAPN substitutes each unsupported Access Point with a set of other supported ones, whose appropriate combination would either broaden or narrow the initial semantics, according to the user's choice.	information retrieval;interoperability;library (computing)	Michalis Sfakakis;Sarantos Kapidakis	2008		10.1145/1389586.1389634	semantic interoperability;interoperability;computer access control;telecommunications;computer science;data mining;database;information retrieval;distributed computing environment	HPC	-37.73944318329807	9.579268038816178	50919
f5d7985c723964c64d0298d790d91ac394ebe7e7	cookingcake: a framework for the adaptation of cooking recipes represented as workflows		This paper presents CookingCAKE, a framework for the adaptation of cooking recipes represented as workflows. CookingCAKE integrates and combines several workflow adaptation approaches applied in process-oriented case based reasoning (POCBR) in a single adaptation framework, thus providing a capable tool for the adaptation of cooking recipes. The available case base of cooking workflows is analyzed to generate adaptation knowledge which is used to adapt a recipe regarding restrictions and resources, which the user may define for the preparation of a dish.	case-based reasoning;multiple inheritance;ontology (information science);parabolic antenna;requirement	Gilbert Müller;Ralph Bergmann	2015			simulation;engineering;knowledge management;operations management	NLP	-44.48534199422721	9.667911556111113	51091
95f1e5c863c4c908381e91cba06a7b3ce4426370	autonomous decentralized system for knowledge refinement of contents published over networks		Recently, a large number of useful contents are accumulated in large-scale networks like the Internet. On the other hand, plural sources often provide slightly different content concerning a same knowledge, and this causes incoherence of the knowledge for the network users. This paper proposes an autonomous decentralized knowledge refinement system that provides reliable knowledge to the users. This system consists of plural subsystems, and they autonomously subscribe contents from a lot of content sources over the network, refine and reconstruct the contents, and publish reliable knowledge. The effectiveness of this system is evaluated by computer simulations.	autonomous decentralized system;decentralised system	Takuma Horiuchi;Shinji Sugawara	2017		10.1007/978-3-319-61566-0_68	the internet;plural;autonomous decentralized system;distributed computing;computer science;publication	ML	-40.47728101730193	13.299975830892297	51306
21ae4b8227bfd4ac11cd3fa1ef24b8a20cd3b7b7	database as a service (dbaas)	databases;software;outsourcing;reliability;data management platform;application software;functional properties;data management;research and development management;relational databases application software database systems satellite broadcasting spatial databases environmental economics object oriented modeling research and development management technology management context aware services;relational database;web applications;technology management;satellite broadcasting;tutorials;persistency layer;programming perspective;spatial databases;database systems;web services;environmental economics;relational database system;service level agreement;software as a service;relational databases;service level agreement database as a service web applications eternal beta applications data management platform relational database persistency layer software as a service database services programming perspective;programming;eternal beta applications;meteorology;web services relational databases;object oriented modeling;database as a service;context aware services;database services	Modern Web or “Eternal-Beta” applications necessitate a flexible and easy-to-use data management platform that allows the evolutionary development of databases and applications. The classical approach of relational database systems following strictly the ACID properties has to be extended by an extensible and easy-to-use persistency layer with specialized DB features. Using the underlying concept of Software as a Service (SaaS) also enables an economic advantage based on the “economy of the scale“, where application and system environments only need to be provided once but can be used by thousands of users. Within this tutorial, we are looking at the current state-of-the-art from different perspectives. We outline foundations and techniques to build database services based on the SaaS-paradigm. We discuss requirements from a programming perspective, show different dimensions in the context of consistency and reliability, and also describe different non-functional properties under the umbrella of Service-Level agreements (SLA).	acid;cloud database;programming paradigm;relational database;requirement;service-level agreement;software as a service	Wolfgang Lehner;Kai-Uwe Sattler	2010	2010 IEEE 26th International Conference on Data Engineering (ICDE 2010)	10.1109/ICDE.2010.5447723	data management;relational database;computer science;technology management;data mining;database;world wide web	DB	-34.17961183531882	13.831661676503437	51356
4646e8ee77691607dbd2f62142b3451761445361	trust management framework for intelligent agent negotiations in ubiquitous computing environments	intelligent multi agent systems;multi agent system;trust management;distributed computing environments;satisfiability;service resource requestors providers;reputation mechanism;information dissemination;intelligent agent;collaborative reputation mechanism;distributed computing environment;empirical evaluation;ubiquitous computing environment	In dynamic ubiquitous computing environments, syste m entities may be classified into two main categories that are, in principle, in conflict . These are the Service Resource Requestors (SRRs) wishing to use services and/or exploit resou rces offered by the other system entities and the Service Resource Providers (SRPs) that offe r the services/resources requested. Seeking for the maximisation of their welfare, whil e achieving their own goals and aims, entities may misbehave (intentionally or unintentio ally), thus, leading to a significant deterioration of system’s performance. In this stud y, a reputation mechanism is proposed which helps estimating SRPs trustworthiness and pre icting their future behaviour, taking into account their past performance in consistently satisfying SRRs’ expectations. Thereafter, under the assumption that a number of SRPs may hand le the SRRs requests, the SRRs may decide on the most appropriate SRP for the service / resource requested on the basis of a weighted combination of the evaluation of the quali ty of their offer (performance related factor) and of their reputation rating (reliability related factor). The proposed trust management framework is distributed, considers both first-hand information (acquired from the SRR’s direct past experiences with the SRPs) an d second-hand information (disseminated from other SRRs’ past experiences with the SRPs), w hile it exhibits a robust behaviour against inaccurate reputation ratings. The designed mechanisms have been empirically evaluated simulating interactions among self-intere sted agents, exhibiting improved performance with respect to random SRP selection.	entity;intelligent agent;interaction;reputation system;scsi rdma protocol;simpl;simulation;trust (emotion);trust management (information system);trust management (managerial science);ubiquitous computing	Malamati D. Louta;Angelos Michalas;Ioannis Anagnostopoulos;Dimitrios J. Vergados	2009	Telecommunication Systems	10.1007/s11235-009-9155-z	simulation;computer science;knowledge management;computer security;intelligent agent;distributed computing environment;computer network;satisfiability	Web+IR	-43.574902676080605	16.513536205315962	51602
ac8ac812a92f1f16349d7eb37ae5b5caa0d8e53b	community division-based swot ontology directory service	prototypes;semantics;internet of things;heuristic algorithms;ontologies;organizations;peer to peer computing	On account of plenty services among Internet of Things (IoT) with a poor efficiency in semantic organization and dynamic management to various service resources, Semantic Web of Things (SWoT) is prompted as its improved form. To solve this problem, a method named as community division-based ontology directory service is proposed to organize domain-related services in SWoT, and a prototype system for community division-based SWoT ontology directory service named as CDB-SWoT-ODS is designed. In system CDB-SWoT-ODS, ontology summarization is used to reduce service ontology volume firstly, then unstructured P2P socialized network are resolved and divided into dynamic semantic communities through an autonomic community generating algorithm (SoFA), which get ready for generating ontology directory. Furthermore, a pattern tree cater to user preferences is generated and stored into ontology directory service module. As a result, by using SWoT ontology directory generating algorithm (SWoT-ODG), a service candidate set can be automatically pushed to service applicants, so that comprehensive and efficient users' personalized service needs can be satisfied.	algorithm;autonomic computing;directory (computing);directory service;internet of things;operational data store;personalization;prototype;semantic web;socialization;user (computing);web of things	Ying Pan;Guan-Yu Li;Zhong-Jun Lu	2016	2016 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC)	10.1109/CyberC.2016.33	upper ontology;lightweight directory access protocol;directory service;bibliographic ontology;computer science;organization;ontology;data mining;database;semantics;prototype;ontology-based data integration;world wide web;computer security;internet of things;process ontology;computer network;suggested upper merged ontology	Web+IR	-42.81384053261558	15.306665501572683	51809
235c06c5370fa3ce3d86f0994c04de227e5bdb63	web service based universal management of workflow resources	web service	Implementing business process solutions in the way of Web service is being positioned in the center of workflow manag ement. However, there is no robust standard to expose and access workflow resources by Web service interfaces. In this paper, we propose a web service based workflow resource management framework named Universal Resource Manage ment Framework (URMF) with declarations of web service interfaces and interaction protocols among them. We also in troduce a substitutive workflow interface model employing Web services and URMF. Finally, a prototype implementati on model of URMF with J2EE platform is also introduced.	business process;java platform, enterprise edition;prototype;resource description framework;web service	Jinyoung Jang;Yongsun Choi	2004			workflow management system;web api;web modeling;workflow engine;web service;database;workflow;computer science;ws-policy;postback	Web+IR	-42.00449650190143	12.185073583746906	51810
5c94394605858d8144c379d3c1f3079149c57649	the electronic tool integration platform as a supplier and user of graph based tools	tool integration;domain specificity	Abstract ETI is an Electronic Tool Integration platform designed for project-oriented, domain-specific, public or private interactive experimentation with heterogeneous tools. Its users can experiment at ease over the web with the integrated tools and functionalities, and build own prototypical problem solvers on the basis of components (tools and transformations) contained in the ETI repository. In this paper we describe the graph concept underlying ETIs flexibility, the significant role played by graphs in the ETI platform, and the kind of support ETI offers to deal with graph objects. This way we sketch the potential of a closer cooperation between the ETI community and the community of research on graph-based tools.	integration platform	Tiziana Margaria	2002	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)80527-4	simulation;computer science;data mining	Logic	-47.67049973025372	11.309829636598117	51928
d9b9737977bc593533459e6e2b3f49b927aa0e0e	a wsmo-based semantic web services discovery framework in heterogeneous ontologies environment	semantic web service;web service;web service modeling ontology	Nowadays, WSMO (Web Service Modeling Ontology) has received great attention of academic and business communities, since its potential to achieve dynamic and scalable infrastructure for web services is extracted. Therefore, we design an ontology-based Semantic Web Services (SWSs) discovery framework based on WSMO so as to searching dynamically web services located at different nodes. Also, we provide several SWSs matching techniques based on this framework.	ontology (information science);semantic web service;wsmo;web services discovery	Haihua Li;Xiaoyong Du;Xuan Tian	2007		10.1007/978-3-540-76719-0_68	web service;web application security;web development;web modeling;data web;web mapping;web design;web standards;computer science;ws-policy;service-oriented architecture;semantic web;web navigation;social semantic web;data mining;semantic web stack;database;web intelligence;web 2.0;world wide web;owl-s;web coverage service	Web+IR	-43.74776758354932	12.034581269877428	51938
8ab56cb22163cfed30beaf0507b4785437fecc1c	integration issues in implementing semantic data models	integration issue;semantic data model		data model	Brian A. Nixon;John Mylopoulos	1987			semantic data model;idef1x;semantic interoperability;semantic computing;semantic integration;semantic search;semantic grid;computer science;social semantic web;semantic web stack;semantic compression;database;ontology-based data integration;semantic technology;programming language;semantic analytics;data mapping	DB	-38.33232058767403	6.91076940015832	52072
886dbe835a7a8b61692f6cedc2b7ce0521119e3b	modeling and discovering data services over sparql services	data models resource description framework ontologies computational modeling books database languages semantics;sparql data service rdf service modeling service discovery;internet data handling;sparql;service discovery;service modeling;jena framework data services sparql services data publishing data sharing world wide web;rdf;data service	Data services have almost become a standard way for data publishing and sharing on the web. Lack of well-defined machine readable model hinders its spreading. In this paper, we devote ourselves to modeling and discovery of data services. We use RDF model to describe the data scheme and semantics of the data services. Then we define a simplified SPARQL query to retrieve the satisfactory data services in means of evaluating the query against the RDF models. Finally we implement a prototype based on Jena framework.	apache jena semantic web framework;html;human-readable medium;on the fly;prototype;relational database;requirement;resource description framework;sparql;triple des;xml	Shuxia Liu;Feng Zhang;Zhengli Zhai	2014	2014 IEEE World Congress on Services	10.1109/SERVICES.2014.39	data modeling;named graph;turtle;computer science;sparql;data mining;database;rdf query language;services computing;world wide web;rdf schema	DB	-38.31112564216086	7.124530374070532	52204
72538b285042f17534e9abdf522f26ee98ae9ca5	extending obdd graphs for composite event matching in content-based pub/sub systems	multisemantic matching;difference operator;constraint optimization;distributed independent application systems;information science;discrete events;large scale system;software engineering;pattern matching subscriptions tree graphs impedance matching constraint optimization software engineering information science xml;content based publish subscribe systems;tree graphs;temporal constraints;temporal sequence;binary decision diagrams;content based pub sub systems;transformation equivalence;pattern matching;impedance matching;publish subscribe;content based retrieval binary decision diagrams graph colouring middleware;xml;temporal constraint variable;subscriptions;middleware;coloured obdd graphs;semantic matching;content based retrieval;composite event matching;discrete event;matching model;transformation equivalence composite event matching content based pub sub systems content based publish subscribe systems large scale system distributed independent application systems multisemantic matching temporal constraint variable discrete events temporal sequence coloured obdd graphs;graph colouring	"""Content-based publish/subscribe offers a convenient abstraction for the information producers and consumers, supporting a large-scale system design and evolution by integrating several distributed independent application systems. Unlike in the traditional address-based unicast or multicast, its core problem is how to match events by predicates on the content of events. In existing matching approaches, matching predicates are composed by the conjunction and disjunction of non-semantic constraints. But, in context of enterprise application integration, although they can match events by their contents, this traditional matching predicates are not enough expressive in manipulating the complex event matching, such as the """"one-to-many"""" and """"many-to-one"""" matching. Therefore, traditional matching approaches should be extended to solve the complex matching problems. After analyzing information matching patterns in enterprise application integration, we propose three matching models, extend the simple matching to the multi-semantic matching and introduce the temporal constraint variable. The multi-semantic matching allows using different operations in accordance with different semantics; the temporal constraint variable supports processing the discrete events in the temporal sequence. Then, we extend OBDD graphs into hierarchy coloured OBDD graphs and prove the equivalence of the transformation. Based on the extended OBDD graphs, the composite matching algorithm is presented and analysed. By experiments, we show the proposed algorithm is efficient"""	algorithm;binary decision diagram;enterprise application integration;enterprise software;experiment;multicast;one-to-many (data model);publish–subscribe pattern;semantic matching;systems design;turing completeness;unicast	Gang Xu;Wei Xu;Tao Huang	2005	The 4th International Symposium on Parallel and Distributed Computing (ISPDC'05)	10.1109/ICIS.2005.63	discrete mathematics;computer science;theoretical computer science;distributed computing	DB	-46.16226596554632	16.74984112418329	52356
e1c04f7b165f73698a1dc6a1b499a4c264de73b8	characterization of 3d shape parts for semantic annotation	semantic annotation;surface mesh;segmentation;product design;ontology;virtual worlds;knowledge base	3D content stored in big databases or shared on the Internet is a precious resource for several applications, but unfortunately it risks to be underexploited due to the difficulty of retrieving it efficiently. In this paper we describe a system called the ”ShapeAnnotator” through which it is possible to perform non-trivial segmentations of 3D surface meshes and annotate the detected parts through concepts expressed by an ontology. Each part is connected to an instance that can be stored in a knowledge base to ease the retrieval process based on semantics. Through an intuitive interface, users create such instances by simply selecting proper classes in the ontology; attributes and relations with other instances can be computed automatically based on a customizable analysis of the underlying topology and geometry of the parts. We show how our part-based annotation framework can be used in two scenarios, namely for the creation of avatars in emerging Internet-based virtual worlds, and for product design in e-manufacturing.	database;focus;feature model;internet;k-3d;knowledge base;online and offline;ontology (information science);polygon mesh;semantic reasoner;semantics (computer science);virtual world	Marco Attene;Francesco Robbiano;Michela Spagnuolo;Bianca Falcidieno	2009	Computer-Aided Design	10.1016/j.cad.2009.01.003	knowledge base;computer science;ontology;data mining;database;product design;segmentation;world wide web	AI	-40.520433850633864	9.480375100030589	52546
14118f58c37f6ef0e8b8af3f84af1225cf94a5b3	export data from i2b2 using the new download data web client plugin.				Nich Wattanasin;Taowei David Wang;Bhaswati Ghosh;Reeta Metta;Vivian S. Gainer;Shawn N. Murphy	2016			database;internet privacy;world wide web	HCI	-40.20594554348946	4.5461626561190425	52722
66127d3a17430dcf9c0e18df4a71747a64de4684	towards an approach for enhancing web services discovery	web service discovery;xml ontologies artificial intelligence pattern matching web services;xml web services discovery ontology pattern matching;web service;satisfiability;ontologies artificial intelligence;pattern matching;web services;xml;web services discovery;web services application software web and internet services stability ontologies web sites xml logic availability resists;ontology	This paper discusses the added-value of combining users' preferences and Web services' capacities during the process of discovering the Web services that permit satisfying users' needs. The needs, preferences, and capacities vary over time, which requires tracking them using contextual details. Examples of needs include hotel booking and loan application. Examples of preferences include time of result delivery and interaction means. Examples of capacities include operations to perform at a certain time/place and non-functional characteristics of operations. In this paper, bringing Web services and users together is supported by an approach that develops respective ontologies for preferences and capacities, represents these latter with SAWSDL, and finally, matches them using a dedicated algorithm.	algorithm;business process;ontology (information science);sawsdl;web services discovery;web service;world wide web	Nomane Ould Ahmed M'Bareck;Samir Tata;Zakaria Maamar	2007	16th IEEE International Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE 2007)	10.1109/WETICE.2007.180	web service;web application security;web development;web modeling;data web;web mapping;web design;web standards;computer science;ws-policy;service-oriented architecture;web navigation;ontology;social semantic web;web page;data mining;ws-addressing;database;services computing;web intelligence;ws-i basic profile;web 2.0;law;world wide web;devices profile for web services	DB	-44.74786653981402	13.63654953117726	53056
1556bb9599b0c5a3b1c3135bb441e3a8313dfa4d	linear scale semantic mining algorithms in microsoft sql server's semantic platform	semantic platform;linear scale;incremental;user experience;indexation;gap analysis;keyword extraction;semantic mining;semantic relations;similarity index;document similarity	This paper describes three linear scale, incremental, and fully automatic semantic mining algorithms that are at the foundation of the new Semantic Platform being released in the next version of SQL Server. The target workload is large (10 -- 100 million) Enterprise document corpuses. At these scales, anything short of linear scale and incremental is costly to deploy. These three algorithms give rise to three weighted physical indexes: Tag Index (top keywords in each document); Document Similarity Index (top closely related documents given any document); and Semantic Phrase Similarity Index (top semantically related phrases, given any phrase), which are then query-able through the SQL interface. The need for specifically creating these three indexes was motivated by observing typical stages of document research, and gap analysis, given current tools and technology at the Enterprise. We describe the mining algorithms and architecture, and also outline some compelling user experiences that are enabled by the indexes.	algorithm;gap analysis;language model;linear scale;microsoft sql server;semantic similarity;server (computing);text corpus;universal quantification;web page	Kunal Mukerjee;Todd Porter;Sorin Gherman	2011		10.1145/2020408.2020447	semantic similarity;semantic computing;user experience design;linear scale;computer science;data mining;semantic web stack;database;information retrieval	Web+IR	-34.18139721418561	5.717065693018083	53259
32c54e1e3a4e388fd4d0e96f4b26219f8fd0eb0a	a heuristic approach for converting html documents to xml documents	document structure;estructura de documento;structure document;heuristic programming;logical programming;programmation logique;traitement document;programmation heuristique;xml document;document processing;information system;programacion logica;systeme information;tratamiento documento;sistema informacion	XML is rapidly emerging, and yet there still exist numerous HTML documents on the Web. In this paper, we present a heuristic approach for converting HTML documents to XML documents. During the conversion process, we eliminate all the HTML elements in an HTML document from the resulting XML document since these elements are designed for the display of data exclusively, but retain the character data of each element along with the implicit hierarchy among the data. The proposed conversion approach extracts the data hierarchy of HTML documents as closely as possible with no human intervention. The approach can be adopted to construct the data hierarchy of an HTML document and to collect data in HTML documents into an XML repository.	html;heuristic;xml	Seung Jin Lim;Yiu-Kai Ng	2000		10.1007/3-540-44957-4_79	well-formed document;xml catalog;xml validation;simple api for xml;xml;document processing;xml schema;computer science;document type definition;document structure description;xml framework;document type declaration;character encodings in html;xml database;xml schema;database;programming language;html element;world wide web;xml schema editor;information retrieval;information system;efficient xml interchange;sgml	DB	-36.246046934377496	10.727642906435248	53330
49b3c718238e399f85eddf6379e64bc0762be282	sissvoc: a linked data api for access to skos vocabularies		The Spatial Information Services Stack Vocabulary Service (SISSVoc) is a Linked Data API for accessing published vocabularies. SISSVoc provides a RESTful interface via a set of URI patterns that are aligned with SKOS. These provide a standard web interface for any vocabulary which uses SKOS classes and properties. The SISSVoc implementation provides web pages for human users, and machine-readable resources for client applications (in RDF, JSON, and XML). SISSVoc is implemented using a Linked Data API façade over a SPARQL endpoint. This approach streamlines the configuration of content negotiation, styling, query construction and dispatching. SISSVoc is being used in a number of projects, mainly in the environmental sciences, where controlled vocabularies are used to support cross-domain and interdisciplinary interoperability. SISSVoc simplifies access to vocabularies for end users, and provides a web API to support vocabulary applications.	application programming interface;communication endpoint;content negotiation;controlled vocabulary;debugging;human-readable medium;hypertext transfer protocol;interoperability;json;linked data;out of the box (feature);representational state transfer;resource description framework;sparql;semantic web;simple knowledge organization system;software deployment;the australian;triplestore;uniform resource identifier;user interface;web api;web page;xml	Simon J. D. Cox;Jonathan Yu;Terry Rankine	2016	Semantic Web	10.3233/SW-140166	computer science;simple knowledge organization system;data mining;database;world wide web	Web+IR	-41.276858075682824	4.423813452258448	53337
f596dae45b113df82abf23f800ab67ef4d1605fe	rt-mlr: a hybrid framework for context-aware systems	context reasoning;context representation;information fusion	This work presents a hybrid framework solution to describe context and develop context-aware systems. The framework intends to provide best domain knowledge expressiveness combining First Order Logic, Temporal Logic, and Fuzzy Logic. The framework engine was developed to be used in Object-Oriented systems, integrating the domain rules and system objects using an event-based architecture. The framework has been exemplified based on its capability to perform information fusion based on a heterogeneous context definition.	context-aware pervasive systems;html5 in mobile devices;learning to rank;windows rt	Pablo Rangel;José G. de Carvalho;Milton Ramos Ramirez;Jano Moreira de Souza	2011		10.1007/978-3-642-21222-2_12	computer science;knowledge management;machine learning;data mining	HCI	-42.13055399029797	11.196442544883201	53482
eeb746864979e39c1bfc21e71539769f62169175	a semantic qos-aware discovery framework for web services	semantic web service;web services ontologies semantic web availability laboratories service oriented architecture automation quality of service diversity reception prototypes;availability;prototypes;service oriented architectures;web service;ontologies artificial intelligence;semantic web services;ontology semantic qos aware discovery framework semantic web services service oriented architectures constraint programming semantic matchmaking;cognition;semantic matchmaking;web services;constraint programming;semantic web;constraint handling;ontologies;quality of service;service oriented architecture;semantic qos aware discovery framework;web services constraint handling ontologies artificial intelligence quality of service semantic web;programming;ontology	Augmenting web services with explicit semantics forms the foundation of Service Oriented Architectures (SOAs) automation. As more and more Semantic Web Services (SWSs) are deployed, similar SWSs could have quite different quality-of-service (QoS) levels. The QoS-aware discovery becomes an important challenge. While some efforts try to solve it via Constraint Programming (CP), they suffer from the purely syntactic matchmaking method. Furthermore, the construction of constraints and the selection of services are completely dependent on the literal translation from QoS descriptions, which increase obstacles to actually apply CP. In this paper, we propose a semantic QoS-aware framework for SWSs discovery by combining the semantic matchmaking and CP. Initially, a QoS ontology is presented to define QoS data into service descriptions. Then the ontology reasoning is adopted to change previous syntactic matchmaking into a semantic way. Through confirming the compatibility of concepts, complex QoS conditions are solved as constraints and a selection algorithm is proposed to obtain the optimal offer. Finally, the prototype implementation of our framework is discussed and a SWSs discovery case is used to illustrate the comprehensive discovery process.	constraint programming;literal (mathematical logic);prototype;quality of service;selection algorithm;semantic web service;service-oriented architecture	Qian Ma;Hao Wang;Ying Li;Guo Tong Xie;Feng Liu	2008	2008 IEEE International Conference on Web Services	10.1109/ICWS.2008.44	web service;computer science;service-oriented architecture;ontology;data mining;database;law;world wide web	Robotics	-44.62638114920384	14.441087385634193	53512
f25e70ccd1f4fd7b19bc26a9813edc1227f967d9	web-based document management for specialised domains: a preliminary evaluation	web documents;gestion document;management system;adquisicion del conocimiento;red www;gestion documento;ingenierie connaissances;reseau web;acquisition connaissances;internet;knowledge acquisition;world wide web;systeme gestion base donnee;sistema gestion base datos;database management system;knowledge engineering;document management	A Web document management system has been developed aimed at small communities in specialised domains and based on free annotation of documents by users. Knowledge acquisition support includes suggesting terms from external ontologies. Preliminary evaluation in a domain of research topics in Computer Science supports the utility of the approach, The most interesting result suggests that although an established external taxonomy can be useful in proposing annotation terms, users appear to be very selective in their use of terms proposed.		Mihye Kim;Paul Compton	2002		10.1007/3-540-45810-7_7	the internet;computer science;artificial intelligence;knowledge engineering;document management system;data mining;management system;database;world wide web	NLP	-37.911333170060736	11.030561650749652	53604
1d742ac0c9be04f209000eb65f02a6675cb5b949	integrating semantic web reasoning into learning object metadata	learning object metadata;semantic web	One of important functions of Learning Object Metadata (LOM) is to associate XML-based metadata with learning objects. The inherent problem of LOM is that it’s XML specified, which emphasizes syntax and format rather than semantic and knowledge representation. Hence, it lacks the semantic metadata to provide reasoning and inference functions. These functions are necessary for the computer-interpretable descriptions that are critical in the reusability and interoperability of the distributed learning objects. This paper aims at addressing this shortage, and proposes a multi-layered semantic framework to allow the reasoning and inference capabilities to be added to the conventional LOM. To illustrate how this framework work, we developed a Semantic-based Learning Objects Annotations Repository (SLOAR) that offers three different approaches to locate relevant learning objects for an e-learning application LOM-based metadata, ontology-based reasoning, and rule-based inference.	accessibility;embedded system;high- and low-level;interoperability;knowledge base;knowledge representation and reasoning;learning object metadata;logic programming;personalization;prototype;ruleml;semantic web stack;uniform resource identifier;user (computing);web ontology language;xml base	Shang-Juh Kao;I-Ching Hsu	2006			semantic computing;semantic web rule language;data web;semantic grid;web standards;computer science;artificial intelligence;semantic web;social semantic web;semantic web stack;database;semantic equivalence;metadata;world wide web;meta data services;information retrieval;semantic analytics;data mapping;metadata repository	AI	-40.77616354923782	7.4246780719459755	53632
798603b78390d5ce726ea2a53af6342959566e1a	migrating a hierarchical legacy database application onto an xml-based service-oriented web platform	modelizacion;base integrada dato;legacy software;critical study;service orientation;xml language;database;service web;base dato;base de donnees integree;tree data structures;web service;etude critique;orientado servicio;structure donnee arborescente;estudio critico;modelisation;logiciel patrimonial;electronic warfare;logicial herencia;tree structure;base de donnees;native xml database;integrated database;guerre electronique;oriente service;guerra electronica;modeling;langage xml;lenguaje xml;evaluation model;servicio web;service oriented	We present a case study where a mission-critical legacy hierarchical database, EWIRDB (Electronic Warfare Integrated Reprogramming Database), and its application software are migrated onto a service-oriented web architecture that is based on an XML-based database. EWIRDB stores its data in an extensible tree structure and serves many purposes of supporting EW systems reprogramming, research, development, test, and evaluation; modeling, and simulation, acquisition; and training. We present the historical stages of this migration that helped us to understand the issues and converge to the most appropriate solution eventually. We conclude that recently emerging web service technologies together with native XML database support meet the unique migration requirements of the EWIRDB and its legacy database application. We believe that the proposed solution addresses an almost 20-year old problem in the EW domain problem and forms an appropriate base for porting some other applications with similar requirements in other domains.	service-oriented device architecture;xml	Özgür Yürekten;Kivanç Dinçer;Berk Akar;Müberra Sungur;Elif Kurtaran Özbudak	2006		10.1007/11902140_68	web service;xml;systems modeling;computer science;operating system;data mining;database;tree structure;tree;world wide web;database schema;computer security;legacy system;electronic warfare	DB	-35.798214861630555	13.037578255894955	54205
03c8f94e6c6076465a7e92e3b90b895e96b11e24	meteor-s wsdi: a scalable p2p infrastructure of registries for semantic publication and discovery of web services	web service discovery;service provider;meteor s project;bepress selected works;ontology p2p uddi semantic web services publication semantic annotation of web services semantic web services semantic web services discovery meteor s project peer to peer domain based registry;domain based registry;distributed computing;p2p;web service;semantic publication;semantic web services;uddi;large scale distributed systems;peer to peer computing;peer to peer;semantic discovery;high performance;ontology;semantic annotation of web services;semantic web services discovery;semantic web services publication	Web services are the new paradigm for distributed computing. They have much to offer towards interoperability of applications and integration of large scale distributed systems. To make Web services accessible to users, service providers use Web service registries to publish them. Current infrastructure of registries requires replication of all Web service publications in all Universal Business Registries (UBR) which provide text and taxonomy based search capabilities. Large growth in number of Web services as well as the growth in the number of registries would make this replication impractical. In addition, the current Web service discovery mechanism is inefficient, as it does not support discovery based on the capability of the services and thus leading to a lot of irrelevant matches. Semantic discovery or matching of services is a promising approach to address this challenge. In this paper, we present a scalable, high performance environment for federated Web service publication and discovery among multiple registries. This work uses an ontology-based approach to organize registries, enabling semantic classification of all Web services based on domains. Each of these registries supports semantic publication of the Web services, which is used during discovery process. We have implemented two algorithms each for semantic publication and one algorithm for semantic discovery of Web services. We believe that the semantic approach suggested in this paper will significantly improve Web services publication and discovery involving a large number of registries. As a part of the METEOR-S project, we have leveraged the peer-to-peer networking as a scalable infrastructure for registries that can support automated and semi-automated Web service publication and discovery.	algorithm;artificial intelligence;black box;book;distributed computing;input/output;interaction;interoperability;java web services development pack;meteor;online marketplace;ontology (information science);peer-to-peer;programming paradigm;prototype;relevance;scalability;search algorithm;semiconductor industry;service discovery;web services description language;web services discovery;web service;world wide web	Kunal Verma;Kaarthik Sivashanmugam;Amit P. Sheth;Abhijit A. Patil;Swapna A. Oundhakar;John A. Miller	2003	Information Technology and Management	10.1007/s10799-004-7773-4	service provider;web service;web application security;web development;web modeling;data web;web mapping;web standards;computer science;ws-policy;semantic web;ontology;social semantic web;peer-to-peer;database;service discovery;services computing;web intelligence;ws-i basic profile;web 2.0;law;world wide web;devices profile for web services;information retrieval;universal description discovery and integration	Web+IR	-41.08885872897683	8.011083587720204	54211
498d069e6c932e824cba2e6f5576fc2a1cdb5dec	emit: a process mining tool	commerce electronique;groupware;banking;systeme intelligent;comercio electronico;traduccion automatica;logistique;red petri;xml language;sistema inteligente;layout problem;import;probleme agencement;secteur bancaire;data mining;process mining;importation;traduction automatique;importacion;sante;logistics;fouille donnee;intelligent system;problema disposicion;workflow;health;salud;petri net;collecticiel;busca dato;langage xml;lenguaje xml;reseau petri;electronic trade;logistica;automatic translation	Process mining offers a way to distill process models from event logs originating from transactional systems in logistics, banking, e-business, health-care, etc. The algorithms used for process mining are complex and in practise large logs are needed to derive a high-quality process model. To support these efforts, the process mining tool EMiT has been built. EMiT is a tool that imports event logs using a standard XML format as input. Using an extended version of the α-algorithm [3, 8] it can discover the underlying process model and represent it in terms of a Petri net. This Petri net is then visualized by the program, automatically generating a “smart” layout of the model. To support the practical application of the tool, various adapters have been developed that allow for the translation of system-specific logs to the standard XML format. As a running example, we use an event log generated by the workflow management system Staffware.	algorithm;electronic business;logistics;petri net;process modeling;xml	Boudewijn F. van Dongen;Wil M. P. van der Aalst	2004		10.1007/978-3-540-27793-4_26	logistics;workflow;computer science;artificial intelligence;operating system;data mining;database;health;process mining;world wide web;computer security;petri net;algorithm	ML	-37.50329051673068	13.192331031894838	54481
1835474d1d739a3aa77fc3dd1baea682f3910dc0	are we moving toward an information superhighway or a tower of babel? the challenge of large-scale semantic heterogeneity	information sources;poles and towers large scale systems information resources context aware services mediation technology management testing layout acoustic noise dictionaries;information services;semantic heterogeneity;large scale;data semantics;information networks;technical presentation;data quality;evolving quality information superhighway large scale semantic heterogeneity information sources context interchange implicit assumptions context definition meaning context characteristics information quality context mediation services data semantics acquisition data quality attributes evolving semantics;technical presentation information networks information services;working paper	"""The popularity and growth of the """"Information SuperHighway"""" have dramatically increased the number of information sources available for use. Unfortunately, there are significant challenges to be overcome. One particular problem is context interchange, whereby each source of information and potential receiver of that information may operate with a different context, leading to largescale semantic heterogeneity. A context is the collection of implicit assumptions about the context definition (i.e., meaning) and context characteristics (i.e., quality) of the information. This paper describes various forms of context challenges and examples of potential context mediation services, such as data semantics acquisition, data quality attributes, and evolving semantics and quality, that can mitigate the problem. Ba'bel n. 1. In the Old Testament, the site of a tower reaching to heaven whose construction was interrupted by the confusion of tongues. 2a. A confusion of sounds and voices. 2b. A scene of noise and confusion. [The American Heritage Dictionary]."""	data quality;dictionary;information source;information superhighway;interrupt;list of system quality attributes;semantic data model;semantic heterogeneity;testament;tower of babel	Stuart E. Madnick	1996		10.1109/ICDE.1996.492083	semantic integration;data quality;computer science;data mining;database;context model;information quality;world wide web;information system	DB	-46.25480519877038	8.603137201594304	54904
2dfe34c9ac4b1596798d05931f794797d3976d6f	a kqml-corba based architecture for intelligent agents communication in cooperative service and network management	network management;intelligent agents communication;cooperative service;service and network management;corba.;intelligent agents;kqml;cooperative and distributed management;service management;intelligent agent	Networks and Services management solutions are now m ving from centralised platforms to distributed approaches. In order to re ach a higher level of scalability and flexibility, an attractive solution can be base d on cooperative intelligent agents. This raises new requirements for communication betw e n management entities, as it implies not only exchange of pure management inf ormation but also exchange of cooperation messages or code. After analysing these communication requirements, t hi paper outlines the need for a communication language able to support all the ty p s of interaction between the agents. As a candidate, KQML is then presented. Its powerful semantic characteristics made us believe it is a suitable su pport for the communication between management intelligent agents. An implement atio of KQML on CORBA leads to define a complete communication layer arch ite ture that can support cooperation in a multi-agents management platform.	automated clearing house;centralisation;common object request broker architecture;entity;intelligent agent;knowledge query and manipulation language;requirement;scalability	Dominique Bénech;Thierry Desprats;Yves Raynaud	1997			network management;agent architecture;fcaps;element management system;systems management;intelligent computer network;network architecture;network management station;knowledge management;network management application;structure of management information;customer service assurance;intelligent agent;computer network	DB	-42.02090849859804	17.40184391262764	54973
e84cd063a9f75e5f41ccc45e00210e6670da9fc8	dynamic profiling for efficiency searching system in distributed computing	information extraction;information infrastructure;distributed computing;rfid tag;real time data;dynamic linking;user profile;distributed computing environment	  RFID technology that identifies objects on request of dynamic linking and tracking is composed of application components supporting  information infrastructure. Despite their many advantages, existing applications, which do not consider elements related to  real-time data communication among remote devices, cannot support connections among heterogeneous devices effectively. As  different network devices are installed in applications separately and go through different query analysis processes, there  happen the delays of monitoring or errors in data conversion. This paper proposes recommendation service that can update and  reflect personalized profiles dynamically in Distributed Computing environment for integrated management of information extracted  from RFID tags regardless of application. The advanced personalized module helps the service recommendation server make regular  synchronization with the personalized profile. The proposed system can speed and easily extend the matching of services to  user profiles and matching between user profiles or between services. Finally dynamic profiling help to reduce the development  investment, improve the system’s reliability, make progress in the standardization of real-time data processing in matching  searching system.    	distributed computing	Chang-Woo Song;Tae-Gan Kim;Kyung-Yong Chung;Kee-Wook Rim;Jung-Hyun Lee	2010		10.1007/978-3-642-17569-5_51	distributed algorithm;computer science;database;distributed computing;utility computing;distributed design patterns;world wide web;autonomic computing;distributed concurrency control	HPC	-36.435027181024296	14.931999228288598	55094
cf6c96b73a565e745da94e636de521edc4b66c84	automatic generation of user-defined virtual documents using query and layout templates		DelaunayMM is an authoring, querying, and visualization framework for multimedia information retrieved from distributed repositories including the Web. Users compose virtual documents by specifying visually templates that contain both layout information and query specification. The query engine generates a set of similarly laid out pages for each template, whose number depends both on the number of retrieved objects that satisfy the queries and on the layout specification. In this paper we focus on the object-oriented data models, on the declarative query languages, and on the automatic generation of the virtual documents. Our framework distinguishes itself from others in that it combines presentation with querying, in particular by allowing for the customization of the layout by users and for the retrieval of the distributed information using database object-oriented technology. © 1998 John Wiley u0026 Sons, Inc.	dynamic web page	Isabel F. Cruz;Wendy T. Lucas	1998	TAPOS		web query classification;computer science;database;programming language;world wide web;information retrieval;query language	Visualization	-35.953700294252585	8.88698810551537	55299
1ef1f0d0046db4f31be3139ccc82f22a8677fe7c	workflow streams: a means for compositional adaptation in process-oriented cbr		This paper presents a novel approach to compositional adap- tation of workflows, thus addressing the adaptation step in process- oriented case-based reasoning. Unlike previous approaches to adaptation, the proposed approach does not require additional adaptation knowledge. Instead, the available case base of workflows is analyzed and each case is decomposed into meaningful subcomponents, called workflow streams. During adaptation, deficiencies in the retrieved case are incrementally compensated by replacing fragments of the retrieved case by appropri- ate workflow streams. An empirical evaluation in the domain of cooking workflows demonstrates the feasibility of the approach and shows that the quality of adapted cases is very close to the quality of the original cases in the case base.		Gilbert Müller;Ralph Bergmann	2014		10.1007/978-3-319-11209-1_23	computer science;data mining;database;world wide web	ML	-41.03968791863479	11.387102852092912	55431
b54af1dcf2991b6799596afbdd8c14e45833d2da	self-orchestration and choreography: towards architecture-agnostic manufacturing systems	semantic web service;manufacturing systems;mass customization;web service;ontologies artificial intelligence;manufacturing systems web services servomechanisms semantic web production facilities manufacturing processes grippers mass customization logic devices service oriented architecture;humanities;semantic web;centralized system selforchestration choreography architecture agnostic manufacturing system device level web service semantic web service automatic service discovery;service discovery;manufacturing system;semantic web humanities manufacturing systems ontologies artificial intelligence	One of the significant challenges for current and future manufacturing systems is that of providing rapid reconfigurability in order to evolve and adapt to mass customization. This challenge is aggravated if new types of processes and components are introduced, as existing components are expected to interact with the novel entities but have no previous knowledge on how to collaborate. This paper reviews the concepts of orchestration and choreography applied to device-level Web services, and proposes the use of semantic Web Services in order to overcome the aforementioned challenge. The capabilities of semantic Web services for performing automatic service discovery, selection, composition and invocation enable manufacturing systems to self-orchestrate without need for manual configuration, and without need for concentrating logic in centralized systems.	centralized computing;entity;reconfigurability;semantic web service;service discovery	Ivan M. Delamer;José L. Martínez Lastra	2006	20th International Conference on Advanced Information Networking and Applications - Volume 1 (AINA'06)	10.1109/AINA.2006.301	web service;web application security;web development;web modeling;business process execution language;data web;web design;web standards;mass customization;computer science;knowledge management;operating system;ws-policy;semantic web;social semantic web;semantic web stack;database;service discovery;web intelligence;web 2.0;law;world wide web;semantic analytics	Robotics	-44.12039159148901	12.875507968704515	55734
c1f1776e49a1cc2f2625aa0c22d78a9275513b77	"""representation and exchange of """"knowledge cards"""": a pen design case study"""		Organizations consider more and more the collaborative aspects to exchange information and knowledge, in complete and correct way, among different systems. These organizations involve many specialists from different domains, with wide-ranging knowledge. Consequently, knowledge is now regarded as a strategic asset which must be managed, with an increasing need for representation formalisms and deployment of tools for knowledge exchange. This paper introduces a contribution to this area giving greater place to the user’s point of view. The knowledge is shared with “knowledge cards” based on standards and simple formalisms to ensure the user’s adoption and the portability of knowledge exchange. UML formalism as well as XML technology have been chosen for their skills to meet the needs of interoperability and their capacity to be treated by users and computers. An application on design knowledge of a pen, resulting from a CYGMA “knowledge book”, was carried out in order to illustrate and to validate our choices.	computer;human-readable medium;interoperability;kad network;point of view (computer hardware company);resource description framework;semantic web;semantics (computer science);software deployment;software portability;unified modeling language;web ontology language;world wide web;xml	Laurent Buzon;Abdelaziz Bouras;Yacine Ouzrout	2003	IJEBM		knowledge base;computer science;knowledge management;body of knowledge;knowledge-based systems;open knowledge base connectivity;data mining;management science;procedural knowledge;knowledge extraction;personal knowledge management;knowledge value chain;domain knowledge	AI	-46.875094205350884	6.235376220706962	55867
a61dae5815d8c770aab437234d251961920e8c33	a database-oriented wrapper for ubiquitous data acquisition/access environments	distributed system;information retrieval;relational database;reputation system;system design;social search;multimedia database;data acquisition	Ubiquitous data acquisition/access environments are dynamic because the number and kinds of devices used to access information are constantly changing. System designers must often identify and incorporate devices and implement ad-hoc application systems based on the environment to execute tasks. The constant change in the environment and the huge number of devices make it difficult and time-consuming for system designers to construct application systems using existing programming styles. We propose a system construction platform based on a relational database mechanism. Devices and the functions are regarded as tables, tuples, and SQL queries. This platform makes it easy to manage huge numbers of devices in a ubiquitous data acquisition/access environment and provides a method of ad-hoc system construction that is accessible to many devices and can accommodate their functions.	data acquisition;hoc (programming language);relational database;sql;table (database)	Yuhei Akahoshi;Yutaka Kidawara;Katsumi Tanaka	2008		10.1145/1352793.1352800	embedded system;relational database;computer science;operating system;data mining;database;data acquisition;world wide web;computer security;systems design	HCI	-36.473668910693384	9.084439453330763	55931
ccd000e90a766d11fd84653cfb89a355218c851e	write-once run-anywhere custom sparql functions	extension function;standards;web of functions;java engines urban areas standards interoperability conferences;query languages public domain software;urban areas;engines;web of functions sparql extension function;sparql;interoperability;conferences;sesame write once run anywhere custom sparql functions sparql engines opensource engines apache jena fuseki openlink virtuoso;java	We show a framework to write SPARQL custom functions that can run on different SPARQL engines. Our current implementation supports some of the major opensource engines, namely Apache Jena/Fuseki, OpenLink Virtuoso and Sesame. In our experiments we show the performance in terms of running time and the ease of building custom functions by using our write-once run-anywhere framework.	experiment;open-source software;sparql;sesame;time complexity;virtuoso universal server;write once, run anywhere	Daniele Stefano Ferru;Maurizio Atzori	2016	2016 IEEE Tenth International Conference on Semantic Computing (ICSC)	10.1109/ICSC.2016.64	interoperability;named graph;telecommunications;computer science;sparql;data mining;database;programming language;java;world wide web	SE	-34.493223639749594	8.189200181351834	56524
440ab62f30324eec2b8d3093a03aeb401041343e	bpel similarity - a metric based on activity constraint graphs	会议论文	As the increasing popularity of Web Service Business Process Ex- ecution Language (WS-BPEL), it is urgent to meet the demand of retrieving the related BPEL processes in BPEL process repository quickly for business per- sonnel. BPEL similarity retrieval technology is one of research focuses in the field of BPEL repository management system. As existing approaches tend to lack metric features and use structural aspects of BPEL processes rather than their behaviors, they are often not applicable for effective similarity search. In this paper, we propose a metric based on BPEL activity constraint graphs (BACGs) to calculate the similarity degree of BPEL processes. It is grounded on the Jaccard coefficient and leverages behavioral relations between BPEL ac- tivities. The metric is successfully evaluated towards its approximation of hu- man similarity assessment.	business process execution language	Jianchun Xing;Xuewei Zhang;Wei Song;Qiliang Yang;Jidong Ge;Hongda Wang	2013		10.1007/978-3-319-02922-1_3	computer science;theoretical computer science;data mining;database	AI	-45.66117542500468	13.568632159118195	56587
356535faa60d7fcf7a45061984a1a9d8301a571b	magnet: supporting navigation in semistructured data environments	metadata;user interface;information retrieval;semistructured data;navigation;searching browsing;domain specificity	With the growing importance of systems containing arbitrary semi-structured relationships, the need for supporting users searching in such repositories has grown. Currently support for users' search needs either has required domain-specific user interfaces or has required users to be schema experts. We have developed a general-purpose tool that offers users helpful navigation and refinement options for seeking information in these semistructured repositories. We show how a tool can be built without requiring domain-specific assumptions about the information being explored. In addition to describing a general approach to the problem, we provide a set of natural, general-purpose refinement tactics, many generalized from past work on textual information retrieval.	domain-specific language;general-purpose modeling;information retrieval;refinement (computing);semiconductor industry;user interface	Vineet Sinha;David R. Karger	2005		10.1145/1066157.1066169	navigation;computer science;database;user interface;metadata;world wide web;information retrieval	DB	-34.37513306593023	4.3282011315209585	56612
d8703c1d3c96efbaa0af351d39f2d87617d0956b	applications of data mining in web services	extraction information;red www;analisis datos;information extraction;maintenance;publisher subscriber middleware;reseau web;service web;service process;web service;satisfiability;data mining;proceso servicio;data analysis;planificacion;processus service;fouille donnee;intergiciel editeur souscripteur;mantenimiento;world wide web;analyse donnee;planning;arquitectura publicacion suscripcion;information system;service discovery;planification;busca dato;extraccion informacion;systeme information;servicio web;sistema informacion	The success of Web services in business relies on the discovery of Web services satisfying the needs of the service requester. In this paper we discuss the use of data mining in the service discovery process. We recommend a set of applications that can leverage problems concerned with the planning, development and maintenance of Web services.	algorithm;automated planning and scheduling;cluster analysis;data mining;multi-user;norm (social);real-time transcription;scheduling (computing);sequence analysis;service discovery;time series;web service	Richi Nayak;Cindy Tong	2004		10.1007/978-3-540-30480-7_22	planning;web service;web application security;web mining;web development;web modeling;business process execution language;web analytics;web mapping;web standards;computer science;ws-policy;service-oriented architecture;data mining;ws-addressing;database;service discovery;services computing;web intelligence;web engineering;data analysis;ws-i basic profile;web 2.0;world wide web;devices profile for web services;information extraction;universal description discovery and integration;information system;satisfiability	ML	-37.894723359051014	13.40236085329516	56775
3b99cc5053bd3bbed00ad3ea5c3bfe7c609ea131	hshrex - a tool for design and evaluation of hybrid xml storage	databases;xml schema;complexity theory;storage model hshrex hybrid xml storage data representation format web applications unstructured data semi structured data structured data hybrid storage data model;xml data models application software relational databases hybrid power systems expert systems information science bioinformatics delay costs;technology;xml data models information storage relational databases software tools web services;teknikvetenskap;hybrid xml storage;data mining;data representation;web applications;data model;information storage;engineering and technology;semi structured data;teknik och teknologier;storage model;data representation format;web services;xml;hybrid storage;software tools;relational databases;native xml storage;xml shredding;use case;unstructured data;hshrex;conferences;hybrid xml storage xml native xml storage xml shredding;data models;structured data;bioinformatics	XML is a commonly used data representation format for web applications. One of the reasons for the attractiveness of XML is its flexibility to store unstructured, semi-structured and structured data.However, supporting this flexibility is challenging from a technical perspective and several approaches have been proposed for storage of XML. The focus of this paper is hybrid storage, combining relational and native solutions as it allows many alternatives for designing the data model. The paper presents our tool HShreX that enables quick design and evaluation of alternative choices by annotating the XML schema.The main benefit of the tool is that it is easy to work with and provides quick design of storage model and import of data. The paper describes the main features of the tool and a use case where we discuss how the tool can be used to design, compare and evaluate storage alternatives.	data (computing);data model;semiconductor industry;storage model;web application;xml schema	Lena Strömbäck;Mikael Asberg;David H Hall	2009	2009 20th International Workshop on Database and Expert Systems Application	10.1109/DEXA.2009.33	data model;computer science;xml framework;data mining;database;world wide web;information retrieval	Web+IR	-33.83634288753746	6.5369691493798205	56963
c8f6c2e392e21d89a1da7670234e0d78c611e61b	an enhanced extract-transform-load system for migrating data in telecom billing	databases;manuals;domain knowledge extract transform load system telecom billing data migration schema mapping;industries;data migration;extract transform load system;data mining;ontologies artificial intelligence;domain knowledge;connectors;business data processing;business;ontologies artificial intelligence business data processing data mining;telecom billing;schema mapping;ontologies;extract transform load;data mining communication industry connectors ontologies thesauri telecommunication services laboratories computers real time systems content management	Data migration has become a priority in many industries, spawned by a variety of business needs. Most of the existing tools for Extract, Transform and Load (ETL) process of data migration are piece-meal and do not present a complete solution. Moreover, while research has focused on the problem of Schema Mapping, a key step in the ETL process, most of the current algorithms do not perform well on real-world data. Researchers have suggested the use of Domain Knowledge to enhance schema mapping. In this paper, we use domain knowledge in an innovative manner to improve schema mapping in an 'actual' industrial setting. Further, we take a comprehensive view of the data migration problem and present an end-to-end system for the ETL process, utilizing existing tools for each step and building connectors, wherever required. We focus on Data Migration for Telecom Billing and utilize domain knowledge captured in an ontology, a thesaurus and a set of rules to improve schema mapping. Experiments conducted on a real-life data demonstrate the effectiveness of our system and validate the utility of domain knowledge in data migration projects.	algorithm;business requirements;digital monetary trust;electronic billing;end system;end-to-end principle;experiment;real life;thesaurus	Himanshu Agrawal;Girish Chafle;Sunil Goyal;Sumit Mittal;Sougata Mukherjea	2008	2008 IEEE 24th International Conference on Data Engineering	10.1109/ICDE.2008.4497537	data migration;computer science;ontology;data mining;database;knowledge extraction;world wide web;domain knowledge	DB	-47.69434293627552	8.378475136690732	57044
3c484060ebee51322d11d133be3e10c1c7a64d6b	studying on the awareness model in ontological knowledge community	sensibilidad contexto;modelizacion;besoin de l utilisateur;ontologie;communaute repartie;context aware;red www;web community;routing;web semantique;exigence usager;reseau web;exigencia usuario;routage;semantics;necesidad usuario;intelligence artificielle;semantica;semantique;or phrases;user assistance;modelisation;assistance utilisateur;internet;user need;user requirement;web semantica;asistencia usuario;semantic web;artificial intelligence;world wide web;ontologia;inteligencia artificial;sensibilite contexte;modeling;ontology;comunidad repartido;enrutamiento	The efficiency of obtaining accurate knowledge in the WWW is becoming more important than ever. It is therefore a critical issue for the Web to precisely understand the semantic meaning of the words or phrases chosen by users as well as to accurately locate the user's requirements. This article focuses on two approaches, CCAA and LCAA, which can help the user acquire knowledge in ontological knowledge community of Semantic Web. It describes concepts and definitions about context-awareness as well as content-awareness, and then brings forward an awareness model in ontological knowledge routing. A proper instance running in this tentative model is also provided in this article.		Xueli Yu;Jingyu Sun;Baolu Gao;Ying Wang;Meiyu Pang;Libin An	2005		10.1007/11495772_71	routing;the internet;systems modeling;computer science;artificial intelligence;user requirements document;semantic web;ontology;data mining;database;semantics;world wide web	AI	-37.9007853131193	12.464471653455522	57083
019d9dcb9eec4fdb182127b422f710b21522036a	access control via lightweight ontologies	social network services;lightweight ontology;access control ontologies semantics permission social network services knowledge based systems cognition;authorisation;access control relbac lightweight ontology;semantics;ontologies artificial intelligence;qa076 computer software;internet;permission;relbac;cognition;ontologies;access control;resource ontologies lightweight ontologies relation based access control complex file system web 2 0 application subject ontologies matching;ontologies artificial intelligence authorisation internet;knowledge based systems	The paper presents Relation Based Access Control RelBAC, a model and a logic for access control which models communities, possibly nested, and resources, possibly organized inside complex file systems, as lightweight ontologies, and permissions as relations between subjects and objects. RelBAC allows us to represent expressive access control rules beyond the current state of the art, and to deal with the strong dynamics of subjects, objects and permissions which arise in Web 2.0 applications (e.g. social networks). Finally, as shown in the paper, using RelBAC, it becomes possible to reason about access control policies and, in particular to compute candidate permissions by matching subject ontologies (representing their interests) with resource ontologies (describing their characteristics).	access control;lightweight ontology;ontology (information science);semantic matching;social network;web 2.0	Fausto Giunchiglia;Bruno Crispo;Rui Zhang	2011	2011 IEEE Fifth International Conference on Semantic Computing	10.1109/ICSC.2011.23	the internet;cognition;computer science;knowledge management;ontology;artificial intelligence;access control;data mining;database;semantics;authorization;world wide web	DB	-42.407556156933495	13.634237517346985	57084
9b5218f144b471f07514522961a3dbcb07125a28	a view-based methodology for collaborative ontology engineering: an approach for complex applications (vimethcoe)	development process;ontologies artificial intelligence;ontologies collaborative work biomedical engineering centralized control international collaboration space technology application software biology computing biomedical computing genomics;ontology engineering;modular development view based methodology collaborative ontology engineering	The development of very large ontologies may involve several domain experts, ontology engineers and final users, all with very different objectives. New scenarios like biomedicine, genomics and biology require methodologies that consider new dimensions in the development process, namely: dynamism, distribution and control. Existing works lack some important aspects with respect to these dimensions. In this paper we propose a view-based methodology for the collaborative, distributed and modular development of large ontologies, which achieves the identified requirements for the proper evolution and use of very large ontologies	ontology (information science);ontology engineering;requirement	Ernesto Jiménez-Ruiz;Rafael Berlanga Llavori	2006	15th IEEE International Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE'06)	10.1109/WETICE.2006.13	upper ontology;idef5;open biomedical ontologies;computer science;knowledge management;ontology;software engineering;data mining;ontology-based data integration;management;software development process;process ontology	Visualization	-43.87675009429799	6.273475179167022	57093
c5e9be87c0677a9f4627e6e094d93d4caad67f9d	a uml 2.0 profile to design association rule mining models in the multidimensional conceptual modeling of data warehouses	conceptual modeling;conceptual model;association rules;data mining;database management;association rule mining;conceptual design;uml profile;association rule;kdd;user requirements;multidimensional modeling;data warehouse;development time	By using data mining techniques, the data stored in a Data Warehouse (DW) can be analyzed for the purpose of uncovering and predicting hidden patterns within the data. So far, different approaches have been proposed to accomplish the conceptual design of DWs by following the multidimensional (MD) modeling paradigm. In previous work, we have proposed a UML profile for DWs enabling the specification of main MD properties at conceptual level. This paper presents a novel approach to integrating data mining models into multidimensional models in order to accomplish the conceptual design of DWs with Association Rules (AR). To this goal, we extend our previous work by providing another UML profile that allows us to specify Association Rules mining models for DW at conceptual level in a clear and expressive way. The main advantage of our proposal is that the Association Rules rely on the goals and user requirements of the Data Warehouse, instead of the traditional method of specifying Association Rules by considering only the final database implementation structures such as tables, rows or columns. In this way, ARs are specified in the early stages of a DW project, thus reducing the development time and cost. Finally, in order to show the benefits of our approach, we have implemented the specified Association Rules on a commercial database management server.		José Jacobo Zubcoff;Juan Trujillo	2007	Data Knowl. Eng.	10.1016/j.datak.2006.10.007	association rule learning;computer science;conceptual model;data science;applications of uml;data warehouse;data mining;database	DB	-34.68498581020006	13.308387171658959	57239
1c2caa4dc92f25386eefd0225fb4f0a00d684085	a refined goal model for semantic web services	semantic web service;web services semantic web;automated detection;service orientation;web service;dynamic detection;semantic web services;wsmo;web service modeling ontology refined goal model semantic web services service orientation dynamic detection automated detection wsmo;web service modeling ontology;web services;semantic web;refined goal model;semantic web web services service oriented architecture concrete runtime problem solving electronics packaging ontologies context aware services formal specifications	The idea of service orientation envisions dynamic detection and execution of suitable Web services for solving a particular request. Most realization approaches pay only little attention to the client side of such architectures. We therefore promote a goal-driven approach: a client merely specifies the objective to be achieved in terms of a goal, and the system resolves this by automated detection, composition, and execution of Web services. Extending the WSMO framework, we present a model for describing goals as formalized client objectives that carry all information relevant for automated detection and execution of Web services.	client-side;cognitive architecture;graphical user interface;metamodeling;owl-s;precondition;problem solving;semantic web service;service composability principle;service-orientation;wsmo	Michael Stollberg;Barry Norton	2007	Second International Conference on Internet and Web Applications and Services (ICIW'07)	10.1109/ICIW.2007.7	web service;web application security;web development;web modeling;data web;web mapping;web standards;computer science;knowledge management;ws-policy;semantic web;social semantic web;ws-addressing;semantic web stack;database;web intelligence;ws-i basic profile;web 2.0;law;world wide web;owl-s;web coverage service	SE	-43.748887463613606	14.448412424872128	57251
7022612364cdb0ddc0e9532245f9ee61dcca3640	towards using owl integrity constraints in ontology engineering		In the GOPSL ontology engineering methodology, integrity constraints are used to guide communities in constraining their domain knowledge. This paper presents our investigation on OWL integrity constraints and its usage in ontology engineering.	data integrity;ontology engineering;ptc integrity;web ontology language	Trung-Kien Tran;Christophe Debruyne	2012		10.1007/978-3-642-33618-8_37	database;owl-s;process ontology	Web+IR	-38.85891932969993	6.535223923311796	57356
851d5ebc7c1ca5d269cf8c1570d8205a4488ed7a	agile decision agent for service-oriented e-commerce systems	electronic commerce;human computer interaction;decision aid;users preferences;service orientation;e commerce;airports;agile decision agent;critiquing interaction agile decision agent service oriented e commerce systems users preferences soft constraint;soft constraints;psychology;software agents decision support systems electronic commerce;software agents;soft constraint;service oriented e commerce systems;decision support systems;web services;environmental economics;uct;xml;merging;artificial intelligence;critiquing interaction;web services human computer interaction decision making artificial intelligence merging xml environmental economics psychology airports;dynamic configuration	In a service-oriented e-commerce environment, it is a crucial task to help consumers choose desired products efficiently from a huge amount of dynamically configured product candidates. Decision agents can be designed to provide interactive decision aids for end-users by eliciting their preferences and then recommending matching products. In reality the users' preferences may keep changing along with the dynamic decision environment and may not be fully satisfied. As a result, the decision agent is required to be agile; it should allow decision making with an incomplete user's preference model and should afford users to add, retract or revise their various preferences with little effort. In this paper we propose the general design of an agile decision agent to meet this need. We model users' preferences with the soft constraint technique and elicit them by the example critiquing interaction paradigm	agile software development;constrained optimization;e-commerce;programming paradigm;service composability principle;service-orientation;service-oriented architecture;service-oriented device architecture;service-oriented infrastructure;service-oriented software engineering;web service	Jiyong Zhang;Pearl Pu;Boi Faltings	2006	The 8th IEEE International Conference on E-Commerce Technology and The 3rd IEEE International Conference on Enterprise Computing, E-Commerce, and E-Services (CEC/EEE'06)	10.1109/CEC-EEE.2006.11	e-commerce;web service;xml;decision analysis;decision field theory;computer science;knowledge management;artificial intelligence;software agent;business decision mapping	Robotics	-44.26077941929102	15.757146839386923	57427
709cba12dd061ed82614951e9c7af16a797ddab6	the event processing odp	datavetenskap datalogi;computer science	In this abstract we present a model for representing heterogeneous event objects in RDF, building on pre-existing work and focusing on structural aspects, which have not been addressed before, such as composite event objects encapsulating other event objects. The model extends the SSN and Event-F ontologies, and is available for download in the ODP portal.	dmz (computing);download;event (computing);ontology (information science);rm-odp;requirement;stream processing;subscriber identity module;vocabulary	Eva Blomqvist;Mikko Rinne	2013			real-time computing;computer science;database;world wide web	DB	-41.352377995006	10.046102060128376	57547
6da79b4bad4242c754c5521097db555afa4dafbb	dynamic query optimization approach for semantic database grid	distributed database;tool support;query processing;query optimization;relational database;distributed database system;dynamic query;resource sharing;semantic grid;problem solving	Fundamentally, semantic grid database is about bringing globally distributed databases together in order to coordinate resource sharing and problem solving in which information is given well-defined meaning, and DartGrid II is the implemented database gird system whose goal is to provide a semantic solution for integrating database resources on the Web. Although many algorithms have been proposed for optimizing query-processing in order to minimize costs and/or response time, associated with obtaining the answer to query in a distributed database system, database grid query optimization problem is fundamentally different from traditional distributed query optimization. These differences are shown to be the consequences of autonomy and heterogeneity of database nodes in database grid. Therefore, more challenges have arisen for query optimization in database grid than traditional distributed database. Following this observation, the design of a query optimizer in DartGrid II is presented, and a heuristic, dynamic and parallel query optimization approach to processing query in database grid is proposed. A set of semantic tools supporting relational database integration and semantic-based information browsing has also been implemented to realize the above vision.	algorithm;autonomy;distributed database;heterogeneous database system;heuristic;mathematical optimization;optimization problem;problem solving;query optimization;relational database;response time (technology);semantic grid;world wide web	Xiaoqing Zheng;Huajun Chen;Zhaohui Wu;Yuxin Mao	2006	Journal of Computer Science and Technology	10.1007/s11390-006-0597-4	materialized view;shared resource;online aggregation;sargable;query optimization;database theory;query expansion;web query classification;database tuning;semantic grid;relational database;computer science;query by example;database model;data mining;database;web search query;view;database schema;distributed database;graph database;information retrieval;alias;database testing;database design;query language	DB	-35.323088002728284	8.625798591712025	57672
ae93ce4aef86f189755d6cfa6fa8dd1902d7ad73	group formation mechanisms for transactions in isis	unsupervised learning;information retrieval;distributed transactions;novelty detection;group formation;thematic browsing;replicated data;hypertext generation;document classification;case based reasoning;symbolico connectionist model	Distributed toolkits like Isis provide means of replicating data but not means for making it persistent. This makes the use of transactions desirable, even in non-database applications. Using Isis can alleviate the programming cost of distributed transaction processing the multi-phase commit protocols. Using the Isis transaction tool, however, imposes additional cost, and we examine the effect of group formation strategies on the overhead. The paper presents three different group formation mechanisms in Isis and compares the costs associated with them.	distributed transaction;isis;list of toolkits;overhead (computing);transaction processing	Neel K. Jain	1994		10.1145/191246.191280	natural language processing;unsupervised learning;case-based reasoning;distributed transaction;computer science;artificial intelligence;machine learning;data mining;database;world wide web;information retrieval	OS	-37.603354054871424	15.020445830219396	57908
11723f01d8a18f49731741c4461eb55745c907b7	ice: self-configuration of information processing in heterogeneous agent teams		Teams of agents, solving complex tasks in dynamic environments, require high-quality information about the current situation. One way of achieving high-quality information is reliable information processing, that is suitable for the application domain. However, the characteristics of some domains such as disaster scenarios are partially unknown at design-time. Therefore, specifying information processing at design-time becomes nearly impossible and leads to unreliable information. We tackle this problem with the ICE middleware which supports adaptive information processing for teams of autonomous agents. It provides a decentralized self-configuration and dynamic integration of information sources. A configuration is created with respect to required information, available sources, and resource constraints. Our evaluation shows that ICE is sufficiently efficient to be operated in highly dynamic domains.	application domain;autonomous robot;information processing;middleware	Stefan Niemczyk;Stephan Opfer;Nugroho Fredivianus;Kurt Geihs	2017		10.1145/3019612.3019653	application domain;information integration;information processing;autonomous agent;middleware;computer science;distributed computing	ML	-37.02004380256815	17.881011158901206	57931
c223e3db98641f286db3a41cca20fcd1aa6be4a6	scenarios for building ontology networks within the neon methodology	informatica;reuse;ontology development;re engineering;methodology	In this poster, we present a set of nine scenarios, identified in the NeOn Methodology, for building ontology networks.	neon (light synthesizer)	Asunción Gómez-Pérez;Mari Carmen Suárez-Figueroa	2009		10.1145/1597735.1597773	computer science;data science;methodology;data mining;reuse;ontology-based data integration;process ontology;suggested upper merged ontology	Networks	-43.5153617218518	5.072570343306151	57956
48e48d319c21b02fd15b48010c0c2c8a1229bc10	automatically adapting source code to document provenance	source code	Being able to ask questions about the provenance of some data requires documentation on each influence on that data’s existence and content. Much software exists, and is being developed, for which there is no provenance-awareness, i.e. at best, the data it outputs can be connected to its inputs, but with no record of intermediate processing. Further, where some record of processing does exist, e.g. as logs, it is not in a form easily connected with that of other processes. We would like to enable compiled software to record useful documentation without requiring prior manual adaptation. In this paper, we present an approach to adapting source code from its original form without manual manipulation, to record information on data provenance during execution.		Simon Miles	2010		10.1007/978-3-642-17819-1_13	computer science;database;programming language;world wide web;source code	SE	-39.88632897502269	11.288609762915835	58207
745cf585498f77d38ccdbda2e27699a8bab0c7c1	requirements ontology and multi-representation strategy for database schema evolution	multi representation strategy;requirements ontology;schema evolution	With the emergence of enterprise­wide information systems, ontologies have become by definition a valuable aid for efficient database schemas modeling and integration besides their disseminated use in other important disciplines such as semantic web and natural language processing. This paper presents another important utilization field of ontology related to database schemas and which is schema evolution topic. More specifically, our research work concentrates on a new three­layered approach for schema evolution based on domain ontology that we have called a requirements ontology and multi­strategies to a powerful change management and cost­ effective evolution. This a priori approach for schema evolution, in contrast with existing a posteriori solutions, can be employed for any data model and for both 1) design from scratch and evolution and 2) redesign and evolution. The paper focuses on the two main foundations of this approach which are the requirements ontology and the multi­representation strategy based on stamping mechanism. 1 Introduction With the emergence of enterprise­wide information systems, the number of ontologies in semantic­driven data access and processing is increasing. It is the case of the use of ontologies in semantic web and natural language processing. In addition to that, ontologies have become by definition a valuable aid for efficient database schemas modeling and integration. In this work, we have investigated another area in which ontologies have a colossal potential of utilization and which is related to information systems. That is database schema evolution, an important, complex and very active research issue. Several solutions have been proposed and much progress has been made in data structures, rules, constraints, schemata models and meta­models. In this 1 This work is carried out as part of IM2 (Interactive Multimodal Information Management)	data access;data model;data structure;database schema;emergence;information management;information system;metamodeling;multimodal interaction;natural language processing;ontology (information science);requirement;schema evolution;semantic web	Hassina Bounif;Stefano Spaccapietra;Rachel Pottinger	2006		10.1007/978-3-540-75474-9_5	upper ontology;schema migration;information schema;semi-structured model;logical schema;computer science;knowledge management;three schema approach;conceptual schema;ontology;data mining;database;ontology-based data integration;database schema;suggested upper merged ontology	DB	-43.49803208639509	6.030419027132966	58215
e4612388efcb18ba6ee16cd510678a99d09a70aa	complex preference queries supporting spatial applications for user groups	categorical base preference;composes complex preference statement;personalized location-based web application;user group;spatial application;demo application;underlying preference sql framework;preference sql;hard constraint;defining spatial;standard sql back-end system;seamless application integration	Our demo application demonstrates a personalized locationbased web application using Preference SQL that allows single users as well as groups of users to find accommodations in Istanbul that satisfy both hard constraints and user preferences. The application assists in defining spatial, numerical, and categorical base preferences and composes complex preference statements in an intuitive fashion. Unlike existing location-based services, the application considers spatial queries as soft instead of hard constraints to determine the best matches which are finally presented on a map. The underlying Preference SQL framework is implemented on top of a database, therefore enabling a seamless application integration with standard SQL back-end systems as well as efficient and extensible preference query processing.	database;location-based service;multi-user;numerical analysis;personalization;sql;seamless3d;user (computing);web application	Florian Wenzel;Markus Endres;Stefan Mandl;Werner Kießling	2012	PVLDB	10.14778/2367502.2367544	computer science;data mining;database;world wide web;spatial query	DB	-34.635676606338976	9.343352977954066	58275
9b28ff53ec6e87b83c3f187ae65df9255f300c7b	representing entities in the ontodm data mining ontology	best practice;data mining;ontology engineering;domain ontology;structured data	Motivated by the need for unification of the domain of data mining and the demand for formalized representation of outcomes of data mining investigations, we address the task of constructing an ontology of data mining. Our heavy-weight ontology, named OntoDM, is based on a recently proposed general framework for data mining. It represent entites such as data, data mining tasks and algorithms, and generalizations (resulting from the latter), and allows us to cover much of the diversity in data mining research, including recently developed approaches to mining structured data and constraint-based data mining. OntoDM is compliant to best practices in ontology engineering, and can consequently be linked to other domain ontologies: It thus represents a major step towards an ontology of data mining investigations.	algorithm;best practice;cluster analysis;computer science;data mining;database;dynamical system;entity;evaluation function;inductive reasoning;machine learning;obo foundry;ontology (information science);ontology engineering;population;predictive modelling;systems biology;taxonomy (general);unification (computer science);upper ontology	Pance Panov;Saso Dzeroski;Larisa N. Soldatova	2010		10.1007/978-1-4419-7738-0_2	upper ontology;open biomedical ontologies;ontology alignment;ontology components;bibliographic ontology;ontology inference layer;ontology;data mining;database;ontology-based data integration;owl-s;information retrieval;process ontology;suggested upper merged ontology	ML	-38.997685087805856	4.649542827793822	58606
f87efbc9aea76fdb56f9dcffa8e46983f8be9fb5	tutorial 3: automatic web service composition	service composition;query reformulation;data integrity;formal model;web service;automatic web service composition;web service composition;service oriented computing;web services;transition systems;formal logic;service oriented computing automatic web service composition	The tutorial aims at providing a deep comprehension of the Web service composition problem and automated techniques to tackle it. Web service composition is currently one the most hyped and addressed issue in the service oriented computing. Starting from an analysis of current technologies and standards for Web service composition, the tutorial will lead the attendees to consider formal models at the base of current proposals, and techniques that can be fruitfully considered to address automatic composition synthesis in each of them. More in detail, attendees will consider: (i) basic technologies and standards for Web service invocation and description (SOAP, UDDI, WSDL,...); (ii) advanced technologies and standards for orchestration and inter-organizational process enactment, in particular WS-BPEL and WS-CDL; (iii) models for Web service composition; (iv) formal tools for both data-centric and process-centric synthesis, including query reformulation a'la data integration, transition-systems based formalisms, trace-based formalisms, logics of programs and processes. In particular, we will show how these formal tools can be applied for automatic Web service composition; (v) current state-of-the-art research results in automatic service composition, drawing a comparison and defining a unifying framework	service composability principle;web service		2006		10.1109/ICWS.2006.138	web service;web development;web modeling;web standards;computer science;service delivery framework;ws-policy;service design;database;multimedia;web 2.0;law;world wide web;universal description discovery and integration	Web+IR	-45.071948733923534	17.43203341560586	58648
a286d7170e15231fb8d9d85c1cbf8bcda7b205d3	a user-aware approach to provide adaptive web services		Web services are rapidly gaining acceptance as a fundamental technology in the web fields. They are becoming the cutting edge of communication between the different applications all over the web. Because of today’s wide diversity of devices together with the variety of the user’s preferences, context-aware web services are becoming a fundamental challenge that must be targeted. This issue is a part of the Human Computer Interaction (HCI) discipline and it aims at adapting the web service behavior according to the user’s context such as his specific work environment, language, type of Internet connection, devices and preferences. Many solutions have been proposed in this area. Nevertheless, the adaptation was carried out only at the runtime and it partially covered the user’s general context. In this paper, we introduce a new context-aware approach that provides adaptive web services. Our approach allows to express requirements by taking into account potential user’s profile in addition to the functional one. While the latter ensures the description of the web service-functionalities, adaptation expresses the ability of a service to be self-adapted to runtime context changes. Our approach deals with adaptation from the very beginning of the modeling step of a web service. Furthermore, it upgrades description and publication usual methods in order to support profile specification.	amazon web services;human computer;human–computer interaction;hypermedia;internet;interoperability;operating system;requirement;sensor;simpletext;system programming language;unified modeling language;user interface;user-centered design;web services description language;web framework;web service	Chiraz El Hog;Raoudha Ben Djemaa;Ikram Amous	2014	J. UCS	10.3217/jucs-020-06-0944	web service;web development;web modeling;simulation;web analytics;web design;web accessibility initiative;web standards;computer science;operating system;ws-policy;web navigation;social semantic web;ws-addressing;multimedia;web intelligence;web engineering;world wide web;mashup	Web+IR	-46.36262792911257	14.223858607442613	58715
3d2542ac32850cb20f6d5882495c82ee5d406fbb	building scalable electronic market places using hyperquery-based distributed query processing	distributed database;data integrity;e business;query processing;portal;electronic markets;distributed query processing;product catalog;dynamic query processing;dynamic data;query evaluation;dynamic query;distributed databases;manufacturing industry;missing data;internet application;electronic business;data warehouse;reference architecture;data integration;data security	Flexible distributed query processing capabilities are an important prerequisite for building scalable Internet applications, such as electronic Business-to-Business (B2B) market places. Architecting an electronic market place in a conventional data warehouse-like approach by integrating all the data from all participating enterprises in one centralized repository incurs severe problems: stale data, data security threats, administration overhead, inflexibility during query processing, etc. In this paper we present a new framework for dynamic distributed query processing based on so-called HyperQueries which are essentially query evaluation sub-plans “sitting behind” hyperlinks. Our approach facilitates the pre-materialization of static data at the market place whereas the dynamic data remains at the data sources. In contrast to traditional data integration systems, our approach executes essential (dynamic) parts of the data-integrating views at the data sources. The other, more static parts of the data are integrated à priori at the central portal, e.g., the market place. The portal serves as an intermediary between clients and data providers which execute their sub-queries referenced via hyperlinks. The hyperlinks are embedded as attribute values within data objects of the intermediary’s database. Retrieving such a virtual object will execute the referenced HyperQuery in order to materialize the missing data. We illustrate the flexibility of this distributed query processing architecture in the context of B2B electronic market places with an example derived from the car manufacturing industry. Based on these HyperQueries, we propose a reference architecture for building scalable and dynamic electronic market places. All administrative tasks in such a distributed B2B market place are modeled as Web services and are initiated decentrally by the participants. Thus, sensitive data remains under the full control of the data providers. We describe optimization and implementation issues to obtain an efficient and highly flexible data integration platform for electronic market places. All proposed techniques have been fully implemented in our QueryFlow prototype system which served as the platform for our performance evaluation.	centralized computing;data security;database;dynamic data;electronic business;embedded system;hyperlink;integration platform;mathematical optimization;missing data;overhead (computing);performance evaluation;prototype;reference architecture;scalability;web service	Alfons Kemper;Christian Dirk Wiesner	2004	World Wide Web	10.1023/B:WWWJ.0000047379.18584.31	reference architecture;dynamic data;missing data;computer science;data integration;data integrity;data mining;electronic business;database;data security;manufacturing;world wide web;distributed database	DB	-35.35602539809358	15.492910099153223	58792
ddfeaf0dfbb0011fb76b82271987e6164eafa819	ds4: introducing semantic friendship in distributed social networks		Existing social networks are based on centralised architectures that manage users, store data, and monitor the security policy of the system. In this work, we present DS4, a Distributed Social and Semantic Search System that allows users to share and search for content among friends and clusters of users that specialise on the query topic. In DS4, nodes that are semantically, thematically, or socially similar are automatically discovered and logically organised into groups. Content retrieval is then performed by routing queries towards social friends and clusters of nodes that are likely to answer them. In this way, search receives two facets: the social facet, addressing friends, and the semantic facet, addressing nodes that are semantically close to the queries. Our experiments demonstrate that searching only among friends is not effective in distributed social networks, and showcase the necessity and importance of semantic friendship.	centralisation;computer cluster;distributed social network;experiment;interaction;microsoft outlook for mac;routing;semantic search	Paraskevi Raftopoulou;Christos Tryfonopoulos;Euripides G. M. Petrakis;Nikos Zevlis	2013		10.1007/978-3-642-41030-7_12	artificial intelligence;mathematics;communication;social psychology	Web+IR	-42.397538216170034	8.878385899013223	58887
07464ce85606aa18b252eed9edb671aec562a6d6	multi-step media adaptation: implementation of a knowledge-based engine	semantic web service;owl;multimedia;standards;device independence;domain knowledge;single domain;semantic web;content adaptation;services;knowledge based engineering	Continuing changes in the domains of consumer devices and multimedia formats demand for a new approach to media adaptation. The publication of customized content on a device requires an automatic adaptation engine that takes into account the specifications of both the device and the material to be published. These specifications can be expressed using a single domain ontology that describes the concepts of the media adaptation domain. In this document, we provide insight into the implementation of an adaptation engine that exploits this domain knowledge. We explain how this engine, through the use of description matching and Semantic Web Services, composes a chain of adaptation services which will alter the original content to the needs of the target device.	ontology (information science);semantic web service;streaming media;user-generated content	Peter Soetens;Matthias De Geyter	2005		10.1145/1062745.1062831	single domain;service;computer science;semantic web;semantic web stack;database;multimedia;world wide web;domain knowledge	Web+IR	-44.42253396735417	12.313989543909802	58906
399405090af27ac8df0c5c1b338bc59b4fb53ce4	a web-based annotation system for improving cooperation in a care network		Coming from the needs of a care network for a cooperating system enabling its members to share information and argue cases, this paper presents a work in progress which aim is to propose a Web-Based system using annotation for cooperation through an electronic patient le (EPF). We rst describe the EPF, and then we focus on our socio-semantic web positioning, on existing web standards for annotation, and on requirements for the EPF annotation tool. Finally, we present a distributed architecture for this web-based application.	digital footprint;distributed computing;eclipse process framework;requirement;social semantic web;web application;web standards	Myriam Lewkowicz;Gaëlle Lortal;Amalia Todirascu-Courtier;Manuel Zacklad;Mohamed-Foued Sriti	2004			data mining;web application;world wide web;web standards;computer science;annotation	HCI	-42.36650436554985	9.992005244778893	58925
65a1c359c9e4396ba67e69277b5bfb406f8b08de	unified publication and discovery of semantic web services	semantic web service;web service discovery;experimental analysis;web service;scalable;semantic web services;unified;web service publication;pyramid s;evaluation;peer to peer	The challenge of publishing and discovering Web services has recently received lots of attention. Various solutions to this problem have been proposed which, apart from their offered advantages, suffer the following disadvantages: (i) most of them are syntactic-based, leading to poor precision and recall, (ii) they are not scalable to large numbers of services, and (iii) they are incompatible, thus yielding in cumbersome service publication and discovery. This article presents the principles, the functionality, and the design of PYRAMID-S which addresses these disadvantages by providing a scalable framework for unified publication and discovery of semantically enhanced services over heterogeneous registries. PYRAMID-S uses a hybrid peer-to-peer topology to organize Web service registries based on domains. In such a topology, each Registry retains its autonomy, meaning that it can use the publication and discovery mechanisms as well as the ontology of its choice. The viability of this approach is demonstrated through the implementation and experimental analysis of a prototype.	autonomy;peer-to-peer;precision and recall;prototype;pyramid texts;scalability;semantic web service	Thomi Pilioura;Aphrodite Tsalgatidou	2009	TWEB	10.1145/1541822.1541826	web service;web development;web modeling;scalability;data web;web mapping;web standards;computer science;evaluation;ws-policy;semantic web;social semantic web;data mining;semantic web stack;database;service discovery;web 2.0;law;world wide web;universal description discovery and integration;experimental analysis of behavior	Web+IR	-41.2765330479047	8.270421352051407	58940
de169e8d5d9d230c2b10155b4e9f1dc01b2f0e02	supporting collaborative conceptualization tasks through a semantic wiki based platform	socio semantics;engenharia do conhecimento engenharia electrotecnica electronica e informatica;social construction of meaning;semantic technologies;conceptual model;capitulo ou parte de livro;shared conceptualization;knowledge representation;formal ontology;semantic wiki	The collective development of conceptual structures has not been satisfactorily addressed in the knowledge representation research literature. Nevertheless and assuming that a shared conceptualization of a given reality is the cornerstone to build semantic artifacts such as ontologies, this paper presents ConceptME, a platform that supports the collaborative modeling of conceptual models. The platform is based on semantic web technologies and proposes a set of functionalities for importing, creating, manipulating, discussing and documenting conceptual models that can act e.g., as the specification of a formal ontology. The platform is in a prototype state and is being tested and used in a research project.	conceptualization (information science);consensus (computer science);interoperability;mediawiki;neon toolkit;semantics (computer science);usability;wiki	Carlos Sá;Carla Sofia Pereira;António Lucas Soares	2010		10.1007/978-3-642-16961-8_60	conceptualization;computer science;knowledge management;data mining;database	Web+IR	-38.93034208281084	12.54869244037627	59107
1aa4f538a1fc89033dc17f2acf9c79bb57bae98d	magic sets for the xpath language	query language;extensible markup language;bottom up;data exchange;xml document;world wide web;logic programs;functional language;deductive databases	The eXtensible Markup Language (XML) is considered as the format of choice for the exchange of information among various applications on the Internet. Since XML is emerging as a standard for data exchange, it is natural that queries among applications should be expressed as queries against data in XML format. This use gives rise to a requirement for a query language expressly designed for XML resources. World Wide Web Consortium (W3C) convened to create the XQuery language, concretely, a typed functional language for querying XML documents. One key aspect of the XQuery language is the use of the XPath language as basis for handling the structure of an XML document. In this paper, we present a proposal for the representation of XML documents by means of a logic program. Rules and facts can be used for representing the document schema and the XML document itself. In addition, we study how to query by means of the XPath language against a logic program representing an XML document. It evolves the specialization of the logic program with regard to the XPath expression. This specialization technique is based on the well-known transformation technique called Magic Sets and studied for deductive databases. The bottom-up evaluation of the specialized program is used for answering the query in the XPath language.	bottom-up parsing;consortium;deductive database;functional programming;logic programming;markup language;partial template specialization;query language;serial digital video out;web resource;whole earth 'lectronic link;world wide web;xml;xpath;xquery	Jesús Manuel Almendros-Jiménez;Antonio Becerra-Terón;Francisco J. Enciso-Baños	2006	J. UCS	10.3217/jucs-012-11-1651	xml validation;xml encryption;xhtml;simple api for xml;xml;xslt;xml schema;streaming xml;computer science;xpath 2.0;document type definition;document definition markup language;document structure description;xml framework;xml database;xml schema;database;schematron;xml signature;programming language;functional programming;xml schema editor;information retrieval;efficient xml interchange;sgml	DB	-34.11605536625261	7.720996999791437	59152
8e6beb2254a33439681d58eceb24843f906f12d3	interoperability among distributed overlapping ontologies--a fuzzy ontology framework	fuzzy membership function;fuzzy reasoning;distributed overlapping ontology;ontology mapping;fuzzy relation;application integration;fuzzy set theory;ontologies artificial intelligence;level of detail;fuzzy ontology;semantic web fuzzy reasoning fuzzy set theory ontologies artificial intelligence open systems;knowledge sharing;semantic web;semantic relation;logical specification;interoperability;semantic relations;open systems;domain ontology;ontologies knowledge engineering power engineering and energy merging mathematics semantic web instruction sets design engineering natural language processing application software;property value;semantic web interoperability distributed overlapping ontology fuzzy ontology knowledge sharing logical specification fuzzy membership function semantic relation;knowledge engineering	"""Ontologies are proposed as a means for knowledge sharing among applications but, it is often not possible to converge to a single unambiguous ontology that is acceptable to all knowledge engineers. Different ontologies vary greatly in terms of the level of detail of their representations, as well as the nature of their underlying logical specifications. Interoperability among different ontologies becomes essential to gain from the power of the existing domain ontologies. In this paper we have proposed a fuzzy ontology framework in which a concept descriptor is represented as a fuzzy relation which encodes the degree of a property value using a fuzzy membership function. Other than concept descriptors, the semantic relations in the ontology like IS-A, HAS-PART etc. are also associated a strength of association. The strength of association between two concepts determines the """"uniformity"""" with which these two concepts have been defined identically across different ontologies. The fuzzy ontology framework provides appropriate support for application integration by identifying the most likely location of a particular term in the ontology"""	circuit complexity;converge;interoperability;knowledge engineer;level of detail;ontology (information science);ontology learning;web ontology language	Muhammad Abulaish;Lipika Dey	2006	2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06)	10.1109/WI.2006.105	upper ontology;idef5;interoperability;ontology alignment;semantic integration;ontology components;ontology inference layer;computer science;knowledge management;ontology;artificial intelligence;level of detail;semantic web;knowledge engineering;data mining;database;fuzzy set;ontology-based data integration;open system;process ontology;suggested upper merged ontology	AI	-43.361629911132525	7.493706926817346	59604
79493ad0f714d5c45ff2a060c0a48f861eb4448a	the darpa knowledge sharing effort: progress report	knowledge sharing	 hatwill enable researchers to develop new systems by selectingcomponents from library of reusable modulesand assembling them together. Their effort will be focusedon creating specialized knowledge and reasonersspecific to the task of their system. Their new systemwould inter-operate with existing systems, usingthem to perform some of its reasoning. In this way,declarative knowledge, problem solving techniques andreasoning services could all be shared among systems.The reusable modules ... 		Ramesh S. Patil;Richard Fikes;Peter F. Patel-Schneider;Donald P. McKay;Timothy W. Finin;Thomas R. Gruber;Robert Neches	1992			computer science;knowledge management;data science;management science	AI	-45.53133886848123	5.284210510038819	59769
06932fdaf495bcaf188a0a16161efbd51d11e327	automated composition of web service workflow - a novel qos-enabled multi-criteria cost search algorithm	search algorithm;web service	The introduction of software technology has dramatically increased the efficiency of completing tasks. Code reusability provides efficiency within the software engineering discipline. With the tumultuous increase in acceptance of service oriented architecture, and thus, a rise in the number of web services, skilled software developers spend a lot of time composing web service workflows, rather than creating innovative and efficient services. Hence, we put forward a technique of code reusability that utilizes heuristic based search methods to automate service workflow composition by weighting quality of service criteria by relevance and importance to the users. We implement a novel and heuristic-based graph creation and search algorithm where the heuristic function value is calculated through the uniform cost search based on each of the quality of service criteria specified by the user. Application of the proposed automated workflow composition algorithm is illustrated with success on an industry-grade service-oriented architecture problem.	code reuse;dijkstra's algorithm;heuristic (computer science);quality of service;relevance;search algorithm;service-oriented architecture;software developer;software engineering;web service	Jaina Sangtani;Gürsel Serpen	2010			web service;data web;computer science;database;web search query;world wide web;workflow management system;information retrieval;search algorithm	SE	-46.686605485196566	15.263624779075766	59832
c8e894760497c36b32ef243a4eabae556f79bc45	meningitis data mining by cooperatively using gdt-rs and rsbr	tratamiento datos;extraction information;discretisation;sistema hibrido;analisis datos;information extraction;systeme discret;discretization;data processing;traitement donnee;discretizacion;continuous system;data mining;systeme continu;data analysis;fouille donnee;sistema continuo;decouverte connaissance;hybrid system;rule discovery;descubrimiento conocimiento;analyse donnee;sistema discreto;rough set;busca dato;extraccion informacion;discrete system;systeme hybride;knowledge discovery	This paper describes an application of two rough sets based systems, namely GDT-RS and RSBR respectively, for mining if-then rules in a meningitis dataset. GDT-RS (Generalized Distribution Table and Rough Set) is a soft hybrid induction system, and RSBR (Rough Sets with Boolean Reasoning) is used for discretization of real valued attributes as a preprocessing step realized before the GDT-RS starts. We argue that discretization of continuous valued attributes is an important pre-processing step in the rule discovery process. We illustrate the quality of rules discovered by GDT-RS is strongly affected by the result of discretization.	data mining;java api for restful web services (jax-rs)	Ning Zhong;Juzhen Dong;Setsuo Ohsuga	2001		10.1007/3-540-45548-5_75	data processing;computer science;artificial intelligence;discretization;data mining;mathematics;information extraction;algorithm	ML	-36.30211507521739	12.746781241081214	59879
260f99a1b7b5e2da9df4836838fad9f3418722f2	ontology-oriented e-gov services retrieval		The semantic e-government is a new application field accompanying the development of semantic web where the ontologies have become a fertile field of investigation. This is due firstly to both the complexity and the size of e-government systems and secondly to the importance of the issues. However, permitting easy and personalized access to e-government services has become, at this juncture, an arduous and not spontaneous process. Indeed, the provided e-gov services to the user represent a critical contact point between administrations and users. The encountered problems in the e-gov services retrieving process are: the absence of an integrated one-stop government, the difficulty of localizing the services’ sources, the lack of mastery of search terms and the deficiency of multilingualism of the online services. In order to solve these problems, to facilitate access to e-gov services and to satisfy the needs of potential users, we propose an original approach to this issue. This approach incorporates a semantic layer as a crucial element in the retrieving process. It consists in implementing a personalized search system that integrates ontology of the e-gov domain in this process.	angular defect;e-government;e-services;experiment;internationalization and localization;ontology (information science);personalization;personalized search;semantic web;spontaneous order	Hassania Ouchetto;Ouail Ouchetto;Ounsa Roudiès	2012	CoRR		human–computer interaction;computer science;artificial intelligence;data mining;multimedia;services computing;world wide web;computer security	AI	-42.924216890955684	7.38242390194234	60035
bb59114e81d08a06b1cb0350f2b1fa0ba38a0056	combining ontologies and markov logic networks for statistical relational mobile network analysis		Mobile networks are managed by means of operations support systems (OSS) which facilitate performance, fault, and configuration management. Network complexity is increasing due to the heterogeneity of cell types, devices, and applications. Characterization and configuration of networks optimally in such a scenario is challenging task. This paper introduces an experimental platform that combines statistical relational learning and semantic technologies by integrating a mobile network simulator, Markov Logic Network model (MLN) and an OWL 2 ontology into a runtime environment tool. Our experiments, based on a prototype implementation, indicate that the combination of OWL and MLN reasoning can be effectively utilized in network status characterization, optimization and visualization.	autonomic computing;configuration management;experiment;faceted classification;graphical user interface;html5;high- and low-level;linked data;markov chain;markov logic network;mathematical optimization;network model;ontology (information science);open sound system;prototype;reasoning system;resource description framework;runtime system;sparql;semantic reasoner;statistical relational learning;web ontology language	Kasper Apajalahti;Eero Hyvönen;Juha Niiranen;Vilho Räisänen	2016			statistical relational learning;computer science;theoretical computer science;machine learning;data mining	Mobile	-41.81082850967081	11.328941862607111	60154
bc564dcc74ee4d6d84efc3b82ec211c6c3681e4e	ontology-based mediation of ogc catalogue service for the web - a virtual solution for integrating coastal web atlases	access point;ontology mapping;geographic information system;information retrieval;decision maker;semantic web technology;data semantics	In recent years significant momentum has occurred in the development of Internet resources for decision makers and scientists interested in the coast. Chief among these has been the development of coastal web atlases (CWAs). While multiple benefits are derived from these tailor-made atlases (e.g., speedy access to multiple sources of coastal data and information), the potential exists to derive added value from the integration of disparate CWAs, to optimize decision making at a variety of levels and across themes. This paper describes the development of a semantic mediator prototype to provide a common access point to coastal data, maps and information from distributed CWAs. The prototype showcases how ontologies and ontology mappings can be used to integrate different heterogeneous and autonomous atlases, using the Open Geospatial Consortium’s Catalogue Services for the Web.	autonomous robot;catalog service for the web;consortium;information system;internet;map;ontology (information science);prototype;relevance;rewriting;web feature service;wireless access point;world wide web	Yassine Lassoued;Dawn J Wright;Luis Bermudez;Omar Boucelma	2008			web service;sensor web;decision-making;web development;web modeling;semantic integration;data web;web mapping;web standards;computer science;semantic web;web navigation;social semantic web;web page;data mining;semantic web stack;geographic information system;web intelligence;world wide web;owl-s;information retrieval;web coverage service	Web+IR	-41.79994168644087	6.475427914051173	60203
b9c99176e49ce0d30d72402b09bc14d61d559a28	knowledge acquisition for importing existing traces to a trace base management system		Trace Base Management System (TBMS) offers processing and querying functionalities for traces that may be of interest to users of tracked systems. Our goal is to ensure the importing of various external traces into kernel for Trace-Based System (kTBS), which is a TBMS developed in the LIRIS laboratory. To overcome the problem of traces heterogeneity, we propose to define a generic collector. To this end, a user with enough knowledge of the tracked system is prompted to define its kTBS trace model and correspondences between the elements of this model and the elements of the trace to import. The system generalises the mappings previously elicited by the user through interaction to create mapping rules. After this phase, the collector will generate modelled traces from the existing ones and the already defined mapping rules.		Mohamed Besnaci;Tahar Bensebaa;Nathalie Guin;Pierre-Antoine Champin	2018	JIKM	10.1142/S0219649218500417	management science;xpath;data mining;knowledge acquisition;management system;computer science;trace (psycholinguistics)	OS	-40.041541318026205	8.916196837744799	60320
2c383773b616162ff0e1c80819c8078bc38afc0a	visual surveillance metadata management	groupware;workflow management;collaborative workflow management;european law regulations;electronic services;workflow management software government data processing groupware;internet;human centric egovernment workflows collaborative workflow management governmental organisations electronic services internet interorganizational collaboration european law regulations;governmental organisations;workflow management software;collaborative work international collaboration law databases expert systems web and internet services runtime monitoring feedback insurance;systems and applications;interorganizational collaboration;government data processing;human centric egovernment workflows	The paper deals with a solution for visual surveillance metadata management. Data coming from many cameras is annotated using computer vision units to produce metadata representing moving objects in their states. It is assumed that the data is often uncertain, noisy and some states are missing. The solution consists of the following three layers: (a) data cleaning layer - improves quality of the data by smoothing it and by filling in missing states in short sequences referred to as tracks that represent a composite state of a moving object in a spatiotemporal subspace followed by one camera, (b) Data integration layer - assigns a global identity to tracks that represent the same object, (c) Persistence layer - manages the metadata in a database so that it can be used for online identification and offline querying, analyzing and mining. A Kalman filter technique is used to solve (a) and a classification based on the moving object's state and its visual properties is used in (b). An object model for layer (c) is presented too.	computer vision;database;emoticon;kalman filter;online and offline;persistence (computer science);smoothing;sputter cleaning	Petr Chmelar;Jaroslav Zendulka	2007	18th International Workshop on Database and Expert Systems Applications (DEXA 2007)	10.1109/DEXA.2007.50	workflow;the internet;computer science;knowledge management;law;collaborative software	DB	-48.22972770035572	9.879929875650307	60350
f80e979df71959a29d43351683a490c43e2af274	an agent-based architecture for distributed imagery & geospatial computing	xml based specification;management system;image processing;image processing geospatial computing aiga ontology parallel algorithms software agents distributed processing xml based specification;agent based;distributed processing;resource management;collaboration;distributed computing;software agents;computer architecture distributed computing collaboration image analysis ontologies resource management switches computer science large scale systems parallel processing;geospatial computing;computer architecture;large scale;complex data;geophysics computing;design and implementation;distributed processing geophysics computing software agents image processing parallel algorithms;information management;agent communication language;image analysis;ontologies;computer science;switches;aiga;ontology;geospatial data;parallel processing;large scale systems;parallel algorithms	Agent-based approaches have not yet been widely applied to highly complex, data intensive,large-scale information processing systems such as are found in the domain of imagery & geospatial computing. Such systems combine diverse and distributed types of imagery and geospatial data, and require collaboration from multiple experts and processing components. This paper gives a description of the design and implementation of the Agent-based Imagery and Geospatial processing Architecture (AIGA). Our approach centers on the development of an ontology, light-weight agents, and an agent communication language for imagery & geospatial computing. AIGA agents cooperate with each other to answer specific queries and to efficiently manage distributed resources. Many of of the imagery & geospatial exploitation tasks that AIGA agents process are highly complex, with several processing steps involved. We describe in detail how the AIGA system is inherently parallel, thus allowing for parallel implementations of both new and legacy imagery & geospatial exploitation algorithms. Because this system is agent driven, the parallelism is highly dynamic, reconfigurable via the AIGA communication language, an XML-based specification called I-XML. These I-XML specifications are distributed and transformed by different AIGA agents in the system to achieve a scalable and manageable system design for collaboration and information management. We describe the details of AIGA, the I-XML specification, and our Java based prototype. This is work is supported under the National Imagery & Mapping Agency’s University Research Initiative program.	agent communications language;algorithm;data-intensive computing;distributed computing;geospatial analysis;information management;information processing;java;middleware;parallel computing;prototype;scalability;systems design;xml namespace;jini	James J. Nolan;Arun K. Sood;Robert Simon	2000		10.1109/AIPRW.2000.953632	computer science;theoretical computer science;database;distributed computing	HPC	-36.20449917853212	17.278019210960984	60354
c1212f76ef4b35cd1cf2aad2cc276ceefdce0846	ontology management and ontology reuse in web environment		As a kind of knowledge representation method, ontology describes knowledge and information semantically in various fields and it has been widely used in Web environment. Ontology management and ontology reuse can solve the problems of knowledge confusion and inefficient knowledge base construction when applying ontology. This paper builds an ontology management framework and presents the system data storage model, ontology maintenance method and role-based ontology collaborative definition method. On this basis, an ontology reuse method based on Semantic Web Rule Language (SWRL) rule is proposed.	ontology (information science)	Yapeng Cui;Lihong Qiao;Yifan Qie	2016		10.1007/978-3-319-61994-1_12	process ontology;ontology inference layer;ontology-based data integration;open biomedical ontologies;owl-s;ontology (information science);upper ontology;suggested upper merged ontology;computer science;knowledge management	Web+IR	-42.54074346863143	6.520173038057757	60373
1228055e0cb6b175434bff14c2ec7c4819093cb3	leveraging semantic technologies for enterprise search	user modelling;information retrieval;semantic technologies;personalization;enterprise search;semantic web;web search;evaluation;expert search	Enterprise search is very different from Web search (for example in the link structure or in the user's needs and goal)and some steps have been already done to exploit these differences in order to improve the effectiveness of enterprise search. In this paper we present the state of the art of the enterprise search field with some open issues. We also present a research plan that aims at using Information Retrieval, Semantic Web, and User Modelling techniques to cope with these issues improving the current state of enterprise search.	information retrieval;semantic web;web search engine	Gianluca Demartini	2007		10.1145/1316874.1316879	enterprise software;semantic search;computer science;evaluation;semantic web;concept search;social semantic web;data mining;personalization;semantic web stack;semantic technology;search analytics;web search query;world wide web;information retrieval;search engine;enterprise information system	Web+IR	-41.0786959020034	7.222431318554781	60393
5acebabee7cef43729fb6f63b80b6f5d62ce1331	cooperative resolution of over-constrained information requests	incremental computation;man machine dialogue;integration information;cooperative information agents;cooperation;cooperative agents;besoin utilisateur;necesidad usuario;constraint satisfaction;cooperacion;solution synthesis;satisfaction contrainte;information integration;user need;commande decentralisee;dynamic csps;integracion informacion;decentralized control;control decentralizado;over constrained csps;dialogo hombre maquina;information agent;satisfaccion restriccion;information system;information need;constraint based modeling;systeme information;constraint hierarchy;dialogue homme machine;sistema informacion	In this paper, we present a constraint-based model of cooperative agents for information systems dialogues, with an emphasis on how the agents detect and resolve situations in which the user's information needs have been over-constrained. The constraint-based model of the information agents integrates and extends the AI techniques of constraint satisfaction, solution synthesis and constraint hierarchy, providing an incremental computational mechanism for constructing and maintaining partial parallel solutions. Such a mechanism supports immediate detection of over-constrained situations. In addition, we explore using the knowledge in the solution synthesis network to support different relaxation strategies to support cooperative dialogue behaviors.	constraint satisfaction;information needs;information system;linear programming relaxation;model of computation;user profile	Yan Qu;Stephen Beale	2002	Constraints	10.1023/A:1017988309378	information needs;simulation;constraint satisfaction;decentralised system;computer science;artificial intelligence;information integration;cooperation;information system	AI	-39.542874535146936	16.650871917724483	60438
6cb498ddc7a244a90780a4bc5d24a028219e18fe	adaptive information provisioning in an agent-based virtual organization: ontologies in the system	agent based;flexible delivery;virtual organization;domain specificity	In this paper we consider utilization of ontologies in an agentbased virtual organization. Specifically, in the system flexible delivery of information is to be based on matching of ontologically demarcated resource profiles, work context(s), and domain specific knowledge. In this paper we introduce basic ontologies and their applications.	ontology (information science);provisioning;virtual organization (grid computing)	Michal Szymczak;Grzegorz Frackowiak;Maciej Gawinecki;Maria Ganzha;Marcin Paprzycki;Myon-Woong Park;Yo-Sub Han;Young Tae Sohn	2008		10.1007/978-3-540-78582-8_28	computer science;knowledge management;data mining;world wide web	AI	-44.55890016753914	10.769174547701345	60441
ab9ecd29bbac2bfb7fce4204c0a40654dcd0083f	a retrospective on semantics and interoperability research		Interoperability is a qualitative property of computing infrastructures that denotes the ability of sending and receiving systems to exchange and properly interpret information objects across system boundaries. Since this property is not given by default, the interoperability problem and the representation of semantics have been an active research topic for approximately four decades. Early database models such as the Relational Model used schemas to express semantics and implicitly aimed at achieving interoperability by providing programming independence of data storage and access. Thereafter the Entity Relationship Model was introduced providing the basic building blocks of modeling real-world semantics. With the advent of distributed and object-oriented databases, interoperability became an obvious need and an explicit research topic. After a number of intermediate steps such as hypertext and (multimedia) document models, the notions of semantics and interoperability became what they have been over the last ten years in the context of the World Wide Web. With this article we contribute a retrospective on semantics and interoperability research as applied in major areas of computer science. It gives domain experts and newcomers an overview of existing interoperability techniques and points out future research directions.	application domain;calendaring software;computer data storage;computer science;consumability;data quality;data store;database model;database schema;desktop computer;email;emergent;entity–relationship model;hypertext;linked data;mpeg-7;markup language;ontology (information science);relational model;scalability;self-documenting code;semantic web;semantic data model;semantic interoperability;semiconductor industry;uncontrolled format string;vocabulary;world wide web	Bernhard Haslhofer;Erich J. Neuhold	2011		10.1007/978-3-642-19797-0_1	semantic interoperability;interoperability;language interoperability;computer science;data mining;database;ws-i basic profile;cross-domain interoperability;world wide web	DB	-40.50551514064061	4.970557908723334	60751
606752edffb2858af77a68a25b2f1125f1fe43b9	document management through hypertext: a logic modeling approach	information access;document management;domain integration;information management;model-based reasoning;logical representation;intelligent personal document processing system;logic modeling;bridge laws;knowledge representation;information system structure modelling;texpros;logic data models;hypertext;formal logic;independent information domains;document handling;hypermedia;model based reasoning;document processing;logic;information system;data model	This work employs logic modeling as a common language t o integrate two independent information domains-document management and hypertext. The concept of hypertext concerns information management and access. Hypertext can also be considered a knowledge representation that models the structure of the information sys tem at serves. A hypertext engine would give the user flexible access t o the information and commands of the underlying application, tailored t o that applicatiov’s structure. This paper focuses on integrating hypertext functionalities into an intelligent personal document processing system called T E X P R O S . W e develop independent logic data models f o r hypertext and T E X P R O S elements, and then present the logical representation of the bridge laws that map between them. W e found logic modeling an invaluable tool in understanding our two domains and eflecting their integration.	data model;document processing;hypertext;information management;knowledge representation and reasoning	Jiangling Wan;Michael Bieber;Jason Tsong-Li Wang;Peter A. Ng	1994			natural language processing;computer science;artificial intelligence;theoretical computer science;data mining;database;information management;logic	DB	-37.369893057414984	7.798826807082174	60970
0f43188f3f483822c886ae2641f178b1083e90c3	"""corrigendum to """"webpie: a web-scale parallel inference engine using mapreduce"""" [web semant. sci. serv. agents world wide web 10 (2012) 59-75]"""	parallel inference engine;web semant;agents world wide web	1. On page 64, the third antecedent of rule 14a of Table 3 is u p w. 2. On page 64, the head of the rule 14b of Table 3 is u p w. 3. On page 67, add, at the end of Section 6.1: In the latest version of the code, the execution of OWL reasoning requires one more job to finish because of an implementation bug of the incremental reasoning procedure. 4. On page 68, in Table 8, the header of the the second column should be ‘‘Input size (millions). The header of the third column should be ‘‘Throughput (Kpts). The value of the input for the Bio2RDF dataset should be 24000 and not 24. Also, notice that the throughput was calculated dividing the input size with the runtime and not the output. 5. On page 69, the value on the y axis of Figure 6(a) should be ‘‘Runtime (minutes)’’. 6. On page 69, Add a footnote after the second sentence of Section 6.5: ‘‘Notice that in some case the implementation used for the duplicate strategies ‘‘threshold’’ and ‘‘end’’ can lead to an infinite loop’’. This can be prevented by simply forcing the deletion after a fixed number of steps. 7. On page 72, add, at the end of the first paragraph of Appendix A.1: As usual, our pseudocode omits details that are not essential for human understanding of the algorithm, such as variable declarations, datatypes and some subroutines (http://en.wikipedia. org/wiki/Pseudocode). 8. On page 71, the first line of Section 8.2 should read RDFS/OWL Horst instead of OWL Horst. 9. In Appendix A, Algorithm 3 (page 72) should read:	algorithm;apache axis;declaration (computer programming);inference engine;infinite loop;information;mapreduce;pseudocode;rdf schema;subroutine;surround sound;throughput;wiki;world wide web	Jacopo Urbani;Spyros Kotoulas;Jason Maassen;Frank van Harmelen;Henri E. Bal	2012	J. Web Sem.	10.1016/j.websem.2012.09.005	computer science;data science;database;world wide web	DB	-35.40448693825447	16.71962551693309	60983
1eb7e3378eba2f50f3a79fca0fea6bd6344268cc	smallwiki: a meta-described collaborative content management system	user interface;object oriented programming;design and implementation;object oriented;smalltalk;content management system;meta modeling;seaside;security policy;meta model	Wikis are often implemented using string-based approaches to parse and generate their pages. While such approaches work well for simple wikis, they hamper the customization and adaptability of wikis to the variety of end-users when more sophisticated needs are required (i.e., different output formats, user-interfaces, wiki management, security policies,...). In this paper we present SmallWiki, the second version of a fully object-oriented implementation of a wiki. SmallWiki is implemented with objects from the top to the bottom and it can be customized easily to accommodate new needs. In addition, SmallWiki is based on a powerful meta-description called Magritte that allows one to create user-interface elements declaratively.	content management system;hypercard;hypertext transfer protocol;internet backbone;meta element;metamodeling;parsing;seaside;serialization;string (computer science);user interface;web application;wiki	Stéphane Ducasse;Lukas Renggli;Roel Wuyts	2005		10.1145/1104973.1104981	computer science;knowledge management;database;world wide web	PL	-40.46902476369547	11.009893648615947	61106
d5611928bbeba55b61082f9f2fdf804e9650de52	emerging web services	special issues and sections web services cloud computing service computing quality of service big data	FROM a technology foundation perspective, services computing has become the default discipline in today’s services industry. As a major implementation technology for modernizing the services industry, Web Services are Internet-based programmable application components that are published using standard interface description languages and that are universally available via standard communication protocols. At their inception,Web Services were based on the SOAP, XML, WSDL and UDDI standards. Subsequently, further standards such as Web Services Atomic Transactions and Web Services Business Activities were developed, and Restful Web Services (which depend on HTTP rather than SOAP) also came to be used in practice. Concurrently, much research related to Web Services was undertaken on topics such as composition, recommendation, etc. However, there are still many unresolved research issues related to Web Services, as they are still a relatively young technology. This IEEE TSC special issue on emerging Web Services includes six articles that address the latest advances in Web Services selection, discovery and recommendation. These advances consider context awareness, quality of service, quality of experience, service usage history and evolution, and cost effectiveness. The first article titled, “Automatic reuse of user inputs to services among end-users in service composition,” by Wang et al. investigates how to leverage inputs from an end-user and from similar end-users to reduce repetitive typing for end-users. The authors develop a method that propagates user inputs across services by linking similar input and output parameters. They also report empirical results from real-world services. The second article, “Time-aware service recommendation for mashup creation,” by Zhong et al. leverages the evolution of the service ecosystem to improve the performance of service recommendations. The authors present a method that extracts service evolution patterns by exploiting Latent Dirichlet Allocation (LDA) and time series prediction. Experiments that are based on a real-world service repository are reported. The third article, “CCCloud: Context-aware and credible cloud service selection based on subjective assessment and objective assessment,” by Qu et al. addresses the cloud service selection problem, considering the diversity and dynamic nature of cloud services. The authors present a selection model based on the comparison and aggregation of subjective assessments extracted from ordinary cloud consumers and objective assessments from quantitative performance testing parties. Experimental results are also presented. The fourth article, “A partial selection methodology for efficient QoS-aware service composition,” by Chen et al. addresses the multi-objective optimization problem for service selection. The authors model QoS-aware service composition using a Pareto set model. They also present a service composition algorithm that uses partial selection techniques. The fifth article, “Towards operational cost minimization in hybrid clouds for dynamic resource provisioning with delay-aware optimization,” by Li et al. analyzes the costminimization problem using a Lyapunov optimization framework. The authors present an online dynamic provisioning algorithm that addresses the challenge, where no a priori information of public cloud renting prices is available and the probability distribution of future user requests is unknown. The results of an experimental study are also presented. The sixth article, “Quality of experience: User’s perception about Web Services,” by Upadhyaya et al. advocates incorporating the perceived quality from the user’s perspective for service selection and composition, i.e., quality of experience (QoE) in addition to quality of service (QoS). The authors develop a method that automatically mines and identifies QoE attributes from the Web. Evaluations using empirical studies are also reported. These six articles not only present the latest advances in Web Services but also suggest directions for future research on Web Services. We hope that you find these articles on emerging Web Services both interesting and thought provoking. In future research, you might consider Web Services as they relate to cloud computing, mobile computing, big data and the Web of Things. There are still many Web Services research opportunities to undertake and application areas to explore. We encourage you to pursue these topics and contribute to the Web Services of the future.	big data;cloud computing;component-based software engineering;context awareness;ecosystem;entity–relationship model;experiment;hypertext transfer protocol;input/output;latent dirichlet allocation;lyapunov fractal;lyapunov optimization;mashup (web application hybrid);mathematical optimization;mobile computing;multi-objective optimization;optimization problem;pareto efficiency;provisioning;quality of service;remote desktop services;representational state transfer;soap;selection algorithm;service composability principle;services computing;software performance testing;time series;web services description language;web services discovery;web of things;web service;world wide web;xml	Bhavani M. Thuraisingham;Liang-Jie Zhang;Louise E. Moser	2015	IEEE Trans. Services Computing	10.1109/TSC.2015.2424612	web service;web application security;web development;service catalog;cloud computing;web standards;computer science;service delivery framework;ws-policy;service-oriented architecture;cloud testing;ws-addressing;database;utility computing;services computing;data as a service;ws-i basic profile;web 2.0;world wide web;computer network	Web+IR	-46.93473821601811	13.792439508833208	61123
dc11d69f5a3bea946668ccdf6afae3cc2e2bcd5f	enhancing learning object content on the semantic web	computer aided instruction;learning materials learning object content semantic web languages metadata description domain ontologies;meta data computer aided instruction semantic web;real world application;learning object;semantic web ontologies web pages intelligent agent proposals carbon capture and storage focusing;semantic web;meta data;domain ontology	This paper gives a proposal to enhance learning object (LO) content using ontologies and semantic Web languages. In the previous work on using ontologies to describe LOs, researchers have built ontologies for description of metadata. However, these ontologies do not improve an LO's content. We suggest creating LOs that have content marked up in accordance with domain ontologies. Accordingly, LOs can be used not only as learning materials, but they can also be used in real-world applications.	ontology (information science);semantic web	Dragan Gasevic;Jelena Jovanovic;Vladan Devedzic	2004	IEEE International Conference on Advanced Learning Technologies, 2004. Proceedings.	10.1109/ICALT.2004.1357633	idef5;web modeling;data web;semantic search;semantic grid;web standards;computer science;artificial intelligence;semantic web;social semantic web;web page;semantic web stack;database;web ontology language;metadata;world wide web;information retrieval;semantic analytics	Robotics	-41.22527462434658	7.958341783618335	61282
96c2ee6e8642cdae6a7f1cf6d06e157cd8b7a8d0	basic considerations for improving interoperability between ontology-based biological information sy	information system	Ontologies are used in biology for the description of multip le kinds of entities. Large ontologies provide categories and relations fo r the basic features found in databases of model organisms. They serve as the basi c means to integrate the data that is generated and interpreted by mult iple heterogeneous groups and stored in distributed biological databases thro ugh ut the world. The use of a common vocabulary and common formal descriptions of the vocabulary’s terms permit the comparison, retrieval and analysis of the data stored in these databases. The ontologies that are used for this purpo se are primarily isolated, single-domain ontologies that have little or no inte rconnections specified among them. Ontology communities such as the Open Biomedical Ontologies (OBO) and the OBO Foundry establish guidelines to maintai n quality and reusability of ontologies, and to facilitate interoperabi lity between ontologies that are included in these projects. I identify several facets of interoperability between onto logy-based information systems in biology which are not currently addressed sa tisfactorily. First, the knowledge representation languages used to represent o ntologies must be sufficiently rich to express the distinctions made by the ont ology designers, and required by the applications of the ontology. Second, the ba sic categories of the biological ontologies must be analyzed and integrated w ithin a common conceptual framework to permit information to flow between t he ontologies. Finally, to let information flow between domain ontologies, the acquisition of	biological database;entity;information system;interoperability;knowledge representation and reasoning;obo foundry;ontology (information science);open biomedical ontologies;vocabulary	Robert Hoehndorf	2009			semantic interoperability;computer science;knowledge management;data mining;database	Web+IR	-43.0423746301854	5.600997736837231	61422
292ae5135312db1b48efbcad7911fc989d324299	a framework for semantic checking of information systems	masterthesis;ontology validation;semantic interoperability;consistency checking	ion Level of specialisation These mismatches are often observed when mapping operations between ontologies are executed. Therefore, this can be associated with the MENTOR methodology approach to the semantic alignment of the involved ontologies. Thus the MO, which uses these tuple based mappings to represent ontology semantic operations and records any mismatches that occur during the operations. 3.4. Semantic Checking Framework There are three approaches to the issue of semantic checking, the one suggested by Li et al. in [46], the one proposed by Haase et al. in [55], and the one by Agostinho et al. in [51]. Starting with the approach described in [46], it features a more general method to the semantic checking issue, since the ontologies are considered as a whole. This means that only the architectural aspects of the ontology based system are considered, i.e., if the system is composed of a single ontology, or if there are multiple separate ontologies interacting each other. This has led the author to adopt this method to serve as basis for the scenarios identified in the framework. Referring to the approaches to the semantic checking issue by Haase et al. and Agostinho et al. these seem quite similar at first sight. However in [55] the approach is more of a structural point of view, encompassing the semantics and data instances of the ontologies. On the other hand, the method described in [51] is more specific, since besides considering the structural aspects of ontologies, namely its semantics and data instances, it also considers the conceptual aspect of ontologies. This conceptual aspect is about the meanings of the used terms, i.e., if the concepts are well characterized. Due to the specificity in this approach, the author chose to use the knowledge mapping types seen in Figure 3.9, applied to the scenarios presented by Li et al. in [46], illustrated in Figure 3.8, to build the framework. To help maintain semantic interoperability in the enrolled systems, the author proposes a semantic checking framework (Table 3.2), which shows the main characteristics that an ontology based information system should comply to maintain semantic consistency. Abstraction Content	information system;interaction;knowledge management;ontology (information science);semantic interoperability;sensitivity and specificity	Gonçalo Alves;João Sarraipa;João P. Mendonça da Silva;Ricardo Jardim-Gonçalves	2012			computer science;data mining;database;information retrieval	AI	-45.09167406021951	6.7705404402795715	61490
5dc8cfdbb1b9fe2ee4d5eb12d2bd01b2bdd8f408	ontology-based mobile communication in agriculture		This paper describes the use of semantic technologies to enable a public/private communication network in the iGreen project. The motivation for using semantic technologies is outlined, and a description of the iGreen ontology-server is given, and the services this provides to users and developers. We discuss the semantic data-sets published in iGreen and the steps taken to enrich and interlink these.	mobile phone;server (computing);telecommunications network	Gunnar Aastrand Grimnes;Malte Kiesel;Ansgar Bernardi	2013	KI - Künstliche Intelligenz	10.1007/s13218-013-0270-3	computer science;knowledge management;multimedia;world wide web	Web+IR	-43.9260601490775	10.215716842691005	61562
ffc5a29064ea2e5651078c52f7359a8ae95e8f02	a critical analysis of lifecycles and methods for ontology construction and evaluation	owl;ontologies spirals maintenance engineering buildings owl documentation remote sensing;remote sensing imagery ontology lifecycle quality evaluation;maintenance engineering;remote sensing;spirals;ontologies;buildings;documentation	Evaluation is a crucial phase in ontological lifecycle, especially for ontologies that are produced using automated or semi-automated methods. In this paper, we focus on the evaluation phase in the frame of the ontology building process. We begin by a review of different ontology construction lifecycles. Next, we try to review state-of-the-art methods for ontology construction with a special interest on evaluation for assessing the quality of ontologies. Afterwards we highlight the main limits and difficulties of these methods with respect to evaluation. We conclude with a first sketch on how evaluation should be undertaken for guiding the development of high quality ontologies that are more relevant to the requirements of a particular domain.	display resolution;iteration;iterative method;ontology (information science);ontology engineering;requirement;semiconductor industry	Hafed Nefzi;Mohamed Farah;Imed Riadh Farah;Bassel Solaiman	2014	2014 1st International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)	10.1109/ATSIP.2014.6834646	upper ontology;idef5;systems engineering;engineering;knowledge management;ontology;data mining;ontology-based data integration;process ontology	AI	-45.859404318081374	6.175725488401004	61631
1555e84874ebcb2c9f1da5d5bac4e74b394bf1ba	a multi-layered approach for the declarative development of data providing services	semantic annotation;xml schema;information retrieval;xml data acquisition information retrieval ontologies artificial intelligence;semantics;resource description framework;web service;ontologies artificial intelligence;multilayered approach declarative development data providing service data retrieval semantic description domain ontology syntactic layer semantic layer declarative definition development model reusable message xml schema data acquisition layer;syntactics;unified modeling language;web services;semantic description;xml;ontologies;semantics resource description framework ontologies syntactics data acquisition unified modeling language web services;domain ontology;data acquisition	Data Providing Services (DPSs) have the sole purpose of retrieving data from existing sources according to their input parameters while also providing a semantic description of the data they provide using a parametrized view over a domain ontology. A layered model of viewing DPSs is proposed consisting of the data acquisition, syntactic and semantic layers. It is shown that by defining all three layers, a DPS may be generated and managed exclusively by its declarative definition. This will increase the agility and efficiency with which DPSs may be deployed and managed. As a development model, a set of reusable messages are created, these messages are to be semantically annotated using a view over the domain ontology and are syntactically represented such that they may be exported to XML Schema. These messages are used within the DPS definition where their views over the domain ontology are parametrized and the data acquisition layer is defined to acquire data from the source.	access control;data acquisition;declarative programming;ontology (information science);pictbridge;plug-in (computing);profile-guided optimization;protection profile;xml schema	Kevin P. Brown;Miriam A. M. Capretz	2011	2011 IEEE International Conference on Web Services	10.1109/ICWS.2011.104	web service;computer science;data mining;database;semantics;law;information retrieval	DB	-38.7154424388199	7.346264549622141	61644
4fc670b976d43301326a2fd5319711cb2dfabfb9	from mappings to modules: using mappings to identify domain-specific modules in large ontologies	application development;ontology mapping;knowledge bases;modularization;snomed ct;semantic web;ontologies;national cancer institute;domain specificity;knowledge base;ontology matching	The problem of ontology modularization is an active area of research in the Semantic Web community. With the emergence and wider use of very large ontologies, in particular in fields such as biomedicine, more and more application developers need to extract meaningful modules of these ontologies to use in their applications. Researchers have also noted that many ontology-maintenance tasks would be simplified if we could extract modules from ontologies. These tasks include ontology matching: If we can separate ontologies into modules based on the topics that these modules cover, we can simplify and improve ontology matching. In this paper, we study a complementary problem: Can we use existing mappings between ontologies to facilitate modularization? We present a novel approach to modularization based on mappings between ontologies. We validate and analyze our approach by applying our methods to identify modules for National Cancer Institutes Thesaurus (NCI Thesaurus) and Systematized Nomenclature of Medicine--Clinical Terms (SNOMED-CT).	ct scan;emergence;national computational infrastructure national facility (australia);ontology (information science);ontology alignment;ontology modularization;semantic web;systematized nomenclature of medicine;thesaurus	Amir Ghazvinian;Natalya Fridman Noy;Mark A. Musen	2010		10.1145/1999676.1999684	upper ontology;idef5;open biomedical ontologies;knowledge base;semantic integration;ontology components;computer science;knowledge management;ontology;artificial intelligence;modular programming;semantic web;snomed ct;data mining;rapid application development;information retrieval;process ontology	AI	-43.08300371628677	4.414619637808774	61931
1e78e6498e17a5157a8657cc04fcb832d0a158b8	using categorial context-shoiq(d+) dl to migrate between the context-aware scenes	sensibilidad contexto;representacion conocimientos;ontologie;context aware;informatique mobile;web semantique;logica descripcion;semantics;semantica;semantique;metalangage;knowledge representation languages;langage representation connaissance;category theory;internet;metalanguage;web semantica;theorie categorie;representation connaissance;semantic web;ontologia;information system;description logic;sensibilite contexte;point of view;knowledge representation;mobile computing;teoria categoria;ontology;systeme information;logique description;sistema informacion;metalenguaje	An important issue in semantic web ontology application is how to improve ontological evolvement to fit the semantics of the unceasingly changing context. This paper presents a context-based formalism- Context-SHOIQ(D+) DL which is under the frame of SHOIQ(D+) DL, a kind of description logic, from the category theory point of view. The core part of the proposed formalism is a categorial context based on the SHOIQ(D+) DL, that captures and explicitly represents the information about contexts. Additionally, this paper presents some meta languages about reasoning and knowledge representation, finally discusses context-aware migration between different scenes with the categorial Context-SHOIQ(D+)DL.		Ruliang Xiao;Shengqun Tang;Lina Fang;Youwei Xu	2006		10.1007/11912873_47	natural language processing;description logic;the internet;metalanguage;computer science;artificial intelligence;semantic web;ontology;semantics;mobile computing;information system;algorithm;category theory	HCI	-37.56095434191126	12.376487824246574	61952
efe8aff8ec2f1e9504e578a4837a1029cf24dde8	applying ontology design patterns in bio-ontologies	science general;domain knowledge;artificial intelligence;computer science;ontology design	Biological knowledge has been, to date, coded by biologists in axiomatically lean bio-ontologies. To facilitate axiomatic enrichment, complex semantics can be encapsulated as Ontology Design Patterns (ODPs). These can be applied across an ontology to make the domain knowledge explicit and therefore available for computational inference. The same ODP is often required in many different parts of the same ontology and the manual construction of often complex ODP semantics is loaded with the possibility of slips, inconsistencies and other errors. To address this issue we present the Ontology PreProcessor Language (OPPL), an axiom-based language for selecting and transforming portions of OWL ontologies, offering a means for applying ODPs. Example ODPs for the common need to represent “modifiers” of independent entities are presented and one of them is used as a demonstration of how to use OPPL to apply it.		Mikel Egaña Aranguren;Alan L. Rector;Robert Stevens;Erick Antezana	2008		10.1007/978-3-540-87696-0_4	natural language processing;upper ontology;bibliographic ontology;computer science;knowledge management;ontology;artificial intelligence;data mining;ontology-based data integration;process ontology;domain knowledge;suggested upper merged ontology	AI	-42.6008112538261	5.113765686717913	62110
9b6f930a898c2b48a2b579c4f8c91fdd941e41e6	extraction of topic map ontology for web service-oriented enterprises		A Service Oriented Enterprise (SOE) is a new model of organization linking business processes and IT infrastructure across the enterprise. It can be enabled through the deployment of Service Oriented Architectures (SOA). At the heart of SOA are the services that are orchestrated using message passing, action coordination etc., web services being an example. However, there is almost no standard business semantics of web services which makes them isolated and opaque. In order to provide a common understanding of business of each other organizations are using trading exchange languages like Universal Business Languages (UBL). Although, these standards provide syntactic interoperability, they do not support efficient sharing of conceptualizations. Ontology can play an important part here, by providing a formal approach to specify shared conceptualization, and thus enabling semantic interoperability. This paper presents an approach for ontology modeling for business process standards used in B2B transactions in web services in terms of a semantic web formalism, viz. Topic Map.	service-oriented software engineering;topic maps;web service	Suman Roy;Kiran Prakash Sawant;Aditya Kale;Olivier Maurice Charvin	2015		10.1007/978-3-662-50539-7_10	upper ontology;bibliographic ontology;knowledge management;ontology;data mining;ontology-based data integration;information retrieval;process ontology	NLP	-44.467357020319774	9.124427985717888	62542
0c32c910c41460308f4650807232454fa3737516	towards automatic merging of domain ontologies: the hcone-merge approach	latent semantic indexing lsi;ontology mapping;latent semantic indexing;wordnet;ontology merging;domain ontology;ontology coordination	Latest research efforts on the semi-automatic coordination of ontologies “touch” on the mapping /merging of ontologies using the whole breadth of available knowledge. Addressing this issue, this paper presents the HCONE-merge approach, which is further extended towards automating the merging process. HCONE-merge makes use of the intended informal meaning of concepts by mapping them to WordNet senses using the Latent Semantic Indexing (LSI) method. Based on these mappings and using the reasoning services of Description Logics, HCONE-merge automatically aligns and then merges ontologies. Since the mapping of concepts to their intended meaning is an essential step of the HCONE-merge approach, this paper explores the level of human involvement required for mapping concepts of the source ontologies to their intended meanings. We propose a series of methods for ontology mapping (towards merging) with varying degrees of human involvement and evaluate them experimentally. We conclude that, although an effective fully automated process is not attainable, we can reach a point where ontology merging can be carried out efficiently with minimum human involvement.	description logic;experiment;merge sort;ontology (information science);ontology merging;semantic integration;semiconductor industry;wordnet	Konstantinos Kotis;George A. Vouros;Kostas Stergiou	2006	J. Web Sem.	10.1016/j.websem.2005.09.004	upper ontology;wordnet;latent semantic indexing;semantic integration;ontology components;computer science;ontology;data mining;database;ontology-based data integration;information retrieval;process ontology;suggested upper merged ontology	AI	-43.56774832727815	6.863656351185968	62549
e189334ad6066a9173b99014f545289a9c71545d	a schema integration framework over super-peer based network	software libraries information resources databases information management content based retrieval rendering computer graphics protocols scalability uniform resource locators navigation;data integrity super peer network meta data record distributed digital library peer to peer network interoperability;data integrity;library service;digital library;digital libraries;heterogeneous environment;schema integration;distributed databases;meta data;peer to peer computing;open systems;distributed databases digital libraries peer to peer computing meta data open systems data integrity	This paper reviews the main efforts in integrating metadata records in distributed digital libraries environment. Each of these solutions does meet some of the requirements of distributed library services somehow, but further study are still needed in some issues, such as limitation on the capacity of collections, retrieval of content-related records built in different metadata formats. In order to alleviate the problem, a novel super-peer based infrastructure for integrating metadata records in distributed and heterogeneous environment is introduced with respect to three situations, namely, sharing a common schema; in different schemata but in same community; in different schema and community. This paper sequentially describes feasible solutions according to these scenarios before coming to the conclusion.	cluster analysis;digital library;distributed library;hilbert space;library (computing);peer-to-peer;requirement;simulation	Hao Ding;Ingeborg Sølvberg	2004	IEEE International Conference onServices Computing, 2004. (SCC 2004). Proceedings. 2004	10.1109/SCC.2004.1358058	computer science;database;world wide web;information retrieval	DB	-35.964215467786666	9.705378161796531	62911
785a02d8305cc1726bb74e339d02a64171fce709	integrating ddi 3-based tools with web services: connecting colectica and exist-db.			colectica;web service	Johan Fihn;Jeremy Iverson	2012			web service;world wide web;computer science	HPC	-39.76869309881373	6.633109136999157	63048
cf4738b42fc8b5c7ab150927cb9aeda474ccc9bd	the role of foundational ontologies in manufacturing domain applications	manufacturing systems;ontologies;bookpart;formal ontology;domain specificity	Although ontology has gained wide attention in the area of information systems, a criticism typical of the early days is still rehearsed here and there. Roughly, this criticism says: general ontologies are not suited for real applications. We believe this is the result of a misunderstanding of the role of general ontologies since, we claim, even foundational ontologies (the most general and formal ontologies) have a crucial role in building reusable, adaptable and transparent application systems. We support this view by showing how foundational ontologies can be used in the manufacturing control area. Our approach (partially presented here through an example) provides a domainspecific ontology which is explicitly designed for applications, theoretically organized by a foundational ontology, driven by the application field for all intents and purposes, suitable for communication across different applications.	applications architecture;applicative programming language;bottom-up proteomics;core ontology;documentation;domain-specific language;experience;information science;information system;ontology (information science);refinement (computing);top-down and bottom-up design;vocabulary	Stefano Borgo;Paulo Leitão	2004		10.1007/978-3-540-30468-5_43	idef5;open biomedical ontologies;ontology components;computer science;knowledge management;ontology;artificial intelligence;data mining;process ontology	AI	-46.40898022878092	5.66000726456063	63218
79a45b83b54a8cba194211caad5f03ff347b31d3	user friendly management of workflow results: from provenance information to grid logical file names	resource description format;performance evaluation;performance evaluation grid logical file names grid workflows workflow management distributed storage environment data provenance information grid catalog features data replication workflow reusability virtual resource browser framework neuroimaging workflows image analysis;provenance workflow grid large result sets logical file catalog;catalogs;distributed storage;data replication;logical file catalog;grid;graphical user interfaces;medical image processing;software reusability;distributed databases;provenance;image analysis application software neuroimaging storage automation conference management biomedical informatics bioinformatics biomedical imaging information analysis mesh generation;workflow;image analysis;ontologies;grid computing;large result sets;software reusability file organisation grid computing medical image processing replicated databases;program processors;replicated databases;file organisation	Grid workflows can produce thousands of results that should be properly organised to enable further analysis. Typically results are stored on locations hard-coded in the workflow or in the components, limiting reusability. In this paper we present an approach to (re)organise the output files generated by a grid workflow in a distributed storage environment. We propose to perform a post-mortem mapping of workflow results into a directory structure. This mapping is based on data provenance information and exploits grid catalog features, namely logical file names, to avoid data replication. By defining different mappings, users can generate their own semantic view of results generated during a workflow execution, which fosters user-friendliness, whereas preserving workflow reusability. An implementation on the Virtual Resource Browser (VBrowser) framework is detailed and evaluated on neuroimaging workflows. Results show that the complex directory structure of an image analysis application cane properly generated by our system. An initial performance evaluation of the mapping resolution and directory structure creation indicates that this approach provides a practical, simple, yet powerful solution to an important roadblock for the adoption of workflows to implement complex image analysis pipelines.	clustered file system;directory (computing);hard coding;image analysis;performance evaluation;pipeline (computing);replication (computing);usability	Tristan Glatard;Sílvia Delgado Olabarriaga	2008	2008 IEEE Fourth International Conference on eScience	10.1109/eScience.2008.31	workflow;computer science;data mining;database;windows workflow foundation;world wide web;workflow management system;workflow engine;workflow technology	HPC	-33.99160587470919	14.46709007138092	63235
cc2c7881c4ef9c205941b8ae6ac334df7085a720	smartmatcher -- how examples and a dedicated mapping language can improve the quality of automatic matching approaches	databases;history;supervised learning;bridges adaptation model meteorology engines impedance matching history matched filters;software systems;bridges;simple one to one alignment;schema matching;schema information;schema heterogeneities;information integration;feedback driven process;adaptation model;engines;machine learning;impedance matching;semantic web;machine learning smartmatcher dedicated mapping language automatic matching information integration computer science database schemas semantic web simple one to one alignment schema information executable mapping language feedback driven process;competitive intelligence;matched filters;ontologies;informatics;automatic matching;executable mapping language;computer science;learning artificial intelligence;dedicated mapping language;database schemas;meteorology;software quality;smartmatcher;supervised learning schema matching schema heterogeneities	Information integration has a long history in computer science. It has started with the integration of database schemas in the early eighties. With the rise of the semantic Web and the emerging abundance of ontologies, the need for an automated integration increased further. A lot of automated matching approaches and tools have been proposed so far. The typical output of such tools is a simple one-to-one alignment mostly based on schema information, e.g., similar names and structures of schema elements. However, these alignments cannot cope with schema heterogeneities, hence, these problems must be resolved manually. Furthermore, there is no automated evaluation of the quality of the alignments based on the instance level, because the matching approaches are not bound to a specific integration scenario, e.g., transformation or merge. In this work we propose the SmartMatching approach, which can be seen as an orthogonal extension to existing matching approaches for increasing the quality of the automatically produced alignments for the transformation scenario. This is achieved by using an executable mapping language for bridging schema heterogeneities and by using instance models to evaluate the quality of the alignments in an iterative and feedback-driven process inspired by machine learning approaches.	bridging (networking);computer science;database schema;executable;iteration;machine learning;one-to-one (data model);ontology (information science);semantic web;xml schema	Horst Kargl;Manuel Wimmer	2008	2008 International Conference on Complex, Intelligent and Software Intensive Systems	10.1109/CISIS.2008.110	computer science;machine learning;data mining;database	SE	-38.44176798309337	4.563029153897905	63479
affba540d5ec613be3e55e462bed7e2c1d907a7a	a tool for data cube construction from structurally heterogeneous xml documents	xml document;data cube;query languages;xml	Data cubes for OLAP (Online Analytical Processing) often need to be constructed from data located in several distributed and autonomous information sources. Such a data integration process is challenging due to semantic, syntactic, and structural heterogeneity among the data. While XML (Extensible Markup Language) is the de facto standard for data exchange, the three types of heterogeneity remain. Moreover, popular path-oriented XML query languages, such as XQuery, require the user to know in much detail the structure of the documents to be processed and are, thus, effectively impractical in many real-world data integration tasks. Several Lowest Common Ancestor (LCA) -based XML query evaluation strategies have recently been introduced to provide a more structure-independent way to access XML	autonomous robot;data cube;document;lowest common ancestor;markup language;olap cube;online analytical processing;query language;xml;xquery	Turkka Näppilä;Kalervo Järvelin;Timo Niemi	2008	JASIST	10.1002/asi.20756	xml catalog;data exchange;xml validation;binary xml;xml encryption;simple api for xml;xml;xml schema;streaming xml;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;world wide web;xml schema editor;cxml;information extraction;information retrieval;efficient xml interchange	DB	-35.85826705515488	6.972539185183657	63514
1338ac51f82edb86def57c7a85f39076ecbdec82	answering an inquiry from heterogeneous contexts	computers;offer semantic consistency maintenance electronic marketplace integration business interoperability business process inquiry;electronic commerce;electronic marketplace integration;offer;business process design;vocabulary;collaboration;computer science cities and towns space technology information science logic collaboration process design floors elevators positron emission tomography;heterogeneous contexts;inquiry;electronic marketplace;receivers;hybrid human agent framework heterogeneous contexts semantic consistency maintenance e marketplace xpm document oriented business rules collaborative business process design automatic service provision;semantic consistency maintenance;internet;automatic service provision;e marketplace;business;defeasible logic;cognition;collaborative business process design;user interfaces electronic commerce internet;consistency maintenance;xpm document oriented business rules;business interoperability;user interfaces;business rules;service provision;business process;hybrid human agent framework;modulation	In this paper we study the semantic consistency maintenance issue between heterogeneous contexts, that is, how an inquiry from an unknown user of an e-marketplace can be received and answered in a semantically consistent way by a firm that is not in the context of the userpsilas e-marketplace. The proposed solution uses XPM to represent semantically consistent business concepts and adopts defeasible logic to reason with XPM document-oriented business rules for inquiring and offering. We motivate the approach with a real-world apartment rental problem, and explain it in architecture of collaborative business process design and automatic service provision. Finally, we report on an implementation specification within a hybrid human-agent framework.	action algebra;business process;defeasible logic;e-commerce;service-oriented device architecture;vocabulary;web application;x pixmap	Jingzhi Guo;Zhuo Hu;Grigoris Antoniou;Chi-Kit Chan	2008	2008 IEEE International Conference on e-Business Engineering	10.1109/ICEBE.2008.65	e-commerce;the internet;cognition;computer science;knowledge management;inquiry;marketing;software engineering;database;business process;user interface;management;business rule;world wide web;computer security;business process modeling;collaboration;modulation	Robotics	-45.69491217410461	13.998343914415104	63535
a022d11af730a0932457fe8e317c2f7c0741fd7d	e-business architecture for web service composition based on e-contract lifecycle	e business;composition;e contract;rest;web service;distributed systems	Nowadays, most of the approaches for compositions of web services are focused on feasibility of implementation rather than on satisfying business concerns. Meeting business concerns also demands flexible and agile implementations. We present an approach for service composition based on the lifecycle of e-contract. Econtracts have clauses and rules that express business concerns on how services are offered and consumed. We propose an architecture that enables the automation of implementation of composite services. The automation is on the configuration of web service engines. The architectural model supports the publication of contracts that describe how services are offered from different providers in order to develop the composition	agile software development;business architecture;distributed computing;electronic business;fault tolerance;scalability;self-replicating machine;service composability principle;web service	José Bernardo Neto;Celso Massaki Hirata	2015		10.5220/0005377902760283	web service;composition;business process execution language;computer science;knowledge management;software engineering;ws-policy;service-oriented architecture;electronic business;database;rest;industrialization of services business model;services computing;management;law;world wide web	SE	-47.58068234938335	17.77935999604128	63622
068bb8b2b3977b889c353e75f75067e94c613b1c	a wfs-based mediation system for gis interoperability	query language;data integrity;geographic information system;spatial data;geographic information systems gis;geography markup language;web feature server wfs;data representation;geography markup language gml;mediation;xml	The proliferation of spatial data on the Internet is beginning to allow a much wider access to data currently available in various Geographic Information Systems (GIS). In order to move to a real Web-based community where geographical data can be accessed and exchanged, we need to provide flexible and powerful GIS data integration solutions. Indeed, GIS are highly heterogeneous: not only they differ by their data representations, but they also offer radically different query languages. A GIS mediation approach should provide (1) an integrated view of the data supplied by all sources, and (2) a geographical query language to access and manipulate integrated data.In this paper we propose an approach that not only focuses on the data integration, but also addresses the integration of query capabilities available at the sources. A GIS may provide a query capability inexistent at another GIS, whereas two query capabilities may be similar but with a slightly different semantics. We introduce the notion of derived wrappers that capture additional query capabilities to either compensate capabilities lacking at a source, or to adjust an existing capability in order to make it homogeneous with other similar capabilities, wrapped at other sources. Finally we describe the implementation of the presented approach that complies with OpenGIS WFS recommendation.	geographic information system;internet;interoperability;query language;wrapper function	Omar Boucelma;Mehdi Essid;Zoé Lacroix	2002		10.1145/585147.585153	distributed gis;enterprise gis;xml;geography markup language;computer science;data integrity;data mining;database;external data representation;spatial analysis;geographic information system;rdf query language;mediation;am/fm/gis;world wide web;query language	DB	-36.13841487424694	9.575073949456945	63737
f2e55566744f58f840379e95ac3fc5e6c3558171	industrial strength ontology management	e commerce;emergent semantics;heterogeneous information;autonomous agent;service discovery;knowledge representation;configuration management	Ontologies are becoming increasingly prevalent and important in a wide range of e-commerce applications. E-commerce applications are using ontologies to support parametric searches, enhanced navigation and browsing, interoperable heterogeneous information systems, supplier enablement, configuration management, and transaction discovery. Applications such as information and service discovery and autonomous agents that are built on top of the emerging Semantic Web for the WWW also require extensive use of ontologies. Ontology-enhanced commercial applications, such as these and others require ontology management that is scalable (supporting thousands of simultaneous distributed users), available (running 365x24x7), fast, and reliable. This level of ontology management is necessary not only for the initial development and maintenance of ontologies, but is essential during deployment, when scalability, availability, reliability and performance are absolutely critical. VerticalNet’s Ontology Builder and Ontology Server products are specifically designed to provide the ontology management infrastructure needed for e-commerce applications. These tools bring the best ontology and knowledge representation practices together with the best enterprise solutions architecture to provide a robust and scalable ontology management solution.	autonomous robot;configuration management;e-commerce payment system;enterprise integration;information system;interoperability;knowledge representation and reasoning;ontology (information science);scalability;semantic web;service discovery;software deployment;verticalnet;www	Aseem Das;Wei Wu;Deborah L. McGuinness	2001			e-commerce;upper ontology;knowledge representation and reasoning;ontology alignment;bibliographic ontology;computer science;knowledge management;ontology;artificial intelligence;autonomous agent;data mining;database;configuration management;service discovery;ontology-based data integration;world wide web;owl-s;process ontology;suggested upper merged ontology	Web+IR	-45.806566596108304	12.361859715898243	64048
3405a0f97d022cbd7e87b49ea843de204879a394	xml databases, are ready for bioinformatics?	xml database	Since the born of XML, more and more bioinformatics tools and biological data sources have embraced it as a common way to publish results. Other previous attempts, like ASN.1, have not been so widespread adopted as XML because they were created in the wrong moment, when the computational power was valuable treasure. Now, the volume of available information in molecular biology and bioinformatics in XML format is big enough to think on the use of XML databases. In this paper we will talk about XML technologies, places where XML is being used, the alternatives to mining data sources in XML, and our experiences in that field. Brief introduction to XML XML[1] (eXtensible Markup Language) was created in 1998 as an attempt to simplify the complexity of SGML. In fact, XML is a subset of SGML, aimed to lower the hardware requirements of SGML language parsers and processors, meanwhile the information correctness could be checked in a reasonable time. Other reason to create XML was to separate the data representation from the data itself. This is very useful, for instance, when you have published some information and someone else wants to extract, process or show it in a different way. The third one was to have a standard way to store semi-structured information, like the biological one. Like ASN.1, XML is a flexible way to store information, because you are not stuck to a fixed format. The structure of any XML content can be thought as a tree, and if it is based on a defined structure, it will follow the imposed restrictions. At the beginning, the only way to define an XML format was creating a DTD (Data Type Definition). The used language for these DTDs is a subset of the used for SGML DTDs. Although it is quite expressive, it lacks some key characteristics like a way to define precise restrictions in the stored data or a powerful and extensible data type system. So, XML Schema was created in order to overcome these and other expressiveness problems. The next important feature in XML is the concept of namespace. When people defined their format, nothing could avoid they had created XML elements with the same name. In principle, that is not a problem, but what does it happen if someone wants to reuse two or more XML formats in a new DTD or XML Schema? Sometimes it was impossible due XML element name collisions. One way to solve the problem is the use of the namespace concept (used by some object-oriented programming languages): if you want to integrate two or more XML formats, each one of them must live under a different namespace, so there is no chance for name collisions. Due XML success, many other features and technologies have grown under the XML shades: XSL (XML Stylesheets), XPath, XInclude, SOAP, XML-RPC, SVG, WSDL, XQuery, RDF, etc... If we focus on molecular biology and bioinformatics areas, these technologies are more or less integrated and (mis)used, but definitively they are now key tools in the development of new projects. XML in Biology and Bioinformatics If we look at the bioinformatics area, the impact of XML technologies has been huge: NCBI Blast tools[2] are able to generate their output in XML, the DAS protocol[3] is based on three XML custom formats, etc... But, even more important, many of the biggest molecular biology data repositories are publicly available in XML: UniProt[4], born from the fusion of SWISSPROT, TrEMBL and PIR; InterPro; IntAct[5]; GO; etc... There are some efforts about having a central repository for the different XML used formats in biology[6], and some common standard formats have been developed (BSML, AGAVE, GAME, MIAME/MAGE, IntAct, etc...) in order to ease the information exchange in areas like sequences, interactions, etc... Also, EBI is providing a translation and integration service for EMBL[7], Genbank and DDBJ databases, so anyone can get nucleotide sequence data in different XML formats, like BSML or AGAVE. Anyhow, the size of the different XML data sources, both in number of entries and in raw size is increasing at the same rate, or even bigger, than the classical molecular biology data sources. We have to use technologies powerful enough so we can deal with these volumes of information. Until now, most of the ways to mine information from these data sources were related to the use of relational databases, but one of the XML features, semi-structured data representation, sometimes makes it very difficult to realize. XML databases: mining XML As we have told above, there is already a toolbox full of technologies related to XML[1]. The most interesting ones in the data mining field are the ones related to query XML contents: XPath, XSLT and XQuery. XPath was created to locate XML fragments in a XML tree which followed some given conditions, and both XSLT and XQuery (among others) depend on XPath. XSLT is the XSL subset which deals with the task of building an XML output tree based on the transformation of XML fragments from a single XML input tree. It is commonly used to translate XML content into HTML or XSL:FO, and the work flow in XSLT can be driven both by XPath expressions and procedural calls. XQuery is the XML query language created to query a set of XML documents, and it is more or less the SQL of XML databases. XML data model is very different from relational model, because the order of the elements in a XML tree does mind, unlike tuples in a relational table. Other difference is that a relational database has a fixed two-level structure (tables, columns inside tables), unlike XML trees. So, XQuery uses XPath as part of its FLWOR (For-Let-Where-Order-Return) expressions, so an XPath expression selects XML fragments a FLWOR expression is going to handle. Although all these query languages can be used to mine single XML trees, what we want to do is mining a forest. So, what can it be understood as an XML database? Basically, a database which is able to store and organize XML trees, which can be queried using any of the XML query languages, returning XML fragments as the answer of the queries. Even more, a desirable feature in a XML database is the implementation of the XML:DB API[8], which is a independent way to send queries to a XML database, like JDBC or DBI are for relational databases. Other desirables features are XUpdate, an extension to allow the update of XML fragments belonging to stored XML documents, and support for organizing and managing XML content in collections. The number of available XML databases is increasing each day, and they can be classified in three different approaches: pure XML databases; XML databases using a classical database engine (relational, object oriented) as the underlying technology; and XML extensions for an existing database engines. A pure XML database is defined as the one which uses its own custom storage format and query processor. A XML database based on a classical database engine stores digested XML content in a database, and it translates input queries to a set of queries to the underlying database engine. XML extensions in a database engine are composed by special data types and a set of procedures and functions which deal with these special data types, and they can be used and embedded in a native query to the database. From our point of view, some XML extensions cannot be though as XML databases in some cases, because they don't usually provide the same integration levels as the other approaches. Our experiences with XML databases There are some very powerful commercial XML databases and XML extensions, like the ones from Oracle and Software AG. After looking for open-source XML databases in the web, we found two promising products, still in development: eXist[9] and dbXML[10]. Both of them are coded in Java, which has its advantages and its drawbacks. On one hand, most advanced XMLrelated libraries are available in Java. On the other hand, Java programs are slower and use more memory than the equivalent ones coded in other languages like C or C++, and XML libraries in Java tend to be much less efficient than the corresponding C and C++ equivalents. Both products are native XML databases which implement XML:DB API and XUpdate, and they also provides additional methods to query and access the stored information: HTTP GET, XML-RPC, servlets, monolitic standalone access, etc... They have some interesting extensions, like full text indexing (FTI) and full text search (FTS), but we have not compared these capabilities with the ones from other specialized products like glimpse or OpenFTS project. dbXML can be queried using XPath and XSLT, meanwhile eXist uses both XPath and XQuery. As any database management system, these XML database engines need some hints to improve query response times, and eXist and dbXML diverge in this point, because eXist build indexes over almost any imaginable XML component (elements, attributes, attribute values, text values) by default, meanwhile dbXML only builds them under explicit notification. Our tests on these databases were done using IntAct as a mid-size XML data source (25~30MB), and UniProt as a big XML data source (5GB). Also, we have used XSLT and Xalan C++[11] as a measurement of the advantages we were getting using XML databases over processing raw XML content. The machine we have used for the tests is a PC with 1GB of memory, a Athlon 1GHz processor and a 540 GB array for the storage. We have also used IBM, Sun, and BEA Java Virtual Machines for our tests, and we have found Sun JVM had better performance than the others because these products are I/O intensive both in query and store operations. The programs we have built to launch queries to the XML databases have been written in Perl, and we have used both XML-RPC and HTTP-GET interfaces for that task. These programs issued both easy and complicate queries, related to returning XML fragments from the content and the number of them, generating new XML fragments for the output and doing correlated queries. We found eXist 	abstract syntax notation one;apache xalan;athlon;blast;bioinformatics;c++;central processing unit;column (database);computation;correctness (computer science);dna data bank ofjapan;data (computing);data mining;data model;database engine;embedded system;external bus interface;flwor;fleet telematics system;genbank;gigabyte;html;hypertext transfer protocol;information exchange;input/output;interpro;interaction;jdbc;java servlet;level structure;library (computing);minimum information about a microarray experiment;open-source software;organizing (structure);parsing;perl dbi;programming language;protein information resource;query language;relational database;relational model;requirement;resource description framework;soap;sql;scalable vector graphics;semi-structured data;semiconductor industry;standard generalized markup language;tree (data structure);type system;uniprot;web services description language;xml database;xml schema;xml tree;xml-rpc;xpath;xquery;xslt;xupdate	José María Fernández;Alfonso Valencia	2004			xml catalog;xml validation;binary xml;xml base;simple api for xml;xml;streaming xml;computer science;document type definition;document structure description;xml framework;xml database;database;xml signature;world wide web;xml schema editor;information retrieval;efficient xml interchange	DB	-34.89536491940365	8.045841265116831	64055
af4a5b34c75883bb4ab97852252a0db4fec8ee21	generic web service wrapper for mediation based data warehousing	document clustering;text mining;semantics;similarity measure	The growing availability of specialized web services targeting only a particular niche has resulted in the use of multiple web services by enterprises for their daily activities. It is significantly difficult for resource-crunched small enterprises to write applications to integrate with each and every dependent web service. Overcoming the missing wide scale adoption of machine readable standards (WSDL, WADL, SAWSDL), we explore the capability to integrate with numerous web services using only the basic web standards (HTTP, JSON, XML, XSD, XSLT) and declarative languages (SQL, datalog queries). In this paper, we specifically focus on the role of a generic web service wrapper to support this declarative approach to describe, query and extract enterprise data from web services.	datalog;human-readable medium;hypertext transfer protocol;json;niche blogging;sawsdl;sql;service wrapper;web application description language;web services description language;web service;web standards;xml;xslt	John Samuel;Christophe Rey	2016		10.1145/2912845.2912881	web service;web application security;web mining;web development;web modeling;data web;web mapping;web design;web standards;computer science;web api;ws-policy;web navigation;social semantic web;web page;data mining;ws-addressing;semantic web stack;database;web intelligence;ws-i basic profile;web 2.0;world wide web	Web+IR	-37.872809894902154	7.932298882683796	64212
85f6168ccdfd9b5a294e5b8150fb82c6601c3b7e	hybrid service matchmaking in ambient assisted living environments based on context-aware service modeling	context awareness;ucl;pervasive computing;discovery;theses;conference proceedings;service matchmaking;ambient assisted living;semantic web services;digital web resources;ucl discovery;open access;ucl library;book chapters;open access repository;knowledge modeling;ucl research	Ambient assisted living (AAL) environments are augmented with sensing and communication technologies to support elderly people with personalized, adaptive and anticipatory requirements. A plethora of heterogeneous devices and services appear and disappear, which expose different behavior with the changing contexts in these environments. Therefore, a service matchmaking mechanism that attempts to identify relevant services in order to fulfill the user needs, has to deal with the heterogeneity of devices and services along with their dynamic behavior. Existing service specifications, such as semantic web services, are often used to abstract the environment’s functionalities and the user tasks without incorporating context-aware properties, which makes them unsuitable for service matchmaking in pervasive and ambient environment. To deal with these issues, we introduce a contExt Aware web Service dEscription Language (wEASEL) that is an abstract service model to represent services and user tasks in AAL environments. Also, we present a set of wEASEL-based service matching algorithms and evaluate them for their suitability. The proposed service matchmaking mechanism can incorporate services that are available in local AAL environment as well as in the cloud computing marketplace.	atm adaptation layer;algorithm;cloud computing;information needs;personalization;pervasive informatics;requirement;semantic web service;semantic reasoner;service composability principle	Aitor Urbieta;Alejandra González Beltrán;Sonia Ben Mokhtar;Jorge Parra;Licia Capra;M. Anwar Hossain;Abdulhameed Alelaiwi;Juan Ignacio Vázquez	2015	Cluster Computing	10.1007/s10586-015-0469-1	computer science;service delivery framework;operating system;database;multimedia;world wide web;ubiquitous computing	Web+IR	-44.080942053798246	11.619903173282255	64294
e35b91733ae5c6e575bb8b099dae39cbfdc0e7cf	n-dimensional matrix-based ontology: a novel model to represent ontologies			ontology (information science)	Ahmad A. Kardan;Hamed Jafarpour	2018	Int. J. Semantic Web Inf. Syst.	10.4018/IJSWIS.2018040103	data mining;information retrieval;ontology (information science);computer science;ontology;matrix (mathematics)	Web+IR	-38.66672493505774	5.209037104639797	64537
285b9e065c5e15d1ec849a8674a715034afb3cdd	a metadata registry for community driven e-learning sites	computer aided instruction;meta data;semantic web;e-learning site;metadata registry;semantic web architecture	We present the architecture and the interface of a metadata registry for a large e-learning site. The metadata registry is very simple to integrate by content and application providers and thereby tries to motivate more members of the community to contribute. It takes its inspiration from currently successful semantic Web architectures and aims to be an evolutionary change to the Web - using long established standards where possible	semantic web;world wide web	Valentin Zacharias	2005	International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)	10.1109/CIMCA.2005.1	web service;semantic computing;web development;web modeling;data web;semantic grid;web standards;computer science;artificial intelligence;semantic web;social semantic web;web page;data mining;semantic web stack;database;web 2.0;metadata;world wide web;data element;meta data services;semantic analytics;data mapping;metadata repository	Robotics	-41.356701095674524	8.184675565894286	64811
ab0b0c584fd02d1c92bb8c3537a09829517d9fe1	formal support for representing and automating semantic interoperability	ontologie;red www;ontology mapping;traitement flux donnee;interoperabilite;interoperabilidad;reseau web;semantics;semantica;semantique;semantic interoperability;semantic heterogeneity;information flow;heterogeneidad;data flow processing;world wide web;ontologia;interoperability;theorie information;analisis semantico;analyse semantique;ontology;information theory;heterogeneity;semantic analysis;heterogeneite;teoria informacion	We discuss approaches to semantic heterogeneity and propose a formalisation of semantic interoperability based on the BarwiseSeligman theory of information flow. We argue for a theoretical framework that favours the analysis and implementation of semantic interoperability scenarios relative to particular understandings of semantics. We present an example case of such a scenario where our framework has been applied as well as variations of it in the domain of ontology mapping.	information flow (information theory);ontology (information science);semantic web;semantic heterogeneity;semantic integration;semantic interoperability;web ontology language	Yannis Kalfoglou;Marco Schorlemmer	2004		10.1007/978-3-540-25956-5_4	upper ontology;semantic interoperability;interoperability;semantic computing;semantic integration;information flow;information theory;semantic grid;computer science;heterogeneity;ontology;data mining;database;semantics;world wide web;statistics	Web+IR	-37.82837393524465	12.228237140024994	65035
065f63db653e7d2d79582d8510d0a8629a4e1cd0	an architecture for the integration of multimedia heterogeneous data sources		The main focus of the paper is the design of a system which integrates data from heterogeneous multimedia repositories in a single, unified database view, in order to build a multimedia information system capable of integrating data that resides in different data base systems as well as in a variety of non-database data servers. We describe the object oriented data model used as a common model from the wrapper/mediator system, so providing a simple mechanism for enabling the integration while maintaining the independence of the repositories and providing a semantic description of the extracted data.	data model;database;information system;view (sql)	Angelo Chianese;Vincenzo Moscato;Antonio Picariello;Antonio Maria Rinaldi	2004			architecture;multimedia;computer science	DB	-35.87021279190722	9.827753299256319	65084
602625acbed8049ce2700951fa57cb4859b37cef	user-centric optimization for constraint web service composition using a fuzzy-guided genetic algorithm system		Service-Oriented Applications (SOA) are being regarded as the main pragmatic solution for distributed environments. In such systems, however each service responds the user request independently, it is essential to compose them for delivering a compound value-added service. Since, there may be a number of compositions to create the requested service, it is important to find one which its properties are close to user’s desires and meet some non-functional constraints and optimize criteria such as overall cost or response time. In this paper, a user-centric approach is presented for evaluating the service compositions which attempts to obtain the user desires. This approach uses fuzzy logic in order to inference based on quality criteria ranked by user and Genetic Algorithms to optimize the QoS-aware composition problem. Results show that the Fuzzy-based Genetic algorithm system enables user to participate in the process of web service composition easier and more efficient.	fuzzy logic;genetic algorithm;quality of service;response time (technology);service composability principle;service-oriented architecture;service-oriented device architecture;web service	Mahdi Bakhshi;Mohsen Hashemi	2012	CoRR	10.5121/ijwsc.2012.3301	computer science;data mining;database;world wide web	Web+IR	-45.61353947687554	15.274536781679306	65169
0a52114b28b0568e8917be79b9b1d4f8f27e488f	crowdsourced knowledge: peril and promise for conceptual structures research	classical logic;question-answering system;repeated assertion;increasing complexity;knowledge base;evolutionary view;crowdsourced knowledge;conceptual structures research;web resource;promising alternative;world wide web	classical logic;question-answering system;repeated assertion;increasing complexity;knowledge base;evolutionary view;crowdsourced knowledge;conceptual structures research;web resource;promising alternative;world wide web	crowdsourcing	Mary Keeler	2011		10.1007/978-3-642-22688-5_10	computer science;artificial intelligence;data mining;mathematics;algorithm	Vision	-40.639720515113844	5.662295669576954	65189
ef20f69d448f4a807617b071592890f4370a3d0a	infoflex: flexible and distributed content management - using web services and semantic web to manage content	content management;web service;semantic web	The development of information and communication technologies and the expansion of the Internet means that nowadays there are huge amounts of information available via these emergent media. The need to manage such information, which was in the past stored on paper media, has become apparent in different fields. A number of content management systems have appeared which aim to achieve this task. Most of these systems are oriented towards Web publishing on a central site, and they do not support collaboration among several, distributed sources of managed content. In this paper we present a proposal for an architecture for the efficient and flexible management of distributed content.	content management system;emergence;internet;portals;semantic web;web service;web syndication;xml	Jesús Villamor-Lugo;Norberto Fernández García;Luis Sánchez Fernández;Jesús Arias-Fisteus;J. Tomás Nogales Flores;Antonio Hernández-Pérez;David Rodríguez-Mateos	2004			web service;web application security;web mining;web development;web modeling;data web;web mapping;content management;web standards;computer science;web api;ws-policy;semantic web;web navigation;social semantic web;web page;semantic web stack;database;web intelligence;web 2.0;world wide web;information retrieval;semantic analytics	Web+IR	-41.853676370954325	8.4466710409474	65334
1b17b602a59e4dcefc418249f43f08d99771f75b	a framework for generating and maintaining global schemas in heterogeneous multidatabase systems	global schema virtual class multidatabase system database autonomy schema evolution query language heterogeneous databases local databases dbms object oriented database global schemas local heterogeneous schemas local schemas integration operators local schema integration model independent representation;multidatabase system;distributed databases object oriented databases;heterogeneous databases;schema integration;integral operator;distributed databases;object oriented databases;data models computer science information systems database languages merging;schema evolution	The problem of creating a global schema over a set of heterogeneous databases is important due to the availability of multiple databases within organizations. The global schema should provide a unified representation of local heterogeneous schemas. In this paper, we provide a general framework that supports the integration of local schemas into a global one. The framework takes into consideration the fact that local schemas are autonomous and may evolve over time, which makes the definition of the global schema obsolete. We define a set of integration operators that integrates local schemas based on the semantic relevance of their classes, and provide a model-independent representation of virtual classes of the global schema. We also define a set of modifications that can be applied to local schemas as a consequence of their local autonomy. For every local modification, we define a propagation rule that will automatically disseminate the effects of that modification to the global schema without having to regenerate it from scratch via integration.		Rehab Duwairi	2003		10.1109/IRI.2003.1251414	schema migration;information schema;logical schema;computer science;knowledge management;conceptual schema;star schema;data mining;database;database schema;distributed database	DB	-34.37965564890748	11.486946181026054	65375
5438f479e5d63fc7f8199611e2dd4d991d4d539a	a collaborative schema integration system	schema integration;hypermedia;cscw;database design	Conceptual modelling as applied to database development can be described as a two stage process: schema modelling followed by schema integration. Schema modelling is the process of transforming individual user requirements into a conceptual schema: an implementation-independent map of data requirements. Schema integration is the process of combining individual conceptual schemas into a single, unified schema. Single-user tools for schema modelling have enjoyed much success partly because the process of schema modelling has become relatively well formalised. Although a number of formal approaches to conducting schema integration have been proposed, it appears that schema integration tools have not enjoyed the same level of success. This we attribute not so much to the problem of formalisation but to the inherent collaborative nature of schema integration work. This paper first discusses the importance of collaboration to schema integration work. It then describes SISIBIS, a demonstrator system employing the IBIS (Issue Based Information System) scheme to support collaborative database design.	argumentation framework;baxter (robot);conceptual schema;continuous integration;database design;database schema;embedded system;graphical user interface;information system;requirement;software propagation;user requirements document;visual programming language	Paul Beynon-Davies;L. Bonde;D. McPhee;Christopher B. Jones	1997	Computer Supported Cooperative Work (CSCW)	10.1023/A:1008627102073	schema migration;information schema;logical schema;human–computer interaction;computer science;knowledge management;three schema approach;conceptual schema;document structure description;computer-supported cooperative work;data mining;xml schema;database;document schema definition languages;database schema;xml schema editor;database design	DB	-33.7447859849867	12.269706350290548	65420
652b9552aca26a3395857f78a492d783136407c5	a semantic web approach to handling soft constraints in virtual organisations	semantic web service;virtual organisations;service provider;semantic web rule language;soft constraints;satisfiability;virtual organisation;interchange format;semantic web;constraint interchange format	In this paper we present a proposal for representing soft constraint satisfaction problems (CSPs) within the Semantic Web architecture. The proposal is motivated by the need for a service-providing agent in a virtual organisation to reason about its commitments as soft constraints. The three essential requirements addressed are: (1) the need to have constraints express commitments in terms of Semantic Web services, (2) the need to associate utility values with constraints, to reflect the relative importance of satisfying them, and (3) the need to make statements about which constraints are satisfied and violated by a given solution. The proposal builds upon previous work in defining a Semantic Web Constraint Interchange Format (CIF), which itself builds on the proposed Semantic Web Rule Language (SWRL). The paper describes an ontology for representing soft CSPs and their solutions, allowing an agent's set of commitments to be expressed as a collection of soft constraints. The ontology is an open interchange format for soft CSPs, allowing commitment to be communicated and exchanged among the members of a virtual organisation.	constrained optimization;constraint (mathematics);constraint satisfaction problem;requirement;semantic web rule language;semantic web service;server message block;virtual organization	Alun D. Preece;Stuart W. Chalmers;Craig McKenzie;Jeff Z. Pan;Peter M. D. Gray	2006		10.1145/1151454.1151487	semantic computing;semantic web rule language;data web;semantic grid;computer science;knowledge management;semantic web;social semantic web;semantic web stack;database;world wide web;owl-s	AI	-43.46062806530318	14.426026134129183	65479
5eda10624b7d8844a430b1f798adc8e6b9913c61	interactive visualisation of geographical objects on the internet		The potential of the World Wide Web as an information distribution network has increased. The expansion of the Internet provides data suppliers with a completely new means for disseminating geospatial information in a visual and interactive manner to the general public. Recent developments indicate that the Web is gradually being transformed from a distributed document system towards a distributed application framework. Several new object-oriented techniques have been introduced as a solution for dynamic processing of information on the Web. The present paper illustrates a research project carried out at the Finnish Geodetic Institute to develop a viewer application (NetGIS) aimed at giving an ordinary Web surfer an opportunity to browse and visualise geographical information (GI) in the form of spatial objects. This new object-based approach to the delivery of GI on the Web enables interactive querying of the properties of an individual geographical object. The application is implemented in Java programming language and the data transfer is based on use of the CORBA standard and on OpenGIS Simple Features Speci® cation.	application framework;browsing;common object request broker architecture;distributed computing;geodetic datum;interactive visualization;internet;java;object-based language;programming language;simple features;world wide web	Jaakko Kähkönen;Lassi Lehto;Tiina Kilpelainen;Tapani Sarjakoski	1999	International Journal of Geographical Information Science	10.1080/136588199241292	web service;web application security;web development;web modeling;data web;geography;computer science;web navigation;web page;data mining;database;multimedia;world wide web;cartography	DB	-38.844655864529294	9.038462624906247	65506
11baf9b19edd02d890b6cb8570a3075b7881b690	the ics-forth rdfsuite: managing voluminous rdf description bases	metadata management;resource description framework;data model;query evaluation;declarative languages;resource availability	Metadata are widely used in order to fully exploit information resources available on corporate intranets or the Internet. The Resource Description Framework (RDF) aims at facilitating the creation and exchange of metadata as any other Web data. The growing number of available information resources and the proliferation of description services in various user communities, lead nowadays to large volumes of RDF metadata. Managing such RDF resource descriptions and schemas with existing low-level APIs and le-based implementations does not ensure fast deployment and easy maintenance of real-scale RDF applications. In this paper, we advocate the use of database technology to support declarative access, as well as, logical and physical independence for voluminous RDF description bases. We present RDFSuite, a suite of tools for RDF validation, storage and querying. Speci cally, we introduce a formal data model for RDF description bases created using multiple schemas. Next, we present the design of a persistent RDF Store (RSSDB) for loading resource descriptions in an ORDBMS by exploring the available RDF schema knowledge. Our approach preserves the exibility of RDF in rening schemas and/or enriching descriptions at any time, whilst it outperforms, both in storage volumes and query execution time, other approaches using a monolithic table to represent resource descriptions and schemas under the form of triples. Last, we brie y present RQL, a declarative language for querying both RDF descriptions and schemas, and sketch query evaluation on top of RSSDB. This work was partially supported by the EC project CWeb (IST-1999-13479) and Mesmuses (IST-2000-26074). Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission by the authors. Semantic Web Workshop 2001 Hongkong, China Copyright by the authors.	cweb;control table;data model;declarative programming;forth;high- and low-level;internet;intranet;object-relational database;rdf schema;rdf query language;resource description framework;run time (program lifecycle phase);semantic web;software deployment;triplestore;while	Sofia Alexaki;Vassilis Christophides;Gregory Karvounarakis;Dimitris Plexousakis;Karsten Tolle	2001			rdf/xml;cwm;data model;computer science;sparql;simple knowledge organization system;rdf;data mining;database;rdf query language;web ontology language;world wide web;information retrieval;rdf schema	Web+IR	-35.709812062778276	5.389234994079972	65509
e82641b714206621f2f7509c4e5a4edee2b4c14c	knowledge based engineering and intelligent personal assistant context in distributed design	distributed system;systeme reparti;design process;ingenierie connaissances;base connaissance;intelligence artificielle;user assistance;sistema repartido;assistance utilisateur;civil engineering;distributed environment;asistencia usuario;artificial intelligence;base conocimiento;genie civil;knowledge based engineering;inteligencia artificial;ingenieria civil;distributed design;knowledge base;knowledge engineering	The work focuses on the problem concerning the application of the KBE approach and its tools in distributed environments. The first period of applying industrial KBE systems did not only show their significant advantages but also revealed their shortcomings. This problem is especially disturbing with distributed design where the potential of communication is very limited. Every design process is closely connected with the designer’s knowledge. In general, this is a very individual knowledge which is stored in the designer’s personal memory. The work represents an attempt of integration of KBE and IPA (Intelligent Personal Assistant), [11], which is the designer’s personal knowledge repository.	compiler;industrial pc;knowledge-based engineering;xfig	Jerzy Pokojski	2006		10.1007/11888598_46	knowledge base;simulation;design process;computer science;artificial intelligence;knowledge engineering;computer security;distributed computing environment	AI	-38.94949202034805	17.430614966567312	65553
8936d2ac516c0f17ce3cd828908a59b10bf004fa	ontology-based user modeling for knowledge management systems	eleccion;representacion conocimientos;ontologie;red www;ingenierie connaissances;lip;reseau web;330 wirtschaft;semantics;user preferences;semantica;semantique;semantic web technology;user profile;internet;levre;comportement utilisateur;inferencia;next generation;preferencia;knowledge management system;labio;world wide web;ontologia;preference;user behavior;knowledge representation;choix;representation connaissances;ontology;choice;inference;comportamiento usuario;user model;knowledge engineering	This paper is presenting a generic ontology-based user modeling architecture, (OntobUM), applied in the context of a Knowledge Management System (KMS). Due to their powerful knowledge representation formalism and associated inference mechanisms, ontology-based systems are emerging as a natural choice for the next generation of KMSs operating in organizational, interorganizational as well as community contexts. User models, often addressed as user profiles, have been included in KMSs mainly as simple ways of capturing the user preferences and/or competencies. We extend this view by including other characteristics of the users relevant in the KM context and we explain the reason for doing this. The proposed user modeling system relies on a user ontology, using Semantic Web technologies, based on the IMS LIP specifications, and it is integrated in an ontology-based KMS called Ontologging. We are presenting a generic framework for implicit and explicit ontology-based user modeling.	knowledge management;knowledge representation and reasoning;management system;semantic web;semantics (computer science);user (computing);user experience;user modeling;user profile	Liana Razmerita;Albert A. Angehrn;Alexander Maedche	2003		10.1007/3-540-44963-9_29	upper ontology;the internet;user modeling;bibliographic ontology;computer science;knowledge management;ontology;artificial intelligence;knowledge engineering;ontology;data mining;database;semantics;ontology-based data integration;world wide web;process ontology;suggested upper merged ontology	Web+IR	-38.05537090583889	12.297535037171036	65617
649f0efb2eabb6c3b2045e7322c727d407ba38c6	decision tree transformation for knowledge warehousing	source model;heterogeneous knowledge;data mining;knowledge harmonization;metamodels;transformation rules;target model;knowledge warehouse;mot	Explicit knowledge extracted from data, formalized tacit knowledge from experts or even knowledge existing in business sources may be in several heterogeneous formal representations and structures: as rules, models, functions, etc. However, a knowledge warehouse should solve this structural heterogeneity before storing knowledge. This requires specific tasks of harmonizing. This paper first presents our proposed definition and architecture of a knowledge warehouse, and then presents some languages for knowledge representations as particular the MOT (Modeling with Object Types) language. In addition, we suggest a metamodel for the MOT, and a metamodel for the explicit knowledge obtained using decision trees technique. As we aim to represent knowledge having different modeling formalisms into MOT, as a unified model, then we suggest a set of transformation rules that assure the move from the decision tree source model into the MOT target model. This work is still in progress, it is currently completed with tranformations for additional.	algorithm;artificial neural network;association rule learning;cluster analysis;decision tree;event condition action;formal language;knowledge management;knowledge representation and reasoning;metamodeling;object type (object-oriented programming);open-source software;semiconductor industry;sketchup 3d warehouse;support vector machine;unified model	Rim Ayadi;Yasser Hachaichi;Saleh Alshomrani;Jamel Feki	2015		10.5220/0005380506160623	software mining;computer science;knowledge management;data science;body of knowledge;knowledge-based systems;open knowledge base connectivity;data mining;database;procedural knowledge;knowledge extraction;domain knowledge	AI	-34.04579525083383	10.89550448173151	65667
9f2962327fa164b74331015f72c3854736545d5f	describing structure and semantics of graphs using an rdf vocabulary	xml schema;semantic information;graph model;dublin core	Mukkai Krishnamoorthy Rensselaer Polytechnic Institute Department of Computer Science email moorthy@cs.rpi.edu ABSTRACT The RDF Graph Modeling Language (RGML) is a W3C RDF vocabulary to describe graph structures, including semantic information associated with a graph. Viewing general graphs as Web resources, RGML defines graph, node, and edge as RDF classes and attributes of graphs (such as label and weight) as RDF properties. Some of these RDF properties establish relationships between graph, node, and edge instances. RDF Statements about graph elements involve subjects, predicates and objects. Subjects and predicates are RDF Resources, while objects are either RDF Resources or RDF Literals. RGML uses the XML Schema datatypes for RDF Literals. RGML can be easily combined with other RDF vocabularies, for example, to add Dublin Core properties. RGML is very useful for describing webgraphs (the structure of a web site), web collections, and sitemaps.	computer science;dublin core;email;graph (discrete mathematics);modeling language;rdf/xml;resource description framework;sitemaps;vocabulary;web resource;xml schema	John R. Punin;Mukkai S. Krishnamoorthy	2001			rdfa;rdf/xml;cwm;semantic web rule language;bibliographic ontology;sparql;simple knowledge organization system;rdf;linked data;xml schema;rdf query language;blank node;rdf schema	Web+IR	-34.75650886111251	7.177362945026212	65731
de5d0ebb29ff0a74178d10a34876085b9a4ef40a	turtleeditor: a web-based rdf editor to support distributed ontology development on repository hosting platforms				Niklas Petersen;Alexandra Similea;Christoph Lange;Steffen Lohmann	2017	Int. J. Semantic Computing	10.1142/S1793351X17400128	ontology;artificial intelligence;machine learning;computer science;rdf;web application	Web+IR	-40.5981295030841	4.689645174906683	65892
0c791f9d72bb83a4cef42a9dffe44f9ed13a9aa6	ontology-based concept similarity in formal concept analysis	ontology mapping;ontology engineering;semantic web;similarity reasoning;domain ontologies;domain ontology;formal concept analysis	Both domain ontologies and Formal Concept Analysis (FCA) aim at modeling concepts, although with different purposes. In the literature, a promising research area concerns the role of FCA in ontology engineering, in particular, in supporting the critical task of reusing independently developed domain ontologies. With this regard, the possibility of evaluating concept similarity is acquiring an increasing relevance, since it allows the identification of different concepts that are semantically close. In this paper, an ontology-based method for assessing similarity between FCA concepts is proposed. Such a method is intended to support the ontology engineer in difficult activities that are becoming fundamental in the development of the Semantic Web, such us ontology merging and ontology mapping and, in particular, it can be used in parallel to existing semi-automatic tools relying on FCA. 2005 Elsevier Inc. All rights reserved.	cluster analysis;component-based software engineering;conceptual clustering;formal concept analysis;ontology (information science);ontology engineering;ontology merging;relevance;semantic web;semantic integration;semantic network;semiconductor industry	Anna Formica	2006	Inf. Sci.	10.1016/j.ins.2005.11.014	upper ontology;conceptualization;open biomedical ontologies;ontology alignment;semantic similarity;semantic integration;ontology components;bibliographic ontology;ontology inference layer;computer science;formal concept analysis;knowledge management;ontology;machine learning;semantic web;data mining;ontology-based data integration;owl-s;information retrieval;process ontology;suggested upper merged ontology	AI	-43.786284859760904	6.512489837293071	65979
d43c2900a7f677b9640953951547d1e2d205a4ae	a taxonomy of geospatial services for global service discovery and interoperability	classification node;computadora;tratamiento datos;computers;planeta tierra;systeme information geographique;ordinateur;technology;earth;taxinomie;data processing;traitement donnee;web service;taxonomia;classification;internet;global earth observation system of systems;lessons learned;geographic information systems;technologie;taxonomy;planete terre;monde;infrastructures;mundo;service discovery;knowledge representation;geospatial web service;clasificacion;global;infrastructure;uniform resource name;classification scheme;tecnologia	Geospatial service taxonomies represent the knowledge about the characteristics of geospatial services from the enterprise, computational, information, engineering, infrastructure, or technology viewpoints. This paper presents a lightweight taxonomy of geospatial services with the aim of promoting the global sharing of and interoperability among geospatial service instances. This taxonomy focuses on the knowledge connected with service interoperability. As a hierarchical taxonomy, it consists of six layers: service category, service type, version, profile, binding and uniform resource name (URN), from the root down to the leaves. Each layer is composed of classification nodes, with each node identifying one classification concept. Each concept, with a concrete semantic meaning, can be used to classify service instances. The application of this classification scheme to the Global Earth Observation System of Systems (GEOSS) Component and Service registry is also introduced. The results of this study may lead to the further development of service taxonomy to thoroughly capture the knowledge about geospatial services. The lessons learned may be useful to others representing and manipulating geoscientific knowledge. & 2008 Elsevier Ltd. All rights reserved.	comparison and contrast of classification schemes in linguistics and metadata;geoweb;information engineering;interoperability;knowledge representation and reasoning;service discovery;system of systems;taxonomy (general);uniform resource identifier;web service	Yuqi Bai;Liping Di;Yaxing Wei	2009	Computers & Geosciences	10.1016/j.cageo.2007.12.018	web service;data processing;classification scheme;computer science;global earth observation system of systems;service design;data mining;database;earth;uniform resource name;service discovery;world wide web;taxonomy;technology;web coverage service	HPC	-38.254852782977004	11.896698108271131	66217
f484a2a35b842fafbc9996ede482ca79efc385be	ontology alignment quality: a framework and tool for validation	alviz;information quality;information visualization ontologies;semantic web technologies;ontology alignment	Recently semantic web technologies, such as ontologies, have been proposed as key enablers for integrating heterogeneous data schemas in business and governmental systems. Algorithms designed to align different but related ontologies have become necessary as differing ontologies proliferate. The process of ontology alignment seeks to find corresponding entities in a second ontology with the same or the closest meaning for each entity in a single ontology. This research is motivated by the need to provide tools and techniques to support the task of validating ontology alignment statements, since it cannot be guaranteed that the results from automated tools are accurate. The authors present a framework for understanding ontology alignment quality and describe how AlViz, a tool for visual ontology alignment, may be used to improve the quality of alignment results. An experiment was undertaken to test the claim that AlViz supports the task of validating ontology alignments. A promising result found that the tool has potential for identifying missing alignments and for rejecting false alignments.	algorithm;align (company);entity;ontology (information science);ontology alignment;semantic web	Jennifer Sampson;John Krogstie;Csaba Veres	2011	IJISMD	10.4018/jismd.2011070101	upper ontology;open biomedical ontologies;ontology alignment;bibliographic ontology;computer science;knowledge management;ontology;data mining;database;information quality;ontology-based data integration;world wide web;owl-s;information retrieval;process ontology;suggested upper merged ontology	AI	-44.104964001042696	6.769004240017587	66414
5a7389f98ba5fc1f75fe52da2f3b8d7873830b39	suits4rdf: incremental query construction for the semantic web	query language;semantic web technology;semantic web;information seeking	With the advance of the Semantic Web technology, increasing data will be annotated with computer understandable structures (i.e. RDF and OWL), which allow us to use more expressive queries to improve our ability in information seeking. However, constructing a structured query is a laborious process, as a user has to master the query language as well as the underlying schema of the queried data. In this demo, we introduce SUITS4RDF, a novel interface for constructing structured queries for the Semantic Web. It allows users to start with arbitrary keyword queries and to enrich them incrementally with an arbitrary but valid structure, using computer suggested queries or query components. This interface allows querying the Semantic Web conveniently and efficiently, while enabling users to express their intent precisely.	information seeking;query language;resource description framework;semantic web	Enrico Minack;Wolf Siberski;Gideon Zenz;Xuan Zhou	2008			semantic computing;query expansion;web query classification;semantic web rule language;data web;semantic search;semantic grid;web standards;computer science;sparql;artificial intelligence;semantic web;social semantic web;linked data;semantic web stack;database;rdf query language;semantic technology;web search query;world wide web;owl-s;information retrieval;semantic analytics;query language	Web+IR	-35.720521027939235	5.9333986563586425	66457
9837aede04045d1649f753f5cf124d7eefb6c743	to implement an open-mas architecture for semantic web services discovery: what kind of p2p protocol do we need?	performance evaluation;p2p protocol;semantic web services;web services discovery;open mas	Recently, several models have been proposed to design distributed and collaborative infrastructures for web services based systems. In this area, Peer-to-Peer (P2P) networks and Multi-Agents Systems offer many techniques for web services discovery and composition. However, both of the two combinations (P2P/web services and MAS/web services) have suffered from some problems. This article presents a generic P2P/MAS architecture for semantic web services discovery. It tries to merge these two distributed technologies and demonstrate how P2P networks can implement open-MAS architectures to build a collaborative distributed system. The main objective of this article is to find the most appropriate P2P protocol to make such systems. Before the presentation of the proposed architecture, it already presents a background of P2P categories and models. After, it chooses four different P2P protocols wherever it analyzes and discusses, for each one, the stabilization and traffic generation of the network. To Implement an Open-MAS Architecture for Semantic Web Services Discovery: What Kind of P2P Protocol Do We Need?	distributed computing;peer-to-peer;semantic web service;web services discovery	Mohamed Gharzouli;Derdour Makhlouf	2014	IJATS	10.4018/ijats.2014070103	web service;web application security;web development;web modeling;data web;web mapping;web design;web standards;computer science;ws-policy;service-oriented architecture;semantic web;social semantic web;data mining;ws-addressing;semantic web stack;database;services computing;web intelligence;ws-i basic profile;web 2.0;world wide web;universal description discovery and integration	Networks	-43.82514587494682	12.516012675685687	66560
539e32edaf040a94226965a401f85dc9a615590a	design and implementation of integration framework for terminal-oriented mobile gis data collection	mobile calculation;data transmission;data transmission mechanism;nanjing municipal facilities collection system terminal oriented mobile gis data collection spatial data collection mobile calculation integration framework version based data distribution retrieval mechanism data transmission mechanism offline transmission environment mobile terminal;spatial data;nanjing municipal facilities collection system;information retrieval;pervasive computing;data collection;integration environment;data communication;data distribution;version based data distribution;computer architecture;design and implementation;global positioning system;geographic information systems;terminal oriented mobile gis data collection;mobile gis;offline transmission environment;sun;retrieval mechanism;spatial data collection;terminal data collection;integration framework;mobile computing geographic information systems information retrieval;mobile computing;transmission model terminal data collection mobile gis integration environment;mobile terminal;geographic information systems mobile computing information retrieval data communication hardware global positioning system embedded software computer architecture pervasive computing sun;embedded software;hardware;transmission model	Based on its characteristics of quickness, portability and friendly interaction interface, mobile GIS for spatial data collection has become an important developing direction of GIS as well as a major method for GIS data collection. However, the current mobile GIS data collection studies mainly focus on the implementation of mobile calculation but overlook the integrative mechanism for mobile data collection. Based on the background information that mentioned above, this paper launched a design and implementation research on integration framework for terminal-oriented mobile GIS data collection. The integration framework mainly includes two parts: first, a version-based data distribution and retrieval mechanism; the second, a data transmission mechanism called offline transmission environment which applies to different data transmission modes between server and mobile terminal. Through Nanjing Municipal Facilities Collection System, this paper gives a detailed description about the implementation of this proposed integration framework to validate the feasibility and validity of it.	geographic information system;mobile phone;online and offline;server (computing)	Yuting Sun;Shuliang Zhang;Yiming Zhang;Guonian Lv	2008	2008 The 3rd International Conference on Grid and Pervasive Computing - Workshops	10.1109/GPC.WORKSHOPS.2008.28	distributed gis;enterprise gis;global positioning system;embedded software;computer science;operating system;data mining;database;spatial analysis;am/fm/gis;mobile computing;world wide web;ubiquitous computing;data transmission;data collection	Robotics	-35.46165230546401	14.849678993857099	66577
e24dd5361101599dcd9ac4b0883fbec9670b3ab3	xml-based advanced uddi search mechanism for b2b integration	microstrip;search engines;web services;markup languages	Exploring an appropriate business application published as a Web Service in the UDDI registry is a critical issue. Search for such an application should be effective in terms of time and uniformed in terms of interfaces. In this paper, an XML-based UDDI exploring engine that provides developers with standard interfaces for efficiently searching business and service information in single or multiple UDDI registries is proposed. We refer this engine as ??Business Explorer for Web Services?? (BE4WS). It is based on the proposed UDDI Search Markup Language (USML) for carrying a search request including multiple queries, key words, UDDI sources, and aggregation operators. The Advanced UDDI Search Engine (AUSE), a core component of BE4WS, processes the USMLr equest and performs advanced exploring. The basic idea of an AUSE is to aggregate search results from different UDDI registries based on the USMLreq uest and its supporting intelligent search facilities. Examples of these facilities are Instant Notification Broker, UDDI Source Dispatching Broker, and Information Aggregation Broker, all of which have both priori knowledge of the meanings of specific categories and the ability to crossreference across multiple categories. An aggregation example using BE4WS is presented to create a shipping agent for B2B integration. The conclusions are provided.	xml	Liang-Jie Zhang;Haifei Li;Henry Chang;Tian Chao	2002		10.1109/WECWIS.2002.10005	web service;computer science;data mining;database;markup language;world wide web;search engine	AI	-44.90603192406187	12.657885707796245	66580
26032d0114ad3e4d32f5c293982ebb0b24672946	prova: rule-based java scripting for distributed web applications: a case study in bioinformatics	distributed system;ontologie;base donnee;sistema experto;systeme reparti;red www;integration information;xml language;rule based;interrogation base donnee;reseau web;biologia molecular;database;service web;interrogacion base datos;base dato;bioinformatique;base connaissance;langage java;web service;structure proteine;protein structure;information integration;sistema repartido;col;internet;molecular biology;estructura datos;algorithme reparti;integracion informacion;event condition action;xml document;world wide web;base conocimiento;ontologia;lenguaje java;algoritmo repartido;structure donnee;bioinformatica;systeme expert;distributed algorithm;data structure;ontology;database query;langage xml;lenguaje xml;agent programming;servicio web;java language;bioinformatics;knowledge base;biologie moleculaire;expert system	Prova is a language for rule-based Java scripting to support information integration and agent programming on the web. Prova integrates Java with derivation and reaction rules supporting message exchange with various comminication frameworks. Prova supports transparent access to databases, retrieval of URLs, access to web services, and querying of XML documents. We briefly illustrate Prova and show how to implement a distributed bioinformatics application, which includes access to an ontology stored in a database and to XML data for protein structures. Finally, we compare Prova to other event-condition-action rule systems.	bioinformatics;database;event condition action;java;logic programming;prova;web service;xml	Alexander Kozlenkov;Rafael Peñaloza;Vivek Nigam;Loïc Royer;Gihan Dawelbait;Michael Schroeder	2006		10.1007/11896548_68	distributed algorithm;knowledge base;xml;data structure;computer science;artificial intelligence;operating system;ontology;data mining;database;programming language;world wide web;expert system	DB	-36.63132229153318	12.262447811973333	66933
72e8be634656c6cfba9a4ca1e42030952a28ecff	on combining a semantic engine and flexible network policies for p2p knowledge sharing networks	p2p knowledge;knowledge sharing;content sharing;semantic engine component;semantic engine exploit;peer-based knowledge sharing;flexible network policies;effective content retrieval;p2p network policy;different peer;different approach;content retrieval;semantic engine;semantic web;data mining;p2p	Peer-to-peer (P2P) systems have recently become popular for content sharing, and a number of different approaches have been studied to perform effective content retrieval in such networks. We discuss how conventional P2P network policies for content retrieval could be combined with a semantic engine component to enforce peer-based knowledge sharing. The semantic engine exploits peer ontologies for providing a semantically rich representation of peer knowledge, and matching techniques for comparing ontologies of different peers for knowledge sharing and discovery purposes.	ontology (information science);peer-to-peer	Silvana Castano;Alfio Ferrara;Stefano Montanelli;Elena Pagani;Gian Paolo Rossi;Stefano Tebaldi	2004	Proceedings. 15th International Workshop on Database and Expert Systems Applications, 2004.	10.1109/DEXA.2004.1333529	computer science;semantic web;peer-to-peer;data mining;database;world wide web;information retrieval	Web+IR	-41.61052617974768	8.866682458010095	67225
6d939b4500a811d624a6530032a2960591c48e62	evaluating the x3d schema with semantic web tools	computer aided design;xml schema;long period;geographic information system;data model;x3d;semantic web;ontology	X3D has been evolving for nearly 20 years (counting the VRML era). This long period, covering a wide range of uses (Computer-Aided Design, medical, Geographic Information System ...) has led to a lot of possibly inconsistent or even conflicting features. By reasoning on the X3D data model, it is possible to detect such issues, list them and propose solutions.  In order to allow such a study of the model, we propose a conversion of the current XML schema to an ontology and then a systematic detection of possible issues. We then categorize them and propose solutions to fix them.	categorization;computer-aided design;data model;geographic information system;lambda calculus;semantic web;semantic publishing;vrml;x3d;xml schema	Marc Petit;Henry Boccon-Gibod;Christophe Mouton	2012		10.1145/2338714.2338737	semi-structured model;computer science;document structure description;data mining;xml schema;database;world wide web;xml schema editor	DB	-33.81550836327926	9.028782149045032	67330
5105472bdd665e7b10b70492ca495b8c95c6fa35	a reference model of an instrument for quality measurement of semantic is standards	bis business information services;instruments;information systems;semantics instruments standards unified modeling language terminology context internet;standards;semantics;ts technical sciences;standard semantics reference model quality measurement information system is;communication information;internet;unified modeling language;information society;terminology;informatics;context	This study describes the design of a reference model for an instrument to measure quality of semantic Information System (IS) standards. This design satisfies requirements gathered among potential users, in a previous study. The reference model features three layers: concerned with quality, semantic IS standards, and the instrumentation, respectively. It serves as a basis for implementation of a quality measuring instrument.	information system;reference model;requirement	Erwin Folmer;Paul Oude Luttighuis;Jos van Hillegersberg	2011	2011 7th International Conference on Standardization and Innovation in Information Technology (SIIT)	10.1109/SIIT.2011.6083607	idef1x;unified modeling language;the internet;computer science;data mining;database;semantics;sociology;information quality;terminology;informatics;information retrieval;information system	SE	-45.03828326993807	8.791562587431233	67373
a9045be228a5b7809a3bc27ed187561c601421e1	a description and retrieval model for web services including extended semantic and commercial attributes	standards;pricing;vocabulary;semantics;web services semantics standards pricing xml vocabulary;service discovery web services semantic web service description;service description;web services;xml;semantic web;service discovery	Commercially available web services pose certain challenges to the customer. Whereas the technical service description is usually well defined and published, the semantic and especially the commercial description does often not reach the necessary depth. Therefore, identifying the most suitable web service from a technical, semantic and commercial perspective is often a difficult quest. This research develops a holistic web service description language integrating the semantic and commercial perspective and based on XML. The newly defined description rules for web services enable a requestor to identify the most suitable web services according to a given set of parameters. The model is tested with a prototype and the proof-of-concept shows that it is a fully functional description. Applying this model on a broad scale would lead to better decisions in web service selection, especially if non-technical staff is involved.	holism;prototype;usability;web service;xml	Jonas Keppeler;Philipp Brune;Heiko Gewald	2014	2014 IEEE 8th International Symposium on Service Oriented System Engineering	10.1109/SOSE.2014.34	pricing;web service;web application security;web development;web modeling;xml;semantic web rule language;data web;web mapping;web standards;computer science;operating system;ws-policy;service-oriented architecture;semantic web;social semantic web;ws-addressing;semantic web stack;database;semantics;service discovery;web intelligence;ws-i basic profile;web 2.0;law;world wide web;devices profile for web services;information retrieval;universal description discovery and integration	Web+IR	-44.919233883075826	13.384219902529278	67592
755632528ed564e2d899dd8f08b421b7ebe07a0b	semantic modeling and design patterns for asynchronous events in web service interaction	message routing;reachability;conceptual model;object oriented programming;message routing semantic modeling semantic design patterns asynchronous events set theoretic semantics web service event subscription ws eventing ws notification event broker design patterns visibility reachability;web service;set theory;event broker design patterns;web service event subscription;semantic model;visibility;web services message passing object oriented programming set theory;semantic modeling;design pattern;web services;message passing;set theoretic semantics;asynchronous events;semantic design patterns;ws eventing;web services subscriptions routing proposals protocols prototypes information analysis pattern analysis telecommunication computing computer languages;ws notification	This paper proposes a conceptual model for modeling static and dynamic event sources by reusing the information in WSDL. A set-theoretic semantics for event subscription is introduced, based on which two metrics, recall and precision, are proposed to measure the accuracy of event subscriptions. We discuss the accuracy of several event subscription strategies under the framework of Web service event subscription standard proposals (WS-Eventing and WS-Notification). Four major types of event broker design patterns are discussed based on two visibility/reachability factors: if sink knows the source and if source can deliver events directly to the sink. The implication on the broker state and message routing is studied in this analysis. A prototype implementation indicated that these design patterns are feasible	complex event processing;design pattern;event (computing);precision and recall;prototype;reachability;routing;set theory;stateless protocol;ws-addressing;ws-security;ws-trust;web services description language;web service	Li Li;Wu Chou;Feng Liu;Dan Zhuo	2006	2006 IEEE International Conference on Web Services (ICWS'06)	10.1109/ICWS.2006.117	web service;computer science;complex event processing;database;distributed computing;programming language;law;world wide web	DB	-42.23556839874126	14.974584225152398	67656
f784823c828188d8a2cd5406e4cfe39fbfbc6623	from text to knowledge for the semantic web: the ontotext project.	first year;semantic web	This paper presents the general objectives of the ONTOTEXT project (From Text to Knowledge for the Semantic Web), and the activities carried out during the first year of its development cycle. First, the task of annotating huge amounts of textual data (e.g. those available on the Web or in local document collections) will be introduced, focusing on its importance in order to enhance the interoperability of such data through ontology-based reasoning. Then, the main issues related to the annotation task will be discussed. These include the choice of an adequate formalism to capture and describe different types of relevant information contained in a text, and the adaptation of existing languagespecific markup formalisms to a new language (Italian in our case). Finally, the results of our experience in the concrete annotation of information about people and temporal expressions for the Italian Content Annotation Bank (I-CAB) being developed at ITC-irst and CELCT will be reported.	ace;automated reasoning;benchmark (computing);database normalization;entity;information extraction;interoperability;knowledge base;markup language;ontology (information science);precision and recall;semantic web;semantics (computer science);temporal expressions;text corpus;web ontology language;world wide web	Bernardo Magnini;Matteo Negri;Emanuele Pianta;Lorenza Romano;Manuela Speranza;Luciano Serafini;Christian Girardi;Valentina Bartalesi Lenzi;Rachele Sprugnoli	2005			semantic computing;web development;semantic web rule language;data web;semantic search;semantic grid;web standards;computer science;artificial intelligence;semantic web;social semantic web;semantic web stack;database;web intelligence;semantic technology;world wide web;owl-s;website parse template;information retrieval;semantic analytics	NLP	-40.72573179855975	6.8648226622992325	67748
ad786fdb63738bda343d24fefc95445fa42ae20c	shaping a cbr view with xml	base relacional dato;raisonnement base sur cas;razonamiento fundado sobre caso;systeme intelligent;red www;case base reasoning;information retrieval;sistema inteligente;distributed computing;relational database;recherche information;knowledge systems;intelligent system;base donnee relationnelle;world wide web;reseau www;technical report;recuperacion informacion;computer science;information system;case based reasoning;systeme information;sistema informacion;knowledge base	Case Based Reasoning has found increasing application on the Internet as an assistant in Internet commerce stores and as a reasoning agent for online technical support. The strength of CBR in this area stems from its reuse of the knowledge base associated with a particular application, thus providing an ideal way to make personalised configuration or technical information available to the Internet user. Since case data may be one aspect of a company’s entire corporate knowledge system, it is important to integrate case data easily within a company’s IT infrastructure, using industry specific vocabulary. We suggest XML as the likely candidate to provide such integration. Some applications have already begun to use XML as a case representation language. We review these and present the idea of a standard case view in XML that can work with the vocabularies or namespaces being developed by specific industries. Earlier research has produced version 1.0 of a Case Based Mark-up Language which attempts to mark-up cases in XML to enable distributed computing. The drawbacks of this implementation are outlined in this paper as well as the developments in XML that allow us to produce an XML “View” of a company’s knowledge system. We will detail the benefits of our system for industry in general in terms of extensibility, ease of reuse and interoperability.	case-based reasoning;distributed computing;e-commerce;extensibility;information system;internet;interoperability;knowledge base;knowledge-based systems;mathematical model;noise shaping;relational database;requirement;stemming;technical support;vocabulary;xml	Conor Hayes;Padraig Cunningham	1999		10.1007/3-540-48508-2_34	xml validation;binary xml;case-based reasoning;knowledge base;xml schema;relational database;computer science;artificial intelligence;technical report;operating system;xml framework;data mining;xml schema;database;distributed computing;ebxml;world wide web;xml schema editor;computer security;cxml;information system;algorithm;efficient xml interchange	Web+IR	-39.685938184191826	12.686498381807128	67813
679f3229364c76edfb881cad64c03105ca6917c7	usando ontologias, serviços web semânticos e agentes móveis no desenvolvimento baseado em componentes		This paper presents an approach that combines Ontologies, Semantic Web Services and Mobile Agents, for the Component-Based Software Development. The Ontologies are employed to improve the problem domain analysis, and to get software components with a semantic description, which may be reused in a wide variety of applications. The Semantic Web Services are used as software components distributed over the Internet, and are composed to perform complex application tasks. The Mobile Agents manager the use of the Semantic Web Services, and can move through the network nodes in order to find, to composite and to monitor these services.	component-based software engineering;domain analysis;internet;mobile agent;ontology (information science);problem domain;semantic web service;software development	Luiz H. Z. Santana;Antônio Francisco do Prado;Wanderley Lopes de Souza;Mauro Biajiz	2007			computer science	Web+IR	-47.87889185121242	14.482734257903097	67824
35ac12dad8f8539a1cfb78760f54fa0a39de4568	relações em linguagens de autoria hipermídia: aumentando reuso e expressividade		Muchaluat-Saade, Débora Christina. Relations in Hypermedia Authoring Languages: Improving Reuse and Expressiveness. Rio de Janeiro, 2003. 215p. Ph.D. Thesis Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro. This work is related to hypermedia authoring and execution environments, and its main focus is declarative document authoring. Starting from studies about architectural description languages (ADL), which are used for specifying software system architectures, this thesis identified facilities found in ADLs that could be applied to the hypermedia domain, with advantages. Aiming at improving the expressiveness and reuse in the specification of relations in hypermedia authoring languages, this work introduced the concept of hypermedia connector, which has a role similar to ADL connectors, that is, representing relations among components of a document. Besides connectors, this work also introduced the concept of hypermedia composite template, which has a role similar to architectural styles in ADLs, that is, representing generic structures of nodes and links that can be reused in several distinct documents. As a validation of the proposed concepts, the 2.0 version of the NCL – Nested Context Language – hypermedia authoring language, based on the NCM – Nested Context Model – conceptual model, was developed and integrated to the HyperProp hypermedia system, incorporating the new facilities. The NCL 2.0 language was developed using a modular structure, following the principles adopted by the W3C – World-Wide Web Consortium. Thus, its modules for the specification of connectors and templates, respectively called XConnector and XTemplate, can be incorporated to other existent languages, such as XLink, XHTML and SMIL, used for web document authoring. This thesis also proposes extensions to these languages, exemplified by the incorporation of XConnector and XTemplate facilities into the XLink standard.	consortium;description logic;hypermedia;nested context language;software system;synchronized multimedia integration language;web page;winsock;world wide web;xhtml;xlink	Débora C. Muchaluat-Saade	2003				SE	-41.93070490381317	4.80898734764277	68159
472e4a38a609860c83229c714a8c48878435dc1d	dsmw: a distributed infrastructure for the cooperative edition of semantic wiki documents	distribution;replication;publish subscribe;communication channels;change propagation;semantic wiki	DSMW is a distributed semantic wiki that offers new collaboration modes to semantic wiki users and supports dataflow-oriented processes.  DSMW is an extension to Semantic Mediawiki (SMW), it allows to create a network of SMW servers that share common semantic wiki pages. DSMW users can create communication channels between servers and use a publish-subscribe approach to manage the change propagation. DSMW synchronizes concurrent updates of shared semantic pages to ensure their consistency.	dataflow;mediawiki;publish–subscribe pattern;software propagation;wiki	Hala Skaf-Molli;Gérôme Canals;Pascal Molli	2010		10.1145/1860559.1860598	distribution;personal wiki;replication;computer science;semantic web stack;database;publish–subscribe pattern;semantic technology;world wide web;information retrieval;channel	OS	-41.12872585937498	10.120448110115701	68341
2bd6ee71b24f8acb65867a0c9ddecb9fe06e7f52	improving the accessibility and efficiency of e-government processes	semantic web service;e government;ambient intelligence;semantics;electronic government ontologies semantics resource description framework web services;resource description framework;e government process;ontologies artificial intelligence;internet;web application;online collaborative workflow e government process ambient intelligence middleware e government services sparql eligibility criteria web application;eligibility criteria;electronic government;web services;symbiotic computing;middleware;ontologies;sparql;e government symbiotic computing ontology semantic web service;e government services;online collaborative workflow;ontology;government data processing;domain specificity;public administration government data processing internet middleware ontologies artificial intelligence;public administration	This paper focuses on the conditions for obtaining real symbiotic applications in the e-Government domain. Specific services have to be delivered to the citizens with the help of civil servants. Ambient intelligence can help the interaction between citizens and specific services only if a rich middleware based on semantics has been previously designed. We propose an approach for improving the accessibility and efficiency of e-Government processes. The most innovative aspects of our approach are the use of semantics to describe e-Government services, and the use of SPARQL to define their eligibility criteria. We present a Web application that, based on our approach, automate the selection of services for citizens, and streamlines the associated e-Government process through an on-line collaborative workflow.	accessibility;ambient intelligence;e-government;middleware;online and offline;sparql;semantic web;web 2.0;web application	Claude Moulin;Marco Luca Sbodio	2010	9th IEEE International Conference on Cognitive Informatics (ICCI'10)	10.1109/COGINF.2010.5599835	computer science;data mining;database;world wide web	Robotics	-44.426561483517055	10.357298653474215	68417
38df1c8d2d49cb38e220e641d25fd1ef1f3ef000	semantic metadata for the integration of web-based data for electronic commerce	representation;information resources;protocols;electronic commerce;metadata;electronic commerce data mining html web and internet services protocols computer science web sites discussion forums vehicles advertising;web and internet services;discussion forums;data exchange;data mining;integration;web based heterogeneous data source integration;semistructured data;html;semantic metadata;automatic processing;modeling assumptions;internet;semantic meta data;web sites;web based heterogeneous data source integration semantic meta data electronic commerce internet global marketplace data exchange automatic processing representation modeling assumptions explicit information implicitly described semistructured data;meta data;explicit information;implicitly described semistructured data;vehicles;computer science;ontology;context;global marketplace;meta data electronic commerce information resources;advertising;heterogeneous data sources	Today, the Internet can be seen as a global marketplace populated by a huge number of providers and consumers that exchange data from a wide range of domains. A combination of data from different sources for further automatic processing is often hindered by differences in the underlying modeling assumptions and representation. In addition, the available sources are in most cases semistructured, i.e., provide no fixed and explicitly specified schema. Therefore, an integrated use of Web-based data requires explicit information about its organization and meaning. In this paper we present a representation model wellsuited for explicit description of implicitly described semistructured data, and show how this model can be used for the integration of heterogeneous data sources from the Web.	data model;e-commerce;internet;java;mix;point and click;population;programming paradigm;prototype;world wide web	Christof Bornhövd	1999		10.1109/WECWIS.1999.788202	computer science;data mining;database;world wide web;data mapping	DB	-38.31814195434951	9.562977758782752	68613
edf4ca934e2aa60950ba1fd12782807923f4385d	hps5dsws: a hybrid p2p strategy of the distributed discovery mechanism for semantic web services	web services knowledge representation languages peer to peer computing semantic web;similarity measures semantic web services sws distributed discovery p2p computing jxta ontology alignment;semantic web services sws;p2p computing;hps5dsws owl s p2p technology sw complexity semantic web services scalable p2p approach heterogeneous environment distributed environment distributed discovery mechanism large scale distributed systems heterogeneous applications distributed applications automated interactions distributed software components hybrid p2p strategy;knowledge representation languages;web services;peer to peer computing ontologies web services semantics terminology owl;jxta;semantic web;similarity measures;distributed discovery;peer to peer computing;ontology alignment	Web services are the new generation of distributed software components. They are important for deploying automated interactions between distributed and heterogeneous applications of large-scale distributed systems. But with the evolution of the number of services available on the Web, in organizations and their large-scale use, a discovery mechanism of such a Web service in a distributed and heterogeneous environment has become a real challenge. In this paper, we describe a scalable P2P approach for automatic discovery of Semantic Web services (SWs), which supports the complexity of both SWs and task. Our architecture is based on P2P technology that has proven its effectiveness and robustness as distributed system. The particularity of our approach is to place the alignment of OWL-S in the heart of this architecture.	component-based software engineering;distributed computing;heterogeneous computing;heuristic (computer science);interaction;jxta;mathematical optimization;metaheuristic;owl-s;peer-to-peer;prototype;quality of service;response time (technology);scalability;semantic web service;semantic interoperability;workspace;world wide web	Adel Boukhadra;Karima Benatchba;Amar Balla	2013	2013 Eighth International Conference on P2P, Parallel, Grid, Cloud and Internet Computing	10.1109/3PGCIC.2013.12	web service;middleware;web development;web modeling;data web;web mapping;web standards;computer science;ws-policy;service-oriented architecture;semantic web;social semantic web;distributed web crawling;semantic web stack;database;services computing;web intelligence;web 2.0;world wide web;owl-s;information retrieval	HPC	-43.84933412507915	12.69513663687992	68758
04008333a9dcaf3c05ecff38b54e4a830bc1e770	agent service matchmaking algorithm for autonomic element with semantic and qos constraints	service description model;semantic similarity;user evaluation;multi agent system;agent service matchmaking;qos constraints;satisfiability;simulation experiment;autonomic element;semantic similar;autonomic computing	Agent service description and matchmaking problem for autonomic element has been taken as one of the most important issue in the field of autonomic computing based on agent and multi-agent system. Considering the semantic and QoS for capability description of agent service are two important issues during matchmaking, the factors of semantic and QoS during matchmaking are considered together, and an agent service description model named ASDM_SQ is proposed. On basis of this model, a matchmaking algorithm with semantic and QoS constraints named ASMA_SQ is presented to find the agent service satisfied both the given semantic similarity threshold and optimal QoS performance. The proposed algorithm lies over two fundamental processes: semantic similarity matchmaking and QoS matchmaking. During QoS matchmaking, evaluation mechanism of confidence of individual QoS attributes, i.e. fidelity factor is introduced to overcome drawbacks such as subjectiveness and unfairness and improve the self-configuration capability for agent element. Simulation experiments demonstrate the effective and correction of our algorithm for agent service matchmaking, and perform better QoS performance than other existing algorithm, which can further show that our algorithm has better compromise between attribute quality and users’ evaluation when selecting agent service. 2009 Elsevier B.V. All rights reserved.	algorithm;autonomic computing;computer simulation;experiment;multi-agent system;quality of service;semantic similarity;set theory;simulation	Zhang Kun;Xu Manwu;Zhang Hong;Xu Jian	2010	Knowl.-Based Syst.	10.1016/j.knosys.2009.07.011	semantic similarity;computer science;knowledge management;artificial intelligence;database;world wide web;autonomic computing;satisfiability	AI	-44.33118285799224	15.839551557585246	68998
1a764f02584b46f1884d0b6e839bc041a2a306f5	the text/enriched mime content-type	software platform;data type	"""Status of this Memo This memo provides information for the Internet community. This memo does not specify an Internet standard of any kind. Distribution of this memo is unlimited. Abstract MIME [RFC-1341, RFC-1521] defines a format and general framework for the representation of a wide variety of data types in Internet mail. This document defines one particular type of MIME data, the text/enriched type, a refinement of the """"text/richtext"""" type defined in RFC 1341. The text/enriched MIME type is intended to facilitate the wider interoperation of simple enriched text across a wide variety of hardware and software platforms. In order to promote the wider interoperability of simple formatted text, this document defines an extremely simple subtype of the MIME content-type """"text"""", the """"text/enriched"""" subtype. This subtype was designed to meet the following criteria: 1. The syntax must be extremely simple to parse, so that even teletype-oriented mail systems can easily strip away the formatting information and leave only the readable text. 2. The syntax must be extensible to allow for new formatting commands that are deemed essential for some application. 3. If the character set in use is ASCII or an 8-bit ASCII superset, then the raw form of the data must be readable enough to be largely unobjectionable in the event that it is displayed on the screen of the user of a non-MIME-conformant mail reader. 4. The capabilities must be extremely limited, to ensure that it can represent no more than is likely to be representable by the user's primary word processor. While this limits what can be sent, it increases the likelihood that what is sent can be properly displayed. This document defines a new MIME content-type, """"text/enriched"""". The content-type line for this type may have one optional parameter, the """"charset"""" parameter, with the same values permitted for the """"text/plain"""" MIME content-type. The syntax of """"text/enriched"""" is very simple. It represents text in a single character set-US-ASCII by default, although a different character set can be specified by the use of the """"charset"""" parameter. (The semantics of text/enriched in non-ASCII character sets are discussed later in this document.) All characters represent themselves, with the exception of the """"<"""" character (ASCII 60), which is used to mark the beginning of a formatting command. Formatting instructions consist of formatting commands surrounded by angle brackets (""""<>"""", ASCII 60 and 62). Each formatting command may be no more than 60 characters …"""	8-bit;character encoding;extended ascii;human-readable medium;internet;interoperability;interoperation;parsing;refinement (computing)	Nathaniel S. Borenstein	1994	RFC	10.17487/RFC1563	computer science;database;internet privacy;world wide web;comma-separated values	PL	-39.995821550560294	11.648179444128072	69002
50f9c25f6fa26b9e05959e09f5e1cc23de96be69	a formal taxonomy to improve data defect description		Data quality assessment outcomes are essential for analytical processes, especially for big data environment. Its efficiency and efficacy depends on automated solutions, which are determined by understanding the problem associated with each data defect. Despite the considerable number of works that describe data defects regarding to accuracy, completeness and consistency, there is a significant heterogeneity of terminology, nomenclature, description depth and number of examined defects. To cover this gap, this work reports a taxonomy that organizes data defects according to a three-step methodology. The proposed taxonomy enhances the descriptions and coverage of defects with regard to the related works, and also supports certain requirements of data quality assessment, including the design of semi-supervised solutions to data defect detection.	software bug	João Marcelo Borovina Josko;Marcio K. Oikawa;João Eduardo Ferreira	2016		10.1007/978-3-319-32055-7_25	database	NLP	-47.663293454789944	6.172762079478577	69213
d4360c21a7cb1197d2da680b0ad7e9ccbeff239c	an approach for systematic definitions construction based on ontological analysis		This research motivation is to find a way to minimize the distance between business concepts and their respective representations in Information Technology artifacts, specially conceptual models. This distance leads to inconsistencies, ambiguities and implementation issues. Our approach is based on ontological analysis, which considers each concept according to its nature, capturing more precisely its essence and generally improving semantic richness and precision. Our main goal is to help in the process of systematic concepts definitions construction, contributing to generate more consistent definitions and associated conceptual modeling artifacts. The foundational ontology used as a theoretical reference is the Unified Foundational Ontology (UFO) which has been, in the last decade, successfully used to evaluate conceptual modeling languages and representations.		Patricia Merlim Lima Scheidegger;Maria Luiza Machado Campos;Maria Cláudia Reis Cavalcanti	2017		10.1007/978-3-319-70863-8_9	conceptualization;conceptual model;data mining;ontology;computer science;information technology	Crypto	-45.826034644590834	6.57407874673371	69327
f0bd8af0523439995379b0b9a10dcd793f2543dc	semantic multi-criteria decision making semcdm	kernel;rail transportation;ontologies artificial intelligence decision making inference mechanisms multimedia systems;adaptive devices;multi criteria decision making;inference engines;decision making ontologies rail transportation knowledge representation engines kernel humans bridges privatization decision support systems;automotive multimedia systems;probability density function;semantic multicriteria decision making;bridges;inference mechanisms;data mining;multimedia systems;ontologies artificial intelligence;ontology development;intelligent autonomous devices;design method;automotive multimedia systems semantic multicriteria decision making intelligent autonomous devices adaptive devices inference engines ontologies semantic matching;ontologies;semantic matching	Multi-Criteria Decision Making promises important development in the capabilities of intelligent autonomous and adaptive devices. Ontologies and inference engines enable semantic matching between criteria. This paper presents an approach to take benefit from these technologies in distributed autonomous devices. The concept of SeMCDM is based on a MCDM ontology and a MCDM-aware inference engine. An ontology development environment has been extended to support design methods of MCDM. Applications from the era of automotive multimedia systems show the benefits and strength of SeMCDM.	autonomous robot;categorization;common criteria;inference engine;microsoft outlook for mac;ontology (information science);prototype;semantic matching	Ghadi Mahmoudi;Christian Müller-Schloer	2009	2009 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making(MCDM)	10.1109/MCDM.2009.4938842	probability density function;kernel;design methods;computer science;ontology;artificial intelligence;theoretical computer science;machine learning;data mining;mathematics;inference engine	AI	-42.37672284202054	10.825106783465895	69335
28afe10e84dadaad1b660d0069dfe6a34e138516	choosing a view update translator by dialog at view definition time	view update translator;view definition time	We consider the problem of updating databases through views composed of selections, projections, and joins of a series of Boyce-Codd Normal Form relations. This involves translating updates expressed against the view to updates expressed against the database. Previously, we enumerated all translations of view updates into database updates that satisfy five criteria. This enumeration shows that the problem of translating view updates to database updates is inherently ambiguous. We give examples of structurally similar views that should have different translations because of the real world semantics. We propose that these semantics be obtained at view definition time. We show how this can be done through a structured dialog with the database administrator to choose a view update translator at view definition time. The questions asked during this dialog are based on the view definition, database structural schema information, and the answers to earlier questions in the dialog. Based on these questions a specific translator is chosen. Using this translator, userspecified view updates can be translated into database This work was started while the author was at the Computer Science Department of Stanford University. This work was sup ported in part by contract NSQ-84-C-0211 (the Knowledge Base Management Systems Project, Prof. Gio Wiederhold, PI) from the Defense Advanced Research Projects Agency and by contract AFOSR-80-0212 (Universal Relations, Prof. Jeff Ullman, PI) from the Air Force OWce of Scientillc Research, both of the United States Department of Defense, and by the Computer Sciences Research and Development Fund of The University of Texas at Austin. The views and conclusions contained in this document are those of the authors and should not be interpreted as representative of the oflicial policies of DARPA, the US Government, or the State of Texas. Author’s address: The University of Texas at Austin, Department of Computer Sciences, Austin, TX 78712-1188. updates without the need for any disambiguating dialog. However, dialog with the user may be desired to con6rm that the (view) side effects resulting from the user’s view update request are acceptable.	ambiguous grammar;boyce–codd normal form;computer science;database;gio;knowledge base;sethi–ullman algorithm;word-sense disambiguation;dialog	Arthur M. Keller	1986			computer science;data mining;database;information retrieval	DB	-34.590729379857194	7.255549824261398	69563
bd1f3baad768e04004df824bafca788d1b8dc45c	caching for semantic web services		This document is an extended abstract on a PhD work that develops an efficient, scalable, and stable Web service discovery engine. These qualities become important for discovery engines that serve as a software component in automated SOA technologies. Based on a profound formal specification, the approach is to capture design time discovery results and then use this knowledge for efficient runtime discovery. The work is evaluated by a statistical time efficiency comparison with other Web service discovery engines, and by a applicability study in realworld SOA applications.	cache (computing);component-based software engineering;formal specification;scalability;semantic web service;service discovery;service-oriented architecture	Michael Stollberg	2007			website parse template;combinatorial chemistry;alkyl;hydrogen;carbon;chemistry	Security	-43.27231601457415	12.122176214917951	69591
d7619197ff6b26b8e4a60a7b77895bbb8c8ad738	employing integrity constraints for query modification and intensional answer generation in multi-database systems	database system;multidatabase system;information content;distributed database system;integrity constraints;semantic association	A significant number of database (DB) users today lack complete knowledge of the semantics of the DB(s) they desire to query. This is a very common phenomenon in the Multidatabase System (MBS) type of distributed database systems (DDBSs). In MBSs, a number of DBs, are loosely linked without creating a global schema in order to enable occasional sharing of their information contents. As some cost is normally associated with querying any particular site in this system, a lack of complete knowledge of the DB semantics can often result in fruitless but costly searches. The aim of the work described in this paper is to provide a tool which can assist Multidatabase users gain an understanding of the semantics of DBs accessible to them. Specifically, we use the explicit integrity constraints(ICs) of the database intension in two essential ways to elicit the semantics of those views of the DB addressed in the user's query; firstly to ascertain the relevance of a user query at a particular site and thus to advise the user in case of any constraint violations, suggesting a modification for the query in the process, and secondly to provide abstract or intensional answers to a user request.The first goal aims to provide a system free of the ambiguities associated with an empty response to some retrieval request while the second goal aims to improve the user's understanding of the semantics associated with the data values generated as answers by providing with the tuples of the answer the general rules that they obey.	data integrity;intensional logic;relational database management system	M. M. Fonkam;W. Alex Gray	1992		10.1007/3-540-55693-1_42	self-information;computer science;data integrity;data mining;database;database schema;information retrieval	DB	-35.455971752898286	5.845321942670151	69758
ecd9d604f7e7b50d35b9775b5999d47a31bc54b2	a united approach to discover multimedia web services	markup language multimedia web service discovery internet unified web service discovery brokering based approach trust based approach qos;web service discovery;multimedia computing internet;web service;software engineering;multimedia computing;business model;internet;web services streaming media xml web and internet services simple object access protocol markup languages application software software agents transportation content management;multimedia data;service discovery;multimedia services;markup language	Web services technology has been making steady progress since its initial emergence in the beginning of this century. Since multimedia data have become ubiquitous on the Internet, it is not surprising that multimedia Web services have been receiving attention by the Web services community. On the Web services platform, UDDI is the current de facto service discovery approach. However, researchers have noticed that the UDDI business model has not really achieved its designated goal. We have proposed an approach to complement UDDI with WSIL in the Web services discovery. The idea behind Unified Web Service Discovery (UWSD) is to use both the brokering-based approach and the trust-based approach in Web services discovery. Further, UWSD is designed for handling multimedia service discovery with specific QoS considerations. The services discovered by UWSD are separated into two groups. The first group contains a relatively limited number of services that are trustworthy and guaranteed. The second group contains a large number of services, but the content is not guaranteed to be trustworthy. A markup language is also designed to facilitate the discovery process. We believe that the UWSD approach can better meet the current demand of multimedia Web services discovery.	emergence;in the beginning... was the command line;internet;markup language;quality of service;service discovery;web services description language;web services discovery;web services inspection language;web service;world wide web	Qianhui Althea Liang;Stanley Y. W. Su;Haifei Li;Jen-Yao Chung	2003	Fifth International Symposium on Multimedia Software Engineering, 2003. Proceedings.	10.1109/MMSE.2003.1254423	web service;business model;web application security;web development;web modeling;the internet;business process execution language;synchronized multimedia integration language;web mapping;web design;web standards;computer science;ws-policy;service-oriented architecture;web page;ws-addressing;database;markup language;service discovery;services computing;internet privacy;ws-i basic profile;web 2.0;law;world wide web;devices profile for web services;universal description discovery and integration	DB	-46.011335534158256	13.901713016373522	69763
9d9f899cd062fc4500037c8a4cbf9711a513f65f	assiss: an active semi-structured scientific information sharing system	dynamic content agents;xml documents;scientific information sharing;electrical capacitance tomography;hypermedia markup languages;scalable active data dissemination technologies;semantic multicast project;multicast communication;user groups;scientific experiment markup language;technology development;information dissemination needs;information processing needs;prototypes;browsing;vocabulary;information filtering;collaboration;active semi structured information sharing system;online electronic experiment logbook;xml data analysis laboratories electrical capacitance tomography information filtering information filters nasa markup languages prototypes collaboration;satisfiability;hyperlinks;searching;assiss;information sharing;multimedia databases scientific information systems electronic data interchange information dissemination hypermedia markup languages multicast communication;collaborative scientific information dissemination;data analysis;adaptively clustered user information requests;user profile;seml;hypermedia documents;information exchange;information processing;information dissemination;markup languages;multimedia databases;xml;information transformation;xml document;user profiles;dynamic content;information filters;nasa;markup language;tags;user profiles assiss active semi structured information sharing system scientific information sharing tags hypermedia documents information exchange seml scientific experiment markup language xml documents hyperlinks online electronic experiment logbook input data sets browsing searching semantic multicast project scalable active data dissemination technologies collaborative scientific information dissemination adaptively clustered user information requests information processing needs information dissemination needs user groups dynamic content agents information filtering information transformation vocabulary;electronic data interchange;data dissemination;input data sets;scientific information systems	The main concept behind our approach to scientific information sharing is to capture scientific experiments in hypermedia documents as the basic unit of information exchange. We defined SEML (Scientific Experiment Markup Language), based on the XML standard that introduces definitions of tags and links for specific and common information describing scientific experiments. A collection of SEML documents can be viewed as an online electronic experiment logbook that captures the entire experiment. The process and inter-relationships between experiments are modeled to allow an experiment to be re-created, perhaps when input data sets change. We have prototyped ASSISS (Active Semi-Structured Information Sharing System), supporting the browsing, searching and dissemination of XML documents. ASSISS extends scalable active data dissemination technologies developed in the Semantic Multicast Project to provide support for collaborative scientific information dissemination. Within the framework, users' information requests are adaptively clustered. The aggregated information processing and dissemination needs of user groups are then satisfied by a network of dynamic content agents that filters and transforms information in the appropriate manner. SEML is used as the vocabulary against which user profiles are specified, and SEML documents capturing an experiment can be proactively multicasted to groups of users who are interested in some aspect of the activity as the experiments are being conducted.		Eddie C. Shek;Greg Kaestle;Son Dao	1999		10.1109/SSDM.1999.787646	xml;information processing;computer science;data mining;database;world wide web;information retrieval	HPC	-44.75167332684719	8.508757435923526	69776
a3a641937e74b1d8771247d11a386a6cfaf90531	utilising semantic tags in xml clustering	data representation;xml document	This paper presents an overview of the experiments conducted using Hybrid Clustering of XML documents using Constraints (HCXC) method for the clustering task in the INEX 2009 XML Mining track. This techique utilises frequent subtrees generated from the structure to extract the content for clustering the XML documents. It also presents the experimental study using several data representations such as the structure-only, content-only and using both the structure and the content of XML documents for the purpose of clustering them. Unlike previous years, this year the XML documents were marked up using the Wiki tags and contains categories derived by using the YAGO ontology. This paper also presents the results of studying the effect of these tags on XML clustering using the HCXC method.	algorithm;cluster analysis;computer cluster;data model;experiment;tree (data structure);wiki;wikipedia;xml;yago	Sangeetha Kutty;Richi Nayak;Yuefeng Li	2009		10.1007/978-3-642-14556-8_41	well-formed document;xml catalog;xml validation;binary xml;simple api for xml;xml;streaming xml;computer science;document type definition;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;xml schema editor;information retrieval;efficient xml interchange;sgml	Web+IR	-33.84655027197845	6.973779277326168	69799
5d2c256e7e94fe7cb48c51138d2b8e8380e681d7	knowledge representation problems for natural language understanding		In artificial intelligence these days, just about anything that’s any good is knowledge-based’. Consequently, knowledge representation formalisms are big business, and are available in a wide range of styles and colors to suit the various demands of consumers in the marketplace. In this paper, I want to argue that consumers in the natural language understanding research community are not as well served as they might be, and many of their needs have been overlooked.		Graeme Hirst	1988		10.1007/978-3-642-73998-9_1	natural language processing;language identification;knowledge representation and reasoning;multinet;universal networking language;question answering;knowledge management;body of knowledge;linguistics	ML	-46.474159116506016	5.524316812021862	69810
d64860157ceca22d9a9e383c8f8218088d06bc34	the research and design of the application domain building based on gridgis	grid;global manager;application domain;application domain manager;gridgis	According to the characteristic of Grid geographical information system (GridGIS), data distribution, heterogeneity and diversity in the grid environment, combining the high demand for shared resources and the purpose to realize unified and efficient management and sharing of distributed and mass resources, this paper proposed a grid GIS application building program on a number of peer-to-peer global Managers designs. This paper discussed the building process of the application domain, the selection criteria of application domain manager, the database design of application domain, and every service function module design of application domain. The integration of the application domain builds the three-tier management system of the node, the application domain, and the global management. The spatial query tests of distributed environment have shown that, the application domain is established to manage and share the distributed resources efficiently, and enhance the business processing functions in the grid environment.	application domain;database design;distributed computing;geographic information system;multitier architecture;peer-to-peer;spatial query;systems management	Zhong Xie;Lina Ma;Liang Wu;Zhanlong Chen	2010	J. Geographic Information System	10.4236/jgis.2010.21007	domain analysis;application domain;domain;business domain;computer science;knowledge management;domain engineering;data mining;database;grid	HPC	-34.408260595929455	12.7640642552342	69838
92f3082b6b72d8adcbf79e37b9dce65d561b10de	multiagent ontology mapping framework for the semantic web	owl;multiagent system;ontology mapping;uncertain reasoning;semantics;ontologies semantics owl decision support systems context humans;ontologies artificial intelligence;uncertain reasoning multiagent systems ontology mapping semantic web;heterogeneous data integration multiagent ontology mapping framework semantic web;multi agent systems;decision support system;decision support systems;semantic web multi agent systems ontologies artificial intelligence;background knowledge;semantic web;heterogeneous data integration;ontologies;humans;multiagent ontology mapping framework;context;domain specificity;question answering;multiagent systems	Ontology mapping is a prerequisite for achieving heterogeneous data integration on the Semantic Web. The vision of the Semantic Web implies that a large number of ontologies present on the web need to be aligned before one can make use of them, for example, a question answering on the Semantic Web. At the same time, these ontologies can be used as domain-specific background knowledge by the ontology mapping systems to increase the mapping precision. However, these ontologies can differ in representation, quality, and size that pose different challenges to ontology mapping. In this paper, we analyze these challenges and introduce a multiagent ontology mapping framework that has been designed to operate effectively in the Semantic Web environment.	agent-based model;ontology (information science);question answering;semantic web;semantic integration;web ontology language	Miklos Nagy;Maria Vargas-Vera	2011	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2011.2132704	upper ontology;ontology alignment;web modeling;semantic integration;data web;web mapping;bibliographic ontology;decision support system;ontology inference layer;web standards;computer science;knowledge management;ontology;artificial intelligence;semantic web;social semantic web;multi-agent system;data mining;semantic web stack;ontology-based data integration;owl-s;information retrieval;process ontology;semantic analytics;data mapping;suggested upper merged ontology	Web+IR	-42.5647619862209	7.799976265614527	70253
7f101151d46790bc53b30f4d1dbfac6e4dbed2ff	configuring intelligent mediators using ontologies	configural processing;besoin de l utilisateur;representacion conocimientos;ontologie;base donnee;architecture systeme;metadata;database;logica descripcion;base dato;base connaissance;necesidad usuario;mediacion;mediation;user need;metadonnee;representation connaissance;preferencia;base conocimiento;ontologia;arquitectura sistema;preference;metadatos;description logic;knowledge representation;system architecture;ontology;logique description;knowledge base	This paper presents a new intelligent mediators configuration approach which exploits high expressive description logics to represent metadata, and reasoning tasks in order to build more flexible mediation systems. A user specifies a needs expression in terms of (i) an interesting view over a given application domain, (ii) sources preferences and (iii) architectural requirements. A well-adapted mediator, is automatically configured according to these needs through a reasoning-based configuration process. A configured mediator can therefore be adapted in order to build knowledge-based mediation systems with an arbitrary architecture. 1 Context and Motivations Mediation systems [1] were introduced to provide an integrated view over distributed and heterogeneous data sources for accessing them in a transparent way. During these last years, their role has constantly evolved. Several mediation approaches, providing different modeling and implementing solutions, have been proposed. In order to provide query expression and metadata management with more semantics, a particular kind of data integration approach, commonly called knowledge-based mediation system, has been proposed. Differently from classical mediation systems, they use high expressive knowledge representation formalisms, i.e., description logics, as basis for data integration. This allows to have a more precise semantic representation of application domains, and to improve classical mediation tasks with inference capabilities. This work focuses on knowledge-based mediation systems. For this reason, we analyzed many existing approaches according to several aspects such as the integration approach, data model and associated query language, and more particularly the mediation system architecture. According to this latter aspect, knowledge-based mediation systems can be mainly divided into two main categories (cf. Figure 1): – centralized mediation systems [2, 3, 5, 7, 8, 6] are based on a domain ontology acting as an integrated view over a set of distributed and heterogeneous data sources. A user formulates a query over the domain ontology. Then the query is rewritten into a set of local expressions over local sources, which are consequently accessed in a transparent way. The mediator represents the single access point to the system, and local sources are directly accessible from it. – distributed mediation systems [4, 9] aim to integrate a very large number of distributed data sources. This makes the construction of an integrated view over them a very difficult task to achieve. Therefore, query processing becomes a distributed task and the centralized mediator is replaced by a net of cooperative components commonly called peers. Each peer provides a local ontology modeling one or more underlying local sources. A user formulates a query over a peer. If locally retrieved data does not fulfill user expectatives, the query is forwarded to some neighbors peers for execution in order to retrieve mode data. To our knowledge, no mediation system, being able to adapt to both applicative contexts, exists today. Fig. 1. Existing approaches. This paper focuses on the intelligent mediator configuration process of ADEMS and on the role of mediators within a knowledge-based mediation system. The architecture of ADEMS has been previously presented in [10], therefore, the paper gives no details on the mediator internal architecture and on query expression and processing. ADEMS exploits the high expressive description logic SHIQ(D) [12] to represent metadata, and exploits reasoning tasks in order to automatically configure well-adapted mediators. A user specifies a needs expression in terms of (i) the interesting view over a given application domain, (ii) sources preferences and, (iii) architectural requirements. ADEMS configures a well-adapted mediator according to these needs, being able to adapt centralized as well as distributed 1 Neighbor peers are those ones that a peer can directly access. Differently from centralized approach, not all resources are directly accessible from a peer (mediator). 2 ADEMS, an ADaptable and Extensible Mediation Service. architectures. A configured mediator manages metadata as knowledge within a set of ontologies, and exploits inference in order to semantically improve the query processing task. Nevertheless, for a lack of space, this paper mainly focuses on the mediator configuration process, and on the role of mediators within a knowledge-based mediation system. No details on the mediator internal architecture and functions are given. The remainder of this document is organized as follows. Section 2 presents the ADEMS approach. It describes the general architecture of a mediation system, its components, i.e. a set of mediators and sources, and the way they interact. Then it introduces the mediator configuration process. Section 3 illustrates the needs expression structure and shows how its ontological representation allows to better represent the semantics of metadata. Section 4 shows how a needs expression is analyzed and a mediator is configured accordingly, by exploiting reasoning tasks. Metadata involved in such a process is also illustrated. Section 5 discusses on implementation issues and experimental validations. Finally, Section 6 concludes the paper.	application domain;applicative programming language;centralized computing;data model;database;description logic;knowledge representation and reasoning;knowledge-based systems;matchware mediator;ontology (information science);query language;requirement;single-access key;systems architecture;wireless access point	Gennaro Bruno;Christine Collet;Genoveva Vargas-Solar	2006		10.1007/11896548_41	knowledge base;description logic;computer science;artificial intelligence;ontology;database;mediation;metadata	AI	-36.93890615552511	11.199323983263303	70255
21d816a01a3f76f3498f57683976e410232ad43d	annotated rdf	view maintenance;query processing;annotated rdf	Real-world use of RDF requires the ability to transparently represent and explain metadata associated with RDF triples. For example, when RDF triples are extracted automatically by information extraction programs, there is a need to represent where the triples came from, what their temporal validity is, and how certain we are that the triple is correct. Today, there is no theoretically clean and practically scalable mechanism that spans these different needs - reification is the only solution propose to date, and its implementations have been ugly. In this paper, we present Annotated RDF (or aRDF for short) in which RDF triples are annotated by members of a partially ordered set (with bottom element) that can be selected in any way desired by the user. We present a formal declarative semantics (model theory) for annotated RDF and develop algorithms to check consistency of aRDF theories and to answer queries to aRDF theories. We show that annotated RDF supports users who need to think about the uncertainty, temporal aspects, and provenance of the RDF triples in an RDF database. We develop a prototype aRDF implementation and show that our algorithms work efficiently even on real world data sets containing over 10 million triples.	algorithm;information extraction;prototype;reification (knowledge representation);resource description framework;scalability;triplestore	Octavian Udrea;Diego Reforgiato Recupero;V. S. Subrahmanian	2010	ACM Trans. Comput. Log.	10.1145/1656242.1656245	rdf/xml;cwm;computer science;sparql;data mining;database;rdf query language;blank node;information retrieval;rdf schema	DB	-36.20694646262399	5.268171768168523	70286
9ccff9d9559a6ddef1476020b3b9788f7ad38a2c	the role of hypertext for cscw applications (panel)	information retrieval;hypertext design	Hypertext and CSCW are both growing fields of research and development that should converge by providing each other with complementary results. Features and concepts inherent to hypertext like the ability to create modules of information, combine them in a flexible way, reuse existing material, define views for different targets and user groups, and annotate multimedia material address many CSCW tool requirements. Furthermore, domains where hypertext already plays an important role (e.g., document preparation, idea management, design problem solving, decision making and argumentation, teaching and training) are or will soon become high priority topics for CSCW applications. There is no doubt that hypertext systems have to become multi-user systems and have to be extended by appropriate capabilities (e.g. shared screens, locking mechanisms in the hyperdocument base, versioning, knowledge-based components for monitoring access and changes by different users and reporting this on the basis of defined rules, etc.). On the other hand, CSCW is lacking a clear concept about which types of “document” structures and formats are needed as an appropriate communication medium for cooperation and collaboration.	computer-supported cooperative work;converge;hypertext;lock (computer science);multi-user;problem solving;requirement	Norbert A. Streitz;Frank G. Halasz;Hiroshi Ishii;Thomas W. Malone;Christine Neuwirth;Gary M. Olson	1991		10.1145/122974.125102	human–computer interaction;computer science;world wide web;information retrieval	DB	-41.5300749242019	8.366125691991178	70506
ffa444030f3332a832b78ec89bd91dc5b8086749	a dynamic aggregation mechanism for agent-based services	agent based	At a time when the web is switching from a data-oriented view to a service-oriented view, we can envision an environment where services are dynamically and automatically combined to solve new problems that one single service cannot solve. Agent technology provides a good basis for creating such an environment but many issues remain to be solved. This paper presents a step towards a dynamic service aggregation mechanism, introducing a pragmatic approach and an implementation. This work was carried out in the context of the Agentcities.RTD project.	service-oriented architecture;service-oriented device architecture	Makram Bouzid;Jérôme Picault;David Bonnefoy	2004			computer science	AI	-45.418670451780606	14.882297607016755	70792
f8fc820ed34f7bf75861c00d1be67d3abe17b437	a distributed planning approach using multiagent goal transformations.	distributed planning	A multiagent goal transformation is defined as a directed alteration of goals by the agents in a multiagent system in order to adjust for lack of resources and/or lack of capabilities. Multiagent goal transformation can be used as a uniform coordination mechanism. We describe preliminary research that implements a distributed planning approach incorporating several co-operative planning agents and that uses a multiagent goal transformation as a coordination mechanism. Each agent in the system represents a domain with capabilities differing from the domains represented by the other agents. The distributed coordination mechanism allows distribution of tasks among agents and concatenation of resultant sub-plans from agents without a central coordination process. Examples are given to illustrate how the system works in the logistics transportation domain.	agent-based model;concatenation;logistics;multi-agent system;resultant	Michael T. Cox;Mohammad M. Elahi;Kevin Cleereman	2003			computer science	AI	-40.15344019973828	16.930485012375044	70877
e20bd488de2c83a7d9d279927c8bb22a5a0c9b88	mutualisation de données et de connaissances pour lagestion intégrée des zones côtières.application au projet syscolag		This Phd is a part of the Syscolag’s research program and studies concepts and tools to set up in order to mutualize data and knowledge to serve integrated coastal zone management (ICZM). The main problems consist in the variety of information resources distributed by the different stakeholders as well as in the multidisciplinary framework which involves the setting up of a common vocabulary between the partners. This work propose a solution to minimize syntax and semantic interoperabilities problems with an additional dificulty involved by the spatial dimension of information concerned. The goal of this PhD is the setting up of generic methods to improve information and knowledge management in the ICZM context. The proposed solution consists in a mutualization infrastructure which core is made of a metadata management service to describe and locate existing information as well as a semantic referential to integrate and share expert knowledge. For the metadata service, the ISO 19115 standard (adapted to spatial information description) has been chosen. A specific profile of the ISO 19115 has been implemented for the specific Syscolag’s context and has been used differently among the different kind of information resources. The thematic referential comes from an a priori ontology model which structures the concepts inventory of this domain and restitutes the knowledge which is related to. This model has been created by an ecosystemic approach. Spatial dimension has been used at the concepts and relations level as well as for the maps representation (OGC recommandations compliant). The whole solution can be accessed by a Web portal.		Julien Barde	2005				AI	-44.66026834270642	4.83494761435984	70991
6c7b4b797fcbdab1e9ddcb22d2b8a0886dd461e6	efficiently querying rdf(s) ontologies with answer set programming	query language;answer set programming;rule based	Ontologies are pervading many areas of knowledge representation and management. To date, most research efforts have been spent on the development of sufficiently expressive languages for the representation and querying of ontologies; however, querying efficiency has received attention only recently, especially for ontologies referring to large amounts of data. In fact, it is still uncertain how reasoning tasks will scale when applied on massive amounts of data. This work is a first step toward this setting: it first shows that RDF(S) ontologies can be expressed, without loss of semantics, into Answer Set Programming (ASP). Then, based on a previous result showing that the SPARQL query language (a candidate W3C recommendation for RDF(S) ontologies) can be mapped to a rule-based language, it shows that efficient querying of big ontologies can be accomplished with a database oriented extension of the well known ASP system DLV, which we recently developed. Results reported in the paper show that our proposed framework is promising for the improvement of both scalability and expressiveness of available RDF(S) storage and query systems. 1Institut für Informationssysteme 184/3, Technische Universität Wien, Favoritenstraße 9-11, A-1040 Vienna, Austria. 2Dipartimento di Matematica, Università della Calabria, I-87036 Rende (CS), Italy. Acknowledgements: The work of the first author has been supported under FWF projects P20841 (Modular HEX-Programs), P17212 (Answer Set Programming for the Semantic Web), and MIUR (Italian Research Ministry) Interlink II04CG8AGG. We gratefully acknowledge Axel Polleres (DERI-Galway) for his insightful comments and for providing a prototypical version of the SPARQL-Datalog translator. To appear on Journal of Logic and Computation, Oxford University Press, Oxford, UK. doi:10.1093/logcom/exn043 Copyright c © 2008 by the authors 2 INFSYS RR RR-1843-08-06	answer set programming;dlv;datalog;experiment;heterogeneous database system;journal of logic and computation;knowledge representation and reasoning;ontology (information science);programming language;query language;rapid refresh;round-robin scheduling;rule-based system;sparql;scalability;semantic web;stable model semantics;whole earth 'lectronic link	Giovambattista Ianni;Alessandra Martello;Claudio Panetta;Giorgio Terracina	2009	J. Log. Comput.	10.1093/logcom/exn043	rule-based system;rdf/xml;computer science;sparql;artificial intelligence;answer set programming;data mining;database;rdf query language;web ontology language;information retrieval;query language;rdf schema	AI	-37.58864256936239	6.286878934283203	71049
abdc0cda0226387266dbacdd4ef0191fcd68fcdc	a product knowledge-sharing system focusing on internet-mediated mode		This research constructs a design-oriented, Internet-mediated, and knowledge-sharing architecture. Therein XML files serve as the basic exchange and storage format of knowledge texts in the transaction-matching process. All sorts of product information needed by the users of the system platform, including various concepts and related features which involve such different fields as the design, production, and marketing of the product, are integrated into the programmed process of an Internet-mediated transaction (IMT). With the help of the Internet, the information can be rapidly delivered to each interested user and be used repetitively; moreover, the product knowledge as well as relevant experiences can be efficiently managed. The XML-based, design-oriented, Internet-mediated, and knowledge-sharing architecture not only accelerates the exchange of product knowledge and information but enables the users to manage, apply, and share their knowledge on this knowledge management platform. Finally, the users are enabled to acquire the needed information and knowledge and thus upgrade their capabilities.	interactive machine translation;internet;knowledge management;xml	Hsin-Hsi Lai;Heng-Chang Lin;Yu-Cheng Chen	2010	JCIT		computer science;knowledge management	DB	-47.96535776261859	8.685018602577872	71304
5365eea3e67073f8d14bde6bdf303c2911499963	semantic selection of healthcare apps	selection owl;biomedical monitoring;software tool semantic selection healthcare application swrl enabled owl ontology;blood pressure mobile communication biomedical monitoring semantics diseases cognition;swrl;semantics;blood pressure;healthcare apps;swrl healthcare apps selection owl;software tools health care knowledge representation languages medical computing mobile computing smart phones;cognition;mobile communication;diseases	"""The paper proposes a computational model, based on SWRL enabled OWL ontologies, in order to create software tools, which will choose the most suitable healthcare Apps for specific requirements. The tool will take into account specific evaluation criteria for the semantic selection of healthcare Apps, which will be based on functionality of Apps, preferences of the Apps' users and the technology required to run the Apps. We call our selection """"semantic"""" for two reasons: a) we must understand the semantic of the environment where the selection happens before we choose an App and b) we will perform reasoning in order to make a decision on the most suitable healthcare App. Our computational model has been tested in various environments and proved that it can dynamically select Apps according to the semantic found in the Apps evaluation criteria."""	computation;computational model;emoticon;ontology (information science);requirement;semantic web rule language;web ontology language	Hamda Binghubash Almarri;Radmila Juric;Bilal B Mughal	2016	2016 49th Hawaii International Conference on System Sciences (HICSS)	10.1109/HICSS.2016.410	cognition;mobile telephony;computer science;knowledge management;blood pressure;data mining;semantics;world wide web	SE	-41.638449368539476	14.375489576547945	71439
b0b395c78cd42031b6099fe655d84574f36ebd49	tuning search engine to fit xml retrieval scenario	search engine;xml retrieval	XML usage is growing to describe documents and consequently systems to search in XML collections are necessary. Various proposals of systems intend to handle XML documents. This paper describes an XML approach based on direct contribution of the components constituting an information need. The search engine is largely configurable in order to be adapted to different context of search. Beyond being globally adapted to a collection of documents an important objective is to define a search engine that can be adapted to different retrieval scenarios and to identify how to adapt it. This paper presents first experiments on INEX testbeds that show how the engine can be adapted to better respond to different retrieval scenarios.	experiment;information needs;information retrieval;web search engine;xml retrieval	Gilles Hubert;Josiane Mothe;Kurt Englmeier	2007			search engine indexing;simple api for xml;database search engine;streaming xml;computer science;database;world wide web;information retrieval;search engine;efficient xml interchange	Web+IR	-45.17741314536076	11.585090029427436	71544
83cc083709b14c304fe1c81e9497d74f5e94689f	modeling multi-party web-based business collaborations	gestion integrada;modelizacion;distributed system;gestion integree;intercambio informacion;gestion entreprise;entreprise;tratamiento transaccion;systeme reparti;red www;organisation entreprise;fournisseur;processus metier;empresa;reseau web;firm management;semantics;integrated management;supplier;semantica;semantique;enterprise organization;modelisation;organizacion empresa;sous traitance;sistema repartido;internet;echange information;information exchange;firm;world wide web;proceso oficio;administracion empresa;subcontratacion;transaction processing;subcontracting;modeling;business process;traitement transaction;proveedor	To remain competitive, enterprises have to mesh their business processes with their customers, suppliers and business partners. Increasing collaboration includes not only a global multi-national enterprise, but also an organization with its relationship to and business processes with its business partners. Standards and technologies permit business partners to exchange information, collaboration and carry out business transaction in a pervasive Web environment. There is however still very limited research activity on modeling multi-party Web-based business collaboration underlying semantics. In this paper, we demonstrate that an in-house business process has been gradually outsourced to third-parties and analyze how task delegations cause commitments between multiple business parties. Finally we provide process semantics for modeling multi-party Web-based collaborations.	behavior model;business process;commitment ordering;erp;enterprise resource planning;mesh networking;outsourcing;pervasive informatics	Lai Xu;Sjaak Brinkkemper	2005		10.1007/11575863_108	business model;business analysis;business transformation;the internet;systems modeling;information exchange;transaction processing;business requirements;computer science;artifact-centric business process model;business process management;business case;process modeling;electronic business;semantics;process management;business process;business relationship management;business process discovery;business rule;new business development;business process modeling;business activity monitoring;business architecture	Web+IR	-39.16180768184474	15.651842518412337	71630
e1ad7d64f97e7fa5a2d094d8b3451da6d3bc80cd	implementation of conceptual graphs using frames in lead	conceptual graph;knowledge representation;data structure;expert system	This paper chiefly discusses the implementation of Sowa's Conceptual Graph notation using a frame-like data structure. Conceptual graphs serve as the basis for knowledge representation in our two systems, LEAD (Learning Expert system for Agricultural Domain) and XLAR (Universal Learning ARchitecture). The importance of conceptual graphs in knowledge representation is also briefly accounted for. Rationale for choosing frames for conceptual graph implementation is presented. Comparison of this implementation with other extant implementations is made.		K. C. Reddy;K. S. Reddy C.S.Reddy;P. G. Reddy	1989		10.1007/BFb0018381	natural language processing;conceptual graph;conceptual model;computer science;knowledge management;data mining	Robotics	-42.333888281370285	5.059348489964422	71729
3d98ed43dcc7f073e6047748f5e3233cb8a6ad45	adapting transactions to exceptional situations using structured messages	groupware;collaborative work;dynamic reconfiguration;exception handling groupware transaction processing database management systems;database management systems;concurrent computing humans petroleum;transaction;structured message;exception handling;advanced transaction model;message transaction model collaborative work structured messages exceptional situations transaction adaptation advanced transaction models collaborative work environments dynamic process reconfiguration exception handling database transactions;transaction processing;communication	We concentrate on the property that the progression of collaborative work involves communication among participants, and we consider a flexible method for managing transactions utilizing structured messages. In the literature, advanced transaction models for collaborative work environment have been proposed. However, since the models treat work process as a target of transaction, it is difficult to adapt to exceptions or dynamic reconfiguration of processes. In this paper, we argue about examples of handling exceptions in an environment involving workflow processes and database transactions, utilizing structured messages. We use the message transaction model for specifying structured messages. The model has ability of specifying workflow processes and various advanced transactions.	color gradient;database transaction;design rationale;information system;interactivity;transaction processing	Sozo Inoue;Mizuho Iwaihara	1999		10.1109/DANTE.1999.844979	extreme transaction processing;real-time computing;database transaction;transaction processing;distributed transaction;computer science;transaction data;database;distributed computing;online transaction processing;compensating transaction;serializability;acid;transaction processing system	SE	-40.18785431784978	17.95258633590021	71804
f1c90ca10c5c89dc167a2d6c381d5c668d8f18d1	multi-agent based web search with heterogeneous semantics	search engine;volume of distribution;multi agent system;agent based;semantic integration;relevance ranking;semantic information;semantic web;web search;semantic search;query expansion;social annotation	Relevance ranking is key to Web search in determining how results are retrieved and ordered. As keyword-based search does not guarantee relevance in meanings, semantic search has attracted enormous and growing interest to improve the accuracy of relevance ranking. Recently heterogeneous semantic information such as thesauruses, semantic markups and social annotations have been adopted in search respectively for this purpose. However, although to integrate more semantics would logically generate better search results in respect of semantic relevance, such integrated semantic search mechanism is still in absence and to be researched. This paper proposes a multi-agent based semantic search approach to integrate both keywords and heterogeneous semantics. Such integration is achieved through semantic query expansion, meta search of expanded queries in varieties of existing search engines, and aggregation of all search results at the semantic level. With respect to the great volumes of distributed and dynamic Web information, this multi-agent based approach not only guarantees efficiency and reliability of search, but also enables automatic and effective cooperations for semantic integration. Experiments show that the proposed approach can effectively integrate both keywords and heterogeneous semantics for Web search.	web search engine	Rui Huang;Zhongzhi Shi	2007		10.1007/978-3-642-01639-4_14	semantic interoperability;semantic similarity;semantic computing;query expansion;semantic integration;semantic search;semantic grid;computer science;artificial intelligence;volume of distribution;phrase search;semantic web;concept search;social semantic web;multi-agent system;data mining;semantic web stack;semantic compression;database;semantic technology;search analytics;web search query;information retrieval;semantic analytics;search engine	NLP	-40.92394105481792	7.49614409548454	72051
3d232a7bd885f8b04b8e15a226577d08a61ca6dc	multilingual natural language interaction with semantic web knowledge bases and linked open data	ontology mapping;information retrieval;cultural heritage;computer and information science;cidoc crm;functional programming;natural sciences;reason able view;grammatical framework;natural language understanding;semantic web;owlim;sparql;language technology computational linguistics;ontology;natural language gen eration;question answering	Cultural heritage appears to be a very useful use case for Semantic Web technologies. The domain provides with plenty of circumstances where linkages between different knowledge sources are required to ensure access to rich information and respond to the needs of professionals dealing with cultural heritage content. Semantic Web technologies offer the technological backbone to meet the requirement of integrating heterogeneous data easily, but they are still more adapted to be consumed by computers than by humans, especially non-engineers or developers. This chapter is about a technique which allows interaction in natural language with semantic knowledge bases. The proposed technique offers a method that allows querying a semantic repository in natural language and obtaining results from it as a coherent text. This unique solution includes several steps of transition from natural language to SPARQL and from RDF to coherent multilingual descriptions, using the Grammatical Framework, GF. The approach builds on a semantic knowledge infrastructure in RDF, it is based on OWLIM-SE and the data integration method Reason-able View supplied with an ontological reference layer. The latter is connected via formal rules with abstract representations derived from the syntactic trees of natural language input using the GF resource grammar library.	linked data;natural language;semantic web	Mariana Damova;Dana Dannélls;Ramona Enache;Maria Mateva;Aarne Ranta	2014		10.1007/978-3-662-43585-4_13	natural language processing;semantic similarity;semantic computing;semantic web rule language;universal networking language;semantic search;semantic grid;computer science;sparql;social semantic web;linked data;semantic web stack;semantic compression;database;information retrieval;semantic analytics	AI	-41.787871345334715	4.233111312838796	72127
9fcb118f3d4732ce555aa6d528c43d665665180b	estudo de caso de mineração de dados multi-relacional: aplicação do algoritmo connetionblock em um problema da agroindústria	semantically related data;traditional algorithm;multi-relational data mining;useful pattern;sugar mill;multiple table;da agroind;case study;better expressiveness;connectionblock algorithm	This paper presents a case study of multi-relational data mining using the ConnectionBlock algorithm, applied to the database of a sugar mill. The algorithm handles multiple tables not explicitly correlated but which influence one another according to the semantics of the data involved. The experiment revealed very interesting and useful patterns that are not found using traditional algorithms. The paper aims to present how the data were prepared to obtain better expressiveness of the rules generated, showing the potential of the algorithm to find patterns in semantically related data. Resumo. Este artigo apresenta um caso de aplicação de mineração de regras de associação multi-relacional usando o algoritmo ConnectionBlock, em uma base de dados real de uma indústria de açúcar e álcool. O algoritmo processa múltiplas tabelas que não são explicitamente relacionadas, mas que têm influência entre si devido à semântica entre os dados envolvidos. O experimento revelou padrões interessantes e úteis, impossíveis de serem encontrados usando algoritmos tradicionais. O artigo tem como objetivo apresentar como os dados foram preparados para melhor expressividade das regras geradas e os resultados obtidos, mostrando o potencial do algoritmo para encontrar padrões em dados que são semanticamente correlacionados.	algorithm;http 404;multi-agent system;relational data mining;unified model;web typography	Ederson Garcia;Marina Teresa Pires Vieira	2008			artificial intelligence;algorithm	ML	-36.42112142306807	12.838858198855078	72434
888b567bbb4f31b86003f92566e7fe97e61c395a	oyster - sharing and re-using ontologies in a peer-to-peer community	distributed system;ontologie;communaute repartie;systeme reparti;metadata;web community;par a par;formulation requete;web semantique;query formulation;formulacion pregunta;peer to peer system;data representation;sistema repartido;poste a poste;web semantica;metadonnee;semantic web;ontologia;metadatos;peer to peer;ontology;comunidad repartido	This paper presents Oyster, a Peer-to-Peer system for exchanging ontology metadata among communities in the Semantic Web. We describe how Oyster assists researchers in re-using existing ontologies, and how Oyster exploits semantic web techniques in data representation, query formulation, query result presentation to provide an online solution to share ontologies.	ontology (information science);peer-to-peer	Raúl Palma;Peter Haase	2005		10.1007/11574620_77	epistemology;computer science;semantic web;ontology;database;external data representation;metadata;world wide web;information retrieval	ECom	-38.0367884531156	11.9724572539759	72798
e497627bf147a00b621a1fda6cfdfb1a4270a9b7	a frame model approach for expert and database system integration	application development;database system;knowledge based system;expert systems;knowledge acquisition;database systems;user requirements;information system;frame model;expert database systems;knowledge based systems;expert system	Expert systems(ES) and database systems(DBS) are major components of information systems and important assets to companies. The development of these systems represent users' knowledge in the application systems. As computer technologies evolve, and as users requirements change, there is a need to upgrade these system to meet the new application requirements. To preserve the knowledge of the existing information systems, a methodology for integrating ES and DBS into an expert database system(EDS) is proposed. The integrated EDS is a knowledge based system(KBS) which derives and stores knowledge in a frame model consisting of a class header, attributes, methods and constraints. It extracts the ES rules and DBS data for an application into coupling classes at run time only. The attributes of the coupling classes are matched with synonyms in a synonym table which resolves their naming and semantic conflicts with user assistance in knowledge acquisition. The resultant EDS is a KBS ready for application development.	constraint (mathematics);database;direct-broadcast satellite;electronic document system;expert system;extended data services;information system;knowledge acquisition;knowledge-based systems;requirement;resultant;run time (program lifecycle phase);system integration	Joseph Fong;Shi-Ming Huang	1999	International Journal of Software Engineering and Knowledge Engineering	10.1142/S021819409900022X	legal expert system;knowledge base;computer science;knowledge management;artificial intelligence;user requirements document;software engineering;knowledge-based systems;data mining;database;subject-matter expert;rapid application development;expert system;information system	DB	-37.326467507403464	9.947430587468556	72964
0d2e2674390698e08cb975348a4e7e7270ca4a54	owl-based description for agent interaction	agent interaction;web pages;foundation of intelligent and physical agents;agent communication;web service;ontologies artificial intelligence;foundation of intelligent and physical agents world wide web semantic web web services web ontology language for service agent interaction description;multi agent systems;web ontology language;web services;semantic web multi agent systems ontologies artificial intelligence;agent interaction description;semantic web;world wide web;web ontology language for service;agent programming;web services owl html simple object access protocol web pages semantic web web sites world wide web intelligent agent resource description framework	The World Wide Web (WWW) has gained wide acceptance partially owing to the ease of writing web pages. Further, the semantic Web facilitates composing Web services in the form of Web ontology language for service (OWLS). However, OWLS requires assistance from agent for automatic discovery, composition, and invocation. This work presents an agent interaction description, based on Web ontology language (OWL), to confine agent communication within the foundation of intelligent and physical agents (FIPA) protocol. The description achieves dynamic OWLS service invocation and improves maintainability of agent program.	ontology (information science);semantic web;www;web ontology language;web page;web service;world wide web	Yong-Feng Lin;Jason Jen-Yen Chen	2007	31st Annual International Computer Software and Applications Conference (COMPSAC 2007)	10.1109/COMPSAC.2007.162	web service;web development;web modeling;web-based simulation;web standards;computer science;artificial intelligence;semantic web;web navigation;social semantic web;multi-agent system;semantic web stack;database;web intelligence;web 2.0;world wide web;owl-s;information retrieval;web server	AI	-43.66531069493981	12.939248141241144	72992
bcfe07f3eb00ce76cdfcba0b4eb0ff4a3683dcfc	context-aware content filtering & presentation for pervasive & mobile information systems	context awareness;filtering;context aware;ambient intelligence;model based approach;olap;presentation;preference mining;adaptivity;information integration;adaptive system;context dependent;mobile information system;information system;context aware systems;user interaction;content integration	What constitutes relevant information to an individual may vary widely under different contexts. However, previous work on pervasive information systems has mostly focused on context-aware delivery of application-specific information. Such systems are only able to operate within narrow application domains and cannot be generalized to handle other heterogeneous types of information. To fill this gap, we propose a context-aware system for information integration that can handle arbitrary information types and determine their relevance to the user's current context. In contrast to existing model-based approaches to context reasoning, we log user interaction and perform usage mining using OLAP to discover context-dependent preferences for different information types. This allows us to build a more generic and adaptive system that automatically selects the most relevant content and presents it to the user in a succinct manner that supports ease of consumption and comprehension.	adaptive system;application domain;content-control software;context-sensitive language;data mining;information system;list comprehension;online analytical processing;pervasive informatics;relevance	Kaijian Xu;Manli Zhu;Daqing Zhang;Tao Gu	2008			computer science;information filtering system;data mining;database;world wide web	HCI	-42.92866114738453	10.968596053578084	73111
232c30b3e4aa70f11c3e7446330bc5f2a125d418	transcendence: enabling a personal view of the deep web	information extraction;web databases;collaborative filtering;deep web;web forms;semantic analysis	A wealth of structured, publicly-available information exists in the deep web but is only accessible by querying web forms. As a result, users are restricted by the interfaces provided and lack a convenient mechanism to express novel and independent extractions and queries on the underlying data. Transcendence enables personalized access to the deep web by enabling users to partially reconstruct web databases in order to perform new types of queries. From just a few examples, Transcendence helps users produce a large number of values for form input fields by using unsupervised information extraction and collaborative filtering of user suggestions. Structural and semantic analysis of returned pages finds individual results and identifies relevant fields. Users may revise automated decisions, balancing the power of automation with the errors it can introduce. In a user evaluation, both programmers and non-programmers found Transcendence to be a powerful way to explore deep web resources and wanted to use it in the future.	collaborative filtering;database;deep web;form (html);information extraction;personalization;programmer;semantic analysis (compilers);transcendence;web resource	Jeffrey P. Bigham;Anna Cavender;Ryan S. Kaminsky;Craig Prince;Tyler Robison	2008		10.1145/1378773.1378796	web service;web application security;web development;web modeling;data web;web analytics;web mapping;web design;human–computer interaction;web standards;computer science;collaborative filtering;machine learning;semantic web;web navigation;social semantic web;web page;data mining;semantic web stack;web intelligence;web 2.0;world wide web;information extraction;information retrieval;web server	Web+IR	-34.43500600529932	4.674943286654414	73144
1f92c1dbf6a3476ce30b713f9811bef92bd50f3c	adapting to changing resource requirements for coalition formation in self-organized social networks	databases;social network services;swarm intelligence;resource management;emergent behavior resource requirements coalition formation self organized social networks local neighborhood structural agents swarm intelligence;software agents;coordination and collaboration;social network;multi agent systems;agents;emergent behavior;intelligent agent;software agents multi agent systems;social network services resource management switches databases intelligent agent multiagent systems autonomous agents;autonomous agents;switches;emergent behavior agents social network coordination and collaboration swarm intelligence;multiagent systems	Coalition formation in social networks allows many choices of which task to select and with whom to partner in the social network. Agents communicate with agents within n network links in their surrounding network. These agents are considered part of an agent’s local neighborhood. Agents maintain a database of skills possessed by agents in their local neighborhood. We compare agents of two different types. Structural agents seek to create a scale-free network. Inventory agents seek to connect to agents who possess a skill not found in their current local neighborhood. We examine the ability of the agents to deal with static skill demand patterns, changing skill demand patterns, and a mismatch of the skills supplied to the skills demanded.	design pattern;http 404;inventory;requirement;social network	Levi Barton;Vicki H. Allan	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.121	simulation;computer science;knowledge management;artificial intelligence;software agent;multi-agent system	AI	-42.67687572182094	17.954633226202823	73581
02c0b242593f76d1ad22bc2d4210e94e58d21ad6	xcerpt and xchange: deductive languages for data retrieval and evolution on the web.	semantic web;data retrieval	In this article, two deductive languages are introduced: the language Xcerpt, for querying data and reasoning with data on the (Semantic) Web, and the language XChange, for evolution and reactivity on the (Semantic) Web. A small application scenario is given as a motivation for these languages.	data retrieval;database;event condition action;han unification;international conference on logic programming;lu decomposition;lecture notes in computer science;prototype;semantic web;simulation;springer (tank);transformation language;vldb;world wide web;xml	François Bry;Paula-Lavinia Patranjan;Sebastian Schaffert	2004			data web;semantic web;social semantic web;semantic web stack;database;world wide web;information retrieval	DB	-38.60125712444889	6.966603563060507	73629
a881087c5a0d7c5f68380cf74eecfd0e823f4b3c	toward intelligent agents on quantum computers	silicon;owl s;multi agent system;nets within nets;optical computing;web service;quantum computing intelligent agent physics computing optical computing quantum mechanics multiagent systems quantum entanglement computer architecture permission silicon;physics computing;computer architecture;renew;permission;quantum computer;quantum mechanics;reference nets;intelligent agent;high level petri nets;workflow;quantum computing;quantum entanglement;business process;multiagent systems	We present an innovative perspective of usage of intelligent agents and multi-agent systems in quantum computing, and summarize preliminary results of our initial research in this direction.	computer;intelligent agent;multi-agent system;quantum computing	Matthias Klusch	2004	Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.	10.1109/AAMAS.2004.275	agent architecture;simulation;computer science;artificial intelligence;theoretical computer science;multi-agent system;distributed computing;quantum computer	Robotics	-42.885386950361614	17.02944073495474	73734
1dc51d44f9379dccadc6d6db6c3eed1d802f72da	sense: a service navigation system linked to a real-time advertising distribution service	service provider;service selection;service type repositories sense a service navigation system real time advertising distribution service agent system end user service provider interaction open distributed object environment multimedia network services service navigation strategy pre registered user information advertising information object services open distributed processing environment;advertising data processing;real time;multimedia networking;distributed objects;distributed object management;navigation real time systems advertising keyword search web and internet services search engines optical fiber networks history distributed processing technology management;agent systems;sense making;navigation system;open systems;open distributed processing;real time systems;real time systems distributed object management advertising data processing open systems	Sense: A Service Navigation System is an agent system supporting interaction between the end user (consumer) and service provider (producer) in an open, distributed object environment. It navigates among the latest multimedia network services based on the end user's wishes, profile and situations. Its service navigation strategy is a key feature of Sense; it navigates based not merely on pre-registered user information, but taking into consideration the history of service selection by the user, advertising information from the service provider and other clues. Moreover, Sense makes use of object services available in the open distributed processing environment, such as traders and service type repositories, enabling it to take the initiative in proposing the latest multimedia network services.	real-time transcription;sense	Ichizo Kogiku;Minoru Katayama;Takashige Hoshiai	1999		10.1109/ISORC.1999.776362	service provider;service level requirement;real-time computing;application service provider;business service provider;differentiated service;computer science;service delivery framework;service design;distributed computing;multimedia;distributed object;internet privacy;open system;service desk;world wide web;service system	Embedded	-45.90874984161742	14.090356317476225	73896
9af0d926ac415b51a4393a4555c7a5315ca7a827	ontology based negotiation case search system for the resolution of exceptions in collaborative production planning	distributed system;ontologie;raisonnement base sur cas;razonamiento fundado sobre caso;systeme reparti;logistique;negociation;systeme recherche;search system;sistema repartido;internet;logistics;sistema investigacion;negociacion;bargaining;programacion produccion;production planning;ontologia;supply chain;case based reasoning;planification production;ontology;logistica	In this paper, we present an ontology based negotiation case search system that supports contractors to solve exceptions generated during the operation of supply chain.		Chang Ouk Kim;Young Ho Cho;Jung Uk Yoon;Choonjong Kwak;Yoon Ho Seo	2005		10.1007/11575863_12	logistics;case-based reasoning;the internet;simulation;computer science;knowledge management;ontology;supply chain;negotiation	AI	-39.36777884098489	16.16735424889883	73977
2f59004af88fa403a9230032220842db3646d4c7	gestion des données temporelles dans un environnement multiversion de schémas	modelizacion;gestion memoire;ensayo no destructivo;essai non destructif;software maintenance;storage management;base donnee temporelle;data management;modelisation;maintenance logiciel;gestion memoria;temporal database;schema versioning;non destructive test;version management;temporal databases;systeme gestion base donnee;information system;modeling;sistema gestion base datos;database management system;systeme information;gestion version;sistema informacion	Temporal database management, where modification operations must be achieved with a non-destructive manner, is complex. It becomes more and more complex in an environment where the DBMS must keep track of component schema evolutions of the information system, since the data to be manipulated could be defined under different schema versions. However, evolution of these schemas is unavoidable and schema versioning is indispensable if one wants to guarantee a complete history of data. In order to better control this management. We first propose a model which allows representing and managing temporal data, in an environment supporting full schema versioning. We then develop a fine strategy for an effective management and a smooth evolution of these data.		Rafik Bouaziz;Zouhaier Brahmia	2009	Technique et Science Informatiques	10.3166/tsi.28.39-74	data management;computer science;data mining;database;temporal database;algorithm	Logic	-34.69108290368375	12.306262811648498	74012
e3e68e458c413ff33f7ffe63ff086d79bb42e5f1	ontological reengineering for reuse	informatica;developpement logiciel;adquisicion del conocimiento;and forward;ingenierie connaissances;information retrieval;conceptual model;computer software reusability;acquisition connaissances;systeme base connaissances;recherche information;desarrollo logicial;knowledge acquisition;software development;reutilisation logiciel;recuperacion informacion;environmental pollutant;knowledge based systems;reverse engineering;knowledge engineering	This paper presents the concept of Ontological Reengineering as the process of retrieving and transforming a conceptual model of an existing and implemented ontology into a new, more correct and more complete conceptual model which is reimplemented. Three activities have been identified in this process: reverse engineering, restructuring and forward engineering. The aim of Reverse Engineering is to output a possible conceptual model on the basis of the code in which the ontology is implemented. The goal of Restructuring is to reorganize this initial conceptual model into a new conceptual model, which is built bearing in mind the use of the restructured ontology by the ontology/application that reuses it. Finally, the objective of Forward Engineering is output a new implementation of the ontology. The paper also discusses how the ontological reengineering process has been applied to the Standard-Units ontology [18], which is included in a Chemical-Elements [12] ontology. These two ontologies will be included in a Monatomic-Ions and Environmental-Pollutants ontologies.	code refactoring;model-driven architecture;ontology (information science);reverse engineering	Asunción Gómez-Pérez;Dolores Rojas-Amaya	1999		10.1007/3-540-48775-1_9	upper ontology;conceptualization;ontology inference layer;computer science;knowledge management;ontology;artificial intelligence;conceptual model;software development;knowledge engineering;data mining;ontology-based data integration;process ontology;reverse engineering;suggested upper merged ontology	AI	-35.46664218621076	12.22074987854725	74090
d4391f4a70e279dca95cab98f1002dcaa1133f09	a framework for goal-oriented discovery of resources in the restful architecture	content extraction;representational stateless transfer system rest;web mining technique;protocols;services agents contents discovery information extraction representational stateless transfer system rest;electronic newspapers;standards;heterogeneous systems;information extraction;information retrieval;goal oriented resource discovery;html resource description framework semantics object oriented modeling protocols standards;semantics;discovery;cross cutting feature discovery;data exchange;representational stateless transfer system;web sites data mining electronic publishing information retrieval multi agent systems semantic web software architecture web services;web resource;resource description framework;data mining;html;content discovery;software architecture;multi agent systems;agents;web crawling;web services;web sites;web crawling goal oriented resource discovery restful architecture web 2 0 phenomenon semantic web data exchange heterogeneous systems web sites naive technique service discovery content discovery content extraction representational stateless transfer system web mining technique feature oriented modeling cross cutting feature discovery web resource electronic newspapers intelligent agent;intelligent agent;semantic web;restful architecture;services;electronic publishing;service discovery;naive technique;web 2 0 phenomenon;feature oriented modeling;object oriented modeling;contents	One of the challenges facing the current web is the efficient use of all the available information. The Web 2.0 phenomenon has favored the creation of contents by average users, and thus the amount of information that can be found for diverse topics has grown exponentially in the last years. Initiatives such as linked data are helping to build the Semantic Web, in which a set of standards are proposed for the exchange of data among heterogeneous systems. However, these standards are sometimes not used, and there are still plenty of websites that require naive techniques to discover their contents and services. This paper proposes an integrated framework for content and service discovery and extraction. The framework is divided into several layers where the discovery of contents and services is made in a representational stateless transfer system such as the web. It employs several web mining techniques as well as feature-oriented modeling for the discovery of cross-cutting features in web resources. The framework is used in a scenario of electronic newspapers. An intelligent agent crawls the web for related news, and uses services and visits links automatically according to its goal. This scenario illustrates how the discovery is made at different levels and how the use of semantics helps implement an agent that performs high-level tasks.	agent architecture;algorithm;fits;high- and low-level;intelligent agent;linked data;representational state transfer;semantic web;service discovery;stateless protocol;web 2.0;web mining;web resource;world wide web	José Ignacio Fernández-Villamor;Carlos Angel Iglesias;Mercedes Garijo	2014	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMCC.2013.2259231	data exchange;web service;web application security;communications protocol;software architecture;web mining;web development;web modeling;service;data web;web mapping;html;web standards;computer science;artificial intelligence;web crawler;software agent;ws-policy;semantic web;web navigation;rdf;social semantic web;data mining;semantic web stack;database;semantics;service discovery;web resource;electronic publishing;web intelligence;web 2.0;world wide web;intelligent agent;information extraction	Web+IR	-43.63507853642816	12.403071715768881	74116
28ec68550f3d900a4efa309a05ed48fd8667a847	assisting domain experts to formulate and solve constraint satisfaction problems	utilisation information;uso informacion;ontologie;gestion labor;information use;ingenierie connaissances;web semantique;structure formation;user feedback;acquisition connaissances;constraint satisfaction;arithmetique;satisfaction contrainte;gestion tâche;aritmetica;arithmetics;web semantica;knowledge acquisition;decouverte connaissance;semantic web;descubrimiento conocimiento;ontologia;constraint satisfaction problem;satisfaccion restriccion;adquisicion de conocimientos;task scheduling;ontology;knowledge discovery;knowledge engineering	Constraint satisfaction is a powerful approach to solving a wide class of problems. However, as many non-experts have problems formula ting t sks as Constraint Satisfaction Problems (CSPs), we have built a number of inter aces for particular kinds of CSPs, including crypt-arithmetic problems, map-c olouring problems, and scheduling tasks, which ask highly focused questions of the user, c.f., the earlier MOLE/MORE, and SALT knowledge acquisition systems. I nformation from each of these interfaces is then transformed initially into a structured format which is semantic web compliant and is secondly transforme d into the format required by the generic Constraint satisfaction problem solve r. When this problem solver is run, the user is either provided with solution(s) or fee dback that the problem is underspecified (when many solutions are feasib le) or over-specified (when no solution is possible). Effectively the system ha s 3 distinct phases, namely; information capture, transformation of the inform ation to that suitable for the standard problem solver, and thirdly the solving and us er feedback phase. We are planning to analyse in detail a greater range of the CSP tasks, and to produce further UIs to support these tasks. Secondly, we plan to explo it the existence of the intermediary representation which is semantic web compliant, b y enhancing the information about tasks with relevant information available f rom the semantic web.	constraint satisfaction problem;information capture;knowledge acquisition;salt (cryptography);scheduling (computing);semantic web;solver	Derek H. Sleeman;Stuart W. Chalmers	2006		10.1007/11891451_5	simulation;constraint satisfaction;structure formation;computer science;knowledge management;constraint graph;artificial intelligence;semantic web;knowledge engineering;ontology;constraint satisfaction dual problem;database;constraint satisfaction problem;algorithm;backtracking	AI	-36.93186778401253	6.758419981628534	74205
21dcc019ce5e1ccddcaa0d92a1b3519c80a86170	assessing the non-technical service aspects by using fuzzy methods	service orientation;service level agreement	The growing expectations for service-oriented applications tailored to the customers needs comprise a definition of the service contract documents, for both, technical and non-technical service elements. The usage of the fuzzy methods and techniques allows for the customized prearrangement of such application systems, which could be developed at the costs of the standard software. In the paper the concepts associated with a service-oriented design of non-technical service aspects especially appropriate for the fuzzy description of them as the requirements destined for Service Level Agreements SLAs, are introduced. The proposed approach is demonstrated on the example of a service accessibility requirement, an important component of a contracted SLA.		Andrzej Pieczynski;Silva Robak	2008		10.1007/978-3-540-69731-2_29	service level requirement;service level objective;service product management;application service provider;differentiated service;computer science;service delivery framework;service design;service guarantee;database;service desk;data as a service	NLP	-47.89250544712286	16.33583627897974	74295
649c2c546f22bb027a474086ddb113c5576ac42d	topincs wiki - a topic maps powered wiki	topic maps;web service	Topincs provides a RESTful web service interface for retrieval and manipulation of topic maps. A Topincs Server implementing the interface can host many stores, which are collections of topic maps. The Topincs Editor is a browser-based application that allows editing of topic maps. In this paper, the Topincs Wiki, another view on the content of a Topincs Store, is presented. Its purpose is to hide the Topic Maps paradigm from the user, thus simplifying the collaborative creation of topic maps. The Wiki still emphasizes formal statements. It uses  ontological reflection to guide users to data conformity rather than enforcing it with a schema.	topic maps;topincs;wiki	Robert Cerny	2007		10.1007/978-3-540-70874-2_7	topic maps;web service;personal wiki;computer science;artificial intelligence;database;world wide web;information retrieval	NLP	-39.693570720330726	9.375467675503382	74334
6bbad9b415509230427aa5ecb3daaaaff80806cf	an architecture for open cross-media annotation services	generic model;information sharing;new media	The emergence of new media technologies in combination with enhanced information sharing functionality offered by the Web provides new possibilities for cross-media annotations. This in turn raises new challenges in terms of how a true integration across different types of media can be achieved and how we can develop annotation services that are sufficiently flexible and extensible to cater for new document formats as they emerge. We present a general model for cross-media annotation services and describe how it was used to define an architecture that supports extensibility at the data level as well as within authoring and visualisation tools.	emergence;extensibility;new media;renderman shading language;world wide web	Beat Signer;Moira C. Norrie	2009		10.1007/978-3-642-04409-0_39	new media;computer science;database;multimedia;world wide web	Web+IR	-40.89574681752845	9.505316070115128	74406
59a63b06807b0fd66507ed69d3e632efd9866602	promoting adaptation of semantic web service composition using context information	silicon;semantic web service;context aware service composition framework;context information;web services information filters ontologies artificial intelligence ubiquitous computing;semantic web context aware services ontologies web services humans context modeling computer science computerized monitoring hybrid power systems information filtering;ontologies artificial intelligence;context model;machine interpretable form;context aware service;hybrid method;composition process semantic web service composition context aware service composition framework ontology based context model machine interpretable form;composition process;web services;semantic web;ontology based context model;ubiquitous computing;planning;ontologies;semantic web service composition;information filters;context modeling;context aware services	In this paper, we propose a context-aware service composition framework to ascertain the efficiency and flexibility for the composition process, and thereby improving the adaptation. The major technical contributions of this paper are: (1) we propose an ontology-based context model, OWL-SC, captures the service-related, environment-related and user-related context and can be used in an unambiguous, machine interpretable form. (2) We propose an approach to use the context information and to derive implicit context. (3) We propose a hybrid method to generate the composition process and filtering the inappropriate services.	semantic web service;service composability principle	Lirong Qiu;Yongcun Cao;Xiaobing Zhao;Guosheng Yang	2008	2008 International Symposium on Computer Science and Computational Technology	10.1109/ISCSCT.2008.325	computer science;knowledge management;database;context model;world wide web	SE	-44.126328922135116	14.428499030924499	74502
200c95dd31828adcf56eb3b2091a295e27cc9795	challenges for rule systems on the web	eca rule;reactive rules;interoperations;de facto;open sources;rule markup languages;rule based;eca rules;query languages;set of rules;production rules;active rules;deductive rule;computer programming languages;rule systems;distributed environment;object oriented;markup languages;semantic web;computer codes;research problems;combination rules;markup language;use case;ontology;open distributed environments;production rule;rule engine;open source;interchanges;linguistics	The RuleML Challenge started in 2007 with the objective of inspiring the issues of implementation for management, integration, interoperation and interchange of rules in an open distributed environment, such as the Web. Rules are usually classified as three types: deductive rules, normative rules, and reactive rules. The reactive rules are further classified as ECA rules and production rules. The study of combination rule and ontology is traced back to an earlier active rule system for relational and object-oriented (OO) databases. Recently, this issue has become one of the most important research problems in the Semantic Web. Once we consider a computer executable policy as a declarative set of rules and ontologies that guides the behavior of entities within a system, we have a flexible way to implement real world policies without rewriting the computer code, as we did before. Fortunately, we have de facto rule markup languages, such as RuleML or RIF to achieve the portability and interchange of rules for different rule systems. Otherwise, executing real-life rule-based applications on the Web is almost impossible. Several commercial or open source rule engines are available for the rule-based applications. However, we still need a standard rule language and benchmark for not only to compare the rule systems but also to measure the progress in the field. Finally, a number of real-life rulebased use cases will be investigated to demonstrate the applicability of current rule systems on the Web.	benchmark (computing);business rules engine;data model;database;entity;event condition action;executable;interoperability;interoperation;knowledge base;logic programming;markup language;national supercomputer centre in sweden;ontology (information science);open-source software;production (computer science);real life;relational model;rewriting;rule interchange format;ruleml;scalability;semantic web;software portability;web service;world wide web	Yuh-Jong Hu;Ching-Long Yeh;Wolfgang Laun	2009		10.1007/978-3-642-04985-9_4	rule-based system;ruleml;business rule management system;conflict resolution strategy;semantic web rule language;association rule learning;computer science;ontology;data mining;production rule representation;database;markup language;programming language;rule interchange format	DB	-38.6808337598076	7.555896288880192	74602
fcb387a65a530abd80581cf86c4f5800ec6e0f44	ontology based meta knowledge extraction with semantic web tools for ubiquitous computing	resource description framework;ontologies;electronic publishing;encyclopedias	The Semantic Web propels ubiquitous computing by driving the evolution of Web technologies for meaningful user-friendly access. It allows users to apply its potential to find, share and combine information. Ontology based annotations can be used in the Semantic Web to provide better support for organizing knowledge exchange between users. We address this issue to extract meta knowledge from the Web, regardless of devices (e.g., desktops, tablets). Accessing and reusing RDF data from many sources of varying credibility and authority is done through the Semantic Web. This makes the tracking, representation and utilization of various meta knowledge aspects such as reliability, provenance and timestamps extremely important. In this paper, we present an ontology based approach deployed to extract meta knowledge with the following tasks: (1) annotate and extract Wikipedia contents (2) create RDF metadata using the annotations (3) query metadata from the RDF files to obtain the required knowledge. Our general purpose is to link documents to Wikipedia fed to DBpedia / Cyc for extracting the shared lightweight ontology and utilize it for SPARQL querying of documents encompassing meta knowledge. In our approach and experiments we focus on academic scenarios. This knowledge extraction process is useful in developing AI tools and mobile apps where semantics is critical. This work enhances ubiquitous computing by providing efficient and accurate Web access for knowledge extraction on many platforms including desktop computers and mobile devices.	artificial intelligence;cyc;dbpedia;database;desktop computer;experiment;lightweight ontology;mobile app;mobile device;organizing (structure);relevance;requirement;resource description framework;sparql;semantic web;semantic publishing;social media mining;software requirements specification;tablet computer;ubiquitous computing;usability;wikipedia;world wide web	Aliva M. Pradhan;Aparna S. Varde	2016	2016 IEEE 7th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)	10.1109/UEMCON.2016.7777817	bibliographic ontology;web standards;computer science;sparql;ontology;knowledge-based systems;semantic web;rdf;social semantic web;linked data;data mining;semantic web stack;database;knowledge extraction;electronic publishing;world wide web;encyclopedia	Web+IR	-41.48388843587949	8.4767458524755	74937
0375c20bd5754957b68c9a0740fbcacf38cec6b7	conflicting viewpoint relational database querying: an argumentation approach	argumentation;decision support system;application within the ecobiocap project;description logics and dlr lite	Within the framework of the European project EcoBioCap, we model a real world use case aiming at conceiving the next generation of food packagings. The objective is to select packaging materials according to possibly conflicting requirements expressed by the involved parties (food and packaging industries, health authorities, consumers, waste management authority, etc.). The requirements and user preferences are modeled by several ontological rules provided by the stakeholders expressing their viewpoints and expertise. Since several aspects need to be considered (CO2 and O2 permeance, interaction with the product, sanitary, cost, end of life, etc.) in order to select objects, an argumentation process can be used to express/reason about different aspects or criteria describing the packagings. We define then in this paper an argumentation approach which combines a description logic (DLR-Lite) within ASPIC framework for relational database querying. The argumentation step is finally used to express and/or enrich a bipolar query employed for packaging selection.	adobe flash lite;description logic;dynamic language runtime;next-generation network;query language;requirement;user (computing)	Nouredine Tamani;Madalina Croitoru;Patrice Buche	2014			decision support system;computer science;knowledge management;artificial intelligence;argumentation theory;data mining;management science	AI	-44.40252567501918	4.832799558170013	75139
2984f6f7622b61821ed94e4fcff8331776277f5f	global and local qos constraints guarantee in web service selection	inf;service composition;web service;global constraint;functional equivalence;commerce;internet;integer programming;commerce internet linear programming constraint theory integer programming quality of service;linear programming;constraint theory;mixed integer linear problem;quality of service;service oriented architecture;web services quality of service service oriented architecture constraint optimization information retrieval mixed integer linear programming adaptive systems;business process;mixed integer linear problem local qos constraints global qos constraints web service selection service oriented architectures business processes web service composition problem	In service oriented architectures, complex applications are composed from a variety of functionally equivalent Web services, which may differ for quality parameters. Under this scenario, applications are defined as high level business processes and service composition can be implemented dynamically by identifying the best set of services available at run time. In this paper, we model the service composition problem as a mixed integer linear problem where both local constraints and global constraints can be specified.	business process;high-level programming language;linear programming;quality of service;run time (program lifecycle phase);service composability principle;service-oriented architecture;web service	Danilo Ardagna;Barbara Pernici	2005	IEEE International Conference on Web Services (ICWS'05)	10.1109/ICWS.2005.66	web service;mathematical optimization;the internet;integer programming;quality of service;differentiated service;computer science;linear programming;service-oriented architecture;database;distributed computing;business process;law	DB	-46.93969962355897	16.53132069744257	75246
95db9e18465ec8b8442b6b994b00bb0ce99a1e3f	web performance tuning - speeding up the web (2. ed.)	web performance		performance tuning;web performance	Patrick Killelea	2002			simulation;computer science;multimedia;world wide web	NLP	-34.80483704924385	17.889976406113377	75259
73147b0ebb7b9eaddb641bc1f4262978d20341ff	an ontology selection and ranking system based on the analytic hierarchy process	analytic hierarchy process;atomic measurements;semantics;computational modeling;abstracts;ontology metrics ontology selection ontology ranking analytic hierarchy process;ontologies;ontologies artificial intelligence analytic hierarchy process;tourism domain ontology selection system ontology ranking system analytic hierarchy process ontology reuse ontology evaluation ahp multiple criteria decision problem user preference end node measurement ontology representation ontology reasoning ontology management system;ontologies analytic hierarchy process semantics atomic measurements abstracts computational modeling	Selecting the desired ontology from a collection of available ones is essential for ontology reuse. We address the problem of evaluating, ranking and selecting ontologies according to user preferences. We exploit the Analytic Hierarchy Process (AHP) to solve the multiple-criteria decision problem and to model the preferences of the users. We use AHP to analyze the available ontologies from different perspectives and at different abstraction levels. The decision is based on the concrete end-node measurements and their relative importance at higher levels. For supporting the selection decision, we developed an ontology representation, reasoning and management system. The system applies different metrics on ontologies in order to feed the Analytic Hierarchy Process with facts. The running scenario applies our method to the task of reusing ontologies from the tourism domain.	abstraction layer;decision problem;emoticon;hierarchical database model;information;multi-agent system;ontology (information science);user (computing)	Adrian Groza;Irina Dragoste;Iulia Sincai;Ioana Jimborean;Vasile Moraru	2014	2014 16th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2014.47	upper ontology;ontology alignment;analytic hierarchy process;ontology components;ontology inference layer;computer science;knowledge management;ontology;artificial intelligence;data mining;semantics;linguistics;ontology-based data integration;computational model;information retrieval;process ontology;analytic network process;suggested upper merged ontology	Web+IR	-43.619663544629105	14.419102077239767	75304
6aaab5c06187370cd2d64925c8004263e5faab9f	ontobrokermature and approved semantic middleware		OntoBroker provides a comprehensive, scalable and high-performance Semantic Web middleware. It suppor ts all of the W3C Semantic Web recommendations for ontology la ngu ges and query languages. It is an ontology repo sitory that includes a high performance deductive reasoning engin . Especially reasoning with rules is a major uniqu e selling point for ontoprise. OntoBroker integrates a connector framework which makes it easy to connect a multitude of data sources like databases, web services etc. Thus it combines structure d and unstructured data in one framework, OntoBroker is asy to extend and to integrate into existing IT landscapes and applic ations as it offers a variety of open interfaces. O ntoBroker is also closely connected to ontoprise’s ontology modeling environm e t OntoStudio which is the development environment for handling ontologies, mappings to information sources, rules, g enerating queries, creating business intelligence r eports etc. At many customers OntoBroker serves as a common semantic layer which is accessed by various applications and integ ra s different information sources. OntoBroker is the run-time enviro nment for industrial solutions like SemanticGuide, S manticXpress, and SemanticIntegrator. As part of those meanwhile thou sands of installations are in productive use.	database;linear algebra;middleware;ontology (information science);query language;scalability;semantic web;software repository;web service	Jürgen Angele	2014	Semantic Web	10.3233/SW-2012-0067		AI	-38.7730917699115	7.461482813600772	75318
7450b810c45cd85b91e458ddd636ad556ef3cf95	a cloud based health insurance plan recommendation system: a user centered approach	plan ranking;recommendation system;multi attribute utility theory;cloud computing	The recent concept of ‘‘Health Insurance Marketplace’’ introduced to facilitate the purchase of health insurance by comparing different insurance plans in terms of price, coverage benefits, and quality designates a key role to the health insurance providers. Currently, the web based tools available to search for health insurance plans are deficient in offering personalized recommendations based on the coverage benefits and cost. Therefore, anticipating the users’ needs we propose a cloud based framework that offers personalized recommendations about the health insurance plans.We use theMulti-attribute Utility Theory (MAUT) to help users compare different health insurance plans based on coverage and cost criteria, such as: (a) premium, (b) co-pay, (c) deductibles, (d) co-insurance, and (e) maximum benefit offered by a plan. To overcome the issues arising possibly due to the heterogeneous data formats and different plan representations across the providers, we present a standardized representation for the health insurance plans. The plan information of each of the providers is retrieved using the Data as a Service (DaaS). The framework is implemented as Software as a Service (SaaS) to offer customized recommendations by applying a ranking technique for the identified plans according to the user specified criteria. © 2014 Published by Elsevier B.V.	big data;cloud computing;data as a service;emoticon;health insurance portability and accountability act;machine learning;personalization;recommender system;software as a service;user-centered design;utility	Assad Abbas;Kashif Bilal;Limin Zhang;Samee Ullah Khan	2015	Future Generation Comp. Syst.	10.1016/j.future.2014.08.010	cloud computing;computer science;operating system;data mining;recommender system	AI	-47.24849180370498	13.121829366504718	75365
376bec2ec00571690f906ad292dd276488b88000	ontology support for translating negotiation primitives	objeto de conferencia;language heterogeneity;negotiating agents;message sending;ciencias informaticas;service oriented architecture;ontology solution	In this paper we present an ontology solution to solve the problem of language heterogeneity among negotiating agents during the exchange of messages over Internet. Traditional negotiation systems have been implemented using different syntax and semantics. Our proposal offers a novel solution incorporating an ontology, which serves as a shared vocabulary of negotiation messages; and a translation module that is executed on the occurrence of a misunderstanding. We implemented a service oriented architecture for executing negotiations and conducted experiments incorporating different negotiation messages. The results of the tests show that the proposed solution improves the interoperability between heterogeneous negotiation agents.	experiment;ontology (information science);run time (program lifecycle phase);scott continuity;semantic interoperability;service-oriented architecture;systems architecture;vocabulary	Maricela Bravo;Máximo López;Azucena Montes Rendón;René Santaolaya Salgado;Raúl Pinto;Joaquín Pérez Ortega	2006		10.1007/978-0-387-34747-9_10	computer science;knowledge management;database;communication	AI	-42.0609416975056	17.5776474416857	75599
65ee78f2e44ccefff1e1ceb75a1a492ae113ea0f	efficient content creation on the semantic web using metadata schemas with domain ontology services (system description)	service system;semantic web;cost effectiveness;distributed collaboration;domain ontology	Metadata creation is one of the major challenges in developing the Semantic Web. This paper discusses how to make provision of metadata easier and costeffective by an annotation editor combined with shared ontology services. We have developed an annotation system supporting distributed collaboration in creating annotations, and hiding the complexity of the annotation schema and the domain ontologies from the annotators. Our system adapts flexibly to different metadata schemas, which makes it suitable for different applications. Support for using ontologies is based on ontology services, such as concept searching and browsing, concept URI fetching, semantic autocompletion and linguistic concept extraction. The system is being tested in various practical semantic portal projects.	ontology (information science);semantic web;uniform resource identifier	Onni Valkeapää;Olli Alm;Eero Hyvönen	2007		10.1007/978-3-540-72667-8_60	upper ontology;ontology alignment;cost-effectiveness analysis;bibliographic ontology;ontology inference layer;semantic grid;image retrieval;computer science;ontology;semantic web;social semantic web;semantic web stack;database;ontology-based data integration;world wide web;owl-s;information retrieval;service system	Web+IR	-41.05890652165848	6.114281512052135	75630
e008f5ba813c284d6ee4c7315c086ceee1e47361	information management for cooperative engineering	information management;cooperative engineering;knowledge base;process model;transaction processing;autonomic computing	In this paper we describe how a set of autonomous computational agents can cooperate in providing coherent management of information in environments where there are many diverse information resources. The agents use models of themselves and of the resources that are local to them. Resource models may be the schemas of databases, frame systems of knowledge bases, or process models of business operations. Models enable the agents and resources to use the appropriate semantics when they interoperate. This is accomplished by specifying the semantics in terms of a common ontology. We discuss the contents of the models, where they come from, and how the agents acquire them. We then describe a set of agents for telecommunication service provisioning and show how the agents use such models to cooperate. Their interactions produce an implementation of relaxed transaction processing.	autonomous robot;coherence (physics);database;information management;interaction;interoperability;knowledge base;provisioning;transaction processing	Michael N. Huhns;Munindar P. Singh	1999			knowledge base;data management;systems engineering;knowledge management;personal information management;management information systems;database;risk management information systems;business;structure of management information;information management;information system	AI	-44.34786163761747	9.915673953229886	75735
7c3949b4d07caaaf10f881be32ffea1a45865529	ontology revision using the concept of belief revision	semantic web;belief revision	One of the problems identified in the development of ontology is the difficulty in maintaining ontology that often faces on issues of changes in knowledge or perception about things within the community of practice. When new information is added, consistency needs to be maintained to ensure it does not cause inconsistency within existing concepts in ontology. This paper discusses the feasibility of using the concept of belief revision as a basis for ontology revision. It is an effort to the use of expansion, revision and contraction operators of belief revision to revise ontology.	belief revision	Seung Hwan Kang;Sim Kim Lau	2004		10.1007/978-3-540-30134-9_2	knowledge management;database;belief revision	AI	-45.15589746090159	6.4092975618893995	75944
b13e6f87daeb4503ff989b6f13009c8bbd39a3de	verifying temporal and epistemic properties of web service compositions	service composition;multi agent system;web service;web service composition;model checking	Model checking Web service behaviour has remained limited to checking safety and liveness properties. However when viewed as a multi agent system, the system composition can be analysed by considering additional properties which capture the knowledge acquired by services during their interactions. In this paper we present a novel approach to model checking service composition where in addition to safety and liveness, epistemic properties are analysed and verified. To do this we use a specialised system description language (ISPL) paired with a symbolic model checker (MCMAS) optimised for the verification of temporal and epistemic modalities. We report on experimental results obtained by analysing the composition for a Loan Approval Service.	information services procurement library;interaction;liveness;model checking;multi-agent system;service composability principle;web service	Alessio Lomuscio;Hongyang Qu;Marek J. Sergot;Monika Solanki	2007		10.1007/978-3-540-74974-5_43	web service;model checking;computer science;multi-agent system;data mining;database;law;world wide web	AI	-44.97136885942926	18.22777815673777	76050
31426955a7253e5abf71a62d6980e7b5b8291eac	wf2oml: a modeling language for mapping web forms to ontology	resource description framework databases ontologies html data mining books algorithm design and analysis;query processing;web sites knowledge representation languages ontologies artificial intelligence query processing relational databases semantic web sql;sql;mapping language;rdfs wf2oml modeling language ontology deep web database front end web form queries sparql queries semantic web application rdb2rdf mapping tool mapping language w3c r2rml relational database virtual sparql web site owl;ontologies artificial intelligence;knowledge representation languages;semantic web mapping language web form deep web ontology engineering sparql query;web form;web sites;ontology engineering;semantic web;relational databases;deep web;sparql query	Deep Web databases contain huge amounts of data that can only be accessed via the front-end Web form queries. Since query clients cannot directly issue SPARQL queries against the Deep Web databases, Semantic Web applications cannot take advantage of this information resource. Recently, many RDB2RDF mapping tools, relying on mapping languages such as W3C's R2RML, provide the ability to view existing relational databases in the RDF model, and offer a virtual SPARQL endpoint over the mapped relational data. However, these mapping languages require the information provider to expose the underlying information structure, and to reconstruct the Web site. In this paper we propose a modeling language, WF2OML, for mapping Web forms to OWL or RDFS ontologies. Based on the WF2OML mapping, SPARQL endpoint queries against the back-end database of the Web form can be achieved. We introduce WF2OML, propose the architecture and algorithm design ideas of a WF2OML-based SPARQL query system, and demonstrate a mapping example using our implemented prototype. The experimental results on real-life Web form query sites (e.g., Amazon.com) confirm the feasibility of the proposed method.	algorithm design;communication endpoint;deep web;form (html);modeling language;ontology (information science);prototype;rdf schema;real life;relational database;sparql;semantic web;web ontology language;web application;world wide web	Lei Chen;Zhuoming Xu;Lixian Ni	2013	2013 10th Web Information System and Application Conference	10.1109/WISA.2013.92	web service;web modeling;web query classification;named graph;data web;web mapping;web design;web standards;computer science;sparql;semantic web;web navigation;social semantic web;semantic web stack;database;rdf query language;web search query;world wide web;information retrieval;query language	Web+IR	-34.77658363310493	5.3918628674324065	76066
060884cd6505d944be0e717aabe0ae9eebec6a5d	system model and implementation of constraint-based distributed intelligent conceptual design system	groupware;cskaccd;design automation;conflict detection;groupware knowledge aided collaborative conceptual design constraint based distributed intelligent conceptual design group design multidisciplinary knowledge cskaccd knowledge representation domain specific knowledge conflict detection conflict resolution;collaborative work;image resolution;system modeling;knowledge aided collaborative conceptual design;intelligent systems knowledge representation design automation collaborative work power system modeling robustness image resolution sun computer science character generation;distributed processing;multidisciplinary knowledge;domain specific knowledge;intelligent design assistants;group design;distributed processing intelligent design assistants groupware knowledge representation constraint handling;conceptual design;character generation;intelligent systems;sun;constraint based distributed intelligent conceptual design;constraint handling;robustness;computer science;power system modeling;knowledge representation;conflict resolution;domain specificity;distributed design	The objective of computer supported knowledge aided collaborative conceptual design (CSKACCD) is to aid group design activities semi-automatically or automatically with multidisciplinary knowledge. This paper presents a general distributed design system model to give a global view of CSKACCD, and then designs a constraint-based distributed knowledge representation model based on different constraints abstracted from the domain specific knowledge of the designing artifact. Finally, this paper describes a robust distributed conflict detection and resolution mechanism developed for different designers to interact with each other effectively.		Yurong Xu;Shouqian Sun;Zongkai Lin;Yingxin Zhao	2001		10.1109/CSCWD.2001.942240	simulation;systems modeling;image resolution;computer science;knowledge management;artificial intelligence;conflict resolution;conceptual design;distributed design patterns;robustness	Robotics	-40.42634311242478	18.238076657421583	76136
c3705fd3d6a612ac11ecfcb6e3a3fb6951a8ecc0	a case study in integrating multiple e-commerce standards via semantic web technology	business to business;e commerce;semantic web technology;ontology integration;classification system;inventory control	Internet business-to-business transactions present great challenges in merging information from different sources. In this paper we describe a project to integrate four representative commercial classification systems with the Federal Cataloging System (FCS). The FCS is used by the US Defense Logistics Agency to name, describe and classify all items under inventory control by the DoD. Our approach uses the ECCMA Open Technical Dictionary (eOTD) as a common vocabulary to accommodate all different classifications. We create a semantic bridging ontology between each classification and the eOTD to describe their logical relationships in OWL DL. The essential idea is that since each classification has formal definitions in a common vocabulary, we can use subsumption to automatically integrate them, thus mitigating the need for pairwise mappings. Furthermore our system provides an interactive interface to let users choose and browse the results and more importantly it can translate catalogs that commit to these classifications using compiled mapping results.	align (company);bridging (networking);browsing;compiler;dictionary;dot-com company;e-commerce;eotd;internet;inventory control;logistics;ontology (information science);semantic web;sensitivity and specificity;subsumption architecture;user interface;vocabulary;web ontology language	Yang Yu;Donald Hillman;Basuki Setio;Jeff Heflin	2009		10.1007/978-3-642-04930-9_57	e-commerce;inventory control;computer science;artificial intelligence;data mining;database;world wide web	Web+IR	-42.606021298609065	4.390779901527427	76164
d6c7502edd3d492c10a7d8e9c6be003add8dd6d0	dynamic collectives and their collective dynamics	architecture systeme;theorie dynamique;group action;systeme information geographique;geographic information system;teoria dinamica;accion grupo;dynamic theory;arquitectura sistema;collective action;information system;system architecture;dynamique collective;action groupe;science information geographique;systeme information;sistema informacion geografica;analyse formelle;sistema informacion	Dynamic collectives are discrete dual-aspect phenomena: they may present themselves from different viewspoints as either objects or events, and they arise from the collective action of groups of individual elements. In this paper we outline a formal theory of dynamic collectives and discuss their relevance to a range of GIScience concerns, including the identities and lifestyles of geographical entities, the architecture of information systems, and the relation between information systems and modelling systems.	enterprise architecture;entity;geographic information science;information system;relevance	Antony Galton	2005		10.1007/11556114_19	artificial intelligence;group action;mathematics;geographic information system;information system;cartography;remote sensing	DB	-37.61154633981522	17.23675398707508	76197
3407d1f9e9bc140c8d2ebd95584226ee1b5f8c88	mappings from owl-s to uml for semantic web services	null	As a member of resources on Semantic Web, Web Services become increasingly important to realize accurate composition for interoperation between human beings and information systems. In order to address the need for semantic annotation of existing Web Services, a special UML profile can be built upon UML metamodel and MMFI4Ontology Registration for Semantic Web Services described with OWL-s. Then the mappings from OWL-s to UML are defined in detail to indicate that our UML profile and transformation rules can be used to represent services with UML activity models. Efforts on our UML profile and further mappings can enrich the semantics of ubiquitous Web Services, facilitate services modeling for domain experts and make it easy to update current tools for supporting growing services on semantic web.	owl-s;semantic web service;unified modeling language	Chong Wang;Keqing He;Yangfan He;Wei Qian	2006		10.1007/0-387-34456-X_40	semantic web rule language;uml tool;applications of uml;semantic web;social semantic web;semantic web stack;owl-s	Web+IR	-43.18655397628918	11.22860160570965	76206
8869e706462f155f46f5fe3e2e87b895acb3248b	distributed systems have distributed risks	distributed system;information communication;systeme reparti;procesamiento informacion;red www;analisis estructural;network analysis;communication information;sistema repartido;internet;information processing;comunicacion informacion;world wide web;reseau www;analyse structurale;traitement information;structural analysis;analyse circuit;analisis circuito		distributed computing	Peter G. Neumann	1996	Commun. ACM		the internet;information processing;network analysis;telecommunications;computer science;artificial intelligence;structural analysis	Graphics	-37.92841754984426	16.930474178652748	76387
059d3c0f4f62265b7d7cc26aa84d0040b96d60f7	combining ontologies for requirements elicitation	requirement deliverable requirement engineering ontology merging techniques requirement specification reasoning combined ontologies requirement elicitation process domain knowledge encapsulation;reasoning about programs formal specification knowledge acquisition knowledge representation ontologies artificial intelligence;ontologies merging cognition electronic commerce requirements engineering semantics software;requirements elicitation;combining ontologies;requirements engineering;ontologies;ontology merging;ontology merging ontologies combining ontologies requirements engineering requirements elicitation	A variety of ontologies are used to define and represent knowledge in many domains. Many ontological approaches have been successfully applied in the field of Requirements Engineering. In order to successfully harness the disparate ontologies, researchers have focused on various ontology merging techniques. However, no serious attempts have been made in the area of Requirements Elicitation where ontology merging has the potential to be quite effective in generating requirements specifications quickly through the means of reasoning based on combined ontologies. This paper attempts to define an approach needed to effectively combine ontologies to enhance the Requirements Elicitation process. A methodology is proposed whereby domain knowledge encapsulated in existing ontologies is combined with an ontology being developed to capture the requirements. Using this, requirements engineers would be able to create more refined Requirements Deliverables.	ontology (information science);ontology merging;requirement;requirements elicitation;requirements engineering	Xiaobu Yuan;Shubhrendu Tripathi	2015	2015 IEEE International Model-Driven Requirements Engineering Workshop (MoDRE)	10.1109/MoDRE.2015.7343873	idef5;computer science;systems engineering;knowledge management;ontology;requirement;requirements elicitation;data mining;process ontology	SE	-44.96923792476868	6.541681597959008	76489
199c2f318705ce90166fedf0f7ecfb85a42a0e91	mashup web data sources and services based on semantic queries	multi data sources;conflict detection;information extraction;web service;semantic queries;xml document;mashup;conflicts detection;heterogeneous data sources	This paper describes a process for mashing heterogeneous data sources based on the Multi-data source Fusion Approach (MFA) (Nachouki and Quafafou, 2008 [52]). The aim of MFA is to facilitate the fusion of heterogeneous data sources in dynamic contexts such as the Web. Data sources are either static or active: static data sources can be structured or semi-structured (e.g. XML documents or databases), whereas active sources are services (e.g. Web services). Our main objective is to combine (Web) data sources with a minimal effort required from the user. This objective is crucial because the mashing process implies easy and fast integration of data sources. We suppose that the user is not expert in this field but he/she understands the meaning of data being integrated. In this paper, we consider two important aspects of the Web mashing process. The first one concerns the information extraction from the Web. The results of this process are the static data sources that are used later together with services in order to create a new result/application. The second one concerns the problem of semantic reconciliation of data sources. This step consists to generate the Conflicts data source in order to improve the problem of rewriting semantic queries into sub-queries (not addressed in this paper) over data sources. We give the design of our system MDSManager. We show this process through a real-life application.		Gilles Nachouki;Mohamed Quafafou	2011	Inf. Syst.	10.1016/j.is.2010.08.001	web service;xml;data web;computer science;linked data;data mining;semantic web stack;database;world wide web;information extraction;mashup	DB	-39.36404752060453	8.121156037757768	76837
c17f6abf3c55c7835e32c2092d353396254d0886	proximal business intelligence on the semantic web	semantic web;business intelligence;article;pervasive informatics	Ubiquitous information systems (UBIS) extend current Information System thinking to explicitly differentiate technology between devices and software components with relation to people and process. Adapting business data and management information to support specific user actions in context is an ongoing topic of research. Approaches typically focus on providing mechanisms to improve specific information access and transcoding but not on how the information can be accessed in a mobile, dynamic and ad-hoc manner. Although web ontology has been used to facilitate the loading of data warehouses, less research has been carried out on ontology based mobile reporting. This paper explores how business data can be modeled and accessed using the web ontology language and then re-used to provide the invisibility of pervasive access; uncovering more effective architectural models for adaptive information system strategies of this type. This exploratory work is guided in part by a vision of business intelligence that is highly distributed, mobile and fluid, adapting to sensory understanding of the underlying environment in which it operates. A proof-of-concept mobile and ambient data access architecture is developed in order to further test the viability of such an approach. The paper concludes with an ontology engineering framework for systems of this type – named UBIS-ONTO.	ambient devices;centralized computing;component-based software engineering;data access;hoc (programming language);information access;management information system;ontology (information science);ontology engineering;pervasive informatics;semantic web;sensor;test data;web ontology language;world wide web	David Bell	2010		10.1007/978-3-642-15141-5_12	computer science;data science;social semantic web;semantic web stack;database;web intelligence;world wide web	HCI	-46.48732581178482	9.689418630762589	76972
172c10eaedef980668b4a161e38561a0073cc995	construction of domain ontologies: sourcing the world wide web	ontology creation methodology;ontology evaluation;semantic web;world wide web;cognitive fit theory;domain ontology	As the World Wide Web evolves into the Semantic Web, domain ontologies, which represent the concepts of an application domain and their associated relationships, have become increasingly important as surrogates for capturing and representing the semantics of real world applications. Much ontology development remains manual and is both difficult and time-consuming. This research presents a methodology for semi-automatically generating domain ontologies from extracted information on the World Wide Web. The methodology is implemented in a prototype that integrates existing ontology and web organization tools. The prototype is used to develop ontologies for different application domains, and an empirical analysis carried out to demonstrate the feasibility of the research. and analyze data and is driven by the role of semantics for automated approaches to exploiting Web resources (Berners-Lee, Hendler, & Lassila, 2001). Ontologies, which are at the heart of the Semantic Web, define the concepts and relationships that make global interoperability possible, facilitate sharing and integration (Horrocks, 2008; Leukel & Sugumaran, 2009; Tun & Tojo, 2008) and serve as surrogates for semantics. Ontologies are also useful for digital libraries and personalized information management (Katifori, Halatsis, Lepouras, Vassilakis, & Giannopoulou, 2007; Meng & Chatwin, 2010). Although their need is well-documented, ontology development is often performed manually and is challenging and time-consuming (Ding DOI: 10.4018/jiit.2011040101 2 International Journal of Intelligent Information Technologies, 7(2), 1-24, April-June 2011 Copyright © 2011, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. & Foo, 2002; Farquhar, Fikes, & Rice, 1997). One of the major reasons for this difficulty is finding relevant knowledge sources to use to create ontologies. The World Wide Web is a great resource of information for almost all imaginable domains. If this information could be properly extracted and organized, it should be possible to effectively use it to create domain ontologies, especially if a process to do so could be automated to some extent (Sánchez & Moreno, 2008). The objectives of this research, therefore, are to: • Develop a methodology for semi-automatically generating domain ontologies by extracting and organizing terms and relationships among those terms using the World Wide Web as a source; • Establish the feasibility of the ontology creation methodology by creating a prototype; and • Assess the performance of the methodology through an empirical analysis. The contribution of the research is to develop a way to semi-automatically create domain ontologies by using the World Wide Web as a source and integrating web tools. Libraries could be used for the Semantic Web and other applications (e.g., heterogeneous databases, conceptual modeling, and web queries (Horrocks, 2008; Ram & Zhao, 2007)). Section 2 of this paper examines related research on domain ontologies and their role in the Semantic Web. A six-step ontology creation methodology is presented in Section 3. Section 4 details the implementation of the methodology in a prototype, WebtoOnto. Section 5 evaluates the methodology using an empirical study. A summary and concluding remarks are found in Section 6.	application domain;database;digital library;foobar;information management;interoperability;library (computing);ontology (information science);ora lassila;organizing (structure);personalization;prototype;random-access memory;semantic web;semiconductor industry;surrogates;tun (product standard);web resource;world wide web	Jongwoo Kim;Veda C. Storey	2011	IJIIT	10.4018/jiit.2011040101	upper ontology;idef5;open biomedical ontologies;ontology alignment;web modeling;ontology components;bibliographic ontology;ontology inference layer;web standards;computer science;knowledge management;ontology;semantic web;web navigation;social semantic web;data mining;semantic web stack;web intelligence;ontology-based data integration;world wide web;owl-s;process ontology;semantic analytics	AI	-43.01792895410447	5.44254720431355	76984
08ee3bbc6426fd346686105dd6b4b22f7472194f	the semantic shadow: structuring the web for adaptations		This paper introduces the concept of the Semantic Shadow (SemS), a model for managing contentual and structural annotations on web page elements and their values. While the model is based on RDF, it supports a contextual weighting of the annotated information, allowing the annotator to specify the annotation values in relation to the evaluation context. This supports dynamic web page adaptation based on structural semantics on the page, whereas a context-awareness of the adaptation can be directly modeled alongside with the structural annotations.	context awareness;data model;document object model;dynamic web page;graphical user interface;html;interaction;online and offline;overhead (computing);real-time web;static program analysis;usage data;world wide web;xpath	Pascal Bihler;Armin B. Cremers	2011	ECEASST	10.14279/tuj.eceasst.37.474	computer science;semantic web stack;database;world wide web;information retrieval	Web+IR	-34.663806040077134	6.541985956820548	77010
ce87d8cccbc7cb1b9ad7dec06e4cee660dcbbb90	conceptual modeling using knowledge of domain ontology		Conceptual modeling play an important role in software development. In initial phase of conceptual modeling domain models are constructed. They should be consistent with the reality they intend to present. Existing ontologies can be perceived as a valuable source for domain knowledge. The paper presents a method supporting development of domain models on the base of knowledge represented by a domain ontology. The main benefit of the method is that the developed domain model preserves consistency with the domain ontology in the context of the given domain problem. The proposed method is illustrated by an example.		Bogumila Hnatkowska;Zbigniew Huzar;Lech Tuzinkiewicz;Iwona Dubielewicz	2016		10.1007/978-3-662-49390-8_54	upper ontology;conceptual model;ontology inference layer;ontology;domain model;ontology-based data integration;process ontology;suggested upper merged ontology	NLP	-45.06637254030986	6.463192266734215	77070
23758985b4e83690ee7ec271c64f2588e6687983	interpreting xml documents via an rdf schema ontology	hypermedia markup languages;information resources;semantic annotation;semantic networks;data model;business data processing;xml resource description framework ontologies data models semantic web computer displays computer vision automation application software information retrieval;data models hypermedia markup languages semantic networks information resources business data processing;semantic web;xml document;semantic annotation xml document interpretation rdf schema ontology business documents xml representation data meaning advanced automated processing semantic web ontologies rdf statements schema specification;data models	"""Many business documents are represented in XML. However XML only describes the structure of data, not its meaning. The meaning of data is required for advanced automated processing, as is envisaged in the """"Semantic Web"""". Ontologies are often used to describe the meaning of data items. Many ontology languages, however, depend on the RDF data model and cannot describe XML data directly. This paper presents a procedure that can be used to turn XML documents into RDF statements via an RDF schema specification. This allows semantic annotation of XML documents via external RDF schema ontologies. This procedure makes the data in XML documents available for the Semantic Web."""	rdf schema;xml	Michel C. A. Klein	2002		10.1109/DEXA.2002.1046008	data exchange;xml validation;rdf/xml;data modeling;xml encryption;cwm;xhtml;xml;semantic web rule language;xml schema;data model;streaming xml;computer science;document structure description;xml framework;semantic web;xml schema;database;xml signature;semantic network;web ontology language;world wide web;xml schema editor;information retrieval;semantic analytics;efficient xml interchange;rdf schema	Web+IR	-37.980133702403215	7.279563478766742	77235
0ebd82d7330e5790936062efdd059dd51841acc7	multi-agent technology for distributed data mining and classification	software tool;collaborative work;multi agent system;information retrieval;distributed processing;data mining;decision support systems multi agent systems data mining distributed processing information retrieval;multi agent systems;decision support systems;data mining collaborative work collaborative software paper technology software design multiagent systems computer architecture software tools protocols collaborative tools;agent technology;ddm protocol multiagent technology distributed data mining distributed classification collaborative work distributed software multiagent system0 multiagent software architecture collaborative design;distributed data mining;collaborative design	The core problem of multi-agent distributed data mining technology not concern particular data mining techniques although the latter is now paid the most attention. Its core problem concerns collaborative work of distributed software in design of multi-agent system destined for distributed data mining and classification. The paper presents the developed and implemented distributed data mining technology, architecture of the multi-agent software tool supporting this technology and demonstrates the key protocols used by agents in collaborative design of an applied multi-agent distributed data mining system.	data mining;distributed computing;multi-agent system;programming tool	Vladimir Gorodetsky;Oleg Karsaev;Vladimir Samoilov	2003		10.1109/IAT.2003.1241116	computer science;knowledge management;data mining;database;distributed design patterns	ML	-40.170433234389066	18.05580755258444	77323
5f47dcfa537d6d97e522227c1c71e5d540525fb4	an agent model for the resolution of feature conflicts in telephony	service provider;agent based;agent modeling;policies;feature interaction;fuzzy logic;agent negotiating;conflict resolution	Most telecommunication service providers resolve the feature interaction problem by providing specific instructions in their management software to handle scenarios where feature interaction may occur. This approach suffers from the complexity of the resulting code and the difficulty of adding new features to the system. Moreover, the system predefines the result of the resolution of the conflicting features and the end user has no means of choosing a different behavior, depending on the preferences of the user. In this paper we propose an agent-based architecture to detect and resolve feature interactions. Our system benefits from the flexibility and the semantic richness of policies and fuzzy logic to allow the end user to alter the behavior of the system, thus obtaining a more personalized service.	agent-based model;feature interaction problem;fuzzy logic;personalization;prototype;run time (program lifecycle phase)	Magdi Amer;Ahmed Karmouch;Tom Gray;Serge Mankovskii	2000	Journal of Network and Systems Management	10.1023/A:1009438422110	fuzzy logic;service provider;simulation;computer science;knowledge management;conflict resolution;data mining;computer security	Web+IR	-43.17612606640549	16.816281853883375	77414
3e8d81d795fcc71243158cc3d71e690cfee8b4c9	using semantic dependencies for consistency management of an ontology of brain-cortex anatomy	software agent;automatic generation;consistency management;ontology maintenance;semantic web;dependencies between concepts or relationships;ontology consistency;knowledge base	OBJECTIVE In the context of the Semantic Web, ontologies have to be usable by software agents as well as by humans. Therefore, they must meet explicit representation and consistency requirements. This article describes a method for managing the semantic consistency of an ontology of brain-cortex anatomy.   METHOD The methodology relies on the explicit identification of the relationship properties and of the dependencies that might exist among concepts or relationships. These dependencies have to be respected for insuring the semantic consistency of the model. We propose a method for automatically generating all the dependent items. As a consequence, knowledge base updates are easier and safer.   RESULT Our approach is composed of three main steps: (1) providing a realistic representation, (2) ensuring the intrinsic consistency of the model and (3) checking its incremental consistency. The corner stone of ontological modeling lies in the expressiveness of the model and in the sound principles that structure it. This part defines the ideal possibilities of the ontology and is called realism of representation. Regardless of how well a model represents reality, the intrinsic consistency of a model corresponds to its lack of contradiction. This step is particularly important as soon as dependencies between relationships or concepts have to be fulfilled. Eventually, the incremental consistency encompasses the respect of the two previous criteria during the successive updates of the ontology.   CONCLUSION The explicit representation of dependencies among concepts and relationships in an ontology can be helpfully used to assist in the management of the knowledge base and to ensure the model's semantic consistency.	anatomic structures;checking (action);computational anatomy;consistency model;entity name part qualifier - adopted;increment;knowledge base;neuroanatomy;ontology (information science);published comment;requirement;rule (guideline);semantic web;software agent;subject-matter expert	Olivier Dameron;Bernard Gibaud;Mark A. Musen	2004	Artificial intelligence in medicine	10.1016/j.artmed.2006.09.004	upper ontology;knowledge base;ontology inference layer;computer science;knowledge management;ontology;artificial intelligence;consistency model;software agent;semantic web;data mining;database;ontology-based data integration	AI	-45.173520288702704	6.466647643726534	77443
0d521aaa0a2e7388c407ad80b4d1b22130780d05	icgrid: enabling intensive care medical research on the egee grid	egee;grid computing;. intensive care medicine	Healthcare Information Systems are nowadays among the world's largest, fastest-growing and most information intensive industries. Additionally, Intensive Care Units are widely considered as the most technologically advanced environments within a hospital. In such environments, physicians are confronted with the challenge of storing and analyzing terabytes of data that stream in real-time from inpatients to centralized clinical information systems. In this paper we present the system architecture and early experiences from ICGrid (Intensive Care Grid), a novel system framework that enables the seamless integration, correlation and retrieval of clinically interesting episodes across Intensive Care Units, by utilizing the EGEE infrastructure. ICGrid is based on a hybrid architecture that combines i) a heterogeneous set of monitors that sense the inpatients and ii) Grid technology that enables the storage, processing and information sharing task between Intensive Care Units. ICGrid makes it feasible for doctors to utilize the EGEE Infrastructure through simple, intuitive user interfaces, while the infrastructure itself handles the complex task of information replication, fault tolerance and sharing.	centralized computing;computational resource;egi;experience;fastest;fault tolerance;genetic heterogeneity;information systems;information system;real-time transcription;research data archiving;seamless3d;systems architecture;terabyte;user interface;inpatient;intensive care	K. Harald Gjermundrød;Marios D. Dikaiakos;Demetrios Zeinalipour-Yazti;George Panayi;Theodoros Kyprianou	2007	Studies in health technology and informatics		data mining;grid;grid computing;architecture;systems architecture;fault tolerance;user interface;information sharing;information system;distributed computing;medicine	HPC	-36.43287522664248	15.352410709958907	77455
332b4c485b0d9d88c4692be61f662a48a9316c92	dwobs: data warehouse design from ontology-based sources	data warehouse design;case tool;data warehousing;data warehouse;domain ontology	In the past decades, data warehouse (DW) applications were built from traditional data sources. The availability of domain ontologies creates the opportunity for sources to explicit their semantics and to exploit them in many applications, including in data warehousing. In this paper, we present DWOBS, a case tool for ontological-based DW design based on domain ontologies. It takes as inputs (i) a set of sources referencing OWL ontology and (ii) a set of decisional requirements formulated using Sparql syntax on that ontology. DWOBS gives a semantic multidimensional model of the DW to be designed.		Selma Khouri;Ladjel Bellatreche	2011		10.1007/978-3-642-20152-3_34	upper ontology;dimensional modeling;computer science;ontology;data warehouse;data mining;database;ontology-based data integration;computer-aided software engineering;information retrieval;process ontology	EDA	-38.32378692535503	6.119618069957041	77556
d59b6c9d6d897573ac4213c5f7dd88d9f24340c3	building a network community support system on the multi-agent platform shine	community;agent platform;dynamic change;organization;multiagent system;reseau communication;architecture systeme;community organization;soporte;support system;support;multi agent architecture;comunidad;arquitectura sistema;system architecture;sistema multiagente;organisation;red de comunicacion;organizacion;systeme support reseau;communication network;communaute;systeme multiagent	An increasing number of applications have been developed for supporting network communities. The authors have developed Community Organizer, which supports people in forming new network communities by providing places where people sharing interests and concerns can meet and communicate. The authors are also developing a platform named Shine to reduce the tasks needed to implement a variety of network community support systems such as Community Organizer. Shine has a multi-agent architecture because it is effective for network community support systems that have to adapt to dynamic changes in community organizations. This paper explains both Community Organizer and Shine, and then gives a description of building Community Organizer on top of Shine.		Sen Yoshida;Koji Kamei;Takeshi Ohguro;Kazuhiro Kuwabara;Kaname Funakoshi	2000		10.1007/3-540-44594-3_7	simulation;organization;artificial intelligence;systems architecture	Robotics	-38.8600064990658	16.658588560488422	77594
08d3068a31ff9c65f7193387cf33b1eb2977c0f3	an approach to extract restful services from web applications	service extraction;reusable tasks;web applications;restful services;design techniques;web services;web task reusability;design tools;web tasks	The web is the largest database with a huge amount f information and services primarily intended for human users. A user performs different tasks on the web, such as reserving a table in a restaurant, and buying a movie ticket. Similar tasks are often performed in various web applications. The reuse of web application comp onents would offer greater productivity and ease the maintenance of web applications. Due to th e short time-to-market and the faster pace of technology development, designing reusable web appl ication components is often not a primary concern for developers. The focus of this paper is to circumvent this limitation by proposing an approach to interactively identify reusable web tas ks in a web application. We represent these tasks as services that developers can reuse to spee d u the development of their web applications. We perform a case study on 21 real world web applic ations from four domains. We identify tasks and services from these web applications. Results sh ow t at our proposed approach can identify tasks correctly with a precision of 89% and a recal l of more than 90%. Our proposed approach also successfully identifies relations among tasks with a precision of 86% and 100% recall. Hence, using our approach, a web developer can semi -automatically extract reusable tasks and represent each task as a RESTful service.	adobe flash;css code;client-side;database;html;input/output;interactivity;javascript;metamodeling;microsoft silverlight;precision and recall;representational state transfer;semiconductor industry;server (computing);tput;user interface;web application;web developer;web service;world wide web	Bipin Upadhyaya;Ying Zou;Foutse Khomh	2015	IJBPIM	10.1504/IJBPIM.2015.071262	web service;web application security;web development;web application;web modeling;data web;web analytics;web mapping;web-based simulation;web design;web standards;computer science;web api;ws-policy;web navigation;web page;data mining;database;multimedia;web intelligence;programming language;web 2.0;law;world wide web;mashup	Web+IR	-40.57891386395119	9.926387851100523	77874
addb6b044eb95da46954cc1ef6070c0a8ace98cd	approaching the interconnection of heterogeneous knowledge bases on a knowledge grid	atmospheric measurements;search engines;information retrieval;heterogeneous knowledge;knowledge base interconnection of heterogeneous knowledge bases knowledge grid;collaborative editing;wiki;information services;wiki heterogeneous knowledge knowledge grid knowledge sharing collaborative editing;knowledge grid;indexing;knowledge based systems information retrieval;knowledge sharing;correlation;search engines collaboration communications technology medical services accidents grid computing navigation;interconnection of heterogeneous knowledge bases;knowledge based systems;knowledge base;knowledge engineering	This paper presents a method for the interconnection of heterogeneous knowledge bases on a knowledge grid for knowledge sharing and provision. Various knowledge bases have been created by using collaborative editing environments such as Wiki in each field. Each knowledge base exists independently. On the other hand, an event affects various aspects of an area, field or community. Therefore, in today's global environment, it is important to transmit significant knowledge related to accidental or irregular events to actual users from various knowledge bases. In this paper, we propose a method for the extraction of related items across heterogeneous fields by interconnecting each knowledge base arranged on a knowledge grid.	database;error-tolerant design;global variable;interconnection;knowledge base;semiconductor industry;web resource;wiki;world wide web	Takafumi Nakanishi;Koji Zettsu;Yutaka Kidawara;Yasushi Kiyoki	2008	2008 Fourth International Conference on Semantics, Knowledge and Grid	10.1109/SKG.2008.64	search engine indexing;knowledge base;computer science;knowledge management;artificial intelligence;data science;knowledge-based systems;knowledge engineering;open knowledge base connectivity;data mining;database;knowledge extraction;world wide web;correlation;information system	DB	-45.117817038556495	7.896553704348852	78053
4dc2da044f5e6bf5729e81f84090d3e1a3971472	wisdom-ii: a network centric model for warfare	modelizacion;distributed system;numero complejo;aplicacion militar;multiagent system;systeme reparti;application militaire;guerra;agent based;complex adaptive system;intelligence artificielle;sistema complejo;systeme adaptatif;modelisation;sistema repartido;war;network centric warfare;systeme complexe;complex system;adaptive system;military application;sistema adaptativo;artificial intelligence;complex number;inteligencia artificial;sistema multiagente;modeling;guerre;nombre complexe;systeme multiagent	With recognition of warfare as a complex adaptive system, a number of agent based distillation systems for warfare have been developed and adopted to study the dynamics of warfare and gain insight into military operations. These systems have facilitated the analysis and understanding of combat. However these systems are unable to meet the new needs of defence arising from the deeper understanding of warfare and the emergence of the theory of network centric warfare. In this paper, we propose a network centric model which provides a new approach to understand and analyse the dynamics of both platform centric and network centric warfare.	agent-based model;algorithm;complex adaptive system;emergence;natural language user interface;network-centric warfare;parallel computing;real-time transcription;semiconductor industry;unified framework	Ang Yang;Hussein A. Abbass;Ruhul A. Sarker	2005		10.1007/11553939_115	complex adaptive system;systems modeling;computer science;artificial intelligence;adaptive system;complex number;war;operations research	AI	-38.00735790627077	16.961295156589095	78126
a75a30d58e5a3423822f223ce2633e0e127ea42b	automated integration of web services in bpel4ws processes	semantic similarity;building block;web service;on the fly;service discovery;service oriented architecture;integrated services	In order to fully exploit the potential of dynamic service-oriented architectures based on Web Services we provide a novel self-integration service infrastructure that supports automatic service discovery and reconfiguration for BPEL4WS processes. Our service discovery approach takes into account possible runtime transformations such that a service in a BPEL4WS process can be replaced by a semantically similar service even if service interfaces and message structures do not match. We present the main building blocks of our solution, i.e. a standard-conformant WSDL schema extension called Mediation Contract Extension (MECE), a corresponding semantic discovery algorithm, and a runtime mediation system that generates appropriate XSLT transformations on the fly. Our solution dynamically instantiates mediators to bind services to service processes specified with BPEL4WS.	algorithm;business process execution language;mece principle;on the fly;service discovery;service-oriented architecture;service-oriented software engineering;web services description language;web service;xslt	Steffen Bleul;Diana Elena Comes;Kurt Geihs;Marc Kirchhoff	2009		10.1007/978-3-540-92666-5_9	web service;semantic similarity;web standards;computer science;service delivery framework;ws-policy;service-oriented architecture;semantic web;service design;social semantic web;data mining;ws-addressing;semantic web stack;database;service discovery;integrated services;services computing;law;world wide web;devices profile for web services;universal description discovery and integration;computer network;web coverage service	HPC	-43.45511065136729	13.362392973478661	78183
4f733d23bcfaeb7630ea0765a823d38f86c2496a	qos-aware automatic composition of web services using ai planners	owl;web services artificial intelligence quality of service ontologies owl semantic web distributed computing collaboration formal languages process planning;web services distributed processing quality of service;distributed processing;collaboration;distributed computing;formal languages;web service;ai planners;pddl based planners qos aware automatic composition web services ai planners distributed computing language independent interfaces;qos aware automatic composition;web services;pddl based planners;semantic web;artificial intelligence;ontologies;process planning;quality of service;language independent interfaces	Web services provide a new way of distributed computing that achieve the interoperability between heterogeneous applications through platform and language independent interfaces. As the number of available Web services in a system increases, it becomes more difficult to find the specific service that can perform the task at hand. There may also be no single service capable of performing that task, but a combination of existing services may provide the capability. One of the main focuses of Web services is the ability to easily combine existing components to create compositions that provide novel functionalities which were not directly available from the existing services. In this paper a new approach for composing Web services based on requested quality of services is provided. The presented procedure exploits numeric function supporting planners to build the composition. It also provides the facility to generate compositions of Web services by using existing PDDL based planners.	distributed computing;heterogeneous computing;interoperability;planning domain definition language;quality of service;web service	Mahsa Naseri;Ahmad Towhidi	2007	Second International Conference on Internet and Web Applications and Services (ICIW'07)	10.1109/ICIW.2007.51	web service;web development;web modeling;business process execution language;web mapping;web standards;computer science;knowledge management;artificial intelligence;ws-policy;service-oriented architecture;ws-addressing;database;services computing;ws-i basic profile;world wide web	Web+IR	-43.69529524438089	13.856365263561342	78280
ce36fd84872535172815080b4e6d528a4c8d454b	ontology and cbr-based dynamic enterprise knowledge repository construction	case based reasoning;ontology	The efficiency of knowledge sharing and learning is the key to obtain sustainable development for the knowledge-intensive industry. However, current application of enterprise knowledge repository can hardly adapt to the personalized retrieval with semantic expansion and can not support the dynamic mechanism of knowledge sharing. This paper focuses on an integrated framework and operating processes of dynamic knowledge repository construction. Through analyzing the key technology points of business logic processing layer and data services layer particularly, the ontology and CBR-based knowledge storage and retrieval mechanism are studied, which improve the effectiveness of knowledge management.	algorithm;business logic;case-based reasoning;centralized computing;dynamic enterprise;dynamic knowledge repository;enterprise modelling;knowledge engineering;knowledge management;numerical analysis;personalization	Huiying Gao;Xiuxiu Chen	2012	JSW		case-based reasoning;knowledge integration;computer science;knowledge management;knowledge-based systems;knowledge engineering;ontology;open knowledge base connectivity;data mining;database;procedural knowledge;knowledge extraction;personal knowledge management;domain knowledge	Web+IR	-47.86002946963398	8.5020630498047	78318
ccab940cce4810212a65b83d2b21016be90c25f0	searching semantic web services: an intelligent agent approach using semantic enhancement of client input term(s) and matchmaking step	semantic web service;owl s;client server system;client server systems;web service;satisfiability;data mining;ontologies artificial intelligence;software agents;knowledge representation languages;smart web query engine;semantic web intelligent agent impedance matching web services simple object access protocol data mining xml ontologies competitive intelligence engines;software agents client server systems data mining knowledge representation languages ontologies artificial intelligence semantic web;semantic description;intelligent agent;semantic web;matchmaking algorithm;uddi;semantic web service discovery;client server system semantic web service discovery uddi intelligent agent owl s semantic description smart web query engine matchmaking algorithm	This paper proposes a smart mechanism to discover semantic Web services satisfying client requirements. The increase in Web services and lack of semantic base in search mechanisms of UDDI make it difficult for clients to find a required Web service. It is proposed to develop an intelligent agent to discover required Web services from Web. The system uses OWL-S for describing semantics of Web services and discovers appropriate semantic Web services through these semantic descriptions. OWL-S allows semantic description of Web services and hence, the intelligent agent is able to understand predefined concepts of semantic Web services, extract necessary information and decide on the requirement of a service for a client. The intelligent agent interacts with client and semantic Web services. The system proposed combines aspects of two research topics (smart Web query engine and matchmaking algorithm of OWL-S/UDDI matchmaker) to facilitate the provision of Web services to a client	algorithm;intelligent agent;owl-s;requirement;semantic web service;web services discovery	Duygu Çelik;Atilla Elçi	2005	International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)	10.1109/CIMCA.2005.1631585	web service;web application security;web development;web modeling;semantic web rule language;data web;web mapping;semantic grid;web standards;computer science;artificial intelligence;software agent;ws-policy;semantic web;social semantic web;web page;data mining;ws-addressing;semantic web stack;database;web intelligence;ws-i basic profile;web 2.0;world wide web;owl-s;intelligent agent;universal description discovery and integration;semantic analytics;satisfiability	Robotics	-44.209730860927486	13.852411887608271	78340
862b1b564c9e681759054f766639b339366c2176	finding compromises between local and global ontology querying in multiagent systems	multiagent system;agent based;hybrid approach;agent based system	As Ontologic knowledge gets more and more important in agent-based systems, its handling becomes crucial for successful applications. In the context of agent-based applications, we propose a hybrid approach, in which part of the ontology is handled locally, using a “client component”, and the rest of the ontological knowledge is handled by an “ontology agent”, which is accessed by the other agents in the system through their client component. In this sort of “caching” scheme, most frequent ontologic queries tend to remain stored locally. We propose specific methods for representing, storing, querying and translating ontologies for effective use in the context of the “JITIK” system, which is a multiagent system for knowledge and information distribution. We report as well a working prototype implementing our proposal, and discuss some performance figures.	agent-based model;multi-agent system;ontology (information science);prototype	Hector G. Ceballos;Ramón F. Brena	2004		10.1007/978-3-540-30469-2_11	simulation;bioinformatics;artificial intelligence;agent-based social simulation	AI	-37.732438997813624	10.378539186444963	78459
30a838aad64098919bcc52742e2aa4e7047dd1c6	towards an operation model for generated web applications	data intensive application;e commerce;code generation;data processing;data intensive applications;web application design;operation modeling;object oriented;web portal;data access;object orientation	This paper describes a new approach for the development of data-intensive web applications that depend on non-trivial data manipulation. E-Commerce web sites, on-line auction systems and large enterprise web portals fall into this category, as they require comprehensive data access, data processing and data manipulation capabilities. However, existing methodologies mainly concentrate on modeling content, navigation and presentation aspects of read-only web sites. Approaches that consider modeling data operations incorporate them into existing models resulting in a less clear design. We argue that existing models are not sufficient to express complex operations that access or modify web application content. Therefore, we propose an additional Operation Model defining operations for data-intensive web applications. We also propose the utilization of a web application generator to create an Operation Layer based on this Operation Model.	data access;data-intensive computing;e-commerce payment system;fourth-generation programming language;online and offline;portals;read-only memory;web application	Mihály Jakob;Holger Schwarz;Fabian Kaiser;Bernhard Mitschang	2006		10.1145/1149993.1150001	web service;ajax;web application security;data access;web development;web modeling;data web;web analytics;web mapping;web-based simulation;data processing;web design;web standards;computer science;web navigation;data mining;database;web intelligence;web engineering;programming language;object-oriented programming;object-orientation;world wide web;code generation;mashup	Web+IR	-40.14876662787791	11.157093033714045	78534
87418e4033f18ff1cc4fc24001d6f5cf51b215b0	automatic generation of social relationships between internet of things in smart home using sdn-based home cloud	protocols;sensors;automatic relationship generation social iot sdn;semantics;smart homes resource description framework aerospace electronics xml protocols semantics sensors;smart home management social relationship automatic generation internet of things sdn based home cloud iot devices iot iot relationship iot network relationship foodservice relationship iot physical space relationship rdf xml format;resource description framework;automatic relationship generation;social iot;aerospace electronics;xml;sdn;xml cloud computing home computing internet of things software defined networking;smart homes	The Internet of Things (IoT) idea is having a significant impact on our daily lives these days. As the number of IoT devices is growing fast, many researchers have declared that the usage of IoT and the impact of IoT will make people always use IoT devices whatever they do or wherever they are. At the same time, IoT devices at home are receiving a lot of attention because they contribute to a comfortable home environment. Thus, IoT devices at home are increasing and diversifying. In addition, it is obvious that people will use various home services using various IoT devices. In this situation, it can be extremely difficult for both users and service providers to solve a problem if there is a fault in the smart home environment. To find the position of a fault easily, we define four social relationships between IoTs: IoT-IoT, IoT-Network, Foodservice and IoT-Physical space relationship. The relationships can be used discover IoT devices, services and resources because it provides a distributed solution that is effective, efficient and reduces the burden on people. In addition, the idea of social relationships guarantees the network navigability of searching. To prevent needless burdens on users, we propose that the four relationships are generated automatically by an SDN-based home cloud. We simulated the automatic generation mechanism with 307 switches and 2007 device nodes. The results confirmed that the relationships are generated properly with high accuracy. The created relationships are stored as a RDF/XML format. The RDF/XML format could be used semantically for such tasks as answering a semantic query or smart home service recommendation. We anticipate that the proposed mechanism will bring huge benefits not only to users but also to home service providers in the purpose of smart home management.	categorization;home automation;internet of things;network switch;rdf/xml;resource description framework;semantic query;software-defined networking;xml	Younggi Kim;Younghee Lee	2015	2015 IEEE 29th International Conference on Advanced Information Networking and Applications Workshops	10.1109/WAINA.2015.93	communications protocol;xml;computer science;sensor;rdf;database;semantics;software-defined networking;internet privacy;world wide web;computer security;internet of things;computer network	HCI	-42.7656931845525	15.448374457175307	78560
6eb603fc9e7c45f5ad6c34a0dc11a55b1aa8f439	ontocase-automatic ontology enrichment based on ontology design patterns	ontology learning;background knowledge;ontology construction;ontology design	OntoCase is a framework for semi-automatic pattern-based ontology construction. In this paper we focus on the retain and reuse phases, where an initial ontology is enriched based on content ontology design patterns (Content ODPs), and especially the implementation and evaluation of these phases. Applying Content ODPs within semiautomatic ontology construction, i.e. ontology learning (OL), is a novel approach. The main contributions of this paper are the methods for pattern ranking, selection, and integration, and the subsequent evaluation showing the characteristics of ontologies constructed automatically based on ODPs. We show that it is possible to improve the results of existing OL methods by selecting and reusing Content ODPs. OntoCase is able to introduce a general top structure into the ontologies, and by exploiting background knowledge the ontology is given a richer overall structure.	correctness (computer science);decision support system;design pattern;experiment;gene ontology term enrichment;graphical user interface;heuristic (computer science);numerical analysis;ontology (information science);ontology learning;refinement (computing);semantic web;semiconductor industry;text corpus;wordnet	Eva Blomqvist	2009		10.1007/978-3-642-04930-9_5	upper ontology;open biomedical ontologies;ontology alignment;ontology components;bibliographic ontology;ontology inference layer;computer science;knowledge management;ontology;data mining;ontology-based data integration;owl-s;information retrieval;process ontology;suggested upper merged ontology	Web+IR	-41.508486696290575	6.052714499042706	78613
1a8a78823d45b69749437cf0ff10361143df50ef	automatically generated daml markup for semistructured documents	red www;ingenierie connaissances;information source;source information;reseau web;semantics;intelligence artificielle;semantica;semantique;semistructured data;lenguaje descripcion;internet;dato semi estructurado;marcacion;marquage;artificial intelligence;world wide web;etiqueta;etiquette;inteligencia artificial;label;langage description;fuente informacion;tagging;description language;donnee semistructuree;knowledge engineering	The semantic web is becoming a realizable technology due to the efforts of researchers to develop semantic markup languages such as the DARPA Agent Markup Language (DAML). A major problem that faces the semantic web community is that most information sources on the web today lack semantic markup. To fully realize the potential of the semantic web, we must find a way to automatically upgrade information sources with semantic markup. We have developed a system based on the STALKER algorithm that automatically generates DAML markup for a set of documents based on previously seen labeled training documents. Our rule-learning approach to semantic markup is highly effective when dealing with semistructured documents.	algorithm;darpa agent markup language;semantic html;semantic web	William Krueger;Jonathan Nilsson;Tim Oates;Timothy W. Finin	2003		10.1007/978-3-540-24612-1_19	ruleml;xhtml;semantic computing;the internet;semantic web rule language;synchronized multimedia integration language;etiquette;html;html5;collaborative application markup language;computer science;artificial intelligence;knowledge engineering;social semantic web;semantic web stack;pcdata;database;semantics;progressive enhancement;programming language;label;wireless markup language;world wide web;owl-s;information retrieval	Web+IR	-38.2124868189587	11.754535361959965	78789
600f44d37b2253c76c315e0d8cca484ca3471a1a	formalizing fuzzy spatial data model for integrating heterogeneous spatial data	modeling technique;spatial data model;enterprise gis;spatial data;fuzzy data;web service;data model;decision making process;fuzzy data model;schema mapping;description logic;geospatial data;fuzzy model	The geospatial information is becoming a major input for various decision making processes. The heterogeneity in the spatial data sets, usually collectd and maintained by diverse organizations in their proprietary formats, posses a serious challenge for the integration process. Thus, the first step in this integration process is to develop a standardize geospatial data model from the individual data models. Then the model needs to be encoded in standard schema (GML) and the spatial web services need to be instantiated. Typically, an Enterprise-GIS (E-GIS) framework incorporates the OGC compliant geospatial web services for integration of heterogeneous spatial data sets. Further, the fuzziness exists in the properties of the geospatial objects and their relationships. Thus, there is a need to incorporate the fuzzy characteristics in the data model (application schema) of the E-GIS framework. The present work proposes a fuzzy geospatial data modeling technique for generation of fuzzy application schema. An approach for formalizing the fuzzy model using description logic has also been attempted. The formalization facilitate automated schema mapping required for the integration process. The efficacy of the proposed methodology has been demonstrated with help of an example.	data model;data modeling;description logic;enterprise gis;geographic information system;geoweb;web service	Indira Mukherjee;Soumya K. Ghosh	2011		10.1145/1999320.1999345	idef1x;spatial data infrastructure;semi-structured model;computer science;knowledge management;geospatial analysis;data mining;database;spatial database	DB	-34.05898541242807	11.613533134868309	78890
bfa954355144ea1d48772ef89cdd93180669608a	a generic data model and a supporting server architecture for the mobile memory aid system	data model		generic data model	Andrei Voinikonis	2005			computer science;architecture;data model;database;data architecture	Robotics	-35.2023709168672	9.787391875393718	79011
7285ac91060bbb4f393f0411cc7706706563c65b	toward self-integrating software applications for supply chain management	mappings;self integration;business strategy;semantics;equivalence metric;semantic mapping;information exchange;semantic web;supply chain;translators;web technology;ontology;supply chain management;inference	Each paper in this special issue deals with a particular aspect of supply chain management. Nevertheless, they all point to a common need for meaningful and timely information exchange across the supply chain. The Internet and the existing Web technologies have made the exchange of information essentially free and instantaneous. On the other hand, determining the meaning of that information, which we call integration, is still very costly. Millions of dollars and hundreds of man-years have been spent developing and coding interface specifications and software applications to achieve this integration. While this approach has been successful in the past, it is not a viable approach for the future in which the Semantic Web is becoming an important business strategy. In this paper, we discuss a new approach, called self-integration in which software applications are imbedded in an environment that allows them to integrate automatically. We first provide some background information on integration and then focus on our research on semantic querying, semantic mapping, and semantic inferencing.	fax;information exchange;internet;optimal design;semantic web;semantic mapper;stack machine;strategic management;world wide web	Albert T. Jones;Nenad Ivezic;Michael Grüninger	2001	Information Systems Frontiers	10.1023/A:1012864503148	semantic computing;supply chain management;semantic integration;information exchange;semantic grid;computer science;knowledge management;marketing;semantic web;ontology;social semantic web;data mining;database;semantics;supply chain;semantic technology;world wide web	AI	-42.35595384330919	5.692404356872885	79052
46f56cb33dba8eb80295c744e04cfa8b918720b4	implementation of the autocomplete feature of the textbox based on ajax and web service	autocomplete;ajax;database;web service;asp net	Textbox input is widely used on the web for user input. In order to provide convenience for the web users and increase the input efficiency, one can use AutoComplete feature with the textbox. AutoComplete feature predicts possible word matches for entries that begin with the prefix typed into the textbox without the user actually typing the whole word completely. This paper describes the design and implementation of an ASP.NET application based on AutoComplete Extender control from AJAX Control Toolkit. The textbox control is bound with the AutoComplete Extender and then web service is deployed to connect the database server that stores the possible words. User keystrokes are monitored and textbox input is completed based on first typed letters that match with data stored in database. An AutoComplete test program is developed and tested.	asp.net;ajax (programming);database server;event (computing);server (computing);web service	Zhiqiang Yao;Abhijit Sen	2013	JCP	10.4304/jcp.8.9.2197-2203	web service;ajax;computer science;operating system;data mining;database;world wide web;computer security	DB	-35.166572748216495	16.676569447888486	79137
6fc735da63adfafdd6f2021e2a2c205748258d77	an http-based infrastructure for mobile agents	http;mobile agents;infrastructure;emerging technology;hypertext transfer protocol;distributed system;electronic commerce;world wide web;information retrieval;artificial intelligent;heterogeneous network;mobile agent	Abstract: Mobile agents are an emerging technology attracting interest from the fields of distributed systems, information retrieval, electronic commerce and artificial intelligence. We present an infrastructure for mobile agents based on the Hypertext Transfer Protocol (HTTP) which provides for agent mobility across heterogeneous networks as well as communications among agents. Our infrastructure supports the implementation and interoperation of agents written in various languages and takes advantage of current research in HTTP and the World Wide Web in general.	artificial intelligence;distributed computing;e-commerce;hypertext transfer protocol;information retrieval;interoperation;mobile agent;world wide web	Anselm Lingnau;Oswald Drobnik;Peter Dömel	1996	World Wide Web Journal		computer network;world wide web;mobile search;data mining;hypertext transfer protocol;computer science;heterogeneous network;web server;mobile agent;mobile web;web navigation	AI	-41.09959531047756	13.166769622603386	79222
ab93f64db8b8af46581d8f290915bd9dfac71fa4	the spoken web application framework: user generated content and service creation through low-end mobiles	architectural design;application framework;service creation;software development;developing regions;world wide telecom web;vxml;voicesites;user generated content;web application frameworks;web technology;voice applications	While the word 'accessibility' seems to require a new definition in the context of making Web available in developing regions, we propose to expand the definition of 'Web' itself in the same context. The prevalent definition of Web as a system of interlinked textual documents available on the Internet and accessed through a web browser, needs to be modified to include new types of hyperlinked content and new access mechanisms.  Spoken Web is emerging as an alternate web for the underprivileged by breaking barriers of illiteracy, affordability and local languages. It is envisioned as being complementary to the existing Web and navigable entirely through a voice based interface using an ordinary telephone. In this paper, we present Spoken Web Application Framework (SWAF) -- a framework for enabling creation of sites in Spoken Web through a simple voice interaction over an ordinary phone call. This unique novelty of SWAF provides an extremely simplified service creation ability using voice interaction rather than through programming by IT experts thus enabling a new software development paradigm. It is meant to be a platform for making the power of Web technologies become accessible to the next billion IT users of the world.  We present the architecture, design and implementation of the SWAF framework.	accessibility;application framework;hyperlink;internet;programming paradigm;software development;user-generated content;web application;web framework;world wide web	Arun Kumar;Sheetal K. Agarwal;Priyanka Manwani	2010		10.1145/1805986.1805990	web service;web application security;web development;web modeling;data web;web mapping;web-based simulation;web design;web accessibility initiative;web standards;computer science;web navigation;social semantic web;web page;database;multimedia;web intelligence;web 2.0;world wide web;web server;mashup	Web+IR	-46.75989121119405	10.892700045939097	79246
cc79a278f2ab0a852840afd84f0c4fb30de22c9d	transformation rules for model migration in relational database preservation.		Digital preservation is about memory and giving easy access to it. If the digital object is a relational database the requirements of normalization may make it hard to access and understand. In order to deal with this problem we have proposed the DBPreserve approach to transform a relational database to a dimensional model as part of the preservation process, making the preserved information more explicit and easier to access. The paper presents a set of transformation rules to deal with aspects of the migration process such as the identification of the fact tables corresponding to the main organizational processes and the choice of the set of relevant dimensions. The rules help to keep the traceability of the migration process and to preserve integrity and authenticity. The rules were implemented in a case study which involved a human resources information system.	accessibility;database normalization;information system;relational database;requirement;traceability;virtual artifact	Arif Ur Rahman;Cristina Ribeiro;Gabriel David	2011			relational model;data mining;database;relational database;computer science	DB	-34.27264739486164	10.64609411653335	79257
719007da6e921f2ba410d47ba037d50afa2fe7b0	information models and ontologies for representing the electronic health record			information model;ontology (information science)	Daniel Karlsson;Martin Berzell;Stefan Schulz	2011			medical record;information model;data mining;ontology (information science);information retrieval;computer science	AI	-39.73981856483474	5.985598965717105	79284
089e69e39625ebcc901b9445d9a40997a559ad6a	a reference architecture for generation systems	information extraction;filologias;reference point;data model;linguistica;complex data;natural language generation;grupo a;system architecture;reference architecture	We present the RAGS (Reference Architecture for Generation Systems) framework, a specification of an abstract Natural Language Generation (NLG) system architecture to support sharing, re-use, comparison and evaluation of NLG technologies. We argue that the evidence from a survey of actual NLG systems calls for a different emphasis in a reference proposal from that seen in similar initiatives in information extraction and multimedia interfaces. We introduce the framework itself, in particular the two-level data model that allows us to support the complex data requirements of NLG systems in a flexible and coherent fashion, and describe our efforts to validate the framework through a range of implementations.	reference architecture	Chris Mellish;Mike Reape;Donia Scott;Lynne J. Cahill;Roger Evans;Daniel S. Paiva	2004	Natural Language Engineering	10.1017/S1351324904003456	enterprise architecture framework;reference architecture;data model;computer science;applications architecture;theoretical computer science;data mining;information extraction;data architecture;complex data type	NLP	-43.240985423605366	4.2615068669728515	79444
0f52811f6086e48f251d933417000af53c62b3c4	representing interaction protocols in daml	adaptability;adaptabilite;fiabilidad;reliability;ontologie;protocole transmission;ingenierie connaissances;reutilizacion;agent logiciel;reuse;adaptabilidad;software agents;protocolo transmision;saber hacer;lenguaje descripcion;know how;fiabilite;savoir faire;ontologia;ontology;langage description;reutilisation;description language;transmission protocol;knowledge engineering	We present an extension to DAML-S for representing interaction protocols. An interaction protocol defines the messaging patterns between communicating entities such as software agents. Serializing interaction protocols in a suitable form for reuse supports creating software agents capable of adapting to various environments. Serialized interaction protocols can be utilized, for example, when specifying details of interaction between a contractor and a subcontractor operating in the Internet. Introduction The importance of web service automatization is going to increase in the future Internet. Initiatives such as Web Services (World Wide Web Consortium 2002) and the Semantic Web (World Wide Web Consortium 2001) aim at shifting the burden of doing certain mechanical and monotonic things from humans to computers. This paper describes a specific interaction protocol based framework contributing to reaching that goal. We believe interaction protocols (IPs) function well as process descriptions for software agents. That is because IPs consist of speech acts or communicative acts (CAs) in certain order. CAs, in turn, are often utilized when modeled software agent communication. We use interaction between a contractor and a subcontractor operating in the Internet as an example displaying a more general approach to software agent programming. The underlying idea of that approach is that software agents’ information and knowledge should be externalized when possible. That idea is based on distributing cognition (see e.g., (Hutchins 1996)). In this paper we put forward our theories presented in earlier work (Toivonen & Helin 2002b; 2002a). We try to create agents that are relatively simple as such, but capable of externalizing their knowledge into their surroundings and also internalizing knowledge from their surroundings. Traditionally the theory of distributed cognition is applied to human agents, but the approach can be extended to software agents as well. Agents have knowledge of facts and tasks. We have concentrated mainly on task-related knowledge or “know-how”. More specifically, in (Toivonen & Helin 2002b) we divided the tasks of agents into general tasks Copyright c 2003, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. and subtasks. We proposed that descriptions of domainspecific subtasks could be distributed in external repositories and the agents could download those descriptions and modify their behavior accordingly. In (Toivonen & Helin 2002a), we focused our approach to agent conversations. Conversations are natural candidates for domain-specific subtasks of agents. We made a categorization of agent conversation elements into typeand instance-level elements as well as primitive and composite elements, as depicted in Table 1. Composite elements differ from primitive elements in that unlike composite elements, primitive elements cover only one state transition. The difference between instanceand type-level elements corresponds to the difference between objects and classes. In this paper we further focus our work to serializing interaction protocols (see Table 1). We start by describing our proposal for an interaction protocol ontology that extends concepts defined in DAML-S (Ankolekar & others 2002; Martin & others 2002). Then, we apply our interaction protocol ontology to web service automatization scenario between a contractor and a subcontractor. We then present some related work. Finally, we draw conclusions and discuss ongoing and future work. Table 1: Division into typeand instance-level elements	adobe photoshop elements;artificial intelligence;categorization;computer;consortium;daml-s;darpa agent markup language;distributed cognition;download;entity;future internet;interaction protocol;messaging pattern;semantic web;serialization;software agent;state transition table;theory;web service;world wide web	Santtu Toivonen;Heikki Helin	2003		10.1007/978-3-540-24612-1_22	adaptability;computer science;artificial intelligence;software agent;knowledge engineering;ontology;reliability;reuse;database;owl-s	AI	-42.746646205428306	12.621062180947595	79734
bb6276b4beec5ca8e319a5e09f9914d92b1fb849	integrating modified cuckoo algorithm and creditability evaluation for qos-aware service composition		Abstract QoS-aware Web service composition is regarded as one of the fundamental issues in service computing. Given the open and dynamic internet environment, which lacks a central control of individual service providers, we propose in this paper a novel method that seamlessly considers Quality of Service (QoS) and credibility of service providers to achieve optimal service compositions. Instead of using creditability as one of the QoS attributes, we treat it as the overall capability of a service provider to deliver its promised QoS. We aggregate both user experience (i.e., user trust) and track record (i.e., service reputation) of a provider for accurate creditability evaluation. To facilitate user decision making when multiple (and sometimes conflicting) QoS attributes are involved, we develop an automatic weight calculation approach based on rough set theory and a fuzzy analytic hierarchy process, which assigns higher weights to the more discriminative attributes. Finally, to achieve an optimal service composition, a two phase optimization process is employed, where local optimization chooses services based on creditable QoS assessment and global optimization tackles a multi-objective problem using an effective cuckoo search algorithm. Extensive experimental results show that the proposed QoS-aware service composition approach achieves desirable QoS with credibility guarantees. The performance of our proposed approach also significantly outperforms other competitive methods.	aggregate data;analytical hierarchy;cuckoo search;global optimization;mathematical optimization;optimization problem;quality of service;rough set;search algorithm;service composability principle;services computing;set theory;user experience;web service	Hongbing Wang;Danrong Yang;Qi Yu;Yong Tao	2018	Knowl.-Based Syst.	10.1016/j.knosys.2017.10.027	global optimization;web service;artificial intelligence;data mining;service provider;quality of service;machine learning;analytic hierarchy process;computer science;algorithm;local search (optimization);user experience design;services computing	Web+IR	-45.670892795255995	15.25999168028085	79911
7ff8ce1f6bc02f7e42dbafea5a312a32f978193d	information microsystems		Given their need to manage the information they have under control, organizations usually choose among two types of widely used IT solutions: 1) information systems based on databases (DBIS) that are powerful but expensive to develop and not much flexible; 2) spreadsheets, which threaten the integrity of the data and are limited in exploiting it. In this paper we propose a new type of IT solution, namely information microsystems (MicroIS), that aims at reconciling the best of these two worlds: the low development and maintenance costs, ease of use and flexibility of spreadsheets, with the structure, semantics and integrity of DBIS. The goal is not to replace any of the above two paradigms but to lie somewhere in between depending on the changing needs of the organization. From the various possible points of interest of this IT solution, the article will focus specifically on issues related to data management, introducing the conceptual model of MicroIS, the transformations and validations that can be done around it, and they way in which the structure of the information is inferred from the data that users provide.	database;information system;point of interest;spreadsheet;usability	Jordi Pradel;José Raya;Xavier Franch	2010				DB	-34.598246743541615	10.356487194913552	79920
261c5b381544b2b7fbde411425934cd6b20b0f6b	knowledge representation in environmental impact assessment - a case of study with high level requirements in validation	knowledge representation;environmental impact assessment	An ontology which allows to represent knowledge of environmental impact assessment (EIA) and its involved processes is presented. This knowledge representation is designed to be used in two contexts. The first one is a repository of the elements and concepts used in EIA by environmental experts as a structured knowledge source. The second one is a formal definition of the concepts to be used in an intelligent system for EIA. The first usage requires obtaining a high level of consensus about what elements have to be defined.	artificial intelligence;high-level programming language;knowledge representation and reasoning;ontology (information science);requirement;scart	Julián Garrido;Ignacio Requena	2009			knowledge representation and reasoning;environmental resource management;environmental impact assessment;management science;environmental science	AI	-44.56884643816126	5.097680171461986	80166
c707b1540fad39d11b1c63f8e1d5e30b2e08920d	talking to the semantic web - query interfaces to ontologies for the casual user	ontologie;information sources;web semantique;interrogation base donnee;interrogacion base datos;web semantica;natural language;background knowledge;formal logic;semantic web;natural language interface;ontologia;ontology;database query;knowledge base	The Semantic Web presents the vision of a dynamically growing knowledge base that should allow users to draw on and combine distributed information sources specified in languages based on formal logic. Common users, however, were shown to have problems even with the simplest Boolean expressions [4]; the use of the logic formalism underlying the Semantic Web is beyond their understanding. So how can we bridge the gap between the logic-based Semantic Web and real-world users, who are ill at ease and, oftentimes, unable to use formal logic concepts? An often proposed solution to address this problem is the use of natural language interfaces (NLIs). Most NLIs, however, only understand some subset of natural language (NL), but often suggest full understanding, which leads to confusing interaction with users [1]. This mismatch between the users’ expectations and the capabilities of a NLI is called the habitability problem [5]. Furthermore, the development of NL tools requires computationally intensive algorithms relying on large amounts of background knowledge making the tools highly domain-dependent and inapplicable to new domains or applications [1]. This project proposes to break the dichotomy between full NLIs and formal query approaches regarding them as ends of a Formality Continuum. It argues that query interfaces should impose some structure on the user’s input to guide the entry but not overly restrict the user with an excessively formalistic language. In this way, we hypothesize that the best solutions for the casual and occasional user lie between the freedom of a full NLI and the structuredness of a formal query language. Furthermore, the use of controlled NLs facilitates to overcome both the habitability problem and the adaptivity barrier of full NLIs. The overarching goal is to turn the vision of the Semantic Web into realization, which can only happen if we bridge the gap between the end-users and the logic-based scaffolding of the Semantic Web.	algorithm;boolean expression;knowledge base;nl (complexity);native-language identification;natural language;ontology (information science);query language;semantic web;semantics (computer science);triune continuum paradigm	Esther Kaufmann	2006		10.1007/11926078_78	natural language processing;knowledge base;description logic;semantic web rule language;natural language user interface;epistemology;computer science;artificial intelligence;machine learning;semantic web;ontology;social semantic web;data mining;semantic web stack;database;natural language;programming language;logic	Web+IR	-35.797860125358746	6.027942887636135	80239
3da2edd32d0986d3304418b955d97e6bdff90b00	rdoa-ws: repositorio distribuido de objetos de aprendizaje soportado con servicios web	computacion informatica;repositorio;interoperabilidad;repository;ciencias basicas y experimentales;servicios web;servicios web learning objects;web services;objetos de aprendizaje;interoperability;scorm	─ Advances in virtual education models have enabled us to optimize production processes, distribution and access content through the reuse of learning objects (LO), which are stored in repositories In order to improve interoperability, integration and reuse of learning objects have been defined as SCORM standards that improve the structural interoperability of learning objects. However, the levels of reuse and integration of learning objects are not the best because they usually are used for specific contexts. Alternatively, the software architecture provides a distributed repository system supported with web services to improve levels of reuse, integration and interoperability of learning objects. Keywords─ Learning objects, Repository, Interoperability, SCORM, Web services.	interoperability;naruto shippuden: clash of ninja revolution 3;software architecture;web service	Jonás A. Montilva Calderón;Willian Mauricio Rojas Contreras;Ailin Orjuela Duarte	2011	RASI		geography;world wide web	AI	-43.34556415068816	10.156981579818803	80264
b099f6626dcfb45fbf2116b517eed89388fb7e2c	ws-negotiation: an overview of research issues	web service description language;negotiation decision making;web service discovery;service level agreement template ws negotiation web service xml language negotiation message negotiation protocol negotiation decision making;service level agreement template;web services simple object access protocol xml logic web and internet services decision making proposals computer industry computer science educational institutions;decision making negotiation support systems internet xml;web service;ws negotiation;business model;universal description discovery and integration;internet;negotiation protocol;web services;xml;negotiation support systems;decision process;service level agreement;negotiation message;simple object access protocol;domain specificity	A Web service is defined as an autonomous unit of application logic that provides either some business functionality or information to other applications through an Internet connection. Web services are based on a set of XML standards such as Simple Object Access Protocol (SOAP), Universal Description, Discovery and Integration (UDDI) and Web Services Description Language (WSDL). In particular, Web services discovery is the process of finding most appropriate Web services providers needed by a Web services requestor. One of the important issues in the discovery process is for Web services providers and Web services requestors to negotiate and find a solution that is acceptable to both sides. Thus, a more sophisticated business model with negotiation feature is required for this challenging research area. As there are increasing demands for negotiation technologies in the context of Web services, this paper proposes an independent declarative XML language called WS-Negotiation for Web services providers and requestors. In general, WS-Negotiation contains three parts: negotiation message, which describes the format for messages exchanged among negotiation parties, negotiation protocol, which describes the mechanism and rules that negotiation parties should follow, and negotiation decision making, which is an internal and private decision process based on a cost-benefit model or other strategies. This paper also presents a service level agreement (SLA) template model with different domain specific vocabularies for supporting different types of business negotiations in WS-Negotiation.	autonomous robot;business logic;internet;soap;service-level agreement;vocabulary;ws-policy;ws-trust;web services description language;web services discovery;web service;xml	Patrick C. K. Hung;Haifei Li;Jun-Jang Jeng	2004	37th Annual Hawaii International Conference on System Sciences, 2004. Proceedings of the	10.1109/HICSS.2004.1265100	web service;web application security;web development;web modeling;business process execution language;web standards;computer science;knowledge management;ws-policy;service-oriented architecture;soap;ws-addressing;database;services computing;web intelligence;ws-i basic profile;web 2.0;law;world wide web;devices profile for web services;universal description discovery and integration	DB	-45.311535797645675	14.214141818533337	80276
a7ca71f5e864b18445d8453a4021a0889ed96c38	effectively generating and delivering personalized product information: adopting the web 2.0 approach	e catalogues;software architecture cataloguing electronic commerce electronic mail marketing;web feed technologies;electronic commerce;electronic mail;electronic mail mashups information filtering information filters companies technology management service oriented architecture recommender systems collaboration data mining;mashups;e commerce;web feed;feeds;information filtering;collaboration;e commerce environment;scattering;cataloguing;web feed technologies personalized product information web 2 0 approach e catalogues e commerce environment e mails marketing strategy service oriented architecture;personalization;data mining;companies;technology management;marketing strategy;body of knowledge;software architecture;servers;marketing;personalization e commerce web feed web 2 0 mashup service oriented architecture;personalized product information;e mails;web 2 0;web 2 0 approach;mashup;service oriented architecture;information filters;user interfaces;recommender systems	E-catalogues are widely used for presenting product information in e-commerce environment. Currently, many companies send product information to potential customers via e-mails, but there is a spam problem preventing such e-mails from reaching their target customers. Nevertheless, personalization is now considered as an important marketing strategy, so that mass distribution of e-catalogues may become less desirable for companies. In this study, we developed an innovative service platform for efficiently creating and effectively delivering personalized e-catalogues to target customers. This study not only utilizes but contributes to the body of knowledge associated with the emergent Web 2.0 concepts, service-oriented architecture, and Web-feed technologies.	e-commerce;email;emergence;personalization;service-oriented architecture;service-oriented device architecture;spamming;web 2.0;web feed;world wide web	Shuchih Ernest Chang;Chia-Wei Wang	2010	2010 IEEE 24th International Conference on Advanced Information Networking and Applications Workshops	10.1109/WAINA.2010.120	e-commerce;computer science;database;world wide web;mashup	DB	-47.179529031705364	9.41473246197822	80559
1246754ec9263d73c6d46620c3da4ab59f8993c1	semantic navigation through multiple topic ontologies	user needs;settore inf 01 informatica;interaction pattern;semantic web;semantic relations;communities of interest	A general problem to support semantic navigation is the definition of the effective presentation of knowledge and the definition of interaction patterns. One of the main challenges of Semantic Web is to understand how to exploit, in terms of visualization, the representation of knowledge classified in documents according to user needs. In this paper, we discuss on the use of a framework of multiple and distinct ontologies to support modelling, integration and visualization of personalized knowledge. The framework is at the base of an application aimed at the management and discovery of knowledge for communities of interest. In particular, we propose an approach to make multiple topic ontologies to support the semantic navigation available to users, exploiting semantic relations as explicit links among concepts, to define and reason on the specific context of working tasks.	browsing;human-readable medium;information system;interaction;ontology (information science);personalization;relevance;semantic web;web service	Roberto Boselli;Flavio De Paoli	2005			semantic data model;semantic similarity;semantic computing;semantic integration;semantic web rule language;semantic search;semantic grid;computer science;artificial intelligence;semantic web;social semantic web;semantic web stack;database;semantic technology;world wide web;information retrieval;semantic analytics	Web+IR	-42.444141311879015	8.688442981190233	80589
be7531dfefea5c805c00d4712a45f923c696a746	meta-workflows and esp: a framework for coordination, exception handling and adaptability in workflow systems	commerce electronique;groupware;web based applications;adaptability;adaptabilite;comercio electronico;logistique;red www;semantica formal;chaine approvisionnement;web semantique;reseau web;service web;adaptive workflow;formal semantics;web service;adaptabilidad;sistema reactivo;semantique formelle;workflow system;internet;logistics;web semantica;defaillance;traitement exception;reactive system;semantic web;systeme reactif;workflow;world wide web;exception handling;coordinacion;supply chain;failures;cadena aprovisionamiento;failure recovery;collecticiel;fallo;electronic trade;servicio web;coordination;logistica	This paper describes a framework for flexible workflows based on events, states, and a new kind of process called a meta-workflow. Meta-workflows have five kinds of meta-activities and facilitate control over other workflows. We describe the framework and illustrate it with examples to show its features. The paper gives an architecture for incorporating it into existing workflows and also provides a formal semantics of execution. This framework can be used in web services, supply chains, and inter-organizational applications where coordination requirements are complex, and flexible and adaptable workflows are needed. It is also useful for handling, not just failure recovery, but also other kinds of exception situations, which arise frequently in web-based applications.	exception handling	Akhil Kumar;Jacques Wainer;Zuopeng Zhang	2003		10.1007/978-3-540-25982-4_3	exception handling;web service;logistics;workflow;web application;adaptability;the internet;reactive system;computer science;operating system;semantic web;formal semantics;database;distributed computing;supply chain;programming language;world wide web;computer security	HPC	-37.46960841505047	13.415818994254913	80658
14b9b3484e4fec1209351dce3c8f08f762ab0d86	agent-based privilege negotiation for e-commerce on world wide web	commerce electronique;multiagent system;comercio electronico;red www;negociation;agent based;e commerce;reseau web;formal semantics;internet;negociacion;bargaining;world wide web;sistema multiagente;electronic trade;systeme multiagent	The World Wide Web (Web) is an important channel for business-to-customer transactions in E-commerce. Most commercial organisations have the basic goal to outreach as many potential customers as possible and establish business relationships with them. Before granting privileges to access some protected resources, organisational systems must facilitate the ability to assess the trustworthiness of entities (users or applications) they encounter. For E-commerce on the Web, a typical service provider faces a wide spectrum of potential users without any pre-existing relationship. The question of interest is how these processes of trust establishment and authorisation can be conducted effectively and efficiently.	authorization;e-commerce payment system;entity;trust (emotion);world wide web	Richard Au;Ming Yao;Mark Looi	2003		10.1007/3-540-45068-8_10	e-commerce;the internet;simulation;computer science;formal semantics;world wide web;computer security;negotiation	Web+IR	-39.260125092637956	15.640830067243794	80783
d839c72fe7fd84d13021a98d3d4ab059ceb0100e	data base processing programs with using extended base semantic hypergraph		For any intelligent system it is necessary to have knowledge base and method of processing. In this paper we will look at globals as a way of storing knowledge in a structural way. As a structure, we chose the base advanced semantic hypergraph frames that are networked. We will conduct an experiment on the performance of our approach.		Alibek Barlybayev;Talgat Sabyrov;Altynbek Sharipbay;Assel Omarbekova	2017		10.1007/978-3-319-56535-4_3	semantic computing;theoretical computer science;machine learning;pattern recognition	DB	-38.479263344229075	5.094889491015947	81034
d2341e482ff18cc1229bec381bcdb470874bad70	distributed web service architecture for scalable content analysis: semi-automatic annotation of user generated content	service component architecture;distributed multimedia;web service;data model;content analysis;automatic annotation;web services;load balance;distributed multimedia analysis;user generated content;multimedia content analysis;mpeg 7	This paper describes distributed web service architecture to allow web level scalability for multimedia content analysis. We introduce service component architecture that allows for orchestration and composition of services as content analysis units to enrich multimedia metadata using common data model built upon MPEG-7 content description standard. We report experiments with automatic load balancing and introduce an end user application that facilitates annotation of people in images and video.	component-based software engineering;data model;experiment;global variable;load balancing (computing);mpeg-7;scalability;semiconductor industry;service component architecture;user-generated content;web service	Mika Rautiainen;Arto Heikkinen;Jouni Sarvanko;Mika Ylianttila	2011		10.1007/978-3-642-20754-9_17	web service;content analysis;computer science;database;multimedia;law;world wide web	Web+IR	-45.77844806782401	11.555078761961434	81038
fba89ff2111a47e834ad68bb0379cd1c4ed8b809	web semantics for intelligent and dynamic information retrieval illustrated within the mental health domain	mental health;book chapter;dynamic information retrieval;multi agent systems;intelligent information retrieval;ontology based multi agent system;ontology;web semantics	Much of the available information covering various knowledge domains is distributed across various information resources. The issues of distributed and heterogeneous information that often come without semantics, lack an underlying knowledge base or autonomy, and are dynamic in nature of are hindering efficient and effective information retrieval. Current search engines that are based on syntactic keywords are fundamental for retrieving information often leaving the search results meaningless. Increasing semantics of web content (web semantics) would solve this problem and enable the web to reach its full potential. This would allow machines to access web content, understand it, retrieve and process the data automatically rather then only display it. In this paper we illustrate how the ontology and multi-agent system technologies which underpin this vision can be effectively implemented within the mental health domain. The full realisation of this vision is still to come.	information retrieval	Maja Hadzic;Elizabeth Chang	2009		10.1007/978-3-540-89784-2_10	web modeling;computer science;data mining;web intelligence;world wide web;information retrieval;human–computer information retrieval	AI	-40.85395559088153	7.266129679258618	81113
846c9af9d9e9d55b19e23987024d839f77c1a056	product configuration as decision support: the declarative paradigm in practice	interactive configuration;decision support;business configuration;constraints;truth maintenance systems;product configuration	Product configuration is a key technology, which enables businesses to deliver and deploy individualized products. In many cases, finding the optimal configuration solution for the user is a creative process that requires them to decide trade-offs between conflicting goals (multicriteria optimization problem). These problems are best supported by an interactive dialog that is managed by a dedicated software program (the configurator) that provides decision support. We illustrate this using a real example (configuration of a business software system). This productively used application makes the user aware of which choices are available in a given situation, provides assistance in resolving inconsistent choices and defaults, and generates explanations if desired. One of the key configurator components used to manage this is a truth maintenance system. We describe how this component is used and two novel extensions to it: methods for declarative handling of defaults (of varying strength) and the declarative handling of incompleteness. Finally, we summarize our experiences made during the implementation of this application and the pros and cons of declarative versus procedural approaches.	anytime algorithm;assistive technology;business software;computer program;decision support system;declarative programming;knowledge-based configuration;mathematical optimization;optimization problem;programming paradigm;reason maintenance;software system;user experience;dialog	Albert Haag;Steffen Riemann	2011	AI EDAM	10.1017/S0890060410000582	configuration management;decision support system;computer science;systems engineering;knowledge management;artificial intelligence	AI	-46.45595110729434	16.029546368666825	81180
6a56126cabc76f831c544aa1cb954957da850827	interactively eliciting database constraints and dependencies	database engineering;information system engineering;constraint discovery;requirements engineering;electronic forms reverse engineering	When designing the conceptual schema of a future information system, it is crucial to define a set of constraints that will guarantee the consistency of the subsequent database once it is implemented and operational. Eliciting and expressing such constraints and dependencies is far from trivial, especially when end-users are involved and when there is no directly usable data to play with. In this paper, we present an interactive process aimed to elicit hidden constraints such as value domains, functional dependencies, attribute and role optionality and existence constraints. Inspired by the principles of Armstrong relations, it attempts to acquire minimal data samples in order to validate declared constraints, to elicit hidden constraints and to reject irrelevant constraints in conceptual schemas. This process is part of the RAINBOW approach, destined to develop the data model of an information system based, among others, on the reverse engineering of user-drawn form-based interfaces.	armstrong's axioms;attribute–value pair;conceptual schema;constraint (mathematics);data model;encode;functional dependency;identifier;information system;interactivity;relational database;relevance;reverse engineering	Ravi Ramdoyal;Jean-Luc Hainaut	2011		10.1007/978-3-642-21640-4_15	computer science;systems engineering;data mining;database;requirements engineering;management	DB	-34.41508194284495	13.169317391653763	81266
8d05c6141e28a8e55df53af50783ef5f233f5d84	applying the realism-based ontology-versioning method for tracking changes in the basic formal ontology		Changes in an upper level ontology have obvious consequences for the domain ontologies that use it at lower levels. It is therefore crucial to document the changes made between successive versions of ontologies of this kind. We describe and apply a method for tracking, explaining and measuring changes between successive versions of upper level ontologies such as the Basic Formal Ontology (BFO). The proposed change-tracking method extends earlier work on RealismBased Ontology Versioning (RBOV) and Evolutionary Terminology Auditing (ETA). We describe here the application of this evaluation method to changes between BFO 1.0, BFO 1.1, and BFO 2.0. We discuss the issues raised by this application and describe the extensions which we added to the original evaluation schema in order to account for changes in this type of ontology. The results of our study show that BFO has undergone eight types of changes that can be systematically explained by the extended evaluation schema. Finally, we discuss problematic cases, possible pitfalls and certain limits of our study that we propose to address in future work.	basic formal ontology;extended precision;ontology (information science);ontology versioning;table (database);upper ontology;xml schema	Selja Seppälä;Barry Smith;Werner Ceusters	2014		10.3233/978-1-61499-438-1-227	process ontology;ontology-based data integration;data mining;knowledge management;ontology components;ontology;ontology (information science);upper ontology;computer science;suggested upper merged ontology;basic formal ontology	Web+IR	-45.64231690403682	6.603749319760573	81290
716a899d906504c0df99acabf9f278ff77a75b79	explaining answers from agent communication of semantic web information	explanation;user needs;multiple distributed ontologies;reasoning process agent communication semantic web information multiple agents logical agent multiple distributed ontologies;inference mechanisms;agent communication;ontologies artificial intelligence;logical agent;agent;multi agent systems;semantic web information;explanation semantic web agent proof;semantic web;reasoning process;multiple agents;semantic web inference mechanisms multi agent systems ontologies artificial intelligence;proof;semantic web ontologies medical services history chemicals medical treatment information technology application software software systems collaborative tools	In semantic Web, an application can not only provide explicit information but also infer implicit knowledge. Besides, an answer can be also derived from multiple sources and by multiple agents. In order to trust and use such an answer, a user needs a proof and/or explanations of that answer, which contain information related to knowledge sources, systems, reasoning methods, relationships of data items, etc. In this paper, we extend our existing logical agent framework so that agents can not just reason with multiple distributed ontologies but also record proofs and provide explanations of their answers. A proof contains essential information related to a reasoning process including agents, sources of ontologies, rules, facts, etc. Then various explanations can be then generated from a proof for users, depending on their purpose and requirement. Agents are capable of creating, exchanging, and accumulating (sub-) answers along with their proofs and explanations among each other when they cooperate in solving common problems.	automated reasoning;intelligent agent;ontology (information science);requirement;semantic web;shattered world;tracing (software);yahoo! answers	Vuong Xuan Tran;Hidekazu Tsuji	2008	22nd International Conference on Advanced Information Networking and Applications - Workshops (aina workshops 2008)	10.1109/WAINA.2008.98	computer science;knowledge management;artificial intelligence;semantic web;multi-agent system;data mining;proof;database	AI	-42.5736934774261	13.58771129588003	81338
2205417006e94a762312142118f58aca6613b08b	e-service composition by description logics based reasoning	description logic	Composition of e-Services is the issue of synthesizing a new composite eService, obtained by combining a set of available component e-Services, when a client request cannot be satisfied by available e-Services. In this paper we propose a general framework addressing the problem of e-Service composition. We then show that, under certain assumptions, composition can be realized through DLbased reasoning.	description logic;e-services;service composability principle	Daniela Berardi;Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini;Massimo Mecella	2003			deductive reasoning;algorithm;t-norm fuzzy logics;description logic;non-monotonic logic;composition (visual arts);computer science	AI	-43.20549714266954	13.863438624035753	81554
db220b2528c57e576bd1da6beda35aa0e34ac1c8	representing and reasoning with modular ontologies	jie;computer science representing and reasoning with modular ontologies iowa state university vasant honavar bao;artificial intelligence;computer science	Realizing the full potential of the semantic web requires the large-scale adoption and use of ontology based approaches to sharing of information and resources. In such a setting, instead of a single, centralized ontology, it is much more natural to have multiple distributed ontologies that cover different, perhaps partially overlapping, domains. Such ontologies represent the local knowledge of the ontology designers, that is, knowledge that is applicable within a specific context. Hence, many application scenarios, such as collaborative construction and management of complex ontologies, distributed databases, and large knowledge base applications, present an urgent need for ontology languages that support localized and contextualized semantics, partial and selective reuse of ontology modules, flexible ways to limit the scope and visibility of knowledge (as needed for selective knowledge sharing), federated approaches to reasoning with distributed ontologies, and structured approaches to collaborative construction of large ontologies. Against this background, this dissertation develops a family of description logics based modular ontology languages, namely Package-based Description Logics (P-DL), to address the needs of such applications. The main contributions of this dissertation include: (1) The identification and theoretical characterization of the desiderata of modular ontology languages that can support selective sharing and reuse of knowledge across independently developed knowledge bases; (2) The development of a family of ontology languages called P-DL, which extend the classical description logics (DL) to support selective knowledge sharing through a novel semantic importing mechanism and the establishment of a minimal set of restrictions on the use of imported concepts and roles to support localized semantics, transitive propagation of imported knowledge, and different interpretations from the point of view of different ontology modules; (3) The development of a family of sound and complete tableau-based federated reasoning algorithms for distributed, autonomous, P-DL ontologies including ALCP and SHIQP , i.e., P-DL onologies where the individual modules are expressed in the P-DL counterpart of DL ALC and SHIQ respectively, that can be used to efficiently reason over a set of distributed, autonomous, ontology modules from the point of view of any specific module, that avoid the need to integrate ontologies using message exchanges between modules as needed; (4) The formulation of criteria for answering queries against a knowledge base using hidden or private knowledge, whenever it is feasible to do so without compromising hidden knowledge, and the development of privacy-preserving reasoning strategies for the case of the commonly used hierarchical ontologies and SHIQ ontologies, along with a theoretical characterization of the conditions under which they are guaranteed to be privacy-preserving; (5) The development of some prototype tools for collaborative development of large ontologies, including support for concurrent editing and partial loading of ontologies into memory.	ontology (information science)	Jie Bao;Vasant Honavar	2006			upper ontology;knowledge representation and reasoning;idef5;ontology components;computer science;knowledge management;ontology;theoretical computer science;knowledge-based systems;data mining;ontology language;ontology-based data integration;commonsense knowledge;web ontology language;process ontology	AI	-44.168225586789	8.64331974447265	81791
147b95ef0dc213f0c333d2384bfe17a421021bc6	ontologies are not the panacea in data integration: a flexible coordinator to mediate context construction	database system;semantic reconciliation;data integrity;heterogeneous information systems;conceptual analysis;heterogeneous databases;semantic interoperability;heterogeneous information;dempster shafer;knowledge elicitation;semantic conflicts;ontology;natural language processing;heterogeneous database systems;data integration	Shared ontologies describe concepts and relationships to resolve semantic conflicts amongst users accessing multiple autonomous and heterogeneous information sources. We contend that while ontologies are useful in semantic reconciliation, they do not guarantee correct classification of semantic conflicts, nor do they provide the capability to handle evolving semantics or a mechanism to support a dynamic reconciliation process. Their limitations are illustrated through a conceptual analysis of several prominent examples used in heterogeneous database systems and in natural language processing. We view semantic reconciliation as a nonmonotonic query-dependent process that requires flexible interpretation of query context, and as a mechanism to coordinate knowledge elicitation while constructing the query context. We propose a system that is based on these characteristics, namely the SCOPES (Semantic Coordinator Over Parallel Exploration Spaces) system. SCOPES takes advantage of ontologies to constrain exploration of a remote database during the incremental discovery and refinement of the context within which a query can be answered. It uses an Assumption-based Truth Maintenance System (ATMS) to manage the multiple plausible contexts which coexist while the semantic reconciliation process is unfolding, and the Dempster-Shafer (DS) theory of belief to model the likelihood of these plausible contexts.	autonomous robot;coexist (image);heterogeneous database system;natural language processing;ontology (information science);reason maintenance;refinement (computing);remote database access;spaces;unfolding (dsp implementation)	Aris M. Ouksel;Iqbal Ahmed	1999	Distributed and Parallel Databases	10.1023/A:1008626109650	natural language processing;semantic interoperability;semantic similarity;semantic computing;semantic integration;dempster–shafer theory;computer science;data integration;ontology;data integrity;data mining;semantic compression;database	DB	-38.01407276070424	5.95776612917563	81868
a542461e6be9d4d567e5091e44c12e21688a592a	owlgres: a scalable owl reasoner	conjunctive queries;knowledge base;use case	We present Owlgres, a DL-Lite reasoner implementation written for PostgreSQL, a mature open source database. Owlgres is an OWL reasoner that provides consistency checking and conjunctive query services, supports DL-LiteR as well as the OWL sameAs construct, and is not limited to PostgreSQL. We discuss the implementation with special focus on sameAs and the supported subset of the SPARQL language. Emphasis is given to the implemented optimization techniques which resulted in significant performance improvement. Based on a confidential NASA dataset and part of the DBpedia dataset, we show a typical use case for Owlgres, i.e. given a terminology and a dataset, Owlgres provides querying on a persistent knowledge base with reasoning at query time in the expressivity of DL-LiteR.	adobe flash lite;confidentiality;conjunctive query;dbpedia;knowledge base;mathematical optimization;open-source software;parallel computing;postgresql;query optimization;sparql;scalability;semantic reasoner;web ontology language	Markus Stocker;Michael Smith	2008			database;data mining;computer science;conjunctive query;terminology;scalability;expressivity;knowledge base;sparql;semantic reasoner	DB	-33.801640297439086	5.161768130922938	81958
fed048a0e46fb14b59bea1cfbca58efc776c5396	a semantic-based approach to generate abstract services for service organization		Service organization has been considered as the key enabler for efficient web service management. It gives a high-level and structured view of the important features of web services, including their functionality and inter-service relationships, which can be leveraged to allow a top-down declarative way of querying and composing web services. Abstract services that conceptualize the functionality provided by web services, has been widely adopted as the kernel component of web service organization. However, how to generate abstract services is non-trivial. Current approaches either assume the existence of abstract services or adopt a manual process that demands intensive human intervention. We propose a novel approach to fully automate the generation of abstract services. We first explain the process of generating homogeneous service spaces, i.e., service communities, which consist of a set of functionally similar services. We then present a process of generating abstract services within a service community. We leverage semantics to address the issues raised by syntactical-level service descriptions. An comprehensive experimental study on real world web service data is conducted to demonstrate the effectiveness and efficiency of the proposed approach.		Xumin Liu;Hua Liu	2014		10.1007/978-1-4614-7535-4_11	service bureau;differentiated service;systems engineering;knowledge management;service delivery framework;marketing;services computing;service system	SE	-47.411263723759774	13.995935877641791	82016
56eb98f8f8fb3109c3e24d0f42f36a2b1129e97d	a multi-agent approach for generating ontologies and composing services into executable workflows	web service description language;multi agent system;web service;automatic generation;semantic mapping;semantic workflow;ontology;business process	This paper proposes rethinking how ontologies are used to compose web services into business processes. Unlike handcrafted ontologies, we describe using a multi-agent system (MAS) to automatically generate semantic mappings from service interfaces. Comparing synonyms and contextual clues, we infer meanings of input and output parameters with no explicit semantics (as in a Web Services Description Language document). We further describe how this semantic mapping can be used to derive executable processes by comparing the derived ontologies of each service interface and mapping each service's outputs to inputs of every other service and finding the paths through the resulting graph.	business process;executable;input/output;multi-agent system;ontology (information science);semantic mapper;semantics (computer science);web services description language;web service	John McDowall;Larry Kerschberg	2010		10.1145/1754239.1754242	semantic search;computer science;semantic web;social semantic web;data mining;semantic web stack;database;world wide web;semantic analytics	AI	-42.56730103790144	11.941357311369485	82041
1f21ad50509508bec2922c05c0ab8d0925267d22	an ontology-driven knowledge management system used in the patent library		The introduction of the ontology-driven mechanism brings new opportunities for the knowledge management. In this paper, we describe the results of our continuing work, by researching on the structure design of the ontology-driven knowledge management system, and propose three stages in the system model, i.e., the knowledge acquisition, the knowledge organization and knowledge application. Based on the proposed model, we choose the technology novelty consulting system in our patent library as the platform to perform the patent knowledge management and the experiment results validate the feasibility and validation of our system model.	knowledge acquisition;knowledge management;knowledge organization;management system;semantic analysis (compilers)	Wei Ding;Yongji Liu;Jianfeng Zhang	2015		10.5220/0005638002480253	patent visualisation	AI	-45.30611273038843	5.144684486224627	82057
6cc2d3198109d761b80d76f30954d42eb402e193	the plants system: enabling mixed societies of communicating plants and artefacts	artefacto;sensibilidad contexto;distributed system;ontologie;architecture systeme;systeme reparti;context aware;formal specification;componente logicial;composant logiciel;intelligence artificielle;agricultura;artefact;specification formelle;especificacion formal;research and development;sistema repartido;software component;artificial intelligence;ontologia;arquitectura sistema;agriculture;inteligencia artificial;precision agriculture;sensibilite contexte;system architecture;ontology	In this paper we discuss research work that enables the development of mixed societies of communicating plants and artefacts. PLANTS is an EUfunded Research and Development project, which aims to investigate methods of creating “interfaces” between artefacts and plants in order to enable people to form mixed, interacting (potentially co-operating) communities. Amongst others the project aims to develop hardware and software components that should enable a seamless interaction between plants and artefacts in scenarios ranging from domestic plant care to precision agriculture. This paper deals with the approach that we follow for the development of the homonymous system and discusses its architecture with special focus on describing the communication among artefacts and plants and on designing an ontology that provides a formal definition of the domain under consideration.	ambient intelligence;component-based software engineering;information source;interaction;logistics;ontology (information science);plant ontology;seamless3d	Christos Goumopoulos;Eleni Christopoulou;Nikos Drossos;Achilles Kameas	2004		10.1007/978-3-540-30473-9_18	agriculture;simulation;computer science;artificial intelligence;ontology;formal specification;precision agriculture;programming language	HCI	-38.952776138957134	13.363469346896732	82085
63a93c35547bd8f030e87d830d292264c0422399	selection of the best composite web service based on quality of service	web service;quality of service	The paper proposes a general framework to composite Web services selection based on multicriteria evaluation. The proposed framework extends the Web services architecture by adding, in the registry, a new Multicriteria Evaluation Component (MEC) devoted to multicriteria evaluation. This additional component takes as input a set of composite Web services and a set of evaluation criteria and generates a set of recommended composite Web services. In addition to the description of the conceptual architecture of the formwork, the paper also proposes solutions to construct and evaluate composite web services. In order to show the feasibility of the proposed architecture, we have developed a prototype based on the open source jUDDI registry.	open-source software;prototype;quality of service;serial digital video out;web service;world wide web	Serge Haddad;Lynda Mokdad;Samir Youcef	2010			computer science;web modeling;architecture;web service;data mining;database;quality of service;ws-i basic profile;service-oriented architecture;composite number	Web+IR	-46.22218818449093	13.606932784454889	82384
9a5e7a6d1501fa1c8f252f00021d7b8829474433	the supplementary r-tree (srt) algorithm used for gis resources allocation in model base system under grid environment	databases;collaborative use;groupware;instruments;geographic information system;grid map file;spatial data;resource allocation;supplementary r tree algorithm;resource management;software systems;gis spatial database;lightweight directory access protocol;spatial index;computer software system;tree data structures;spatial database;computer networks;model base system;geographic information systems;model based systems;minimized boundary rectangle;gis resource allocation;indexation;grid map file supplementary r tree algorithm gis resource allocation model base system grid environment computer software system collaborative use geographic information system lightweight directory access protocol distributed gis resources index rule creation spatial index gis spatial database minimized boundary rectangle r tree data structure;computer network management;grid environment;access protocols;distributed gis resources;grid computing geographic information systems geophysical techniques tree data structures groupware visual databases access protocols;high end computing;mesh generation;grid computing;geographic information systems resource management computer networks grid computing software systems mesh generation collaborative software databases instruments computer network management;r tree data structure;data structure;index rule creation;geophysical techniques;collaborative software;visual databases	This model base system (MBS) is a computer software system which classify and maintain a great number of models, and support generation, storage, query, running and analysis of models, and the term of Grid refers to a system that enables the integrated, collaborative use of high-end computers, networks, databases, and scientific instruments owned and managed by multiple organizations. Model base system (MBS) for GIS (geographic information system) application in grid, usually utilizes the LDAP (lightweight directory access protocol) to manage the distributed GIS resources in grid. It is imperative to create index rules for the resources location. The paper is mainly about the effective algorithm to create spatial index used for locating GIS resources in grid. Similar to the usage of most GIS spatial database, we use the MBR (minimized boundary rectangle) as the indexed value keys, and push them in a new data structure of R-tree, named the supplementary R-tree, and find that it's relatively more efficient in seeking and querying spatial data than the other algorithm, such as grid map file (GMF) and R- tree.	computer;data structure;distributed gis;division algorithm;geographic information system;graphical modeling framework;imperative programming;lightweight directory access protocol;r-tree;software system;spatial database	Haibo Hu;Jing Li;Yunhao Chen	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1525238	distributed gis;enterprise gis;grid file;computer science;data mining;database;geographic information system;am/fm/gis;world wide web;spatial database;collaborative software;remote sensing	HPC	-33.99901799043012	14.47470930329588	82805
018eeb3eda711734af4a2976f8d95de4027ec5c1	aifdb: infrastructure for the argument web		This paper introduces AIFdb, a database solution for the Argument Web. AIFdb offers an array of web service interfaces allowing a wide range of software to interact with the same argument data.	web service	John Lawrence;Floris Bex;Chris Reed;Mark Snaith	2012		10.3233/978-1-61499-111-3-515	software;web service;knowledge management;database;computer science	Web+IR	-40.03633683436623	6.33582078575195	82925
8867b81f4d802edbcbabeb0d890e4d4b27b9f7d7	using ontologies to mitigate ldap deficiencies		Semantic technology powered access control schemes have been recently proposed to enhance the flexibility of role-based access control (RBAC) and its variants. These access control mechanisms depend heavily on rich, contextual data sourced from an identity attribute store. Unfortunately, most identity stores in use today use the Lightweight Directory Access Protocol (LDAP) representational schema which has several deficiencies as a knowledge representation, particularly when applied to fine-grained, contextual access decision policies. This paper reviews some of these gaps and shows how the same semantic infrastructure used for the access control mechanisms can be employed to mitigate LDAP assumptions.	control system;knowledge representation and reasoning;lightweight directory access protocol;ontology (information science);role-based access control	Joshua Powers	2010			computer science;role-based access control;database;world wide web;computer security	Security	-40.12613924474836	14.39020116501528	82971
47505bb41503ae63fd7557f5792291eeeeabd2e9	mining candidate web services from legacy code	software;reverse engineering feature;query processing;software system;service oriented system;software maintenance;xml based interface;service orientation;free text query;wsdl document;legacy system service identification web service;software systems;mining candidate web service;text analysis;text extraction;web service;data mining;calendars;software architecture;xml data mining query processing reverse engineering software architecture software maintenance text analysis web services;feature extraction;feature extraction calendars web services software software systems reverse engineering xml;web services;service identification;xml;source code;reverse engineering feature mining candidate web service legacy code software system service oriented system free text query wsdl document xml based interface;legacy system;legacy code;performance assessment;reverse engineering;open source	Mining services from existing software system is a relevant challenge to enable the development of service-oriented systems. Very often, although a service realizing a feature needed to develop a service-oriented system is not available, such a feature might have been implemented in existing software systems. This paper proposes an approach for the identification of services in existing legacy code. The approach combines the use of textual similarity between a free-text query and the text extracted from the legacy code with a structural matching between a goal, expressed as a WSDL document, and XML-based interfaces of features reverse engineered from the source code. A performance assessment has been performed on two open source projects.	borg (star trek);legacy code;open-source software;precision and recall;programming language;reverse engineering;service-oriented architecture;service-oriented device architecture;software system;structural similarity;web services description language;web service;xml	Lerina Aversano;Luigi Cerulo;C. Palumbo	2008	2008 10th International Symposium on Web Site Evolution	10.1109/WSE.2008.4655393	kpi-driven code analysis;web service;text mining;computer science;software engineering;data mining;database;world wide web;software system;source code	SE	-44.90389989264311	12.486217604169912	83155
07c5c2895ad6638a54d81d15c23a8961f559020d	tip: a temporal extension to informix	database system;temporal data;database connectivity;temporal information;data integration and mediation;data access;relational database system	Commercial relational database systems today provide only limited temporal support. To address the needs of applications requiring rich temporal data and queries, we have built TIP (Temporal Information Processor), a temporal extension to the Informix database system based on its DataBlade technology. Our TIP DataBlade extends Informix with a rich set of datatypes and routines that facilitate temporal modeling and querying. TIP provides both C and Java libraries for client applications to access a TIP-enabled database, and provides end-users with a GUI interface for querying and browsing temporal data.	graphical user interface;ibm informix;information processor;java;library (computing);relational database	Jun Yang;Huacheng C. Ying;Jennifer Widom	2000		10.1145/342009.336578	data access;relational database management system;computer science;data mining;database;temporal database;information retrieval;database design	DB	-33.70483894986411	7.997493819591593	83303
1a6dfcccbaadc350d8f0fcee75e26347991f7da6	sagisc: a geo-collaborative system	groupware;geological mapping;shared memory;recoleccion dato;data gathering;social decision;integration information;memoria compartida;localization;cartographie geologique;localizacion;collaborative system;cartografia geologica;data mining;information sharing;information integration;localisation;fouille donnee;integracion informacion;portugues;computer science;information system;decision colectiva;decision collective;collecte donnee;collecticiel;article;portuguese;busca dato;systeme information;remote collaboration;memoire partagee;portugais;sistema informacion	The production of geological mapping by conventional processes is a complex work of data gathering and integration, along with expert and team analysis. This process is very time consuming, since it implies several expeditions to the study location. In the organization studied by this paper, this process can take several years to be completed. The objective of this project is to build a remote collaborative system that supports information sharing by the teams that participate in geological data gathering. The developed system integrates several tools for information sharing and geological/topographical data referencing, as well as support to group discussion and decision. The integration of these tools makes up a geo-collaborative system. The development of this system was done in the context of the Portuguese Geological and Mining Institute (IGM). The evaluation of the prototype by 30 experts from IGM revealed that the proposed goals were accomplished: the system was considered better than the conventional approach.	prototype;topography	Paula André;Pedro Antunes	2004		10.1007/978-3-540-30112-7_15	shared memory;computer science;artificial intelligence;information integration;operating system;database;computer security;information system;portuguese	DB	-34.20541346314782	15.336493240968926	83308
4a3b98154faebd11cd35a2d5f4b82bb8b3aa9ade	modeling genealogical domain - an open problem	genealogy;conceptual modeling;ontologies	The automated processing, storing and knowledge inference of genealogical data presents several difficulties. Roughly eighteen years ago, the FamilySearch organization published GEDCOM, a new standard file format to allow genealogy software and tools to exchange genealogical data. Five years later, the GENTECH Data Modeling Project, proposed a new genealogical logic data model to support research in genealogy and to allow data inter-exchange between genealogy software. Despite being initial reference models, they still have some limitations to adapt to different cultural and social environments. Additionally, sharing genealogical data between systems is difficult since, even though they are syntactical reference models, they may have semantic mismatches. Today, we have not a common and unified proposal as a standard recognized genealogical model. In this way, in this paper we propose to consider the ontology paradigm to extend expressiveness of concepts and relationships in such standards.	computer;data model;data modeling;mathematical model;ontology (information science);programming paradigm;reference implementation;semantic web rule language;software deployment;spatial variability;web ontology language	Joan Campanyà Artés;Jordi Conesa;Enric Mayol	2012			computer science;ontology;artificial intelligence;conceptual model;data mining	DB	-37.95586286619473	8.25251692665937	83477
1943f170de4f5dbe9de1a90d04002bf829ec77b4	broad data: what happens when the web of data becomes real?			data web;semantic web;world wide web	James A. Hendler	2014			world wide web;computer science	ML	-40.12631870631067	5.775992261829357	83556
d5200319f868c752517aa5cca6d9f36f915cbdc2	towards creating workflows on-the-fly and maintaining them using the semantic web: the sbutler project at linköping university	service model;web service;rule based;petri net;semantic web	In the sButler project (Semantic Web Butler) at Linköpings universitet, Sweden, we examine the modeling and instantiation of workflows based on Web services (services described in a rule-based language). The aim of the project, which is part of the Swedish Semantic Web initiative [1], is to support on-the fly creation of workflows and their subsequent maintenance. Our sButler agent will manage workflows and Web services modeled as Petri Nets, use the language facilities provided by the Semantic Web, and employ a new push-and-pull protocol to handle communication with other sButlers.	petri net;rule-based system;semantic web;universal instantiation;web service	Nahid Shahmehri;Juha Takkinen;Cécile Aberg	2003				Web+IR	-45.476541005064156	9.303416843625332	83723
c62d8008a75749c97692af29fb187c465b7bfa31	improving the algorithm for mapping of owl to relational database schema	owl rdf;transform to rdb;ontology	Ontologies are applied to many applications in recent years, especially on the semantic web, information retrieval, information extraction, and question answering. The purpose of domain-specific ontology is to get rid of conceptual and terminological confusion. It accomplishes this by specifying a set of generic concepts that characterizes the domain as well as their definitions and interrelationships. There are some languages in order to represent ontologies, such as RDF, OWL. However, these languages are only suitable with ontologies having small data. For representing ontologies having big data, database is usually used. Many techniques and tools have been proposed over the last years. In this paper, we introduce an improved approach for mapping RDF or OWL to relational database based on the algorithm proposed by Kaunas University of Technology. This approach can be applied to ontologies having many classes and big data.	algorithm;database schema;relational database;web ontology language	Chien D. C. Ta;Tuoi Phan Thi	2015		10.1007/978-3-319-21024-7_9	idef5;computer science;ontology;data mining;database;web ontology language;owl-s;information retrieval;rdf schema	DB	-42.75801138596607	5.461008769380439	83813
7606541c896622f066513acfb7e5f23dfb1a3963	connecting closed world research information systems through the linked open data web	common european research information format;conceptual modeling;cris;lod;linked open data;current research information systems;ontologies;cerif;ld	Received Revised Accepted Research Information Systems (RIS) play a critical role in the sharing of scienti¯c information and provide researchers, professionals and decision makers with the required data for their activities. Existing RIS standards have proposed data models to represent the main entities for storage and exchange. These account for the needs of multiple stakeholders through a high °exibility based on a formal syntax and declared semantics, but for techno-historical reasons they assume the completeness of information within system boundaries. The distributed nature of research information across systems calls for a mechanism to link the local entities from the closed world of concrete RISs with other possibly underspeci¯ed entities exposed through other means, as for example, the Linked Open Data Web. By transformation of a relational model into an open graph model, di®erences between the two system paradigms are revealed. The main principles and techniques for exposing CERIF-driven relational data as linked data will be provided as a ¯rst step demonstrating e®ective RISs interconnection through the linked open data (LOD) Web.	data web;data model;digital library;entity;europeana;existential quantification;facebook platform;formal grammar;functional requirements for bibliographic records;hypertext transfer protocol;information system;interconnection;language technology;library (computing);linkage (software);linked data;marc (archive);programming paradigm;reference implementation;reference model;relational interface system;relational model;sparql;world wide web	Brigitte Jörg;Ivan Ruiz-Rube;Miguel-Ángel Sicilia;Jan Dvorák;Keith G. Jeffery;Thorsten Höllrigl;Henrik S. Rasmussen;Andreas Engfer;Thomas Vestdam;Elena García Barriocanal	2012	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194012400074	computer science;ontology;artificial intelligence;conceptual model;software engineering;level of detail;linked data;data mining;database;world wide web	SE	-40.3569795471033	5.2393716568424535	83844
040a70b73cfcda9d81d7c2e8e8561cee19a89f12	supporting knowledge work with knowledge stance-oriented integrative portals		Portals are an enabling technology for knowledge management (KM): They provide users with a consolidated interface that allows accessing various types of structured and semi-structured information. From the view of KM, their success depends not only on their ability to provide information and knowledge depending on the user’s tasks in business processes (exploitation of knowledge) but also on their ability to support unstructured, creative and learning-oriented actions of knowledge work (exploration of knowledge). However, knowledge management lacks concepts for integrated support of these orientations of knowledge work. The concept of knowledge stance is seen as a promising starting point. This paper presents the INWISS portal prototype that employs Semantic Web technologies for context-based integration of various information sources and applications on a semantic level and discusses extensions to this portal for the support of knowledge stances.	business process;knowledge management;portals;prototype;semantic web;semiconductor industry	Thomas Hädrich;Torsten Priebe	2005			knowledge management;human–computer interaction;personal knowledge management;computer science	AI	-46.850342043657314	7.906201674831146	83972
548945facf2c5cd4ad609d388d029f5bac07c39a	an implementation of a lightweight argumentation engine for agent applications	argumentacion;multiagent system;argumentation;agent based;autonomous system;software agent;circonscription;semantics;langage java;intelligence artificielle;logical programming;agent logiciel;semantica;semantique;circumscription;sistema autonomo;software agents;internet;programmation logique;design and implementation;systeme autonome;non monotonic reasoning;artificial intelligence;lenguaje java;inteligencia artificial;sistema multiagente;programacion logica;systeme multiagent;java language;circonscripcion	Argumentation is becoming increasingly important in the design and implementation of autonomous software agents. In this paper we discuss our current work on a prototype lightweight Java-based argumentation engine that can be used to implement a non-monotonic reasoning component in Internet or agent-based applications. As far as possible we are aiming towards implementing a general purpose argumentation engine that can be configured to conform to one of a range of	agent-based model;autonomous robot;lightweight java;non-monotonic logic;prototype;software agent	Daniel Bryant;Paul J. Krause	2006		10.1007/11853886_39	computer science;artificial intelligence;theoretical computer science;algorithm	AI	-38.815981140339375	13.201007391205813	84100
0b777ff153dad4a8cb6c993fedad6c7c97c03c4b	wrapper generation for automatic data extraction from large web sites	extraction information;site web;distributed system;design of web based information systems;database system;systeme reparti;www and database systems;web pages;red www;empaqueteur;information extraction;generacion automatica;data and web mining;reseau web;web based information system;automatic generation;envolvero;semistructured data;semi structured data;generation automatique;sistema repartido;internet;dato semi estructurado;data extraction;web application systems;web mining;world wide web;information system;sitio web;extraccion informacion;systeme information;web site;sistema informacion;wrapper;donnee semistructuree	The paper investigates techniques for extracting data from large set of dynamic web pages. Dynamically generated web pages from a single web site have a common semi structure for all the data objects. A wrapper of these dynamic web pages is defined as a common template for these pages with different data objects embedded in each web page. Information Extraction is done in three steps: (a) Data Rich Section Extraction from each web page (b) Automated generation of wrapper (c) Data extraction from each web page by comparing it with the wrapper. Wrapper generation is the most important part of this process. Our focus was on developing new improved techniques for wrapper generation. Our technique is fully automated and we were able to achieve good increase in accuracy and speed.	algorithm;dynamic web page;embedded system;emoticon;experiment;information extraction;semiconductor industry;table (database);wrapper library	Nitin Jindal	2005		10.1007/978-3-540-31970-2_3	web service;web mining;static web page;semi-structured data;site map;the internet;data web;computer science;web page;database;world wide web;website parse template;information extraction;information retrieval;information system	DB	-36.57805740099903	11.992670632394503	84112
9eab1a227f042ecf3e9debd0269f58c7d00577e2	the production and use of semantically rich accounting reports on the internet: xml and xbrl	extensible markup language;resource discovery;information sources;web pages;metadata;information retrieval;extensible markup language xml;development process;satisfiability;extensible business reporting language xbrl;reliable communication;internet;information dissemination;intelligent agent;web retrieval;world wide web;web search;financial reporting;communication;formal ontology;intelligent software agent;human computer interface	Most major corporations in the U.S. (and a growing number of companies around the world) are reporting some level of financial information on their Web sites. However, it is not clear that the stakeholders are fully satisfied with this Web-based data. The time and effort allocated to the mechanics of Web retrieval are actually increasing because of the difficulty of finding pages and specific data within the enormity of the public Web (over 1 billion pages) or of many corporate intranets. One way to deal with this vast information source would be to automate the Web search mechanics by developing and using intelligent software agents. However, developing these agents in the current Web environment is very problematic. Three factors are preconditions for effective utilization of the Web. First, appropriate metadata representation of financial reporting information on the Web is required that could improve the accuracy of searches (the resource discovery problem). Second, accounting data points within Web pages should be able to be reliably parsed (the attribute recognition problem). Third, standard mechanisms are required that will encourage or require corporations to report in a consistent fashion. The reality of the Web is that it falls far short of a reliable communication medium for accounting and financial information on all three of these factors. The eXtensible Markup Language (XML) provides a method to tag financial information to greatly improve the automation of information location and retrieval, and provides technical solutions to the resource discovery and attribute recognition problems. However, if every company were free to develop its own labels for its XML tags, then the searching for financial information would be only marginally improved. The recent development by a consortium lead by the American Institute of CPAs (AICPA) of the so-called “eXtensible Business Reporting Language” (XBRL) is an initiative to develop an XML-based Web-based business reporting specification. The widespread adoption of XBRL would mean that both humans and intelligent software agents could operate on financial information disseminated on the Web with a high degree of accuracy and reliability. XBRL provides rich research opportunities, including new taxonomies, database accounting, financial statement assurance, intelligent agents, human/computer interfaces, standard development process, adoption incentives, global adoption, and formal ontologies.	xml	Roger S. Debreceny;Glen L. Gray	2001	Int. J. Accounting Inf. Systems	10.1016/S1467-0895(00)00012-9	web service;web application security;web development;web modeling;the internet;xml;data web;web analytics;web design;web accessibility initiative;web standards;computer science;ws-policy;social semantic web;web page;data mining;database;web intelligence;web engineering;web 2.0;metadata;world wide web;intelligent agent;software development process;satisfiability	Web+IR	-46.60243766326161	8.931375689294857	84483
233a02d02df3d8dc3d26139ff0e69e7ea7047675	web services modeling and composition approach using object-oriented petri nets		Nowadays, with the emergence and the evolution of new technologies, such as e-business, a large number of companies are connected to Internet, and have proposed web services to trade. Web services as presented, are conceptually limited components to relatively simple functionalities. Generally, a single service does not satisfy the users needs that are more and more complex. Therefore, services must be made able to be composed to offer added value services. In this paper, a web services composition approach, modelled by Objects-Oriented Petri nets, is presented. In his context, an expressive algebra, which successfully solves the web services complex composition problem, is proposed. A java tool that allows automating this approach; based on a definite algebra and a G-nets meta-model, proposed by us, is developed.	computer science;conformity;electronic business;emergence;internet;java;maude system;metamodeling;petri net;programming language;robustness (computer science);semantics (computer science);sensor;simulation;software portability;unified modeling language;verification and validation;web service	Sofiane Chemaa;Raida Elmansouri;Allaoua Chaoui	2012	CoRR		web service;web modeling;simulation;computer science;ws-policy;service-oriented architecture;data mining;ws-addressing;services computing;ws-i basic profile;world wide web	SE	-45.353482227313485	16.674503412081645	84656
7d804bb782ff138b410d2451c35e3ddfbfca662d	querying similar process models based on the hungarian algorithm	petri net process models hungarian algorithm structural similarity query processing;query processing;companies;hungarian algorithm;computational modeling;process models;petri nets;petri net;context computational modeling petri nets context modeling query processing companies;context modeling;context;structural similarity	The structural similarity between two process models is usually considered as the main measurement for ranking the process models for a given query model. Current process query methods are inefficient since too many expensive computations of the graph edit distance are involved. To address this issue, using Petri-net as the modeling method, this paper presents the Hungarian algorithm based similarity query method. Unlike previous work where the non-task nodes (i.e., place nodes in the Petri-net) were lightly studied or even ignored, we think these non-task nodes also play an essential role in measuring the structural similarity between process models. First, we extract the context for each place and define the similarity for a pair of place nodes that are from different process models from two perspectives: commonality and the graph edit distance. Then, the place mapping is transformed to classical assignment problem that can be solved by Hungarian algorithm efficiently. Furthermore, we propose a new process similarity measurement on the basis of the place similarity. The extensive experimental evaluation shows that our Hungarian based methods outperform the baseline algorithm in both retrieval quality and query response time.	assignment problem;baseline (configuration management);business process;computation;graph edit distance;hungarian algorithm;information retrieval;petri net;process modeling;response time (technology);structural similarity	Bin Cao;Jiaxing Wang;Jing Fan;Jianwei Yin;Tianyang Dong	2017	IEEE Transactions on Services Computing	10.1109/TSC.2016.2597143	computer science;machine learning;data mining;database;petri net	DB	-45.30026965750632	13.515456065103825	84866
fa7800bec46567a7f346465225dcfece769b58e3	serfing the web: the re-web approach for web re-structuring	web system;management system;e commerce;digital library;levels of abstraction;object model	In our emerging digital paper-less society, massive amount of information is being maintained in on-line repositories and diverse web site representations of this information must be served over the Internet to different user groups. E-commerce and digital libraries are two representative sample applications with such needs. In this paper we present a database-centric approach called Re-Web that addresses this need for flexible web site generation, re-structuring, and maintenance. Re-Web is based on two key ideas. First, we exploit the web site structure by associating web semantics (XML equivalents) with the modeling constructs of the ODMG object model to aid the web site generation process. By capturing the logical structure of web views within the OODB system, we can efficiently maintain the web views using standard database techniques. Secondly, to ease the process of specification and construction of multiple customized web view sites, we also propose the notion of generic web view transformations that are encapsulated into re-usable templates. Thus desired new web view sites can be generated simply by applying the corresponding transformations on the underlying database to produce web view classes and then by applying the web semantics on the newly built view classes. The Re-Web system has been implemented using PSE by Object Design Inc. as object repository, ODMG as object model, OQL as transformation language, SERF as OODB evolution facility and IBM XML parser and LotusXSL processor to aid the web site generation. A case study using Re-Web is also presented to illustrate the working of the system. To the best of our knowledge, Re-Web is the first web site management system focusing on the issue of re-usable view generation templates at the content and not at the presentation style level of abstraction.	cost efficiency;database-centric architecture;dictionary;digital library;digital paper;e-commerce payment system;library (computing);object data management group;object query language;online and offline;run time (program lifecycle phase);transformation language;usability;webmaster;words (unix);world wide web;xml	Li Chen;Kajal T. Claypool;Elke A. Rundensteiner	2000	World Wide Web	10.1023/A:1019277528839	e-commerce;web service;ajax;web application security;web development;web modeling;digital library;data web;web mapping;web-based simulation;object model;web design;web standards;computer science;web api;web navigation;social semantic web;web page;data mining;management system;semantic web stack;database;web 2.0;world wide web;mashup	DB	-40.58917787903531	10.872931393114461	84997
d2debe5266f9b7771cdeafb2efea9cc6343689ca	integrating fuzzy logic in ontologies	fuzzy logic;fuzzy ontology;semantic web	Ontologies have proved to be very useful in sharing concepts across applications in an unambiguous way. Nowadays, in ontology-based applications information is often vague and imprecise. This is a well-known problem especially for semantics-based applications, such as e-commerce, knowledge management, web portals, etc. In computer-aided reasoning, the predominant paradigm to manage vague knowledge is fuzzy set theory. This paper presents an enrichment of classical computational ontologies with fuzzy logic to create fuzzy ontologies. So, it is a step towards facing the nuances of natural languages with ontologies. Our proposal is developed in the KAON ontology editor, that allows to handle ontology concepts in an high-level environment.	e-commerce;fuzzy logic;fuzzy set;gene ontology term enrichment;high- and low-level;kaon;knowledge management;natural language;ontology (information science);portals;programming paradigm;set theory;vagueness	Silvia Calegari;Davide Ciucci	2006			fuzzy logic;description logic;fuzzy cognitive map;ontology inference layer;fuzzy classification;computer science;artificial intelligence;neuro-fuzzy;semantic web;data mining;ontology language	AI	-38.362346958314134	7.8344545835013415	85216
acead844dfb3ec3a3bdc5ac3de3c9730f363100d	"""the """"spree"""" expert finding system"""	resource description format;owl;graph pattern matching;sparql query language;query processing;grouping query;database management systems;inference mechanisms;aggregates semantic web resource description framework database languages pattern matching data models books ontologies computer science application software;data representation;inference query;ontologies artificial intelligence;data model;query languages;knowledge representation languages;aggregate query;owl semantic web database grouping query aggregate query data representation semantic graph entity attribute value format graph pattern matching inference query data model rdf graph resource description format sparql query language;pattern matching;rdf graph;entity attribute value format;semantic web database;semantic web data models database management systems inference mechanisms knowledge representation languages ontologies artificial intelligence pattern matching query languages query processing;semantic web;data models;semantic graph	"""This paper proposes a system to facilitate exchange of information by automatically finding experts, competent in answering a given question. Our objective is to provide an online tool, which enables individuals within a potentially large organization to search for experts in a certain area, which may not be represented in company organization or reporting lines. The advantage of the proposed system over standard forums or group-ware systems is that full-formatted questions can be compared to stored qualification profiles, which were automatically derived from documents, without Human search effort, and possibly refined manually. This allows us to find competent colleagues (or helpful literature such as How-Tos) for a given problem in a single step, and without intermediate iterations. The system is symmetric in that it does not distinguish between """"questioners"""" (asking questions) and """"experts"""" (answering them), therefore forming a """"community"""" of users, which are distributed over an ontology covering the total knowledge. This ontology can either be given (i.e. in the form of an organizational chart), or it can be derived from the experts' knowledge."""	algorithm;client-side;collaborative software;iteration;machine learning;matching (graph theory);microsoft outlook for mac;online chat;prototype;server (computing);usability;user interface;warez;web application	Florian Metze;Christian Bauckhage;Tansu Alpcan	2007	International Conference on Semantic Computing (ICSC 2007)	10.1109/ICSC.2007.38	data modeling;data model;computer science;sparql;pattern matching;semantic web;data mining;database;external data representation;rdf query language;information retrieval;query language	DB	-39.12094108650206	10.082379197475266	85233
4a2334036964f0346d4ffa87aea9b0f2d3e458fc	on defining a domain specific multi-modeling workflow language to address complex multi-modeling activities	analytical models;drugs;complex multimodeling activities;domain identification;formal specification;multi modeling;geospatial data domain specific multimodeling workflow language complex multimodeling activities multimodeling platform model interconnection nontechnical users domain identification modeling technique characterization model interoperation domain specific ontology domain semantic concepts workflow creation process drug interdiction and intelligence domain jiatf south interagency cooperation intelligence fusion disparate data drug smuggling activity data analysis best course of action identification social networks timed influence nets organization structures;multimodeling platform;organization structures;semantics;timed influence nets;best course of action identification;ontologies artificial intelligence;model interoperation;data analysis;disparate data;police data processing;nontechnical users;workflow management software data analysis formal specification ontologies artificial intelligence police data processing sensor fusion;social networks;drug interdiction and intelligence domain;workflows;drug smuggling activity;unified modeling language;domain specific multimodeling workflow language;interagency cooperation;workflow management software;meta modeling;intelligence fusion;ontologies;domain specific languages multi modeling meta modeling workflows;domain semantic concepts;sensor fusion;jiatf south;domain specific ontology;workflow creation process;metamodeling;modeling technique characterization;geospatial data;model interconnection;domain specific languages;data models;unified modeling language drugs analytical models data models semantics metamodeling ontologies	With the increase use of Multi-Modeling platforms, in which multiple models interoperate to complement/supplement each other, a number of challenges are still to be addressed. Although a major challenge exists in providing the technical means for the interconnections between models through their supporting tools, there is also a need to make such platforms usable for non-technical users. In this paper, we propose the development and use of a Domain Specific Multi-Modeling Workflow Language to address complex Multi-Modeling activates for a domain of interest. The first step of our approach is domain identification including the characterization of modeling techniques used in the domain. Then, a Domain Specific Multi-Modeling Workflow Language is developed to allow for capturing Multi-Modeling activities in the form of workflows. These workflows define interoperations between models. In our approach we also propose the use of a Domain Specific Ontology to capture the semantic concepts of the domain and then to guide the workflow creation process. The approach is illustrated using a case study from the Drug Interdiction and Intelligence domain. JIATF-South, an agency well known for interagency cooperation and intelligence fusion, receives huge amounts of disparate data regarding drug smuggling activities. Analysis of such data using various modeling techniques is essential in identifying best Courses of Action (COAs). We apply our approach to this case study by showing how a Domain Specific Multi-Modeling Workflow Language is developed and then used to create workflows of model interoperations. Models of Social Networks, Timed Influence Nets, Organization Structures and Geospatial Data are utilized in this example.	domain-specific language;interoperability	Ahmed Abu Jbara;Alexander H. Levis	2013	2013 IEEE International Systems Conference (SysCon)	10.1109/SysCon.2013.6549884	domain analysis;domain;computer science;data science;domain engineering;data mining;database;workflow technology	SE	-48.06824373849448	6.9123250371958695	85263
06062ac7be0e7c4c272a6526572c40a9e7f6381e	a society of cooperative agents on the information network: towards intelligent information gathering	distributed system;intelligent agent intelligent networks content addressable storage internet proposals explosions database systems;agent based;information retrieval;cooperative agents;information network;information gathering;software agents;internet;cooperative systems;agent based approach;database systems;distributed system cooperative agents information network intelligent information gathering agent based approach internet;intelligent agent;information retrieval internet cooperative systems software agents;explosions;intelligent networks;content addressable storage;proposals;intelligent information gathering	In this paper, we propose a completely agent-based approach for Information Gathering on information network with a large number of distributed heterogeneous sources, such as Internet. Locating and accessing information in such a large distributed system which changes dynamically is challenging. Agentbased solution is most suitable in such situation. We rst review the drawbacks of existing non-agent and partially agent-based solutions for locating information on Internet. With motivation for solving this, we introduce CAS model to express a Society of Cooperative Agents and then explain how our completely agent based solution based on the proposed CAS model overcome the weak point of the existing approaches. keywords | Information Network, Information Gathering, Society of Agents, Cooperation.	agent-based model;distributed computing;internet;supratik chakraborty	Roberto Okada;Eun-Seok Lee;Norio Shiratori	1995		10.1109/ICNP.1995.524837	intelligent network;the internet;computer science;software agent;data mining;world wide web;computer security;intelligent agent	AI	-40.4058612126641	13.517464773024372	85269
944327627a9eb6cfb71c245f42a46e989d58c3cf	oai protocol for chinese culture resources metadata harvesting	added value;international cooperation;value added services;metadata;open archives;interoperabilite;interoperabilidad;digital library;musica;open archive initiative;chino;theory methods;archives ouvertes;partage des ressources;archivos abiertos;biblioteca electronica;musique;resource sharing;metadonnee;particion recursos;valor anadido;electronic library;metadatos;collaborative research;interoperability;chinois;chinese;music;valeur ajoutee;bibliotheque electronique	AGRIS AP and AGROVOC offer advantages to the service providers: defining richer interoperability layer to aid additional resource discovery through the use of a qualified element set; giving flexibility to provide more relevant query results through targeted searching; providing rich, qualified metadata and giving adequate information about the content of the resource through the use of agriculture specific qualifiers. The AGRIS AP and AGROVOC offer advantages to the service providers: defining richer interoperability layer to aid additional resource discovery through the use of a qualified element set; giving flexibility to provide more relevant query results through targeted searching; providing rich, qualified metadata and giving adequate information about the content of the resource through the use of agriculture specific qualifiers. Dspace was created as digital repository with the objective of storing, indexing, preserving and disseminating an organization's research material in digital formats. MIT and Hewlett Packard Company designed the system between 2000 and 2002. DSpace supports the Open Archives Initiative's Protocol for Metadata Harvesting (OAI-PMH) v2.0 as a data provider. Dspace was created as digital repository with the objective of storing, indexing, preserving and disseminating an organization's research material in digital formats. MIT and Hewlett Packard Company designed the system between 2000 and 2002. DSpace supports the Open Archives Initiative's Protocol for Metadata Harvesting (OAI-PMH) v2.0 as a data provider. The AGRIS AP was created specifically to enhance the description, exchange and subsequent retrieval of agricultural Document-like Information Objects (DLIOs). It is a metadata schema which draws elements from Metadata standards such as Dublin Core (DC), Australian Government Locator Service Metadata (AGLS) and Agricultural Metadata Element Set (AgMES) namespaces. The AGRIS AP provide mechanisms for sharing information in a standardized manner by recommending the use of common semantics and interoperable syntaxes. The AGRIS AP was created specifically to enhance the description, exchange and subsequent retrieval of agricultural Document-like Information Objects (DLIOs). It is a metadata schema which draws elements from Metadata standards such as Dublin Core (DC), Australian Government Locator Service Metadata (AGLS) and Agricultural Metadata Element Set (AgMES) namespaces. The AGRIS AP provide mechanisms for sharing information in a standardized manner by recommending the use of common semantics and interoperable syntaxes. Custom metadata schemas A schema for representation of AGRIS AP metadata elements Custom metadata schemas A schema for representation of AGRIS AP metadata elements AGROVOC is a multilingual, structured and controlled vocabulary designed to cover the terminology of all …	agris;agrovoc;agmes;controlled vocabulary;dspace;digital data;digital library;dublin core;interoperability;online locator service	Qiaoying Zheng;Wei Zhu;Zongying Yang	2004		10.1007/978-3-540-30544-6_20	interoperability;digital library;computer science;music;database;multimedia;metadata;world wide web;chinese;added value	Web+IR	-41.236250868216246	5.799817473691903	85290
b445c4c9c6f4ef0e8664bbe4ebf3b8ea25456040	constraint-based playlist generation by applying genetic algorithm	databases;formal specification;genetics;genetic algorithms databases biological cells user interfaces mood vectors genetics;formal verification;biological cells;vectors;genetic algorithm constraint based playlist;user interfaces formal specification formal verification genetic algorithms music;genetic algorithm;genetic algorithms;music constraint based playlist generation genetic algorithm parameter specified constraint derived constraint user defined constraint user behaviour user interactions;constraint based playlist;mood;user interfaces;music	In this paper, we propose a formal model of playlist generation and define three types of constraints, i.e., parameter-specified constraints, derived constraints, and user-defined constraints. Some constraints can be derived through user behaviour and user interactions. Given a set of constraints, we apply a genetic algorithm to generate a playlist which optimizes the number of matched constraints. We also implement our prototype, and perform experiments to show the feasibility and effectiveness of prototype.	data mining;experiment;formal language;genetic algorithm;interaction;prototype;recommender system;software release life cycle;systems architecture;user profile;weka	Jia-Lien Hsu;Shuk-Chun Chung	2011	2011 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/ICSMC.2011.6083868	genetic algorithm;computer science;theoretical computer science;machine learning;constraint;algorithm	Robotics	-41.92106015625911	15.899383634556521	85523
67d518992785320bc3b27b5b02cc6c34f8114e69	full semantic transparency: overcoming boundaries of applications		Complex workflows require intelligent interactions. In this paper we attack the problem of combining user interfaces of specialized applications that support different aspects of objects in scientific/technical workflows with semantic technologies. We analyze the problem in terms of the (new) notion of full semantic transparency, i.e., the property of user interfaces to give full access to an underlying semantic object even beyond application lines. In a multiapplication case full semantic transparency is difficult, but can be achieved by representing the semantic objects in a structured ontology and actively supporting the application-specific framings of an object in a semantic interface manager. We evaluate the proposed framework in a situation where aspects of technical constructions are distributed across a CAD system, a spreadsheet application, and a knowledge base.	computer-aided design;interaction;knowledge base;microsoft windows 1.0;spreadsheet;user interface;web ontology language	Andrea Kohlhase;Michael Kohlhase;Constantin Jucovschi;Alexandru Toader	2013		10.1007/978-3-642-40477-1_25	semantic computing;computer science;knowledge management;database;semantic technology;world wide web	Web+IR	-39.07038792638822	9.180758255449613	85555
eebebfa7b3e1a9544240fa173b3d65cad7ddb9ee	natural language interface to relational database (nli-rdb) through object relational mapping (orm)		This paper proposes a novel approach for building a Natural Language Interface to a Relational Database (NLI-RDB) using Conversational Agent (CA), Information Extraction (IE) and Object Relational Mapping (ORM) framework. The CA will help in disambiguating the user’s queries and guiding the user interaction. IE will play an important role in named entities extraction in order to map Natural Language queries into database queries. The ORM framework i.e. the Hibernate framework resolves the impedance mismatch between the Object Oriented Paradigms (OOP) and Relational Databases (RDBs) i.e. OOP concepts differ from RDB concepts, thus it reduces the complexity in generating SQL statements. Also, by utilizing ORM framework, the RDBs entities are mapped into real world objects, which bring the RDBs a step closer to the user. In addition, the ORM framework simplify the interaction between OOP and RDBs. The developed NLI-RDB system allows the user to interact with objects directly in natural language and through navigation, rather than by using SQL statements. This direct interaction tends to be easier and more acceptable for humans whom are nor technically orientated and have no SQL knowledge. The NLI-RDB system also offers friendly and interactive user interface in order to refine the query generated automatically. The NLI-RDB system has been evaluated by a group of participants through a combination of qualitative and quantitative measures. The experimental results show good performance of the prototype and excellent user’s satisfaction.	amiga rigid disk block;characteristic impedance;dialog system;impedance matching;information extraction;named entity;native-language identification;natural language user interface;nosql;object-relational mapping;prototype;relational database;sql;word-sense disambiguation	Abdullah Alghamdi;Majdi Sabe Owda;Keeley A. Crockett	2016		10.1007/978-3-319-46562-3_29	natural language processing;domain relational calculus;sql;relational model;statistical relational learning;relational calculus;relational database;computer science;database;programming language;database design	DB	-35.06800312963381	5.701719114600645	85730
12678c947625e20ddd4e10c1e05ded6f44f92a87	the mosaic system - a clinical data exchange system with multilateral agreement support		As more and more data become available, the task of accessing and exploiting the large number of distributed clinical data repositories becomes increasingly complex. Moreover, accessing to a certain data set in a federated data warehouse may have constrains, and multilateral agreements may solve it. Such agreements may be very complex to be solved manually. Current systems for clinical data sharing do not support multilateral agreements. MOSAIC, intends to give a modular and efficient solution to the clinical data exchange problem with multilateral agreements. The proposed system takes advantage of agent based systems and the current standarised Interaction Protocols together with the current protocols for clinical data transfer.	agent-based model;bilateral filter;data mart;federated database system;federation (information technology);interchange fee;mathematical optimization;multi-agent system;ncsa mosaic;service-level agreement;software deployment	Magí Lluch i Ariet;Josep Pegueroles-Valles	2010		10.1007/978-3-642-23635-8_35	data mining;information retrieval;data transmission;modular design;data warehouse;data exchange;data sharing;computer science	DB	-47.468785705661716	4.686418883433604	85821
eedcdf0504be4b9df754d0de66dc662ee3042e3b	augmenting digital documents with negotiation capability	active document;mobile computing;automatic bargaining	Active digital documents are not only capable of performing various operations using their internal functionality and external services, accessible in the environment in which they operate, but can also migrate on their own over a network of mobile devices that provide dynamically changing execution contexts. They may imply conflicts between preferences of the active document and the device the former wishes to execute on. In the paper we propose a solution for solving such conflicts with automatic negotiations, allowing documents and devices to find contracts satisfying both sides. It is based on a simple bargaining model reinforced with machine learning mechanisms to classify string sequences representing negotiation histories.	machine learning;mobile device	Jerzy Kaczorek;Bogdan Wiszniewski	2013		10.1145/2494266.2494305	simulation;computer science;knowledge management;database;mobile computing;world wide web	AI	-41.733837152777454	15.613747084992402	85865
f2e266e19f6107983e1b1335a8595dc620408270	r-u-in? - exploiting rich presence and converged communications for next-generation activity-oriented social networking	social network services;portals;converged communication tools;mobile device;motion pictures;social networking online internet semantic web;converged communications social networks;real time;contextual information;social network services real time systems motion pictures portals facebook context modeling context;internet service provider;indexing terms;collaborative tools;converged communications;business model;social network;value centric usage models;telecom operators;semantic web next generation activity oriented social networking internet service providers telecom operators mobile device web 2 0 technologies converged communication tools value centric usage models leverages contextual modeling reasoning techniques collaboration tool;web 2 0 technologies;internet;internet service providers;reasoning techniques;social networks;user experience;social networking online;facebook;next generation;leverages contextual modeling;semantic web;communication technology;social search;user generated content;collaboration tool;social media;context modeling;context;next generation activity oriented social networking;real time systems	With the growing popularity of social networking,traditional Internet Service Providers (ISPs) and Telecom operators have both started exploring new opportunities to boost their revenue streams. The efforts have facilitated consumers to stay connected to their favorite social networks,be it from an ISP portal or a mobile device. The use of Web 2.0technologies and converged communication tools has further led to a rise in both user-generated content as well as contextual information (i.e. rich presence) about users – including their current location, availability, interests and moods. In this evolving landscape, social networking players need to innovate for value-centric usage models that increase customer stickiness,along with business models to monetize the social media. To this end, we present R-U-In? - an activity-oriented social networking system for users to collaborate and participate in activities of mutual interest. Activities can be initiated and scheduled on-demand and be as ephemeral as the user interests themselves. R-U-In? leverages contextual modeling and reasoning techniques to enable “social search” based on real-time user interests and finds potential matches for the proposed activity. Further, it exploits next-generation presence and communication technologies to manage the entire activity lifecycle in real-time. Initial survey results, based on a prototype implementation of R-U-In?, attest to the promise of realtime activity-oriented social networking - both in terms of an effective collaboration tool for value-oriented social networking users and an enhanced end-user experience.	ecosystem;interaction;mobile device;monetization;portals;privacy;prototype;real-time clock;real-time transcription;real-time web;relevance;scalability;social media;social network;social search;user experience;user-generated content	Nilanjan Banerjee;Dipanjan Chakraborty;Koustuv Dasgupta;Sumit Mittal;Seema Nagar;Saguna	2009	2009 Tenth International Conference on Mobile Data Management: Systems, Services and Middleware	10.1109/MDM.2009.33	user experience design;computer science;database;multimedia;internet privacy;world wide web;computer network;social network	HCI	-47.234209971260604	12.34404556241917	86061
55a6ebda0a39331365f9160aa4bb29d995996c9a	debugging owl-dl ontologies: a heuristic approach	distributed system;debugging;puesta a punto programa;representacion conocimientos;ontologie;systeme reparti;analyse fonctionnelle;heuristic method;web semantique;service web;logica descripcion;recommandation;metodo heuristico;web service;debogage;sistema repartido;functional analysis;web semantica;inferencia;representation connaissance;semantic web;recomendacion;recommendation;ontologia;methode heuristique;description logic;knowledge representation;ontology;inference;servicio web;logique description;analisis funcional	After becoming a W3C Recommendation, OWL is becoming increasingly widely accepted and used. However most people still find it difficult to create and use OWL ontologies. On major difficulty is “debugging” the ontologies discovering why a reasoners has inferred that a class is “unsatisfiable” (inconsistent). Even for people who do understand OWL and the logical meaning of the underlining description logic, discovering why concepts are unsatisfiable can be difficult. Most modern tableaux reasoners do not provide any explanation as to why the classes are unsatisfiable. This paper presents a ‘black boxed’ heuristic approach based on identifying common errors and inferences.	debugging;description logic;heuristic;ontology (information science);semantic reasoner;web ontology language	Hai Wang;Matthew Horridge;Alan L. Rector;Nick Drummond;Julian Seidenberg	2005		10.1007/11574620_53	functional analysis;web service;description logic;computer science;artificial intelligence;semantic web;ontology;debugging;world wide web;algorithm	AI	-37.2250126988139	12.981850599613997	86101
3fbd5d6a9eeba335aa678986d40df7df3b9690e0	the pragmatic web: some key issues	human technology interaction;pragmatics;first year;social interaction;semantics;pragmatic web;semantic web;social semantic web;knowledge representation;ontology	In recent years interests towards the Pragmatic Web have been increasing. Since the preliminary sketches and initial ideas in the first years of this decade, some attempts at defining and characterizing the Pragmatic Web has emerged. Building upon the initial characterizations, the aim of this paper is to develop some of the key issues concerning the Pragmatic Web, especially those that distinguish it from the semantic web. By drawing ideas from the linguistic metaphor and from research conducted in technology studies, the paper aims at characterizing certain central features that make the web a pragmatic one. The central argument of this paper is that the Pragmatic Web is a collaborative technology, dependent on communities of users and the practices of use of the community.	pragmatic web;semantic web	Pasi Pohjola	2010		10.1145/1839707.1839745	web modeling;web standards;computer science;knowledge management;semantic web;social semantic web;data mining;semantic web stack;web intelligence;world wide web	Web+IR	-46.33302438692164	4.55663077648827	86148
38160ce415509cd9726c7c26f0f20a54846b5408	ontology transformation in multiple domains	sistema multiple;entreprise;ontologie;empresa;semantics;multiple system;intelligence artificielle;almacen dato;semantica;semantique;intelligence economique;enterprise information system;firm;competitive intelligence;artificial intelligence;ontologia;inteligencia artificial;business intelligence;information system;entrepot donnee;analisis semantico;data warehouse;analyse semantique;inteligencia economica;ontology;systeme information;conference proceeding;semantic analysis;sistema informacion;systeme multiple	We have proposed a new approach called ontology services-driven integration of business intelligence (BI) to designing an integrated BI platform. In such a BI platform, multiple ontological domains may get involved, such as domains for business, reporting, data warehouse, and multiple underlying enterprise information systems. In general, ontologies in the above multiple domains are heterogeneous. So, a key issue emerges in the process of building an integrated BI platform, that is, how to support ontology transformation and mapping between multiple ontological domains. In this paper, we present semantic aggregations of semantic relationships and ontologies in one or multiple domains, and the ontological transformation from one domain to another. Rules for the above semantic aggregation and transformation are described. This work is the foundation for supporting BI analyses crossing multiple domains.	enterprise information system;ontology (information science);report;web ontology language	Longbing Cao;Dan Luo;Chao Luo;Li Liu	2004		10.1007/978-3-540-30549-1_90	competitive intelligence;computer science;artificial intelligence;data science;data warehouse;ontology;data mining;database;semantics;business intelligence;ontology-based data integration;information system;enterprise information system	AI	-37.513309699755204	12.054309000089106	86190
9292ed7e3b14ad6fdf03c3515e5f33ad68babaa1	semantic invalidation of annotations due to ontology evolution	semantic dependencies;semantic annotation;ontology evolution;semantic invalidation;annotation maintenance	Semantic annotations assign concepts of a reference ontology to artifacts like documents, web-pages, schemas, or web-services. When an ontology changes, these annotations have probably to be maintained as well. We present an approach for automatically checking whether an annotation is invalidated by a change in the reference ontology. The approach is based on annotation paths and on the explicit definition of change-dependencies between ontology artifacts. Ontology change-logs and change dependencies are then exploited to identify those annotation paths which require maintenance.	ontology (information science);upper ontology;web service	Julius Köpke;Johann Eder	2011		10.1007/978-3-642-25106-1_25	upper ontology;open biomedical ontologies;ontology alignment;bibliographic ontology;ontology inference layer;computer science;ontology;database;ontology-based data integration;world wide web;owl-s;information retrieval;process ontology;suggested upper merged ontology	AI	-40.04802438105048	8.747776034358123	86220
3c949cd44a9799fc1460f48c6c1c17480a6fdb35	assessing the safety of knowledge patterns in owl ontologies	ontology engineering;knowledge engineering	The availability of a concrete language for embedding knowledge patterns inside OWL ontologies makes it possible to analyze their impact on the semantics when applied to the ontologies themselves. Starting from recent results available in the literature, this work proposes a sufficient condition for identifying safe patterns encoded in OPPL. The resulting framework can be used to implement OWL ontology engineering tools that help knowledge engineers to understand the level of extensibility of their models as well as pattern users to determine what are the safe ways of utilizing a pattern in their ontologies.	algorithm;embedded system;extensibility;instance (computer science);knowledge engineer;locality of reference;mathematical optimization;ontology (information science);ontology engineering;scope (computer science);vocabulary;web ontology language	Luigi Iannone;Ignazio Palmisano;Alan L. Rector;Robert Stevens	2010		10.1007/978-3-642-13486-9_10	idef5;computer science;knowledge management;ontology;artificial intelligence;knowledge engineering;data mining;database;process ontology	Web+IR	-44.0903931424681	6.275973061949738	86247
a1a071ebd0f609b8af631ae4e753398287300191	ikristal: an integrated information retrieval system using metadata on distributed environments	integrated information retrieval;metadata registry;schema mapping	Recently, user demands for integrated searches over different information sources have been increasing rapidly. However, it is not easy to seamlessly integrate distributed heterogeneous information retrieval systems. In particular, resolving the discrepancies in the structures and semantics in the databases that are to be integrated is one of the most challenging research issues. In this paper, we propose an iKRISTAL system that integrates distributed, heterogeneous information retrieval systems without requiring source codelevel modifications of the participating systems. The key element of our approach to address the structural and semantic discrepancies is to make use of schema mapping and a standardized framework for metadata registry. We have implemented and deployed a prototype implementation that integrates six different information sources, each of which is built upon heterogeneous schemas. Furthermore, we evaluated the performance of our approach under various configurations. © 2013 Elsevier Ltd. All rights reserved.	database;heterogeneous computing;information retrieval;prototype	Jeong-Hyeon Seo;Jong-Suk Ruth Lee;Young-Kwang Nam;Byoung-Dai Lee	2013	Mathematical and Computer Modelling	10.1016/j.mcm.2012.12.039	cognitive models of information retrieval;computer science;data mining;database;information retrieval	DB	-37.63151198722157	9.62611864270896	86330
17ba0cdc1b4be3d5a52a81893f31bffd15eb621d	sur l'utilisation de lda en ri pair-à-pair		We revisit the problem of defining a peer-to-peer system for Information Retrieval when each peer’s topic-based profile is obtained using Latent Dirichlet Allocation. This method, defined for a centralized collection, provides a rich representation of the topics and of the documents. We describe two ways of using it in a distributed system and analyze their advantages and drawbacks. Then, we illustrate the use of the obtained topic-based profiles within two systems. The first one is unstructured and uses a gossip-based algorithm to obtain dynamic overlays of topically related peers. This requires defining a similarity between profiles. The second one uses super-peers and maintains a topic-based index of the peers, which is recorded in a distributed Hash table. The keys are derived from the topic-based profiles. MOTS-CLÉS : Recherche d’information, systèmes pair-à-pair, Allocation Latente de Dirichlet (LDA).	algorithm;centralized computing;distributed computing;distributed hash table;information retrieval;latent dirichlet allocation;peer-to-peer	Sylvie Cazalens;Yulian Yang;Sylvie Calabretto;Esther Pacitti	2013				Web+IR	-39.14875375795613	11.835779509629376	86442
6bfdb8e37e39ce81ab019e5e9dbefca8bde5cc60	a high performance distributed database system for enhanced internet services	distributed database;web databases;distributed database system;object oriented database systems;internet services;internet application;high performance	Abstract   Using a distributed database system as a part of the distributed web sever architecture has obvious advantages. It is shown that a first phase distributed database system can be build by extending an existing object oriented database system with application-specific additions. A web database is implemented, as a part of the traditional HTTP-based distributed web server, using this distributed database system.	distributed database;web service	Arjan J. H. Peddemors;Louis O. Hertzberger	1999	Future Generation Comp. Syst.	10.1016/S0167-739X(98)00084-3	middleware;database search engine;data web;database server;intelligent database;distributed data store;database transaction;database tuning;mobile database;computer science;remote database access;database model;data mining;database;view;world wide web;database schema;distributed database;database testing;database design;distributed concurrency control	DB	-36.007212446456435	13.77817276312794	86512
1e221a452a507844149b18c5be9ea84863a0816e	a relevant portion of an ontology: defining a system of ed rules using a part-whole relationship	erbium;ontology definition meta model;owl;ontologies unified modeling language heart information systems owl magnesium compounds erbium asia computational modeling computer simulation;heart;information systems;odm part whole relationship ontology server existence dependency unified modeling language uml ontology web language owl ontology definition meta model;uml;odm ontology ontology server part whole uml;existence dependency;magnesium compounds;ontologies artificial intelligence;part whole;ontology server;part whole relationship;knowledge representation languages;qa75 electronic computers computer science;odm;computational modeling;unified modeling language;ontologies;ontology web language;computer simulation;ontology;formal ontology;meta model;unified modeling language knowledge representation languages ontologies artificial intelligence;tk electrical engineering electronics nuclear engineering;asia	This paper defines rules for an ontology server that enables the selection of the relevant portion of an ontology based on the notion of existence dependency (ED). We examine the ED on the part-whole relation represented in a unified modeling language (UML) and discuss its importance as a modeling primitive for the systems such as OWL (ontology Web language) and UML for ontology definition meta-model (ODM). We then raise a few anomalies in these systems regarding ontological semantics. We then employ the theory of formal ontology which ontologically resolves the semantics of the part-whole relation. As a result, we define rules so the server can find what is needed and present it to the user.	formal ontology;metamodeling;ontology definition metamodel;server (computing);unified modeling language;web language;web ontology language	Mohammad Nazir Ahmad;Robert M. Colomb;Shazia Wasim Sadiq	2008	2008 Second Asia International Conference on Modelling &#x00026; Simulation (AMS)	10.1109/AMS.2008.28	natural language processing;upper ontology;bibliographic ontology;ontology inference layer;computer science;ontology;data mining;database;ontology-based data integration;owl-s;process ontology;suggested upper merged ontology	DB	-42.211767313468165	14.504664317294651	86551
9c8e261bda2bc7ce4b22bdc847b20f1fd6878c69	form management in the vicomte workflow system	hypermedia markup languages;management system;workflow system;html workflow management software humans electrical capacitance tomography specification languages reactive power control system analysis information analysis displays internet;specification languages;workflow management software;workflow management system;effective form processing facility vicomte workflow system form management workflow system form management system form specification language html pages client side validation processing modules;specification languages workflow management software hypermedia markup languages	It is one of the distinguishing characteristics of workflow management systems to integrate automated processes with human processes in an application system. The way a human process is modelled in a workflow system is by capturing the significant events of the process. These events include notifying the user who is assigned to perform the process, disseminating information that is necessary for the user to carry out the process, and collecting information that is generated as the result of the user process. Such events are often presented to the user or captured from the user using forms. This paper presents a form management system that is part of the Vicomte workflow management system. The major components include a form specification language, a facility for dynamically generating HTML pages with client-side validation to be used over the internet, as well as processing modules for the generated HTML pages on the server side. The architecture of the form management system is rather general and therefore is applicable for many systems requiring an effective form processing	cobol;client-side;dynamic data;embedded system;graphical user interface;html;persistence (computer science);server (computing);server-side;specification language;usability	Daniel K. C. Chan	1999		10.1109/HICSS.1999.772958	workflow;xpdl;computer science;business process management;operating system;workflow management coalition;document management system;management system;database;windows workflow foundation;programming language;world wide web;workflow management system;workflow engine;workflow technology	DB	-45.35513048703198	10.25011703783112	86618
56885488b1a10101c74b1a8f6444fc73a864d776	best-match querying from document-centric xml	query language;full text xml querying;xml retrieval;xpath;information need	On the Web, there is a pervasive use of XML to give lightweight semantics to textual collections. Such document-centric XML collections require a query language that can gracefully handle structural constraints as well as constraints on the free text of the documents. Our main contributions are three-fold. First, we outline two fragments of XPath tailored to users that have varying degrees of understanding of the XML structure used, and give both syntactic and semantic characterizations of these fragments. Second, we extend XPath with an about function having a best-match semantics based on the relevance of the document component for the expressed information need. Third, we evaluate the resulting query language using the INEX 2003 test suite, and show that best-match approaches outperform exact-match approaches for evaluating content-and-structure queries.	information needs;query language;relevance;test suite;world wide web;xml;xpath	Jaap Kamps;Maarten Marx;Maarten de Rijke;Börkur Sigurbjörnsson	2004		10.1145/1017074.1017089	xml catalog;xml validation;information needs;xml encryption;simple api for xml;xml;processing instruction;xslt;xml schema;streaming xml;computer science;xpath 2.0;document type definition;document structure description;xml framework;xml database;xml schema;database;schematron;xml signature;world wide web;xml schema editor;information retrieval;query language;efficient xml interchange	Web+IR	-34.16558920916874	6.5870654938985975	86790
a3eb16f53aacbeb8c4dea5ca1d90a6148ae377fe	k-discovery using topic maps to identify distributed knowledge structures in groupware-based organizational memories	groupware;semantic networks groupware knowledge engineering;collaborative software collaborative work navigation knowledge management databases publishing distributed computing organizing information management power system management;topic maps;knowledge management;organizational memory;knowledge structures distributed knowledge structures groupware based organizational memories shared databases semantic structuring link networks k discovery project topic maps conceptual fi amework;semantic networks;conceptual framework;knowledge structures;organizational memories;knowledge structure;knowledge engineering	Many of today's organizations already have a strong integration of groupware systems in their ITinfrastructure. The shared databases of these groupware systems form organizational memories, which comprise the complete knowledge of an organization collected over the time of its existence. One key problem is how to find relevant knowledge or information in continuously growing and distributed organizational memories. The basic functionalities and mechanisms in groupware systems are not sufficient to support users in finding required knowledge or information. Topic maps provide strong paradigms and concepts for the semantic structuring of link networks and therefore, they are a considerable solution for organizing and navigating large and continuously growing organizational memories. The K-Discovery project suggests applying topic maps to groupware systems to address the mentioned challenges. Thus, the K-Discovery project introduces a conceptual framework, an architecture and an implementation approach to create knowledge structures by generating topic maps in organizational memories.	collaborative software;database;knowledge discovery metamodel;map;organizing (structure);topic maps	Stefan Smolnik;Ludwig Nastansky	2002		10.1109/HICSS.2002.994047	topic maps;organizational learning;computer science;knowledge management;artificial intelligence;knowledge engineering;data mining;conceptual framework;database;semantic network;world wide web	Web+IR	-46.31560953636415	7.939205386751578	86950
36b7f3a1ca7e6eab9f0d68439b9873340173a564	rapid ontology-based web application development with jstl	web application development;knowledge representation;data model	This paper presents the approach followed by the ODESeW framework for the development of ontology-based Web applications. ODESeW eases the creation of this type of applications by allowing the use of the expression language JSTL over ontology components, using a data model that reflects the knowledge representation of common ontology languages and that is implemented with Java Beans. This framework has been used for the development of a number of portals focused on the dissemination and management of R&D collaborative projects.	cima: the enemy;data model;eswc;electromagnetically induced grating;form (html);html;jstl;java;knowledge representation and reasoning;neon (light synthesizer);ontology (information science);ontology components;portals;schmidt decomposition;semantic web;unified expression language;web application development;web development;wiki	Angel López-Cima;Óscar Corcho;Asunción Gómez-Pérez	2007			process ontology;data mining;ontology-based data integration;open biomedical ontologies;ontology (information science);database;ontology components;ontology inference layer;suggested upper merged ontology;upper ontology;computer science	SE	-39.253131198203135	6.709196774800698	87301
3ab399af20c771b0c775851f666fdb7cdc4919f0	querying for meta knowledge	query processing;area of concern;rdf modeling;semantic web;sparql;rdf	The Semantic Web is based on accessing and reusing RDF data from many different sources, which one may assign different levels of authority and credibility. Existing Semantic Web query languages, like SPARQL, have targeted the retrieval, combination and reuse of facts, but have so far ignored all aspects of meta knowledge, such as origins, authorship, recency or certainty of data, to name but a few.  In this paper, we present an original, generic, formalized and implemented approach for managing many dimensions of meta knowledge, like source, authorship, certainty and others. The approach re-uses existing RDF modeling possibilities in order to represent meta knowledge. Then, it extends SPARQL query processing in such a way that given a SPARQL query for data, one may request meta knowledge without modifying the original query. Thus, our approach achieves highly flexible and automatically coordinated querying for data and meta knowledge, while completely separating the two areas of concern.	database;query language;sparql;semantic web	Bernhard Schueler;Sergej Sizov;Steffen Staab;Thanh Tran	2008		10.1145/1367497.1367582	named graph;computer science;sparql;semantic web;rdf;data mining;database;rdf query language;web search query;world wide web;information retrieval;rdf schema	Web+IR	-36.547334280019406	5.405925514305127	87302
8853977a156d55a3836f2277c69606c426b3d42c	development of middleware architecture to realize context-aware service in smart home environment	context aware;smart home;profile;service;middleware;reasoning;ontology	A smart home provides automated services based on the context of the home environment and user activity. Context acquiring, processing, reasoning, and disseminating to the services are complex tasks for a context-aware system. An appropriate middleware architecture could handle such complexity. In this paper, we proposed a middleware architecture for a context-aware system in smart home environment. Here, the context is modeled based on ontology using Web Ontology Language (OWL). In addition, a profile applied improved rule-based reasoning algorithm is integrated into this middleware to infer high-level contexts from available low-level contexts. Experimental result shows that the middleware provides more accurate and faster reasoning outcome compare with the traditional rule-based reasoning method. Moreover, context-aware service is also selected using the rule-based algorithm, where the service can be extended easily by adding new service rules in the service rule base.	algorithm;context awareness;forward chaining;high- and low-level;home automation;image sensor;logic programming;machine learning;middleware;ontology (information science);personalization;preprocessor;recursion;reinforcement learning;rule-based system;testbed;user (computing);user profile;wearable computer;web ontology language	Hyunwook Kim;Robiul M. Hoque;Seo Hyungyu;Sung-Hyun Yang	2016	Comput. Sci. Inf. Syst.	10.2298/CSIS150701010H	service;differentiated service;computer science;ontology;middleware;data mining;database;world wide web;reason	SE	-41.8614695754329	11.534479609342597	87319
04f8782f703e6f1f03013c368f826b9ebd75c5eb	serching for services on the semantic web using process ontologies		The ability to rapidly locate useful on-line services (e.g. software applications, software components, process models, or service organizations), as opposed to simply useful documents, is becoming increasingly critical in many domains. As the sheer number of such services increases it will become increasingly more important to provide tools that allow people (and software) to quickly find the services they need, while minimizing the burden for those who wish to list their services with these search engines. This can be viewed as a critical enabler of the ‘friction-free’ markets of the ‘new economy’. Current service retrieval technology is, however, seriously deficient in this regard. The information retrieval community has focused on the retrieval of documents, not services per se, and has as a result emphasized keyword-based approaches. Those approaches achieve fairly high recall but low precision. The software agents and distributed computing communities have developed simple ‘frame-based’ approaches for ‘matchmaking’ between tasks and on-line services increasing precision at the substantial cost of requiring all services to be modeled as frames and only supporting perfect matches. This paper proposes a novel, ontology-based approach that employs the characteristics of a processtaxonomy to increase recall without sacrificing precision and computational complexity of the service retrieval process.	artificial intelligence;component-based software engineering;computational complexity theory;distributed computing;information retrieval;online and offline;ontology (information science);semantic web;software agent;web search engine	Mark Klein;Abraham Bernstein	2001				Web+IR	-42.45469332085792	7.74890372402606	87450
67930642744842ac34ac8fbada1b89d3fe1b475a	conversational querying	humans converse;query language;effective mechanism;database schema;database system;basic mechanism;conversational querying;sql query;traditional interaction mechanism;whole query;incomplete sql query	The traditional interaction mechanism with a database system is through the use of a query language, the most widely used one being SQL. However, when one is facing a situation where he or she has to make a minor modification to a previously issued SQL query, either the whole query has to be written from scratch, or one has to invoke an editor to edit the query. This, however, is not the way we converse with each other as humans. During the course of a conversation, the preceding interaction is used as a context within which many incomplete and/or incremental phrases are uniquely and unambiguously interpreted, sparing the need to repeat the same things again and again. In this paper, we present an effective mechanism that allows a user to interact with a database system in a way similar to the way humans converse. More specifically, incomplete SQL queries are accepted as input which are then matched to identified parts of previously issued queries. Disambiguation is achieved by using various types of semantic information. The overall method works independently of the domain under which it is used (i.e., independently of the database schema). Several algorithms that are variations of the same basic mechanism are proposed. They are mutually compared with respect to efficiency and accuracy through a limited set of experiments on human subjects. The results have been encouraging, especially when semantic knowledge from the schema is exploited, laying a potential foundation for conversational querying in databases. r 2004 Elsevier B.V.. All rights reserved.	algorithm;database schema;experiment;query language;sql;select (sql);word-sense disambiguation	Yannis E. Ioannidis;Stratis Viglas	2006	Inf. Syst.	10.1016/j.is.2004.09.002	computer science;query by example;data mining;database;programming language;view;information retrieval;spatial query	DB	-34.57906361256685	5.609912620447516	87488
3984a1ba2c29467de8b0cc26c1444162015658c6	bh : behavioral handling to enhance powerfully and usefully the dynamic semantic web services composition	semantic web service;interleaving;composition;selection;control flows compatibility;integration;behaviour	Service composition enables users to realize their complex needs as a single request, and it has been recognized as a flexible way for resource sharing and application integration since the appearance of Service-Oriented Architecture. Many researchers propose their approaches for dynamic services composition. In this paper we mainly focus on behaviour driven dynamic services composition, and more precisely on process integration and interleaving. To highly enhance the dynamic task realization, we propose a way to not only select service process, but also to integrate and interleave some of them, and we also take advantage of control flows compatibility. Furthermore, our solution ensures the correct service consumption at the provider and requester levels, by services behavioural fulfillment.	semantic web service;x86	Mansour Mekour;Sidi Mohamed Benslimane	2011		10.1007/978-3-642-24443-8_8	computer science;knowledge management;database;world wide web	Robotics	-48.19752175919873	15.941903017582217	87681
3a4cce44a95bbe503614cc252e8de0db9f702903	utility function for semantic video content adaptation	utility function;satisfiability;universal multimedia experience;universal multimedia access;semantic adaptation;mpeg 21 dia;multimedia communication;content adaptation;digital item adaptation	The vision of Universal Multimedia Access (UMA) and Universal Multimedia Experience (UME) has driven research in the multimedia community for a long time. Implementing content adaptation frameworks to satisfy heterogeneous types of constraints is among the main requirements for UMA and UME. At the core of these frameworks lies the adaptation decision engine, which computes the appropriate adaptation plans. Though much research has already been done in this domain, the problem of semantic constraints has generally been neglected. This paper addresses the problem of selecting the optimal adaptation operation to satisfy semantic constraints while maximizing the utility of the adapted video. To this end, we define a utility function that computes a value for each possible adaptation operation. We represent our utility function using the MPEG-21 Digital Item Adaptation (DIA) tools. This facilitates the integration of semantic constraints with other types of constraints in MPEG-21 Universal Constraint Description format.	content adaptation;decision support system;digital item;digital video;mpeg-21;requirement;uniform memory access;utility	Vanessa El-Khoury;Nadia Bennani;David Coquil	2010		10.1145/1967486.1967649	simulation;computer science;multimedia;world wide web	DB	-45.811681446343364	12.131882229032456	87887
6d076ba4960b02ab745a4bc8c9e47c3aa7b14107	proactive gossip-based management of semantic overlay networks	gossip based protocols;p2p;article letter to editor;peer to peer search networks;simulation study;file sharing;semantic relations;peer to peer;semantic overlay networks;semantic overlay network	Much research on content-based P2P searching for file-sharing applications has focused on exploiting semantic relations between peers to facilitate searching. Current methods suggest reactive ways to manage semantic relations: they rely on the usage of the underlying search mechanism, and infer semantic relations based on the queries placed and the corresponding replies received. In this paper we follow a different approach, proposing a proactive method to build a semantic overlay. Our method is based on an epidemic protocol that clusters peers with similar content. Peer clustering is done in a completely implicit way, that is, without requiring the user to specify preferences or to characterize the content of files being shared. In our approach, each node maintains a small list of semantically optimal peers. Our simulation studies show that such a list is highly effective when searching files. The construction of this list through gossiping is efficient and robust, even in the presence of changes in the network.	cluster analysis;file sharing;gossip protocol;overlay network;peer-to-peer;proactive parallel suite;simulation	Spyros Voulgaris;Maarten van Steen;Konrad Iwanicki	2007	Concurrency and Computation: Practice and Experience	10.1002/cpe.1225	semantic computing;computer science;peer-to-peer;database;internet privacy;world wide web;file sharing	Web+IR	-39.18013547145875	12.003245247766849	87994
af014a8ba24ee3d158662729e592b949b20683fe	intelligent search agents for web-based intelligent tutoring systems	dynamic change;distributed knowledge bases;intelligent tutoring system;query processing;web;web based intelligent tutoring systems;materials;ontology based domain knowledge bases;ontologies artificial intelligence;domain knowledge;globe ontology intelligent search agents web based intelligent tutoring systems ontology based domain knowledge bases domain knowledge representation formal query model distributed knowledge bases semantic information;semantic information;query processing intelligent tutoring systems internet ontologies artificial intelligence;internet;formal query model;globe ontology;intelligent tutoring systems;intelligent agent intelligent systems ontologies artificial intelligence computer science knowledge representation education software engineering sun information technology;intelligent search agent;domain knowledge representation;artificial intelligence;ontologies;computer science;knowledge representation;intelligent tutoring system ontology intelligent search agent knowledge base web;ontology;knowledge based systems;intelligent search agents;knowledge base	In this paper, after the introduction of ontology-based domain knowledge bases and domain knowledge representation, a formal query model of intelligent search agents that can search over distributed knowledge bases with heterogeneity for Web-based intelligent tutoring systems is proposed; and the searching process of these agents is analyzed. Ontologies are introduced to describe the semantic information of knowledge bases in order to express fully the semantics of queries in our Web-based intelligent tutoring system. This model improves the relevance of the searching results. At the same time, it reflects the dynamic change of the information in the knowledge bases and reduces the consistency and efficiency problem caused by using globe ontology.	knowledge base;knowledge representation and reasoning;ontology (information science);relevance;visual instruction set	Yu Sun;Tianwei Xu;Zhiping Li	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.19	natural language processing;knowledge base;the internet;intelligent decision support system;computer science;knowledge management;ontology;artificial intelligence;ontology;data mining;domain knowledge	AI	-40.9563350008965	7.450448732197258	88085
31ef268187a4c13e67cef673ff338d451af5a90a	formalizing mobility in service oriented computing	service composition;service orientation;mobility mechanisms;formal methods;web service;indexing terms;formal method;service oriented computing;web services	The usual scenario of service oriented systems is characterized by several services offering the same functionalities, by new services that are continuosly deployed and by other ones that are removed. In this context it can be useful to dynamically discover and compose services at runtime. Orchestration languages provide a mean to deal with service composition, while the problem of fulfilling at runtime the information about the involved services is usually referred to as open-endedness. When designing service-based applications both composition and open endedness play a central role. Such issues are strongly related to mobility mechanisms which make it possible to design applications that acquire, during the execution, the information which are necessary to invoke services. In this paper we discuss the forms of mobility for the service oriented computing paradigm. To this end we model a service by means of the notions of interface, location, internal process and internal state, then we formalize a calculus supporting the mobility of each element listed above. We conclude by tracing a comparison between the proposed calculus and the mobility mechanisms supported by the Web Services technology.	programming paradigm;run time (program lifecycle phase);service composability principle;service-oriented architecture;web service;world wide web	Claudio Guidi;Roberto Lucchi	2007	JSW	10.4304/jsw.2.1.1-13	web service;formal methods;service product management;differentiated service;computer science;service delivery framework;operating system;service design;database;distributed computing;services computing;world wide web;computer security;computer network	SE	-46.2688942918537	17.80868046199843	88108
cd459d3c0aec14ab073fa76c20ba36680df39e50	compatibility analysis of web services	web services automata collaboration collaborative work information analysis proposals information technology travel services protocols system recovery;automata compatibility analysis web services;web service;automata;internet;web services;compatibility;automata theory;automata web services compatibility;automata theory internet	The compatibility analysis is absolutely necessary for guaranteeing the correct composition of web services, no matter what styles the composition takes, statically or dynamically. In this paper, we provide a formalization of web services behavior using the approach of automata. With this understanding, we propose a definition of role among web services interactions. As a result, we can check whether two or more web services are compatible in collaboration or not.	automata theory;interaction;web service	Yuliang Shi;Liang Zhang;Fangfang Liu;Lili Lin;Baile Shi	2005	The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05)	10.1109/WI.2005.50	web service;web application security;web development;web modeling;business process execution language;data web;web analytics;web design;web standards;computer science;knowledge management;ws-policy;service-oriented architecture;web navigation;social semantic web;ws-addressing;database;services computing;web intelligence;ws-i basic profile;web 2.0;law;world wide web	SE	-45.962569786325325	18.118229191426554	88175
a0362d01fe6d147a190a8d268fc048ab3d8fd810	expert system personalized knowledge retrieval	knowledge objects;expert systems;digital library;digital libraries;personalized knowledge retrieval;knowledge management;time pressure;digital content;next generation;knowledge management system;expert system;informatics management	In modern Economies, knowledge management systems (KMSs) applications are gradually adopted from a growing number of enterprises, organizations and governments. As digital content availability is increasing dramatically through centralized of distributed digital libraries operation, a great research interest is developed upon “clever” knowledge retrieval based on each user’s individual preferences. Modern man usually requests to get knowledge under time pressure. In many cases it is not possible to have enough time to evaluate extensively the huge amount of results presented after a “string-based” criteria query to a digital library. Next generation KMSs should be able to eliminate the results of a search query, based on a certain user’s profile. This profile carries information about the level of preexistent knowledge that maybe a user has on a certain knowledge area, the exact scope of its research and the time that he has available in order to exploit the results of this research. In this paper a new generation of KMSs is proposed that are supporting personalized knowledge retrieval. This is achieved through an innovative architecture of enriched knowledge objects for knowledge representation and the development of an expert system for diagnosis and dynamic knowledge composition.	expert system;personalization	Dionisios Koutsantonis;John-Christ Panayiotopoulos	2011	Operational Research	10.1007/s12351-009-0048-4	legal expert system;knowledge base;digital library;computer science;knowledge management;artificial intelligence;knowledge-based systems;open knowledge base connectivity;data mining;knowledge extraction;personal knowledge management;expert system;information retrieval;domain knowledge	AI	-42.846312185868115	7.5827473307897915	88288
e2c4353cb428e639bf5f4f46eac147043d9266f7	yet another meta-model to specify non-functional properties	service orientation;functional properties;software engineering;service quality model;proof of concept;non functional properties;software life cycle;service discovery;requirement specification;service quality;meta model	In service-oriented systems non-functional properties become very important to support run-time service discovery and composition. Software engineers should take care of them for guaranteeing the service quality in all the software life-cycle phases, from requirements specification to design, to system deployment and execution monitoring. This wide scope and the criticality of non-functional properties demand that they are expressed in a language which is intuitive and easy to use for the service quality specification, and at the same time is machine-processable to be automatically handled at run-time. In this paper we present a Property Meta-Model that aims to reach these two main objectives and show as a proof of concept its use for the modeling of two different properties.	care-of address;criticality matrix;metamodeling;service discovery;service-oriented architecture;service-oriented device architecture;software deployment;software engineer;software release life cycle;software requirements specification;system deployment;yet another	Antinisca Di Marco;Claudio Pompilio;Antonia Bertolino;Antonello Calabrò;Francesca Lonetti;Antonino Sabetta	2011		10.1145/2031746.2031751	reliability engineering;software requirements specification;computer science;systems engineering;database;service discovery;data as a service	SE	-46.714970612677604	17.65773917727394	88350
758737cbdcc84385c2ea482cb238ed80bfe846d4	ontoalign++: a combined strategy for improving ontologies alignment		Ontology reuse is very important nowadays but, more specifically, ontology alignment still represents a challenge, despite the proposal of a great number of techniques and tools that implement it. This paper presents an approach that builds upon two already existent techniques. It considers both the enrichment of the ontologies with implicit terms and relationships contained on the ontologies terms definitions and on associating concepts of the ontologies to categories of foundational ontologies. Besides confirming the improvement on alignment results when using each of these approaches, our experiments showed even better results when these techniques were applied together.	conceptualization (information science);experiment;gene ontology term enrichment;ontology (information science);ontology alignment;semiconductor industry;upper ontology;web ontology language	Miguel Gabriel Prazeres Carvalho;Maria Luiza Machado Campos;Linair Maria Campos;Maria Cláudia Reis Cavalcanti	2013			ontology;data mining;ontology alignment;ontology (information science);reuse;computer science	Web+IR	-43.94438838230322	6.801865062684783	88439
f3552fcc1962c8dacf6d6840a7a9833e63598df8	predicate trees: a tool for descriptive subgraph extraction	rdf data access;linked data;scala;pattern matching;data access;domain specific language;semantic web;sparql;subgraph extraction	Extracting a subgraph descriptive of a single resource from a given RDF graph is an issue relevant to the Linked Data initiative, RDF triple stores and Semantic Web in general. Existing methods of subgraph extraction tend to be either simple and inexpressive, or else powerful and complex. This paper introduces a novel method of descriptive subgraph extraction as a new expression language and as a domain-specific language library in Scala, which aims to be more expressive than the former and easier to write and use than the latter methods. These expressions can then be translated into SPARQL, used directly on triple pattern matching interfaces for RDF graphs, or as DESCRIBE handlers in SPARQL engines. A comparison highlighting advantages and limitations of the new method is also given.	apache jena semantic web framework;domain-specific language;ietf language tag;immutable object;linked data;pattern matching;recursion;resource description framework;sparql;scala;self-reference;semantic web;sourceforge;tree (data structure);triplestore;unified expression language	Hrvoje Simic	2012		10.1145/2254129.2254160	rdf/xml;turtle;computer science;sparql;data mining;database;rdf query language;programming language;blank node;rdf schema	Web+IR	-34.62335733887504	6.2705730583547155	88506
0bc89be37a77f69669ef65a987b3432790c84366	static parameter binding approach for web service mashup modeling			mashup (web application hybrid);web service	Eunjung Lee;Hyung-Joo Joo	2012			database;programming language;world wide web	Web+IR	-41.90526860974928	12.329153513152152	88521
119aa9289ed882fadb3093eaa17b4cc1e6dbf530	pdq: proof-driven query answering over web-based data		The data needed to answer queries is often available through Webbased APIs. Indeed, for a given query there may be many Webbased sources which can be used to answer it, with the sources overlapping in their vocabularies, and differing in their access restrictions (required arguments) and cost. We introduce PDQ (ProofDriven Query Answering), a system for determining a query plan in the presence of web-based sources. It is: (i) constraint-aware – exploiting relationships between sources to rewrite an expensive query into a cheaper one, (ii) access-aware – abiding by any access restrictions known in the sources, and (iii) cost-aware – making use of any cost information that is available about services. PDQ takes the novel approach of generating query plans from proofs that a query is answerable. We demonstrate the use of PDQ and its effectiveness in generating low-cost plans.	payment terminal;query plan;rewrite (programming);vocabulary;web application	Michael Benedikt;Julien Leblay;Efthymia Tsamoura	2014	PVLDB	10.14778/2733004.2733028	sargable;query optimization;query expansion;web query classification;boolean conjunctive query;computer science;data mining;database;web search query;range query;information retrieval;query language	DB	-33.974792658384885	4.860623076079299	88632
d12ccdcb36d8dee2daa3c4778fd99e8fd482e90a	t-laima: answer set programming for modelling agents with trust	agent platform;theoretical framework;multi agent system;answer set programming;design and implementation	In a time where multi-agent systems (MAS) become increasing ly more popular, they come in many forms and shapes depending on the requirements of the agents that n eed to populate them. Amongst the more demanding properties with respect to the design and impleme ntation is how these agents may individually reason and communicate about their knowledge and beliefs, w ith a view to cooperation and collaboration. With information coming from various sources, it becomes vi tal for agents to have an idea how reliable these information providers are, especially if they start t o contradict each other. In this paper we present a hybrid multi-agent platform, called T-LAIMA, using an ext nsion of answer set programming (ASP). We show that our framework is capable of dealing with the spec ification and implementation of the system’s architecture, communication and the individual reas oning capacities of the agents. We discuss both the theoretical framework which models a single, fixed encou ter between a number of agents and the implemenation that sets ups these encounters in a open multi -agent domain.	answer set programming;ext js javascript framework;multi-agent system;population;requirement;spec#;stable model semantics	Marina De Vos;Owen Cliffe;Richard Watson;Tom Crick;Julian Padget;Jonathan Needham	2005			computer science;knowledge management;artificial intelligence;theoretical computer science;answer set programming;multi-agent system;data mining;inductive programming	AI	-41.557759603516665	17.64315180360313	88711
b78c828fd2c5e589985e54826a92569c4cf17c81	the architectural design of a system for interpreting multilingual web documents in e-speranto		E-speranto is a formal language for generating multilingual texts on the World Wide Web. It is currently still under development. The vocabulary and grammar rules of E-speranto are based on Esperanto; the syntax of E-speranto, however, is based on XML (eXtensible Markup Language). The latter enables the integration of documents generated in E-speranto into web pages. When a user accesses a web page generated in E-speranto, the interpreter interprets the document into a chosen natural language, which enables the user to read the document in any arbitrary language supported by the interpreter. The basic parts of the E-speranto interpreting system are the interpreters and information resources, which complies with the principle of separating the interpretation process from the data itself. The architecture of the E-speranto interpreter takes advantage of the resemblance between the languages belonging to the same linguistic group, which consequently results in a lower production cost of the interpreters for the same linguistic group. We designed a proof-of-concept implementation for interpreting E-speranto in three Slavic languages: Slovenian, Serbian and Russian. These languages share many common features in addition to having a similar syntax and vocabulary. The content of the information resources (vocabulary, lexicon) was limited to the extent that was needed to interpret the test documents. The testing confirmed the applicability of our concept and also indicated the guidelines for future development of both the interpreters and E-speranto itself.	commonsense knowledge (artificial intelligence);exception handling;formal language;interpreter (computing);lexicon;markup language;natural language;vocabulary;web page;world wide web;xml	Grega Jakus;Jaka Sodnik;Saso Tomazic	2011	J. UCS	10.3217/jucs-017-03-0377	natural language processing;computer science;programming language;world wide web	NLP	-40.97219384564373	4.3875998973591335	88724
9fc8ee1865cfa8b3818ec34727e1f4c94e1d0f28	compatibility of service contracts in service-oriented ap	distributed application;application development;formal specification;design and development;service orientation;software architecture business data processing formal specification internet;ws business activity service contract compatibility distributed application service oriented architecture;software architecture;internet;business data processing;service oriented architecture;contracts web services algorithm design and analysis programming profession service oriented architecture standards development failure analysis runtime protocols web and internet services;service contract compatibility;ws business activity	One of the major difficulties for service-based application designers and developers is ensuring that the services participating in a distributed application are compatible when composed together under service oriented architectures. This paper examines this problem, and proposes a set of solutions by first defining ways of specifying compatibility conditions on contracts using standards such as WS-business activity and then presenting a tool that enables application developers check those conditions at design time	alan m. frieze;business architecture;distributed computing;fekete polynomial;interaction;run time (program lifecycle phase);soap service description language;service-oriented architecture;service-oriented device architecture;terminate (software);verification and validation;ws-federation	Surya Nepal;John Zic;Thi Chau	2006	2006 IEEE International Conference on Services Computing (SCC'06)	10.1109/SCC.2006.33	service level requirement;application service provider;business service provider;differentiated service;systems engineering;service delivery framework;software engineering;service design;database;service;business;data as a service	Visualization	-47.48921146839191	18.000094763705544	88764
36f37ba1cdf89f86486231759669a13c25f624b1	agent exchange - virtual trading environment	troc;trading agents;distributed system;user agent;ontologie;multiagent system;systeme reparti;interconnection;markets;multi agent system;realite virtuelle;integrated circuit;realidad virtual;mercado;implementation;virtual reality;circuito integrado;test bed;agent communication;secure communication;trading strategy;interconexion;sistema repartido;mediacion;distributed environment;mediation;marche;interconnexion;ontologia;swap;implementacion;sistema multiagente;ontology;circuit integre;systeme multiagent	Agent Exchange is a virtual trading environment serving as a test bed for experiments with market simulations, trading strategies and auctioning techniques. Agent Exchange is the distributed environment implementing up-to-date knowledge from multi-agent systems and secure communication. The developed agent community consists of independent agents communicating via defined protocols and ontology. The secure agent communication is an important part of this project. Roles that agents can play in the Agent Exchange community are Trading-agent, Bank-agent, Exchange-agent, Scenario-agent, User-agent, and Central treading authority agent. This paper describes the design of the Agent Exchange project, its functions and implementation details.	agent-based model;anytime algorithm;artificial intelligence;code;experiment;microsoft dynamics ax;multi-agent system;secure communication;simulation;testbed;traffic exchange;user agent	Jirí Hodík;Milan Rollo;Petr Novák;Michal Pechoucek	2003		10.1007/978-3-540-45185-3_17	simulation;engineering;autonomous agent;operations management;computer security;intelligent agent	AI	-39.38670299983543	16.57307929884957	88801
5e0c03d67f366e327c8c1f4fed407bfcbe04639a	semantic-based segmentation and annotation of 3d models	semantic driven annotation;semantic annotation;3d model;indexation;intelligent system;shape segmentation and structuring;data structure;object model	3D objects have become widely available and used in different application domains. Thus, it is becoming fundamental to use, integrate and develop techniques for extracting and maintaining their embedded knowledge. These techniques should be encapsulated in portable and intelligent systems able to semantically annotate the 3D object models in order to improve their usability and indexing, especially in innovative web cooperative environments. Lately, we are moving in this direction, with the definition and development of data structures, methods and interfaces for structuring and semantically annotating 3D complex models (and scenes) even changing in time according to ontology-driven metadata and following ontology-driven processes. Here, we concentrate on the tools for segmenting manifold 3D models and on the underline structural representation that we build and manipulate. We also describe the first prototype of an annotation tool which allows a hierarchical semantic-driven tagging of the segmented model and provides an interface from which the user can inspect and browse the entire segmentation graph.	3d modeling;application domain;browsing;data structure;embedded system;manifold regularization;ontology (information science);prototype;segmented regression;usability	Laura Papaleo;Leila De Floriani	2009		10.1007/978-3-642-04146-4_13	computer vision;object model;data structure;computer science;data mining;world wide web	AI	-39.017842759409675	9.21873717198197	88846
71c69343771cbdf680adeb2161e2de3bfba99d97	modeling context information for capture and access applications	query language;ubiquitous;xml schema;context information;xslt;computing;capture and access;capture and access applications;uml models;ubiquitous computing;xmi	The Contextractor is an XSLT-based transformation system that gathers information from extended UML models to produce XML Schemas that model the information captured by an application and define a query language that allows the submission of queries over the captured content.	query language;unified modeling language;xml namespace;xslt	Maria da Graça Campos Pimentel;Laércio A. Baldochi;Ethan V. Munson	2006		10.1145/1166160.1166187	computing;xslt;computer science;xs3p;applications of uml;xml schema;database;world wide web;ubiquitous computing;information retrieval;query language	DB	-35.67615591715471	13.43645441088159	88979
d2b33bdc02d4fadedb734da8709bb4737973c7b7	an approach to construct it service automatically	software;measurement;information technology;quality of service qos information technology it business process management bpm;software architecture business process re engineering;rapidly adapting;satisfiability;service architecture;computer architecture;software architecture;business;process control;information technology it;business process management;quality of service qos;business process management bpm;information technology it service construction business services service architecture;business services;quality of service;business process re engineering;service oriented architecture;quality of service service oriented architecture information technology quality management mathematical model packaging automation protocols computer science proposals;it service construction	Business services are required not only to rapidly adapt to changing conditions in the marketplace, but also to be built and deployed quickly and efficiently. The paper presents a proposal to bind particular applications to provide their business services automatically, which presents service architecture to describe its requirements, and decomposes them into multiple parts, maps the individual service into packet selections based on packet similar selection, and apply automated tools to accomplish its service, which can satisfy service requirement automatically and efficiently.	business requirements;map;network packet;requirement;xml	Wen Jun	2010	2010 International Conference on Service Sciences	10.1109/ICSS.2010.16	differentiated service;systems engineering;knowledge management;software engineering;business	Networks	-48.00977602278464	16.94940181437244	89094
b140f6305e2045d90002434975ec826101a10bbb	modeling trust management system for grids	trust management;inference rule;large scale;resource sharing;grid computing	Grid computing has been widely accepted as a promising paradigm for large-scale resources sharing in recent years. However, the general authorization mechanism, called trust management system, for grids is not wellunderstood. The purpose of this paper is to provide a logic-based formal approach to modeling authorization mechanisms of grids. We develop a logicbased language, called Trust Logic, to represent policies, credentials, and requests in distributed authorization. We give the inference rules of the logic as well as a logic-based compliance-checking algorithm for trust management in grids. In our formalism, the compliance-checking problem of trust management is equal to constructing a logical proof, which indicates whether a request is a logical consequence of a set of credentials and policies.	algorithm;authorization;credential;encode;formal proof;grid computing;management system;programming paradigm;proof (truth);semantics (computer science);trust management (information system);trust management (managerial science)	Baiyan Li;Wensheng Yao;Jinyuan You	2003		10.1007/978-3-540-24679-4_151	shared resource;computer science;knowledge management;operating system;data mining;database;distributed computing;grid computing;rule of inference	AI	-40.632076758676256	14.893008448786514	89149
1d5097b3c40d37ebe607a3b61d577d151179d0b8	publishing and discovering knowledge services for design on uddi	concepcion asistida;universal description;computer aided design;design process;cooperation;service connaissance;knowledge management;acquisition connaissances;cooperacion;knowledge service;bearing design;universal description discovery and integration;knowledge acquirement;turbo expander design;active magnetic bearings;knowledge acquisition;decouverte connaissance;discovery and integration;conception assistee;descubrimiento conocimiento;uddi;adquisicion de conocimientos;collaborative design;web based design;knowledge discovery	A precondition for internet-based collaborative design under an environment of distributed intelligent resources is that knowledge service providers can publish their own services globally and knowledge service requesters can conveniently and accurately discover the units, which can provide matched services via the internet. The widely supported specification of Universal Description, Discovery and Integration (UDDI) gives a considerable capability to publish and discover services via the internet. The paper introduces a conceptual framework and proposes a lifecycle model of publishing and discovering the intelligent resource units and matched services with UDDI. A project for improving the bearing system design of a turbo-expander is presented as a case study. The project deals with changing the traditional sliding bearings with active magnetic bearings. Technologies of different areas are involved in design and many resource units in prominent universities in China join the collaborative activities. The publication of knowledge services and discovery of knowledge services are implemented on a private UDDI registry operated by the Internet-Based Collaborative Research Center on Modern Design and Manufacturing.	web services discovery	Ai-bin Zhu;You-Bai Xie	2005	IJCAT	10.1504/IJCAT.2005.006420	simulation;computer science;engineering;knowledge management;artificial intelligence;marketing;computer aided design;data mining;knowledge extraction;ws-i basic profile;management;world wide web;universal description discovery and integration;mechanical engineering	HCI	-48.0123052448596	7.87536569443674	89656
b767c10a8a30719f760de16e7ff3be0afe0bdf80	the iff foundation for ontological knowledge organization	distributed database;model theory;information flow;semantic integration;digital library;first order logic	This paper discusses an axiomatic approach for the integration of ontologies, an approach that extends to first order logic a previous approach (Kent 2000) based on information flow. This axiomatic approach is represented in the Information Flow Framework (IFF), a metalevel framework for organizing the information that appears in digital libraries, distributed databases and ontologies (Kent 2001). The paper argues that the integration of ontologies is the two-step process of alignment and unification. Ontological alignment consists of the sharing of common terminology and semantics through a mediating ontology. Ontological unification, concentrated in a virtual ontology of community connections, is fusion of the alignment diagram of participant community ontologies – the quotient of the sum of the participant portals modulo the ontological alignment structure.	axiomatic system;diagram;digital library;distributed database;first-order logic;information flow;knowledge organization;library (computing);modulo operation;ontology (information science);organizing (structure);portals;unification (computer science)	Robert E. Kent	2002	CoRR		ontology alignment;digital library;semantic integration;information flow;computer science;artificial intelligence;unification;ontology;data mining;database;semantics;distributed database;logic;theory;model theory	AI	-42.22909795283601	7.0556856945515785	89679
e62727267d4d95b49863d9136146cb5440d5c9fb	matching ontologies in open networked systems: techniques and applications	ajustamiento modelo;ontologie;execution time;web semantique;semantics;intelligence artificielle;systeme ouvert;data mining;semantica;semantique;ajustement modele;transferencia conocimiento;analyse syntaxique;transfert des connaissances;fouille donnee;analisis sintaxico;web semantica;syntactic analysis;model matching;decouverte connaissance;semantic web;knowledge transfer;artificial intelligence;descubrimiento conocimiento;temps execution;ontologia;inteligencia artificial;tiempo ejecucion;semantic relations;networked systems;open systems;sistema abierto;busca dato;ontology;matching model;dynamic configuration;knowledge discovery;ontology matching	In open networked systems a varying number of nodes interact each other just on the basis of their own independent ontologies and of knowledge discovery requests submitted to the network. Ontology matching techniques are essential to enable knowledge discovery and sharing in order to determine mappings between semantically related concepts of different ontologies. In this paper, we describe the H-Match algorithm and related techniques for performing matching of independent ontologies in open networked systems. A key feature of H-Match is that it can be dynamically configured for adaptation to the semantic complexity of the ontologies to be compared, where the number and type of ontology features that can be exploited during the matching process is not known in advance as it is embedded in the current knowledge request. Furthermore, this number can vary, also for the same ontologies, each time a new matching execution comes into play triggered by a knowledge request. We describe how H-Match enforces this capabilities through a combination of syntactic and semantic techniques as well as through a set of four matching models, namely surface, shallow, deep, and intensive. Then, we describe the application of H-Match and its implementation for knowledge discovery in the framework of the Helios peer-based system. Finally, we present experimental results of using H-Match on different test cases, along with a discussion on precision and recall.	ontology (information science);open network architecture	Silvana Castano;Alfio Ferrara;Stefano Montanelli	2006		10.1007/11617808_2	computer science;artificial intelligence;parsing;semantic web;ontology;data mining;database;semantics;open system	DB	-38.17041987789657	12.807406097971436	89706
813ab4d82ed4ce9de7e78db23bab518df21f4c5b	workflow context as a means for intelligent information support	systeme gestion electronique document;groupware;systeme intelligent;context information;information sources;information source;source information;sistema inteligente;flujo informacion;classification;flux information;information flow;support information intelligent;electronic document management system;intelligent system;workflow;workflow management system;information system;collecticiel;sistema gestion electronica documento;clasificacion;systeme information;fuente informacion;sistema informacion	The paper presents Workflow Management Systems (WfMS) as valuable information sources for context information for accomplishing intelligent information support. First, the main advantages of WfMS for retrieving context are presented. Two projects from our research department show different approaches for retrieving workflow context and enabling an intelligent information support. However, they have to cope with the absence of a comprehensive representation concept for workflow context in WfMS. To overcome this, this paper presents a comprehensive classification scheme for workflow context.	comparison and contrast of classification schemes in linguistics and metadata;management system	Heiko Maus	2001		10.1007/3-540-44607-9_20	workflow;information flow;biological classification;computer science;document management system;data mining;database;world wide web;workflow management system;information system;workflow engine;workflow technology	AI	-38.11959547600171	12.5546348978539	89843
3af49c7d468b499d7e15c00ee43e9b8e53d0d22c	leveraging the expressivity of grounded conjunctive query languages	query language;conjunctive queries;aggregation operator;semantic web	We present a pragmatic extension of a Semantic Web query lang uage (including so-called grounded conjunctive queries) with a ermination safe functional expression language. This addresses problems encou ntered in daily usage of Semantic Web query languages for which currently no stand ardized solutions exist, e.g., how to define aggregation operators and used-de fine filter predicates. We claim that the solution is very flexible, since users can de fine and executead hoc extensions efficiently and safely on the Semantic Web reasoning server w ithout having to devise and compile specialized “built-ins” an d “plugins” in advance. We also address the scalability aspect by showing how aggreg ation operators can be realized efficiently in this framework.	application programming interface;assignment (computer science);client-side;compiler;conjunctive query;declarative programming;html;hoc (programming language);knowledge base;lambda calculus;mathematical optimization;plug-in (computing);query language;scalability;semantic web;server (computing);unified expression language;web ontology language	Alissa Kaplunova;Ralf Möller;Michael Wessel	2007		10.1007/978-3-540-76890-6_45	web query classification;boolean conjunctive query;computer science;data mining;database;programming language;web search query;query language	DB	-34.209131867782475	8.466521732864068	89901
44424b56edc0728d262027d403dc309313397d76	the wsmo-pa service editor	semantic web service;ontologies web services semantic web computational modeling user interfaces government writing;human computer interaction;formal specification;user friendly interface;semantic web wsmo pa service editor user friendly interface public administration formal specification web service modeling ontology;e government;government;editor;wsmo pa service editor;e government semantic web service editor wsmo;ontologies artificial intelligence;web services formal specification human computer interaction ontologies artificial intelligence public administration semantic web user interfaces;computational modeling;wsmo;web service modeling ontology;web services;semantic web;writing;ontologies;user interfaces;public administration	Semantic Web Service (SWS) editors did not support until now the expression of domain specific semantics. On the other hand, the adoption of SWSs in different domains, i.e. eGovernment, eBusiness, has created the need for enhancing SWS descriptions with domain specific semantics. Therefore, models and tools that allow this integration have to be developed. The WSMO-PA Service Editor is an effort made towards this direction. It provides a friendly user interface which facilitates the creation of semantically-enhanced descriptions for Public Administration (PA) services. To do so, the WSMO-PA Service Editor combines the GEA PA Service Model, the WSMO Framework and the WSMO-PA specification.	domain-specific language;semantic web service;sinewave synthesis;user interface;wsmo	Lemonia Giantsiou;Alex Simov;Nikos Loutas;Vassilios Peristeras;Konstantinos A. Tarabanis	2008	2008 IEEE International Conference on Semantic Computing	10.1109/ICSC.2008.22	computer science;knowledge management;semantic web;formal specification;database;writing;world wide web;government	Visualization	-45.136428523869796	17.316260601450406	89950
2832baa056d09d9a9afa10b6707df0928da746d6	protégé as a vehicle for developing medical terminological systems	terminologie;medical ontologies;representacion conocimientos;ontologie;knowledge based system;terminologia;aplicacion medical;protege;spectrum;universiteitsbibliotheek;conceptual framework;systeme terminologique medical;intensive care;knowledge acquisition;representation connaissance;terminological systems;ontologie medicale;ontologia;terminology;medical application;knowledge representation;ontology;knowledge modeling;application medicale	A medical terminological system (TS) is essentially an ontology consisting of concepts, attributes and relationships pertaining to medical terms. There are many TSs around today, most of which are essentially frame-based. Various efforts have been made to get a better understanding of the requirements and the conceptual and formal structures of TSs. However, the actual implementation of a TS consisted so far of ad hoc approaches starting from scratch and, due to ad hoc semantics of the representation, the interoperability with external applications of the knowledge represented is diminished. In recent years, PROTÉGÉ has been gaining in popularity as a software environment for the development of knowledge-based systems. It provides an architecture for integrating frame-based ontologies with knowledge acquisition and other applications operating on these ontologies. In its recent version, PROTÉGÉ provides the ability to specify meta-classes and -slots. This contributes to an explicit separability of knowledge levels and allows for an increased modeling flexibility. These properties, and the fact that it complies with a standard knowledge model, enable PROTÉGÉ to be an attractive candidate for the implementation of frame-based TSs. This paper investigates how to specify a TS in PROTÉGÉ and demonstrates this in a specific application in the domain of intensive care. Our approach is characterized by the utilization of a conceptual framework for see front matter r 2005 Elsevier Ltd. All rights reserved. .ijhcs.2005.02.005 nding author. Tel.: +3120 5665959; fax: +31 20 6919840. dresses: a.abu-hanna@amc.uva.nl (A. Abu-Hanna), r.cornet@amc.uva.nl (R. Cornet), c.uva.nl (N. de Keizer), crubezy@smi.stanford.edu (M. Crubézy), tu@smi.stanford.edu	fax;frame language;hoc (programming language);interoperability;knowledge acquisition;knowledge representation and reasoning;knowledge-based systems;linear separability;ontology (information science);protégé;requirement	Ameen Abu-Hanna;Ronald Cornet;Nicolette de Keizer;Monica Crubézy;Samson W. Tu	2005	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2005.02.005	spectrum;computer science;knowledge management;artificial intelligence;ontology;data mining;conceptual framework;terminology	AI	-43.56975074117077	6.656046747849464	90025
50c5f7fdb5adc28bb43a6ae6df1302970147b8eb	web services semantic searching enhanced by case reasoning	databases specification languages arithmetic expert systems application software computer science information systems pediatrics kernel constraint theory;data integrity;database management systems;flexible database integrity checking;database management systems data integrity;integrity checking;straitjacket;straitjacket flexible database integrity checking	Web services integrate various business application systems to provide worldwide platform to serve customers directly over the Internet. As the increasing number of business applications joining into the integration, the service activities are involving more and more complex situations. The enormous workload and the complex business processes involved by a request cannot be dealt with the simple request-answer model. It is highly desirable that Web services system can support vast clients' requests efficiently, effectively and promptly. This paper provides a searching mechanism to serve clients intelligently. It utilizes an efficient matchmaker to discover clients' requests by reusing past experiences. The matchmaker is enhanced by an automated discovery algorithm. It uses not only OWLS for identifying different matching levels by related domain ontologies but also is enhanced with reuse mechanism of case-based reasoning (CBR) with a formula for similarity. The proposed matchmaker takes advantages of semantics and CBR techniques to improve the efficiency and effectiveness of Web services searching.	algorithm;business process;business software;case-based reasoning;internet;ontology (information science);semantic search;web service	Ling Wang;Jinli Cao	2007	18th International Workshop on Database and Expert Systems Applications (DEXA 2007)	10.1109/DEXA.2007.81	computer science;data integrity;data mining;database	Web+IR	-44.77972874894565	13.965913030767554	90139
3a16ac11b7100d46c553e357ece1aef69b3cb54e	a combined hyperdatabase and grid infrastructure for data stream management and digital library processes	distributed system;base composition;digital library;data stream;information management;information need;stream processing	Digital libraries in healthcare are hosting an inherently large and continually growing collection of digital information. Especially in medical digital libraries, this information needs to be analyzed and processed in a timely manner. Sensor data streams, for instance, providing continuous information on patients have to be processed on-line in order to detect critical situations. This is done by combining existing services and operators into streaming processes. Since the individual processing steps are quite complex, it is important to efficiently make use of the resources in a distributed system by dynamically parallelizing operators and services. The Grid vision already considers the efficient routing and distribution of service requests. In this paper, we present a novel information management infrastructure based on a hyperdatabase system that combines the process-based composition of services and operators needed for sensor data stream processing with advanced grid features.		Manfred Wurz;Gert Brettlecker;Heiko Schuldt	2004		10.1007/11549819_5	computer science;data mining;database;world wide web	HPC	-36.41044372740547	15.170641344131464	90148
1fe310a81e27933f46dea6194cd96f8e1e8a520a	agent-based project management	multiagent system;systeme intelligent;project management;articulo sintesis;systeme cooperatif;agent based;article synthese;project manager;sistema inteligente;organizacion proyecto;planificacion;agents;cooperative systems;intelligent system;como kit;gestion projet;planning;planification;sistema multiagente;review;systeme multiagent	Integrated project management means that design and planning are interleaved with plan execution, allowing both the design and plan to be changed as necessary. This requires that the right eeects of change are propagated through the plan and design. When this is distributed among designers and planners, no one may have all of the information to perform such propagation and it is important to identify what eeects should be propagated to whom when. We describe a set of dependencies among plan and design elements that allow such notiica-tion by a set of message-passing software agents. The result is to provide a novel level of computer support for complex projects.	integrated project support environment;message passing;software agent;software propagation;technical support	Charles J. Petrie;Sigrid Goldmann;Andreas Raquet	1999		10.1007/3-540-48317-9_14	planning;project management;simulation;software agent	AI	-39.39287744206285	17.57794147436386	90177
2b2aaedc0acbbc9ace2176ea13ea85e7b375a751	production rule based selection decision for dynamic flexible workflow	multiple workflow engine;dynamic flexible workflow;workflow routines;workflow engines;production web services engines clustering algorithms neural networks support vector machines artificial intelligence expert systems grid computing dispatching;scientific workflow;workflow management software decision making natural sciences computing web services;web service;dynamic selection;dynamic workflow;data clustering;web services;workflow management software;production rule strategy;production rule based selection decision;dynamic workflow production rule based selection decision dynamic flexible workflow scientific workflow dynamic selection workflow routines web services workflow engines multiple workflow engine production rule strategy;natural sciences computing;production rule;neural network	Scientific workflow often requires dynamic selection of workflow routines, Web services or workflow engines. Multiple copies of a Web service or multiple workflow engines with different performance are chosen at run time to optimise the workflow. However, simple performance formula for selecting Web services or workflow engines is difficult to find. In some cases, the services with the same functions but with different algorithms (for example, data clustering services implemented by using the algorithms of neural network or SVM) may be only chosen at the just pre-execution point according to the intermediate results of workflow execution. We here use production rule strategy to solve the above problem. In this paper, we present a framework for production rule based selection decision for dynamic workflow, and give out a primary implementation of this framework.	algorithm;artificial neural network;clips;cluster analysis;e-science;experiment;expert system;formal grammar;mathematical optimization;parallel computing;prototype;run time (program lifecycle phase);web service;world wide web	Lican Huang	2007	Third IEEE International Conference on e-Science and Grid Computing (e-Science 2007)	10.1109/E-SCIENCE.2007.61	web service;workflow;computer science;knowledge management;data mining;database;windows workflow foundation;law;workflow management system;artificial neural network;workflow engine;workflow technology	DB	-41.21269017285656	15.716072592410839	90296
52d8c14b3925c012b3c88f0c8f0963d436d571e2	swrl rule editor - a web application as rich as desktop business rule editors		The Semantic Web Rule Language (SWRL) allows the combination of rules and ontology terms, defined using the Web Ontology Language (OWL), to increase the expressiveness of both. However, as rule sets grow, they become difficult to understand and error prone, especially when used and maintained by more than one person. If SWRL is to become a true web standard, it has to be able to handle big rule sets. To find answers to this problem, we first surveyed business rule systems and found the key features and interfaces they used and then, based on our finds, we proposed techniques and tools that use new visual representations to edit rules in a web application. They allow error detection, rule similarity analysis, rule clustering visualization and atom reuse between rules. These tools are implemented in the SWRL Editor, an open source plug-in for Web-Protégé (a web-based ontology editor) that leverages Web-Protégé’s collaborative tools to allow groups of users to not only view and edit rules but also comment and discuss about them. We evaluated our solution comparing it to the only two SWRL editor implementations openly available and showed that it implements more of the key features present in traditional rule systems.	cluster analysis;cognitive dimensions of notations;collaborative software;error detection and correction;ontology (information science);open-source software;plug-in (computing);protégé;semantic web rule language;web ontology language;web application;web standards	João Paulo Orlando;Adriano Rivolli;Saeed Hassanpour;Martin J. O'Connor;Amarendra K Das;Dilvan A. Moreira	2012			database	Web+IR	-40.43572507665334	10.461856655421867	90341
ddbb7fcfb802cb73f18345ad0891c68dd0343b08	implementing a flexible compensation mechanism for business processes in web service environment	protocols;web services business data processing compensation protocols;backward recovery;swinburne;web service;satisfiability;web service environment;multiple compensation mechanism business processes web service environment backward recovery compensation protocol;compensation;business data processing;web services;multiple compensation mechanism;compensation protocol;web services business communication xml computer architecture service oriented architecture web and internet services simple object access protocol communications technology australia computer languages;business process;business process integration;business processes	Web services have been emerging as a promising technology for business process integrations. Due to their long-duration and loosely coupled properties, Web service based applications require transactional support beyond traditional transactions. Some Web service standards have been proposed to deal with the transaction aspect of Web service applications. Compensation is a commonly used mechanism in these standards for backward recovery. However, the compensation mechanism usually adopted is too fixed and cannot satisfy the various requirements of different applications. In this paper, we first analyse the compensation protocol of current standards. Then we enrich the protocol by allowing flexible compensation and extend our proposed multiple-compensation mechanism in Web service environment. The implementation of the extended compensation mechanism is discussed and the incorporation of the mechanism into current standards is also addressed	business process;database transaction;loose coupling;requirement;web service	Zaihan Yang;Chengfei Liu	2006	2006 IEEE International Conference on Web Services (ICWS'06)	10.1109/ICWS.2006.72	web service;web modeling;computer science;knowledge management;ws-policy;database;business process;law;world wide web	DB	-47.33212621692245	18.05944973947133	90395
5261c657ed2ae0ff26f8af6a9629de2a474945b8	semantic wikis versioning with bifröst		One of the powerful and popular tools that are used to support Collaborative Knowledge Engineering are Semantic Wikis. They are easily accessible and provide ACL mechanisms, but they lack a good versioning mechanism. In this paper an extended version of such a mechanism is proposed. Besides elements that appear in every Wiki system, like simple changelog and place for discussion, it incorporates changes ontologies, rich metadata, semantic metrics and reasoning unit tests. All of them are gathered into the form of the provenance graph that can be serialized into Turtle syntax and (automatically) analyzed. First prototype of such a mechanism for Loki wiki called BiFröST was developed.	access control list;knowledge engineering;ontology (information science);prototype;sparql;software versioning;unit testing;wiki	Krzysztof Kutt	2016			database;computer science	Web+IR	-39.67215601706219	9.280487750716842	90637
9b77c3db37df31f80b017e8812a62aec6d8544b9	new headway for the semantic web.	semantic web			Stephanie Parker	2010	ERCIM News		semantic computing;semantic web rule language;data web;semantic search;semantic grid;web standards;computer science;artificial intelligence;semantic web;web navigation;social semantic web;semantic web stack;database;web intelligence;semantic technology;world wide web;owl-s;website parse template;information retrieval;semantic analytics	Web+IR	-39.83257730597346	7.152156155185933	90804
0b83045fabe1d81361c9c06e9f0c0e5059fe0785	distributed application framework for earth science data processing	distributed application;flexible framework;long period;general or miscellaneous;geoscience data processing xml standards development parallel processing java numerical simulation nasa algorithm design and analysis filters;filters;data processing;data type;geophysical measurement technique;distributed application framework;algorithm;instruments and techniques;standards development;geoscience;earth science data;geophysics computing;geophysical signal processing;remote sensing;xml;on the fly;cost effectiveness;system development;remote sensing geophysical signal processing geophysics computing;pipelining;extensible framework;nasa;algorithm design and analysis;parallel processing;pipeline;numerical simulation;java;flexible framework geophysical measurement technique data processing remote sensing distributed application framework algorithm pipeline pipelining extensible framework	"""One of the characteristics of Earth science data is their diversity, which results in a large number of similar algorithms tailored to a specific data set. Moreover, it is often difficult to connect several algorithms in a """"pipeline"""" where output of one is the input of the other. The solution we propose in this paper is a flexible and an extensible framework that enables us to decouple the data from the application and lay the foundation for integration of several algorithms into a single system."""	application framework;distributed computing	Andrew Neuschwander;Joseph C. Coughlan	2002		10.1109/IGARSS.2002.1025645	parallel processing;data processing;computer science;theoretical computer science;data mining;database;pipeline	DB	-36.011507618984666	16.93631569312816	90899
3b0968582bf8ac56b0411e8a7f7f1393a09be64d	building an operational product ontology system	e catalog;e commerce;reference model;relational database;product information management;document database;product ontology;information management;ontology construction;ontology;knowledge base	A base of clearly defined product information is a key foundation for an e-commerce system. The manipulation and exchange of semantically enriched and precise product information can enhance the quality of an e-commerce system and offer a high level of interoperability with other systems. Product information consists of product attributes and the relationships between products. Product categorization (or classification) is one type of such relationships. Ontology can play an important role in the formalization of product information. Although the idea of utilizing ontology for e-Catalogs has been raised before, we are yet to find an operational implementation of applying ontology in the domain. In this paper, we report on our recent effort to build an operational product ontology system for a government procurement service. The system is designed to serve as a product ontology knowledge base; not only for the design and construction of product databases but also for search and discovery of products. Especially, the keyword-based searching over product ontology database demands different techniques from those over conventional document databases or relational databases, and should be designed to reflect particular characteristics of product ontology. We also introduce some other issues that we have experienced in the project, and those issues include product ontology modeling, ontology construction and maintenance, and visualization. Our work presented herein may serve as a reference model for similar projects in the future. 2005 Elsevier B.V. All rights reserved.	categorization;e-commerce;high-level programming language;interoperability;knowledge base;procurement;reference model;relational database	Taehee Lee;Ig-hoon Lee;Suekyung Lee;Sang-goo Lee;Dongkyu Kim;Jonghoon Chun;Hyunja Lee;Junho Shim	2006	Electronic Commerce Research and Applications	10.1016/j.elerap.2005.08.005	e-commerce;upper ontology;knowledge base;ontology alignment;reference model;ontology components;bibliographic ontology;ontology inference layer;relational database;computer science;knowledge management;ontology;ontology;data mining;database;information management;ontology-based data integration;world wide web;owl-s;process ontology;suggested upper merged ontology	Web+IR	-42.70310870504765	6.01910805880168	91058
50bb5686b75c173e22dbd4a57da55fa61160578a	service-oriented architectures and mobile services	service oriented computing;distributed system;service oriented architecture	Service-Oriented architectures and Service-Oriented Computing are the most recent approaches aiming at facilitating the design and development of applications on distributed systems. The primary goal of this paper is to investigate how the construction of mobile services can benefit from the Service-Oriented paradigm. The paper provides an elucidation of the ServiceOriented architecture. A general discussion of equivalence between service components is then undertaken, in order to enable an analysis of ServiceOriented architectures for mobile services. The paper proceeds with a mapping of existing mobile services on Service-Oriented architectures. The requirements of mobile services, which must be taken into consideration in the ServiceOriented architecture, are identified from a generic model of mobile services. A Service-Oriented architecture supporting mobile services is proposed.	distributed computing;entity;personalization;programming paradigm;requirement;scott continuity;service-oriented architecture;turing completeness	Ivar Jørstad;Schahram Dustdar;Do Van Thanh	2005			service delivery framework;mobile qos;systems engineering;architecture;mobile business development;mobile service;computer science;service-orientation;service-oriented architecture;distributed computing;service (systems architecture)	SE	-46.122519067111796	17.123964575566884	91105
cb90c1b47f8a5396c25e06aeca70d6262043bd84	a web services-based cloud interface agent for intelligent yoho information processing	intelligent young home information processing web service based cloud interface agent intelligent yoho information processing cloud interactive diagrams yoho information systems internet system developments tableaux;query processing;software agents;graphical user interfaces;web services;web services cloud computing graphical user interfaces query processing software agents;monitoring cloud computing artificial intelligence information processing databases;cloud computing	In this paper, a cloud interface agent with Web service techniques for intelligent YOHO information processing was presented. The agent is not only able to explore related technologies in order to establish Web service platforms, but it can also investigate the construction of cloud interactive diagrams using Web service techniques for extensively and seamlessly integrating backend YOHO information systems on the Internet. The preliminary system developments and tableaux show that the research results yield positive outcomes.	diagram;information processing;information system;intelligent user interface;web service	Sheng-Yuan Yang;Dong-Liang Lee;Lawrence Y. Deng	2013	2013 International Conference on IT Convergence and Security (ICITCS)	10.1109/ICITCS.2013.6717797	computer science;cloud testing;database;distributed computing;world wide web	Robotics	-42.32074049139431	9.976788862162026	91345
d5446acea727eb59378980884ad4b5389dfcbb52	advanced profile similarity to enhance semantic web services matching		In this paper, we present a fine-grained matching method of the services based on a hybrid similarity measure. We propose a novel encoding of the services descriptions, allowing the match between a request and an advertisement in order to make more efficient publishing and searching process of Web services and reduce the number of comparisons required. By this kind of similarity between concepts of profile, a precise matching method is developed to match the profile of the Web services and user. Searching process in the UDDI registry is done via an algorithm that allows us to extract the search concepts and retrieve the topk services, thereby further reducing the search engine's response time. The approach is illustrated through some experiments both on real and synthetic data to demonstrate its consistency and effectiveness.	algorithm;experiment;fuzzy set;response time (technology);semantic web service;similarity measure;sinewave synthesis;synthetic data;user profile;web services discovery;web search engine;world wide web	Chouiref Latreche Zahira;Abdelkader Belkhir;Allel HadjAli	2013	iJES		computer science;data mining;world wide web;information retrieval	DB	-44.573852558257016	13.388539972328148	91355
dc6b5bb07df17b68bb6cf7d28eabd3a385c3d9ed	creating a semantic integration system using spatial data	owl;data integrity;spatial data;semantic integration;semantic web;geospatial;ontology;rdf	Data integration is complex often requiring much te chnical knowledge and expert understanding of the data and its meaning. In this paper we investigate the use of cu rrent semantic tools as an aid to data integration, and identify t he need to modify these tools to meet the needs of spatial dat a. Illustrating the benefits of exposing the semantics of integrati on hrough creation of a demonstrator.	semantic integration	Jennifer B Green;Glen Hart;Catherine Dolbear;Paula C. Engelbrecht;John Goodwin	2008			idef1x;semantic computing;semantic integration;semantic web rule language;computer science;artificial intelligence;semantic web;rdf;ontology;social semantic web;data integrity;data mining;semantic web stack;database;spatial analysis;ontology-based data integration;information retrieval;semantic analytics;data mapping	DB	-38.631687836758445	5.820007031232911	91362
ef63f5cc75e6fd86cbc355241880e2135ba4e962	nine principles of semantic harmonization		Medical data is routinely collected, stored and recorded across different institutions and in a range of different formats. Semantic harmonization is the process of collating this data into a singular consistent logical view, with many approaches to harmonizing both possible and valid. The broad scope of possibilities for undertaking semantic harmonization do lead however to the development of bespoke and ad-hoc systems; this is particularly the case when it comes to cohort data, the format of which is often specific to a cohort's area of focus. Guided by work we have undertaken in developing the 'EMIF Knowledge Object Library', a semantic harmonization framework underpinning the collation of pan-European Alzheimer's cohort data, we have developed a set of nine generic guiding principles for developing semantic harmonization frameworks, the application of which will establish a solid base for constructing similar frameworks.		James A. Cunningham;Michel Van Speybroeck;Dipak Kalra;Rudi Verbeeck	2016	AMIA ... Annual Symposium proceedings. AMIA Symposium		underpinning;bespoke;data mining;harmonization;collation;cohort;computer science;guiding principles	Embedded	-42.98017098361203	4.289225032853885	91577
0e7c37e87eb273ab4e1c1fe25f350854fb89bc17	enabling knowledge representation on the web by extending rdf schema	metadata;xslt;formal semantics;schema;xml;interoperability;knowledge representation;rdf	Recently, there has been a wide interest in using ontologies on the Web. As a basis for this, RDF Schema (RDFS) provides means to define vocabulary, structure and constraints for expressing metadata about Web resources. However, formal semantics are not provided, and the expressivity of it is not enough for full-fledged ontological modeling and reasoning. In this paper, we will show how RDFS can be extended in such a way that a full knowledge representation (KR) language can be expressed in it, thus enriching it with the required additional expressivity and the semantics of this language. We do this by describing the ontology language OIL as an extension of RDFS. An important advantage of our approach is a maximal backward compatability with RDFS: any meta-data in OIL format can still be partially interpreted by any RDFS-only-processor. The OIL extension of RDFS has been carefully engineered so that such a partial interpretation of OIL meta-data is still correct under the intended semantics of RDFS: simply ignoring the OIL specific portions of an OIL document yields a correct RDF(S) document whose intended RDFS semantics is precisely a subset of the semantics of the full OIL statements. In this way, our approach ensures maximal sharing of meta-data on the Web: even partial interpretation of meta-data by less semantically aware processors will yield a correct partial interpretation of the metadata. We conclude that our method of extending is equally applicable to other KR formalisms.	central processing unit;declaration (computer programming);expectation–maximization algorithm;glossary of computer graphics;knowledge representation and reasoning;maximal set;monica s. lam;ontology (information science);ontology inference layer;rdf schema;semantic web;semantics (computer science);vocabulary;web resource;world wide web	Jeen Broekstra;Michel C. A. Klein;Stefan Decker;Dieter Fensel;Frank van Harmelen;Ian Horrocks	2001		10.1145/371920.372105	knowledge representation and reasoning;interoperability;xml;xslt;ontology inference layer;computer science;rdf;formal semantics;data mining;schema;database;programming language;web ontology language;metadata;world wide web;information retrieval;rdf schema	Web+IR	-35.768241957490716	6.666385451623964	91691
445b0b453aa885a5c69879c0b19794a3ebb3c6a4	research on construction and swrl reasoning of ontology of maize diseases	ontology of maize diseases;rule reasoning;sqwrl;swrl	In this paper, according to the characteristics of maize disease knowledge, OWL DL language was used to build maize diseases ontology, and the reasoning rule of maize diseases was defined by using the expressive ability of SWRL rule language.The author introduced several realizable reasoning functions,and achieved the diagnostic reasoning of maize disease knowledge by Jess inference engine.The results indicated that constructing the maize diseases ontology,and introducing SWRL rule into maize disease ontology provided an effective way for the construction of high-intelligent, shareable and reused maize disease knowledge database and diagnostic rule database.	disease ontology;inference engine;jess;semantic web rule language;web ontology language	Li Ma;Helong Yu;Guifen Chen;Liying Cao;Yueling Zhao	2012		10.1007/978-3-642-36137-1_45	engineering;knowledge management;data mining;information retrieval	AI	-37.65979504151039	6.941986594397724	92206
e6342e2516ce9ffd1ec3e302fcc244b78cd2374f	semantic sparql query in a relational database based on ontology construction	owl;mapping rule relational database ontology sql algebra sparql;ontologies resource description framework semantics algebra owl database languages;sql algebra;semantics;relational database;resource description framework;relational databases knowledge representation languages ontologies artificial intelligence query languages relational algebra;mapping rule;algebra;ontologies;sparql;semantic query language semantic sparql query relational database ontology construction semantic web owl rdf triples owl ontology rdb schema direct mapping rules semantic query engines sparql query rewriting sql relational algebra web ontology language resource description framework;ontology;database languages	Constructing an ontology from RDBs and its query through ontologies is a fundamental problem for the development of the semantic web. This paper proposes an approach to extract ontology directly from RDB in the form of OWL/RDF triples, to ensure its availability at semantic web. We automatically construct an OWL ontology from RDB schema using direct mapping rules. The mapping rules provide the basic rules for generating RDF triples from RDB data even for column contents null value, and enable semantic query engines to answer more relevant queries. Then we rewriting SPARQL query from SQL by translating SQL relational algebra into an equivalent SPARQL. The proposed method is demonstrated with examples and the effectiveness of the proposed approach is evaluated by experimental results.	amiga rigid disk block;ontology (information science);relational algebra;relational database;resource description framework;rewriting;sparql;sql;semantic web;semantic query;web ontology language	Mohamed A. G. Hazber;Ruixuan Li;Xiwu Gu;Guandong Xu;Yuhua Li	2015	2015 11th International Conference on Semantics, Knowledge and Grids (SKG)	10.1109/SKG.2015.14	upper ontology;f-logic;sargable;query optimization;named graph;semantic web rule language;turtle;bibliographic ontology;relational database;computer science;sparql;ontology;artificial intelligence;query by example;rdf;ontology;data mining;database;semantics;rdf query language;conjunctive query;web ontology language;owl-s;information retrieval;rule interchange format;query language;rdf schema	Web+IR	-35.04122357579665	6.676529629573619	92265
34cf8de0fde17e01fd24a922dc5427a61c56b8b4	a grid-based flavonoid informatics portal	portail web;interfase usuario;xml schema;user interface;xml language;information technology;service web;technologie information;biology;biologia;web service;grid;portal web;efficient implementation;web portal;rejilla;grille;interface utilisateur;tecnologia informacion;langage xml;lenguaje xml;biologie;servicio web	Recently new techniques to efficiently manage biological information of biology have played an important role in the area of information technology. The flavonoids are members of a class of natural compounds that recently has been the subject of considerable scientific and therapeutic interest. This paper presents a Grid-based flavonoids web portal system. We designed relational schema, XML schema for flavonoids information and their user interfaces and proposed interoperable web service components for an efficient implementation of flavonoids web portal.	database schema;informatics;interoperability;user interface;web service;xml schema	HaiGuo Xu;Karpjoo Jeong;Seunho Jung;Hanku Lee;Segil Jeon;Kumwon Cho;Hyunmyung Kim	2006		10.1007/11758549_44	web service;xml;computer science;data mining;xml schema;database;user interface;grid;information technology;world wide web	HPC	-36.82276465935365	12.521346076248523	92520
5647d2c042fa7e2cccdf98454591a73954de2ac7	data and metadata on the semantic grid	information systems;metadata;data;distributed computing;computer networks;grid;internet;metacomputing metadata semantic grid information systems;visual databases bioinformatics artificial intelligence dynamic programming joining processes image databases computer displays data visualization logic programming grid computing;web sites;semantic;meta data;information system;metacomputing;metacomputing meta data information systems;semantic grid	Introduction Last issue, we covered the important role that the Grid is expected to have in information systems; roughly the Grid of Sensors and databases. We contrasted this with the major original Grid focus of metacomputing or the Grid linking distributed computers together. Whatever type of Grid one has, metadata or data about data will be important. The term is as always loosely defined and is often equivalent to information presented in “small highvalue records”. Examples of generally important Grid metadata include user information (name, address and the many profiles and preferences one accumulates) and specifications of the resources on the Grid. These could include for each computer the CPU, memory, and number of nodes (if parallel) while for software, there would location, compiler options and perhaps specification of needed input data. We have explained in previous articles, the underlying service model developed for both the Web and the Grid. This allows us to identify “service metadata” as a critical feature of the evolving architecture. Computers and software are both services in the Grid model but so also are databases, sensors, network and user information systems. In this article, we try to describe some of the many different sources and approaches to metadata.	central processing unit;compiler;computer;database;information system;metacomputing;semantic grid;sensor;world wide web	Geoffrey C. Fox	2003	Computing in Science and Engineering	10.1109/MCISE.2003.1225865	grid file;semantic grid;computer science;data grid;database;metadata;world wide web;information retrieval;information system;grid computing;metadata repository	HPC	-36.70535898258174	9.687099836195799	92699
c71f254ffc15d97a76b038941f5df5466c1e0274	query expansion methods and performance evaluation for reusing linking open data of the european public procurement notices	relevant data;open data;information variable;public procurement notice;query expansion method;accurate information retrieval system;open data approach;user query;performance evaluation;european public procurement notice;data vocabulary;unstructured information	The aim of this paper is to present some methods to expand user queries and a performance evaluation to retrieve public procurement notices in the e-Procurement sector using semantics and linking open data. Taking into account that public procurement notices contain information variables like type of contract, region, duration, total value, target enterprise, etc. di erent methods can be applied to expand user queries easing the access to the information and providing a more accurate information retrieval system. Nevertheless expanded user queries can involve an extra-time in the process of retrieving notices. That is why a performance evaluation is outlined to tune up the semantic methods and the generated queries providing a scalable and time-e cient system. On the other hand this system is based on the use of semantic web technologies so it is necessary to model the unstructured information included in public procurement notices (organizations, contracting authorities, contracts awarded, etc.), enrich that information with existing product classi cation systems and linked data vocabularies and publish the relevant data extracted out of the notices following the linking open data approach. In this new LOD realm these techniques are considered to provide added-value services like search, matchmaking geo-reasoning, or prediction, specially relevant to small and medium enterprises (SMEs).	e-procurement;information retrieval;linked data;performance evaluation;procurement;query expansion;scalability;semantic web;vocabulary	Jose María Álvarez Rodríguez;José Emilio Labra Gayo;Ramón Calmeau;Ángel Marín;José Luis Marín	2011		10.1007/978-3-642-25274-7_50	computer science;data mining;database;world wide web	Web+IR	-40.578568708008774	7.872068706105281	92801
b45115a178d4279d882c44c6c4ce398c88ee03c9	articulating information needs in xml query languages	mathematical model;experimental analysis;query language;xml document;information need	Document-centric XML is a mixture of text and structure. With the increased availability of document-centric XML documents comes a need for query facilities in which both structural constraints and constraints on the content of the documents can be expressed. How does the expressiveness of languages for querying XML documents help users to express their information needs? We address this question from both an experimental and a theoretical point of view. Our experimental analysis compares a structure-ignorant with a structure-aware retrieval approach using the test suite of the INEX XML Retrieval Evaluation Initiative. Theoretically, we create two mathematical models of users' knowledge of a set of documents and define query languages which exactly fit these models. One of these languages corresponds to an XML version of fielded search, the other to the INEX query language.Our main experimental findings are: First, while structure is used in varying degrees of complexity, two-thirds of the queries can be expressed in a fielded-search-like format which does not use the hierarchical structure of the documents. Second, three-quarters of the queries use constraints on the context of the elements to be returned; these contextual constraints cannot be captured by ordinary keyword queries. Third, structure is used as a search hint, and not as a strict requirement, when judged against the underlying information need. Fourth, the use of structure in queries functions as a precision enhancing device.	information needs;query language;xml	Jaap Kamps;Maarten Marx;Maarten de Rijke;Börkur Sigurbjörnsson	2006	ACM Trans. Inf. Syst.	10.1145/1185879	xml catalog;xml validation;binary xml;information needs;xml encryption;simple api for xml;xml;relax ng;xml schema;streaming xml;computer science;document structure description;xml framework;mathematical model;data mining;xml database;xml schema;database;xml signature;world wide web;xml schema editor;cxml;information retrieval;query language;efficient xml interchange;statistics;experimental analysis of behavior	DB	-34.17061054308093	6.693741350371974	92857
7cac47d80dcbb3e04d80bc5d8114573f0d16fb5b	matchmaking by difference in service discovery.	service discovery;matching model	Service discovery over the Web is considered a crucial issue; in particular, flexibility of the discovery process, that is, the ability of recognizing not only exact matches between the requests and offers, but also partial ones, needs to be enhanced. We have defined a composite approach to flexible service matchmaking with different matching models. In this paper we present the novel difference-based matching model. The approach is based on an ontological framework adding semantics to service descriptions. Optimization and ranking techniques are provided.	curve fitting;mathematical optimization;ontology (information science);program optimization;service discovery;world wide web	Devis Bianchini;Valeria De Antonellis;Michele Melchiori	2006			database;world wide web	DB	-44.610857930261695	13.740365688500923	93053
7b5082dc8a78f35e01aedf79b53745067b21dbc5	methods and techniques for ontology-based semantic interoperability in networked enterprise contexts.	resource discovery;network organization;semantic interoperability;request for information;virtual organization;semantic description;user requirements;middleware;context dependent;ministry of education;semantic matching	By exploiting open distributed architectures like Peer-to-Peer (P2P) and Grids, networked organizations can dynamically cooperate and share resources often in response to opportunities or challenges that cannot be anticipated in advance and require a rapid response [1, 7]. One of the major obstacles in the deployment of networked organizations like Virtual and Extended Enterprises is the lack of semantic interoperability techniques, to adequately ensure the effective, coordinated, and virtualized access to distributed and heterogeneous resources, under dynamic and context-dependent requirements. In particular, in order to support the correct and effective sharing and composition of informational resources to provide complex services in virtual organizations, an important requirement is the availability of a semantic description of the resources to be shared, to make them understandable and usable by the target community. In addition, to enable seamless access and retrieval of the right information resources, in the time frame that the users require, while preserving the information representation and management requirements of each single party involved in the networked organization coalition, appropriate matchmaking techniques are required, to dynamically perform the matching of requests for information resources against a multitude of, possibly heterogeneous, ontology descriptions. In this paper, we address the problem of semantic interoperability in networked, multi-ontology enterprise contexts, where ontologies are employed for representing informational resources that each party provides for dynamic sharing and exchange with other parties. We study the problem of semantic interoperability under the assumption that no agreement exists among the various parties about the way resources are described in each ontology and about the vocabulary to be adopted for semantic description of resources. We propose a reference ontology model and associated semantic matching techniques to enforce dynamic resource discovery, by exploiting semantic resource descriptions available in an ontology	context-sensitive language;entrepreneurial network;grid computing;network-centric organization;ontology (information science);peer-to-peer;requirement;seamless3d;semantic interoperability;semantic matching;software deployment;upper ontology;virtual organization (grid computing);vocabulary	Silvana Castano;Alfio Ferrara;Stefano Montanelli	2004			semantic data model;upper ontology;semantic interoperability;semantic computing;semantic integration;semantic grid;computer science;knowledge management;user requirements document;context-dependent memory;middleware;semantic web stack;database;semantic technology;world wide web;semantic analytics	Web+IR	-44.03637948662228	9.809049838827738	93377
18139fa7e02ea043ef68f0b96f1436bbcfda8c59	mydeepweb: an integration service for your own deep web data	software;object linking deep web data integration service;google;relational data;electronic mail;distributed database;storage system;data integrity;api;internet service provider;data mining;mydeepweb system;integration service;private cloud based data store mydeepweb system integration service internet services api;internet;data integration service;application program interfaces;web sites;distributed databases;internet services;search problems;object linking;data mining google distributed databases search problems software electronic mail;deep web;user interaction;web sites application program interfaces cloud computing internet;private cloud based data store;reading and writing;cloud computing;integrated services	Current Web users usually have their own files, work documents, communications and personal contacts distributed in the storage systems of many widely-used Internet services(e.g. Google Docs, Gmail, Facebook, Zoho). Therefore, they face the challenge of being not able to have an integrated view for their related data objects (e.g. mails, pics, docs, contacts). Recently, most of the major Internet services provide standard APIs that allow developing software applications that can read and write data from their underlying data store after providing the credential access information of registered accounts. The MyDeepWeb system is designed to let the Web users interact with their Internet services normally while, behind the scene, the information of their objects will be extracted, consolidated, linked and then populated into a single private cloud-based data store where the user can have integrated access to their data objects from anywhere through multiple devices.	application programming interface;cloud computing;credential;docs (software);data store;deep web;gmail;platform for internet content selection;population;web service;world wide web	Sherif Sakr;Anna Liu	2011	2011 IEEE International Conference on Services Computing	10.1109/SCC.2011.20	computer science;database;internet privacy;world wide web	DB	-38.857028314792586	10.28316487440441	93393
d04a2efa2380b37b412e31aa1e81d2b17eddad84	a profile based security model for the semantic web	modelizacion;controle acces;model based reasoning;raisonnement base sur modele;security model;pervasive computing;web semantique;service web;semantics;web service;securite donnee;administration publique;semantica;semantique;informatica difusa;modelisation;large scale;semantic model;multi domain;informatique diffuse;web semantica;semantic web;civil service;access control;information system;administracion publica;public service;modeling;security of data;systeme information;domain specificity;servicio web;sistema informacion	The trend towards ubiquitous public services is driving the deployment of large scale, heterogeneous, semantic distributed service infrastructures. The critical and valuable assets of open services need to be protected using heterogeneous security models such as multiple domain-specific authorisation and access control mechanisms. A dynamic approach to managing inter-domain security to support openness is required. A semantic model that uses profiles, that supports policy type constraints and that supports profile-based security information interchange for multi-domain services has been developed.	semantic web	Juan Jim Tan;Stefan Poslad	2004		10.1007/978-3-540-30209-4_4	semantic data model;computer security model;web service;systems modeling;security information and event management;asset;computer science;artificial intelligence;access control;model-based reasoning;semantic web;database;security service;semantics;world wide web;computer security;information system	Web+IR	-38.301258635241204	13.357005678360647	93416
1ea0a94f57358098450b62b9c21aed5d157e7692	yasemir: yet another semantic information retrieval system	terminologies;ontologies;semantic search;information retrieval models;lucene	In this paper we present YaSemIR, a free open-source Semantic Information Retrieval system based on Lucene. It takes one or more ontologies in OWL format and a terminology associated to each ontology in SKOS format to index semantically a text collection. The terminology is used to annotate concepts in documents, while the ontology is used to exploit the taxonomic information in order to expand these with their subsumers. YaSemIR is a flexible system that may be configured to work with different ontologies, on various types of documents.	information retrieval;ontology (information science);open-source software;simple knowledge organization system;web ontology language;yet another	Davide Buscaldi;Haïfa Zargayouna	2013		10.1145/2513204.2513211	computer science;concept search;data mining;database;information retrieval	Web+IR	-40.178250068564736	5.482807867757729	93489
7ebe0e76423e91e518df593eff9ccb5bb706b30d	a rule-based service customization strategy for smart home context-aware automation	service customization;decision support;smart home;rough set theory based rule generation method rule based service customization strategy smart home context aware automation smartphone built in modules embedded sensing techniques decision support dynamic varying context information computing service semantic distance based rule matching method context aware service decision making;sensors;context context aware services sensors semantics context modeling smart homes computational modeling;semantics;q science general;computational modeling;context aware automation;rule generation smart home context aware automation decision support service customization;rule generation;semantic networks decision making decision support systems embedded systems knowledge acquisition mobile computing rough set theory;context modeling;context;smart homes;context aware services	The continuous technical progress of the smartphone built-in modules and embedded sensing techniques has created chances for context-aware automation and decision support in home environments. Studies in this area mainly focus on feasibility demonstrations of the emerging techniques and system architecture design that are applicable to the different use cases. It lacks service customization strategies tailoring the computing service to proactively satisfy users' expectations. This investigation aims to chart the challenges to take advantage of the dynamic varying context information, and provide solutions to customize the computing service to the contextual situations. This work presents a rule-based service customization strategy which employs a semantic distance-based rule matching method for context-aware service decision making and a Rough Set Theory-based rule generation method to supervise the service customization. The simulation study reveals the trend of the algorithms in time complexity with the number of rules and context items. A prototype smart home system is implemented based on smartphones and commercially available low-cost sensors and embedded electronics. Results demonstrate the feasibility of the proposed strategy in handling the heterogeneous context for decision making and dealing with history context to discover the underlying rules. It shows great potential in employing the proposed strategy for context-aware automation and decision support in smart home applications.	algorithm;decision support system;embedded system;home automation;logic programming;prototype;rough set;sensor;set theory;simulation;smartphone;systems architecture;time complexity	Z. Meng;Joan Lu	2016	IEEE Transactions on Mobile Computing	10.1109/TMC.2015.2424427	computer science;knowledge management;sensor;data mining;database;semantics;context model;computational model	HCI	-41.35567979770525	12.503723589432159	93502
684edbe0b1f533ff7143583d3638cf8af61454ab	simplified sparql rest api - crud on json object graphs via uri paths		Within the Semantic Web community, SPARQL is one of the predominant languages to query and update RDF knowledge. However, the complexity of SPARQL, the underlying graph structure and various encodings are common sources of confusion for Semantic Web novices. In this paper we present a general purpose approach to convert any given SPARQL endpoint into a simple to use REST API. To lower the initial hurdle, we represent the underlying graph as an interlinked view of nested JSON objects that can be traversed by the API path.	communication endpoint;create, read, update and delete;directed graph;json;sparql;semantic web;uniform resource identifier	Markus Schröder;Jörn Hees;Ansgar Bernardi;Daniel Ewert;Peter Klotz;Steffen Stadtmüller	2018		10.1007/978-3-319-98192-5_8	database;confusion;rdf;sparql;computer science;semantic web;json;graph	Web+IR	-34.88268732552606	6.122433219743043	93555
d83e73ba8db9b242057cd8d080c5016b8d82c5b6	an aspect-oriented extension to the owl api		Aspect-Oriented Programming (AOP) is a technology for the decomposition of software systems based on cross-cutting concerns. As shown in our previous work, cross-cutting concerns are also present in ontologies, and Aspect-Oriented Ontology Development (AOOD) can be used for flexible and dynamic ontology modularization based on functional and non-functional requirements. When ontologies are used in applications, application and ontology-related requirements often coincide. In this paper, we show that aspects in ontologies can be expressed as software aspects and directly referred to from software code using the well-known AspectJ language and Java annotations. We present an extension of the well-known OWL API with aspect-oriented means that allow transparent access to and manipulation of ontology modules that are based on requirements.a	application programming interface;aspect-oriented programming;aspectj;cross-cutting concern;declaration (computer programming);functional requirement;java annotation;microsoft outlook for mac;non-functional requirement;ontology (information science);ontology modularization;pointcut;sparql;semantic web;software system;web ontology language;web application;whole earth 'lectronic link	Ralph Schäfermeier;Lidia Krus;Adrian Paschke	2015		10.5220/0005591601870194	bioinformatics;ontology;programming language;world wide web;owl-s	SE	-41.6675837183663	13.157785358923773	93565
0b0d3606075da4d2c9eafa9b88958aa9dab32063	an agents and artifacts metamodel based e-learning model to search learning resources		In this paper, an e-learning model based on Agents and Artifacts (Au0026A) Metamodel to search learning resources from multiple sources is proposed. Multi agent system (MAS) based e-learning models with the same functionality are available in the literature. However, they are mostly developed as standalone systems that contain a single agent responsible for searching and retrieving learning resources. With the highly distributed nature of learning resources over multiple repositories, giving this responsibility to only one agent decreases scalability. The proposed model exploits the Au0026A Metamodel to overcome this issue. Au0026A Metamodel focuses on environment modeling in MAS design and models entities in the environment as artifacts, that are first class entities like agents. From the perspective of MAS based e-learning systems, learning resources are the main components in the environment that agents interact with. Thus, an efficient solution can be achieved with an e-learning model that searches learning objects by using an e-learning environment model based on Au0026A Metamodel. The proposed e-learning system is developed with Jason and the e-learning environment model is implemented with CArtAgO framework. Finally, current limitations and future directions of the proposed approach are discussed.	metamodeling	Birol Ciloglugil;Mustafa Murat Inceoglu	2017		10.1007/978-3-319-62392-4_40	computer science;scalability;metamodeling;first class;exploit;machine learning;multi-agent system;artificial intelligence	NLP	-45.08886260262654	11.467356230933314	93675
654e30538c07df8929fbb2c7bf22337f0dd397b9	enio: an enterprise application integration ontology	human produced summary;summary document similarity;text information retrieval summary document similarity notary domain legal document handwritten summary human produced summary;law legal factors information retrieval information analysis databases expert systems data mining information systems informatics information processing;information retrieval;legal document;handwritten summary;text analysis;text analysis information retrieval;text information retrieval;notary domain;document similarity	If we try to increase the level of automation in enterprise application integration (EAI) scenarios, we confront challenges related to the resolution of data heterogeneities, service discovery and process composition. In this paper, we propose the enterprise interoperability ontology (ENIO) that provides a shared, common understanding of data, services and processes within enterprise application integration scenarios. ENIO consists of an upper EAI ontology, which is based on the DOLCE-SUMO alignment, with extensions called facets that cover several dimensions of the EAI domain. Each facet contains a relative meta-model that utilizes widely adopted standards.	enterprise application integration;enterprise interoperability;enterprise software;metamodeling;service discovery	Athanasios Bouras;Panagiotis Gouvas;Gregoris Mentzas	2007	18th International Workshop on Database and Expert Systems Applications (DEXA 2007)	10.1109/DEXA.2007.25	document retrieval;text mining;relevance;document clustering;computer science;data science;data mining;vector space model;information retrieval	DB	-43.05363242492339	6.03625819691863	93821
48b89c7cc44c2d39b067ced83261b1d422ab6048	sports: semantic + portal + service	web service	Ontology-based web portal generation and management is an active field of research and development. Recently, many systems have been developed. However, many of them lack the integration of web services which may provide more dynamic information and richer functionalities. In this paper, we describe SPortS, an OWL-DL based portal generation system that integrates semantic web services into generated portals at the presentation level, the data level and the function level. We present the portal generation process, the query decomposition and the service recommendation algorithms through which semantic web services are integrated tightly into the generated portal.	algorithm;portals;semantic web service;web ontology language	Chenxi Lin;Lei Zhang;Jian Zhou;Yin Yang;Yong Yu	2004			web modeling;computer science;web standards;social semantic web;data web;world wide web;semantic analytics;database;semantic web stack;ws-policy;enterprise portal	HPC	-42.57536028190427	9.589609266597025	93862
996b4dda059867e396a01d3d7046c2968e9fba38	xeta: extensible metadata system - for extensibility, accuracy, suitability and convenience		This paper presents an eXtensible mETAdata system (XETA system) which makes it possible for the enduser to organize and extend the structure of metadata. We discuss four requirements of the flexible metadata system in semantic web and a methodology to implement the requirements. Using the XETA system, the end-user can flexibly extend metadata, enhance its semantic accuracy and selectively apply the metadata in context. The main purpose of XETA system provides end-users with a way to construct metadata in bottomup, not force them to accept fixed form and fixed meanings for metadata.	bottom-up parsing;extensibility;requirement;semantic web;top-down and bottom-up design	Yeojin Kim;SukBong Lee;SungJun Lee;SangGyoo Sim	2008			computer science;operating system;database;world wide web	OS	-40.47867112230101	9.077446894374685	93897
26042aeb1e2af6c374fbd931ab5be7be327c015a	build your mashup with web services	mashups;feeds;web service;web services;xml;service oriented architecture;mashups web services xml feeds service oriented architecture	Mashup is presenting new kind of application in web 2.0 world. Mashup is not simply about the AJAX technologies, rather, it is typically related to reuse the data and other services from other web side and web applications. There are many ways to build up the mashup. This half-day tutorial will focus on using XML and JSON format of data and service and will introduce the following to the participants.	ajax (programming);json;mashup (web application hybrid);web 2.0;web application;web service;xml	Ning Yan	2007	IEEE International Conference on Services Computing (SCC 2007)	10.1109/ICWS.2007.61	web service;ajax;web application security;web development;web application;web modeling;data web;web mapping;web-based simulation;web design;web standards;computer science;web api;ws-policy;service-oriented architecture;web navigation;web page;ws-addressing;database;internet privacy;web intelligence;web 2.0;law;world wide web;mashup	DB	-39.76770678099658	10.076989681359487	93914
6ba62f43564bff6da082a3e955cc9c02fc411b40	sparqlist: markdown-based highly configurable rest api hosting server for sparql		SPARQList is a REST API server which executes a SPARQL query, transform the result into formatted data if defined, and then send it back to web client application. In SPARQList, the configuration of API is written in the Markdown format in which parameters of the API, SPARQL endpoints and SPARQL queries, and JavaScript functions for data transformation are defined along with a free text documentation. Each SPARQList server instance can host multiple API configurations, therefore, each service can also be considered as a repository of reusable SPARQL queries with documentation. The source code of the SPARQList is freely available at https://github.com/dbcls/sparqlist and can be easily deployed as it is implemented in the Node.js.	application programming interface;client (computing);documentation;javascript;node.js;representational state transfer;sparql;server (computing)	Toshiaki Katayama;Shuichi Kawashima	2017			database;markdown;sparql;computer science	OS	-39.89261571003535	11.109372474786671	93927
887485571829af5f674bf5d751aaeb84670b2c86	on the role of context and subjectivity on scientific information systems	knowledge management;semantic web;provenance;subjectivity;ontology;context	The explicit representation of context and subjectivity enables an information system to support multiple interpretations of the data it records. This is a crucial aspect of learning and innovation within scientific information systems. We present an ontology-based framework for context and subjectivity that integrates two lines of research: data provenance and ontological foundations of the Semantic Web. Data provenance provides a set of constructs for representing data history. We extend the definition of these constructs in order to describe multiple viewpoints or interpretations held within a domain. The W7 model, the Toulmin model, and the Proof Markup Language (PML) provide the Interlingua for creating multiple viewpoints of data in a machine-readable and sharable form. Example use cases in space sciences are used to demonstrate the feasibility and value of our approach.	information systems	Thomas William Narock;Victoria Y. Yoon;Salvatore T. March	2012	CAIS		computer science;knowledge management;semantic web;ontology;data mining;database;subjectivity;world wide web	HPC	-38.78813475423555	5.455125386040863	93929
c8d4436efbfba1025130daad072363d08da35110	added value in the context of research information systems	added value;micro metadata;macro metadata;cris;data model;cerif	Purpose – The purpose of this paper is to discuss added value in the context of current research information systems (CRISs) based on metadata enrichment. Design/methodology/approach – This discussion paper uses literature review as well as analysis of CRISs specifications to discuss added value possibilities. Findings – Added value of the CRISs is in their integration and interoperability with the same and similar information systems. Since metadata plays key roles in interoperability of information systems, therefore focussing on metadata-related issue can add considerable values to CRISs. Two types of metadata can be distinguished in every CRISs including macroand micro-metadata. In terms of macro-metadata common European research information format (CERIF) by itself is an added value for CRIS because it draws a complete view of the research landscape including entities and their relations. CERIF metadata structure is designed in such a way that supports microand macro-metadata. Originality/value – There is a lack of literature on adding value to research information systems especially CRIS and particularly how value can be added in CRISs still is an unanswered question. CRIS developers can use this paper as a road map to choose the most valuable strategy for adding value to their systems.	content format;data model;etrax cris;entity;eurocris;gene ontology term enrichment;green paper;information needs;information system;interoperability;koopmans' theorem;nl (complexity);no symbol	Majid Nabavi;Keith G. Jeffery;M. R. Jamali HamidR.Jamali	2016	Program	10.1108/PROG-10-2015-0067	computer science;data mining;database;world wide web;metadata repository	HCI	-42.80860717124873	5.791540390045315	93999
614308b70413575b0b3214993b624ea9dd85722a	an approach for ontology-enhanced query refinement in information portals	portals;bibliographic database;free text based querying ontology enhanced query refinement information portal information content information need bibliographic database information retrieval;query processing;information needs portals information retrieval query processing bibliographic systems;information retrieval;query refinement;information needs;information content;domain knowledge;ontologies portals information retrieval navigation databases information resources taxonomy collaboration content based retrieval refining;bibliographic systems;information need	We present an approach that uses domain knowledge in order to support of queries posted to an information portal. This approach enables a user to navigate through the information content incrementally and interactively. In each refinement step a user is provided with a complete but minimal set of refinements, which enables him to develop/express his information need in a step-by-step fashion. In a case study regarding searching a bibliographic database we demonstrate the benefits of using our approach in the traditional information retrieval tasks, especially the combination of the free-text based querying and the ontology-based query refinement.	bibliographic database;information needs;information retrieval;interactivity;portals;refinement (computing);self-information;text-based (computing)	Nenad Stojanovic	2004	16th IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2004.25	information needs;query expansion;ranking;relevance;cognitive models of information retrieval;computer science;information filtering system;concept search;data mining;database;information quality;information retrieval;query language;human–computer information retrieval	Robotics	-34.64501745100462	4.235328866098477	94159
30e5f54978a812533a739034ba4a0b7ff6432338	mapping of core components based e-business standards into ontology	ontology mapping;core components;universal business language;ubl;ontology development;cii;web ontology language;e business standards	A mapping of Core Components specification based e-business standards to an ontology is presented. The Web Ontology Language (OWL) is used for ontology development. In order to preserve the existing hierarchy of the standards, an emphasis is put on the mapping of Core Components elements to specific constructs in OWL. The main purpose of developing an e-business standards’ ontology is to create a foundation for an automated mapping system that would be able to convert concepts from various standards in an independent fashion. The practical applicability and verification of the presented mappings is tested on the mapping of Universal Business Language version 2.0 and Cross Industry Invoice version 2.0 to OWL.	cartography;electronic business;ontology (information science);web ontology language	Ivan Magdalenic;Boris Vrdoljak;Markus Schatten	2010		10.1007/978-3-642-16552-8_10	upper ontology;open biomedical ontologies;ontology alignment;semantic integration;ontology components;bibliographic ontology;ontology inference layer;computer science;ontology;artificial intelligence;data mining;database;ontology-based data integration;web ontology language;world wide web;owl-s;process ontology;suggested upper merged ontology	Web+IR	-42.250425994993876	5.060882567535715	94189
9edbe788e0b9aacd55aef41b455dfc50c0a6b1c6	behavioral analysis of scientific workflows with semantic information		The recent development in scientific computing related areas has shown an increasing interest in scientific workflows because of their abilities to solve complex challenges. Problems and challenges that were too heavy or time consuming can be solved now in a more efficient manner. Scientific workflows have been progressively improved by means of the introduction of new paradigms and technologies, being the semantic area one of the most promising ones. This paper focuses on the addition of semantic Web techniques to the scientific workflow area, which facilitates the integration of network-based solutions. On the other hand, a model checking technique to study the workflow behavior prior to its execution is also described. Using the unary resource description framework annotated Petri net formalism, scientific workflows can be improved by adding semantic annotations related to the task descriptions and workflow evolution. This technique can be applied using a complete environment for the model checking of this kind of workflows that is also shown in this paper. Finally, the proposed methodology is exemplified by its application to a couple of known scientific workflows: the first provenance challenge and the InterScan protein analysis workflow.		Javier Fabra;Mar&#x00ED;a Jos&#x00E9; Ib&#x00E1;&#x00F1;ez;Pedro &#x00E1;lvarez;Joaqu&#x00ED;n Ezpeleta	2018	IEEE Access	10.1109/ACCESS.2018.2878043	model checking;software engineering;task analysis;rdf;petri net;computer science;distributed computing;semantics;semantic web;unary operation;workflow	HPC	-40.5582402014767	13.728871691133062	94287
9cfdaa919240016ad8ad23ddb07e7828bbe2f8d0	xtopo: an xml-based topology for information highway on the internet	data conversion;topology;data dependencies;relational database;information highway;xml document	The need for interoperation and data exchange through the Internet makes XML a dominant standard language. Much work has been done on translating relational data into XML documents and vice versa. However, there is not an integrated method to combine them together as one technology for mass information transmission on the Internet. Furthermore, most XMLenabled database management systems can only translate a few relations into an XML document without data semantics constraints consideration, which is not sufficient for information highway on the Web. This paper presents a methodology, XTOPO, to transmit a relational database on the Internet using XML document as medium. XTOPO facilitates the translation from relational database to XML document and vice versa. XTOPO divides an XML document hierarchical structure into four different topologies—single sub-element (element, sub-element), multiple sub-elements (element, multiple sub-elements), group (element, group of subelements), and referral element (element, element—and captures their semantics into classification tables as a knowledge-based repository. The view of a sender company’s information in a relational database is mapped into four topological XML documents according to their data semantics constraints. The intermediate XML documents are integrated into an XML document using DOM (Document Object Model). The XML document instances are loaded into a relational database with generated Object Identity as key. The receiver company translates the XML document back to relational database for processing. The result is a mechanism of mass information transmission on the Internet for an information highway.	information superhighway;internet;interoperation;relational database;world wide web;xml schema	Joseph Fong;Hing Kwok Wong	2004	J. Database Manag.	10.4018/jdm.2004070102	xml;data conversion;relational database;computer science;data mining;database;world wide web	DB	-35.992782202036615	7.4894304440703054	94399
00e45611eb5105707b7b73eca898c11909c8996e	querying xml using structures and keywords in timber	text;query processing;xml;native xml database;dbms	This demonstration will describe how Timber, a native XML database system, has been extended with the capability to answer XML-style structured queries (e.g., XQuery) with embedded IR-style keyword-based non-boolean conditions. With the original structured query processing engine and the IR extensions built into the system, Timber is well suited for efficiently and effectively processing queries with both structural and textual content constraints.	embedded system;xml database;xquery	Cong Yu;H. V. Jagadish;Dragomir R. Radev	2003		10.1145/860435.860554	xml catalog;xml validation;xml encryption;xml base;xml;streaming xml;computer science;document structure description;xml framework;xml database;xml schema;database;xml signature;world wide web;xml schema editor;information retrieval;efficient xml interchange	DB	-33.918163012664586	6.616662376988589	94526
dd60c7139eaa5619eed7f26feaa0ce7ad67e41c0	semantic service integration support for web portal	system development cost semantic service integration web portal parameter passing service composition service ontology semiautomated source code generation;portals;service composition;knowledge and ontology aggregation and ranking;knowledge provenance;ontologies artificial intelligence;semantic web ontologies artificial intelligence portals;web portal;semantic web;system development;source code;ontology merging;portals ontologies books costs web and internet services filtering contracts permission ieee services web services;service integration	In this paper, we propose an ontology-based framework that integrates existing web applications into web portals. The framework helps Web portal developers to discover, evaluate, and execute semantically composed services on the Web. Parameter passings on service composition are associated using service ontology. Semi-automated source code generation reduces the cost of system development.We show the proposed technologies are useful for constructing statically composed service integration system for current Web world.	automatic programming;code generation (compiler);portals;service composability principle;web application;world wide web	Naoki Fukuta;Tetsuya Osawa;Tadashi Iijima;Takahira Yamaguchi	2005	The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05)	10.1109/WI.2005.131	web service;web application security;web development;web modeling;data web;web mapping;web design;web standards;computer science;artificial intelligence;ws-policy;service-oriented architecture;semantic web;web navigation;social semantic web;web page;semantic web stack;database;web intelligence;web 2.0;world wide web;owl-s;information retrieval;mashup;source code;web coverage service	Web+IR	-43.95745759668824	13.27492290496312	94671
b30f1bb943c0c078de6ea4f6e3ef563ad8ab452f	towards a common sense base in portuguese for the linked open data cloud	linguistic resource;web semantic;linked open data;portuguese language	The Linked Open Data (LOD) cloud is a promising reality since the major content producers are offering their data on an open and linked network, through RDF (Resource Description Framework), with the aim of providing Semantic Web applications with a single global database for retrieval of related content and to perform inferences over the network. However, bases with Portuguese-language content are still incipient. In this paper we present the process of inclusion of the InferenceNet --- the first resource with common sense and inferentialist knowledge in Portuguese language --- on the LOD. Our main goal is to leverage the use and development of Semantic Web applications by content producers in Portuguese language. We develop and evaluated a platform, called SemWidgets, for the creation and execution of widgets able to access and reason over InferenceNet and the open linked data, like DBPedia, Yago, and Article Search API of the New York Times.		Vladia Pinheiro;Vasco Furtado;Tarcisio H. C. Pequeno;Caio Ferreira	2012		10.1007/978-3-642-28885-2_15	engineering;linked data;data mining;database;world wide web	NLP	-40.02474240977098	7.133873648806829	94834
e52121070bc051d5faccd310db6716faac8c0e62	a web-based collaborative environment based on a shared ontology for the maintenance of steam turbines	plugin datagenie web based collaborative environment shared ontology steam turbines maintenance customized white board video conferencing steam turbine components database protege ontology editor;manuals;mechanical engineering computing;collaboration ontologies turbines application software video sharing videoconference knowledge management collaborative work management information systems computer industry;collaborative work;videoconference;application software;knowledge management;collaboration;construction industry;web based collaborative environment;maintenance engineering;collaborative system;computer industry;steam turbines maintenance engineering mechanical engineering computing ontologies artificial intelligence;system on a chip;plugin datagenie;automatic generation;ontologies artificial intelligence;steam turbine components database;collaborative environment;video conferencing;steam turbines maintenance;data structures;collaborative systems;steam turbine ontologies collaborative systems;video sharing;protege ontology editor;management information systems;ontologies;steam turbine;steam turbines;customized white board;shared workspace;turbines;shared ontology	This paper describes the constituents, as well as, the general layout of a collaborative environment dedicated to the maintenance of steam turbines. The core component of the environment is a shared ontology of the domain. The developed ontology contains information on every component, its relationship with the upper and lower component class, as well as a list of previous failure symptoms and diagnostics. The environment is composed of a number of tools such as: a customized white board, chat, audio and video conferencing, FTP, shared workspace and the shared steam turbine ontology. The ontology is semi-automatically generated from the steam turbine components database via Protege ontology editor using the plugin DataGenie.	ontology (information science);protégé;semiconductor industry;workspace	Tarek Khadir;Mokhtar Sellami	2008	2008 7th Computer Information Systems and Industrial Management Applications	10.1109/CISIM.2008.12	maintenance engineering;computer science;operating system;data mining;database;videoconferencing;steam turbine;world wide web;process ontology;suggested upper merged ontology;collaboration	SE	-45.383173621564694	10.164340510525218	94896
7fff0c469696c29266e1ac27c0b2675915098fb0	mashup feeds: continuous queries over web services	information extraction;continuous query;web service;integration;stream processing	Mashup Feeds is a system that supports integrated web service feeds as continuous queries. We introduce collection-based stream processing semantics to enable information extraction by monitoring source evolution over time.	information extraction;mashup (web application hybrid);stream processing;web feed;web service	Jun'ichi Tatemura;Arsany Sawires;Oliver Po;Songting Chen;K. Selçuk Candan;Divyakant Agrawal;Maria Goveas	2007		10.1145/1247480.1247630	web service;stream processing;computer science;database;world wide web;information extraction;information retrieval;mashup	DB	-39.390883468917245	7.421016025808037	94931
26d487e31a297ec86fac14a129fce254888e2851	design, specification and application	representation level;abstraction method;relevant information;data modeling;unnecessary detail;current research;application level;following dilemma;data abstraction tool	Current research in data modeling is motivated by the following dilemma:  - At the <underline>application</underline> level - being confronted with “slices of reality” - details are perceived that, in general, cannot be represented.  - At the <underline>representation</underline> level - being confronted with “levels of machines” - details are represented that, in general, cannot be perceived.  Abstraction methods cope with that problem by suppressing unnecessary details and by formalizing and structuring the relevant information.	data modeling	Joachim W. Schmidt	1980		10.1145/800227.806917	computer architecture;programming language;language of temporal ordering specification	ML	-34.49236595598787	13.701186082230361	95055
acb30dab262742a034a12010e82bee466b429ebb	an approach for matching schemas of heterogeneous relational databases	relational database	"""AbstrAct: Schema matching is a basic problem in many database application domains, such as data integration. The problem of schema matching can be formulated as follows, """" given two schemas, S i and S j , find the most plausible correspondences between the elements of S i and S j , exploiting all available information, such as the schemas, instance data, and auxiliary sources """" [24]. Given the rapidly increasing number of data sources to integrate and due to database heterogene-ities, manually identifying schema matches is a tedious, time consuming, error-prone, and therefore expensive process. As systems become able to handle more complex databases and applications, their schemas become large, further increasing the number of matches to be performed. Thus, automating this process , which attempts to achieve faster and less labor-intensive, has been one of the main tasks in data integration. However, it is not possible to determine fully automatically the different correspondences between schemas, primarily because of the differing and often not explicated or documented semantics of the schemas. Several solutions in solving the issues of schema matching have been proposed. Nevertheless, these solutions are still limited, as they do not explore most of the available information related to schemas and thus affect the result of integration. This paper presents an approach for matching schemas of heterogeneous relational databases that utilizes most of the information related to schemas, which indirectly explores the implicit semantics of the schemas, that further improves the results of the integration."""	cognitive dimensions of notations;field (computer science);relational database	Yaser Karasneh;Hamidah Ibrahim;Mohamed Othman;Razali Yaakob	2010	JDIM		data mining;database application;information retrieval;schema migration;semantics;relational database;data integration;schema matching;schema (psychology);computer science	DB	-35.92302487592644	6.243377097351517	95080
8d16057cd751a299d9e53769686506e52dcb43e4	advances in gml for geospatial applications	computadora;tratamiento datos;computers;geospatial data storage;base donnee;geospatial data processing;mobile device;ordinateur;base dato;data processing;traitement donnee;geography markup language;stockage donnee;data bases;web service;geospatial database;modeling language;data storage;echantillon reference;xml geospatial database geospatial data storage geospatial data processing;xml;gml;almacenamiento datos;geografia;geographie;standard samples;open standard;roca patron;geography	This paper presents a study of Geography Markup Language (GML), the issues that arise from using GML for spatial applications, including storage, parsing, querying and visualization, as well as the use of GML for mobile devices and web services. GML is a modeling language developed by the Open Geospatial Consortium (OGC) as a medium of uniform geographic data storage and exchange among diverse applications. Many new XML-based languages are being developed as open standards in various areas of application. It would be beneficial to integrate such languages with GML during the developmental stages, taking full advantage of a non-proprietary universal standard. As GML is a relatively new language still in development, data processing techniques need to be refined further in order for GML to become a more efficient medium for geospatial applications.	computer data storage;consortium;geography markup language;mobile device;modeling language;parsing;web service;xml	Chang-Tien Lu;Raimundo F. Dos Santos;Lakshmi N. Sripada;Yufeng Kou	2007	GeoInformatica	10.1007/s10707-006-0013-9	web service;simple features;xml;open standard;data processing;geography markup language;computer science;computer data storage;mobile device;database;modeling language;world wide web;spatial database;information retrieval;web feature service	DB	-33.98146146935255	8.375013971248874	95125
1209eb7fa306e2f03b16f59039bc6975d8b6c48c	facilitating dynamic web service composition with fine-granularity context management	composite web service;fine granularity context management;history;ddl context granularity web services dynamic composition;composition;ddl;web services context aware services artificial intelligence simple object access protocol concrete road transportation technology management laboratories information processing computers;web service;data mining;web service composition;context aware service dynamic web service composition fine granularity context management;context model;context aware service;dynamic composition;cognition;web services;context management;dynamic web service composition;artificial intelligence;ubiquitous computing;web services ubiquitous computing;context granularity;context modeling;dynamic;context;dynamic service composition	Context is an important factor for the success of dynamic service composition. Although many context-based AI or workflow approaches have been proposed to support dynamic service composition, there is still an unaddressed issue of the support of fine-granularity context management. In this paper, we propose a granularity-based context model together with an approach to supporting the intelligent context-aware service composing problem. The corresponding case study is provided to show the validity of our approach.	service composability principle;web service	Wenjia Niu;Gang Li;Xinghua Yang;Xu Han;Zhongzhi Shi	2009	2009 IEEE International Conference on Granular Computing	10.1109/GRC.2009.5255078	web service;computer science;knowledge management;database;context model;world wide web;ubiquitous computing	SE	-44.08072868911703	15.520352002304618	95138
74742112a5d7b10954ab305f810488632ef67dda	orion: p2p-based inter-space context discovery platform	context aware application;sensor phenomena and characterization;context information;ontologies artificial intelligence peer to peer computing query processing table lookup semantic web;query processing;application software;orion;information retrieval;pervasive computing;peer to peer architecture;p2p;matchmaking;data heterogeneity;ontologies artificial intelligence;context model;computer architecture;inter space context discovery platform;semantic web ontology driven approach;semantic web;smart spaces;overlay network;lookup query;ubiquitous computing;ontologies;peer to peer computing;table lookup;intelligent sensors application software semantic web context modeling sensor phenomena and characterization peer to peer computing computer architecture ontologies ubiquitous computing pervasive computing;peer to peer;context modeling;intelligent sensors;smart space;semantic overlay network;matchmaking inter space context discovery platform orion information retrieval overlay network peer to peer architecture p2p smart space lookup query data heterogeneity semantic web ontology driven approach context modeling	Context information produced by various interconnected sensors and software sources needs to be efficiently identified and located by a context-aware application. In this paper, we propose an inter-space context discovery platform, called Orion, which allows context to be discovered and retrieved from multiple smart spaces. Orion employs semantic overlay network peer-to-peer architecture to provide connectivity between the smart spaces, allowing lookup query to be forwarded to the destined spaces. On the other hand, to overcome data heterogeneity, semantic Web ontology-driven approach is used for the context modeling and matchmaking.	lookup table;overhead (computing);overlay network;peer-to-peer;semantic web;sensor;simulation	Chung-Yau Chin;Daqing Zhang;Gurusamy Mohan	2005	The Second Annual International Conference on Mobile and Ubiquitous Systems: Networking and Services	10.1109/MOBIQUITOUS.2005.45	human–computer interaction;computer science;database;context model;world wide web;ubiquitous computing;information retrieval	Mobile	-42.65819238701348	11.542223400980918	95324
5dfbb4cba6f3c4eebac6971993ed7fbc2e716d38	towards extensible information brokers based on xml	gestion informacion;multiagent system;navegacion informacion;red www;interoperabilite;navigation information;integration information;information browsing;information integration;information management;integracion informacion;world wide web;reseau www;exponential growth;interoperability;information system;gestion information;sistema multiagente;systeme information;systeme multiagent;sistema informacion	The exponential growth in the number and size of information services available on the internet has created an urgent need for information agents which act as brokers in that they can autonomously search, gather and integrate information on behalf of a user. Moreover, the inherent volatility of the internet and the wide range of information processing tasks to be carried out, calls for a framework that facilitates both the construction and evolution of such information brokers. This paper proposes such a framework named XIB (eXtensible Information Brokers). Based on descriptions of relevant information services, XIB supports the interactive generation of an integrated query interface, generates wrappers for each information service dynamically, and returns to the user the composed result to a query. XIB depends heavily on XML-related techniques. More speciically, we will use DTDs to model the input and output of the service, use XML elements to denote the input and output values. By using this representation, service integration is investigated in the form of DTD integration, and query decomposition is studied in the form of XML element decomposition. Within the proposed framework, it is easy to add or remove information services on the internet to a broker, thereby facilitating maintenance, evolution and customization.	information processing;information retrieval;input/output;time complexity;volatility;xml	Jianguo Lu;John Mylopoulos;Jamie Ho	2000		10.1007/3-540-45140-4_4	interoperability;exponential growth;computer science;information integration;operating system;data mining;database;distributed computing;information management;world wide web;computer security;information system	DB	-38.48181685977064	13.629008466893945	95349
86323d4fc8e24b67053d0a8b606871f0571a4246	distributed transaction management in a peer-to-peer process-oriented environment	distributed system;commerce electronique;tratamiento transaccion;reseau pair;electronic commerce;systeme reparti;comercio electronico;cognitive impairment;distributed transactions;point of presence;peer to peer p2p;social policy;sistema repartido;transaction processing;peer to peer;user interaction;digital divide;domain specificity;electronic trade;traitement transaction	As electronic commerce continues to permeate every aspect of our society, a significant segment of the population is being marginalized due to issues of accessibility. This is commonly referred to as the “Digital Divide” [4] Social policy initiatives are attempting to increase accessibility through infrastructure enhancements and increased public points of presence. While this is helpful, it does little to address accessibility issues for persons who are functionally illiterate or cognitively impaired. Computerized systems will remain inaccessible to this population until the complexity of interacting with these systems is significantly reduced. The Knowledge-Acquiring Layered Infrastructure (KALI) project at Dalhousie University is attempting to reduce interaction complexity for this population through domain-specific personalization techniques that customize end-user interactions with computerized systems based on the abilities, preferences, and needs of individual end-users.	distributed transaction;peer-to-peer	Theodore Chiasson;Michael McAllister;Jacob Slonim	2002		10.1007/3-540-36261-4_17	commit;digital divide;database transaction;transaction processing;distributed transaction;computer science;point of presence;database;distributed computing;online transaction processing;serializability;world wide web;computer security;acid;social policy;transaction processing system	DB	-38.71288237160808	15.517169991594576	95355
ce03c0df4cda621b52828d9f65f15beff2b22460	trust-annotated ontology integration using social modelling	trust;trust revision;ontology integration;ontology	Abstract: As ontologies become more prevalent for information management the need to manage the ontologies increases. Multiple organizations, within a domain, often combine to work on specific projects. When separate organizations come together to communicate, an alignment of terminology and semantics is required. Ontology creation is often privatized for these individual organizations to represent their view of the domain. This creates problems with alignment and integration, making it necessary to consider how much each ontology should influence the current decision to be made. To assist with determining influence a trust-based approach on authors and their ontologies provides a mechanism for ranking reasoning results. A representation of authors and the individual resources they provide for the merged ontology becomes necessary. The authors are then weighted by trust and trust for the resources they provide the ontology is calculated. This is then used to assist the integration process allowing for an evolutionary trust model to calculate the level of credibility of resources. Once the integration is complete semantic agreement between ontologies allows for the revision of the authors' trust.		Dennis Hooijmaijers;Markus Stumptner	2008	Expert Systems	10.1111/j.1468-0394.2008.00458.x	upper ontology;ontology alignment;bibliographic ontology;computer science;knowledge management;ontology;ontology;data mining;ontology-based data integration;trustworthy computing;information retrieval;process ontology;suggested upper merged ontology	AI	-43.41778068055839	7.1221656928255115	95366
a40236af7acf46abfd73107cd09aa04c2cc2ed9c	ontology management and evolution for business intelligence	information management system;etude utilisation;gestion informacion;ontologie;ontology evolution;estudio utilizacion;heterogeneous data;intelligence economique;information management;life sciences;ontology management;competitive intelligence;ontologia;business intelligence;gestion information;inteligencia economica;ontology;use study;knowledge base	The amount of heterogeneous data that is available to organizations nowadays has made information management a seriously complicated task, yet crucial since this data can be a valuable asset for business intelligence. Ontologies can act as a semantically rich knowledge base in systems that specialize in information management. The present work investigates the potential of ontologies in supporting the information lifecycle within a corporate environment for business intelligence. The paper demonstrates the use of Heraclitus II, a framework that employs ontology management and evolution in the context of information management systems. The capabilities of the framework in facilitating information management and business intelligence are evaluated through a real-life case study from the life sciences	information management;knowledge base;ontology (information science);real life	Alexander Mikroyannidis;Babis Theodoulidis	2010	Int J. Information Management	10.1016/j.ijinfomgt.2009.10.002	knowledge base;information technology management;competitive intelligence;data management;computer science;engineering;knowledge management;electrical engineering;data science;ontology;personal information management;data mining;process management;business intelligence;information management;business relationship management;information system;business activity monitoring	DB	-47.52552531690313	8.44633856860869	95376
6a0d4a54e8d3af9f98bcdb522e196a21bb99f55b	a task ontology driven approach for live geoprocessing in a service-oriented environment	null	Abstract#R##N##R##N#In a service-oriented environment, Web geoprocessing services can provide geoprocessing functions for a variety of applications including Sensor Web. Connecting Sensor Web and geoprocessing services together shows great potentail to support live geoprocessing using real-time data inputs. This article proposes a task ontology driven approach to live geoprocessing. The task in the ontology contains five aspects: task type, task priority, task constraints, task model, and task process. The use of the task ontology in driving live geoprocessing includes the following steps: (1) Task model generation, which generates a concrete process model to fulfill user demands; (2) Process model instantiation, which transforms the process model into an executable workflow; (3) Workflow execution: the workflow engine executes the workflow to generate value-added data products using Sensor Web data as inputs. The approach not only helps create semantically correct connections between Sensor Web and Web geoprocessing services, but also provides sharable problem solving knowledge using process models. A prototype system, which leverages Web 2.0, Sensor Web, Semantic Web, and geoprocessing services, is developed to demonstrate the applicability of the approach.	geoprocessing;service-oriented device architecture	Ziheng Sun;Peng Yue;Xianchang Lu;Xi Zhai;Lei Hu	2012	Trans. GIS	10.1111/j.1467-9671.2012.01364.x	computer science;data mining;database;world wide web;geoprocessing	Security	-43.80886907421026	13.886478112573368	95531
9aed2b3c2015ecab7272accc13b7f243067c578b	a semantic matching approach for mediating heterogeneous sources	pervasive computing;p2p networks;semantic matching	Approaches to make multiple sources interoperable were essentially investigated when one are able to resolve a priori the heterogeneity problems. This requires that a global schema must be elaborat ed or that mappings between local schemas must be established before any query can be posed. The object of this paper is to study to what extend a mediation a pproach can be envisaged when none of these features are a priori available. Our solution consists in matching a query with each of the local schema. We designed a first prototype which showed that the approach could be efficient. We propose in this paper a new more sophisticated prototype. A friendlier quer y language is available. The detection of matching is more successful. This kind of system can be installed on super-nodes in P2P networks in order to facilita te accesses to data by their semantics. It can thus contribute to the pervasive computing paradigm.	interoperability;peer-to-peer;programming paradigm;prototype;semantic matching;ubiquitous computing	Michel Schneider;Lotfi Bejaoui;Guillaume Bertin	2007		10.1007/978-0-387-77745-0_53	semantic computing;semantic grid;computer science;ubiquitous computing	DB	-37.57777552401236	9.751637827600414	95603
b188adc94edae8f2955e5c83df57dd802db4bf97	data exchange strategy for manufacturing simulation of shop floor information systems	databases;data transmission;intercambio informacion;extensible markup language;nist;information model;interfaces;xml language;manufacturing simulation;production system;simulation;database;systeme production;base dato;shopfloor information systems;information models;data exchange;pregunta documental;simulacion;sistema produccion;simulator;data interface;algorithme;algorithm;simulador;scheduling;echange information;information exchange;transmission donnee;structured query;manufacturing;xml;base de donnees;simulateur;query;validation;information system;systeme information;langage xml;lenguaje xml;ordonnancement;transmision datos;data transfer;requete;reglamento;algoritmo;sistema informacion	Simulation is defined as the imitation of the operation of a system or real-world process over time, and in many cases, manufacturing provides one of the most important applications of simulation (Zolfaghari and Roa, 2006). Standard interfaces could make information sharing effective, and hence promote the utilization of simulators. An information model (McLean et al., 2005), which represents machine shop data and facilitates data sharing among machine shop’s manufacturing execution system, scheduling system, and simulation system, has been developed at the National Institute of Standards and Technology (NIST). This paper briefs the machine shop information model. This paper discusses information exchange, using NIST’s information model, between different representations and presents an algorithm to exchange data between a database system and an eXtensible Markup Language (XML) [1] document. The algorithm has been built based on Document Object Model (DOM), XML Path Language (XPath), and Open Database Connectivity Database Engine (ODBC). The paper also describes interfaces for XML schema’s validation, structured query, and data transfer.	algorithm;angela mclean (biologist);database engine;document object model;information exchange;information model;information system;manufacturing execution system;markup language;open database connectivity;scheduling (computing);simulation;xml schema;xpath	Yan Luo;Y. Tina Lee	2009	IJRFITA	10.1504/IJRFITA.2009.025153	data exchange;embedded system;xml;simulation;computer science;operating system;data mining;database;world wide web	DB	-35.91798779417734	12.216858731885951	95688
fdd7ed81cd31d63cdbfb867a64b6e4923ad2e2ab	intermediate notation for provenance and workflow reproducibility	computer communication networks;operating system;computers and society;information system;information storage and retrieval	We present a technique to capture retrospective provenance across a number of tools in a statistical software suite. Our goal is to facilitate portability of processes between the tools to enhance usability and to support reproducibility. We describe an intermediate notation to aid runtime capture of provenance and demonstrate conversion to an executable and editable workflow. The notation is amenable to conversion to PROV via a template expansion mechanism. We discuss the impact on our system of recording this intermediate notation in terms of runtime performance and also the benefits it brings.	cobham's thesis;dataflow;executable;in-memory database;list of statistical packages;logical disk manager;mathematical optimization;overhead (computing);run time (program lifecycle phase);sql;software portability;software suite;template processor;usability;while	Danius T. Michaelides;Richard Parker;Chris Charlton;William J. Browne;Luc Moreau	2016		10.1007/978-3-319-40593-3_7	computer science;theoretical computer science;operating system;database;programming language;world wide web;information system	PL	-39.66496762232053	11.520675623661356	95781
c0ad01ef3365d8058c50119c8d96d1ff4127f751	viqie: a new approach for visual query interpretation and extraction		Web services are accessed via query interfaces which hide databases containing thousands of relevant information. User’s side, distant database is a black box which accepts query and returns results, there is no way to access database schema which reflect data and query meanings. Hence, web services are very autonomous. Users view this autonomy as a major drawback because they need often to combine query capabilities of many web services at the same time. In this work, we will present a new approach which allows users to benefit of query capabilities of many web services while respecting autonomy of each service. This solution is a new contribution in Information Retrieval research axe and has proven good performances on two standard datasets. KeywordsInformation Retrieval ; Model of Query Representation; Query Extraction	autonomous robot;autonomy;black box;content-based image retrieval;database schema;information retrieval;performance;query string;web service	Radhouane Boughammoura;Lobna Hlaoua;Mohamed Nazih Omri	2012	CoRR		online aggregation;sargable;query optimization;query expansion;web query classification;ranking;computer science;query by example;data mining;database;rdf query language;web search query;view;range query;world wide web;information retrieval;query language	DB	-35.446836792543024	5.831557840826526	95837
5b041e10a8dffaf5607a134f6f923bbcb6c69956	overview and outlook on the semantic desktop	building block;knowledge management;software architecture	In this paper we will give an overview of the Semantic Desktop paradigm, beginning with the history of the term, a definition, current work and its relevance to knowledge management of the future. Existing applications and research results are listed and their role as building blocks of the future Semantic Desktop described. Based on the analysis of existing systems we propose two software architecture paradigms, one for the Semantic Desktop at large and another for applications running on a Semantic Desktop. A view on the context aspect of the Semantic Desktop and the Knowledge Management aspect is given. Based on the current events and projects, we give an outlook on the next steps.	desktop metaphor;knowledge management;microsoft outlook for mac;programming paradigm;relevance;semantic desktop;software architecture	Leo Sauermann;Ansgar Bernardi;Andreas Dengel	2005			semantic computing;haystack;desktop management interface;computer science;knowledge management;database;semantic technology;world wide web	Web+IR	-41.83471482432217	7.2184822930282575	95986
7b21689bbe5ada68e546096a6010c888a06a0c57	fast dynamic re-planning of composite owl-s services	resource selection;service composition;planning process owl s service composition planner quasi online replanning;planning artificial intelligence internet;planning artificial intelligence;semantic web services;internet;dynamic web service composition;constraint satisfaction problem;experimental evaluation;process planning;planning process;owl s service composition planner;quasi online replanning	In this paper, we present an extension of our OWL-S service composition planner OWLS-XPlan that allows for quasi-online re-planning of composite OWL-S services without full restart of the actual planning process, and preliminary experimental evaluation results.	automated planning and scheduling;database;owl-s;scheduling (computing);semantic web;service composability principle;vldb;web service;world wide web	Matthias Klusch;Kai-Uwe Renner	2006	2006 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology Workshops	10.1109/WI-IATW.2006.72	planning;the internet;simulation;computer science;knowledge management;management;law;constraint satisfaction problem	Robotics	-44.07030139541637	14.264430281836889	96049
00963cfb2e3ed1b3a8aabf78fb687d1d54b8c15a	ontoshare: a knowledge management environment for virtual communities of practice	community;community of practice;ontology evolution;knowledge management;virtual community;semantic web technology;information sharing;knowledge sharing;semantic web;knowledge management environment;it evaluation	An ontology-based knowledge sharing system OntoShare and its evaluation as part of a case study is described. RDF(S) is are used to specify and populate an ontology, based on information shared between users in virtual communities. We begin by discussing the advantages that use of Semantic Web technology afford in the area of knowledge management tools. The way in which OntoShare supports WWW-based communities of practice is described. Usage of OntoShare semi-automatically builds an RDF-annotated information resource for the community (and potentially for others also). Observing that in practice the meanings of and relationships between concepts evolve over time, OntoShare supports a degree of ontology evolution based on usage of the system - that is, based on the kinds of information users are sharing and the concepts (ontological classes) to which they assign this information. A case study involving OntoShare was carried out. The evaluation exercise for this case study and its results are described. We conclude by describing avenues of ongoing and future research.	knowledge management;lazy evaluation;population;semantic web;semiconductor industry;virtual community;www	John Davies;Alistair Duke;York Sure-Vetter	2003		10.1145/945645.945652	upper ontology;community;bibliographic ontology;computer science;knowledge management;ontology;semantic web;data mining;personal knowledge management;world wide web	HCI	-45.506715443629595	5.368749808390539	96065
4e1f92001aec753c34b3b831d620f27e3438d6c1	the agent component approach, combining agents, and components	added value;ontologie;multiagent system;belief;multi agent system;customization;personnalisation;base connaissance;composant logiciel;langage dedie;croyance;contexto;software component;agent technology;personalizacion;domain specific language;contexte;base conocimiento;ontologia;valor anadido;creencia;sistema multiagente;ontology;valeur ajoutee;context;systeme multiagent;lenguaje dedicado;knowledge base	In this paper we introduce a new approach, the so-called AgentComponent (AC) approach which combines component and agent technology. A multi agent system (MAS) is composed of AC instances, each AC instance consists of a knowledge base, storing the beliefs of an AC instance, of slots, storing the communication partners of an AC instance, of a set of ontologies, that represent domain specific languages for certain contexts, and of so-called ProcessComponents (PC) representing the behaviours of an AC instance. The AC is a generic component that can be reused (instantiated ACs) and parametrized by customizing the communication partners (slots), the ontologies and the behaviours (PCs) that can be added and removed from any AC instance. Hereby we achieve added value for agents and components. Agents can be easily composed, customized and reused whereas components get enhanced communication and interaction facilities from agents. We present this approach in detail, show how to construct a component-based MAS by a simple example and present a graphical tool for composing systems of AgentComponents.	component-based software engineering;domain-specific language;graphical user interface;knowledge base;multi-agent system;ontology (information science)	Richard Krutisch;Philipp Meier;Martin Wirsing	2003		10.1007/978-3-540-39869-1_1	knowledge base;simulation;computer science;domain-specific language;artificial intelligence;component-based software engineering;operating system;belief;ontology;multi-agent system;database;programming language;algorithm;added value	AI	-39.525017750565226	14.785775095827425	96095
322b3711d86636a474cb49c6e62a8b8552f78692	adaptive context assessment and context management	context data integration sensors data models resource management context modeling adaptive systems;sensor fusion ontologies artificial intelligence;adaptive context assessment and context management methods baseline fusion system sources ontologies organic data fusion systems products dnn fusion dnn technical architecture dual node network technical architecture df rm data fusion and resource management conformity measurement predictive situational models external data sources adaptive cacm methods information fusion systems;model based classification context conformity assessment and management adaptive information exploitation joint directors of laboratories jdl data fusion model machine learning context relevance situation assessment model management	Adaptive context assessment and context management (CACM) methods opportunistically exploit non-traditional data sources to improve the robustness of information fusion systems. Adaptive CACM methods find relevant data in external data sources and create and refine predictive situational models based on the relevance, quality, and means of employing such data. These CACM methods also measure the conformity of this non-traditional data with Level 1-4 fusion system products. The method proposed here is developed as an extension to the Data Fusion and Resource Management (DF&RM) Dual Node Network (DNN) technical architecture by incorporating the CACM into the DNN fusion Level 4. Techniques are described that automatically learn to characterize and search non-traditional contextual data to enable fusion or comparison of data with organic data fusion systems products and ontologies. Non-traditional data can improve the quantity, quality, availability, timeliness, and diversity of the baseline fusion system sources and therefore can improve prediction, estimation accuracy and robustness at all levels of fusion.	baseline (configuration management);conformity;information technology architecture;ontology (information science);relevance	Alan N. Steinberg;Christopher Bowman;Gary Haith;Charles Morefield;Michael Morefield;Erik Blasch	2014	17th International Conference on Information Fusion (FUSION)		engineering;knowledge management;data science;data mining	Robotics	-47.91657893749782	4.314313759946261	96208
1a34b5187c52f1c49e171dc83535f7089bff563f	service mining for trusted service composition in cross-cloud environment	measurement;quality of service measurement mobile communication mobile handsets wireless sensor networks servers educational institutions;servers;trusted service composition cross cloud cyber physical systems cps service mining;mobile communication;mobile handsets;extended top k iteration composition process service mining trusted service composition charismatic storage computation power social networking service location based services cross cloud mobile applications mobile cyber physical systems cps quality of service qos evaluation index;quality of service;trusted computing cloud computing cyber physical systems data mining iterative methods mobile computing quality of service social networking online;wireless sensor networks	Nowadays, with the cloud's charismatic storage and computation power, more and more traditional services (social networking service, location-based services, etc.) are being migrated onto cloud platforms. These cloud services on different cloud platforms could be employed to form cross-cloud mobile applications of mobile cyber-physical systems (CPS). However, a cloud service may have various versions of quality of service (QoS) information revealed in different mobile CPS applications, which is often advertised as the elastic computation power. This characteristic makes it costly and time consuming to mine qualified ones from massive candidate cloud services for developing a mobile CPS application, as a service composition solution may have various evaluated values initiated by the various QoS properties. In view of this challenge, a cloud service selection method, named CSSM, is proposed in this paper. It takes the utility value as the evaluation index and aims at finding optimal or near-optimal trusted service composition solutions from a set of cloud services on users' demands. Technically, the user preference on each QoS metric is formalized as the preference interval for enhancing the fitness of a service composition solution. Furthermore, an extended top- $k$ iteration composition process is performed among cloud services to get an optimal or near-optimal trusted service composition solution. Both theoretical analysis and experimental evaluation are conducted to guarantee the feasibility and efficiency of the CSSM.	cloud computing;computation;cyber-physical system;iteration;location-based service;mobile app;quality of service;service composability principle	Taotao Wu;Wan-Chun Dou;Chunhua Hu;Jinjun Chen	2017	IEEE Systems Journal	10.1109/JSYST.2014.2361841	service level requirement;mobile qos;service catalog;wireless sensor network;quality of service;mobile telephony;differentiated service;computer science;engineering;service delivery framework;operating system;data as a service;world wide web;computer security;server;measurement;computer network;trusted service manager	Metrics	-45.86676575971156	15.376080049959432	96367
80b095b9d901ccf082806af45c48e3022129932c	building scalable mediator systems	service integration;integrated services	This paper deals with PICSEL mediator systems which integrate services. We propose a scalable approach which exploits standardized specifications provided by normalization organisms. The paper focuses on the use of such specifications to automate the construction of the mediator. An illustration in the tourism domain with OTA specifications is given.	exploit (computer security);matchware mediator;scalability;software deployment	Chantal Reynaud	2004		10.1007/978-1-4020-8157-6_4	world wide web;computer security;computer network	SE	-47.76623394752775	13.554409711268788	96832
47e8757f7f9569b2c17c375c07818d3cccbbac92	qos-aware service selection using qdg for b2b collaboration	b2b collaboration;graph theory;analytic hierarchy process;service composition;b2b;service selection;dependence graph;collaboration;ahp qos aware web service selection b2b collaboration qdg quality of service quality dependency graph analytic hierarchy process;swinburne;ahp b2b qos service composition service dependency quality dependency graph;contracts;web services business data processing decision making graph theory quality of service;web service;ahp;service dependency;qos;guidelines;web services collaborative work quality of service collaborative software international collaboration chaos laboratories australia acceleration costs;business data processing;business;web services;qos aware web service selection;organizations;quality of service;similarity function;quality dependency graph;qdg	Collaboration among enterprises through Web service has become a hot topic. Before the collaboration, how to select the most appropriate enterprise to collaborate with, from a set of enterprise candidates that provide similar functions, is an important issue. Existing work focus on proposing evaluation rules, and aggregating these rules to evaluate a service, where subjectiveness is usually involved. In this paper, we propose to utilize ¿serve, be served¿ relationship to evaluate the quality of services. In more detail, we use quality dependency graph (QDG) method model the relationship among enterprises, and then, by traveling the built QDG, an analytic hierarchy process (AHP) model is used to calculate the evaluation result of each candidate organization. Our method provides a more objective way for collaboration on enterprise level.	analytical hierarchy;quality of service;web service	Chao Lv;Wan-Chun Dou;Jinjun Chen	2008	2008 14th IEEE International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2008.10	web service;analytic hierarchy process;quality of service;computer science;knowledge management;graph theory;database;world wide web	DB	-47.77071044795746	16.305610373352614	97060
6e4f653d1b036696a641dab3cc296711e7dbfdda	a semantic framework for integrated asset management in smart oilfields	intuitive user interface semantic framework integrated asset management smart oilfields it enabled transformation reservoir modeling reservoir simulation performance prediction rapid decision making production forecasting domain knowledge formal modeling language;intuitive user interface;it enabled transformation;production forecasting;user interfaces decision making digital simulation petroleum industry production engineering computing specification languages;formal model;asset management;user interface;integrable system;hydrocarbon reservoirs;formal modeling language;reservoir modeling;maintenance engineering;production engineering computing;domain knowledge;asset management computational modeling predictive models grid computing hydrocarbon reservoirs production petroleum maintenance engineering decision making user interfaces;rapid decision making;petroleum;computational modeling;specification languages;smart oilfields;integrated asset management;reservoir simulation;petroleum industry;production;performance prediction;predictive models;grid computing;user interfaces;digital simulation;semantic framework	Integrated asset management (IAM) is the vision of IT- enabled transformation of oilfield operations where information integration from a variety of tools for reservoir modeling, simulation, and performance prediction will lead to rapid decision making for continuous production optimization. This paper describes the design of a model-based IAM system for production forecasting. Domain knowledge is captured through a formal modeling language that forms the basis for an intuitive user interface to the system. An IAM metacatalog captures domain knowledge as well as metadata about computational resources and data sets in a single ontological framework, thereby providing a unified mechanism for application, data, and workflow integration . The framework is designed to be portable across oilfield assets, to allow different classes of end users to interact with the integrated system, and to accomodate new domain knowledge, software applications, data sets, and workflows for IAM.	computational resource;identity management;mathematical optimization;modeling language;ontology (information science);performance prediction;simulation;user interface	Ramakrishna Soma;Amol Bakshi;Viktor K. Prasanna	2007	Seventh IEEE International Symposium on Cluster Computing and the Grid (CCGrid '07)	10.1109/CCGRID.2007.11	maintenance engineering;computer science;knowledge management;operating system;data mining;database;user interface	DB	-34.424553655755425	14.472728379039934	97394
ca0439647cce0f186c24ecbfe47d144a195384d3	efficient schema-based revalidation of xml	xml schema;definicion tipo documento;validacion;xml language;definition type document;xml document;validation;langage xml;lenguaje xml;document type definition	As XML schemas evolve over time or as applications are integrated, it is sometimes necessary to validate an XML document known to conform to one schema with respect to another schema. More generally, XML documents known to conform to a schema may be modified, and then, require validation with respect to another schema. Recently, solutions have been proposed for incremental validation of XML documents. These solutions assume that the initial schema to which a document conforms and the final schema with which it must be validated after modifications are the same. Moreover, they assume that the input document may be preprocessed, which in certain situations, may be computationally and memory intensive. In this paper, we describe how knowledge of conformance to an XML Schema (or DTD) may be used to determine conformance to another XML Schema (or DTD) efficiently. We examine both the situation where an XML document is modified before it is to be revalidated and the situation where it is unmodified.	algorithm;automata theory;compiler;conformance testing;data pre-processing;database schema;experiment;finite-state machine;interaction;preprocessor;programming language;web service;xml namespace;xml schema	Mukund Raghavachari;Oded Shmueli	2004		10.1007/978-3-540-24741-8_37	well-formed document;xml catalog;xml validation;xml encryption;simple api for xml;xml;relax ng;xml schema;streaming xml;computer science;document type definition;xs3p;document definition markup language;document structure description;xml framework;data mining;xml schema;database;document schema definition languages;schematron;xml signature;world wide web;xml schema editor;efficient xml interchange	DB	-36.528975563552756	12.737961103307645	97398
f7bd28d4c908190714ba269985b300729803505f	towards knowledge management based on harnessing collective intelligence on the web	anotacion;bottom up method;consensus;ontologie;multiagent system;bottom up;metodo ascendente;red www;social construction;ingenierie connaissances;sociologia;organizacion social;web semantique;knowledge management;reseau web;semantics;base connaissance;annotation;semantica;semantique;methode ascendante;social network;internet;consenso;web semantica;decouverte connaissance;organisation sociale;social organization;semantic web;world wide web;descubrimiento conocimiento;base conocimiento;ontologia;collective intelligence;sociologie;sistema multiagente;ontology;sociology;systeme multiagent;knowledge base;knowledge discovery;knowledge engineering	The Web has acquired immense value as an active, evolving repository of knowledge. It is now entering a new era, which has been called “Web 2.0”. One of the essential elements of Web 2.0 is harnessing the collective intelligence of Web users. Large groups of people are remarkably intelligent, and are often smarter than the smartest people in them. Knowledge as collective intelligence is socially constructed from the common understandings of people. It works as a filter for selecting highly regarded information with collective annotation based on bottom-up consensus and the unifying force of Web-supported social networks. The rising interest in harnessing the collective intelligence of Web users entails changes in managing the knowledge of individual users. In this paper, we introduce a concept of knowledge management based on harnessing the collective intelligence of Web users, and explore the technical issues involved in implementing it.	collective intelligence;knowledge management;world wide web	Koji Zettsu;Yasushi Kiyoki	2006		10.1007/11891451_31	knowledge base;the internet;consensus;computer science;knowledge management;artificial intelligence;social organization;semantic web;knowledge engineering;ontology;social semantic web;top-down and bottom-up design;semantics;collective intelligence;web intelligence;world wide web;social network	Web+IR	-38.33494888880578	12.313206852923006	97456
75def3b629b257f8a7e259256ca76c39c9f4b399	multi-agent system for semantic web service composition	semantic search engine;web service composition;multi agent systems;semantic web;mase	Agent-oriented analysis and design is a prosperous approach to model and build software systems. On the other hand, semantic web services are software components that have been emerging to enable dynamic service discovery, composition, invocation, and provide services, which can be considered as the main function of an agent. Semantic web service composition is a feature that improves the flexibility of the system. In this paper we propose a multi-agent system for web service composition. We modeled it by MaSE (Multi-agent System Engineering) methodology, which is a top-down approach. Also an implementation of our system is illustrated for semantic search engines. The case study shows that our system is feasible and effective for composition of semantic web services in a distributed network.	multi-agent system;semantic web service	Elham Paikari;Emadoddin Livani;Mohammad Moshirpour;Behrouz Homayoun Far;Günther Ruhe	2011		10.1007/978-3-642-25975-3_27	semantic data model;web service;semantic computing;web development;web modeling;semantic web rule language;data web;web mapping;semantic search;semantic grid;web standards;computer science;artificial intelligence;ws-policy;semantic web;social semantic web;multi-agent system;semantic web stack;database;web intelligence;semantic technology;world wide web;owl-s;information retrieval;semantic analytics	AI	-43.81046775783062	13.29067725368608	97717
40b509ff1d7d1c48de246c9b5b319f19aafc0d69	a new trust model based on social characteristic and reputation mechanism for the semantic web	trust model;reputation mechanism;semantic web;semantic web security of data;world wide web;world wide web trust model social characteristic reputation mechanism semantic web;social characteristic;security of data;semantic web world wide web humans information resources information security data mining geology geophysics information technology computer science	A serious problem on the WWW is finding reliable information. Not everything found on the Web is true and the semantic Web does not change that in any way. The problem will be even more crucial for the semantic Web, where agents will be integrating and using information from multiple sources. Statements published on the semantic Web have to be seen as claims rather than as facts, and there should be a way to decide which among many possibly inconsistent sources is most reliable. In this work, we propose a trust model for the semantic Web. The proposed model is inspired by the use trust in human society. Trust is a type of social knowledge and encodes evaluations about which agents can be taken as reliable sources of information or services. Our proposed model allows agents to decide which among different sources of information to trust and thus act rationally on the semantic Web.	semantic web;www;world wide web	Xue Wang;Fan Zhang	2008	First International Workshop on Knowledge Discovery and Data Mining (WKDD 2008)	10.1109/WKDD.2008.84	web service;web application security;web mining;semantic computing;web development;web modeling;data web;web mapping;semantic search;semantic grid;web standards;computer science;semantic web;web navigation;social semantic web;data mining;semantic web stack;internet privacy;web intelligence;web 2.0;world wide web;semantic analytics	Web+IR	-42.567880278512604	13.57322018649667	97721
d9452565e6474038ba4f81c28ba9b4afe2cbf071	the role of medicinal ontologies in querying and exchanging pharmaceutical information	e healthcare;information search;medicinal ontologies;e prescriptions;electronic prescriptions;pharmaceutical information;information exchange;semantic web;interoperability;rdf;electronic healthcare	The technology developed for interoperable autonomous systems has significantly changed during the past few years. In particular, XML is rapidly becoming the key standard for data representation and transportation. However, the introduction of XML is not enough but many other XML-based technologies have to be deployed in order to achieve a seamless interoperability between the organisations of the healthcare sector. In this article we have restricted ourselves on electronic prescription systems and on the chances that Semantic Web technologies can provide. In particular, we report our work on medicinal ontologies and how they can be exploited (i) in providing querying facilities on electronic prescriptions and (ii) in achieving semantic interoperability between medicinal information systems. Especially, we introduced our adopted RDF-based messaging and show its gains over XML-based messaging.		Juha Puustjärvi;Leena Puustjärvi	2009	International journal of electronic healthcare	10.1504/IJEH.2009.026269	semantic interoperability;interoperability;information exchange;telecommunications;computer science;knowledge management;semantic web;rdf;data mining;database	DB	-45.83749453757901	8.189602709101601	97845
dc10f1885c9ca5d033ab116adc1a835b84fb71f1	pushing the quality level in networked news business: semantic-based content retrieval and composition in international news publishing	semantic based content coupling;news distribution;news management	Electronic publishing exploits numerous possibilities to present or exchange information and to communicate via most current media like the Internet. By utilizing modern Web technologies like Web Services, loosely coupled services, and peer-to-peer networks we describe the integration of an intelligent business news presentation and distribution network. Employing semantics technologies enables the coupling of multinational and multilingual business news data on a scaleable international level and thus introduce a service quality that is not achieved by alternative technologies in the news distribution area so far. Architecturally, we identified the loosely coupling of existing services as the most feasible way to address multinational and multilingual news presentation and distribution networks. Furthermore we semantically enrich multinational news contents by relating them using AI techniques like the Vector Space Model. Summarizing our experiences we describe the technical integration of semantics and communication technologies in order to create a modern international news network.	ana (programming language);display resolution;distributed computing;experience;hyperlink;information exchange;internet;loose coupling;peer-to-peer;prototype;scalability;software prototyping;systems architecture;universal storage platform;web service;xml	Markus W. Schranz	2006			computer science;multimedia;world wide web;information retrieval;news aggregator	Web+IR	-46.338180937126914	11.188657989603955	98008
f00a606cf0c62c84c2f0c74d68774be19d65a3e7	benchmarking results of semantic reasoners applied to an ambient assisted living environment	semantic reasoner;pellet reasoner;great amount;computational performance;elderly people;fastest choice;memory consumption experiment;increasing lifetime;custom java rule;reasonable memory requirement;benchmarking result;owl api	Ambient Assisted Living (AAL) environments and applications have been receiving a great amount of interest in recent years. The main reason of this interest is the increasing lifetime of elderly people. In this work, we present the benchmarking results of a simulated AAL environment using a knowledge driven approach modeled using ontologies and different semantic reasoners. Our goal is to determine and clarify which combination of semantic reasoners and frameworks is the most suitable for building knowledgedriven smart environments. In particular, we are interested in the combination which provides a better computational performance and reasonable memory requirements. The obtained results show that the fastest choice under the employed dataset is to use the OWL API accessing the Pellet reasoner. On other hand, the less memory consumption experiment is provided by a combination of HermiT and custom Java rules.	atm adaptation layer;application programming interface;computation;distributed computing;fastest;java;ontology (information science);requirement;semantic reasoner;smart environment;web ontology language	David Ausín;Federico Castanedo;Diego López-de-Ipiña	2012		10.1007/978-3-642-30779-9_45	embedded system;simulation;artificial intelligence;data mining;computer security	AI	-35.57535871858986	18.018473656303964	98065
c7a35580dbd5d6c0a81d433414415acabd5f0849	analyses of rdf triples in sample datasets		Linked Data principles supported especially by RDF triples appeared recently to enrich the Web of Documents by the Web of Data. However, each application that wants to process RDF triples has to deal with their distribution, dynamics and scaling. Thus, having understood structural and other features of such data, we may have better chances to propose these applications more efficiently. Especially when we consider issues of data storing, indexing and querying. The aim of this paper is to propose characteristics that appropriately capture and describe such features of RDF triples, and to provide experimental results over a few selected real-world RDF datasets.	image scaling;linked data;norm (social);resource description framework;tag cloud;world wide web	Jakub Stárka;Martin Svoboda;Irena Holubová	2012			rdf/xml;cwm;computer science;sparql;linked data;data mining;database;information retrieval;rdf schema	Web+IR	-36.10199309870027	4.19276097155016	98075
108630c3abfa54327f9d2c102b0f2e0f9b1e4ac0	qola: fostering collaboration within qa	web service;question answering	In this paper we suggest a QA pilot task, dubbed QolA, whose joint rationale is allow for collaboration among systems, increase multilinguality and multicollection use, and investigate ways of dealing with different strengths and weaknesses of a population of QA systems. We claim that merging answers, weighting answers, choosing among contradictory answers or generating composite answers, and verifying and validating information, by posing related questions, should be part and parcel of the question answering process. The paper motivates these ideas and suggests a way to foster research in these areas by deploying QA systems as Web services.	design rationale;question answering;software quality assurance;verification and validation;web service	Diana Santos;Luís Fernando Costa	2006		10.1007/978-3-540-74999-8_70	natural language processing;web service;question answering;computer science;knowledge management;data mining;world wide web;information retrieval	NLP	-45.899513790229896	6.98120638831342	98284
d62f8a229aaabbf70042b0e376273886996b0bc9	user's preferences and experiences based web service discovery using ontologies	web service discovery;multi agent system;application software;web services ontologies artificial intelligence user interfaces;quality of service user preference user experience web service discovery ontologies semantic service matching customer request description web service description;discovery;web service;ontologies artificial intelligence;software agents;multi agent systems;customer s preferences;web services;ontologies;explosions;quality of service;service oriented architecture;simple object access protocol;experience base;proposals;user interfaces;multi agent systems web services ontologies discovery customer s preferences;web services ontologies quality of service service oriented architecture laboratories software agents application software simple object access protocol proposals explosions	The lack of semantic parts, increasing number of web services in the web, and inefficient discovery mechanisms are the main problems of current web service technologies. In this paper, we propose a novel approach to enhance web services discovery based on, among others, customer's preferences and past experiences. In our approach, ontologies are used to describe web services, QoS, customer's preferences and experiences. We develop an efficient semantic service matching which takes into account customer's preferences to calculation the similarity degree between customer request description and web service description.	experience;ontology (information science);quality of service;service discovery;web services discovery;web service	Rohallah Benaboud;Zaïdi Sahnoun;Ramdane Maamri	2010	2010 Fourth International Conference on Research Challenges in Information Science (RCIS)	10.1109/RCIS.2010.5507371	web service;web application security;web development;web modeling;data web;web analytics;web mapping;web design;web standards;computer science;knowledge management;artificial intelligence;ws-policy;semantic web;web navigation;social semantic web;multi-agent system;ws-addressing;semantic web stack;database;web intelligence;ws-i basic profile;web 2.0;world wide web;universal description discovery and integration	Web+IR	-44.58510279986933	14.052305787361249	98314
4bb4f5fa982f2ef48b975e8c1ac7444eb4ede6f8	how grid could improve e-learning in the environmental science domain	e learning system;web service;open grid service architecture;grid;gis;web services;grid service;environmental science;interactive e learning	This paper will outline the requirements for an interactive e-learning system defined as part of the German research project GIMOLUS [1]. After a short overview over the Open Grid Service Architecture (OGSA) it will be shown that the capabilities of existing e-learning solutions are too limited in order to fulfil these requirements. The last part will show how a GIMOLUS system could be built using a GRID service architecture and what the benefits are in doing so.	open grid services architecture;requirement;scalability	Stefan Wesner;Konrad Wulf;Mark Muller	2002			semantic grid;computer science;systems engineering;knowledge management;world wide web;grid computing	Robotics	-45.33921843215133	10.585563664870152	98386
2a207b0e76dce32047a4ff8210e0f03cb23ef29f	verifying integrity constraints on web sites	integrity constraints;knowledge base	Data-intensive Web sites have created a new form of knowledge base, as richly structured bodies of data. Several novel systems for creating dataintensive Web sites support declarative specification of a site's structure and content (i.e., the pages, the data available in each page, and the links between pages). Declarative systems provide a platform on which A1 techniques can be developed that, further simplify the tasks of constructing and maintaining Web sites. This paper addresses the problem of specifying and verifying integrity constraints on a Web site's structure. We describe a language that can capture many practical constraints and an accompanying sound and complete verification algorithm. The algorithm has the important property that if the constraints are violated, it proposes fixes to either the constraints or to the site definition. Finally, we establish tight bounds on the complexity of the verification problem we consider.	algorithm;data integrity;formal specification;knowledge base;ptc integrity;tight binding;verification and validation	Mary F. Fernández;Daniela Florescu;Alon Y. Halevy;Dan Suciu	1999			knowledge base;web modeling;computer science;artificial intelligence;data integrity;data mining;database;world wide web	AI	-38.66196351241745	9.561368040733063	98471
fc47de80b9119e49fd7656931aca27fa4c03bc97	collaboration control in distributed knowledge-based systems	distributed system;query language;multiagent system;systeme reparti;knowledge based system;interrogation base donnee;interrogacion base datos;base connaissance;lenguaje interrogacion;graphe communication;sistema repartido;communication graph;base conocimiento;langage interrogation;information system;sistema multiagente;database query;systeme information;systeme multiagent;grafo comunicacion;sistema informacion;knowledge base	A distributed knowledge-based system (DKBS) is a collection of autonomous knowledge-based systems called agents. We assume here that these agents are capable of interacting with each other. They work together in solving queries submitted to DKBS according to their respective abilities. Each agent is represented by an information system (collection of data) and a dictionary (collection of rules). In [10], we proposed that an agent of DKBS can request from other agents rough descriptions of all unknown attribute values used in a query submitted to him. Some of these descriptions do not have to be consistent because they are created independently by dierent agents. The problem of repairing such descriptions (see [15]) requires additional interaction between all involved agents. Communication control between them is necessary.	autonomous robot;dictionary;information system;interaction;knowledge-based systems	Zbigniew W. Ras	1997	Inf. Sci.	10.1016/S0020-0255(96)00190-9	knowledge base;computer science;artificial intelligence;data mining;database;information system;query language	AI	-38.79630239658902	14.50301405270883	98522
25848fa34abaa59fb7e50846835b4ff2d58d969d	masque: an approach for flexible metadata storage and querying in rdf		The maintenance and use of metadata, such as provenance and time-related information (when was a data entity created or retrieved), is of increasing importance in the Semantic Web, especially for Big Data applications, that work on heterogeneous data from multiple sources and which require high data quality. In an RDF dataset it is possible to store metadata alongside the actual RDF data and several possible Metadata Representation Models (e.g. Singleton Property and n-ary relation) have been proposed. However, studies investigating the performance of these models show that choosing the appropriate metadata representation depends on the used data and metadata, queries and RDF store. To allow a flexible storage and querying of data and its metadata independent of the applied Metadata Representation Model, we propose MaSQue (Metadata Storage and Querying). The approach introduces an intermediate (meta)data serialization format and query annotations as metadata layer on top of RDF and SPARQL.	big data;data quality;resource description framework;sparql;semantic web;serialization;triplestore	Johannes Frey;Sebastian Hellmann	2017			database;rdf;metadata;computer science	DB	-36.98190187336653	4.964986607977927	98602
4979534f3ded8e99902153881d7d833c4c572935	keys graph - based relational to xml translation algorithm		The authors propose two algorithms for generating a DTD and an XML document respectively from the metadata and the content of a relational database without any intermediary language or user intervention. Such algorithms always generate semantically correct XML output by respecting database functional dependencies represented in a graph structure they take as input. Finally, different XML representations (or views) meeting expectations of different kind of users can be obtained from the same data according to the data entity chosen as translation pivot.	algorithm;angela mclean (biologist);conference on information and knowledge management;correctness (computer science);database design;directed graph;functional dependency;os-tan;relational database;sql;silk road;vldb;world wide web;xml namespace;xml schema	Wilmondes Manzi de Arantes Júnior;Christine Verdier	2004			xml validation;xml encryption;wait-for graph;relational calculus;streaming xml;relational database;computer science;theoretical computer science;document structure description;xml framework;database;xml signature;xml schema editor;graph database;information retrieval;efficient xml interchange	DB	-34.613086836935686	7.181595106185529	98625
b95c1c6666dc22069a2e1e9cfcd093f0a65b7a43	grid-enabling the global geodynamics project: automatic rdf extraction from the esml data description and representation via grddl	extensible markup language;investments;bottom up;geodynamics;top down;informing science;data engineering;resource description framework;data mining;global geodynamics project;data model;geoscience;geodynamics resource description framework data mining xml data models geoscience ontologies vehicles data engineering investments;xml;ontologies;vehicles;formal ontology;data models	An eXtensible Markup Language (XML) based data model for the Global Geodynamics Project (GGP) has been previously developed. Mindful of the need to incorporate metadata into the description and representation, a Resource Description Framework (RDF) based approach is introduced that extends the previous data model. Specifically, use of RDF allows relationships to be described and represented, and will eventually result in an ‘informal ontology’. The bottom-up approach makes use of GRDDL (Gleaning Resource Descriptions from Dialects of Languages) - a vehicle that allows for the extraction of RDF from XML according to rules. Because there exists some latitude in such extractions, complimentary top-down approaches will be required - especially when reconciling with formal ontologies. From this ‘information science’ perspective, GGP data has the potential to factor in the broader context being defined by the ‘new geoinformatics’.	bottom-up proteomics;data model;grddl;general game playing;geoinformatics;grid computing;integrated development environment;markup language;ontology (information science);programming paradigm;resource description framework;top-down and bottom-up design;xml	L. Ian Lumb;Keith D. Aldridge	2006	20th International Symposium on High-Performance Computing in an Advanced Collaborative Environment (HPCS'06)	10.1109/HPCS.2006.26	rdf/xml;cwm;xml;information engineering;computer science;top-down and bottom-up design;data mining;database;rdf query language;web ontology language;world wide web;rdf schema	DB	-42.83147984728287	4.896540819263218	98810
c32326c8d072f8a90b3a1dcadcb3aef470025990	study on food safety semantic retrieval system based on domain ontology	intelligent retrieval food safety semantic retrieval system domain ontology similarity computation semantic query expansion information retrieval system;information retrieval system;query processing;computer model;concept similarity;ontologies artificial intelligence;food safety;query processing food safety ontologies artificial intelligence;query expansion;domain ontology;semantics ontologies safety information retrieval knowledge based systems computational modeling certification;food safety semantic retrieval domain ontology query expansion concept similarity;semantic retrieval	From aspects of domain ontology construction, concept similarity computation based on ontology and semantic query expansion study the related technologies of information retrieval system based on ontology; establish food safety domain ontology and ontology-based concept similarity computation model, put forward a new semantic query expansion method based on concept similarity computation; design and implement food safety semantic retrieval system. The experiments show that this food safety semantic retrieval system is superior to the retrieval system based on keywords both in the recall ratio and the precision, and realize certain intelligent retrieval.	experiment;information retrieval;model of computation;ontology (information science);query expansion;semantic query	Yuehua Yang;Junping Du;MeiYu Liang	2011	2011 IEEE International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2011.6045028	computer simulation;upper ontology;semantic similarity;query expansion;food safety;bibliographic ontology;ontology inference layer;computer science;ontology;concept search;data mining;database;ontology-based data integration;information retrieval;process ontology;human–computer information retrieval	AI	-40.52801734859258	7.426348450633447	98908
512ba89e9d0d4aff57b5524e22c0438f789b8a8d	compatibility of xml language versions	software;paper;xml language;heuristic method;gestion configuration;metodo heuristico;ingenieria logiciel;compatibilidad;software engineering;langage dedie;compatibility;domain specific language;genie logiciel;world wide web;version;compatibilite;methode heuristique;language;configuration;management;markup language;configuration management;langage xml;lenguaje xml;lenguaje dedicado	Individual organisations as well as industry consortia are currently defining application and domain-specific languages using the eXtended Markup Language (XML) standard of the World Wide Web Consortium (W3C). This trend introduces new challenges for version and configuration management. We show that configuration management for XML languages is considerably more complicated for an XML Schema or DTD than it is for traditional software engineering artifacts. In addition to internal consistency of the language definition, also consistency between the language and its instance XML documents needs to be preserved when evolving the language definition. We propose a definition for compatibility between versions of XML languages that takes this additional need into account. Compatibility between XML languages in general is undecidable. We argue that the problem can become tractable using heuristic methods if the two languages are related in a version history. We propose to evaluate the method by using different versions of the Financial products Markup Language (FpML) in whose definition we participate.	cobham's thesis;configuration management;consortium;domain-specific language;heuristic;markup language;software engineering;undecidable problem;world wide web;xml schema	Daniel Dui;Wolfgang Emmerich	2003		10.1007/3-540-39195-9_11	ruleml;natural language processing;xml validation;regular language description for xml;xhtml;xml;xml schema;computer science;domain-specific language;document type definition;document definition markup language;document structure description;xml framework;xml schema;pcdata;database;schematron;markup language;configuration management;language;xml signature;programming language;compatibility;configuration;xml schema editor;algorithm;efficient xml interchange;sgml	DB	-35.612733877627115	13.22841170193872	99088
7d056c6441d14d7ac80df88a5af5b9eb3cd0ec98	improving multimedia as a service (maas) approach for dynamic multimedia content integration	multimedia content integration;soa;cc pp;web services;ontology;context	Multimedia content is derived from various autonomous, distributed and heterogeneous content sources. Service Oriented Architecture SOA is seen as a general answer to data integration problems. In this research work, we extend previous research works based on a model called Multimedia as a Service MaaS; through which multimedia content providers expose their content. We propose now a method to automatically discover and invoke relevant MaaSs services, which enables dynamic and transparent multimedia content integration according to user's preferences and device capabilities. For that, we propose to enhance the semantics in the MaaSs services description using domain ontology and use the standard Composite Capabilities/Preferences Profile CC/PP of W3C to manage the user profile.		Fatma Zohra Lebib;Hakima Mellah;Valérie Monfort	2013	IJMSO	10.1504/IJMSO.2013.057765	web service;epistemology;computer science;service-oriented architecture;ontology;database;multimedia;law;world wide web	DB	-44.33074984532247	12.314827195043476	99315
f7a3386bc8eabc10d398458e70fdcfb52a841471	ontoparts: a tool to select part-whole relations in owl ontologies		Representing part-whole and mereotopological relations in an ontology is a well-known challenge. We have structured 23 types of part-whole relations and hidden the complexities of the underlying mereotopological theory behind a user-friendly tool: OntoPartS. It automates modelling guidelines using, mainly, the categories from DOLCE so as to take shortcuts in the selection process, and it includes examples and verbalizations to increase understandability. The modeller’s domain ontology, represented in any of the OWL species, can be updated automatically with the selected relation with a simple one-click button.	1-click;mereotopology;ontology (information science);usability;web ontology language	Annette Morales-González;Francis C. Fernández-Reyes;C. Maria Keet	2012		10.1007/978-3-662-46641-4_44	modeller;data mining;ontology (information science);ontology;computer science	AI	-34.6789360238308	12.551684078324612	99502
a6ed6392fc528aec38a404f5ae179bf3cf799717	on context aware predicate sequence queries	context aware	Due to the limited input capabilities of small mobile information system clients like mobile phones, it is not a must to support a descriptive query language like SQL. Furthermore, information systems with mobile clients have to address characteristics resulting from clients mobility as well as from wireless communications. These additional functions can be supported by a reasonable, well-defined notation of queries. Moreover, such systems should be context aware. In this paper we present a query notation named “context aware predicate sequence queries” which respects these issues.	information system;mobile phone;query language;sql	Hagen Höpfner	2006			computer science;database	HCI	-35.488624300374916	13.65134320754349	99538
3f3bad387de9c398f47693ad223820fa5701096d	xhtml with rdfa as a semantic document format for ccts modelled documents and its application for social services	semantic document format;cefact ccts;xml schema;ccts modelled document;rdfa document;core components library;social services;finnish national project;extensible hypertext markup language;core component;layout markup;w3c xhtml;social service	For achieving semantic interoperability, messages or documents exchanged electronically between systems are commonly modelled using standard specifications, such as the UN/CEFACT CCTS (core components technical specification). However, additional requirements, such as the need for layout markup or common metadata for certain archiving scenarios might be applied to the documents. Furthermore, the management of resulting artefacts, i.e., core components, XML schemas and related infrastructure, could be cumbersome in some cases. This paper investigates the use of the W3C XHTML+RDFa (extensible hypertext markup language with resource description framework attributes) for representing both the layout and semantics of documents modelled according to CCTS. The paper focuses on the validation of XHTML+RDFa documents against a core components library represented as an ontology. In addition, the paper illustrates and validates this demand-driven solution in the scope of the Finnish National Project for IT in Social Services.	archive;comparison of web template engines;customer relationship management;database schema;digital signature;html;human-readable medium;hypertext;information system;line number;markup language;prototype;rdfa;report generator;requirement;resource description framework;semantic html;semantic interoperability;server (computing);type signature;un/cefact;user interface;xhtml;xhtml+rdfa;xml namespace;xml schema;xslt	Konstantin Hyppönen;Miika Alonen;Sami Korhonen;Virpi Hotti	2011		10.1007/978-3-642-25953-1_19	rdfa;xhtml;computer science;database;wireless markup language;world wide web;information retrieval	Web+IR	-41.878106807174724	4.7119714459770945	99613
d3ff505c7a39638da79fa67949f1f95242ba7033	a service-oriented architecture for proactive geospatial information services	service chain;geospatial information service;linked service;service oriented architecture;journal magazine article	The advances in sensor network, linked data, and service-oriented computing has indicated a trend of information technology, i.e., toward an open, flexible, and distributed architecture. However, the existing information technologies show a lack of effective sharing, aggregation, and cooperation services to handle the sensors, data, and processing resources to fulfill user’s complicated tasks in near real-time. This paper presents a service-orientated architecture for proactive geospatial information services (PGIS), which integrates the sensors, data, processing, and human services. PGIS is designed to organize, aggregate, and co-operate services by composing small scale services into service chains to meet the complicated user requirements. It is a platform to provide real-time or near real-time data collection, storage, and processing capabilities. It is a flexible, reusable, and scalable system to share and interoperate geospatial data, information, and services. The developed PGIS framework has been implemented and preliminary experiments have been performed to verify its performance. The results show that the basic functions such as task analysis, managing sensors for data acquisition, service composition, service chain construction and execution are validated, and the important properties of PGIS, including interoperability, flexibility, and reusability, are achieved.	aggregate data;data acquisition;distributed computing;experiment;geographic information system;interoperability;linked data;real-time clock;real-time computing;real-time data;requirement;scalability;sensor;service composability principle;service-oriented architecture;service-oriented device architecture;task analysis;testbed;user requirements document	Haifeng Li;Bo Wu	2011	Future Internet	10.3390/fi3040298	computer science;service delivery framework;operating system;service-oriented architecture;data mining;database;services computing;law;world wide web;computer security;computer network	OS	-36.48552448897112	15.219117666688891	99689
926fdd71d9b9f370f823ddfb90dd342f42339b6f	w3 trust-profiling framework (w3tf) to assess trust and transitivity of trust of web-based services in a heterogeneous web environment	consumidor;consumer perception;commerce electronique;confiance;psychologie sociale;comercio electronico;red www;metadata;consommateur;reseau web;service web;cle publique;web service;user assistance;confidence;public key;assistance utilisateur;internet;confianza;consumer;asistencia usuario;metadonnee;llave publica;psicologia social;world wide web;social psychology;metadatos;web technology;electronic trade;public key infrastructure;servicio web;web based service	The growth of eCommerce is being hampered by a lack of trust between providers and consumers of Web-based services. While researchers in many disciplines have addressed Web trust issues, a comprehensive approach has yet to be established. This paper proposes a conceptual trust-profiling framework through a range of new user-centred trust measures. W3TF is a generic form of trust assessment that can help build user confidence in an eCommerce environment. It incorporates existing measures of trust (such as Public Key Infrastructure), takes account of consumer perceptions by identifying trust attributes, and uses Web technology (in the form of metadata), to create a practical, flexible and comprehensive approach to trust assessment.	e-commerce;factor analysis;norm (social);prototype;public key infrastructure;relevance;theory;vertex-transitive graph;ws-trust;web application	Yinan Yang;Lawrie Brown;Edward Lewis;Jan Newmarch	2006		10.1007/11610113_33	web service;web of trust;the internet;consumer;computer science;public key infrastructure;database;public-key cryptography;confidence;metadata;law;world wide web;computer security;computational trust	Web+IR	-39.1553796363959	15.273860889063432	99732
75b5bc97ddfa6b809ca447640bc3261f100e74e0	a multi-agent system for service discovery, selection and negotiation	argumentation;multi agent system;coordination mechanisms;multi agent sys tems;soa;grid;service oriented computing;interaction protocol;service discovery;negotiation	Service-oriented computing can benefit from multi-agent system technologies by adopting the coordination mechanisms, interaction protocols, and decision-making tools designed for multi-agent systems. We demonstrate here the use of a fully decentralised multi-agent system supporting the discovery, selection, and negotiation of services.	decision support system;multi-agent system;service discovery;service-oriented device architecture	Stefano Bromuri;Visara Urovi;Maxime Morge;Kostas Stathis;Francesca Toni	2009		10.1145/1558109.1558312	computer science;knowledge management;service-oriented architecture;multi-agent system;distributed computing;service discovery;world wide web	AI	-43.54629431360131	16.34577339551583	99815
153c31807e4252bc4e94d258f18d496058179fd0	the role of space and time for knowledge organization on the semantic web	sensors and observations;space and time;semantic heterogeneity;geospatial semantics;time use;semantic web;ontologies;context;knowledge organization	Space and time have not received much attention on the Semantic Web so far. While their importance has been recognized recently, existing work reduces them to simple latitude-longitude pairs and time stamps. In contrast, we argue that space and time are fundamental ordering relations for knowledge organization, representation, and reasoning. While most research on Semantic Web reasoning has focused on thematic aspects, this paper argues for a unified view combining a spatial, temporal, and thematic component. Besides their impact on the representation of and reasoning about individuals and classes, we outline the role of space and time for ontology modularization, evolution, and the handling of vague and contradictory knowledge. Instead of proposing yet another specific methodology, the presented work illustrates the relevance of space and time using various examples from the geo-sciences.	computer science;conceptualization (information science);knowledge engineering;knowledge organization;ontology (information science);ontology modularization;relevance;semantic web;semantic heterogeneity;situated;vagueness;yet another	Krzysztof Janowicz	2010	Semantic Web	10.3233/SW-2010-0001	knowledge representation and reasoning;computer science;knowledge management;social semantic web;data mining;information retrieval;semantic analytics	AI	-38.55449326733682	5.0092430603236675	99828
2ee5eec3643a03cc5dae9d53ec935975c71836a5	si-designer: a tool for intelligent integration of information	electronic commerce;description logics;design support;information extraction;e commerce;clustering techniques;e commerce environment si designer intelligent information integration source integrator designer designer support tool semi automatic integration heterogeneous sources schemata semi structured sources momis project semantic approach intelligent description logics based techniques clustering techniques extended odmg odl language odl i sup 3 integrated information local schemata integrated view global schema virtual catalogs;integrated design;semistructured data;semantic heterogeneity;information integration;electrical capacitance tomography internet instruments postal services intelligent structures data mining explosions ontologies electronic catalog database languages;distributed databases;formal logic;data handling;description logic;object oriented languages;knowledge based systems;object oriented languages data handling distributed databases electronic commerce knowledge based systems formal logic	SI-Designer(SourceIntegratorDesigner)is a designersupport tool for semi automaticintegrationof heterogeneous sourcesschemata(relational,object and semi structured sources); it hasbeenimplementedwithin theMOMIS project andit carriesout integrationfollowing a semanticapproach whichusesintelligentDescriptionLogics-basedtechniques, clusteringtechniquesand an extendedODMG-ODL language, , to represent schemata, extracted,integrated information. Starting from the sources’ descriptions(local schemata) SI-Designersupportsthedesignerin thecreationof an integratedview of all thesources(global schema)which is expressedin the same language. We proposeSI-Designerasa tool to build virtual catalogs in theE-Commerceenvironment.	semiconductor industry	Domenico Beneventano;Sonia Bergamaschi;Ilario Benetti;Alberto Corni;Francesco Guerra;G. Malvezzi	2001		10.1109/HICSS.2001.927284	e-commerce;description logic;computer science;artificial intelligence;theoretical computer science;data mining;database;programming language;world wide web	Logic	-38.244622676325726	9.450477201139508	100006
6c0032783ffd0235f52f880a63a53fe7538ba3ac	generic xml schema definition (xsd) to gui translator	interfase usuario;xml schema;xsd;navegacion informacion;user interface;navigation information;xml language;information browsing;distributed computing;langage java;data model;general solution;internet;xml;calculo repartido;lenguaje java;interface utilisateur;modele donnee;wml;calcul reparti;langage html;langage xml;lenguaje xml;html language;lenguaje html;data models;java language	Organizations are seeking for a special kind of browser for exchanging structured information across business entities. In this paper, we present a generic solution called as XML Schema Definition (XSD) [6] to GUI translator. This translator processes data model defined in XML schema document and then generates user interface dynamically. This translator generates user interfaces in different rendering languages such as Java swings, HTML and WML.	bing translator;graphical user interface;xml schema	V. Radha;S. Ramakrishna;N. Pradeep Kumar	2005		10.1007/11604655_33	xml validation;xml;relax ng;xml schema;computer science;xs3p;document definition markup language;document structure description;operating system;xml schema;database;document schema definition languages;programming language;world wide web;xml schema editor	Web+IR	-35.91251745228335	11.20299120417998	100087
c14d66c3df105564b8eb46901e8b1d0e8782af0d	a business-level service model supporting end-user customization	domain specific reuse;service customization;domain engineering;feature modeling;swinburne;web service;service model;variability supported service matching;end user friendly;business level service model;service oriented computing;user requirements;domain specificity	If end users can utilize the abundant Web services to construct business applications on demand and independently, the ever changing business requirements will be met efficiently and timely. However, Web services are difficult for end users to use directly due to both the diversity of Web services and the diversity of end-user requirements. To tackle the problem, we introduce feature modeling and configuring techniques in domain engineering into service-oriented computing, and correspondingly propose a business-level service model and an end-user friendly service customization mechanism. Feasibility of the proposals is demonstrated on an example.		Jianwu Wang;Jian Yu	2007		10.1007/978-3-540-93851-4_29	web service;service level requirement;business domain;service product management;differentiated service;computer science;knowledge management;service delivery framework;user requirements document;service-oriented modeling;ws-policy;domain engineering;service-oriented architecture;service design;law;world wide web	HCI	-47.5501423796304	16.230190311505932	100195
1304faf04a8aba4c1d869e710ef0332812190251	garss: a generic annotation and recommendation service system for digital repositories	libraries;garss services generic annotation and recommendation service system digital repositories web 2 0 social networking user interactions service delivery model internet service providers user participation personalized services digital resources annotation sharing natural service paradigm social networking context user generated annotations saas solution;mashups;saas digital repository annotation recommendation service oriented computing mashups;web pages;mashups engines algorithm design and analysis libraries tag clouds web pages;digital repository;resource allocation;annotation;social networking online cloud computing recommender systems resource allocation service oriented architecture;engines;service oriented computing;tag clouds;social networking online;recommendation;service oriented architecture;recommender systems;algorithm design and analysis;cloud computing;saas	The popularity of Web 2.0 and social networking has greatly promoted user interactions in Internet services and changed the service delivery model. As typical Internet service providers, traditional digital repositories are facing two increasing demands: introducing user participation and providing personalized services. For the former, annotating digital resources and sharing the annotations with other users has been a natural service paradigm in the social networking context. For the latter, user-generated annotations in return can help digital repositories to supply personalized services such as recommendation. This paper proposes a Generic Annotation and Recommendation Service System (GARSS), which provides a SaaS solution to enhancing digital repositories with annotation and recommendation services. We presented the design and implementation of GARSS, and an example is shown to demonstrate the usage of GARSS services.	cloud computing;digital library;itil;interaction;interoperability;personalization;programming paradigm;software as a service;user-generated content;web 2.0;web service	Xiaodong Huang;Yong Zhang;Chunxiao Xing	2012	2012 IEEE 36th Annual Computer Software and Applications Conference Workshops	10.1109/COMPSACW.2012.16	digital library;computer science;service-oriented architecture;database;internet privacy;world wide web;recommender system	Networks	-46.09638871094249	12.030318224830577	100363
7153615cd0ac93eb5c0634c86de494c7b9c5071c	an ontology-based system for cloud infrastructure services' discovery	sql;cloud computing;ontologies (artificial intelligence);pattern matching;recommender systems;semantic web;service-oriented architecture;amazon;azure;cloudrecommender system;cocoon;gogrid;owl-based ontology;sql;cloud computing ontology;cloud infrastructure service discovery;cloud infrastructure services landscape;cloud providers descriptions;cloud service identification;functional concepts;heterogeneous types;nonfunctional concepts;nonstandardised naming conventions;ontology-based system;regular expressions;relational model;service configuration selection;service descriptions;user request matching;cloud computing;recommender system;semantic web;service descriptions	The Cloud infrastructure services landscape advances steadily leaving users in the agony of choice. As a result, Cloud service identification and discovery remains a hard problem due to different service descriptions, non-standardised naming conventions and heterogeneous types and features of Cloud services. In this paper, we present an OWL-based ontology, the Cloud Computing Ontology (CoCoOn) that defines functional and non-functional concepts, attributes and relations of infrastructure services. We also present a system, CloudRecommender-that implements our domain ontology in a relational model. The system uses regular expressions and SQL for matching user requests to service descriptions. We briefly describe the architecture of the CloudRecommender system, and demonstrate its effectiveness and scalability through a service configuration selection experiment based on a set of prominent Cloud providers' descriptions including Amazon, Azure, and GoGrid.	cloud computing;microsoft azure;ontology (information science);regular expression;relational model;sql;scalability;web ontology language	Miranda Zhang;Rajiv Ranjan;Armin Haller;Dimitrios Georgakopoulos;Michael Menzel;Surya Nepal	2012	8th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom)		sql;service catalog;cloud computing;computer science;operating system;service-oriented architecture;pattern matching;semantic web;cloud testing;data mining;database;world wide web;recommender system	HPC	-44.85210799498271	13.424926574633288	100518
3913a9e5fa3d5344882bf238cfd9a77c3e34f5fe	inducing declarative logic-based models from labeled traces	declarative logic-based model;artificial dataset;process executiontraces;ane-commerce protocol;declarative graphicallanguage;declarative description;learning integrity constraint;chosen language;declarative rule;automatic discoveryof logic-based model;health case process	In this work we propose an approach for the automatic discovery of logic-based models starting from a set of process execution traces. The approach is based on a modified Inductive Logic Programming algorithm, capable of learning a set of declarative rules. The advantage of using a declarative description is twofold. First, the process is represented in an intuitive and easily readable way; second, a family of proof procedures associated to the chosen language can be used to support the monitoring and management of processes (conformance testing, properties verification and interoperability checking, in particular). The approach consists in first learning integrity constraints expressed as logical formulas and then translating them into a declarative graphical language named DecSerFlow. We demonstrate the viability of the approach by applying it to a real dataset from a health case process and to an artificial dataset from an e-commerce protocol. Topics: Process mining, Process verification and validation, Logic Programming, DecSerFlow, Careflow.	algorithm;conformance testing;data integrity;declarative programming;digital footprint;e-commerce;experiment;formal system;graphical user interface;human-readable medium;icl;inductive logic programming;interoperability;run time (program lifecycle phase);tracing (software);verification and validation;visual programming language	Evelina Lamma;Paola Mello;Marco Montali;Fabrizio Riguzzi;Sergio Storari	2007		10.1007/978-3-540-75183-0_25	computer science;theoretical computer science;data mining;database;algorithm	AI	-41.199627064852585	14.070384813810504	100532
2f7d6dcdb1dcb59d7054a75bf523ea3cc4dc864c	enabling the semantic web of things: framework and architecture	protocols;resource discovery;semantic web knowledge based systems;semantics;semantics ontologies semantic web radiofrequency identification protocols mobile communication;internet of things;ubiquitous computing semantic web internet of things resource discovery;mobile communication;semantic web;ubiquitous computing;ontologies;information processing semantic web of things internet of things semantic annotation knowledge base models information storage information communication;radiofrequency identification;knowledge based systems	The Semantic Web of Things is a novel paradigm combining the Semantic Web and the Internet of Things, aiming to associate semantic annotations to real-world objects, locations and events. This paper presents a general framework for the Semantic Web of Things, based on an evolution of classic Knowledge Base models, also providing architectural solutions for information storage, communication and processing.	information management;internet of things;interoperability;knowledge base;programming paradigm;semantic web;web of things	Michele Ruta;Floriano Scioscia;Eugenio Di Sciascio	2012	2012 IEEE Sixth International Conference on Semantic Computing	10.1109/ICSC.2012.42	semantic interoperability;communications protocol;semantic computing;web development;web modeling;semantic integration;semantic web rule language;data web;web of things;mobile telephony;semantic search;semantic grid;web standards;computer science;ontology;artificial intelligence;semantic web;social semantic web;semantic web stack;database;semantics;web intelligence;semantic technology;world wide web;owl-s;internet of things;ubiquitous computing;information retrieval;semantic analytics	DB	-42.74807983324928	11.305094560861031	100545
2270abee39156a8645cca23d4859e08f3b1b06b1	warehousing complex data from the web	etl process;decision support;xml cube;complex data warehouse;analisis datos;xml language;knowledge extraction;data web;online analytical processing;olap;almacen dato;data mining;xml warehousing;web data;olap online analytical processing;data analysis;decision support system;dss;complex data;decision support systems;data warehousing;extraction connaissances;xml document;extraccion conocimiento;analyse donnee;entrepot donnee;complex data etl process;data warehouse;x warehousing;langage xml;lenguaje xml	Data warehousing and Online Analytical Processing (OLAP) technologies are now moving onto handling complex data that mostly originate from the web. However, integrating such data into a decision-support process requires their representation in a form processable by OLAP and/or data mining techniques. We present in this paper a complex data warehousing methodology that exploits eXtensible Markup Language (XML) as a pivot language. Our approach includes the integration of complex data in an ODS, in the form of XML documents; their dimensional modelling and storage in an XML data warehouse; and their analysis with combined OLAP and data mining techniques. We also address the crucial issue of performance in XML warehouses.	data mining;markup language;online analytical processing;operational data store;world wide web;xml	Omar Boussaïd;Jérôme Darmont;Fadila Bentayeb;Sabine Loudcher	2008	Int. J. Web Eng. Technol.	10.1504/IJWET.2008.019942	data exchange;xml;decision support system;online analytical processing;streaming xml;computer science;xml framework;data mining;xml database;database;world wide web;xml schema editor;efficient xml interchange	DB	-34.687726421086886	10.358941129070677	100571
2d0cc08387655912e834fb93c4155a71df0a9ff4	instance migration between ontologies having structural differences	structural difference;software tool;methodology;ontology;instance migration	Ontology instance migration is one of the complex and not fully solved problems in knowledge management. A solution is required when the ontology schema evolves in the life cycle and the assertions have to be transferred to the newer version. The problem may become more complex in distributed settings when, for example, several autonomous software entities use and exchange partial assertional knowledge in a domain that is formalized by different though semantically overlapping descriptive theories. Such an exchange is essentially the migration of the assertional part of an ontology to other ontologies belonging to or used by different entities. The paper presents our method and tool for migrating instances between the ontologies that have structurally different but semantically overlapping schemas. The approach is based on the use of the manually coded transformation rules describing the changes between the input and the output ontologies. The tool is implemented as a plug-in for the ProjectNavigator prototype software framework. The article also reports the results of our three evaluation experiments. In these experiments we evaluated the degree of complexity in the structural changes to which our approach remains valid. We also chose the ontology sets in one of the experiments to make the results comparable with the ontology alignment software. Finally we checked how well our approach scales with the increase of the quantity of the migrated ontology instances to the numbers that are characteristic to industrial ontologies. In our opinion the evaluation results are satisfactory and suggest some directions for the future work.	autonomous robot;database schema;entity;experiment;knowledge management;ontology (information science);ontology alignment;plug-in (computing);prototype;software framework;theory	Maxim Davidovsky;Vadim Ermolayev;Vyacheslav Tolok	2011	International Journal on Artificial Intelligence Tools	10.1142/S0218213011000553	upper ontology;idef5;ontology alignment;ontology components;computer science;ontology;artificial intelligence;theoretical computer science;machine learning;ontology;methodology;data mining;database;ontology-based data integration;process ontology;suggested upper merged ontology	AI	-44.58322395752836	7.823224847363283	100743
f5f48800864c1e0734e70d9c6024cd2027b051d5	how to build an rss feed using asp	xml;seo;really simple syndication	RSS is a XML based format. The Current popular version of RSS is RSS version 2.0. The purpose of adding an RSS feed to your site is to show if anything new is added to the site. For example, if a new article or blog or news item is added to your site that should automatically appear in the RSS feed so that the visitors/ RSS readers will automatically get updated about this new addition. The RSS feed is also called RSS channel. There are two main elements of the RSS XML file, one is the header or channel element that describes the details about the site/feeder and other is the body or item element that describes the consists of individual articles/entries updated in the site. As the format of the RSS feed file is pretty simple, it can be coded in any language, ASP, PHP or anything of that sort. We will build an RSS feeder using classical ASP (Active Server Pages) code in this article.	blog;php;rss;xml	Umakant Mishra	2013	CoRR		xml;atom;computer science;rss;database;internet privacy;world wide web;news aggregator	HCI	-39.396593460957064	10.184034224321113	101028
6bb5f99d5687e1f85ca463cca533f7a2cf7e7588	efficient ir-style search over web services	8903 information services;search strategy;school of engineering and science;web service;service oriented computing;web services discovery;0806 information systems;respubid18144;keyword searches	In service-based systems, one of the most important problems is how to discover desired web services. In this paper, we propose a novel IR-Style mechanism for discovering and ranking web services automatically. In particular, we introduce the notion of preference degree for web services and then we define service relevance and service importance as two desired properties for measuring the preference degree. Furthermore, various algorithms are given for computing the relevance and importance of services, respectively. Experimental results show the proposed IR-style search strategy is efficient and practical.	algorithm;computation;graph edit distance;relevance;time complexity;web service	Yanan Hao;Jinli Cao;Yanchun Zhang	2009		10.1007/978-3-642-02144-2_26	web service;web application security;web development;web modeling;business process execution language;web analytics;web mapping;web standards;computer science;service delivery framework;ws-policy;service-oriented architecture;data mining;ws-addressing;services computing;web intelligence;ws-i basic profile;web 2.0;law;world wide web;devices profile for web services;information retrieval;universal description discovery and integration	Web+IR	-44.674899177824415	13.632370219656464	101094
b4eeec41a8843aacee728ffb45721b195daaa471	an iterative and incremental approach for e-learning ontology engineering	e learning;ontology engineering;methodology	There is a boost in the interest on ontology with the developments in Semantic Web technologies. Ontologies play a vital role in semantic web. Even though there is lot of work done on ontology, still a standard framework for ontology engineering has not been defined. Even though current ontology engineering methodologies are available they need improvements. The effort of our work is to integrate various methods, techniques, tools and etc to different stages of proposed ontology engineering life cycle to create a comprehensive framework for ontology engineering. Current methodologies discuss ontology engineering stages and collaborative environments with user collaboration. However, discussion on increasing effectiveness and correct inference has been given less attention. More over, these methodologies provide little discussion on usability of domain ontologies. We consider these aspects as more important in our work. Also, ontology engineering has been done for various domains and for various purposes. Our effort is to propose an iterative and incremental approach for ontology engineering especially for e-learning domain with the intention of achieving a higher usability and effectiveness of e-learning systems. This paper introduces different aspects of the proposed ontology engineering framework and evaluation of it. Key words—ontology engineering, e-learning, methodology	iteration;iterative and incremental development;iterative method;ontology (information science);ontology engineering;refinement (computing);semantic web;synergy;usability	Sudath Rohitha Heiyanthuduwage;D. D. Karunaratna	2009	iJET	10.3991/ijet.v4i1.696	upper ontology;conceptualization;ontology alignment;computer science;knowledge management;ontology;methodology;data mining;ontology-based data integration;law;process ontology;suggested upper merged ontology	Web+IR	-44.90700803742974	6.037111966204243	101132
ca54e7b0c9097c793670cf380b92b35862b6d80b	introducing ontology best practices and design patterns into robotics: usarenv	unstructured environments;search and rescue robots;ontologies;ontology design patterns	Systems with knowledge representation and reasoning functionality are quite common within the robotics community. Nevertheless, most of the proposed architectures are ad-hoc implementations, which lack of modularity, standardization, and that cannot be reused or shared among different users. We fill that the Semantic Web effort over the past years in promoting standard representations and ontology best practices should be adopted by the robotic community, to foster the design of more effective and reusable systems, adapt for non expert users in everyday activities. In this paper, we investigate how to integrate ontology best practices and standard representations into the robotic system design, modelling an ontology for a real application field: urban search and rescue robotics. We also discuss the benefits of this methodology for robotic systems, as well as presenting some effective design guidelines.	best practice;design pattern;robotics	Gabriele Randelli;Daniele Nardi	2010		10.3233/978-1-60750-544-0-67	upper ontology;simulation;systems engineering;engineering;knowledge management;ontology;robotic paradigms;ontology-based data integration;process ontology;suggested upper merged ontology	HCI	-47.139595534498	7.098609091210045	101511
2a47b822fd1a6e82ebe114338f48fa9fc42f2f2f	expressing preferences in a viewpoint ontology	interfase usuario;ontologie;user interface;industrie alimentaire;user preferences;industria alimenticia;food industry;food products;comportement utilisateur;preferencia;ontologia;interface utilisateur;preference;user behavior;information system;ontology;systeme information;comportamiento usuario;sistema informacion	This paper proposes a definition of viewpoints in a “kind of” ontology. The use of viewpoints allows one to simplify user interface and to facilitate the expression of user preferences on such an ontology. This work has been applied in the framework of an information system dedicated to the quality of food products.	fuzzy set;information system;level of detail;multiple inheritance;ontology (information science);partial template specialization;relation (database);user (computing);user interface;viewpoint	Rallou Thomopoulos	2005		10.1007/11575801_41	upper ontology;food industry;human–computer interaction;computer science;knowledge management;ontology;ontology;ontology-based data integration;user interface;world wide web;process ontology;information system;suggested upper merged ontology	AI	-36.461203493903405	11.967939082795775	101665
022b16bead6451a745c8ec0a22341d169c827dda	ontology design principles and normalization techniques in the web	resource description framework rdf;uniform resource identifier uri;uniform resource identifier;design principle;ontology web language owl;biopax;knowledge sharing;semantic web;normal form;ontology;ontology design	The fundamental issue of knowledge sharing in the web is the ability to share the ontological constrains associated with the Uniform Resource Identifiers (URI). To maximize the expressiveness and robustness of an ontological system in the web, each ontology should be ideally designed for a confined conceptual domain and deployed with minimal dependencies upon others. Through a retrospective analysis of the existing design of BioPAX ontologies, we illustrate the often encountered problems in ontology design and deployment. In this paper, we identify three design principles --- minimal ontological commitment, granularity separation, and orthogonal domain --- and two deployment techniques --- intensionally normalized form (INF) and extensionally normalized form (ENF) --- as the potential remedies for these problems.	world wide web	Xiaoshu Wang;Jonas S. Almeida;Arlindo L. Oliveira	2008		10.1007/978-3-540-69828-9_5	upper ontology;biopax : biological pathways exchange;uniform resource identifier;open biomedical ontologies;bibliographic ontology;ontology inference layer;computer science;ontology;semantic web;ontology;data mining;database;world wide web;owl-s;process ontology	EDA	-36.96159024672496	5.824429239197594	101714
28d664aaa73ca6c40d729b5de69b50f39f75787d	non-functional aspects of information integration and research for the web science	linked data;information sources;data model;information integration;semantic web	Web Science is emerging as an interdisciplinary field that views the Web as a relevant source of information to be analysed for diverse scientific purposes. Semantic and linked data techniques and standards have been used to integrate existing Web information developing e-research efforts. Web applications and sources have to publish enriched information and data models, which are then matched and aligned. Some aspects of this integration are related with the evolutionary tracking, trustiness and plurality of the information available for e-science research. These aspects need to be modeled independently of the information and data sources of the research discipline. Here we provide an integrative ontology that enables to model such non-functional, informational aspects, to integrate information sources from enriched linked data applications. Finally we describe an application case of information research in the e-culture field.	case-based reasoning;data model;e-science;effective method;information source;linked data;programming paradigm;query expansion;relevance;sparql;semantic reasoner;trust (emotion);web ontology language;web application;web science;world wide web	Ivan Ruiz-Rube;Juan Manuel Dodero;John Stoitsis	2011		10.1016/j.procs.2011.04.176	idef1x;web modeling;semantic integration;data web;web mapping;data model;web standards;computer science;data science;information integration;semantic web;social semantic web;linked data;data mining;web intelligence;information retrieval	Web+IR	-40.009988529801085	5.9694550058285865	101971
9efc23f0e7b5dd6e6e8076ff3f3ad5e78ea116be	combining rule-based and plug-in components in agents for flexible dynamic negotiations	commerce electronique;agent interaction;movilidad;multiagent system;sistema experto;comercio electronico;agent mobile;negociation;compra;componente logicial;mobility;e commerce;software agent;localization;agente movil;rule based;base connaissance;composant logiciel;intelligence artificielle;localizacion;mobilite;agent logiciel;software agents;localisation;negociacion;bargaining;software component;artificial intelligence;base conocimiento;achat;inteligencia artificial;systeme expert;mobile agent;sistema multiagente;electronic trade;purchases;systeme multiagent;knowledge base;expert system	For software agents to become part of e-commerce they have to be flexible– to engage in negotiations of forms which are not known in advance, and mobile– to migrate to remote locations. This note aims at combining flexibility with mobility by joining rule-based mechanism representation with modular mobile agents. Furthermore, we focus on a more complete e-commerce scenario and address questions like: what happens before negotiations start and after they are finished, where from the purchase is actually made etc. Description of agent interactions in such a complete e-commerce scenario is presented.	e-commerce;interaction;logic programming;mobile agent;software agent	Costin Badica;Maria Ganzha;Marcin Paprzycki;Amalia Pirvanescu	2005		10.1007/11559221_59	knowledge base;simulation;computer science;artificial intelligence;software agent;expert system	AI	-39.535199759828714	16.008576782649563	102178
f831f793b9356418988b678da8c892bac1f8add1	analysis and modelling of trust in distributed information systems	modelizacion;trust;distributed system;systems;base donnee repartie;confiance;ciclo desarrollo;systeme reparti;psychologie sociale;formal specification;distributed database;life cycle;securite informatique;base repartida dato;analysis and modelling;systematique;specification formelle;computer security;modelisation;especificacion formal;confidence;sistema repartido;sistematica;distributed information system;confianza;seguridad informatica;taxonomy;psicologia social;cycle developpement;social psychology;analysis;distributed;modeling;information	In this paper, we consider the analysis and modelling of trust in distributed information systems. We review the relations of trust relationships in our previous work. We discuss trust layers and hierarchy based on formal definition of trust relationship. We provide a set of definitions to describe the properties of trust direction and trust symmetry under our taxonomy framework. In order to analyze and model the scope and diversity of trust relationship, we define trust scope label under our taxonomy framework. We provide some example scenarios to illustrate the proposed definitions about properties of trust relationship. The proposed definitions are new elements of the taxonomy framework for enabling the analysis and modelling of trust. We provide some discussions about the life cycle of trust relationships. The proposed trust structure and properties are currently being used in the development of the overall methodology of life cycle of trust relationships in distributed information systems.	information system;taxonomy (general);ws-trust	Weiliang Zhao;Vijay Varadharajan;George Bryan	2005		10.1007/11593980_8	biological life cycle;systems modeling;information;computer science;knowledge management;artificial intelligence;analysis;formal specification;system;confidence;trustworthy computing;operations research;distributed database;computer security;computational trust;taxonomy	Web+IR	-37.9092801525285	17.19150637833106	102182
162f1bd08aa1faf7a24b3ca89caa97d810854889	experiences in building a restful mixed reality web service platform	web engineering;rest;web service;identity management;social network;web services;point cloud;mixed reality;spatial orientation;architectural style	This paper reports the development of a RESTful Web service platform at Nokia Research Center for building Mixed Reality services. The platform serves geo-spatially oriented multimedia content, geo-data like streetview panoramas, building outlines, 3D objects and point cloud models. It further provides support for identity management and social networks, as well as for aggregating content from third party content repositories. The implemented system is evaluated on architecture qualities like support for evolution and mobile clients. The paper outlines our approach for developing RESTful Web services from requirements to an implemented service, and presents the experiences and insights gained during the platform development, including the benefits and challenges identified from adhering to the Resource Oriented Architecture style.	mixed reality;representational state transfer;web service	Petri Selonen;Petros Belimpasakis;Yu You	2010		10.1007/978-3-642-13911-6_27	web service;web development;web modeling;computer science;service-oriented architecture;multimedia;internet privacy;web engineering;law;world wide web	Web+IR	-47.239798502500506	11.63570825120425	102299
e9f7e879d8a09f6878bddc84dac73798d76dccd4	accessing linked open data via a common ontology		In the paper we present the construction of the FactForge service. FactForge represents a reason-able view over several Linked Open Data (LOD) datasets including DBPedia, Freebase and Geonames. It enables users to easily identify resources in the LOD cloud by providing a general unified method for querying a group of datasets. FactForge is designed also as a use case for large-scale reasoning and data integration. We describe the datasets, ontologies, inference rules, and manipulations done over the data. The datasets are unified via a common ontology – PROTON, whose concepts are mapped to the concepts of the involved LOD datasets. Each of the mapping rules relates a PROTON class or a PROTON property to the corresponding class or property of the other ontologies. This mechanism of constructing a reason-able view over selected LOD datasets ensures that the redundant instance representations are cleaned as much as possible. The instances are grouped in equivalent classes of instances.	conceptualization (information science);dbpedia;freebase;geonames;interoperability;linked data;ontology (information science);requirement;semantic web;software incompatibility	Kiril Ivanov Simov;Atanas Kiryakov	2017			database;linked data;ontology;computer science	DB	-36.739261235074856	5.246247050754648	102361
3fa7aee06d2ece4eb0d9d1cc548a91f64366c470	using iso 10303 data standard and xml standard web technology to enable iso 9000 document management	extensible markup language;iso 10303;iso 9000;hierarchical systems;extensible markup language xml;sgml;electronic document management;internet;database systems;xml;world wide web;iso 10013;web technology;user interfaces;document management	The economic growth pattern of India demonstrates that continuous consumption of fossil fuel is increasing the level of CO2 emissions. This issue also highlights the problem of energy efficiency. An error correction model has been formulated to investigate the causal association between fossil fuel consumption, energy efficiency, economic growth, and CO2 emission of India. The study has been conducted for the duration of 1971–2010. Major findings of this study are (a) the pattern of economic growth is resulting in energy efficiency, and (b) this energy efficiency is attained by establishing the missing feedback link in the Environmental Kuznets Curve (EKC).	causal filter;emoticon;error correction model;fossil;iso 10303;xml	Yin-Ho Yao;Amy J. C. Trappey	2003	IJCAT	10.1504/IJCAT.2003.002132	well-formed document;topic maps;xhtml;xml;iso/iec 12207;xml schema;geography markup language;computer science;document type definition;document management system;database;document schema definition languages;schematron;world wide web;information retrieval;common management information service;sgml	AI	-34.863450394990764	9.387168022260402	102406
fa09e80c85b583dc5a509b8be0ced1b2031ba256	making business sense of the semantic web	base donnee repartie;oferta;embryonics;base donnee;distributed database;red www;offer;web databases;feasibility analysis;reseau web;database;base repartida dato;base dato;semantics;administration publique;semantica;semantique;feasibility;prototipo;internet;descrestacion;success factor;clipping;semantic web;world wide web;civil service;administracion publica;ecretage;prototype;offre;practicabilidad;faisabilite;public administration	The Semantic Web and RDF offer a powerful platform for a wide range of applications. However, at this time, the semantic business is still in an embryonic stage. This paper analyses real-life applications: (i) A system in operation used to improve the presentation of corporate data, (ii) a prototype of a news clipping service and (iii) a feasibility analysis of a distributed, public administration database. Common to these applications is a ”web database” model where resources are ”URIed” made available on the Web and ”RDF-ed” described by one or multiple authorities. Descriptions are processed by RDFStore and made available to applications. The success factors of these three applications are analysed, and an application model is drawn up with requirements and desirable features for RDF Engines. In conclusion, we look at how and where the Semantic Web business can grow.	critical mass (sociodynamics);data mining;database model;distributed database;inference engine;prototype;real life;requirement;resource description framework;semantic web;usability;world wide web;xml	Zavisa Bjelogrlic;Dirk-Willem van Gulik;Alberto Reggiori	2003		10.1007/978-3-540-39718-2_52	feasibility study;cwm;web modeling;the internet;semantic web rule language;data web;web standards;computer science;sparql;clipping;artificial intelligence;operating system;semantic web;social semantic web;linked data;data mining;semantic web stack;database;prototype;web engineering;web 2.0;world wide web;computer security;semantic analytics	Web+IR	-37.386775623796595	11.748474309477519	102761
4a4d0f6d746585ad792186959f09aa4cef07e564	organizing ontology design patterns as ontology pattern languages		Ontology design patterns have been pointed out as a promising approach for ontology engineering. The goal of this paper is twofold. Firstly, based on well-established works in Software Engineering, we revisit the notion of ontology patterns in Ontology Engineering to introduce the notion of ontology pattern language as a way to organize related ontology patterns. Secondly, we present an overview of a software process ontology pattern language.	ontology engineering;organizing (structure);pattern language;process ontology;software design pattern;software development process;software engineering	Ricardo de Almeida Falbo;Monalessa Perini Barcellos;Julio Cesar Nardi;Giancarlo Guizzardi	2013		10.1007/978-3-642-38288-8_5	upper ontology;ontology alignment;ontology components;bibliographic ontology;ontology inference layer;computer science;knowledge management;ontology;data mining;ontology chart;database;ontology language;ontology-based data integration;owl-s;process ontology;suggested upper merged ontology	SE	-44.0566478512554	6.02793074729267	102823
74d2642b1d2f8ac54461d96748fa06d8cd9cccde	construction of fuzzy ontologies from fuzzy xml models	fuzzy xml model;fuzzy dtd;fuzzy ontology;fuzzy xml document;reasoning;construction	The success and proliferation of the Semantic Web depends heavily on construction of Web ontologies. However, classical ontology construction approaches are not sufficient for handling imprecise and uncertain information that is commonly found in many application domains. Therefore, great efforts on construction of fuzzy ontologies have been made in recent years. In particular, XML is imposing itself as a standard for representing and exchanging information on the Web, topics related to the modeling of fuzzy data have become very interesting in the XML data context. Therefore, constructing fuzzy ontologies from fuzzy XML data resources may make the existing fuzzy XML data upgrade to Semantic Web contents, and the constructed fuzzy ontologies may be useful for improving some fuzzy XML applications. This paper proposes a formal approach and an automated tool for constructing fuzzy ontologies from fuzzy XML data resources. Firstly, we propose a formal definition of fuzzy XML models (including the document structure fuzzy DTDs and the document content fuzzy XML documents). On this basis, we propose a formal approach for constructing fuzzy ontologies from fuzzy XML models, i.e., transforming a fuzzy XML model (including fuzzy DTD and fuzzy XML document) into a fuzzy ontology. Also, we give the proof of correctness of the construction approach, and provide a detailed construction example. Furthermore, we implement a prototype tool called FXML2FOnto, which can automatically construct fuzzy ontologies from fuzzy XML models. Finally, in order to show that the constructed fuzzy ontologies may be useful for improving some fuzzy XML applications, we focus on investigating how to reason on fuzzy XML models (e.g., conformance, inclusion, and equivalence) based on the constructed fuzzy ontologies, and it turns out that the reasoning tasks of fuzzy XML models can be checked by means of the reasoning mechanism of fuzzy ontologies.	ontology (information science);xml	Fu Zhang;Zongmin Ma;Li Yan	2013	Knowl.-Based Syst.	10.1016/j.knosys.2012.12.015	xml validation;construction;fuzzy classification;computer science;document structure description;neuro-fuzzy;xml framework;data mining;xml schema;database;xml schema editor;fuzzy set operations;information retrieval;reason	Web+IR	-38.26560841696855	8.227711034642207	102856
3871ec770435e737a39c0b929f209f7bdd20f5a6	towards a modification exchange language for distributed rdf repositories	query language;red www;metadata;interoperabilite;interoperabilidad;reseau web;semantics;semantica;semantique;lenguaje interrogacion;peer to peer p2p;internet;metadonnee;world wide web;langage interrogation;metadatos;interoperability;use case;distributed rdf repositories;reseau paire	Many RDF repositories have already been implemented with various access languages and mechanisms. The aim of the EDUTELLA framework is to allow communication between different RDF repository implementations. Part of EDUTELLA is a Query Exchange Language (QEL) which can be used as lingua franca to retrieve information from RDF repositories. This work shows why we also need standardization of distributed modification capabilities. We describe use case scenarios for annotation and replication services and use them as guideline for our approach towards a Modification Exchange Language (MEL) for distributed RDF repositories.		Wolfgang Nejdl;Wolf Siberski;Bernd Simon;Julien Tane	2002		10.1007/3-540-48005-6_19	use case;rdf/xml;interoperability;cwm;the internet;computer science;sparql;database;semantics;rdf query language;metadata;world wide web;information retrieval;query language;rdf schema	Web+IR	-36.936338851260274	11.6705114872868	102948
1267f5de0112b2735d71da643fad2f8a5a50e1bb	referencing within evolving hypertext		The classic hypertext model omits the process of text growth, evolution and synthesis. With hypertext creation becoming increasingly collaborative and change timescales becoming shorter, explicitly addressing text evolution is the key to the next stage of hypertext development. Uniform Resource Identifier (URI) is a proven general concept that enabled the Web. In application to versioned deep hypertext, expressive power of a classical hyperlink becomes insufficient. Based on the Causal Trees model, we introduce a minimalistic but powerful query language of specifiers that provides us great flexibility of referencing within a changing hypertext. Specifiers capture the state of the text, point at changes, expose authorship or blend branches. Being a part of an URI, a specifier puts advanced distributed revision control techniques within reach of a regular web user.	causal filter;distributed version control;event (computing);expressive power (computer science);hyperlink;hypertext;lamport timestamps;query language;scenario (computing);software versioning;uniform resource identifier;world wide web	Victor Grishchenko;Samar Al-Saaqa;Henk J. Sips	2011				DB	-40.23180045542752	9.326079183623575	103092
311a8b8d070411763097259d94af628d187a20d8	towards a semantic-based approach for software reusable component classification and retrieval	program understanding;semantic representation;system develop;web service;semantic web technology;domain knowledge;reusable library;conceptual graph;natural language;software reusability;semantic matchmaking;semantic description;software component;domain knowledge base;semantic web;world wide web;system development;semantic matching;reuse repository;domain ontology;ontology;natural language processing	In this paper, we propose a semantic-based approach to improve software component reuse. The whole approach extends the software reusable library to the World Wide Web; overcomes the keyword-based barrier by allowing user queries in natural language; treats a software component as a service described by semantic service representation format; enhances the retrieval by semantically matching between a user query semantic representation and software component semantic descriptions against a domain ontology; and finally stores the relevant software components into a reusable repository based UDDI infrastructure. The technologies applied to achieve the goal include: Natural Language Processing, Web services, Semantic Web, Conceptual Graph, domain ontology. The research in the first phase will focus on the classification and retrieval for software reusable components. In the classification process, natural language processing and domain knowledge technologies are employed for program understanding down to code level, and Web services and Semantic Web technologies as well as Conceptual Graph are used to semantically describe/represent a component. In the retrieval process, a user query in natural language is translate into semantic representation formats in order to augment retrieval recall and precision by deploying the same semantic representation technologies on both the user query side and the component side.	component-based software engineering;conceptual graph;library (computing);natural language processing;ontology (information science);precision and recall;program comprehension;semantic web;third-party software component;web service;world wide web	Haining Yao;Letha H. Etzkorn	2004		10.1145/986537.986564	semantic data model;semantic similarity;semantic computing;multinet;semantic web rule language;explicit semantic analysis;semantic search;semantic grid;computer science;semantic web;social semantic web;semantic web stack;semantic compression;database;semantic technology;world wide web;owl-s;information retrieval;semantic analytics;semantic gap	Web+IR	-40.58498233756256	7.086315171465024	103146
afef9ed22fbe7be9654ec3dce3f259b3ae369479	ontology-based classification of unstructured information	power supplies;document handling;classification algorithm;metadata;information retrieval;knowledge management;semantics;text analysis;resource description framework;data mining;classification;ontologies artificial intelligence;indexes;ontology based classification;metadata ontology based classification unstructured information knowledge management information retrieval heterogeneous data sources document handling;business;unstructured information;meta data;ontologies;ontologies data mining indexes power supplies resource description framework semantics business;text analysis classification information retrieval knowledge management meta data ontologies artificial intelligence;heterogeneous data sources	The area of knowledge management (KM) has been addressed with a considerable amount of research in order to develop concepts and technologies for the retrieval of information and knowledge out of a set of heterogeneous data sources. Especially when we deal with files which contain unstructured information, i.e. documents, it is still a huge challenge to classify them automatically into certain domain-dependant categories. Therefore, this paper describes an application and the underlying concepts which are used for a classification based on the available metadata of files, whereas the classification categories can be found in form of ontology classes. This paper discusses experiences and challenges during the implementation with special regard to ontology-based classification algorithms, the underlying framework as well as the importance metadata quality.	algorithm;categorization;computer science;dublin core;inference engine;knowledge management;sparql;semantic search;text mining;william jolitz	Stefan Burger;Bernd Stieger	2010	2010 Fifth International Conference on Digital Information Management (ICDIM)	10.1109/ICDIM.2010.5664634	text mining;bibliographic ontology;computer science;artificial intelligence;data mining;database;semantics;metadata;world wide web;information retrieval	DB	-40.721471202661384	6.02252363229168	103323
e0db2a3130732314912ae73c3d74689a34667a3f	integration of reliable sensor data stream management into digital libraries	electronic patient record;digital library;data stream;information filtering;wireless communication;stream processing	Data Stream Management (DSM) addresses the continuous processing of sensor data. DSM requires the combination of stream operators, which may run on different distributed devices, into stream processes. Due to the recent advantages in sensor technologies and wireless communication, the amount of information generated by DSM will increase significantly. In order to efficiently deal with this streaming information, Digital Library (DL) systems have to merge with DSM systems. Especially in healthcare, the continuous monitoring of patients at home (telemonitoring) will generate a significant amount of information stored in an e-health digital library (electronic patient record). In order to stream-enable DL systems, we present an integrated data stream management and Digital Library infrastructure in this work. A vital requirement for healthcare applications is however that this infrastructure provides a high degree of reliability. In this paper, we present novel approaches to reliable DSM within a DL infrastructure. In particular, we propose information filtering operators, a declarative query engine called MXQuery, and efficient operator checkpointing to maintain high result quality of DSM. Furthermore, we present a demonstrator implementation of the integrated DSM and DL infrastructure, called OSIRIS-SE. OSIRIS-SE supports flexible and efficient failure handling to ensures complete and consistent continuous data stream processing and execution of DL processes even in the case of multiple failures.	application checkpointing;digital library;information filtering system;sensor;stream processing	Gert Brettlecker;Heiko Schuldt;Peter M. Fischer;Hans-Jörg Schek	2007		10.1007/978-3-540-77088-6_7	real-time computing;computer science;data mining;database	DB	-36.28380849149577	15.224825769332279	103508
d40bf362fe91684c952ab908d45221c7d753afb4	domain-specific ontology building for semantic web using data mining.	data mining;semantic web;domain specificity			Ji Suk Kim	2005			semantic data model;web mining;semantic computing;semantic web rule language;data web;bibliographic ontology;semantic search;ontology inference layer;semantic grid;web standards;semantic web;social semantic web;data mining;semantic web stack;web intelligence;ontology-based data integration;semantic technology;world wide web;owl-s;information retrieval;semantic analytics	Web+IR	-39.8292645160009	6.831828845114693	103538
73dc041c26998ee182c214a3dfa428ec491fc14c	understanding advances in web technologies: evolution from web 2.0 to web 3.0	advantages;shortcomings;critical success factors;barriers;definition;architectural framework;web 2 0;web 3 0;applications	The current generation of Web applications (Web 2.0) have made them an outright phenomenon in today’s society helping to redefine the way organisations and individuals communicate and collaborate with each other. The purpose of this paper is to conceptualise the evolution of Web technologies from a user perspective. Based on inference from existing studies, this paper attempts to identify the architectural direction that the next generation (Web 3.0) of Web applications would meld itself into. The paper emphasizes limitations of current Web technologies and how future trends may address these limitations by focusing on migration that has been witnessed in the scope of the applications presented and features delivered on the Web from a users’ perspective.	collective intelligence;enterprise architecture framework;focal (programming language);meld (software);neural binding;next-generation network;scalability;semantic web;video synopsis;web 2.0;web application;web development;web server;world wide web	Yogesh Kumar Dwivedi;Michael D. Williams;Amit Mitra;Suraj Niranjan;Vishanth Weerakkody	2011			web application security;web modeling;definition;web design;web accessibility initiative;web standards;computer science;knowledge management;social semantic web;architecture framework;web intelligence;web engineering;critical success factor;web 2.0;information technology;world wide web	Web+IR	-46.62018501639044	10.1459401089791	103848
0d6f7dc6169b24fd07fe9ce96c73eb497935e0fd	towards a formal modeling of cloud services during the life-cycle of service level agreement		Service Level Agreement (SLA) represents a means of regulating and controlling the interaction between service providers and their customers. In the first part of this paper, we use Bigraphical Reactive Systems (BRS) to model customers, offered services by different providers and the SLA between them. Concretely, we use bigraphs, the static structure of BRS, to model these entities and to describe their relationships. In addition, we propose a set of reaction rules to show the evolution of their states during the different stages of the SLA's lifecycle. In the second part, we apply these models in the domain of cloud computing. Cloud computing architecture is usually represented as a stack of different layers. We show that the proposed models can be applied to any computing model (e.g., Software as a Service, Platform as a Service and Infrastructure as service layer) and they allow describing the SLA across the cloud stack layers. We show also that these models can represent the composition of services from several layers to offer complete solutions to end users.	brs/search;bigraph;cloud computing architecture;computer architecture;entity;platform as a service;service layer;service-level agreement;software as a service	Oussama Kamel;Allaoua Chaoui;Mohamed Gharzouli	2017		10.1145/3175684.3175727	end user;service provider;bigraph;data mining;cloud computing;cloud computing architecture;service-level agreement;service layer;computer science;distributed computing;software as a service	Web+IR	-46.74764247755243	17.93624994546399	103929
a6b93a30b874ed8c4b60ce3f330a3e98abd39484	a methodology for building semantic web mining systems	outil logiciel;software tool;ontologie;interoperabilite;interoperabilidad;web semantique;inductive logic programming;intelligence artificielle;data mining;fouille donnee;web semantica;semantic web;artificial intelligence;ontologia;inteligencia artificial;interoperability;herramienta software;ontological engineering;busca dato;ontology;programmation logique inductive	In this paper we present a methodology based on interoperability for building Semantic Web Mining systems. In particular we consider the still poorly investigated case of mining the Semantic Web layers of ontologies and rules. We argue that Inductive Logic Programming systems could serve the purpose if they were more compliant with the standards of representation for ontologies and rules in the Semantic Web and/or interoperable with well-established Ontological Engineering tools that support these standards.		Francesca A. Lisi	2006		10.1007/11875604_35	semantic interoperability;interoperability;web mining;semantic computing;web modeling;semantic web rule language;data web;semantic search;semantic grid;web standards;computer science;artificial intelligence;semantic web;ontology;social semantic web;data mining;semantic web stack;database;web intelligence;world wide web;semantic analytics	AI	-38.009025109274134	12.011377906635202	103951
86fb7d364b152bd1038ddd0d9b3a3b6e14786136	a semantic web approach to represent and retrieve information in a corporate memory	semantic web	This paper outlines the first stage of the ODARyT Tool, a semantic web approach to represent and retrieve information in a corporate memory. After a brief discussion about the meet points between semantic web and corporate memories, we describe our semantic approach related to a specific corporate memory located in the Networks and Telecommunication domain. The corporate knowledge is represented in an OWL Ontology, which provides an intuitive way to create annotations of heterogeneous resources composing a Corporate Semantic Web.	semantic web;web ontology language	Ana B. Rios-Alvarado;R. Carolina Medina Ramírez;Ricardo Marcelín-Jiménez	2009			semantic similarity;semantic computing;semantic web rule language;data web;semantic grid;computer science;semantic web;social semantic web;data mining;semantic web stack;database;world wide web;owl-s;information retrieval;semantic analytics	Web+IR	-41.55594082692627	5.673644437171369	104013
7ab06d8df96025396c14189d2b2e95149b18b465	distributed query processing for federated rdf data management		The publication of freely available and machine-readable information has increased significantly in the last years. Especially the Linked Data initiative has been receiving a lot of attention. Linked Data is based on the Resource Description Framework (RDF) and anybody can simply publish their data in RDF and link it to other datasets. The structure is similar to the World Wide Web where individual HTML documents are connected with links. Linked Data entities are identified by URIs which are dereferenceable to retrieve information describing the entity. Additionally, so called SPARQL endpoints can be used to access the data with an algebraic query language (SPARQL) similar to SQL. By integrating multiple SPARQL endpoints it is possible to create a federation of distributed RDF data sources which acts like one big data store. In contrast to the federation of classical relational database systems there are some differences for federated RDF data. RDF stores are accessed either via SPARQL endpoints or by resolving URIs. There is no coordination between RDF data sources and machine-readable meta data about a source’s data is commonly limited or not available at all. Moreover, there is no common directory which can be used to discover RDF data sources or ask for sources which offer specific data. The federation of distributed and linked RDF data sources has to deal with various challenges. In order to distribute queries automatically, suitable data sources have to be selected based on query details and information that is available about the data sources. Furthermore, the minimization of query execution time requires optimization techniques that take into account the execution cost for query operators and the network communication overhead for contacting individual data sources. In this thesis, solutions for these problems are discussed. Moreover, SPLENDID is presented, a new federation infrastructure for distributed RDF data sources which uses optimization techniques based on statistical information.	big data;data store;directory (computing);entity;html;human-readable medium;linked data;mathematical optimization;overhead (computing);query language;relational database;resource description framework;run time (program lifecycle phase);sparql;sql;world wide web	Olaf Görlitz	2015			rdf/xml;cwm;turtle;computer science;sparql;simple knowledge organization system;linked data;data mining;database;rdf query language;information retrieval;rdf schema	DB	-35.67529166147015	4.809241441285142	104147
a9c209f02fa251d273c79ef43c593890eb05871a	personalized information recommendation service system based on gis	location service;personal computing;personalized information recommendation service system;geographic information systems filtering paper technology wireless networks object oriented modeling geology system testing database systems position measurement information systems;service system;location based service;personalized location service;object oriented database system;personalized maps resources filtering module;personal computing geographic information systems information filters object oriented databases;gis;geographic information systems;object oriented database systems;feature selection module;feature selection;object oriented databases;wireless technology;gis personalized information recommendation service system personalized location service feature selection module user interest module personalized maps resources filtering module object oriented database system;information filters;user interest module	To realize personalized location service, personalized information recommendation service system based on GIS is proposed and realized. The structure of system, workflow and key technologies of realizing feature selection module, user interest module, personalized maps resources filtering module are introduced in the paper. After the test use of the system by some certain users, we found that the system could improve the learners' initiative participation in location based service. The paper serves as a case study tries to expose the potential of wireless technology to serve LBSs. The future study focuses on an object-oriented database system capable for large spatial dataset.	database;feature selection;geographic information system;location-based service;map;personalization	Xiao-Fang Zu;Luo Jin;Luo Qi	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.552	computer science;data mining;database;world wide web	Mobile	-36.15648232576953	14.52870663892528	104252
81a882a6f221e2228e8f40e634fa13e92f1d85ac	context-sensitive service discovery experimental prototype and evaluation	location service;dynamic change;hierarchical system;context awareness;context aware;experimental analysis;context information;network scalability;hierarchical networks;context management;distributed context management;service discovery;network services;prototype	The amount of different networks and services available to users today are increasing. This introduces a need for ways to locate and sort out irrelevant services in the process of discovering available services for a user. In this paper, we describe and evaluate a prototype of an automated discovery and selection system, which locates services that are relevant to a user based on his/her context and the context of the available services. The prototype is based on a multi-level, hierarchical system approach and introduces entities called User-nodes, Super-nodes, and Root-nodes. These entities separates the network into domains that handle the complex distributed service discovery, which is based on dynamically changing context information. In the prototype, a method for performing context-sensitive service discovery has been achieved. The service discovery part utilizes UPnP, which has been extended in order to increase network scalability. The experimental analysis of service discovery times, based on different scenarios is used to optimize parameter settings of the service discovery system in order to achieve short response times.	algorithm;context-sensitive grammar;discovery system;entity;prototype;relevance;scalability;semiconductor industry;service discovery;superuser;universal plug and play	Robin Balken;Jesper Haukrogh;Jens L. Jensen;Morten N. Jensen;Lars Jessen Roost;Per Nesager Toft;Rasmus L. Olsen;Hans-Peter Schwefel	2007	Wireless Personal Communications	10.1007/s11277-006-9200-0	differentiated service;computer science;service delivery framework;operating system;data mining;database;prototype;service discovery;hierarchical control system;world wide web;computer network;experimental analysis of behavior	Mobile	-38.957677699773726	13.868107997422726	104530
5ec350980adb1e30806be4f6c37dcd8f86c82210	metrics for evaluating the semantic implications of changes in evolving ontologies	inference;semantic im- plications;ontology evolution;owl;change evaluation metrics	We propose a set of metrics for measuring the semantic implications of changes during ontology evolution. Our metrics focus on the changes of classes and associated axioms or annotations in an evolving OWL ontology. We identify three categories of changes and present class, class definition, class subsumption hierarchy, and logical model oriented metrics for quantifying these changes. We implement the proposed metrics using OWL API and Pellet OWL-DL reasoner. We evaluate our metrics by applying them to the evolution of the Gene Ontology.	chomsky hierarchy;definition;gene ontology;ontology (information science);semantic reasoner;subsumption architecture;web ontology language	Chuming Chen;Manton M. Matthews	2008			web ontology language;process ontology;information retrieval;ontology-based data integration;ontology (information science);ontology;logical data model;data mining;semantic reasoner;upper ontology;computer science	AI	-37.862198071803185	5.217146936589208	104679
0eafa409022d182a5a1fd8ddb2cab1affbbc4867	developing web 2.0 applications for semantic web of trust	libraries;application development;selection model;computer languages;electronic commerce;web of trust;java programming;information technology;resource description framework;semantic web resource description framework electronic commerce feedback information technology lan interconnection java computer languages libraries encoding;lan interconnection;knowledge representation languages;feedback;semantic web;web 2 0 application;semantic web knowledge representation languages;resource description language;resource description language web 2 0 application semantic web;encoding;historical data;java;open source	"""Historically data is processed, presented and shared by few applications that either generated it or that are specifically tailored to share it. Text and comma separated files are amongst the few which can be exchanged and interchanged with most applications. But in all such cases, the uniformly understandable information about the data and how it relates to the real world objects is missing. These missing links inhibit current Web applications to recognize common real world entities among various data sets by starting with one data set and then moving to other sets of data. Web 2.0 applications aim to interconnect users and contents so that users can use contents to find participants and vice versa. Resource description language (RDF), a language for Web 2.0, targets to represent information semantically and exchange knowledge in the current Web environment. """"Web of Trust"""" is recognized as one of the foremost goals for Web 2.0 and it is a part of 'W3C seven point goals for the future Web'. Trust is recognized as one of the impacting forces in the Web and RDF is the main language to represent information for the Web 2.0. In this paper, a model is selected which conceptually connects users' online trust evaluations (contents) and physical participants (users and online merchants). The paper first outlines the essential features of a Web 2.0 application. Followed by this, a structured application development and implementation process is outlined for the selected model. For application development, Java programming language is selected due to the vast availability of open source libraries"""	entity;foremost;java;library (computing);open-source software;programming language;semantic web;ws-trust;web 2.0;web application;web of trust;world wide web	Omer Mahmood	2007	Fourth International Conference on Information Technology (ITNG'07)	10.1109/ITNG.2007.76	web service;web application security;web of trust;web development;web modeling;data web;web analytics;web mapping;web design;web standards;computer science;operating system;software engineering;semantic web;web navigation;rdf;social semantic web;web page;data mining;feedback;semantic web stack;database;web intelligence;web engineering;programming language;java;rapid application development;web 2.0;information technology;world wide web;encoding;mashup	Web+IR	-39.46423341064366	10.247201044791348	105071
346ef7358c1a6c0141339c77275ed1e98747e406	dynamic ontology co-construction based on adaptive multi-agent technology	agent technology	Ontologies have become an important means for structuring knowledge and defining semantic information retrieval systems. Ontology engineering requires a significant effort, and recent researches show that human language technologies are useful means to acquire or update ontologies from text. In this paper we present DYNAMO, a tool based on a Multi-Agent System, which aims at assisting ontologists during the ontology building and evolution processes. This work is carried out in the context of the DYNAMO project. The main novelty of the agent system is to take advantage of text extracted terms and lexical relations together with some quantitative features of the corpus to guide the agents when self-organizing. We exhibit the first experiment of ontology building that shows promising results, and helps us to identify key issues to be solved to the DYNAMO system behavior and the resulting ontology.	information retrieval;language technology;lexical substitution;multi-agent system;network computing system;ontology (information science);ontology engineering;organizing (structure);prototype;self-organization;software bug;tor messenger	Zied Sellami;Marie Pierre Gleizes;Nathalie Aussenac-Gilles;Sylvain Rougemaille	2009			computer science;knowledge management	AI	-44.223180073468775	7.767802933220344	105199
ac11c496993ed1e2dbe177a14c75059d3d1e906a	towards distributed configuration	distributed system;configuracion;systeme reparti;red www;system configuration;cooperation;logic;reseau web;developpement economique;organisation systeme;cooperacion;organizacion sistema;resolucion problema;sistema repartido;internet;algorithme reparti;world wide web;completitud;algoritmo repartido;economic development;completeness;configuration;completude;distributed algorithm;desarrollo economico;logique;logica;problem solving;resolution probleme	Shorter product cycles, lower prices, and the production of highly variant products tailored to the customer needs are the main reasons for the proceeding success of product configuration systems. However, today’s product configuration systems are designed for solving local configuration tasks only, although the economic development towards webs of highly specialized solution providers demands for distributed problem solving functionality. In this paper we motivate the integration of several configurators and give a formal definition of the distributed configuration task based on a logic theory of configuration. Furthermore, we present a basic architecture comprising several configuration agents and propose an algorithm for cooperation between distributed configuration systems that ensures correctness and completeness of configuration results.	algorithm;business process;configuration management;correctness (computer science);entity;heuristic (computer science);internet protocol suite;knowledge-based configuration;problem solving;requirement;system requirements;theory (mathematical logic)	Alexander Felfernig;Gerhard Friedrich;Dietmar Jannach;Markus Zanker	2001		10.1007/3-540-45422-5_15	configuration management;distributed algorithm;completeness;computer science;artificial intelligence;distributed computing;configuration item;configuration;logic;cooperation;algorithm	AI	-39.45991992085497	16.384298252588774	105211
c06915788f46aa0edb3515f9afb89d248f40a760	riki: a wiki-based knowledge sharing system for collaborative research projects	semantic wikis;collaborative tools;shared knowledge;support group;knowledge sharing;collaborative research;wikis;semantic wiki	During a collaborative research project, each member's knowledge and progress need to be managed and shared with other members. For effective knowledge sharing, each member needs to be able to express their own knowledge within the given project context and easily find and understand other members' knowledge. In this paper, we present our RIKI prototype that supports group communication and knowledge sharing in research projects via the Wiki-based platform. The main aim of RIKI implementation is to manage the shared knowledge semantically and to provide users with straightforward access to necessary information.	wiki	Sang Keun Rhee;Jihye Lee;Myon-Woong Park	2008		10.1007/978-3-540-70585-7_8	computer science;knowledge management;data mining;knowledge value chain;world wide web;domain knowledge	HCI	-45.47856665186051	5.597849421521599	105412
2a50dbf89ef28fc45849eb96c9b191167448af2b	an enterprise content management solution based on open source	web pages;relational database;enterprise content management;information organization;public sector;information integration;local governance;small and medium enterprise;non structural;open source software;open source	Spread out on the Internet, mail servers, and hard-drives everywhere, unstructured information in the form of Web pages, email, RSS feeds, office documents, images, video, and sound, accounts for approximateJy ten times the structured information stored in databases. Seeking to organize their large volume of non-structured content, companies implement Enterprise Content Management (ECM) systems to make this information more accessible for users and make these systems communicate witb reJational database backed solutions througb a single interface. Small and medium enterprises (SMEs) and local governments also have the same need for unstructured information organization. However, most of times, tbey cannot afford the high acquisition and customization costs, or don't want to become dependent of proprietary ECM solutions. This paper aims to present NSI2, an ECM solution totally built on top of open source software that offcrs the functionalities demanded by this kind of system.	database;email;enterprise content management;information retrieval;internet;knowledge organization;open-source software;rss;structured content;web page	Rogério Atem de Carvalho	2007		10.1007/978-0-387-75902-9_17	functional software architecture;enterprise system;enterprise application integration;enterprise systems engineering;enterprise software;nist enterprise architecture model;content management;knowledge management;architecture domain;integrated enterprise modeling;digital firm;enterprise architecture management;database;enterprise data management;enterprise architecture;enterprise integration;world wide web;enterprise planning system;enterprise information security architecture;enterprise information system;enterprise information integration;enterprise life cycle	Web+IR	-48.20540382501429	8.95019071853799	105574
a0fc4d56db063b6dc7334a70a6a12e994b196fda	a semantically enhanced service repository for user-centric service discovery and management	visualized service discovery;semantic services;swinburne;service ontology;user centricity;service repository	Article history: Received 14 May 2010 Received in revised form 28 October 2011 Accepted 31 October 2011 Available online 10 November 2011 User centricity represents a new trend in the currently flourishing service oriented computing era. By upgrading end-users to prosumers (producer+consumer) and involving them in the process of service creation, both service consumers and service providers can benefit from a cheaper, faster, and better service provisioning. The EU-IST research project OPUCE (Open Platform for User-Centric Service Creation and Execution) aims at building a unique service environment by integrating recent advances in networking, communication and information technology where personalized services can be dynamically created and managed by prosumers. This paper particularly discusses the design and development of a service repository, which is at the very core of the OPUCE platform. The repository consists of two main components: a fully functioned XML registry supporting facet-based access to service descriptions, and a novel semantic service browser that supports prosumers who are not technically experienced to explore and discover services in an intuitive and visualized manner. We demonstrate the benefits of our design by conducting usability and performance studies. © 2011 Elsevier B.V. All rights reserved.	context awareness;emergence;formal concept analysis;mashup (web application hybrid);mobile device;ontology (information science);ontology alignment;open platform;personalization;provisioning;semantic web;service discovery;service-oriented architecture;usability;web 2.0;world wide web;xml	Jian Yu;Quan Z. Sheng;Jun Han;Yanbo Wu;Chengfei Liu	2012	Data Knowl. Eng.	10.1016/j.datak.2011.10.005	service provider;service level requirement;mobile qos;service catalog;service product management;application service provider;differentiated service;knowledge management;service delivery framework;service design;data mining;database;service discovery;service desk;data as a service;customer service assurance;world wide web;service system	Web+IR	-46.515458708672156	13.573805000772778	105609
52469afe4098155d5d3172624e122458f9f8fffe	towards a multi-agent platform for automatic b2b exchanges	agent platform;commitments;business to business;interoperability multi agent platform automatic b2b exchange web technology information systems business to business exchange jade development framework;information systems;multi agent platform;multi agent system;business to business exchange;semantics;receivers;process mediation;multi agent systems;internet;monitoring;multi agent system business to business monitoring process mediation commitments;mediation;business data processing;business;jade development framework;interaction protocol;automatic b2b exchange;interoperability;information system;open systems;web technology;context;open systems business data processing information systems internet multi agent systems;multiagent systems;business receivers semantics mediation monitoring context multiagent systems	In the era of Internet-based services, companies increasingly use Web technologies to interact. Due to the inherent heterogeneity both at data and process level, ensuring interoperability between Information Systems is difficult. To automate electronic exchanges between businesses, the classical approach is to follow strictly at runtime a common interacting protocol that must be defined beforehand. This imposes a costly design time and a constrained runtime. Our goal is to propose a mediation platform that allows partners to engage directly into business-to-business exchanges. In this paper we present a multi-agent system developed with an institutional extension of the JADE development framework. Agents of the platform dynamically monitor the exchanges by interpreting business interacting actions according to each enterprise's business logic and behave in order to fulfill the business transaction.	business logic;elementary;game theory;information system;interaction;internet;interoperability;jade;multi-agent system;run time (program lifecycle phase)	Sébastien Picant;Fabrice Bourge;Abdel-Illah Mouaddib	2010	2010 22nd IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2010.73	computer science;knowledge management;artificial intelligence;multi-agent system;world wide web;information system	DB	-44.36745188694925	18.060520948189414	105699
7e0492c8f39081e9589ac0282a68c20222c35dc3	a configuration management task ontology for semantic integration	semantic integration;task ontology;configuration management	Configuration Management (CM) is an important task for developing complex products. It is a complex task and there are many CM systems that aim to support it. However, generally, these systems work in isolation and there is a need for integrating them. In this context, ontologies have an important role, acting as an inter-lingua to help achieving a shared conceptualization that allows semantic integration. This paper presents an ontology of the CM task. This ontology was built with the purpose of supporting semantic integration of CM systems, mainly in service and process layers of integration.	conceptualization (information science);configuration management;ontology (information science);semantic integration	Rodrigo Fernandes Calhau;Ricardo de Almeida Falbo	2012		10.1145/2245276.2245344	upper ontology;semantic integration;computer science;knowledge management;ontology;database;configuration management;ontology-based data integration;information retrieval;process ontology	AI	-43.37979135551602	7.974369271710756	105878
c78585870944c6607e076a726a7f63bb32253509	using event semantics for modeling contracts	electronic commerce contracts negotiation support systems;electronic commerce;contracts;business contracts on line support systems electronic contracting logical formalism human negotiator formal language for business communication event semantics flbc prolog;formal language for business communication;support system;contracts humans formal languages business communication buildings companies employment standardization costs data handling;negotiation support systems	Currently a number of these on-line support systems for electronic contracting are under development. In this paper we develop a logical formalism to represent the content of business contracts. This formalism can be used to develop applications that can automatically negotiate and process contracts, or it can be used to develop online help systems that explain to the human negotiator what for him the implications are of a certain contract that is proposed to him by the counter-party. The formalism we develop is based upon recent developments in the field of the Formal Language for Business Communication (FLBC) and event semantics. In the paper we show how many key constructs of the content of business contracts can be modeled using event semantics. We also show that the formalism can be implemented in Prolog.	formal language;formal system;online and offline;prolog;semantics (computer science)	Yao-Hua Tan;Walter Thoen	2002		10.1109/HICSS.2002.994149	e-commerce;computer science;knowledge management;database;world wide web;commerce	Web+IR	-44.89309304798772	17.74715314364805	106017
808244d89f58464443e5fe726a0a2c42cc473dfd	using jesstab to integrate protégé and jess	ontologies engines java knowledge representation pattern matching problem solving knowledge acquisition runtime environment virtual machining tcpip;ontology development;authoring systems;knowledge acquisition tool jesstab protege jess problem solvers ontology development knowledge modeling tools knowledge bases scripting environment knowledge representation;knowledge acquisition;knowledge representation knowledge based systems knowledge acquisition problem solving authoring systems;datavetenskap datalogi;computer science;knowledge representation;knowledge modeling;knowledge based systems;problem solving;knowledge base	Integration with external systems, such as problem solvers, is becoming increasingly important for ontology development and knowledge-modeling tools. The author's JessTab extension lets you write Jess programs that manage Protege ontologies and knowledge bases. Protege is a popular, modular ontology development and knowledge acquisition tool.	jess;protégé	Henrik Eriksson	2003	IEEE Intelligent Systems	10.1109/MIS.2003.1193656	natural language processing;knowledge representation and reasoning;knowledge base;knowledge integration;computer science;knowledge management;artificial intelligence;knowledge-based systems;knowledge engineering;open knowledge base connectivity;data mining;procedural knowledge;knowledge extraction;commonsense knowledge;domain knowledge	Embedded	-39.02577716301501	7.626558089284169	106133
53bd6ce27a0edcc43e44f888b10ab9964dbf8505	graphical xquery in the aqualogic data services platform	bea;xquery;graphical user interface;heterogeneous information;information integration;data services;xml;data access;graphic user interface;middleware;service oriented architecture	The AquaLogic Data Services Platform (ALDSP) is a middleware platform developed at BEA Systems for building services, referred to as data services, that integrate, access, and manipulate information coming from multiple heterogeneous sources of data (including databases, files, and other services). ALDSP uses functions that produce and consume XML to model heterogeneous information sources, so the integration logic for data access services in ALDSP is specified declaratively using the XQuery language. A challenge that we faced in developing ALDSP was providing effective graphical tooling to help data service developers to develop information integration queries. In this paper, we describe the graphical XQuery Editor (XQE) that resulted from our attempt to tackle this challenge. XQE handles the full XQuery language and provides a robust two-way editing experience involving both graphical and source views of each query. XQE is novel in being the first commercial graphical XQuery editor to support both of these features.	data access;database;graphical user interface;middleware;xml;xquery	Vinayak R. Borkar;Michael J. Carey;Sebu Koleth;Alexander Kotopoulis;Kautul Mehta;Joshua Spiegel;Sachin Thatte;Till Westmann	2010		10.1145/1807167.1807288	computer science;data mining;graphical user interface;database;world wide web	DB	-36.46521204637531	8.553166428895553	106304
e4e9601ce5d97ba260c9eb8b2052c33d1e0ecb38	revising wsdl documents: why and how, part 2	publishing web services data models information analysis documentation ports computers web services description language electronic publishing;document handling;wsdl;service architectures;publishing;data mining;services discovery process and methodology;web services data mining document handling;service discovery web service description language wsdl document top down method counterpart method bottom up method;web services modeling;web services;wsdl web services modeling service architectures web services description language web services publishing services discovery process and methodology;electronic publishing;ports computers;web services description language;web services publishing;information analysis;documentation;data models	In a previous article, the authors demonstrated that effectively discovering Web services helps developers avoid several common design errors in Web Service Description Language (WSDL) documents. Their proposed guidelines are unfortunately applicable only when publishers follow the top-down, or contract-first, method of building services, which isn't very popular due to its inherent costs. Here, they present an approach for preventing such errors when using a counterpart method - namely, bottom-up or code-first - and measure the approach's impact on service discovery. The rationale behind the study is that because code-first service interfaces are automatically generated by tools that - given a service implementation - deterministically map programming language constructs onto WSDL elements, the measurable properties of service implementations could influence resulting service interfaces.	design rationale;programming language;service discovery;top-down and bottom-up design;web services description language;web service	Cristian Mateos;Marco Crasso;Alejandro Zunino;José Luis Ordiales Coscia	2013	IEEE Internet Computing	10.1109/MIC.2013.4	documentation;computer science;data mining;database;publishing;electronic publishing;law;world wide web;universal description discovery and integration	Web+IR	-46.31587928892503	17.965115801572317	106325
4504e8417102122305914e00b0eb66703a372519	a meta-ontological framework for multi-agent systems design	multi agent system;info eu repo semantics article;ingenierias	The paper introduces an approach to using a meta-ontology framework for complex multi-agent systems design, and illustrates it in an application related to ecological-medical issues. The described shared ontology is pooled from private sub-ontologies, which represent a problem area ontology, an agent ontology, a task ontology, an ontology of interactions, and the multi-agent system architecture ontology.	interaction;multi-agent system;ontology (information science);systems architecture;systems design	Marina V. Sokolova;Antonio Fernández-Caballero	2007		10.1007/978-3-540-73055-2_54	upper ontology;conceptualization;ontology alignment;ontology inference layer;computer science;knowledge management;ontology;artificial intelligence;multi-agent system;data mining;database;ontology-based data integration;owl-s;process ontology;suggested upper merged ontology	AI	-44.482286353341394	9.564747428754599	106343
dfa7e57e95d4d2967926ded29648916fc4188df0	infogrid: providing information integration for knowledge discovery	engineering design;data integrity;wrappers;heterogeneous information resources;information referencing;heterogeneous information;information integration;distributed querying;middleware;annotating information;grids;data integration;knowledge discovery	Many scientific experiments produce large amounts of data using high-throughput devices. In order to analyse this type of data Knowledge Discovery systems are required. However, generic laboratory systems do not provide any contextual information about the system that is being studied. In these situations, Knowledge Discovery can be aided and validated by the use of Information integration tools. In this paper, we introduce InfoGrid, a data integration, middleware engine, designed to operate under a Grid framework. It focuses on providing information access services and offers all users a query system which is able to retain the familiarity with their specific scientific applications while being diverse, flexible and open at the same time. The assumption there is that defining a common language for all queries is not desirable. Using this design, we show how the InfoGrid architecture can be used to provide contextual features for a data table to be used for analysis (i.e. the Annotation Problem). We also show how it can be used to find relevant background knowledge for a user (i.e. the Information Comprehension problem). Both of these issues are repeatedly found in Knowledge Discovery tasks, which we illustrate with a worked example.	brushing and linking;data model;data point;data visualization;experiment;graphical user interface;grid computing;high-throughput computing;information access;interactive visualization;list comprehension;middleware;table (information);throughput;visual programming language;zero-knowledge proof	Nikolaos Giannadakis;Anthony Rowe;Moustafa Ghanem;Yike Guo	2003	Inf. Sci.	10.1016/S0020-0255(03)00170-1	computer science;information integration;data integration;middleware;data integrity;data mining;database;knowledge extraction;information retrieval;engineering design process	ML	-34.18644202966584	9.776920841341068	106419
efce7bf9da1c8a3652b06fd2d16f486edc6c8cdf	algorithms for integrating temporal properties of data in data warehousing	data warehousing	One of the most complex issues of the integration and transformation interface is the case where there are multiple sources for a single data element in the enterprise data warehouse. While there are many facets to the large number of variables that are needed in the integration phase, what we are interested in is the temporal problem. It is necessary to solve problems such as what happens when data from data source A is available but data from data source B is not. This paper presents our work into data integration in the Data Warehouse on the basis of the temporal properties of the data sources. Depending on the extraction method and data source, we can determine whether it will be possible to incorporate the data into the Data Warehouse. We shall also examine the temporal features of the data extraction methods and propose algorithms for data integration depending on the temporal characteristics of the data sources and on the data	algorithm;data element;data model;dreamwidth;requirement;system administrator	Francisco Araque;Alberto Salguero;Cecilia Delgado;Eladio Garví;José Samos	2006			data mining;computer science;data warehouse	DB	-34.739816222226494	10.487984027860412	106446
06ec4e7aef0a03b61f8c09f94bcb14e501286602	semantics based customization of ubl document schemas	context ontology;semantic representation;universal business language;semantics;semantic interoperability;ebusiness interoperability;universal business language ubl;business context;north american industry classification system;ontology alignment	Universal Business Language (UBL) is an OASIS initiative to develop common business document schemas to provide document interoperability in the eBusiness domain. Since the data requirements change according to a context, UBL schemas need to be customized and UBL defines a guideline to be followed for customization of schemas. XSD derivation based customization as proposed by UBL provides syntactic interoperability, that is, an XML parser that can interpret standard UBL documents can also interpret customized UBL documents. We argue that for UBL to become mainstream, syntactic interoperability alone is not enough. It needs to be supported by semantic interoperability, that is, it must be possible for users and even automated processes to discover and reuse customizations provided by other users. In this paper, we describe how to improve the UBL customization mechanism by providing semantic representations for context domains and describe how these semantics can be utilized by automated processes for component discovery and schema customization. For this purpose, we derive ontologies from taxonomies like the North American Industry Classification System (NAICS), the Universal Standard Products and Services Classification (UNSPSC) and relate corresponding concepts from different ontologies through ontology alignment. Then, we process these aligned ontologies using a reasoner to compute inferred ontologies representing context domains. We show that when custom UBL components are annotated using classes from these ontologies, automated discovery and customization becomes possible.	electronic business;knowledge base;ontology (information science);ontology alignment;requirement;semantic interoperability;semantic reasoner;taxonomy (general);web ontology language;xml	Yalin Yarimagan;Asuman Dogac	2007	Distributed and Parallel Databases	10.1007/s10619-007-7014-z	semantic interoperability;ontology alignment;computer science;data mining;database;semantics;north american industry classification system;world wide web	Web+IR	-42.039260017092964	4.834462751237577	106505
bf177260168b71e80e881c735b940fe259b54e9a	dstoolkit: an architecture for flexible dataspace management	dataspace management system;incremental improvement;dataspace lifecycle	The vision of dataspaces is to provide various of the benefits of classical data integration, but with reduced up-front costs. Combining this with opportunities for incremental refinement enables a ‘pay-as-yougo’ approach to data integration, resulting in simplified integrated access to distributed data. It has been speculated that model management could provide the basis for Dataspace Management, however, this has not been investigated until now. Here, we present DSToolkit, the first dataspace management system that is based on model management, and therefore, benefits from the flexibility provided by the approach for the management of schemas represented in heterogeneous models, supports the complete dataspace lifecycle, which includes automatic initialisation, maintenance and improvement of a dataspace, and allows the user to provide feedback by annotating result tuples returned as a result of queries the user has posed. The user feedback gathered is utilised for improvement by annotating, selecting and refining mappings. Without the need for additional feedback on a new data source, these techniques can also be applied to determine its perceived quality with respect to already gathered feedback and to identify the best mappings over all sources including the new one.	dataspaces;feedback;refinement (computing)	Cornelia Hedeler;Khalid Belhajjame;Lu Mao;Chenjuan Guo;Ian Arundale;Bernadette Farias Lóscio;Norman W. Paton;Alvaro A. A. Fernandes;Suzanne M. Embury	2012	Trans. Large-Scale Data- and Knowledge-Centered Systems	10.1007/978-3-642-28148-8_6	computer science;data mining;database;world wide web	DB	-47.248742155376895	7.7417155331531475	106538
6d4fcb92a51fa77e2711c7121500de865154a219	declaratively querying and visualizing knowledge bases in xml	owl;prolog;rule based;xml query transformations;visualization;rule bases;visualization technique;visual inspection;knowledge systems;xml document;knowledge base;knowledge engineering	The maintenance of large knowledge systems usually is a rath er complex task. In this paper we will show that extensions or modifi cations of a knowledge base can be supported by appropriate visualizations te chniques, e.g. by illustrating dependencies of the considered knowledge. In particular, we introduce a declarative approach for quer ying and visualizing rule-based knowledge represented as X ML documents; a knowledge engineer can extract and visually inspect parts of the knowledge base by a d-hoc declarations in a flexible manner.	automated planning and scheduling;declarative programming;gxl;html;hoc (programming language);interactivity;knowledge base;knowledge engineer;knowledge-based systems;logic programming;usability;xml	Dietmar Seipel;Joachim Baumeister;Marbod Hopfner	2004		10.1007/11415763_2	xml validation;knowledge base;xml;visualization;computer science;artificial intelligence;knowledge-based systems;knowledge engineering;open knowledge base connectivity;data mining;database;knowledge extraction;prolog;information retrieval;visual inspection	DB	-36.87099197919748	7.113404727259594	106677
a5fa151959aec26b0de87fc37e5227b7270e0ac3	vacuuming xml	transaction time;xml;temporal	A transaction-time XML database archives XML data versions to create a complete history of the data and processes transaction-time queries to fetch requested versions of data. The database will grow over time as new versions are created, and its unrestrained growth will eventually exceed storage capacity. To become an autonomous, self-managing database this growth must be controlled by vacuuming versions of data, which reclaims space and prevents the data from exceeding storage capacity. Vacuuming policies specify which versions to vacuum. This paper develops micro and macro vacuuming policies for an XML data collection. A micro vacuuming policy targets specific elements in the data, while a macro policy applies to the entire collection. We show how the mix of policies can effectively manage the growth of a database. Vacuuming hierarchical data differs from vacuuming relational data since vacuuming data high in the hierarchy also potentially removes data versions, not explicitly vacuumed, lower in the hierarchy. Vacuuming controls the growth of the archive by sacrificing the completeness of the data's history. Some requests can no longer be satisfied since a requested version might have been vacuumed. An autonomous transaction-time database must also be self-repairing, so this paper also presents a query repair strategy to redirect queries to data that has not been vacuumed. All of the new functionalities were designed to be backwards compatible with existing protocols (e.g., SOAP) and standards (e.g., XML), so a data collection can become a vacuum-enhanced, transaction-time collection at any time.	archive;autonomous robot;backward compatibility;hierarchical database model;redirection (computing);soap;transaction time;vacuum cleaner;xml database	Curtis E. Dyreson	2014	2014 IEEE 11th Intl Conf on Ubiquitous Intelligence and Computing and 2014 IEEE 11th Intl Conf on Autonomic and Trusted Computing and 2014 IEEE 14th Intl Conf on Scalable Computing and Communications and Its Associated Workshops	10.1109/UIC-ATC-ScalCom.2014.36	xml;computer science;operating system;transaction time;data mining;database;world wide web;computer security	DB	-34.67824698783802	15.529083184395416	106844
48d4ca5353b38794375859a8c459dd2a421a1015	a fuzzy evaluation system for web services selection using extended qos model	model design;cybernetics;fuzzy measure;service selection;system theory;web service;fuzzy logic;worldwide web;systems theory;distributed environment;quality evaluation;quality criteria;quality of service	Purpose – This paper aims to define an extended QoS model to accurately describe the quality of web service in the open distributed environment and propose a fuzzy evaluation approach for services selection based on the extended QoS model.Design/methodology/approach – The extended QoS model classifies the quality criteria of web service as five composite quality criteria, and each composite quality criterion is composed by one or more sub‐quality criteria. Considering the multiple forms of representation for the quality criteria and different types of quality value could not be compared directly, a scaling fuzzy measure for quality criteria is introduced. Based on the scaling fuzzy measure of the quality criteria, a fuzzy synthetic evaluation system for services selection is proposed.Findings – The quality of web service has multiple facets and multiple forms of representation. The fuzzy synthetic evaluation system can deal well with the fuzzy and implicit concepts about quality evaluations and provides h...		Hongxia Tong;Jian Cao;Shensheng Zhang;Yujie Mou	2009	Kybernetes	10.1108/03684920910944236	cybernetics;computer science;artificial intelligence;data mining;systems theory	Theory	-46.51333296377161	15.10349986663587	107414
63456c7eb1024428da30d24e4bb046e2f9eb2253	a framework for linking and pricing no-cure-no-pay services	incremental computation;information systems design information systems implementation;electronic commerce;web service;formal languages formal systems formal proof of program correctness;information system;petri nets;information system design;petri net;information systems internet;formal language;program correctness	In this paper, we present a framework that allows us to orchestrate web services such that the web services involved in this orchestration interact properly. To achieve this, we predefine service interfaces and certain routing constructs. Furthermore, we define a number of rules to incrementally compute the price of such a properly interacting orchestration (i.e. a web service) from the price of its web services. The fact that a web service gets only payed after its service is delivered (no-cure-no-pay) is reflected by considering a probability of success. To determine a safe price that includes the risk a web service takes, we consider the variance of costs.	interaction;iteration;orchestration (computing);regular language description for xml;routing;web framework;web service	Kees M. van Hee;H. M. W. Verbeek;Christian Stahl;Natalia Sidorova	2009	Trans. Petri Nets and Other Models of Concurrency	10.1007/978-3-642-00899-3_11	web service;web modeling;business process execution language;web standards;computer science;knowledge management;service delivery framework;ws-policy;ws-addressing;database;distributed computing;universal description discovery and integration	Web+IR	-45.02290464723074	15.902493676658127	107595
380f4c0e51a4767cb178d03a3b5d0577639dfa8a	service-level interoperability issues of platform as a service	cloud apis;service composition;ontology driven mediation;platform as a service ontologies interoperability mediation java semantics;semantics;google app engine service level interoperability issues platform as a service application programming interface api user information ontology driven data mediation web services service owl ontology web ontology language sawsdl xslt cross paas concepts java web application vosao content management system;web services application program interfaces ontologies artificial intelligence open systems;ontology driven mediation platform as a service interoperability service composition cloud apis;mediation;platform as a service;ontologies;interoperability;java	The numerous heterogeneities among different providers make platform as a service interoperability an interesting and complex research and practical problem. For example, each provider offers its own remote application programming interfaces (APIs). The main aim of this paper is to identify and address service-level interoperability issues when using APIs from different commercial providers of platform as a service. First, we define use case to add current user information from one platform as a service offer to the application hosted on another offer. To address interoperability problems, the ontology driven data mediation will be used and tested in this use case. Remote vendors' APIs are implemented as web services. Resulting web operations and their inputs/outputs are semantically annotated using cross-PaaS concepts from the developed platform as a service OWL ontology. Next, SAWSDL and XSLT are used to define service type mappings. Actual composition of platform as a service APIs is implemented by means of AI planner and developed Java web application. Testing and validation was performed on a case where current Sales force's user is added to data container in Vosao content management system deployed on Google App Engine. Novelty of the paper is a specific application domain (composition of operations defined in PaaS APIs) and new algorithm for identification of interoperability problems.	algorithm;application domain;application programming interface;automated planning and scheduling;business process execution language;content management system;google app engine;interoperability;java;lambda lifting;platform as a service;sawsdl;service composability principle;software as a service;web ontology language;web application;web service;xml;xslt	Darko Androcec;Neven Vrcek;Peep Küngas	2015	2015 IEEE World Congress on Services	10.1109/SERVICES.2015.60	upper ontology;web service;semantic interoperability;computer science;service delivery framework;data mining;database;world wide web	Web+IR	-43.12877761735364	12.987594109877733	107750
94ada084394799421b5301889ba3b674035a840e	towards action-oriented continuous queries in pervasive systems	services;data streams;continuous query;non-conventional data sources;pervasive systems;query opti- mization;query optimization;legacy system;relational data;information system	Pervasive information systems give an overview of what digital environments should look like in the future. From a data-centric point of view, traditional databases have to be used alongside with non-conventional data sources like data streams, services and events to deal with new properties of such information systems including dynamicity, autonomy and decentralization. In this context, the definition of continuous queries combining standard relations, data streams and services in a declarative language extending SQL is clearly an ambitious and motivating goal. Those continuous queries could express an event management functionality (e.g. event filtering, event composition), associate events with data from legacy systems, and perform cost-based optimal calls to services. In this paper, we define virtual tables with binding patterns to represent services of the pervasive environment. By the way, relations, data streams and virtual tables can be homogeneously queried using a SQL-like language, on top of which query optimization can be performed. We also introduce a new clause defining the optimizing criteria to dynamically choose the best way to handle each event. A prototype on top of STREAM, a DBMS devoted to data streams, has been devised on which first experiments have been carried out on synthetic data.	autonomy;benchmark (computing);blocking (computing);contextual query language;database;decentralized computing;declarative programming;experiment;hoc (programming language);information system;legacy system;mathematical optimization;name binding;non-blocking algorithm;pervasive informatics;procedural programming;prototype;query optimization;query plan;sql;synthetic data;ubiquitous computing;virtual method table	Yann Gripay;Frédérique Laforest;Jean-Marc Petit	2007				DB	-34.64365211388747	9.223077135456183	108002
0525abb7d0c08af5228429ee73333eccdf2128cc	a weighted-tree similarity algorithm for multi-agent systems in e-business environments	multi agent system;e business;relfun;arc labeled trees;object oriented ruleml;arc weighted trees;business environment;object oriented;e learning;production services;functional logic programming;e learning environment;semantic matching;similarity measure;buyer seller matching	A tree similarity algorithm for match-making of agents in e-Business environments is presented. Product/service descriptions of seller and buyer agents are represented as node-labelled, arc-labelled, arc-weighted trees. A similarity algorithm for such trees is developed as the basis for semantic match-making in a virtual marketplace. The trees are exchanged using an XML serialization in Object-Oriented RuleML. Correspondingly, we use the declarative language Relfun to implement the similarity algorithm as a parameterised, recursive functional program. Three main recursive functions perform a top-down traversal of trees and the bottom-up computation of similarity. Results from our experiments aiming to match buyers and sellers are found to be effective and promising for eBusiness/e-Learning environments. The algorithm can be applied in all environments where weighted trees are used.	algorithm;bottom-up proteomics;computation;declarative programming;electronic business;experiment;functional programming;recursion;ruleml;serialization;top-down and bottom-up design;tree traversal;xml	Virendrakumar C. Bhavsar;Harold Boley;Lu Yang	2004	Computational Intelligence	10.1111/j.0824-7935.2004.00255.x	computer science;theoretical computer science;machine learning;functional logic programming;multi-agent system;electronic business;database;object-oriented programming	AI	-42.56184728322351	14.195273587084559	108029
7b09150924ba154da97cd6ed2a4af8a302c94a6b	adaptive process execution in a service cloud: service selection and scheduling based on machine learning	business data processing;learning artificial intelligence;cloud computing	Given a process specification, it is a complex task to dynamically select constituent services and compose them in an execution plan to satisfy users' non-functional preferences. Process scheduling approaches assume users can clearly specify their non-functional preferences and there are formulas (e.g., utility functions) to compute process level QoS from the QoS of constituent services and their connections. However, these assumptions are not always true. Users' preferences can be subjective, implicit, vague, mixed and different for various types of processes. Besides, not all the preferences for example easy-to-use can be computed using formulas. We proposed a machine learning based approach to evolutionarily learn user preferences according to their ratings on historical execution plans, recommend existing or generate new execution plans for business processes that adapt to user preferences.	business process;cluster analysis;cold start;computation;execution pattern;k-means clustering;logistic regression;machine learning;process specification;quality of service;query plan;response time (technology);responsiveness;scheduling (computing);type signature;user (computing);vagueness	Dhanwant S. Kang;Hua Liu;Munindar P. Singh;Tong Sun	2013	2013 IEEE 20th International Conference on Web Services	10.1109/ICWS.2013.51	real-time computing;cloud computing;computer science;operating system;data mining;database	DB	-46.49300493738554	16.097949180405607	108470
f681de3647ad6f2a1893c594a6b9d0e738030324	automatic ontology learning from heterogeneous relational databases: application in alimentation risks field		In this paper, we propose a semantic approach for automatic ontology learning from heterogeneous relational databases in order to facilitate their integration. The semantic enrichment of heterogeneous databases, which cover the same domain, is essential to integrate them. Our approach is based on Wordnet and Wup’s measure for measuring the semantic similarity between elements of these databases. It is described by a detailed process that can allow not only the generation of ontology but also its evolution as the evolution of its databases. We applied our approach in the alimentation risks field that is characterized by a large number of scientific databases. The developed prototype has been compared with similar tools of generation ontology from databases. The result confirms the quality of our prototype that returns the generic ontology from many relational databases.	ontology learning;relational database	Aicha Aggoune	2018		10.1007/978-3-319-89743-1_18	computer science;semantic similarity;artificial intelligence;machine learning;data mining;ontology learning;relational database;ontology;wordnet	NLP	-39.253838065916014	4.494470144033839	108612
7d63b3137f06b85dd8eae1a1f3b4de9f9c935293	fuzzy semantic web languages and beyond		The aim of this talk is to present the state of the art in representing and reasoning with fuzzy knowledge in Semantic Web Languages such as triple languages RDF/RDFS, conceptual languages of the OWL 2 family and rule languages. We further show how one may generalise them to so-called annotation domains, that cover also e.g. temporal and provenance extensions.	conjunctive query;fuzzy concept;fuzzy set;ontology (information science);quantifier (logic);rdf schema;resource description framework;semantic web;web ontology language;web framework	Umberto Straccia	2017		10.1007/978-3-319-60042-0_1	social semantic web;rdf;semantic computing;natural language processing;web ontology language;database;semantic web rule language;semantic analytics;semantic web;artificial intelligence;semantic web stack;computer science	AI	-37.91744777833169	6.600179506562497	108615
8cd2b1602f6ab7c689a3f732da320fe246f49229	swift linked data miner: mining owl 2 el class expressions directly from online rdf datasets		In this study, we present Swift Linked Data Miner, an interruptible algorithm that can directly mine an online Linked Data source (e.g., a SPARQL endpoint) for OWL 2 EL class expressions to extend an ontology with new SubClassOf: axioms. The algorithm works by downloading only a small part of the Linked Data source at a time, building a smart index in the memory and swiftly iterating over the index to mine axioms. We propose a transformation function from mined axioms to RDF Data Shapes. We show, by means of a crowdsourcing experiment, that most of the axioms mined by Swift Linked Data Miner are correct and can be added to an ontology. We provide a ready to use Protégé plugin implementing the algorithm, to support ontology engineers in their daily modeling work.	algorithm;communication endpoint;crowdsourcing;download;linked data;mined;protégé;sparql;swift (programming language);web ontology language	Jedrzej Potoniec;Piotr Jakubowski;Agnieszka Lawrynowicz	2017	J. Web Sem.	10.1016/j.websem.2017.08.001	swift;rdf;web ontology language;protégé;linked data;ontology learning;data mining;database;sparql;computer science;expression (mathematics)	Web+IR	-36.52852248622348	5.110427077016187	108716
a31529db65beabd2b43b8aed5b78b95f4c185234	the table and the tree: on-line access to relational data through virtual xml documents	relational data;data integrity;relational database;e commerce;xml document	"""For speed and convenience, applications routinely cache XML data locally, and access it through standard parser (SAX) or tree (DOM) interfaces. When the source of this data is a relational database, the consistency and integrity guarantees of the database are sacrificed for speed. We present the ROLEX system (standing for Relational On- Line Exchange with XML) which provides applications """"live"""" virtual XML views of relational data through standard interfaces. This technology, combined with a main-memory database platform, promises to provide data integrity, consistent interoperation with relational applications, and the performance required by busy web or e-commerce applications."""	xml	Philip Bohannon;Henry F. Korth;P. P. S. Narayan	2001			streaming xml;database;data mining;xml encryption;document structure description;computer science;relational database;xml schema editor;xml database;xml validation;efficient xml interchange	DB	-35.182048081872075	16.406414896109936	108871
ec1b59554b258546be5a3f4d294ad0a78903ca14	patterns 2.0: a service for searching patterns		With ever-increasing number of patterns in the literature and online repositories, it can be hard for non-experts to know about new patterns and select patterns appropriate to their needs. We argue that a systematic way for searching patterns is required and we present the Patterns 2.0 service, a composite software service for facilitating pattern search and selection. The service combines several pattern-related services with a recommendation service that allows users to share their experiences in using patterns. The contributions of the paper are: the overview of existing services related to the problem of pattern selection, the definition of Patterns 2.0 service, and description of its possible uses.	levenberg–marquardt algorithm;pattern search (optimization);service-oriented modeling;software as a service	Aliaksandr Birukou;Michael Weiss	2009			data mining;knowledge management;pattern search;computer science;service (systems architecture)	Web+IR	-46.025102517470046	13.687422389646427	108908
acf6201ff1407c54486bc2cf9c3ef603b1a81b32	xrl: a xml-based query language for advanced services in digital libraries	document structure;analisis contenido;query language;estructura de documento;information retrieval;structure document;digital library;xml language;pregunta documental;lenguaje interrogacion;question documentaire;content analysis;biblioteca electronica;recherche documentaire;recherche information;recuperacion documental;query;langage base donnee;langage interrogation;electronic library;document retrieval;recuperacion informacion;analyse contenu;content based retrieval;database languages;recherche par contenu;langage xml;lenguaje xml;bibliotheque electronique	In this paper we present a new XML-based query language for XML documents denoted XRL. This language expresses database conditions concerning the attributes and the structure of documents, as well as Information Retrieval conditions over their contents and their relevance. XRL queries can be stored into a XML repository and manipulated as any other XML document. In order to illustrate the usefulness of this language, we also describe some general guidelines of its current implementation, and present an example application. This application consists in a subscription/notification service of news articles, which are periodically retrieved from a digital library of newspapers according to the preferences of each user.	digital library;information retrieval;java servlet;mathematical optimization;notification service;query language;relevance;xml;xml database;xpath;xquery	Juan Manuel Vidal Pérez;María José Aramburu Cabo;Rafael Berlanga Llavori	2002		10.1007/3-540-46146-9_30	document retrieval;xml validation;digital library;content analysis;computer science;document structure description;xml framework;xml database;xml schema;database;xml signature;world wide web;xml schema editor;information retrieval;query language;efficient xml interchange	DB	-33.91211773131977	7.684752361253071	109002
7163b8ebad76587a2a1599dfa9df96a338e89ecf	ontology query answering on databases	busqueda informacion;gestion memoire;ontologie;base donnee;logical framework;backward chaining;chainage avant;chainage arriere;information retrieval;storage management;web semantique;epistemologie;interrogation base donnee;customization;database;service web;personnalisation;interrogacion base datos;base dato;semantics;base connaissance;constraint integrity;logical programming;web service;semantica;semantique;gestion memoria;datalog;programmation logique;recherche information;web semantica;inferencia;integrity constraints;personalizacion;epistemology;semantic web;integrite contrainte;base conocimiento;ontologia;forward chaining;query answering;epistemologia;integridad constrenimiento;programacion logica;ontology;database query;inference;servicio web;knowledge base	With the fast development of Semantic Web, more and more RDF and OWL ontologies are created and shared. The effective management, such as storage, inference and query, of these ontologies on databases gains increasing attention. This paper addresses ontology query answering on databases by means of Datalog programs. Via epistemic operators, integrity constraints are introduced, and used for conveying semantic aspects of OWL that are not covered by Datalog-style rule languages. We believe such a processing suitable to capture ontologies in the database flavor, while keeping reasoning tractable. Here, we present a logically equivalent knowledge base whose (sound and complete) inference system appears as a Datalog program. As such, SPARQL query answering on OWL ontologies could be solved in databases. Bi-directional strategies, taking advantage of both forward and backward chaining, are then studied to support this kind of customized Datalog programs, returning exactly answers to the query within our logical framework.	abox;backward chaining;bi-directional text;cobham's thesis;data integrity;database;datalog;digital light processing;inference engine;knowledge base;logical framework;mathematical optimization;ontology (information science);resource description framework;sparql;scalability;semantic web;web ontology language;web application	Jing Mei;Li Ma;Yue Pan	2006		10.1007/11926078_32	web service;backward chaining;knowledge base;logical framework;forward chaining;computer science;sparql;artificial intelligence;semantic web;ontology;data integrity;data mining;database;semantics;rdf query language;datalog;world wide web;query language	DB	-36.67135805778473	11.04692053562114	109060
502519596c4a080c625bbdefe3e17a9c677dd6a5	challenges in managing implicit and abstract provenance data: experiences with provmanager		Running scientific workflows in distributed and heterogeneous environments has been motivating the definition of provenance gathering approaches that are loosely coupled to workflow management systems. We have developed a provenance management system named ProvManager to manage provenance data in distributed and heterogeneous environments independent of a specific Scientific Workflow Management System. The experience of using ProvManager in real workflow applications has shown many provenance management issues that are not addressed in current related work. We have faced challenges such as the necessity of dealing with implicit provenance data and the lack of higher provenance abstraction levels. This paper discusses and points to directions towards these challenges, contextualizing them according to our experience in developing ProvManager.	experiment;loose coupling;management system;operating system	Anderson Marinho;Marta Mattoso;Cláudia Maria Lima Werner;Vanessa Braganholo;Leonardo Gresta Paulino Murta	2011			workflow management system;management system;abstraction;provenance;computer science;knowledge management;workflow	DB	-46.70186417971337	9.75346194583758	109097
fdc42b161bb20b6d57eace26c05675fb0e852c6e	web service apis for scribe registrars, nexus diristries, portal registries and doors directories in the npd system		The Nexus-PORTAL-DOORS System (NPDS) has been designed with the Hierarchically Distributed Mobile Metadata (HDMM) architectural style to provide an infrastructure system for managing both lexical and semantic metadata about both virtual and physical entities. We describe version 0.8 of NPDS, including the separation of concerns between the original Problem-Oriented Registry of Tags And Labels (PORTAL) registries and the Domain Ontology Oriented Resource System (DOORS) directories, the combined registry and directory functionality of Nexus diristries, and the RESTful read-only web service API through which resource representation metadata records can be retrieved from these NPDS servers. We also introduce Scribe registrars with a corresponding RESTful read-write web service API for management of metadata records by both software agents accessing the web services directly and human users accessing them indirectly via web applications.	application programming interface;directory (computing);entity;new product development;ontology (information science);rational doors;read-only memory;read-write memory;representational state transfer;semantic web;separation of concerns;server (computing);software agent;uniform resource identifier;web application;web service	Adam Craig;Seung Ho Bae;Teja Veeramacheneni;Koby Taswell;Carl Taswell	2016			world wide web;web service;database;doors;nexus (standard);engineering	Web+IR	-41.86541073611628	12.07656695279628	109128
ee983f1225560e5cbd4504336cffeaf1383fa800	towards usable and interoperable workflow provenance: empirical case studies using pml		In this paper, we describe how a semantic webbased provenance Interlingua called the Proof Markup Language (PML) has been used to encode workflow provenance in a variety of diverse application areas. We highlight some usability and interoperability challenges that arose in the application areas and show how PML was used in the solutions.	best practice;encode;global variable;information exchange;interoperability;markup language;ontology (information science);semantic web;usability	James Michaelis;Li Ding;Zhenning Shangguan;Stephan Zednik;Rui Huang;Paulo S Pinheiro;Nicholas Del Rio;Deborah L. McGuinness	2009			computer science;data mining;database;world wide web	HCI	-40.845361176005866	5.474475591645471	109329
d8bc73ecf633b26af87ebf90f117f712c12a1e53	a common data manipulation language for nested data in heterogeneous environments	distributed and heterogeneous queries;data query languages;type systems;programming languages	One key aspect of data-centric applications is the manipu- lation of persistent data repositories, which is moving fast from querying a centralized relational database to the ad- hoc combination of constellations of data sources. Query languages are being typefuly integrated in host, general purpose, languages in order to increase reasoning and optimizing capabilities of interpreters and compilers. However, not much is being done to integrate and orches- trate different and separate sources of data. We present a common data manipulation language, that abstracts the nature and localization of the data-sources. We define its semantics and a type directed compilation, query optimization, and query orchestration mechanism to be used in development tools for heterogeneous environments. We provide type safety and language integration. Our approach is also suitable for an interactive query construction environment by rich user interfaces that pro- vide immediate feedback on data manipulation operations. This approach is currently the base for the data layer of a development platform for mobile and web applications.	centralized computing;compiler;data manipulation language;global variable;hoc (programming language);interpreter (computing);mathematical optimization;programming tool;query optimization;relational database;type safety;user interface;web application	João Costa Seco;Hugo Lourenço;Paulo Ferreira	2015		10.1145/2815072.2815074	query optimization;web query classification;data manipulation language;data control language;computer science;theoretical computer science;database;rdf query language;programming language;view;query language	DB	-34.181691086333615	8.842637686152502	109489
0bd832b9a0ecbe9fdf19853e34657e8a2b971b87	a general architecture for connecting nlp frameworks and desktop clients using web services	text mining;text analysis;web service;software engineering;semantic web;service oriented architecture	Despite impressive advances in the development of generic NLP frameworks, content-specific text mining algorithms, and NLP services, little progress has been made in enhancing existing end-user clients with text analysis capabilities. To overcome this software engineering gap between desktop environments and text analysis frameworks, we developed an open service-oriented architecture, based on Semantic Web ontologies and W3C Web services, which makes it possible to easily integrate any NLP service into client applications.	algorithm;desktop computer;natural language processing;ontology (information science);semantic web;service-oriented architecture;service-oriented device architecture;software engineering;text mining;web service	René Witte;Thomas Gitzinger	2008		10.1007/978-3-540-69858-6_31	web service;text mining;web modeling;computer science;service-oriented architecture;semantic web;social semantic web;web page;data mining;semantic web stack;database;world wide web	Web+IR	-41.216857092633305	7.743962089598984	109711
7181fb991ac0a191eff1d383d127c17b4ba2cc1c	the research and design of the semantic search engine based on ontology	text retrieval;retrieval efficiency;traditional search engine;semantic search engine;poor semantic processing capability;semantic search engine framework;semantic processing;semantic web;search engines;semantic search	The paper brings forth a semantic search engine framework based on ontology, the technology overcomes traditional search engine's shortcomings such as poor semantic processing capability and understanding capability because of the adoption of text retrieval and greatly lifts the retrieval efficiency.	document retrieval;semantic search;web search engine	Qi Yong;Peijie Hao;Yu Hou;Ya-nan Qiao	2007	Third International Conference on Semantics, Knowledge and Grid (SKG 2007)	10.1109/SKG.2007.124	semantic data model;natural language processing;upper ontology;semantic interoperability;search engine indexing;semantic computing;query expansion;semantic integration;explicit semantic analysis;semantic search;semantic memory;semantic grid;computer science;semantic web;concept search;social semantic web;data mining;semantic web stack;semantic compression;semantic technology;web search query;world wide web;information retrieval;semantic analytics;search engine	DB	-40.564862306919935	7.160555805370015	109789
e43fd340b3ea6c874374c16bb587795a9c3c3122	evaluation of qos-based web service matchmaking algorithms	constraint programming qos based web service matchmaking algorithms quality of service discovery mechanisms;filtering;sorting;prototypes;discovery mechanisms;performance;software performance evaluation;web service;web service matchmaking;qos;qos based web service matchmaking algorithms;mixed integer program;precision;web services constraint handling quality of service software performance evaluation;engines;web services;recall;constraint programming;mixed integer programming;constraint handling;ontologies;evaluation;difference set;mixed integer programming web service qos evaluation performance precision recall web service matchmaking constraint programming;quality of service;potential function;algorithm design and analysis	Web service (WS) discovery is a prerequisite for achieving WS composition and orchestration. Although a lot of research has been conducted on the functional discovery of WSs, the proposed techniques fall short when faced with the foreseen increase in the number of (potentially functionally-equivalent) WSs. The above situation can be resolved with the addition of non-functional (quality of service (QoS)) discovery mechanisms to WS discovery engines. QoS-based WS matchmaking algorithms have been devised for this reason. However, they are either slow - as they are based on ontology reasoners - or produce inaccurate results. Inaccuracy is caused both by the syntactic matching of QoS concepts and by wrong matchmaking metrics. In this paper, we present two constraint programming (CP) QoS-based WS discovery algorithms for unary constrained WS specifications that produce accurate results with good performance. We also evaluate these algorithms on matchmaking time, precision and recall in different settings in order to demonstrate their efficiency and accuracy.	algorithm;categorization;constraint programming;precision and recall;quality of service;unary operation;ws-discovery;web service	Kyriakos Kritikos;Dimitris Plexousakis	2008	2008 IEEE Congress on Services - Part I	10.1109/SERVICES-1.2008.53	computer science;data mining;database;world wide web	DB	-44.72739923577211	14.538823209498428	109977
c968941f9f169fb72afa6dfa2ed40bd02b4d0588	using the extensible markup language (xml) as a medium for data exchange	extensible markup language;selected works;data exchange;bepress	The amount of information being collected and stored electronically continues to increase as does the need to share this data among disparate applications and non-compatible computer systems. The eXtensible Markup Language (XML) was introduced to meet this challenge by providing a standardized way to exchange data. XML is being adopted rapidly, and is positioned to thrive in the electronic marketplace. A main premise behind Microsoft's .Net strategy and the recent release of Sun’s J2EE platform is the belief that XML marks a turning point in the evolution of the Internet and computing architectures. The power behind XML is its simplicity; however there is still much confusion about this technology. XML will evolve as its structure, its strengths, its weaknesses, and how it can be used more effectively are better understood. This paper includes an overview of XML and XML specifications and corresponding components, technical implementation requirements, the development of schemas for defining industry standard data definitions, a scenario employing XML technologies, and a discussion of the potential impact of XML on information systems.	information system;internet;java platform, enterprise edition;markup language;requirement;technical standard;xml	Meg Murray	2002	CAIS	10.17705/1CAIS.00907	ruleml;data exchange;xml validation;xml encryption;xml base;xml;xml schema;geography markup language;streaming xml;computer science;document type definition;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;world wide web;xml schema editor;cxml;efficient xml interchange;sgml	DB	-39.818879057485304	12.88166820325354	110194
d5aafa651be2f1e14d1c249fb0e0885b667b7d96	light-weight composition of personal documents from distributed information	web annotation;document composition	Digital information is typically distributed across several resources, but users are annoyed by the need to deal with different formats and applications for its retrieval, processing and creation. Corporate solutions for document composition are heavy-weight and unwieldy for extemporaneous usage. On the contrary, we propose a light-weight interaction framework, in which document specification results from the selection of existing resources, also comprising annotations retrieved from the Web. Style-sheets allow on-the-fly generation of actual documents.		Danilo Avola;Paolo Bottoni;Riccardo Genzone	2011		10.1007/978-3-642-21530-8_17	computer science;database;world wide web;information retrieval	NLP	-40.79724607271453	9.15232126506868	110290
2efa50d9d8fd3bdad076fa518ab0363b45432a6e	queries, the missing link in automatic data integration		This paper introduces the ontology mapping approach of a system that automatically integrates data sources into an ontology-based data integration system (OBDI). In addition to the target and source ontologies, the mapping algorithm requires a SPARQL query to determine the ontology mapping. Further, the mapping algorithm is dynamic: running each time a query is processed and producing only a partial mapping sufficient to reformulate the query. This approach enables the mapping algorithm to exploit query semantics to correctly choose among ontology mappings that are indistinguishable when only the ontologies are considered. Also, the mapping associates paths with paths, instead of entities with entities. This approach simplifies query reformulation. The system achieves favorable results when compared to the algorithms developed for Clio, the best automated relational data integration system. We have developed an Ontology-based Data Integration (OBDI) system that departs from the conventional OBDI organization. The goal is to include automatic integration of new data sources, provided those data sources publish a self-describing ontology. A consequence of that goal is there is no longer the opportunity for an engineer to review and correct an ontology matching prior to its use by the query reformulation system. As ontology matching is understood to be an uncertain process, some other method of mapping refinement is needed. Our system uses queries for this purpose. Ontology mapping in conventional OBDI systems is determined prior to, and without knowledge of the queries to be executed [3]. A static representation of a mapping between target and source ontologies serves as input to a query reformulation module (Fig. 1(a)). In the system described here, ontology mapping is a dynamically computed component whose result depends on the query that is being processed (Fig. 1(b)). In effect, the query becomes a third argument to the ontology mapping algorithm. The query provides context for selecting among competing mappings. Since a mapping is specific to a query, the results may be limited to the partial mapping required by the query reformulation system. The organization was motivated by the following observations. A mapping method may determine that an entity in one ontology maps with equal likelihood to two or more entities in the other ontology. The mapping and reformulation of certain queries is correct only if one pairing is chosen. The correct choice may be different for different queries. The query itself may lend additional semantics that correctly resolve the ambiguity. These observations are supported by the example in Fig. 2. Looking at the ontologies alone, there is insufficient information to determine if the class T :People should Ontology Mapping Query Reformulation Ontology T Query q Ontology S Reformulated query (a) Traditional Ontology Mapping Query Reformulation Ontology T Query q Ontology S Reformulated query	algorithm;entity;map;ontology (information science);ontology alignment;ontology-based data integration;refinement (computing);sparql;self-documenting code;semantic integration;web ontology language	Aibo Tian;Juan Sequeda;Daniel P. Miranker	2012			query optimization;computer science;data mining;database;ontology-based data integration;information retrieval;data mapping	Web+IR	-36.103709921668795	5.988008579546523	110315
1c158cfdc3bab396c820518dcfa6c66f00dcb783	proposal of a goal-oriented shared catalog model	databases;libraries;knowledge database shared catalog linked data extensive catalog model data detector;detectors;knowledge database;catalogs libraries databases next generation networking detectors data models resource description framework;linked data;catalogs;cataloguing;resource description framework;goal oriented shared catalog model;semantic web cataloguing learning by example;learning by example;shared catalog;data detector;semantic web;model;extensive catalog;next generation networking;thematic knowledge database;learning by example goal oriented shared catalog model linked data semantic web data detector thematic knowledge database;data models	This research aims to explore the integration possibilities of forms and techniques taken from Linked data and semantic web initiatives in the context of vertical applications, such as shared catalog systems. A theoretical model based on the combination of data detectors, learning by examples applications (LBE) and thematic knowledge databases is proposed as a valid methodology to explore in order to provide a next generation of shared catalogs.	computer simulation;database;information retrieval;interaction;knowledge base;linked data;semantic web;sensor;theory;user experience	Alicia Sellés Carot;Enrique Orduña-Malea;Jorge Serrano-Cobos;Nuria Lloret Romero	2010	2010 IEEE Fourth International Conference on Semantic Computing	10.1109/ICSC.2010.43	data modeling;knowledge base;detector;computer science;artificial intelligence;semantic web;rdf;linked data;data mining;database;world wide web	DB	-42.64655536024992	8.674726027085816	110418
96e11c0e2d5c7b67f1e3d5eafcdbd0cb6acb2fb1	hands-off spreadsheets		The wealth of functionality in the Excel software package means it can go beyond use as a static evaluator of predefined cell formulae, to be used actively in manipulating and transforming data. Due to human error it’s impossible to ensure a process like this is always error free, and frequently the sequence of actions is recorded only in the operator’s head. If done regularly by highly paid staff it will be expensive. This paper applies to those spreadsheets which involve significant operator intervention, describes a method that has been used to improve reliability and efficiency, and reports on how it has worked in practice.	human error;interpreter (computing);spreadsheet	Colin A. Kerr	2012	CoRR		computer science;engineering;data mining;engineering drawing	SE	-34.10719138262881	16.427305321726966	110434
0f6828dcbce88730faea0383dae9feaac355db2d	web services-based and multi-agent information platform	web services multi agent systems software agents;information systems;software agent;correlative information system;soap;web service;software agents;web service based information platform;computer architecture;multi agent systems;information exchange;web services;information exchange platform;uddi;software agent web service based information platform multiagent information platform information exchange platform correlative information system;information systems multiagent systems simple object access protocol service oriented architecture data models computer architecture;information system;soap web services software agent uddi;service oriented architecture;simple object access protocol;multiagent systems;multiagent information platform;data models	The task of information platform is to manage, gather and share data in correlative departments. The function of information platform is in two ways. First, it is the information hinge across the correlative departments, through which information is shared and exchanged; second, it is the information exchange platform of the correlative information systems. This paper presents a Web services-based and Multi-Agent information platform, which implements sharing data and transferring data from heterogeneous and dispersive systems. Due to introducing software agent, the platform takes on autonomy and intelligence and openness.	autonomy;dispersive partial differential equation;distributed computing;information exchange;information system;openness;software agent;web service	Yu-Qian Xue;Ai-xia Chen;Zhan-Kun Zhao;Xin-Wen Wang	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5580920	web service;computer science;artificial intelligence;software agent;soap;multi-agent system;open platform;data mining;database;world wide web;information system	AI	-40.604464657496294	17.693079513192135	110482
d080ec98274bde4b0bceb074c208a9d264601bc0	automated rea (area): a software toolset for a machine-readable resource-event-agent (rea) ontology specification		This paper demonstrates a toolset developed by the authors to enable a machine-readable REA ontology specification. Information modelling techniques are used to provide a unified enterprise ontology by capturing the business semantics using Conceptual Graphs (CGs) using Common Logic (CL) and the Conceptual Graph Interchange Format (CGIF) dialect for information exchange and transmission. Formal Concept Analysis (FCA) is used for model verification, knowledge discovery and extraction. The enterprise design follows the Open Groups definition of the TOGAF Architecture Development Method (ADM) to define the system architecture and subsequently provide a method for defining and automating the (REA) design models for; Business Architecture, Information System Architecture and Technology Architecture.	business architecture;common logic;conceptual graph;formal concept analysis;human-readable medium;information exchange;information model;information system;programming tool;systems architecture;the open group architecture framework	Richard Fallon;Simon Polovina	2016			enterprise architecture framework;reference architecture;the open group architecture framework;computer science;systems engineering;data mining;database;solution architecture;architecture framework;view model	SE	-43.348309578303116	4.873436217460849	110621
14b73a94975c5eeeada9d371d547af397a3243ba	a general purpose architecture for image retrieval in databases	application development;generic model;image database;information retrieval image retrieval image databases application software image processing spatial databases content based retrieval data models computer architecture image analysis;visual databases image retrieval;graid architecture image retrieval architecture image database wired image retrieval system;visual databases;image retrieval	Most of the solutions proposed in image database applications are limited to a specific application domain. The lack of generic models causes problems for researchers. This work defines a general purpose retrieval architecture for image databases (GRAID) that facilitates the processes of defining and constructing specific image retrieval applications and retrieving through integration of images and traditional data. The main goal is to provide an extensible and flexible common platform to ease the work of the image application developers. The WIRED image retrieval system was implemented to validate the GRAID architecture. Two image database applications using the WIRED system are also described.	image retrieval	Sahudy Montenegro González;Akebo Yamakami	2000		10.1109/DEXA.2000.875098	computer vision;visual word;image retrieval;computer science;digital image processing;database;rapid application development;automatic image annotation;data retrieval;information retrieval	Vision	-36.03199358741645	9.85548133454762	110737
0b6856d9872bba09b025078f3dd5a86fe01f800d	alive: a distributed live-link documentation system	traitement automatise;distributed system;base relacional dato;interfase usuario;technical information;systeme reparti;hipertexto;exigence;edition electronique;concepcion sistema;user interface;real time;sistema informatico;alive;computer system;requirement;relational database;document office automation;sistema repartido;edicion electronica;system design;traitement document;tratamiento automatizado;documentacion;temps reel;exigencia;base donnee relationnelle;tiempo real;vinculo;interface utilisateur;systeme informatique;electronic publishing;document processing;informacion tecnica;data consistency;link;hypertexte;conception systeme;hypertext;lien;bureautique documentaire;documentation;tratamiento documento;automated processing;information technique	This paper presents the ALIVE project, which has been developed at Italtel to provide the means for automatic management of technical documentation. The main goal of ALIVE is to enable the user of a technical publishing system to establish live-links with data stored on remote databases. Live-links allow for automatic update of a document with database contents: whenever a modification occurs in the database data referenced from the document text, the document is updated accordingly. The ALIVE user interface has been implemented by exploiting the functionality offered by the Interleaf1 technical publishing environment, providing the user with a fully integrated environment. It allows the user to browse through a description of the available databases and to formulate queries related to data stored in them by means of a menu-based interface. By enriching technical publishing features with data consistency controls in a uniform way, ALIVE moves towards the integrated desktop concept.	berkeley madonna;browsing;common lisp;communications protocol;comparison of command shells;decnet;database;desktop computer;graphics;hp 9000;ibm systems network architecture;internet protocol suite;microvax;microsoft windows;openvms;technical documentation;telecommunications network;ultrix;user interface;vax;workstation	Silvano Pozzi;Augusto Celentano;Luisa Salemme	1992	Electronic Publishing		document processing;hypertext;link;lien;documentation;relational database;computer science;requirement;operating system;technical documentation;data mining;database;electronic publishing;data consistency;user interface;world wide web	DB	-36.39086961813206	16.302686921124437	110996
9cbaf8d4f4fc8a598db1b533a95d1b834707b05c	service recommendation with adaptive user interests modeling	public service	In composing and using services, user’s requirements are subject to uncertainty and changes. It can be difficult for users to maintain an overview of all available services and to make good choices among them. This paper proposes an approach to proactively recommending suitable services to users. Our major contribution is to have devised a novel user-interest model to describe user’s interests adaptively. A reasonable way is put forward for picking up suitable services timely and its key problem is defined formally. Important properties of the model are theoretically proved, and the effectiveness of recommendations is verified with prototypical implementation and tryouts in public service area.	benchmark (computing);data recovery;experiment;requirement;user modeling	Cheng Zhang;Yanbo Han	2007		10.1007/978-3-540-77115-9_27	computer science;knowledge management;data mining;database;world wide web;computer security	Web+IR	-48.08178127639891	11.752997844423566	111165
29fe88a8ac7d304533a53ff29c7ff96f43e09bcd	qos ontology for service selection and reuse	semantic web service;service composition;service selection;qos;service discovery;tracking data;business process re engineering;ontology	Web service technologies become popular in software development in all sectors. Variety services related standards are defined but they still have limitations to represent the services such as the service registry does not support the Quality of Service (QoS) properties, web service description language does not allow specifying the QoS properties and there is no common ontology’s structure to store services. Our research aims to enhance the representation of services to assist the service selection and composition process in order to reduce the development costs. The existing resources are analyzed to define a web service ontology (WSOnto). Furthermore, a service selection algorithm is proposed to validate the proposed WSOnto. It considers the multiple criteria of inputs such as: context, functional and non-functional properties of services. The WSOnto and a service selection algorithm are studied to assist the re-engineering of business processes from users’ designed business processes.	business process;categorization;functional programming;ontology (information science);quality of service;requirement;run time (program lifecycle phase);selection algorithm;server (computing);service composability principle;software development;web services discovery;web service	Sophea Chhun;Néjib Moalla;Yacine Ouzrout	2016	J. Intelligent Manufacturing	10.1007/s10845-013-0855-6	service provider;web service;service level requirement;service level objective;mobile qos;service catalog;quality of service;service product management;application service provider;business service provider;differentiated service;computer science;knowledge management;service delivery framework;ws-policy;service design;ontology;database;service discovery;service desk;data as a service;world wide web;universal description discovery and integration;service system	Web+IR	-47.151944225627695	16.24022683953739	111709
a854bded79ea81037d21fc6ad6084f33b6b4cead	a hierarchical constraint satisfaction approach to product selection for electronic shopping support	electronic commerce;hierarchically organized constraints hierarchical constraint satisfaction approach product selection electronic shopping support advanced information technologies telecommunications technologies interorganizational business transactions electronic trading systems electronic marketplace;application software;helium;information retrieval;resource management;consumer electronics taxonomy application software electronic commerce communication industry office automation resource management information retrieval database systems;consumer electronics;hierarchically organized constraints;indexing terms;trading system;constraint satisfaction;electronic shopping support;electronic marketplace;home shopping;expressive power;taxonomy hierarchies;communication industry;telecommunications technologies;hierarchical constraint satisfaction approach;advanced information technologies;interorganizational business transactions;multiattribute product selection;database systems;constraint theory home shopping;taxonomy;constraint theory;electronic trading systems;product selection;office automation;hierarchical constraint satisfaction	The development of advanced information and telecommunications technologies has established more convenient ways of interorganizational business transactions. Especially, various forms of electronic trading systems are introduced, which replicate, and often improve, functions of physical market places. We propose a product selection mechanism for such an electronic marketplace, which is viewed as a satisfaction problem of hierarchically organized constraints over product attributes. The proposed approach is more expressively powerful and flexible than product selection based on a single product taxonomy hierarchy.	hierarchical constraint satisfaction	Young U. Ryu	1999	IEEE Trans. Systems, Man, and Cybernetics, Part A	10.1109/3468.798056	application software;index term;constraint satisfaction;computer science;knowledge management;resource management;helium;expressive power;new product development;taxonomy;product engineering	DB	-35.41443375037955	17.540004900177415	111797
d7e51f58175546c3103d8c64529fc999e489f7e8	informationbase - a new information system layer	data entry;end users;enterprise resource planning;database applications;data quality;information system;database design	In the paper, the concept of informationbase (IB) is described. The IB is a top layer of the information system (IS). The IB layer uses information systems database and existing application programs to provide a structured set of reports and graphs, directly and permanently available to the user. The informationbase described in the article acts as an intelligent and very agile information manager. At appropriate times, it activates predefined options for report generation, supplies the reports with parameters, stores generated information into a well-organized database and manages the authorization of information retrieval.	agile software development;authorization;database;information management;information retrieval;information system	Kovach Dragan	2005		10.4018/978-1-931777-09-4.ch022	cost database;data modeling;intelligent database;database tuning;data model;computer science;data administration;data mining;database;view;world wide web;database schema;human resource management system;physical data model;database testing;database design;enterprise information system	DB	-37.21225252544131	10.103616759054486	111932
57f1e259dc5286bf2edd463a23bb2c0cbd8f44a5	handling spatio-temporal sensor data in global geographical context with sensord	context awareness;context aware;geographic information;spatiotemporal element;sensor event;context aware service;sensor;data access;emergency response system;middleware;information service;spatial information	It is important to manage sensors' locations and their attributes in a coordinated manner to realize context-aware services based on sensing data. A coordinated means of management is also necessary for middleware providing various information services. We have been developing Sensor-Event-Driven Service Coordination Middleware (SENSORD) to realize uniform management of various sensors, their locations and their attributes and higher-level service. It provides sensor locations for users using a unified view with region-specific geographical information, so SENSORD provides data access interfaces like GIS. Sensor locations are a component of that spatial information. Therefore, it is effective to aggregate them into geographical information. In this paper, we first describe SENSORD. Second, we explain methods of managing spatial information and the computational flow of acquiring sensor location information. Moreover, we show an application of SENSORD: an indoor emergency response system in our laboratories.		Takeshi Ikeda;Yutaka Inoue;Akio Sashima;Koichi Kurumatani	2007		10.1007/978-3-540-76772-5_3	data access;computer science;sensor;operating system;middleware;data mining;spatial analysis;internet privacy;computer security	Robotics	-35.63575288233814	14.827928624397428	111974
5080c908ccfdb0421528020868aa928cca4c80f4	knowledge capture and utilization in virtual communities	feedback mechanism;knowledge management;virtual community;explicit knowledge;shared knowledge;knowledge sharing;world wide web;information agent;knowledge sharing environments;virtual communities	The literature on knowledge management highlights issues of fit between IT-based systems for knowledge management and the socially situated leveraging of knowledge assets by organisations [1]. This paper explores the way in which a knowledge-sharing environment (KSE) can facilitate knowledge capture and utilization in virtual communities. The KSE (Jasper II) is a system of information agents for organising, summarizing and sharing knowledge from a number of internal and external sources, including the World Wide Web (WWW). The paper describes the features and functionality of Jasper II, and goes on to show how it can be leveraged to support the capture of both tacit and explicit knowledge in virtual communities. The final discussion focuses on the dynamics of the knowledge capture and utilization process, highlighting the importance of the feedback mechanisms that enable the KSE to meet the specific needs of diverse, evolving communities. It suggests that besides supporting the dynamic knowledge requirements of communities, the KSE can play a key role in the evolution of existing communities.	feedback;knowledge management;requirement;situated;virtual community;www;world wide web	Yasmin Merali;John Davies	2001		10.1145/500737.500754	knowledge base;computer science;knowledge management;explicit knowledge;body of knowledge;knowledge-based systems;open knowledge base connectivity;data mining;feedback;multimedia;personal knowledge management;knowledge value chain;world wide web;domain knowledge	AI	-46.937694785697104	7.944120696794072	112048
1173b47cdb1d441b4cc9f0fc4c7e6e3a7322f96d	case study of wsmo based semantic web services			semantic web service;wsmo	Aekyung Moon;Yoo-Mi Park;SangKi Kim	2010			semantic web;web standards;data web;world wide web;social semantic web;semantic analytics;semantic web rule language;wsmo;semantic web stack;computer science	Web+IR	-39.83489747747566	7.253172594407924	112393
9bf93ca41178a6ef6b9bb23db307da3f42e07e82	joined view editor for mashups of web data stores	google;web data editing data publish globally shared data resources web data stores associative data representation web data browsing data elements multiple web services tabular grid joined view editor;mashups;web service;online front ends;information integration;graphical user interfaces;schedules data models mashups graphical user interfaces load modeling google;web services;schedules;join operation;electronic publishing;information integration web service join operation;load modeling;grid computing;web services electronic publishing grid computing online front ends;data models	Recent years have witnessed the emergence of Web services that publish data that can be used as globally shared data resources, and Web data stores offer data resources that can be shared and modified by users or groups. We think associative data representation among Web data elements is important for the reuse of the data for various purposes even when treating personal/group data. This paper aims to provide a general-purpose widget for Web data browsing/editing where users can easily create and update their data elements that are associated with other data elements, and proposes a table editor for joined views of Web services. With our editor, the user can join multiple Web services, and edit the left/right elements on the joined view in a tabular grid. To support easy manipulation of associated data during the modification, our editor continuously provides the renewed view reflecting the modification. The user can also try out multiple modifications while checking their impact on the view, and decide later whether to commit the modifications to the Web data stores or to discard the modifications. This paper presents some illustrations that demonstrate the use of this joined view editor, discusses the design, and briefly describes its implementation.	automated planning and scheduling;data (computing);data store;emergence;general-purpose modeling;table (information);web service;world wide web	Yoshio Kumagai;Masaya Senba;Takakazu Nagamine;Tomio Kamada	2012	2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing	10.1109/SNPD.2012.100	web service;web development;web modeling;data web;computer science;operating system;web page;data mining;database;electronic publishing;world wide web	DB	-38.59243287908481	10.432865205984559	112426
d00515fa4d620a2d5df33a129321af11caf8146b	a service oriented architecture combining agents and ontologies towards pervasive adaptation	service oriented architecture	Contemporary software technologies complying with the ServiceOriented Architectural (SOA) paradigm, such as OSGi, UPnP, and the Web services do not meet on their own the adaptability and interoperability challenges of the Ambient Intelligence (AmI) environments. In this paper we present a solution based on the combination of the SOA model with Agents and Ontologies. The agent approach complements the SOA infrastructure by providing high level adaptation to user’s tasks, as an intelligent control layer above SOA. Ontologies are used to tackle the semantic heterogeneity that arises in AmI spaces and provide to agents a common repository of system knowledge, policies and state.	ambient intelligence;high-level programming language;intelligent agent;intelligent control;interoperability;multi-user;osgi;ontology (information science);pervasive informatics;semantic heterogeneity;service-oriented architecture;software agent;universal plug and play;web service;world wide web	Christos Goumopoulos;Achilles Kameas	2009		10.3233/978-1-60750-034-6-228	enterprise architecture framework;human–computer interaction;ontology (information science);reference architecture;computer science;service-oriented architecture;solution architecture;applications architecture;service (systems architecture)	AI	-43.774053041156996	11.525011157660881	112510
a60317d724d8414dc7282fa6a3ebdceb7b68c46c	adaptable information clipping service supported by evolving user profile and multiple ontologies	commerce electronique;filtering;filtrage;ontologie;comercio electronico;filtrado;user profile;diffusion information;information dissemination;difusion informacion;intranet;ontology;electronic trade	This paper describes a system for selective dissemination of information based on multiple-ontology in a business intranet. The ontology includes different types of hierarchical knowledge of social organization, occupational category working, themes, technical fields, technological relationship, and so on. The model represents user's interests in the form of attribute sets using the multiple-ontology. It can deal with multidimensional factors and selectively disseminates information to unprofiled users with explicit dissemination reasons.		Takahiro Ikeda;Akitoshi Okumura;Kazunori Muraki	1997			filter;computer science;knowledge management;artificial intelligence;ontology;data mining;world wide web	Theory	-42.10941589353762	8.823845645230373	112517
152b328dac582e1cfc3a4a3974ebb012c0acf192	desiderata for an authoritative representation of mesh in rdf		The Semantic Web provides a framework for the integration of resources on the web, which facilitates information integration and interoperability. RDF is the main representation format for Linked Open Data (LOD). However, datasets are not always made available in RDF by their producers and the Semantic Web community has had to convert some of these datasets to RDF in order for these datasets to participate in the LOD cloud. As a result, the LOD cloud sometimes contains outdated, partial and even inaccurate RDF datasets. We review the LOD landscape for one of these resources, MeSH, and analyze the characteristics of six existing representations in order to identify desirable features for an authoritative version, for which we create a prototype. We illustrate the suitability of this prototype on three common use cases. NLM intends to release an authoritative representation of MeSH in RDF (beta version) in the Fall of 2014.	bibframe;baseline (configuration management);compiler;data infrastructure;epidemiologic research design;extensible markup language;interoperability;limit of detection;linked data;national library of medicine (u.s.);netware loadable module;prototype;resource description framework;semantic web;software release life cycle;united states national institutes of health;usability;world wide web;x-linked combined immunodeficiency diseases;xml	Rainer Winnenburg;Olivier Bodenreider	2014	AMIA ... Annual Symposium proceedings. AMIA Symposium		simple knowledge organization system;rdf;sparql;linked data;rdf schema;information retrieval;semantic web;cwm;rdf/xml;computer science	Web+IR	-40.39272501993329	4.270357986866662	112663
6816c98a85aff39dddeb8eea0566e57558667c53	an autonomous system-based distribution system for web search	distributed system;search engines;load balanced algorithm web search computer networks search engines autonomous system distributed system;information access;information networks search engines;information overload;web search engine;computer network;autonomic system;keyword search;information networks;indexation;web search search engines internet uniform resource locators ip networks system testing packaging robots network servers web server;web search;load balance	The tremendous growth in computer networks and storage has fueled the explosive growth of the web. The amount of information accessible from the web has dramatically increased by several orders of magnitude in the last few years, and shows no signs of abating. Developers of web search engines are confronted with the consequent information overload problem. Although many algorithms are widely used to optimize the performance of web search engines, it is still difficult to decrease the updating cycles of the web data for a traditional general-purpose search and indexing system. In this paper, an autonomous system (AS) based solution is presented. One of its main merits is to limit the network load inside an AS. Since AS is the element of the Internet interconnection and a uniformed route strategy is adopted inside an AS, it will be helpful to solve global network overload problem. At the other side, updating cycles are greatly decreased since the distributed system is working in a parallel way at different ASes. Some key algorithms and techniques are put forward and tested for management and synchronization of the whole system. The effectiveness of the solution is verified in laboratory simulation.	autonomous system (internet);web search engine	Xiaohui Zhang;Huayong Wang;Guiran Chang;Zhao Hong	2001		10.1109/ICSMC.2001.969851	web service;search engine indexing;web modeling;simulation;data web;metasearch engine;web search engine;computer science;spamdexing;load balancing;information overload;web navigation;distributed web crawling;database;search analytics;world wide web;web server;search engine	ECom	-34.65506905783728	17.33826511019123	112954
4a79bfa403a846791ad58be01393af6b4efe2f27	data integration and reconciliation in data warehousing: conceptual modeling and reasoning support	tratamiento datos;data reconciliation;conceptualization;data integrity;integration information;data processing;conceptual model;correspondance interschema;traitement donnee;stockage donnee;conceptualizacion;data storage;information integration;modelisation conceptuelle;reecriture;reconciliation donnee;integracion informacion;data warehousing;query;almacenamiento datos;integration donnee;data warehouse;rewriting;conceptualisation;requete;reescritura	Integration is one of the most important aspects of a Data Warehouse. When data passes from the sources of the application-oriented operational environment to the Data Warehouse, possible inconsistencies and redundancies should be resolved, so that the warehouse is able to provide an integrated and reconciled view of data of the organization. We describe a novel approach to data integration and reconciliation, based on a conceptual representation of the Data Warehouse application domain. The main idea is to declaratively specify suitable matching, conversion, and reconciliation operations to be used in order to solve possible conflicts among data in different sources. Such a specification is provided in terms of the conceptual model of the application, and is effectively used during the design of the software modules that load the data from the sources into the Data Warehouse.	application domain;declarative programming	Diego Calvanese;Giuseppe De Giacomo;Riccardo Rosati	1999	Networking and Information Systems		conceptualization;data modeling;rewriting;dimensional modeling;computer science;conceptual model;data science;information integration;data warehouse;computer data storage;data integrity;data mining;database	DB	-34.22366091857169	10.79636504525138	113186
b7be28ac54c6957f2605cea83d8f2c6bbdc26f51	annotation classes: a structuring mechanism for owl ontologies	annotationclass;radlex;owl;ontology visualization	OWL provides AnnotationProperties as a means of providing extralogical information about ontologies, classes, properties, and individuals. We propose analogous AnnotationClasses as a means of adding hierarchical or other structure to an ontology without additional entailments, primarily for the benefit of human users. We use the RadLex vocabulary as a motivating example and also address the more general problem of understanding OWL ontologies.	ontology (information science);vocabulary;web ontology language	Mike Dean	2008			structuring;ontology;ontology (information science);data mining;computer science;annotation;vocabulary	Web+IR	-35.809491088371935	7.961873433550083	113204
e2f739b98a8431913c51789864b5a2fd02efec9a	cognitive memory for semantic agents in robotic interaction	programming language semantics;semantics;inference mechanisms;human robot interaction;ontologies artificial intelligence;information sharing;software agents;computer architecture;semantic agents;knowledge;ubiquitous computing cognition decision making human robot interaction inference mechanisms ontologies artificial intelligence peer to peer computing programming language semantics software agents;robot brain;pervasive architecture;information exchange;webservice cognitive memory pervasive architecture semantic agents ontology knowledge;robots;cognition;robotic interaction decision process robot brain multimodal interaction semantic agents architecture ontologies inference engine information sharing information exchange knowledge representation language pervasive architecture cognitive memory;robotic interaction;multimodal interaction;ubiquitous computing;decision process;ontologies;humans;inference engine;agent architecture;peer to peer computing;knowledge representation;knowledge representation language;semantic agents architecture;ontology;semantics ontologies computer architecture robots knowledge based systems context humans;context;knowledge based systems;cognitive memory;webservice	Since 1960, lots of AI researchers work on intelligent and reactive architectures able to manage multiple events and act in the environment. This issue is also part of Robotics domain. A decision process must be implemented in the robot brain to accomplish the multimodal interaction with human in human environment. In this article, we present a semantic agents architecture giving the robot the ability to well understand what is happening and thus provide more robust responses. We will describe here our agent component. Intelligence and knowledge about objects in the environment is stored in two ontologies linked to a reasoner, the inference engine. To share and exchange information, an event knowledge representation language is used by semantic agents. This architecture brings other advantages: pervasive, cooperating, redundant, automatically adaptable and interoperable. It is independent of platforms.	human–robot interaction;inference engine;interoperability;knowledge representation and reasoning;kripke semantics;model checking;multimodal interaction;ontology (information science);parse tree;pervasive informatics;robot;robotics;semantic reasoner	Sébastien Dourlens;Amar Ramdane-Cherif	2010	9th IEEE International Conference on Cognitive Informatics (ICCI'10)	10.1109/COGINF.2010.5599685	natural language processing;computer science;knowledge management;artificial intelligence	Robotics	-41.60579297704289	16.19042633945899	113211
e0c00d894f9849b35898412720949dff65e22342	a case study on visualizing large spatial datasets in a web-based map viewer		Lately, many companies are using Mobile Workforce Management technologies combined with information collected by sensors from mobile devices in order to improve their business processes. Even for small companies, the information that needs to be handled grows at a high rate, and most of the data collected have a geographic dimension. Being able to visualize this data in real-time within a map viewer is a very important deal for these companies. In this paper we focus on this topic, presenting a case study on visualizing large spatial datasets. Particularly, since most of the Mobile Workforce Management software is web-based, we propose a solution suitable for this environment.		Alejandro Cortiñas;Miguel R. Luaces;Tirso V. Rodeiro	2018		10.1007/978-3-319-91662-0_23	data mining;business process;web application;mobile device;computer science;software;workforce management	HCI	-47.7676048173071	7.406911112642447	113294
9d01023120ee565663a601c9420397288e86a4be	a methodology to define qos and sla requirements in service choreographies	quality of service proposals logic gates petri nets computational modeling web services;quality of service peer to peer computing;choreography interaction model sla requirements service choreographies p2p systems interconnection models interaction models distributed application qos attributes;computational modeling;logic gates;web services;petri nets;peer to peer computing;quality of service;proposals	Service choreography allows the composition of services in a collaborative way, taking advantage of various benefits of P2P systems. The existing modeling approaches to evaluate choreographies are the interconnection and interaction models. However, these approaches don't evaluate choreographies focusing on QoS or in the earlier stages of development of the distributed application. This paper proposes an approach to assess the impact of QoS attributes specified in a choreography interaction model. With our proposal it is possible to establish requirements for QoS and SLA in early stages of development in order to plan the capacity of the network elements connecting the hosts involved in the enactment and deployment of the choreography.	algorithm;business process model and notation;distributed computing;interconnection;peer-to-peer;quality of service;requirement;service choreography;service-level agreement;software deployment	Alfonso Phocco Diaz;Daniel M. Batista	2012	2012 IEEE 17th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD)	10.1109/CAMAD.2012.6335333	web service;quality of service;logic gate;computer science;distributed computing;computational model;world wide web;petri net;computer network	Embedded	-47.08768788298723	17.43337607379014	113561
5e1e3bbde49f16af1a34ca7c3933965634ac8919	generating domain-specific web-based expert systems	web based expert systems;prolog;automatic generation;xml;domain specificity;expert system	This paper describes a method to automatically generate web-based expert systems from XML descriptions of the knowledge domain. A domain-specific XML parser produces software modules for web-based information servers that feature dynamic expert system analysis. The case study is university course rules. An XML data definition file is developed featuring common rules and restrictions regarding courses. Web-based course information and enrollment guides can be automatically generated that dynamically execute a Prolog interpreter to provide expert advice. Crown Copyright 2007 Published by Elsevier Ltd. All rights reserved.	crown group;data definition language;expert system;prolog;system analysis;web application;xml	Neil Dunstan	2008	Expert Syst. Appl.	10.1016/j.eswa.2007.07.048	web service;xml validation;legal expert system;xml;computer science;soap;web page;data mining;database;xml signature;programming language;prolog;xml schema editor;expert system;efficient xml interchange	AI	-40.03648822364328	12.113664850455399	113587
3d447899a9e812af131784b43914a57d76730ff1	research on ontology building of product reviews in chinese	cybernetics;semantics;semantic web mobile computing ontologies artificial intelligence reviews;ontologies artificial intelligence;mobile phone;semantic information;product review;mobile phone product review domain ontology;machine learning;ontology building;mobile handsets;product reviews;semantic web;ontologies;semantic relations;reviews;mobile computing;chinese ontology;domain ontology;mobile phone ontology building product reviews core technology semantic web chinese ontology;cameras;buildings;ontologies mobile handsets semantics buildings cameras machine learning cybernetics;core technology	Product reviews as one type of semantic information can directly affect the decision made by customers. Ontology as the core technology of semantic web can transform the product review into information which can be automatically understandable by computers. In this paper, the method to build product reviews in Chinese Ontology is proposed. Product reviews about mobile phone are taken as a data source, the source of relevant concepts and the standard of concepts classification are discussed. In particular, the semantic relations (hierarchical and non-hierarchical relations) between classes are analyzed.	computer;mobile phone;ontology engineering;semantic web	Neng Wen;Shengchun Ding;Ting Jiang	2011	2011 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2011.6016949	upper ontology;bibliographic ontology;cybernetics;computer science;knowledge management;ontology;artificial intelligence;semantic web;data mining;semantics;ontology-based data integration;mobile computing;world wide web	Web+IR	-42.5677775454243	6.609034609324104	113691
eb4a3bff3fdac9be28b7d1c5e636c29e5955c503	an approach for selecting best available services through a new method of decomposing qos constraints	service composition;decomposition of qos constraints;qos based service selection;web services;local service selection	Quality of Service (QoS) plays as a discriminating factor for selecting appropriate services that meet the given user’s non-functional requirements during service composition. There is a compelling need to select suitable services quickly so that the composition can meet dynamic needs. Recently, local selection approaches for QoS-based selection have been put forward toward reduced time complexity. A methodology for selecting the best available service combination for a given user requirement (workflow) with a new method of decomposing QoS constraints is proposed in this paper. The methodology consists of two phases, namely ‘Constraint Decomposition Phase’ and ‘Service Selection Phase’. In the Constraint Decomposition Phase, a unique method is proposed to decompose the given non-functional (global or workflow level) constraints into local constraints for individual tasks in the workflow. Each individual task with its local constraints forms a subproblem. In the Service Selection phase, each subproblem is resolved by finding the best available service from its respective service class using an iterative searching procedure. A prototype has been implemented, and the low computation time of the proposed method makes it well suited to dynamic composition. The proposed method of decomposing constraints is independent of number of services in a service class, and the method is applicable to any combinational workflow with AND, OR and Loop patterns. Further, a new method for computing response time of OR execution pattern which guarantees successful execution of each path in an OR pattern is a remarkable contribution of this work.	combinational logic;computation;execution pattern;functional requirement;iterative method;non-functional requirement;prototype;quality of service;response time (technology);service composability principle;time complexity;user requirements document	Chellammal Surianarayanan;Gopinath Ganapathy;Manikandan Sethunarayanan Ramasamy	2014	Service Oriented Computing and Applications	10.1007/s11761-014-0154-x	web service;real-time computing;computer science;database;distributed computing;law	HPC	-46.403048024773135	15.958225944741654	113692
b14d1af66de2116414711bb9b61f5161a6839881	dynamic querying of mass-storage rdf data with rule-based entailment regimes	optimization technique;rule based;relational database;conference paper;close relationships	RDF Schema (RDFS) as a lightweight ontology language is gaining popularity and, consequently, tools for scalable RDFS inference and querying are needed. SPARQL has become recently a W3C standard for querying RDF data, but it mostly provides means for querying simple RDF graphs only, whereas querying with respect to RDFS or other entailment regimes is left outside the current specification. In this paper, we show that SPARQL faces certain unwanted ramifications when querying ontologies in conjunction with RDF datasets that comprise multiple named graphs, and we provide an extension for SPARQL that remedies these effects. Moreover, since RDFS inference has a close relationship with logic rules, we generalize our approach to select a custom ruleset for specifying inferences to be taken into account in a SPARQL query. We show that our extensions are technically feasible by providing benchmark results for RDFS querying in our prototype system GiaBATA, which uses Datalog coupled with a persistent Relational Database as a back-end for implementing SPARQL with dynamic rule-based inference. By employing different optimization techniques like magic set rewriting our system remains competitive with state-of-the-art RDFS querying systems.	benchmark (computing);datalog;lightweight ontology;logic programming;mathematical optimization;named graph;ontology (information science);prototype;rdf schema;relational database;rewriting;sparql;scalability	Giovambattista Ianni;Thomas Krennwallner;Alessandra Martello;Axel Polleres	2009		10.1007/978-3-642-04930-9_20	rule-based system;relational database;computer science;sparql;artificial intelligence;data mining;database;information retrieval;rdf schema	DB	-34.551304386875536	5.0639075633620525	113700
39c1522e1063d117a98677be518ae1a86fd5963a	a evaluation method for web service with large numbers of historical records	historical qos record;quality of service web services accuracy mathematical model equations time complexity proposals;web services quality of service;time complexity;partial historical record based service evaluation method web service evaluation method network environment quality of service data qos data web service providers historical qos records subsequent service selection service composition lagging effect partial hr;web service;accuracy;quality evaluation;web services;mathematical model;quality of service;proposals;large numbers web service quality evaluation historical qos record;large numbers	Due to the unstable network environment or fake advertisement reasons, the QoS (quality of service) data published by web service providers is not always trusted. Therefore, it is a promising way to evaluate the service quality, based on the historical QoS records generated from the past invocations of web services. However, for some web services with large numbers of historical QoS records, the evaluation efficiency is usually low and cannot meet the quick response requirements of subsequent service selection or service composition. Moreover, it will lead to the 'Lagging Effect' (i.e., The evaluation result cannot reflect the up-to-date quality change trend of a web service), if all the historical QoS records are treated equally in service evaluation. In view of these challenges, a novel service evaluation method named Partial-HR (Partial Historical Records-based service evaluation method) is put forward in this paper, by which only partial important historical QoS records are employed to evaluate the service quality. Finally, a set of experiments are designed and deployed to validate the feasibility of our proposal, in terms of evaluation accuracy and efficiency.	control theory;experiment;marginal model;quality of service;requirement;service composability principle;volatility;web service	Lianyong Qi;Jiancheng Ni;Xiaona Xia;Chao Yan;Hua Wang;Wanli Huang	2014	2014 IEEE 13th International Conference on Trust, Security and Privacy in Computing and Communications	10.1109/TrustCom.2014.94	service provider;service level requirement;service level objective;mobile qos;differentiated service;computer science;service delivery framework;ws-policy;service design;data mining;database;world wide web;service system	Web+IR	-45.12672912542999	15.9256570827677	113727
bbb0b63ae6a7fb862ed0cf4a509c6875f0e64e05	intégration de services. une analyse structurée	added value;information technology;service web;technologie information;systematique;web service;internet;sistematica;taxonomy;coordinacion;valor anadido;tecnologia informacion;valeur ajoutee;servicio web;coordination	The democratization of internet along with recent advances in information technologies have made the global networked marketplace vision a reality. To stay competitive in such an environment, companies promote and integrate their services in order to build new added-value services (applications). While the number of works that come up with new ideas and solutions for integrating services grows in an ever-increasing pace, few surveys try to draw a state of the art of existing solutions. A step in this direction, this paper proposes a taxonomy that characterizes services integration through three main concepts, service, service binding and orchestration. The proposed taxonomy is used as a grid for comparing existing services integration solutions and highlighting their advantages and limitations.		Khalid Belhajjame;Genoveva Vargas-Solar;Christine Collet	2005	Ingénierie des Systèmes d'Information	10.3166/isi.10.3.91-110	web service;the internet;simulation;services computing;law;information technology;world wide web;computer security;added value;taxonomy	Crypto	-47.81801745420989	13.184772573270607	113937
c0838dc33e140694e567ca9e6ff663467aaf18d0	an efficient algorithm for temporal financial information monitoring	financial data processing;pricing;financial information information monitoring temporal query web information;stock markets;online monitoring temporal financial information monitoring web currency exchange interest rates commodity prices stock markets organizations balance sheets market shares temporal conditions oil price gold price real time financial value storage algorithm;stock markets economic indicators financial data processing internet pricing real time systems;internet;monitoring web services html xml time series analysis algorithm design and analysis;economic indicators;real time systems	The web contains a huge amount of real-time financial information related to currency exchange, interest rates, commodity prices, and stock markets in addition to more information regarding relevant information such as organizations' balance sheets, market shares, and history of their performance. Temporal financial monitoring basically help observe/store financial information to be processed based on pre-defined temporal conditions. Specific actions are performed whenever these conditions are met. An example of temporal conditions is online monitoring for a financial value's increase or decrease by a certain amount within a specified time period. This financial value could be the price of a particular stock, oil price, gold price, or any other real-time financial value. Such approach requires storing large amounts of data over some extended period of time until the conditions are met. This paper develops an efficient storage algorithm for temporal financial monitoring. The algorithm preprocesses retrieved data based on the specified condition and stores only the data relevant to the condition. This reduces the amount of storage needed for this kind of monitoring and it provides fast financial monitoring.	algorithm;real-time locating system;real-time web;world wide web	Jameela Al-Jaroodi;Nader Mohamed;Klaithem Al Nuaimi	2013	2013 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCSim.2013.6641401	indirect finance;pricing;market data;the internet;position;market depth;economic indicator;mark to model;financial market participants	DB	-34.75883703804218	15.629103067868373	113972
7db1d983f2bc93d7028cb42e39e1b52decc20a68	mapping owl-s processes to multi agent systems: a verification oriented approach	verification;ispl;protocols;computer languages;service composition;owl s process mapping;multi agent system;web services knowledge representation languages multi agent systems ontologies artificial intelligence program verification;probability density function;interpreted systems programming language;web service;program verification;data mining;web services protocols computer languages logic programming educational institutions program processors service oriented architecture semantic web process control;ontologies artificial intelligence;knowledge representation languages;multi agent systems;system description language;mcmas;symbolic model checker;model checking;synchronization;web service owl s process mapping multi agent system verification system description language symbolic model checker ispl interpreted systems programming language;web services;process control;semantic web;multi agent system verification;process model;mcmas model checking web services semantic web verification	We investigate the mapping of OWL-S process models to ISPL - the system description language for MCMAS, a symbolic model checker tailored to the verification of multi agent systems. In our approach, services are viewed as agents, and service compositions as multi agent systems. We show how various composition constructs defined in OWL-S can be encoded into ISPL by using the proposed mapping rules.  We use an extended version of the BravoAir process model from the OWL-S suite of examples to illustrate the technique.	information services procurement library;model checking;owl-s;process modeling;web ontology language	Alessio Lomuscio;Monika Solanki	2009	2009 International Conference on Advanced Information Networking and Applications Workshops	10.1109/WAINA.2009.52	web service;computer science;artificial intelligence;theoretical computer science;multi-agent system;database;programming language	SE	-44.8753436797267	18.115133243258676	114028
97f2696a28420bd7b997b9ebee960b6ee6702e27	a semi-automatic ontology acquisition method for the semantic web	tratamiento automatico;semantic similarity;modelizacion;base relacional dato;gestion informacion;ontologie;red www;web semantique;reseau web;service web;semantics;intelligence artificielle;relational database;web service;semantica;semantique;similitude;modelisation;automatic processing;machine learning;web semantica;information management;relational model;similarity;base donnee relationnelle;semantic web;artificial intelligence;world wide web;ontologia;inteligencia artificial;similitud;gestion information;traitement automatique;modeling;ontology;servicio web	The success of the Semantic Web strongly depends on the proliferation of ontologies, which requires fast and easy engineering of ontologies. The paper analyzes the semantic similarity between relational model and ontology, and proposes a semi-automatic ontology acquisition method(SOAM) based on data in relational database. SOAM tries to ensure the quality of constructed ontology and the automatic degree of acquiring process by balancing the cooperation between user contributions and machine learning. Because OWL is the latest ontology language standard recommended by W3C, the implementation of SOAM is given to acquire OWL ontology automatically as much as possible. Different from existing methods, the implementation method not only can acquire OWL ontology from relational database directly without demanding a middle model, but also can refine obtained ontology according to existing lexical knowledge repositories semi-automatically.	data acquisition;data-intensive computing;database schema;enhanced entity–relationship model;information repository;information retrieval;knowledge acquisition;knowledge engineering;machine learning;natural language;ontology (information science);ontology learning;relational database;relational model;reverse engineering;semantic web;semantic similarity;semiconductor industry;springer (tank);symposium on applied computing;web ontology language;world wide web;xml	Man Li;Xiaoyong Du;Shan Wang	2005		10.1007/11563952_19	upper ontology;web service;open biomedical ontologies;ontology alignment;semantic similarity;relational model;systems modeling;ontology components;similarity;bibliographic ontology;ontology inference layer;relational database;computer science;ontology;artificial intelligence;similitude;semantic web;ontology;data mining;database;semantics;information management;ontology-based data integration;world wide web;owl-s;process ontology;suggested upper merged ontology	Web+IR	-37.56826721221593	11.987358758165573	114144
7817ab3d50d41f520866e2f5a7c5a943cce213b9	multimedia content adaptation modelled as a constraints matching problem with optimisation	optimisation;constraint optimization;constraints matching;adaptation model constraint optimization cats engines mpeg 7 standard delta modulation multimedia systems computational efficiency network servers transcoding;mpeg 21;video coding multimedia systems optimisation;user preferences;delta modulation;multimedia systems;video coding;network servers;adaptation model;engines;content adaptation;multimedia content adaptation engine;mpeg 21 multimedia content adaptation engine constraints matching optimisation;cats;mpeg 7 standard;computational efficiency;transcoding	Within a multimedia adaptation engine working with different adaptation tools and parameters, an important function is the selection of the most appropriate parameters to execute these adaptation tools. MPEG-21 compliant multimedia adaptation engines make use of MPEG-21 description tools to drive the decisions targeting to adapt multimedia content to the usage environment (terminal capabilities, network conditions, user preferences, etc). In this paper, a simple but effective method to model the decision phase of the adaptation engine is presented. This method is based on constraints matching with a final optimisation phase. The proposed system works over MPEG-21 compliant usage environment descriptions, also making use of MPEG- 7 content descriptions when available.	content adaptation;effective method;mpeg-21;mathematical optimization;terminal capabilities;user (computing)	Fernando López;José María Martínez Sanchez	2007	Eighth International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS '07)	10.1109/WIAMIS.2007.61	computer vision;delta modulation;constrained optimization;real-time computing;simulation;transcoding;computer science;multimedia	Web+IR	-46.17487612919055	12.21348319641361	114490
1effbfb02dd0666cf5670f8d89b37ffae0f7f1ae	developing multi agent systems on semantic web environment using seagent platform	ontologie;multiagent system;architecture systeme;correspondance ontologie;multi agent system;ontology mapping;web semantique;semantics;intelligence artificielle;semantica;semantique;orientado servicio;web semantica;semantic web;artificial intelligence;ontologia;arquitectura sistema;oriente service;inteligencia artificial;system architecture;sistema multiagente;ontology;correspondencia ontologia;systeme multiagent;service oriented	In this paper, we discuss the development of a multi agent system working on the Semantic Web environment by using a new framework called SEAGENT. SEAGENT is a new agent development framework and platform, which includes built-in features for semantic web based multi agent system development. These features provide semantic supports such as a new specific content language for transferring semantic knowledge, specifically designed agent’s internal architecture to handle semantic knowledge, a new directory facilitator architecture based on semantic service matching engine and ontology management service to provide ontology translations within the platform’s ontologies. The implemented case study shows the effectiveness of these features in terms of semantically enriched multi agent system development.	canonical account;multi-agent system;ontology (information science);semantic web;service discovery	Oguz Dikenelli;Riza Cenk Erdur;Geylani Kardas;Özgür Gümüs;Inanç Seylan;Önder Gürcan;Ali Murat Tiryaki;Erdem Eser Ekinci	2005		10.1007/11759683_1	semantic computing;semantic web rule language;semantic search;semantic grid;computer science;knowledge management;semantic web;social semantic web;semantic web stack;database;semantic technology;world wide web;owl-s;semantic analytics	Web+IR	-38.935146701589396	12.618401146765454	114652
4f2000e16a3fb7c82a940747d79135b80001bde4	bto supply chain configuration via agent-based negotiation	build to order;make to order;intercambio informacion;iterative process;multiagent system;build to order supply chain configuration;procesamiento informacion;agent based systems;protocole transmission;logistique;negociation;produccion flujo tirado;agent based;agent based negotiation;mas;intelligence artificielle;contrato;constraint satisfaction;production a la commande;satisfaction contrainte;proceso iterativo;processus iteratif;protocolo transmision;multi agent systems;intelligent agents;logistics;contract;negociacion;scm;contract net negotiation protocol;echange information;agent intelligent;information exchange;information processing;bargaining;intelligent agent;artificial intelligence;supply chain;agente inteligente;constraint based information representation;inteligencia artificial;contrat;satisfaccion restriccion;traitement information;sistema multiagente;bto supply chains;supply chain configuration;supply chain management;systeme multiagent;logistica;transmission protocol	This paper presents a constraint-based contract net protocol for agent-based configuration of a Build-To-Order (BTO) supply chain. Applicability of BTO supply chain configuration using intelligent agents has been proved in many research works. Mainly this is due to the fact that both agent-based systems and supply chains consist of a number of independent units that have to collaborate in order to achieve the goal set. Here presented negotiation protocol is a development of the contract net protocol. The latter has been extended with iterative negotiation process, constraint-based information exchange / processing and other features. This allowed improving solutions, but the negotiation time increased. The analysis of the negotiation time for the modified protocol showed that it linearly depends on the product complexity (number of product components) and quadratically on the being configured supply chain complexity (number of suppliers).	academy;agent-based model;constraint logic programming;contract net protocol;hardware description language;information exchange;intelligent agent;internet;iteration;iterative method;run time (program lifecycle phase)	Alexander V. Smirnov;Nikolay Shilov;Alexey Kashevnik	2009	IJMTM	10.1504/IJMTM.2009.023784	build to order;supply chain management;simulation;economics;computer science;engineering;artificial intelligence;marketing;operations management;intelligent agent	AI	-39.43927406241853	16.5005012749843	114822
12c90a479a72a74db96de5f0eb9dce06a7056b9a	towards the creation of semantic models based on computer-aided designs	computer aided design;cad;semantic design;ontologies;survey	Nowadays one of the biggest problems many manufacturing companies face is the loss of knowledge from the information it possesses. Whether it tries to make business or improves the exchange of information within its different areas, valuable knowledge does not reach all stakeholders due to abstraction and ambiguity. A clear example in which both problems have a clear effect in terms of knowledge loss occurs during the interpretation of Computer-Aided Designs (CAD). If there is no experience doing such task the only data extracted will be limited to the elements contained on the drawing. By creating a semantic model we are able to know the contents specific details of a CAD without the use of a graphical tool, also ambiguity problems disappear as the terms used on the semantic model are based on a controlled vocabulary derived from an ontology.	computer-aided design	Nestor Velasco Bermeo;Miguel González-Mendoza;Alexander García Castro;Irais Heras Dueñas	2012		10.1007/978-3-642-37807-2_26	natural language processing;computer science;knowledge management;ontology;artificial intelligence;computer aided design;machine learning;data mining;cad	EDA	-45.39371155653807	5.800870870994616	115051
1031951a96d057f311e08b4675b029f12394cd1e	modeling the behavior of olap applications using an uml compilant approach	class diagram;gestion informacion;procesamiento informacion;almacenamiento informacion;interrogation base donnee;interrogacion base datos;conceptual model;information storage;information management;information processing;base donnee orientee objet;user requirements;stockage information;object oriented databases;information system;gestion information;traitement information;database query;systeme information;sistema informacion	We argue that current approaches in the area of conceptual modeling of OLAP applications are lacking in some basic multidimensional (MD) semantics as well as they do not consider the behavior of these systems properly. In this context, this paper describes an OO conceptual modeling approach (based on a subset of UML) to address the above-mentioned issues. Thus, the structure of the system is specified by a class diagram, and from then, we define cube classes for specific user requirements. Then, the behavior of the system is considered by means of state and interaction diagrams (based on certain OLAP operations) on these cube classes.	online analytical processing;unified modeling language	Juan Trujillo;Jaime Gómez;Manuel Palomar	2000		10.1007/3-540-40888-6_2	information processing;online analytical processing;computer science;conceptual model;user requirements document;applications of uml;class diagram;data mining;database;information management;programming language;world wide web;information system;algorithm	Robotics	-35.14938621870077	11.948689994437606	115108
c7a37de8b92d9e050a8502a0f924fc5f6a34b47c	terp: syntax for owl-friendly sparql queries		Web Ontology Language (OWL) [5] can be seen as an extension of Resource Description Framework (RDF). The primary exchange syntax for OWL is RDF/XML, and every OWL ontology can be represented as an RDF graph. But there is no standard query language specifically for OWL ontologies. The most commonly used Semantic Web query language is SPARQL [7], which is intended to be used for RDF. Roughly speaking, SPARQL is specified as queries matching RDF graphs with simple RDF entailment. However, it allows this definition to be extended to OWL entailment. A semantics for SPARQL compatible with OWL DL has been defined in SPARQL-DL [8] and a similar formalization of OWL-compatible SPARQL semantics is being developed by the W3C’s SPARQL Working Group as part of SPARQL 1.1. The semantics extension of SPARQL allows one to query OWL ontologies and get the expected results with respect to OWL entailments. However, writing SPARQL queries that involve complex OWL expressions ranges from challenging to unpleasant because SPARQL query syntax is based on Turtle [1], which isn’t intended for OWL. SPARQL queries against OWL data have to encode the RDF serialization of OWL expressions: these queries are typically verbose, difficult to write, and difficult to understand. In this paper we present Terp, a new syntax that combines Turtle and Manchester syntaxes to provide maximum legibility and conciseness when querying OWL with SPARQL. More precisely, Terp syntax allows class, property, and data range expressions, expressed in Manchester syntax, to be used inside SPARQL queries. In this paper, we provide examples to demonstrate how Terp reuses existing features from well-known syntaxes to make SPARQL queries of OWL data more concise and more legible.	encode;ontology (information science);query language;rdf/xml;resource description framework;sparql;semantic web;serialization;web ontology language;xml	Evren Sirin;Blazej Bulka;Michael Smith	2010			xml;web ontology language;semantic web;rdf;database;programming language;computer science;sparql;ontology (information science);query language;semantics	Web+IR	-35.46045094405302	6.662116852309539	115330
684251c8fe97911e016819c429ed298600dd6f14	achieving interoperability of genome db through intelligent web mediators	user interfaces genetics open systems distributed databases scientific information systems biology computing authorisation sql relational databases information resources internet application program interfaces;biology computing;animals;information resources;query language;genomics;user transparent disparity resolution;high level global view;authorisation;sql;genomics bioinformatics deductive databases genetics database languages distributed databases biotechnology animals computer science proteins;genome database interoperability;genetics;self maintainable web based interface genome database interoperability intelligent web mediators world wide web user friendliness database query interface declarative query language genetic solutions source independent syntax proteomics abstraction levels set based querying structural disparities semantic disparities user transparent disparity resolution sql3 compliant database query language genomic query language gql publicly accessible globally distributed genomic databases high level global view firewalls;internet;proteins;self maintainable web based interface;source independent syntax;application program interfaces;genomic query language;structural disparities;gql;intelligent web mediators;declarative query language;set based querying;sql3 compliant database query language;firewalls;distributed databases;abstraction levels;world wide web;relational databases;genetic solutions;computer science;proteomics;open systems;biotechnology;user interfaces;user friendliness;database languages;database query;database query interface;semantic disparities;scientific information systems;publicly accessible globally distributed genomic databases;bioinformatics;deductive databases	Resolving the tension between the j’lexibilities offered by a database query language and the so called LLuser-friendliness” supported by a f o r m based or m e n u driven database query interface i s undoubtedly a daunting task. However, researchers believe that the flexabilities offered by a declarative query language should be ful ly exploited t o harvest genetic solutions within the wealth of information offered by genomics and proteomics. Additionally, the need t o query databases at different abstraction levels, in a set based fashion, and process the queries efficiently by resolving the structural and semantic disparities among the genomic databases in a user transparent way has never been more evident t han in recent years. Inspired by such needs, we have developed a n S Q L 3 compliant database query language called Genomic Query Language, or G Q L , for manipulating globally distributed genomic databases that are publicly accessible. G Q L , a n S Q L like language, is given a source independent syntax and exploits a high level global view of the underlying databases. The uniqueness of this language relies heavily o n the supporting interface through which all communications take place and the interoperability of databases behind firewalls is realized. In this paper, we describe how the web based interface we have built for G Q L makes it possible to understand the scheme of any participating database, assimilates the syntactic and semantic i n format ion into the global view, and responds to user queries. The interface is autonomous f o r m a n y genomic databases and is also fu l l y self-maintainable.	autonomous robot;database;execution unit;firewall (computing);han unification;high-level programming language;interoperability;proteomics;query language	Hasan M. Jamil	2000		10.1109/BIBE.2000.889598	sargable;query optimization;genomics;data control language;computer science;bioinformatics;query by example;data mining;database;rdf query language;proteomics;web search query;view;world wide web;query language	DB	-37.7585523784205	4.927629528159923	115335
430942cd0c58e2f2534c4251109813914f751e66	service-based resource brokering for grid-based data mining	data mining;resource broker;gridbus;complex data	The shift towards intrinsically distributed complex problem solving environments is prompting a need for new systems, which utilize the virtually unlimited data and computational resources of the Grid and at the same time hide all the related complexity from the user. Currently, there is no coherent framework, which offers data miners, who are usually not Grid experts, the ability to easily construct data mining tasks and execute them on the Grid. Therefore, there is a need to assemble a complete system that includes: a) a user-friendly environment for defining complex data mining tasks and b) a Grid middleware that supports execution of such tasks, while utilizing mechanisms for managing data and computational resources as well as having sophisticated job-monitoring capabilities. This paper will focus on the high-level design of such a system, which currently is being developed in the DataMiningGrid project with emphasis on the design and implementation of the resource broker service. We show how different resources from various domains can be exploited, in order to give the data mining researchers the ability to access and utilize resources needed for modern, distributed and computationally intensive data mining algorithms.	algorithm;coherence (physics);computation;computational resource;data mining;emoticon;high- and low-level;level design;middleware;problem solving;usability	Valentin Kravtsov;Thomas Niessen;Vlado Stankovski;Assaf Schuster	2006			grid;data mining;data stream mining;complex data type;computer science;middleware	HPC	-36.61206775986803	17.619680082468392	115394
032fa142121383857d875f582ac0904b3aeb7db5	gradual trust and distrust in recommender systems	engineering;fuzzy set;procesamiento informacion;bilattice theory;web of trust;trust network;mathematics and statistics;fuzzy relation;conjunto difuso;ensemble flou;trust model;trust propagation;ingenierie;recommender system;trusted third party;information processing;ingenieria;sistema difuso;systeme flou;58a25;traitement information;fuzzy system	Trust networks among users of a recommender system (RS) prove beneficial to the quality and amount of the recommendations. Since trust is often a gradual phenomenon, fuzzy relations are the pre-eminent tools for modeling such networks. However, as current trust-enhanced RSs do not work with the notion of distrust, they cannot differentiate unknown users from malicious users, nor represent inconsistency. These are serious drawbacks in large networks where many users are unknown to each other and might provide contradictory information. In this paper, we advocate the use of a trust model in which trust scores are (trust,distrust)-couples, drawn from a bilattice that preserves valuable trust provenance information including gradual trust, distrust, ignorance, and inconsistency. We pay particular attention to deriving trust information through a trusted third party, which becomes especially challenging when also distrust is involved.	distrust;fuzzy concept;malware;recommender system;trusted third party;web of trust	Patricia Victor;Chris Cornelis;Martine De Cock;Paulo S Pinheiro	2009	Fuzzy Sets and Systems	10.1016/j.fss.2008.11.014	web of trust;information processing;trusted third party;computer science;artificial intelligence;data mining;fuzzy set;computer security;computational trust;fuzzy control system;recommender system	ECom	-37.785452444545086	17.030187536978545	115667
074ebd2264d76eafeb6120474af1f9e7066765d2	context-aware service provisioning in next-generation networks: an agent approach	next generation network;electronic markets;intelligent agents;context aware service;value chain;agent technology;semantic matching;knowledge discovery	This article presents an application of a multi-agent system in ubiquitous computing scenarios characteristic of next-generation networks. Next-generation networks will create environments populated with a vast number of consumers, which will possess diverse types of context-aware devices. In such environments, the consumer should be able to access all the available services anytime, from any place, and by using any of its communication-enabled devices. Consequently, next-generation networks will require efficient mechanisms which can match consumers’ demands (requested services) to network-operators’ supplies (available services). The authors propose an agent approach for enabling autonomous coordination between all the entities across the telecom value chain, thus enabling automated context-aware service provisioning for the consumers. Furthermore, the authors hope that the proposed service discovery model will not only be interesting from a scientific point of view, but also amenable to real-world applications.	anytime algorithm;autonomous robot;entity;multi-agent system;next-generation network;population;provisioning;service discovery;ubiquitous computing	Vedran Podobnik;Krunoslav Trzec;Gordan Jezic	2007	IJITWE	10.4018/jitwe.2007100103	next-generation network;value chain;computer science;knowledge management;artificial intelligence;database;service discovery;knowledge extraction;world wide web;intelligent agent	HCI	-42.58798602931124	15.892860454668602	115780
8626e40f1bcefd0d94d97eadd3b2b35f8b89fdef	a proposal for integrating artificial intelligence and database techniques	artificial intelligent	This paper describes some aspects of a current, interdisciplinary project carried out at the French National Center for Scientific Research (CNRS), and centered around a proposal conceming the integration of Artificial Intelligence (AI) and Database (DB) techniques. Our aim is to establish a sound methodology for the construction of Large Knowledge Based Systems (LKBSs) by combining the high-level information modeling features, the deductive capabilities, and the flexibility of the AI Knowledge Representation Languages (KRLs) with the characteristics of efficiency, security, concurrency, recovery and persistence of data provided by the Data Base Management Systems (DBMSs).We give in particular a succint description of the work already accomplished within the project in the knowledge representation area (set up of a powerful “hybrid representation system”).	artificial intelligence	Gian Piero Zarri	1990			computer science;knowledge management;artificial intelligence;data mining;database;programming language;world wide web	AI	-43.738809412865336	4.494315647516144	115994
db95fa61591388e18128b079832d70794d602a80	assessing semantic quality of web directory structure	task performance;semantics;web directory;semantic web;artificial intelligence;ontology;ontology alignment	The administration of a Web directory content and associated structure is a labor intensive task performed by human domain experts. Because of that there always exists a realistic risk of the structure becoming unbalanced, uneven and difficult to use to all except for a few users proficient in a particular Web directory. These problems emphasize the importance of generic and objective measures of Web directories structure quality. In this paper we demonstrate how to formally merge Web directories into the Semantic Web vision. We introduce a set of objective criterions for evaluation of a Web directory's structure quality. Some criteria functions are based on heuristics while others require the application of ontologies.	semantic web	Marko Horvat;Gordan Gledec;Nikola Bogunovic	2009		10.1007/978-3-642-04441-0_33	web service;web application security;ontology alignment;web development;web modeling;data web;web mapping;web design;web standards;computer science;artificial intelligence;semantic web;web navigation;ontology;social semantic web;semantic web stack;database;semantics;web intelligence;web 2.0;world wide web;owl-s;information retrieval	Web+IR	-44.665737570530986	11.187268001159037	116031
e45dfa56a9b9ac6577c2d4dfaa56c9bf8b496777	an efficient and flexible web services-based multidisciplinary design optimisation framework for complex engineering systems	multidisciplinary design;epistemic uncertainty;data sharing;data integrity;evidence theory;web service;multidisciplinary design optimisation;data model;engineering system;proof of concept;agent;web services;workflow;product design;data integration and sharing;ontology	Multidisciplinary design optimisation MDO involves multiple disciplines, multiple coupled relationships and multiple processes, which is implemented by different specialists dispersed geographically on heterogeneous platforms with different analysis and optimisation tools. The product design data integration and data sharing among the participants hampers the development and applications of MDO in enterprises seriously. Therefore, a multi-hierarchical integrated product design data model MH-iPDM supporting the MDO in the web environment and a web services-based multidisciplinary design optimisation Web-MDO framework are proposed in this article. Based on the enabling technologies including web services, ontology, workflow, agent, XML and evidence theory, the proposed framework enables the designers geographically dispersed to work collaboratively in the MDO environment. The ontology-based workflow enables the logical reasoning of MDO to be processed dynamically. The evidence theory-based uncertainty reasoning and analysis supports the quantification, aggregation and analysis of the conflicting epistemic uncertainty from multiple sources, which improves the quality of product. Finally, a proof-of-concept prototype system is developed using J2EE and an example of supersonic business jet is demonstrated to verify the autonomous execution of MDO strategies and the effectiveness of the proposed approach.	mit engineering systems division;mathematical optimization;multidisciplinary design optimization;web service	Liansheng Li;Jihong Liu	2012	Enterprise IS	10.1080/17517575.2011.651627	web service;computer science;systems engineering;knowledge management;ontology;data mining;database;product design;law	DB	-44.35745286678558	9.675893614022392	116089
3d768083cf31a790ed6cb29fde009ba7d92dc155	supporting e-commerce systems formalization with choreography languages	e commerce;web service;security requirements;web services;choreography;orchestration	E-commerce as well as B2B applications are essentially based on interactions between different people and organizations (e.g. industry, banks, customers) that usually exploit the Internet as communication media. Web Services provide a mean to deal with these aspects. In this paper we show, via a case study, how choreography and orchestration languages allow us to express behaviour policies between the involved entities (interactions modalities, interdependencies, security requirements); in particular we consider that they can be used not only for describing behavioural rules but also for designing and testing whether the involved entities move according with system specifications.	e-commerce payment system;entity;interaction;interdependence;requirement;web service	Mario Bravetti;Claudio Guidi;Roberto Lucchi;Gianluigi Zavattaro	2005		10.1145/1066677.1066867	e-commerce;web service;computer science;multimedia;world wide web;computer security	Web+IR	-47.88130523909549	17.117321354946494	116224
44fccf95893efea23c8e86705d618cc3b9e52ab8	exploiting tourism destinations' knowledge in an rdf-based p2p network	semantic web service;anotacion;eje troncal;arquitectura red;semantic annotation;adaptability;adaptabilite;keyword;model combination;management system;red www;metadata;interoperabilite;interoperabilidad;normalisation;par a par;web semantique;reseau ordinateur;reseau web;semantics;p2p;pregunta documental;annotation;palabra clave;mot cle;architecture reseau;semantica;semantique;semantic web technology;adaptabilidad;computer network;partage des ressources;reseau federateur;codificacion;poste a poste;adaptive management;servicio de red;diffusion information;web semantica;resource sharing;information dissemination;coding;normalizacion;metadonnee;particion recursos;query;semantic web;service reseau;red informatica;world wide web;design;network architecture;metadatos;interoperability;difusion informacion;p2p networks;backbone;analisis semantico;destination management systems;analyse semantique;peer to peer;management;network service;standardization;distributed rdf repositories;requete;documentation;semantic analysis;codage	Destination Management Systems (DMS) is a perfect application area for Semantic Web and P2P technologies since tourism information dissemination and exchange are the key-backbones of tourism destination management. DMS should take advantage of P2P technologies and semantic web services, interoperability, ontologies and semantic annotation. RDF-based P2P networks allow complex and extendable descriptions of resources instead of fixed and limited ones, and they provide query facilities against these metadata instead of simple keyword-based searches. The layered adaptive semantic-based DMS (LA_DMS) and Peer-to-Peer (P2P) project aims at providing semantic-based tourism destination information by combining the P2P paradigm with Semantic Web technologies. In this paper, we propose a metadata model encoding semantic tourism destination information in an RDF-based P2P network architecture. The model combines ontological structures with information for tourism destinations and peers. r 2006 Elsevier Ltd. All rights reserved.	antivirus software;data mining;digital signature;electronic business;embedded system;emergence;expressive power (computer science);extensibility;feedback;information system;knowledge management;network architecture;ontology (information science);peer-to-peer;personalization;population;programming paradigm;requirement;resource description framework;semantic web service;semantic interoperability;software agent;web services interoperability	Dimitris Kanellopoulos;Alkiviadis A. Panagopoulos	2008	J. Network and Computer Applications	10.1016/j.jnca.2006.03.003	interoperability;design;semantic computing;adaptability;network architecture;semantic grid;documentation;computer science;semantic web;social semantic web;data mining;management system;semantic web stack;database;semantics;coding;metadata;world wide web;semantic analytics;standardization;computer network	DB	-38.332248253577646	12.135334496147618	116243
b9fd3a4a6e66eec867bf9521786ef9d80ebdd852	fuzzy query interface for a business database	pseudo natural language;relational database;fuzzy logic;economic and financial indicators;natural language;fuzzy adverbs;profitability;natural language processing;flexible queries	Managers, in today's corporations, rely increasingly on the use of databases to obtain insights and updated information to make their decisions. This paper describes a flexible query interface based on fuzzy logic. Hence, queries in natural language with pre-defined syntactical structures are performed, and the system uses a fuzzy natural language process to provide answers. This process uses the fuzzy translation rules of the meaning representation language PRUF, proposed by [Zadeh, 1978 #735]. The interface was built for a relational database of the 500 biggest non-financial Portuguese companies. The attributes considered are the economic and financial indicators. Examples of pseudo natural language queries, such as “is company X very profitable?” or “are most private companies productive?”, are presented to show the capabilities of this human-oriented interface.	context-sensitive language;fuzzy logic;legacy system;linear algebra;natural language processing;query language;relational database;relational operator;sql;usability;dialog	Rita Almeida Ribeiro;Ana Moreira	2003	Int. J. Hum.-Comput. Stud.	10.1016/S1071-5819(03)00010-7	fuzzy logic;natural language processing;language identification;natural language user interface;data control language;relational database;computer science;artificial intelligence;data mining;database;linguistics;natural language;fuzzy control language;profitability index	DB	-37.134806013577	8.294917640099111	116300
5dfeb16e2ac4a9b069f7e59d059680ba22fca413	a context query language for pervasive computing environments	query language;aggregation function;context information;information retrieval;pervasive computing;information filtering;database languages pervasive computing mobile computing information filtering information filters ontologies information retrieval subscriptions context telecommunications;complex filtering mechanisms;aggregation functions;heterogeneous representations;context query language;satisfiability;ontologies artificial intelligence;query languages;context model;ubiquitous computing ontologies artificial intelligence query languages;cql;ontology integration;ontology context query language cql context model pervasive computing;subscriptions;ubiquitous computing;ontologies;ontology integration context query language pervasive computing environments context information mobile computing heterogeneous representations complex filtering mechanisms aggregation functions;pervasive computing environments;mobile computing;information filters;ontology;database languages;context;telecommunications	This paper identifies requirements for querying and accessing context information in mobile and pervasive computing environments. Furthermore it studies existing query languages showing that they satisfy only a subset of these requirements or cover some of them only to a limited extent. A new context query language is presented to overcome these shortcomings, improving the state of the art in several respects: heterogeneous representations of context information, definition of complex filtering mechanisms, elaborate aggregation functions and ontology integration, all in one language.	aggregate function;contextual query language;finalize (optical discs);music-n;middleware;requirement;ubiquitous computing	Roland Reichle;Michael Wagner;Mohammad Ullah Khan;Kurt Geihs;Massimo Valla;Cristina Frà;Nearchos Paspallis;George Angelos Papadopoulos	2008	2008 Sixth Annual IEEE International Conference on Pervasive Computing and Communications (PerCom)	10.1109/PERCOM.2008.29	query expansion;human–computer interaction;computer science;data mining;database;context model;rdf query language;ubiquitous computing;information retrieval;query language	DB	-42.86820759698659	11.38011995258109	116420
e3ddcb57b5341bbf5c848a7a777926a91b4781e4	change propagation based incremental data handling in a web service discovery framework	web services crawlers uniform resource locators web pages portals metadata data mining;incremental processing strategy incremental data handling web service discovery framework heterogeneous sources bottom up approach published service descriptions service repository service crawler url change propagation technique event based state machine web scale framework;portals;web service discovery;service crawler;web pages;change management;metadata;data mining;incremental processing;web services data handling finite state machines;web services;crawlers;uniform resource locators;incremental processing web service discovery service crawler change management	Due to the explosive growth in availability of Web services over the open Web and the heterogeneous sources in which they are available, discovering relevant web services for a given task continues to be challenging. In order to deal with these problems, a bottom-up approach based on finding published service descriptions to automatically build a service repository for developing a web service discovery framework was proposed. This framework employs a Service Crawler to find published service descriptions on the Web. Since the service crawler will periodically make repeated runs to find new service descriptions or to check the continued availability of already crawled services, the framework is of an inherently dynamic nature. Hence, it is critical to keep track of various entities like visited URLs, already added & successfully processed service descriptions to avoid rework when the service data set changes. In order to cope with this problem, we developed a change propagation technique based on an event based state machine to incorporate an incremental processing strategy in this Web scale framework.	algorithm;bottom-up parsing;change management (engineering);entity;finite-state machine;open web;portals;rework (electronics);scalability;service discovery;software propagation;web crawler;web service;world wide web	S. Kamath SowmyaKamath;S. V. AnanthanarayanaV.	2014	2014 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)	10.1109/ISSPIT.2014.7300635	web service;web application security;web mining;static web page;web development;web modeling;data web;web mapping;web standards;computer science;web crawler;ws-policy;change management;web page;data mining;database;focused crawler;web 2.0;metadata;world wide web;universal description discovery and integration;web server;mashup;web coverage service	Embedded	-43.879023838472634	12.544876299092447	116425
b9287ce3f81eb94ff45f5289dba55fab2f4159b5	enabling language resources to expose translations as linked data on the web	informatica	Language resources, such as multilingual lexica and multilingual electronic dictionaries, contain collections of lexical entries in several languages. Having access to the corresponding explicit or implicit translation relations between such entries might be of great interest for many NLP-based applications. By using Semantic Web-based techniques, translations can be available on the Web to be consumed by other (semantic enabled) resources in a direct manner, not relying on application-specific formats. To that end, in this paper we propose a model for representing translations as linked data, as an extension of the lemon model. Our translation module represents some core information associated to term translations and does not commit to specific views or translation theories. As a proof of concept, we have extracted the translations of the terms contained in Terminesp, a multilingual terminological database, and represented them as linked data. We have made them accessible on the Web both for humans (via a Web interface) and software agents (with a SPARQL endpoint).	communication endpoint;dictionary;lexicon;linked data;natural language processing;sparql;semantic web;software agent;theory;world wide web	Jorge Gracia;Elena Montiel-Ponsoda;Daniel Vila-Suero;Guadalupe Aguado de Cea	2014			natural language processing;computer science;database;linguistics;world wide web	NLP	-40.235610173312395	8.41092262480284	116842
7ffa6a06f05cd9b480aa30e0841ba79e84edea22	description logic based bdi implementation for goal-directed semantic agents	content management;programming language semantics;argumentation;concrete semantic web cognition service oriented architecture computer architecture barium robots;semantic web content management formal logic multi agent systems programming language semantics;computer architecture;multi agent systems;bdi agents;formal logic;semantic web;belief desire intention description logic bdi goal directed semantic agents intelligent agents web content semantic web languages dl based goal model;possibility theory;description logic;service oriented architecture	The Semantic Web, in its visionary architecture, employs intelligent agents fulfilling the user goals on the web content that is declared with Description Logic (DL) based Semantic Web languages. In order to meet this task, two important points must be taken into account in agent frameworks. First, frameworks must support comprehensive goal models that allow to pursue goals rationally. Second, these goal models must be integrated with the Semantic Web languages to enable defining goals depending on the web content. However, to execute and manage such goal models, elements of the agent architecture must be adapted with respect to the Semantic Web languages and the DL components behind these languages. For this purpose, in this paper, we propose a DL based goal model and introduce a Belief-Desire-Intention (BDI) architecture which is built on top of DL components. In this architecture, we focus on how declarative goals are represented and managed.	agent architecture;belief–desire–intention software model;declarative programming;description logic;intelligent agent;semantic web;web content	Tayfun Gökmen Halaç;Erdem Eser Ekinci;Oguz Dikenelli	2011	2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2011.192	possibility theory;semantic computing;web modeling;description logic;semantic web rule language;semantic search;semantic grid;content management;web standards;computer science;knowledge management;artificial intelligence;service-oriented architecture;semantic web;social semantic web;multi-agent system;semantic web stack;database;programming language;logic;semantic analytics	AI	-43.473927964374944	14.4757407831916	116892
3777b6ce389cbd19a735fb030b927e0a38d68fce	hemocomponents and hemoderivatives ontology (hemonto): an ontology about blood components		Ontologies has been widely used in the formal description of scientific knowledge, as well in the practice of the conceptual modeling. Considering the description of scientific knowledge, several ontologies has been proposed in the biomedical domain. This article describes an ongoing research in the domain of blood transfusion, presenting the construction of a domain ontology about hemocomponents and hemoderivatives. Such ontology, named HEMONTO, has been developed using top-level ontologies, biomedical domain ontologies, among other resources. The ontology is based in a set of philosophical principles that has been identified in the literature under the label “ontological realism”and relies on technologies developed in the scope of Semantic Web. HEMONTO aims to provide both a knowledge repository about blood transfusion and an auxiliary instrument for modeling and evaluation of the information systems. The results presented here refer to the partial content of the ontology, encompassing classes, relations and representation diagrams.	diagram;information system;ontology (information science);semantic web	Fabrício Mendonça;Mauricio Barcellos Almeida	2013			ontology;semantic web;knowledge management;conceptual model;information system;sociology of scientific knowledge;ontology (information science);computer science	AI	-42.91108896503364	4.741639372287089	116920
95f7392d63d8eb7507fae1037adc4ae706b3ece1	access control validation by ontologies	query processing;authorisation;query processing authorisation inference mechanisms linux ontologies artificial intelligence;inference mechanisms;owl access control windows permissions linux permissions rbac acl ontology;ontologies artificial intelligence;access control ontologies permission linux owl resource description framework;linux;access control validation automatic access control information extraction windows linux devices data extract ontology reasoning engines ontology querying security policy	The aim of this paper is to show how we can automatically extract access control information from Windows and Linux devices in order to analyze them in an automatic way by using ontologies. The extracted data related to access control are used to instantiate a defined ontology. After that, some reasoning engines are used to query the ontology in order to understand the implemented security policy or to compare it with an expected one.	access control;inference engine;linux;microsoft windows;ontology (information science)	Mohamed Mejri;Etienne Theodore Sadio;Khadija Arrachid;Memel Emmanuel Lathe	2013	2013 IEEE 12th International Conference on Intelligent Software Methodologies, Tools and Techniques (SoMeT)	10.1109/SoMeT.2013.6645642	computer science;ontology;operating system;data mining;database;authorization;world wide web;linux kernel;process ontology	SE	-37.67046358057521	8.874855911680172	117078
168a3bc4d3b6dfe35f198d7c50158ea4a03386f5	selecting and tailoring ontologies with joyce		The final publication is available at Springer via https://link.springer.com/chapter/10.1007/978-3-319-58694-6 12. Abstract. We present Joyce, a scalable tool for identifying and assembling relevant (pieces of) ontologies from a repository of source ontologies, thus enabling the effective and efficient reuse of formalized domain knowledge. Joyce includes a conceptual filter to identify relevant classes, minimizes unintended redundancies, i.e. concept duplicates, and excludes knowledge considered irrelevant for the specific conceptual design task.	joyce;ontology (information science);relevance;scalability;springer (tank)	Erik Faessler;Friederike Klan;Alsayed Algergawy;Birgitta König-Ries;Udo Hahn	2016		10.1007/978-3-319-58694-6_12	theoretical computer science;algorithm	AI	-43.43013356556296	5.485696620115678	117127
4658d79f33e34b176d19de243e0e85721050a812	xml query object model: a general view	object model	We have observed an increase in document search needs in organizations. This increase in document search demand has required better efficiency on search software languages. The current technologies are using specific patterns to search documents. The most usual pattern, called XML, has been proposed by W3C and some languages (e.g. XQL, XML-QL etc.) were created to recover XML documents. The implementation of a XML query server, which abstracts the language used, has been an emerging necessity. This paper surveys technology used to develop a XML query server, called XQOM Schema, from the modelling to the implementation. In an organization setting, we have adopted new abstraction forms of searching for XML documents without the usage limitation established by a specific language. XQOM Schema may contribute to the creation of an universal document searching pattern in XML format.	server (computing);xml;xquery	Fábio Ribeiro Silvestre;Hernane Borges de Barros Pereira	2006			xml validation;simple api for xml;method;deep-sky object;object model;semi-structured model;object code;computer science;xml schema editor;object query language;object definition language	Web+IR	-34.34178768129099	8.507720905480339	117135
18a5ca04726163e5e82143dde3e5f74c1fe68ce7	on-the-fly construction of provably correct service compositions - templates and proofs	verification;hoare calculus;correctness by construction;templates;service compositions	Today, service compositions often need to be assembled or changed on-the-fly, which leaves only little time for quality assurance. Moreover, quality assurance is complicated by service providers only giving information on their services in terms of domain specific concepts with only limited semantic meaning.In this paper, we propose a method for constructing service compositions based on pre-verified templates. Templates, given as workflow descriptions, are typed over a (domain-independent) template ontology defining concepts and predicates. Their meaning is defined by an abstract semantics, leaving the specific meaning of ontology concepts open, however, only up to given ontology rules. Templates are proven correct using a Hoare-style proof calculus, extended by a specific rule for service calls. Construction of service compositions amounts to instantiation of templates with domain-specific services. Correctness of an instantiation can then simply be checked by verifying that the domain ontology (a) adheres to the rules of the template ontology, and (b) fulfills the constraints of the employed template. Create service compositions from templates.Proof calculus to verify templates.Compositions are provably correct by construction.	correctness (computer science);foreach loop;postcondition;universal instantiation	Sven Walther;Heike Wehrheim	2016	Sci. Comput. Program.	10.1016/j.scico.2016.04.002	verification;computer science;database;programming language;algorithm	Logic	-41.86467254560456	13.758562621076978	117286
43c2e469d5938442299201d391b6c59abef41dca	web service selection for multiple agents with incomplete preferences	directed graphs;web service selection;web services directed graphs multi agent systems quality of service;qualitative graphical representation tool;cp nets;service selection;services multiple agents services selection preference;web service;satisfiability;directed graph web service selection nonfunctional quality of service qos cp nets qualitative graphical representation tool;qos;multi agent systems;directed graph;graphical representation;web services;nonfunctional quality of service;incomplete preferences;preference;services;quality of service;multiple agents;services selection	A qualitative way is desirable for web service selection according to agents’ preferences on non-functional Quality of Service (QoS) attributes of services. However, it is challenging when the decision has to be made for multiple agents with preferences on attributes that may be incomplete. In this paper, we first use a qualitative graphical representation tool called CP-nets to describe preference relations in a relatively compact, intuitive and structured manner. We then propose algorithms based on this representation to select web services for multiple agents despite the presence of incompleteness in their preference orderings. Our experimental results indicate that this method can always obtain some optimal outcomes which closely satisfy all agents.	algorithm;graphical user interface;quality of service;web service	Hongbing Wang;Jie Zhang;Cheng Wan;Shizhi Shao;Robin Cohen;Junjie Xu;Peicheng Li	2010	2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2010.27	web service;directed graph;quality of service;computer science;multi-agent system;data mining;database;law;world wide web	AI	-44.75136252575279	15.428315294657809	117382
c27524b23b31b5a8ac73719abba44b0e22a36d53	type-aware web service composition using boolean satisfiability solver	shortest path type aware web service composition boolean satisfiability solver web services syntactic features semantic features wsc problem semantic aspects supertype subtype relationship web service description requirement web service reachability problem state transition system;shortest path;state transition system;owl;supertype subtype relationship;reachability problem;web and internet services;wsc problem;probability density function;computability;software systems;requirement web service;usa councils;web service;satisfiability;data mining;boolean algebra;web service composition;boolean satisfiability;sat;boolean satisfiability solver;semantic aspects;formal verification;syntactic features;web services usa councils proposals artificial intelligence software systems software design web and internet services service oriented architecture semantic web formal verification;web services;semantic web;artificial intelligence;planning;optical wavelength conversion;software design;service oriented architecture;semantic features;proposals;web services boolean algebra computability;web service description;type aware web service composition;sat web service composition boolean satisfiability	"""The goal of the Web Service Composition (WSC) problem is to find an optimal """"composition"""" of web services to satisfy a given request using their syntactic and/or semantic features, when no single service satisfies it. In this paper, in particular, we study the WSC problem from semantic aspects, exploiting the supertype-subtype relationship among parameters, and propose a novel solutionbased on techniques for the boolean satisfiability problem (SAT). Given a set of web service descriptions and a requirement web service, we reduce the WSC problem into a reachability problem on a state-transition system, and then we find the shortest path for the reachability problem, which is amount to the optimal composition. A preliminary experiment using 7 examples reveals that our proposal can find optimal compositions of web services efficiently."""	binary search algorithm;boolean satisfiability problem;model checking;ontology (information science);reachability problem;refinement (computing);shortest path problem;solver;state space;transition system;web service;world sudoku championship	Wonhong Nam;Hyunyoung Kil;Dongwon Lee	2008	2008 10th IEEE Conference on E-Commerce Technology and the Fifth IEEE Conference on Enterprise Computing, E-Commerce and E-Services	10.1109/CECandEEE.2008.108	web service;computer science;artificial intelligence;maximum satisfiability problem;theoretical computer science;database;world wide web	DB	-44.513208266377994	14.919194782487466	117567
d241c90acc960efd1990f412a7665200eb7c338e	toward an ontology in the domain of information systems delivery and evolution	information system	In this study we examine the relevant information systems delivery and evolution literature in order to develop an ontology for helping to reduce the fuzziness and ambiguity that often arises from inconsistent use of terminology in this domain. Such an undertaking necessarily encompasses a broader set of concepts that can be accommodated in this paper. We have therefore narrowed the scope of this ontology to the more controversial terms used at the heart of the information systems life cycle. We hope to extend knowledge by contributing to a standard vocabulary, classifying concepts and terms, identifying synonyms and homonyms, and providing clarifying examples (instances) in order to facilitate scholarly activities that require a deeper understanding of the structure of information in this domain. We use a frame-like representational scheme for the ontology and set the stage for validating the results using brainstorming and card sort to evaluate it effectiveness.	americas conference on information systems;information system;ontology (information science);system lifecycle;vocabulary	Lila Rao-Graham;Gunjan Mansingh;Evan W. Duggan	2006			ontology-based data integration;process ontology	AI	-46.33024858299426	6.266868286958068	117609
9cbc47f96b3bc3a51f7929cb9c98b1cd3c853496	the conceptual basis of with, a collaborative writer system of clinical trials	semantic network;clinical trial;process management;process design;standardisation;relational database management system;collaborative writing	The paper illustrates the conceptual basis of the system WITH (Write on Internet clinical Trials in Haematology) which supports the collaborative writing of a clinical trial document. The requirements of this system have been defined analysing the writing process of a clinical trial and then modelling the content of its sections together with their logical and temporal relationships. This analysis generates a semantic network. The system WITH is based on XML mark-up language, and on a relational database management system. This choice guarantees: a) process standardisation; b) process management; c) efficient delivery of information-based tasks; and d) explicit focus on process design.		Paola Fazi;Daniela Luzi;Fabrizio L. Ricci;Marco Vignetti	2002		10.1007/3-540-36104-9_10	computer science;knowledge management;data mining;database	AI	-47.850023699130986	6.886805312740189	117771
b0ee94133704cd4e4a3c3476fa765ecbe83f14a1	interaction models for profiling assets in an extensible and semantic wot framework				Mohammed Amir;Yim-Fun Hu;Prashant Pillai;Yongqiang Cheng;Kirils Bibiks	2013			theoretical computer science;data mining	NLP	-39.51297774678606	7.128514521023621	117916
cf2fc1e1ec546e33128f2a0102c7502ab7a3c6cd	semantic web services - fundamentals and advanced topics	semantic web service;developpement logiciel;protocolo acceso;business to business;replacement;remplacement;client server architecture;architecture client serveur;integration information;semantic integration;web semantique;service web;entreprise vers entreprise;web service;access protocol;remote procedure call;information integration;lenguaje descripcion;appel procedure a distance;internet;object oriented;desarrollo logicial;web semantica;software development;integracion informacion;architecture editeur souscripteur;semantic web;arquitectura cliente servidor;oriente objet;reemplazo;arquitectura publicacion suscripcion;protocole acces;enterprise application integration;orientado objeto;remote procedure calls;publish subscriber architecture;empresa hacia empresa b2b;langage description;servicio web;description language	The 'traditional' Web Service triple SOAP, WSDL and UDDI, while widely praised as the next Silver Bullet, has been heavily criticized at the same time as being just the most recent replacement technology for remote procedure calls. The main criticism lies in the syntactic nature that SOAP, WSDL and UDDI retain compared to previous solutions: Enterprise Application Integration (EAI) and Business-to-Business (B2B) Integration are possible on a syntactic level only; however, the biggest problem - the Semantic Integration - still cannot be solved by traditional Web Services at all, whatsoever. The goal of Semantic Web Services is to change exactly that. Fundamentals of Semantic Web Services are discussed as well as advanced topics that are necessary for Business Integration in the real sense.	semantic web service	Christoph Bussler	2004		10.1007/978-3-540-30196-7_1	web service;computer science;operating system;social semantic web;semantic web stack;database;ws-i basic profile;remote procedure call;world wide web;computer security;universal description discovery and integration	DB	-35.804685593640876	12.760671714396569	118148
621b34498735b9ca497ab1e0ae4219708b8370ae	the sri tipster iii project	sri tipster phase;complicated rule;data extraction;new domain pattern;sri tipster iii project;nonexpert user;iii research program;linguistic approach;phase ii;information extraction	One step towards ease-of-use by nonexperts was the development reported in Phase II [1] of SRI's FastSpec language which enabled greater facility in generating and modifying the syntactic and semantic patterns necessary for identifying pertinent data. This was a motivating factor for the establishment of the Common Pattern Specification Language (CPSL) Working Group devoted to formulating a CPSL in order that different IE systems could share pattern libraries .2	library (computing);relevance;specification language	Steven J. Maiorano	1998			engineering;knowledge management;data science;data mining	SE	-43.342464200135076	4.29774916286075	118656
c233f2e1a90c23c7a5cb99049511857aecbb9a84	dht facilitated web service discovery incorporating semantic annotation	lenguaje programacion;distributed system;sistema operativo;composite web service;semantic annotation;ontologie;reseau pair;web service discovery;systeme reparti;metadata;programming language;gollete estrangulamiento;web semantique;service web;p2p;sistema n niveles;web service;application integration;integration et decouverte description universelle;igual a igual p2p;goulot etranglement;sistema repartido;operating system;systeme n niveaux;web semantica;multilevel system;metadonnee;architecture editeur souscripteur;semantic web;langage programmation;ontologia;systeme exploitation;arquitectura publicacion suscripcion;metadatos;peer to peer;bottleneck;publish subscriber architecture;ontology;servicio web	Web services enable seamless application integration over the net| work regardless of programming language or operating system. A critical factor to the overall utility of Web services is an efficient discovery mechanism. In this paper, limitations of typical UDDI registry are analyzed. We propose a novel decentralized Web services registry infrastructure. We extract three levels metadata from Web services description. These metadata are annotated with service ontology, which acts as the bridge of P2P (peer to peer) and Web services. We have discussed multi-level Web services discovery under this infrastructure. Our experiment results testify that this infrastructure can effectively overcome bottlenecks of traditional UDDI registry. The evaluation shows that WordNet lexical dictionary facilitated and composite Web services discovery excel notably with large amounts of services.		Shoujian Yu;Jianwei Liu;Jiajin Le	2004		10.1007/978-3-540-30214-8_32	web service;web application security;web development;web modeling;data web;web mapping;web standards;computer science;ws-policy;service-oriented architecture;semantic web;ontology;social semantic web;peer-to-peer;data mining;ws-addressing;semantic web stack;database;services computing;web intelligence;ws-i basic profile;web 2.0;metadata;world wide web;devices profile for web services;universal description discovery and integration	NLP	-37.98329827715131	12.438638878089543	118711
1348bd33687f747d929f13bb7fb6ad91bfce3e44	applying a knowledge based system for metadata integration for data warehouses	distributed system;knowledge based system;cwm;computer and information science;software engineering;common warehouse metamodel;metadata integration;data quality;data warehouse;data och informationsvetenskap	Data warehouses is a typical example of distributed systems where diverse tools and platforms need to communicate to understand each other. For the communication, metadata integration is significant. Seamless metadata interchange improves the data quality and the system effectiveness. Metadata standards exist, for instance, Common Warehouse MetaModel (CWM), which have enhanced the metadata integration. However, it is far from solving the problem of metadata integration in data warehouse environment. This paper proposes an approach to apply a knowledge-based system that supports the metadata integration. By utilizing the knowledge of software engineers on Common Warehouse MetaModels and the metadata interchange models, the knowledge-based system can give metadata interchange model suggestions. Such a knowledge-based system intends to partly automate the metadata integration to improve the efficiency and the quality of metadata integration in data warehouses.		Dan Wu;Anne Håkansson	2010		10.1007/978-3-642-15384-6_7	metadata modeling;geospatial metadata;software mining;data transformation;computer science;data mining;database;metadata;data element;meta data services;information retrieval;data mapping;metadata repository;data dictionary	DB	-43.78488845342045	6.28918608778081	118744
aba075e1869bfcf599cd3fa4c0c40f3258361531	a geosemantic proximity-based prototype for the interoperability of geospatial data	base donnee;interoperabilite;interoperabilidad;analisis espacial;research agenda;accesibilidad;implementation;database;base dato;conceptual framework;prototipo;internet;accessibility;interoperability;spatial analysis;analisis semantico;implementacion;analyse semantique;experimentation;prototype;geospatial data;analyse spatiale;accessibilite;semantic analysis;experimentacion	The research agenda related to the interoperability of geospatial data is influenced by the increased accessibility of geospatial databases on the Internet, as well as their sharing and their integration. Although it is now possible to get and use geospatial data independently of their syntax and structure, it is still difficult for users to find the exact data they need as long as they do not know the precise vocabulary used by the organizations supporting geospatial databases. It is now a necessity to take into consideration the semantics of geospatial data to enable its full interoperability. To this end, we designed a new conceptual framework for geospatial data interoperability and introduced the notion of geosemantic proximity based on human communication and cognition paradigms. This paper reviews this framework and the notion of geosemantic proximity. It also presents the GsP Prototype, which demonstrates the relevance of our framework and of the notion of geosemantic proximity for geospatial data interoperability. More specifically, we describe the architecture of the GsP Prototype, its implementation, and tests that have been conducted. 2004 Elsevier Ltd. All rights reserved. 0198-9715/$ see front matter 2004 Elsevier Ltd. All rights reserved. doi:10.1016/j.compenvurbsys.2004.04.001 * Corresponding author. Fax: +1 819 564 5698. E-mail address: brodeur@rncan.gc.ca (J. Brodeur). 1 Also at Centre for Research in Geomatics (CRG). 670 J. Brodeur et al. / Comput., Environ. and Urban Systems 29 (2005) 669–698	accessibility;application domain;classification research group;cognition;database;fax;geomatics;interaction;internet;interoperability;like button;natural language;ontology (information science);prototype verification system;relevance;research data archiving;software agent;theory;topography;transmitter;vocabulary;world wide web;xml	Jean Brodeur;Yvan Bédard;Bernard Moulin	2005	Computers, Environment and Urban Systems	10.1016/j.compenvurbsys.2004.04.001	semantic interoperability;interoperability;the internet;computer science;geospatial analysis;accessibility;data mining;conceptual framework;database;spatial analysis;prototype;implementation;law;world wide web	DB	-37.9386570392364	11.403341762897213	118774
d66796d6112c4105cde37a12479511b9162c0778	development of knowledge management based on task circumstance	semantic similarity;computers;task circumstance semantic similarity;knowledge reuse;knowledge management;knowledge match algorithm;ontologies artificial intelligence knowledge management;semantics;ontologies artificial intelligence;task circumstance annotation;semantic similarity task circumstance knowledge management ontology;computer architecture;knowledge match algorithm knowledge management domain ontology task circumstance annotation task circumstance minitree task circumstance semantic similarity;ontologies semantics knowledge management organizations context computer architecture computers;task circumstance minitree;ontologies;task circumstance;organizations;domain ontology;ontology;context	Knowledge Management (KM) plays the key role in upgrading the competitiveness of an organization, one of objectives of KM is to promote the share and reuse of all kind of knowledge. To this end, task circumstance(TC) of knowledge is crucial. Base on task circumstance, this paper proposes a framework of KM to facilitate knowledge reuse. According to the definition of domain ontology, this paper introduces the concept of TC, and then presents a method of TC annotation. Base on the TC MiniTree, we propose a novel method of evaluating TC semantic similarity, and finally introduce a knowledge match algorithm to retrieve appropriate knowledge for application precisely and quickly.	algorithm;knowledge management;ontology (information science);semantic similarity	Ming-jian Zhou;Jun-cai Tao	2010	2010 International Conference on E-Business and E-Government	10.1109/ICEE.2010.455	semantic similarity;computer science;organization;knowledge management;ontology;body of knowledge;knowledge-based systems;ontology;data mining;database;semantics;procedural knowledge;knowledge value chain	AI	-43.32903754145816	7.3933260084432195	118788
519560993740bb92ec1b11a6370a461ac73b9691	utilisation of case-based reasoning for semantic web services composition	semantic web service;case base reasoning;sw;web services;semantic web;cbr;matchmaking and composition;case based reasoning;institutional repository research archive oaister	With the rapid proliferation of Web services as the medium of choice to securely publish application services beyond the firewall, the importance of accurate, yet flexible matchmaking of similar services gains importance both for the human user and for dynamic composition engines. In this article, we present a novel approach that utilizes the case based reasoning methodology for modelling dynamic Web service discovery and matchmaking, and investigate the use of case adaptation for service composition. Our framework considers Web services execution experiences in the decision making process and is highly adaptable to the service requester constraints. The framework also utilizes OWL semantic descriptions extensively for implementing both the components of the CBR engine and the matchmaking profile of the Web services.	case-based reasoning;firewall (computing);semantic web service;service composability principle;service discovery;web ontology language;world wide web	Taha Osman;Dhavalkumar Thakker;David Al-Dabass	2009	IJIIT	10.4018/jiit.2009092102	web service;web application security;case-based reasoning;web development;web modeling;data web;web standards;computer science;knowledge management;artificial intelligence;ws-policy;semantic web;social semantic web;ws-addressing;semantic web stack;database;services computing;web intelligence;ws-i basic profile;web 2.0;world wide web;owl-s	Web+IR	-44.47390147336724	13.541708954519851	118817
0f897298b1c85770a9f9d56a5d154090b52e0702	automatic data reuse in grid workflow composition	data reuse;soa;web service;web services;semantic description;grid computing;semantic grid;automated workflow composition	Many papers, research projects, and software products have tackled the problem of automatic composition of a workflow of computer processes, which computes certain data or performs a specific task. In recent years this has also gained popularity in grid computing, especially in connection with semantic description of resources usable in the workflow. However, most of the works dealing with semantically-aided workflow composition propose solutions only for workflows of processes, without the data necessary to execute them. We describe the design of a system, which will be able to find not only the processes, but also the content for their execution, based solely on the list of available resources and a description of the required target of the workflow. The solution is based on our previous work in the project K-Wf Grid, utilizes semantic description of resources by means of ontologies, and operates on a SOA-based grid composed of web services. It is being developed in the context of a project called SEMCO-WS.		Ondrej Habala;Branislav Simo;Emil Gatial;Ladislav Hluchý	2008		10.1007/978-3-540-69384-0_25	web service;workflow;semantic grid;computer science;data mining;semantic web stack;database;windows workflow foundation;world wide web;workflow management system;workflow engine;grid computing;workflow technology	HPC	-45.30499347037444	13.425459752748212	118889
a9cea9392998af3ff7be02238a67be74b97fc94b	e-service classification techniques to support discovery in a mobile multichannel environment	quality factor;ontologies information systems adaptive systems context aware services web services q factor quality of service web and internet services design methodology distributed information systems;classification;e service classification mobile multichannel environment dynamic service delivery reference quality parameters ontology based approach;user profile;internet;mobile computing classification knowledge representation internet;service discovery;knowledge representation;mobile computing;service delivery	Featuring aspect of a mobile multichannel environment is that users want to be able to obtain e-services in any context in which they can operate and always with the best possible performances. Therefore, a flexible and adaptive e-service delivery approach is essential to cope with possible limitations imposed by changes in the context (i.e., user profile, geographical location, available channels and usable devices). In particular, an effective dynamic delivery of services requires the capability of discovering groups of services, which can substitute each other, with similarity properties based on their functionalities and quality factors. In this framework, a relevant role is thus assumed by methods and tools for the classification of services by considering offered functionalities and reference quality parameters. In this paper, we present an ontology-based approach and classification techniques based on the analysis of e-service descriptions, for both functional and nonfunctional aspects, and we discuss how they support e-service discovery.	e-services	Devis Bianchini;Valeria De Antonellis;Michele Melchiori	2003		10.1109/WISEW.2003.1286809	mobile qos;the internet;biological classification;computer science;service delivery framework;operating system;data mining;database;service discovery;services computing;q factor;mobile computing;world wide web	Mobile	-44.952998074603364	13.109525043357129	118909
d5a22f9fdcc54825bec8a18cf983804eac8e225d	towards the faster transformation of xml documents	document structure;documento;balisage document;xml schema;estructura documental;structure document;xml language;xslt extensible stylesheet language transformations;transformacion;markup;document;algorithme;algorithm;xml document;etiqueta;transformation;experimentation;langage xml;lenguaje xml;xslt script;experimentacion;algoritmo	XML is so flexible that several different schemas are often used even in the same application domain. To interchange XML documents between two parties, it is necessary to transform XML documents into ones that conform to the schema of a partner. Since a transformation is repeatedly applied to a large volume of XML documents, the transformation speed is important. This paper proposes a method for generating XSLT scripts, which support the fast transformation of XML documents, given one-to-one matching relationships between leaf nodes of XML schemas. The proposed method consists of two steps: computing matches between internal nodes and generating XSLT scripts. Specifically, the proposed method considers many-to-one matches among cardinality nodes. The transformation of recursive structures is also supported. The proposed method generates XSLT scripts with fewer templates that are proportional to the number of the matches between recursive nodes. Experimental results show that the proposed method generates XSLT scripts that support the faster transformation of XML documents, compared with previous work.	application domain;experiment;mathematical optimization;merge algorithm;one-to-many (data model);one-to-one (data model);recursion;tree (data structure);xml namespace;xml schema;xslt	Dong-Hoon Shin;Kyong-Ho Lee	2006	J. Information Science	10.1177/0165551506064391	xml catalog;xml validation;xml encryption;simple api for xml;xml;relax ng;xslt;streaming xml;computer science;document structure description;xml framework;xml database;xml schema;database;schematron;xml signature;programming language;world wide web;xml schema editor;information retrieval;efficient xml interchange;sgml	DB	-35.673607314752786	11.011610667417749	119147
fcc13486abfc7d4d81048fd8b5628976efd87fec	web service composition via problem decomposition across multiple ontologies	web services ontologies cities and towns computer science semantic web logic educational institutions web sites internet software systems;service composition;htn planning;logic;hierarchical task network;software systems;web services ontologies artificial intelligence;ontologies artificial intelligence;web service composition;domain knowledge;general solution;internet;web services;web sites;hierarchical task network planning model;semantic web;cities and towns;ontologies;computer science;description logic;description logic web service composition problem ontology hierarchical task network planning model;ontology;web service composition problem;structural properties	Web service composition (WSC) problems involve using domain knowledge about the underlying problem domains during the composition process. Existing research on Web service composition procedures has generally assumed that this domain knowledge is encoded as a single ontology that must be provided as an input to a composition procedure. In composition problems that span over knowledge across multiple ontologies that are connected via concept inheritence/extension, the users must examine the available ontologies and services in order to decide for the extent of the domain knowledge and the set of services relevant to their composition problems in hand. In this paper, we describe an automated way to generate solutions for service composition problems that span over multiple ontologies that have similar but not exactly the same structure. Our approach is based on a Hierarchical Task Network (HTN) planning model and extends the HTN-DL framework. This HTN planning model allows ontological problem decomposition in order to evaluate structural properties that span across multiple ontologies that are relevant to an input composition problem. We present a demonstration of the advantages of this approach.	hierarchical task network;interaction;ontology (information science);precondition;problem domain;service composability principle;tbox;web service;world sudoku championship	Naiwen Lin;Ugur Kuter;James A. Hendler	2007	2007 IEEE Congress on Services (Services 2007)	10.1109/SERVICES.2007.70	computer science;knowledge management;database;world wide web	AI	-44.456627387662245	14.640989390737847	119148
089959b4cb1feeea5b5528027e76f5f5759fd47b	temporal aggregation on user-defined granularities	user defined temporal granularities;telic atelic data;temporal aggregation;temporal databases	Time-varying data play a major role in many applications, and, starting from the 80’s, they have been widely studied in temporal databases. In the last two decades, several researchers have shown that, to deal with many application domains, user-defined temporal granularities must be coped with. When data are stored at multiple user-defined temporal granularities, the task of defining proper conversion functions to aggregate data from an origin granularity (e.g., business days) to a task granularity (e.g., months) is of primary importance. However, current temporal database approaches mostly demand such a task to system administrators, or to specific applications, providing no methodology or general guideline to accomplish it. In this paper, we propose a general and application-independent methodology which, on the basis of the temporal relationship between two user-defined granularities, provides users with a set of conversion/aggregation functions between them, consistent with the telic vs. atelic character of the data to be aggregated. The correctness of the approach is also proved.	aggregate data;aggregate function;correctness (computer science);system administrator;temporal database	Paolo Terenziani	2011	Journal of Intelligent Information Systems	10.1007/s10844-011-0179-y	real-time computing;computer science;data mining;database;temporal database	DB	-34.376280944866295	15.050396399110511	119216
9b7585de37a4cc6220dd9fc58030b5cf02c2d90c	a new trust model using hidden markov model based mixture of experts	trust;multi agent system;time series trust reputation hidden markov model expert agent;service provider;service selection;hidden markov model;mixture of experts;trust model;reputation;time series;web service;data mining;web services data mining hidden markov models multi agent systems recommender systems security of data software architecture;agent;software architecture;multi agent systems;hidden markov models;service oriented computing;web services;coarse grain hidden markov model trust model service oriented computing web service service selection reputation mechanism multiagent system trust establishment trust based decision service provider time series;expert;hidden markov models context markov processes time series analysis web services mathematical model equations;coarse grained;recommender systems;security of data;trust and reputation	In service-oriented computing, selection of an appropriate web service is a challenging problem. The more services are available, the more difficult is the service selection. Trust and reputation mechanisms have been used to filter good services from bad ones. Trust and reputation system of web services can often be modeled as a multi-agent system where agents are used to manage and reason about trust and reputation on behalf of their users providing or consuming services. In this paper, we propose a trust establishment framework for such a system based on direct experience and recommended trust. While making trust based decision of accessing a web service from a service provider, the value of the trust on which the decision is based is predicted from the direct trust values in the past. If the direct trust values in the past are not available, a recommended trust value is established by mixing the opinions obtained from a number of so-called “experts”. These experts are trained to learn regions of different volatilities in a time series constructed from the recommended trust values. The dynamics between the experts and the mixing weights are obtained using a coarse-grain Hidden Markov Model.	hidden markov model;interaction;markov chain;multi-agent system;reputation system;service-oriented device architecture;time series;trust (emotion);web service	Sarangthem Ibotombi Singh;Smriti Kumar Sinha	2010	2010 International Conference on Computer Information Systems and Industrial Management Applications (CISIM)	10.1109/CISIM.2010.5643457	web service;computer science;knowledge management;multi-agent system;data mining;law;world wide web;computational trust;hidden markov model	Web+IR	-44.34814966115083	16.34131553292498	119238
9f586819bf3b7ff3127708b101dd9b3ce8bb467c	collaborative generalisation: formalisation of generalisation knowledge to orchestrate different cartographic generalisation processes	geographic information;cartographic generalisation;process parameters;interoperability;domain ontology;ontology;constraints;space application;geographic database	Cartographic generalisation seeks to summarise geographical information from a geographic database to produce a less detailed and readable map. This paper deals with the problem of making different automatic generalisation processes collaborate to generalise a complete map. A model to orchestrate the generalisation of different areas (cities, countryside, mountains) by different adapted processes is proposed. It is based on the formalisation of cartographic knowledge and specifications into constraints and rules sets while processes are described to formalise their capabilities. The formalised knowledge relies on generalisation domain ontology. For each available generalisation process, the formalised knowledge is then translated into process parameters by an adapted translator component. The translators allow interoperable triggers and allow the choice of the proper process to apply on each part of the space. Applications with real processes illustrate the usability of the proposed model.		Guillaume Touya;Cécile Duchêne;Anne Ruas	2010		10.1007/978-3-642-15300-6_19	interoperability;computer science;knowledge management;artificial intelligence;ontology;data mining	AI	-44.64793998422796	5.229194277995041	119336
7e145cf8587bb6a6367d6176e4a0717db2cc2b78	a multimedia document model for e-government information systems	digital documents;document handling;information systems;document model;information retrieval;public administration services;documental streams multimedia document model e government information systems public administration services dematerialization activities bureaucratic digital documents;semantics;multimedia document model;public administration document handling government data processing information systems;multimedia systems electronic government information systems law legal factors ontologies streaming media information retrieval image segmentation resource description framework;law;media;indexing;dematerialization activities;streaming media;e government document;multimedia communication;e government information systems;ontologies;information system;knowledge engineering semantic management e government document information retrieval;documental streams;government data processing;bureaucratic digital documents;semantic management;public administration;knowledge engineering	E-government processes are dedicated to the improvement of the efficiency, inexpensiveness and accessibility of public administration services: dematerialization activities, introduced to manage bureaucratic digital documents in a proper way, are among the main tasks of the e-government works. In this paper we present a novel RDF model of digital documents for improving the dematerialization effectiveness, that constitutes the starting point of an information system able to manage documental streams in the most efficient way. Such model takes into account the important need that is required in several e-government applications which, depending on authorities or final users or time, provides different representations of the same multimedia contents.	accessibility;document;e-government;information retrieval;information system	Flora Amato;Antonino Mazzeo;Vincenzo Moscato;Antonio Picariello	2010	2010 International Conference on Complex, Intelligent and Software Intensive Systems	10.1109/CISIS.2010.170	computer science;database;world wide web;information retrieval	DB	-42.59939951510352	7.729463270131895	119529
7b223de30a71455de63d27d64df3152cc1827c20	multimedia information systems	information systems;multimedia systems;browsing support;data model design;information system architecture;multimedia data;multimedia information systems;query processing;data models;filtering;information system;neurosurgery;computer architecture;data model	Multimedia applications require representation and management of non traditional data, such as text documents, images, audio and video data, possibly together with traditional (e.g. relational or object oriented) data. In particular, in multimedia applications, different data types are often coexisting within the same application domain. Hence data management systems which are able to treat, in an integrated and uniform way, different types of data are needed.	application domain;image;information system;relational model	Maria Luisa Sapino;K. Selçuk Candan	2008		10.1007/978-0-387-78414-4_47	multimedia;ip multimedia subsystem	DB	-35.677779037216986	9.980442695483518	119532
7510af0b3cf0d8e41825fd247e2caa42f0d7671d	parla: a cooperation language for cognitive multi-agent systems	distributed system;multiagent system;systeme reparti;multi agent system;cognitive agents;intelligence artificielle;logical programming;agent communication;computer network;planificacion;sistema repartido;programmation logique;scheduling;distributed artificial intelligence;agent systems;agent communication language;artificial intelligence;ordonamiento;planning;inteligencia artificial;planification;sistema multiagente;programacion logica;ordonnancement;systeme multiagent	One of the main goals of Distributed Artiicial Intelligence is to devise methods to join a community of Computational Agents into a Multi-Agent System, where these agents can cooperate to reach common goals. Cooperation in a Cognitive Agent community is usually supported by an Agent Communication Language (ACL) which allows the agents to exchange knowledge and information through a computer network. In this paper, we propose Parla, a high level agent communication language to cognitive multi-agent systems. This language is based on a standard message format, that contains the necessary information for the message integrity, network security and groupware services to be implemented. These services can either be performed by the lower layers of the system or be included in the high level agent communication support. This message format has a speciic slot to store the cooperation language expressions. These expressions consist of a primitive name and an argument, which should be a valid expression of a knowledge representation formalism supported by the cognitive agent. The following aspects of the Parla language are presented: language layers, agent communication support requirements, message format, primitive set and primitive semantics. To demonstrate the language use, a cooperation example among four agents is presented. In this example, each agent has its own speciic domain of knowledge but, because of global interdependences, cooperation is necessary. This example refers to the recomposition of part of the South Brazil's electrical network.	agent communications language;collaborative software;computation;high-level programming language;information security;interdependence;knowledge representation and reasoning;multi-agent system;network security;requirement;semantics (computer science)	Augusto Cesar Pinto Loureiro da Costa;Guilherme Bittencourt	1997		10.1007/BFb0023923	planning;simulation;computer science;artificial intelligence;machine learning;multi-agent system;database;distributed computing;scheduling;computer security;algorithm	AI	-39.495835680874265	17.263075378090267	119567
7af7f7f06749b93300b427586b53373ad455bba8	microsearch: a search engine for embedded devices used in pervasive computing	network architecture and design;search engine;theoretical model;computer communication networks;information retrieval;pervasive computing;indexation;embedded search engine;off the shelf;embedded device;real time and embedded systems;ubiquitous computing environment	In this article, we present Microsearch, a search system suitable for embedded devices used in ubiquitous computing environments. Akin to a desktop search engine, Microsearch indexes the information inside a small device, and accurately resolves a user's queries. Given the limited hardware, conventional search engine design and algorithms cannot be used. We adopt Information Retrieval (IR) techniques for query resolution, and proposed a new space-efficient top-k query resolution algorithm. A theoretical model of Microsearch is given to better understand the trade-offs in design parameters. Evaluation is done via actual implementation on off-the-shelf hardware.	algorithm;desktop computer;embedded system;high-level programming language;information retrieval;theory;ubiquitous computing;web search engine	Chiu Chiang Tan;Bo Sheng;Haodong Wang;Qun Li	2010	ACM Trans. Embedded Comput. Syst.	10.1145/1721695.1721709	search-oriented architecture;embedded system;search engine indexing;real-time computing;computer science;operating system;database;world wide web;ubiquitous computing;search engine	HCI	-35.510828465514905	17.998817156144927	119609
ea7087d01e4591390fd37f8b8b4bd3ea5e0212ee	the s hexdump format		This document specifies the S Hexdump Format (SHF), a new, XML-based open format for describing binary data in hexadecimal notation. SHF provides the ability to describe both small and large, simple and complex hexadecimal data dumps in an open, modern, transportand vendor-neutral format. RFC 4194 The S Hexdump Format October 2005	binary data;hex dump;hexadecimal;xml	Joachim Strömbergson;Linus Walleij;Patrik Faltstrom	2005	RFC	10.17487/RFC4194	matroska;hexadecimal;computer science;database;dot-decimal notation;world wide web;information retrieval	DB	-36.2896281771968	8.523302829670948	119814
e68d635fd84aea580bf93b2485d9a219de4d1cf1	a comparative evaluation of semantic web service discovery approaches	semantic web service;web service discovery;web service;semantic web services;web services;semantic web;point of view	Currently, most enterprises deploy their services on the Web. This augments the request for tools to perform discovery, selection, composition and invocation of Web services. Among them, Web service discovery should be considered more important. Along with the growing number of available Web services, there is a need for tools not only to perform discovery, but also to realize them in an efficient and effective manner. A number of approaches to Web service discovery have been proposed. In this paper, we provide a taxonomy which categorizes Web service discovery systems from different points of view. Moreover, current approaches to Semantic Web service discovery are classified and described. In addition, we compare the approaches with respect to some criteria from different aspects of view. The results of this study can help researchers in both academia and industry to implement a new or to select the most appropriate existing approach for Semantic Web service discovery with the aid of different criteria.	semantic web service;service discovery;taxonomy (general);world wide web	Keyvan Mohebbi;Suhaimi Ibrahim;Mojtaba Khezrian;Kanmani Munusamy;Sayed Gholam Hassan Tabatabaei	2010		10.1145/1967486.1967496	web service;web application security;web development;web modeling;data web;web mapping;web design;web standards;computer science;ws-policy;semantic web;web navigation;social semantic web;data mining;semantic web stack;database;web intelligence;web 2.0;world wide web;universal description discovery and integration	Web+IR	-45.35318868734002	13.463946345115192	119972
c20a199c3746344ee291db493e8f413680fc9cf9	exploring e-learning knowledge through ontological memetic agents	information structure;electronic learning;ontological memetic agent;intelligent explorers;learning experience;multi agent e learning system;computer aided instruction;complex information;e learning system;open systems business data processing computer aided instruction internet multi agent systems ontologies artificial intelligence;online learning;satisfiability;memetics;ontologies artificial intelligence;information structuring;e learning knowledge;business companies;multi agent systems;computational modeling;learning methods;internet;business data processing;multicore processing;unstructured information;interoperability e learning knowledge ontological memetic agent tertiary education business companies online learning world wide web information structuring multi agent e learning system knowledge representation complex information unstructured information intelligent explorers;tertiary education;world wide web;ontologies;optimization;exponential growth;electronic learning ontologies proposals companies costs web sites learning systems knowledge representation knowledge management intelligent agent;interoperability;knowledge representation;road transportation;open systems;institutional repository research archive oaister	E-Learning systems have proven to be fundamental in several areas of tertiary education and in business companies. There are many significant advantages for people who learn online such as convenience, portability, flexibility and costs. However, the remarkable velocity and volatility of modern knowledge due to the exponential growth of the World Wide Web, requires novel learning methods that offer additional features such as information structuring, efficiency, task relevance and personalization. This paper proposes a novel multi-agent e-Learning system empowered with (ontological) knowledge representation and memetic computing to efficiently manage complex and unstructured information that characterize e-Learning. In particular, differing from other similar approaches, our proposal uses (1) ontologies to provide a suitable method for modeling knowledge about learning content and activities, and (2) memetic agents as intelligent explorers in order to create ¿in time¿ and personalized e-Learning experiences that satisfy learners' specific preferences. The proposed method has been tested by realizing a multi-agent software plug-in for an industrial e-Learning platform with experimentations to validate our memetic proposal in terms of flexibility, efficiency and interoperability.	computational intelligence;experience;graph theory;interoperability;knowledge representation and reasoning;memetic algorithm;memetics;multi-agent system;ontology (information science);pl/p;personalization;plug-in (computing);relevance;time complexity;user modeling;velocity (software development);volatility;world wide web	Giovanni Acampora;Vincenzo Loia;Matteo Gaeta	2010	IEEE Computational Intelligence Magazine	10.1109/MCI.2010.936306	multi-core processor;knowledge representation and reasoning;interoperability;memetics;exponential growth;the internet;computer science;knowledge management;ontology;artificial intelligence;data science;machine learning;multi-agent system;data mining;open system;computational model;satisfiability	AI	-45.91342009591974	9.559241225040141	120201
dc1d4ed37252d741145f483795dd543470410677	experiments and functional analysis in integrating sub-ontology extraction and tailoring	sub ontology;prototypes;sub ontology tailoring;optimization schemes;semantics;data mining;ontologies artificial intelligence;semantic grid environment;large scale;servers;performance improvement;ontologies servers semantics data mining prototypes optimization functional analysis;functional analysis;ontologies artificial intelligence grid computing;functional analysis sub ontology extraction sub ontology tailoring semantic grid environment;sub ontology extraction;ontology reuse;ontologies;optimization;ontology tailoring ontology sub ontology optimization schemes;ontology tailoring;grid computing;ontology;semantic grid	A sub-ontology extracted from its base ontology data contains only the concepts and relationships pertaining to the information required by the user. Sub-ontology extraction and sub-ontology tailoring are then viewed as possible solutions to efficiently utilize large-scale ontology data sparsely distributed on a Semantic Grid environment. This shall contribute to reduce the communication overhead in transporting the ontology data, increase the processing performance, improve the reliability in aggregating the required ontology/sub-ontology, and improve the tailoring process of sub-ontology reuse. This paper discussed preliminary experiments as a showcase to illustrate sub ontology extraction and tailoring on a prototype Semantic Grid environment. Functional analysis and discussion of the results are described.	experiment;ontology learning;overhead (computing);prototype;semantic grid;web ontology language	Toshihiro Uchibayashi;Bernady O. Apduhan;Norio Shiratori	2011	2011 International Conference on Computational Science and Its Applications	10.1109/ICCSA.2011.71	functional analysis;semantic grid;computer science;ontology;operating system;ontology;data mining;database;semantics;prototype;ontology-based data integration;information retrieval;process ontology;grid computing;server	HPC	-43.241362964444924	9.665119473014073	120360
517a00cbe0d423001ac170bad9b7dce6bfb6564d	interactions between agents as shared resources in multi-agents systems	resource allocation multi agent systems ontologies artificial intelligence;software;agent interaction;protocols;multiagent system;multi agent system;resource allocation;road traffic;virtual reality;traffic control;ontologies artificial intelligence;agent interaction multiagent system resource sharing ontology agent behavior;computer architecture;multi agent systems;roads;resource sharing;multiagent systems world wide web ontologies roads context virtual reality resource management application software traffic control web sites;vehicles;ontology;agent behavior;multiagent systems	In this paper we defend the advantages of a representation of agent behaviors, based on the concept of interaction. Although the behavior of an agent is usually included in its structure, we dissociate here the concept of agent from that of interaction, each one being associated with a specific ontology. This approach is especially valuable for increasing the reutilisability of the interactions, which are very often generic even in contexts where the agents are not. In a first part, we describe and compare our approach to the classic one. Then a list of properties is given to show the benefit to consider an interaction, not only as a communication means, but also as an entity shared by all agents and not reserved to some of them. An illustration of our approach is given in a road traffic application using VRML.	interaction;simulation;vrml	Adil Sayouti;Fatima Qrichi Aniba;Hicham Medromi	2008	2008 New Technologies, Mobility and Security	10.1109/NTMS.2008.ECP.52	shared resource;communications protocol;simulation;resource allocation;computer science;knowledge management;multi-agent system;world wide web	AI	-40.95531250799017	17.677951213162174	120619
c3f86eafcbbeb8e9fecbd568852dd7b061c2f21a	linking in a global information architecture	information infrastructure;linking;knowledge based system;information architecture	As network-based applications and knowledge-based systems become more prevalent and useful, it will become increasingly important to provide a common, long-lived model of the infrastructure on top of which such applications will exist. Not only does the longevity and evolvability of the infrastructure have a direct impact on the value of the information in it, but also the provisioning of a meta-level infrastructure capable of defining relationships among the pieces of information become increasingly important. In this paper, we examine, first, a general, flexible information architecture and then enhance it with an extensible, simple model of linking, recommended as a component of the substrate of network-based applications and knowledge-based systems of the future.	information architecture;knowledge-based systems;provisioning	Karen R. Sollins;Jeffrey R. Van Dyke	1996	World Wide Web Journal		information architecture;data mining;computer science;enterprise architecture framework;information technology architecture;reference architecture;data architecture;applications architecture;solution architecture;knowledge management;space-based architecture	Metrics	-45.95334215751396	10.804946150221872	121000
77f1a5dda1e5fee51a1105cfc1850187da84baf5	semantic enrichment for building information modeling	article	Significant difficulties remain in exchanging information between building information modeling BIM tools. The industry foundation classes IFC exchange schema is too generic to capture the full semantic meaning needed for direct use by different construction project stakeholders' BIM tools. Although BIM standards that prescribe model view definitions MVD for domain-specific exchanges are under development, insufficient semantic definition of exchanges prevents achievement of the full potential of BIM through seamless interoperability. We propose an innovative approach for supplementing an IFC exchange file with semantically useful concepts inferred from the explicit and implicit information contained in the building model. A prototype software was implemented to test the applicability of the approach. It consists of a rule-processing engine and allows composition of inference rule-sets that can be tailored for different domains. The tests demonstrate semantic enrichment with precast concrete building models, adding inferred joint, slab aggregation and connection concepts.	building information modeling;gene ontology term enrichment;information model	Michael Belsky;Rafael Sacks;Ioannis K. Brilakis	2016	Comp.-Aided Civil and Infrastruct. Engineering	10.1111/mice.12128	simulation;computer science;knowledge management;data mining;database	DB	-45.18161851867908	4.501475808150477	121051
f287a08b17a340074153b8460be0c6ab0780d314	personalized faceted navigation in the semantic web	software tool;information space;design and implementation;semantic web;user model	This paper presents the prototype of an adaptive faceted semantic browser – Factic. Factic implements our novel method of navigation in open information spaces represented by ontologies based on an enhanced faceted browser with support for dynamic facet generation and adaptation based on user characteristics. It is developed as part of a modular framework that supports personalization based on an automatically acquired ontological user model. We describe software tool design and implementation together with discussion on several problems mostly related to the general immaturity of current Semantic Web solutions.	faceted classification;ontology (information science);personalization;programming tool;prototype;semantic web	Michal Tvarozek;Mária Bieliková	2007		10.1007/978-3-540-73597-7_46	user modeling;computer science;semantic web;web navigation;social semantic web;semantic web stack;database;world wide web;information retrieval	Web+IR	-42.27678783679925	9.95745318217102	121144
1e3e81380901b28c014275f7ff2b25c72c1a9c3a	enabling cross constraint satisfaction in rdf-based heterogeneous database integration	databases;informatica;european commission;semantic mediation approach;resource description format;local as view;data translation methods;query processing;medicina;heterogeneous databases;databases scalability ontologies mediation information retrieval resource description framework artificial intelligence data warehouses query processing cancer;data translation methods cross constraint satisfaction heterogeneous database integration data transformation based systems query translation query processing semantic mediation approach;database integration;ontologies database integration cross constraints query processing;constraint satisfaction;mediation;data transformation;xml;distributed databases;merging;ontologies;scalability;query translation;data warehouse;cross constraint satisfaction;heterogeneous database integration;bioquimica;query processing distributed databases;cross constraints;data transformation based systems	The problem of database integration has been widely tackled through different approaches. While data transformation based systems, such as Data Warehouses, reached the acceptation of the industry during the 80's, in the last decade query translation based approaches have gained popularity given their adequacy to dynamic domains. While the former are based on gathering actual data in central repositories, the latter allow data to remain in the original databases. There still exist several issues to be tackled in query translation, mainly if no ad-hoc schema is described, such as problems with scalability and query processing. In this paper we describe a complete database integration and semantic mediation approach, borrowing techniques from both data transformation and local as view data translation methods, and addressing the cross referencing problem. This work has been carried out in the framework of ACGT (Advancing Clinico-Genomic Trials on Cancer) project, supported by the European Commission.	constraint satisfaction;cross-reference;heterogeneous database system;hoc (programming language);research data archiving;resource description framework;sparql;scalability	Luis Martín;Alberto Anguita;Ana Jiménez;José Crespo	2008	2008 20th IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2008.67	scalability;xml;constraint satisfaction;computer science;ontology;data integration;data warehouse;data mining;database;mediation;data transformation;information retrieval	DB	-35.96025814461575	5.570779071274557	121178
97a64f04f9b3160ae47b99418ab5b2973aebc088	search-based qos ranking prediction for web services in cloud environments	average precision;qos ranking prediction;particle swarm optimization;期刊论文;web services;similarity computation;fitness function	Unlike traditional quality of service (QoS) value prediction, QoS ranking prediction examines the order of services under consideration for a particular user. To address this NP-Complete problem, greedy strategybased solutions, such as CloudRank algorithm, have beenwidely adopted. However, they can only produce locally approximate solutions. In this paper, we propose a search-based prediction framework to address the QoS ranking problem. The traditional particle swarm optimization (PSO) algorithm has been adapted to optimize the order of services according to their QoS records. In real situations, QoS records for a given consumer are often incomplete, so the related data from close neighbour users is often used to determine preference relations among services. In order to filter the neighbours for a specific user, we present an improved method for measuring the similarity between two users by considering the occurrence probability of service pairs. Based on the similarity computation, the top-k neighbours are selected to provide QoS information support for evaluation of the service ranking. A fitness function for an ordered service sequence is defined to guide search algorithm to find high-quality ranking results, and some additional strategies, such as initial solution selection and trap escaping, are also presented. To validate the effectiveness of our proposed solution, experimental studies have been performed on real-world QoS data, the results fromwhich show that our PSO-based approach has a better ranking for services than that computed by the existing CloudRank algorithm, and that the improvement is statistically significant, in	approximation algorithm;cloud computing;computation;definition;experiment;fitness function;greedy algorithm;information retrieval;mathematical optimization;np-completeness;particle swarm optimization;quality of service;search algorithm;software engineering;web service	Chengying Mao;Jifu Chen;Dave Towey;Jinfu Chen;Xiaoyuan Xie	2015	Future Generation Comp. Syst.	10.1016/j.future.2015.01.008	web service;computer science;machine learning;data mining;database;particle swarm optimization;ranking svm;world wide web;fitness function	Web+IR	-45.62675999485028	15.272517735277312	121208
416c55ed89a1badcde72e6077a279cbf0fa29243	policy-driven business management over web services	reusable services;electronic commerce;web services logic computer science software systems computer interfaces software reusability internet application software petri nets disaster management;web services electronic commerce;web service;satisfiability;software engineering;software engineers;conference paper;software engineers policy driven business management web services service oriented architecture reusable services end user business processes;end user business processes;web services;service oriented architecture;policy driven business management;business process	Service-oriented architecture allows for reusable services to be composed in such a way that business tasks or activities are easily satisfied. However, currently there is a lack of abstraction past the composition layer, and thus a gap between the service and business domains. We propose a method for depicting end-user business processes, that are further specified and refined by policies. Policies describe information that the end-user can specify, such as requirements, preferences and constraints. The result is a technique for end-users or business analysts to use, rather than software engineers.	business process;requirement;service-oriented architecture;service-oriented software engineering;software engineer;user (computing);web service	Stephen Gorton;Stephan Reiff-Marganiec	2007	2007 10th IFIP/IEEE International Symposium on Integrated Network Management	10.1109/INM.2007.374836	e-commerce;web service;business rule management system;business logic;business process execution language;business requirements;business service provider;computer science;knowledge management;artifact-centric business process model;business process management;ws-policy;service-oriented architecture;software as a service;electronic business;business process model and notation;industrialization of services business model;mobile business development;services computing;business software;business rule;law;business process modeling;business activity monitoring;business architecture	SE	-48.25382579197341	17.488699525487732	121272
55792b209e5244f1b6a9b16df3c7d2236d9dc051	a facet-based methodology for geo-spatial modeling	software agent;space domain;semantic interoperability;qa076 computer software;large scale;spatial relation;theory;geo spatial ontology;methodology;principle;domain ontology;spatial model	Space, together with time, is one of the two funda mental dimensions of the universe of knowledge. Geo-spatial ontologie s are essential for our shared understanding of the physical universe and t o achieve semantic interoperability between people and between software agent s. In his paper we propose a methodology and a minimal set of guiding principl es, mainly inspired by the faceted approach, to produce high quality ontologie s in terms of robustness, extensibility, reusability, compactness and flexibili ty. We demonstrate with step by steps examples that by applying the methodolog y and those principles we can model the space domain and produce a high quali ty f cet-based large scale geo-spatial ontology comprising entities, entity cl asses, spatial relations and attributes.	brookgpu;confluence;council for educational technology;display resolution;entity;extensibility;faceted classification;geonames;getty thesaurus of geographic names;graphical user interface;gulf of execution;lakes of wada;library and information science;modular ocean model (mom);ontology (information science);semantic interoperability;software agent;underground;waterfall model;wordnet	Biswanath Dutta;Fausto Giunchiglia;Vincenzo Maltese	2011		10.1007/978-3-642-20630-6_9	upper ontology;computer science;knowledge management;data mining;database;process ontology	AI	-42.54547136096492	6.342066760845671	121316
51352a587d7e6d38a4df4c9a762f88c5780d6c31	generating rdf for application testing		Application testing is a critical component of application development. Testing of Semantic Web applications requires large RDF datasets, co nforming to an expected form or schema, and preferably, to an expecte d da a distribution. Finding such datasets often proves impossible, while generating in put datasets is often cumbersome. The G RR (Generating Random RDF) system is a convenient, yet powerful, tool for generating random RDF, based on a SPARQLlike syntax. In this poster and demo, we show how large datasets can be ea sily generated using intuitive commands.	resource description framework;round-robin scheduling;semantic web;web application;web testing	Daniel Blum;Sara Cohen	2010			cwm;computer science;theoretical computer science;linked data;data mining;database;rdf schema	Web+IR	-39.593786357373105	10.734485509321816	121372
4b94f3d73c3aa5f58096eeb51c5b196bb37263ff	verdi: an automated tool for web sites verification	site web;sistema experto;formal specification;automatic proving;web pages;red www;rule based;web semantique;formal specification language;reseau web;base connaissance;integrite;intelligence artificielle;integridad;demostracion automatica;logical programming;specification language;specification formelle;demonstration automatique;especificacion formal;automated verification;integrity;internet;programmation logique;web semantica;semantic web;reparation;artificial intelligence;world wide web;base conocimiento;lenguaje especificacion;inteligencia artificial;sitio web;systeme expert;reparacion;programacion logica;langage specification;lenguaje formal;formal language;web site;repair;knowledge base;expert system;langage formel	Verdi is a system for the automated verification of Web sites which can be used to specify integrity conditions for a given Web site, and then automatically check whether these conditions are actually fulfilled. It provides a rule-based, formal specification language which allows us to define syntactic/semantic properties of the Web site as well as a verification facility which computes the requirements not fulfilled by the Web site, and helps to repair the errors by finding out incomplete/missing Web pages.	formal specification;logic programming;requirement;specification language;web page;world wide web	María Alpuente;Demis Ballis;Moreno Falaschi	2004		10.1007/978-3-540-30227-8_67	web service;knowledge base;formal language;web modeling;the internet;specification language;computer science;artificial intelligence;semantic web;web page;formal specification;database;programming language;world wide web;expert system;algorithm	Security	-37.684830641692336	12.760964228369515	121503
644752a362c5544fe55e26ab6717638aab7f7edc	logic for media - the computational media metaphor	labelled deductive system;multi agent system;generic model;labelled deductive systems;software agents rewriting systems;software agents;rewriting systems;rewriting logic;multi agent system computational media rewriting logic labelled deductive systems netacademy;medium;communication technology;logic technology management computer architecture internet knowledge management vocabulary communications technology information technology multiagent systems transport protocols;new media	New media as they are established by information and communications technology demand for reconsidering the notion of a medium as a carrier of information and with it the concepts for representation, organization, processing and dissemination of information. We explore a general model for media, the Computational Media Metaphor and utilize Rewriting Logic and Labelled Deductive Systems to model media. We obtain a general description of media according to which the medium can be built as well as a generic media architecture. Our example is the NetAcademy, a computational medium for the scienti c community.	computation;new media;rewriting	Ulrike Lechner;Beat F. Schmid	1999		10.1109/HICSS.1999.772919	information and communications technology;new media;rewriting;computer science;artificial intelligence;theoretical computer science;software agent;multi-agent system;distributed computing;algorithm	NLP	-46.58128537551432	11.710831279161436	121518
f3e8a77624480e438fe824aef5e6a7f13d3699fd	concept covering for automated building blocks selection based on business processes semantics	business processes semantics;greedy algorithms;data mining;companies;acceleration;sap r 3 automated building blocks selection erp business processes semantics greedy concept covering algorithm mandatory requirements mandatory preferences;erp;business data processing;best practices;enterprise resource planning;greedy algorithms business data processing enterprise resource planning;semantic web;sap r 3;production;terminology;inference algorithms;automated building blocks selection;greedy concept covering algorithm;business process re engineering;mandatory preferences;best practices inference algorithms enterprise resource planning companies acceleration semantic web business process re engineering production terminology data mining;mandatory requirements	In this paper we present a novel approach and a system for automated selection of building blocks, by exploiting business processes semantics. The selection process is based on a novel greedy concept covering algorithm, which computes, given a description of the request, the set of needed building blocks. If such a set does not exist the algorithm returns the covered subset and explanation information on both the left uncovered part and conflicting part of the request description, exploiting non-standard inference services. The requested description can be expressed as conjunction of mandatory requirements and preferences. The approach has been deployed in a system specifically designed for SAP R/3 best practices reusability, which is fully functional and is currently being evaluated in an industrial setting	add-ons for firefox;approximation algorithm;best practice;business process;e-commerce;e-services;feedback;greedy algorithm;np-hardness;numerical analysis;polynomial;requirement;sap r/3;semantic web	Francesco di Cugno;Tommaso Di Noia;Eugenio Di Sciascio;Azzurra Ragone	2006	The 8th IEEE International Conference on E-Commerce Technology and The 3rd IEEE International Conference on Enterprise Computing, E-Commerce, and E-Services (CEC/EEE'06)	10.1109/CEC-EEE.2006.26	computer science;knowledge management;data mining;database	Robotics	-46.11159842508636	14.719676932739796	121544
01effe44b974c4a9fe6071b8ed2a73cf8c701d6b	p2p query reformulation over both-as-view data transformation rules	query reformulation;data integrity;query processing;both as view;p2p;data transformation	The both-as-view (BAV) approach to data integration has the advantage of specifying mappings between schemas in a bidirectional manner, so that once a BAV mapping has been established between two schemas, queries may be exchanged in either direction between the schemas. In this paper we discuss the reformulation of queries over BAV transformation pathways, and demonstrate the use of this reformulation in two modes of query processing. In the first mode, public schemas are shared between peers and queries posed on the public schema can be reformulated into queries over any data sources that have been mapped to the public schema. In the second, queries are posed on the schema of a data source, and are reformulated into queries on another data source via any public schema to which both data sources have been mapped.	database schema	Peter McBrien;Alexandra Poulovassilis	2006		10.1007/978-3-540-71661-7_30	computer science;peer-to-peer;data integrity;data mining;database;data transformation;information retrieval	DB	-36.03350276857163	6.439677119355419	121599
af489e2185d034d4c39b8501c7140cdde7dc372b	a survey on the complementarity between database and ontologies: principles and research areas	databases;conceptual data model;cdm;semantic;complementarity;ontologies;survey	This paper presents a survey on the complementarity between databases and ontologies. It interests semantic web and database communities. Ontologies can help in overcoming the drawbacks of DBs and vice-versa. We conducted a deep analysis, which reveals that the collaboration between these two disciplines can be done in five different ways: facilitating the database design, increasing their semantics for information retrieval, integration and maintenance, designing ontologies from existing databases, and allowing storage of large number of instances in ontological systems. We present in this paper the concepts related to this field, the reasons for this rapprochement and a state-of-the-art on existing approaches in this area. For each of the axes presented, we will present the axis, underlying issues and cite relevant research. Originality of our work comes from the fact that it covers the whole field of rapprochement between databases and ontologies. Therefore, it provides the reader, who may not be very familiar with this domain of research, with an introduction to axes in this research and with pointers to different research projects.	complementarity theory;database design;information retrieval;ontology (information science);optic axis of a crystal;pointer (computer programming);semantic web	Fatima Zohra Laallam;Mohammed Lamine Kherfi;Sidi Mohamed Benslimane	2014	IJCAT	10.1504/IJCAT.2014.060528	idef5;clean development mechanism;computer science;knowledge management;conceptual schema;ontology;complementarity;data mining;information retrieval	AI	-43.70162769203357	4.777803995855758	122059
360a70bcc29c70f7db519b202819da08a8756d7b	planning and monitoring the execution of web service requests	informatica;information retrieval system;service web;sistema de recuperacion de informacion;web service;orientado servicio;systeme de recherche d information;informatique;oriente service;computer science;information system;systeme information;business process;servicio web;service oriented;qa075 electronic computers computer science;sistema informacion	Interaction with web services enabled marketplaces would be greatly facilitated if users were given a high level service request language to express their goals in complex business domains. This can be achieved by using a planning framework which monitors the execution of planned goals against predefined standard business processes and interacts with the user to achieve goal satisfaction. We present a planning architecture that accepts high level requests, expressed in a service request language known as XSRL. The planning framework is based on the principle of interleaving planning and execution. This is accomplished on the basis of refinement and revision as new service-related information is gathered from service repositories such as UDDI and web services instances, and as execution circumstances necessitate change. The planning system interacts with the user whenever confirmation or verification is needed.	algorithm;automated planning and scheduling;business process;computation;constraint satisfaction;executable;experiment;forward error correction;high-level programming language;interaction;million book project;model checking;refinement (computing);web services discovery;web service	Alexander Lazovik;Marco Aiello;Mike P. Papazoglou	2006	International Journal on Digital Libraries	10.1007/s00799-006-0002-5	web service;simulation;business process execution language;computer science;database;business process;world wide web;information system	Web+IR	-40.677684119082244	15.960802901345074	122115
87e59e80aa46cf209bbf026d188a68905b704e85	communication adapter design of multi-agent in webgis	multi agent systems geographic information systems internet;design model;control systems;adaptive design;geographic information systems distributed computing technology management laboratories simple object access protocol control systems hardware dispersion information management information technology;communication adapter design;information technology;analysis and design;distributed computing;soap;communication model;multi agent;technology management;multi agent systems;webgis;internet;geographic information systems;information management;multiagent;webgis communication adapter design multiagent;adapter;simple object access protocol;dispersion;adapter webgis multi agent soap;hardware	On the characteristics of multi-source, heterogenous structure, mass data of WebGIS as well as the complexity of the system, the paper proposes the design model based on Web and multi-agent. Moreover, through analyzing the characteristics of each agent of the design model, the paper makes a further research on the communication problem of the agents. It also proposes the communication model and gives analysis and design to communication adapter	multi-agent system;multi-source	Guangru Li;Jingfeng Hu;Xian Wu	2006	2006 5th IEEE International Conference on Cognitive Informatics	10.1109/COGINF.2006.365570	computer science;distributed computing;world wide web;computer security	Robotics	-40.56862800852102	18.0065734488765	122178
a7e7c25a74132fe208f6dd801301101a5d7a1064	improving search and navigation by combining ontologies and social tags	search engine;social bookmarking;tag;web2 0;social tagging;ontology	The Semantic Web has the ambitious goal of enabling complex autonomous applications to reason on a machine-processable version of the World Wide Web. This, however, would require a coordinated effort not easily achievable in practice. On the other hand, spontaneous communities, based on social tagging, recently achieved noticeable consensus and diffusion. The goal of the TagOnto system is to bridge between these two realities by automatically mapping (social) tags to more structured domain ontologies, thus, providing assistive, navigational features typical of the Semantic Web. These novel searching and navigational capabilities are complementary to more traditional search engine functionalities. The system, and its intuitive AJAX interface, are released and demonstrated on-line.	ajax (programming);autonomous robot;extensibility;folksonomy;online and offline;ontology (information science);overhead (computing);refinement (computing);semantic web;spontaneous order;usability;user experience;web 2.0;web search engine;world wide web	Silvia Bindelli;Claudio Criscione;Carlo Curino;Mauro Luigi Drago;Davide Eynard;Giorgio Orsi	2008		10.1007/978-3-540-88875-8_26	computer science;social semantic web;data mining;world wide web;information retrieval	Web+IR	-41.66303075015748	9.433419588842032	122242
a5464012ea854868ad7edb37ddd3987d08005437	toward a web of systems		Web and semantic technologies will form the foundation for ecosystems of machines that interact with each other and with people as never before.	ecosystem;world wide web	Florian Michahelles;Simon Mayer	2015	ACM Crossroads	10.1145/2845163	web service;web development;web modeling;web accessibility initiative;web standards;knowledge management;social semantic web;multimedia;web intelligence;world wide web	Web+IR	-40.213412530290285	6.467047442504043	122281
6cb6e71aaef6d394c22dc7efecf3ce9ba33e8955	sacaddos: a support tool to manage multilevel documents	support tool;manage multilevel documents	This paper describes SACADDOS, a decision support tool to derive the sensitivity of a document when this document is transmitted, and to control the evolution of this sensitivity over time. For this purpose, SACADDOS manages a set of classification security policies. A classification security policy corresponds to a set of rules which are used to derive, from the content of a document, the classification of this document at a given time. SACADDOS includes an intelligent document management tool to analyze the content of a document in order to derive which classification rules apply to this document. When several contradictory rules apply, SACADDOS suggests to solve the conflict by defining an order of preference between the contradictory rules.		Jérôme Carrère;Frédéric Cuppens;Claire Saurel	1998			well-formed document;computer science;document management system;data mining;database;information retrieval;vision document;design document listing	HCI	-37.30154215727151	9.857174371380978	122350
96146b1cd367e86857838771d7b6db23ed05195c	secoa: autonomous semantic service composition algorithm in symbiotic networks	energy conservation;tree searching computer network performance evaluation energy conservation network theory graphs open systems resource allocation;owl;service composition;performance evaluation;symbiosis;energy efficient;resource allocation;search algorithm;semantics;service model;computer network performance evaluation;ibcn;technology and engineering;performance evaluation secoa autonomous semantic service composition algorithm symbiotic networks network resource sharing scalability dependability energy efficiency colocated networks software services interoperability semantic domain service model tunable best first search algorithm;cognition;symbiosis ontologies semantics owl cognition educational institutions;ontologies;tree searching;network theory graphs;open systems	We propose symbiotic networks, a novel approach toward sharing of network resources in order to increase the scalability, dependability and energy efficiency of colocated networks. As symbiotic networks offer large amounts of software services, one challenge is to allow these services to operate “symbiotically” as well. By combining services from different parties, service compositions arise, which allow for a richer set of functionality. Creating such compositions, however, requires intricate knowledge about services and their interoperability. Using a semantic domain and service model, we describe SeCoA, a tunable best-first search algorithm for autonomously constructing symbiotic service compositions. A performance evaluation of SeCoA was conducted, showing that the algorithm offers acceptable performance for moderately sized compositions.	best-first search;colocation centre;dependability;experiment;interoperability;owl-s;performance evaluation;real life;scalability;search algorithm;semantic web rule language;service composability principle;web ontology language	Tim De Pauw;Bruno Volckaert;Veerle Ongenae;Filip De Turck	2012	2012 IEEE Network Operations and Management Symposium	10.1109/NOMS.2012.6211957	cognition;energy conservation;resource allocation;computer science;knowledge management;ontology;service-oriented modeling;distributed computing;semantics;efficient energy use;open system;world wide web;search algorithm;symbiosis	Mobile	-43.44645709649721	15.667898852101958	122396
cb0f56c0e5591f3976d7688f3f7a35c759d7bc3e	an rdf storage and query framework with flexible inference strategy	storage system;red www;metadata;backward chaining;chainage avant;chainage arriere;base donnee tres grande;web semantique;interrogation base donnee;reseau web;interrogacion base datos;alimentacion maquina;resource description framework;feasibility;internet;amenage piece;web semantica;systeme memoire;machine feed;inferencia;metadonnee;specification rdf;semantic web;alimentation machine;world wide web;alimentacion pieza;forward chaining;metadatos;very large databases;sistema memoria;feeder;database query;practicabilidad;faisabilite;inference;rdf	In the Semantic Web, RDF (Resource Description Framework) and RDF Schema are commonly used to describe metadata. There are a great many RDF data in current web, therefore, efficient storage and retrieval of large RDF data sets is required. So far, several RDF storage and query system are developed. According to the inference strategy they used, they can be classified into two categories, one exclusively use forward chaining strategy; the other exclusively use backward chaining strategy. In most cases, the query performance of the former is superior to that of the latter. However, in some cases, the disadvantage of larger storage space may at some point outweigh the advantage of faster querying. Further, the existing systems that exclusively use forward chaining strategy have not yet presented a good solution to the deletion operation by now. In this paper, we design a RDF storage and query framework with flexible inference strategy, which can combine forward and backward chaining inference strategies. In addition, a new solution to the deletion operation is also given within our framework. The feasibility of our framework is illustrated by primary experiments.	backward chaining;experiment;forward chaining;rdf schema;resource description framework;semantic web;semiconductor industry	Wennan Shen;Yuzhong Qu	2006		10.1007/11610113_16	rdf/xml;feasibility study;cwm;forward chaining;computer science;sparql;artificial intelligence;rdf;data mining;database;rdf query language;world wide web;inference engine;rdf schema	AI	-36.68786701566527	11.09992471866571	122588
ccea7421a193adbeab7c10c8e81a9ac337f6d0b1	role of obligations in multiagent coordination	goal orientation;social interaction;client server;peer to peer;business process	Carrying out distributed business processes over networks is rapidly shifting the nature of application architectures from the simple command and control client-server model to complex peer-to-peer models supporting dynamic patterns of social interaction and behavior among autonomous, proactive, goal oriented agents. Trusting agents to autonomously make decisions and execute actions on behalf of humans, as part of global business processes, requires both understanding and modeling of the social laws that govern collective behavior and a practically useful operationalization of the models into agent programming tools. In this article we present a solution to these problems based on a representation of obliged and forbidden behavior in an organizational framework, together with an inference method that also decides which obligations to break in conicting situations. These are integrated into an operational, practically useful agent development language that covers the spectrum from the denition of organizations, roles, agents, obligations, goals, and conversations to inferring and executing coordinated agent behaviors in multiagent applications. The major strength of the approach is the way it supports coordination by exchanging constraints about obliged and forbidden behavior among agents. We illustrate this and the entire system with solution examples to the feature interaction problem in the telecommunications industry and to integrated supply chain management .	agent-based model;autonomous robot;business process;client–server model;feature interaction problem;peer-to-peer;programming tool;server (computing);trust (emotion)	Mihai Barbuceanu	1999	Applied Artificial Intelligence	10.1080/088395199117478	social relation;simulation;computer science;knowledge management;artificial intelligence;goal orientation;business process;client–server model	AI	-43.31804620753815	18.21477977475297	122603
395338562e4427ed578086ab3acf42172277a033	revisiting the concept of geospatial data interoperability within the scope of human communication processes	conceptual framework;geospatial data	Geospatial data interoperability has been the target of major efforts by standardization bodies (e.g. OGC, ISO/TC 211) and the research community since the beginning of the 1990s. It is seen as a solution for sharing and integrating geospatial data, more specifically to solve the syntactic, schematic, and semantic as well as the spatial and temporal heterogeneities between various representations of real-world phenomena. A few models have been proposed to automatically overcome heterogeneity of geospatial data and, as a result, increase the interoperability of geospatial data. However, the addition of a conceptual framework of geospatial data interoperability would contribute to understanding geospatial data interoperability, the appreciation of where existing contributions specifically apply, and would foster new contributions. In this paper, we revisit the concept of geospatial data interoperability within the broader scope of human communication and cognition. Human communication appears to be a rich framework since humans interoperate more easily than computers do. Accordingly, we present a conceptual framework of geospatial data interoperability that is broader in scope than existing frameworks and supported by several practical examples. An ontology of geospatial data interoperability is also introduced in order to refine the description of the conceptual framework. In such a communication-based framework, the notions of concept, Address for correspondence: Jean Brodeur, Natural Resources Canada, Centre for Topographic Information, 2144 King Street West, Sherbrooke, Quebec J1J 2E8, Canada. E-mail: Brodeur@RNCan.gc.ca 244 J Brodeur, Y Bédard, G Edwards and B Moulin © Blackwell Publishing Ltd. 2003 context, proximity, and ontology appear to be fundamental elements. These elements constitute a new approach to geosemantic proximity .	blackwell (series);cad data exchange;cognition;computer;context-sensitive language;decoding methods;geomatics;ground proximity warning system;interoperability;jean;ontology (information science);prototype;schematic;simulation;subdivision surface;theory;topography;world wide web	Jean Brodeur;Yvan Bédard;Geoffrey Edwards;Bernard Moulin	2003	Trans. GIS	10.1111/1467-9671.00143	semantic interoperability;computer science;knowledge management;geospatial analysis;data mining;conceptual framework;database;cross-domain interoperability;remote sensing	DB	-43.88691867715494	4.388837375188906	122628
1f7b11ad7ea7a001b19ba0d57a80a7e6d8199387	repairc: a tool for ensuring data consistency by means of active integrity constraints		Consistency of knowledge repositories is of prime importance in organization management. Integrity constraints are a well-known vehicle for specifying data consistency requirements in knowledge bases; in particular, active integrity constraints go one step further, allowing the specification of preferred ways to overcome inconsistent situations in the context of database management. This paper describes a tool to validate an SQL database with respect to a given set of active integrity constraints, proposing possible repairs in case the database is inconsistent. The tool is able to work with the different kinds of repairs proposed in the literature, namely simple, founded, well-founded and justified repairs. It also implements strategies for parallelizing the search for them, allowing the user both to compute partitions of independent or stratified active integrity constraints, and to apply these partitions to find repairs of inconsistent databases efficiently in parallel.	algorithm;data integrity;database;database caching;expert system;information repository;knowledge management;ontology (information science);overhead (computing);ptc integrity;parallel computing;prototype;requirement;sql;undo;word lists by frequency	Luís Cruz-Filipe;Michael Franz;Artavazd Hakhverdyan;Marta Ludovico;Isabel Nunes;Peter Schneider-Kamp	2015	CoRR			DB	-33.88061747289613	9.927964754654637	122727
47dabde418964eeb3c246047a432f6b3cd6f5741	are your rules online? four web rule essentials	search space;controlled natural language;knowledge representation	Four principal Web rule issues constitute our starting points: I1) Formal knowledge representation can act as content in its own right and/or as metadata for content. I2) Knowledge on the open Web is typically inconsistent but closed ‘intranet’ reasoning can exploit local consistency. I3) Scalability of reasoning calls for representation layering on top of quite inexpressive languages. I4) Rule representation should stay compatible with relevant Web standards. To address these, four corresponding essentials of Web rules have emerged: E1) Combine formal and informal knowledge in a Rule Wiki, where the formal parts can be taken as code (or as metadata) for the informal parts, and the informal parts as documentation (or as content) for the formal parts. This can be supported by tools for Controlled Natural Language: mapping a subset of, e.g., English into rules and back. E2) Represent distributed knowledge via a module construct, supporting local consistency, permitting scoped negation as failure, and reducing the search space of scoped queries. Modules are embedded into an ‘Entails’ element: prove whether a query is entailed by a module. E3) Develop a dual layering of assertional and terminological knowledge as well as their blends. To permit the specification of terminologies independent of assertions, the CARIN principle is adopted: a terminological predicate is not permitted in the head of a rule. E4) Differentiate the Web notion of URIs as in URLs, for access, vs. URNs, for naming. A URI can then be used: URL-like, for module import, where it is an error if dereferencing does not yield a valid knowledge document; URN-like, as an identifier, where dereferencing is not intended; or, as a name whose dereferencing can access its (partial) definition.	controlled natural language;dereference operator;develop;documentation;embedded system;intranet;knowledge representation and reasoning;local consistency;negation as failure;open web;scalability;uniform resource identifier;web standards;wiki;world wide web	Harold Boley	2007		10.1007/978-3-540-75975-1_2	natural language processing;knowledge representation and reasoning;searching the conformational space for docking;computer science;artificial intelligence;machine learning;data mining	Web+IR	-40.14695629408516	8.656528282519218	122779
71fb9612992ae1decb0f447f48f0b4b42fd20775	automatic generation of ontology from data models: a practical evaluation of existing approaches	data models enterprise applications ea knowledge acquisition ontology learning process semiautomatic ontology generation approach tool evaluation tool operability ontology evaluation semantic interoperability knowledge sharing approach;ontologies artificial intelligence;evaluation ontology ontology learning semantic interoperability semi automatic generation data model;knowledge acquisition;learning artificial intelligence;organisational aspects data models knowledge acquisition learning artificial intelligence ontologies artificial intelligence open systems;open systems;ontologies data models semantics interoperability resource description framework databases unified modeling language;data models;organisational aspects	Nowadays, ontology as a knowledge sharing approach plays an important role in semantic interoperability of enterprise applications (EAs). However, the manual process of ontology construction requires deep understanding of the domain. This approach is difficult, expensive and time-consuming. To overcome the knowledge acquisition bottleneck, ontology learning field aims to provide automatic and semi-automatic approaches for ontology generation. Several approaches have been emerged for this purpose. In this paper, we present a practical study of methods that take data models as input to the learning process. The main contributions of this work are: (i) the evaluation of the availability of existing approaches for (semi-)automatic generation of ontology from data models; (ii) the evaluation of tools according to their operability; and (iii) the evaluation of the resulting ontologies to assess their quality in supporting semantic interoperability. Our goal through this study is to find a response to the question: Is there a tool that extracts (semi-)automatically an application ontology from data models, intended for use in semantic interoperability?.	data model;enterprise software;knowledge acquisition;ontology (information science);ontology learning;operability;semantic interoperability;semantic network;semiconductor industry	Bouchra El Idrissi;Salah Baïna;Karim Baïna	2013	IEEE 7th International Conference on Research Challenges in Information Science (RCIS)	10.1109/RCIS.2013.6577694	upper ontology;semantic interoperability;data modeling;ontology alignment;bibliographic ontology;ontology inference layer;computer science;knowledge management;ontology;artificial intelligence;data mining;database;ontology-based data integration;open system;commonsense knowledge;owl-s;process ontology;suggested upper merged ontology	SE	-44.59337269454548	7.237189336494316	122788
08065242d3c6a657c712b6eb5d397505fcce7175	xml fragments extended with database operators	information retrieval;natural extension;xml document;information need;structured data	XML documents represent a middle range between unstructured data such as textual documents and fully structured data encoded in databases. Typically, information retrieval techniques are used to support search on the “unstructured” end of this scale, while database techniques are used for the structured part. To date, most of the works on XML query and search have stemmed from the structured side and are strongly inspired by database techniques. In a previous work we described a new query approach via pieces of XML data called “XML Fragments” which are of the same nature as the queried XML documents and are specifically targeted to support the information needs of end-users in an intuitive way. In addition to its simplicity, XML Fragments represent a natural extension to traditional free text information retrieval queries where both documents and queries are represented as vectors of words and as such it enables a natural extension of IR ranking models to rank XML documents by context and structure. In this paper, we extend XML Fragments with database operators thus allowing both IR style approach together with database “structured” query capabilities.	database;information needs;information retrieval;xml	Yosi Mass;Dafna Sheinwald;Benjamin Sznajder;Sivan Yogev	2007			well-formed document;xml catalog;xml validation;binary xml;xml encryption;xml namespace;simple api for xml;xml;streaming xml;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;qname;xml signature;xml schema editor;cxml;information retrieval;efficient xml interchange	DB	-34.050760174404445	5.167992440063915	122883
6e711099a2dc27f1c7e0889f27f6a1d0a7ef40e6	browsing semi-structured web texts using formal concept analysis	base relacional dato;navegacion informacion;donnee textuelle;information sources;red www;real estate;analisis datos;dato textual;information source;source information;navigation information;conference output;conceptual analysis;information browsing;robotics;relational database;data mining;analisis conceptual;data analysis;information storage retrieval and management;internet;fouille donnee;textual data;base donnee relationnelle;robotica;world wide web;analyse donnee;280103;reseau www;robotique;analyse conceptuelle;fuente informacion;formal concept analysis	Query-directed browsing of unstructured Web-texts using Formal Concept Analysis (FCA) confronts two problems. Firstly on-line Web-data is sometimes unstructured and any FCA-system must include additional mechanisms to structure input sources. Secondly many online collections are large and dynamic so a Web-robot must be used to automatically extract data. These issues are addressed in this paper. We report on the construction of a Web-based FCA system for browsing classified advertisements for real-estate properties. Real-estate advertisements were chosen because they are typical of semi-structured textual information sources accessible on the Web. Furthermore, the analysis of real-estate data using FCA is a classic example used in introductory courses on FCA. However, unlike the classic FCA real-estate example, whose input is a structure relational database, we automatically mine Web-based texts for their structure.	browsing;cyclic redundancy check;diagram;donald becker;formal concept analysis;graphical user interface;internet;java applet;online and offline;parsing;regular expression;relational database;semiconductor industry;server (computing);source data;strand (programming language);the australian;world wide web;xml	Richard Cole;Peter W. Eklund	2001		10.1007/3-540-44583-8_23	the internet;relational database;computer science;formal concept analysis;artificial intelligence;data mining;database;robotics;data analysis;world wide web;computer security;real estate	AI	-36.06127640264447	10.62687803035858	122911
133732afb20cee7c23a39e5d85bafdd6aae351f8	enacting social argumentative machines in semantic wikipedia		This research advocates the idea of combining argumentation theory with the social web technology, aiming to enact large scale or mass argumentation. The proposed framework allows mass-collaborative editing of structured arguments in the style of semantic wikipedia. The long term goal is to apply the abstract machinery of argumentation theory to more practical applications based on human generated arguments, such as deliberative democracy, business negotiation, or selfcare. The ARGNET system was developed based on ther Semantic MediaWiki framework and on the Argument Interchange Format (AIF) ontology.	argument map;mediawiki;social media;software agent;wikipedia	Adrian Groza;Sergiu Indrie	2013	CoRR		natural language processing;computer science;knowledge management;artificial intelligence	AI	-46.47432418033	4.623031654294689	123014
edda8994daee4888fe6b2a95e4ed3073ee1e6989	iswive: an integrated semantic web interactive visualization environment	topic features;information resources;visualization interfaces;user interfaces data visualisation semantic web interactive systems;topic maps;interactive visualization;web searching;information visualization;resource description framework;data mining;visualization interfaces integrated semantic web interactive visualization environment information visualization data semantics topic maps topic features resource descriptions interactive local viewer visual query interface web browsing web searching;semantic web resource description framework data visualization information resources computer science councils web sites data models data mining large scale systems;data visualisation;data semantics;visual query interface;web sites;data visualization;semantic web;councils;web browsing;computer science;interactive local viewer;visual interfaces;interactive systems;resource descriptions;user interfaces;large scale systems;integrated semantic web interactive visualization environment;data models	The techniques of information visualization have been initially applied to the semantic Web to facilitate the presentation of data semantics. Currently, the semantic Web visualization interfaces are separately based on two distinctive frameworks, RDF and topic maps. Since the visualization interfaces based on each framework only present a partial view of semantic Web, an integration mechanism is needed to demonstrate them together for the panorama of semantic Web. In this paper, we present an integrated semantic Web interactive visualization environment (ISWIVE) to incorporate the topic features from topic maps into RDF. Both the detailed resource descriptions and the overall topic relationship can be clearly visualized in ISWIVE. Besides, an interactive local viewer and visual query interface facilitate browsing and searching over the semantic Web resources.	information visualization;interactive visualization;map;resource description framework;semantic web;topic maps;web resource	Ing-Xiang Chen;Chun-Lin Fan;Pang-Hsiang Lo;Li-Chia Kuo;Cheng-Zen Yang	2005	19th International Conference on Advanced Information Networking and Applications (AINA'05) Volume 1 (AINA papers)	10.1109/AINA.2005.228	topic maps;data modeling;semantic computing;web modeling;data web;web mapping;semantic search;semantic grid;web standards;computer science;sparql;semantic web;rdf;social semantic web;web page;linked data;semantic web stack;database;web intelligence;semantic technology;user interface;world wide web;information retrieval;semantic analytics;data visualization	Visualization	-42.470223965052845	8.812138388000117	123095
825419d10eed006797bf3c04124c662269e7990e	making knowledge machine-processable: some implications of general semantic search	linked data;search engines;sociology of technology;swinburne;knowledge;semantic web;semantic search	ABSTRACTThe central argument of this paper is that the design, implementation and use of technologies that underpin general semantic search have implications for what we know and the way in which knowledge is understood. Semantic search is an assemblage of technologies that most Internet users would use regularly without necessarily realising. Users of search engines implementing semantic search can obtain answers to questions rather than just retrieve pages that include their search query. This paper critically examines the design of the Semantic Web, upon which semantic search is based. It demonstrates that implicit in the design of the Semantic Web are particular assumptions about the nature of classification and the nature of knowledge. The Semantic Web was intended for interoperability within specific domains. It is here argued that the extension to general semantic search, for use by the general public, has implications for what type of knowledge is visible and what counts as legitimate knowledge. T...	knowledge machine;semantic search	Vivienne Waller	2016	Behaviour & IT	10.1080/0144929X.2016.1183710	semantic data model;semantic interoperability;semantic similarity;semantic computing;semantic integration;semantic search;semantic grid;computer science;semantic web;social semantic web;linked data;data mining;semantic web stack;semantic compression;knowledge;world wide web;information retrieval;semantic analytics	NLP	-39.71242875413204	5.224470321065062	123206
f1fb7ac37ba24dd82fc7150012fa55a873fd49bb	establishing the semantic web reasoning infrastructure on description logic inference engines	proof of concept;semantic web;description logic;knowledge discovery	The recent advent of the Semantic Web has given rise to the need for efficient and sound methods that would provide reasoning support over the knowledge scattered on the Internet. Description Logics and DL-based inference engines in particular play a significant role towards this goal, as they seem to have overlapping expressivity with the Semantic Web de facto language, OWL. In this paper we argue that DLs currently constitute one of the most tempting available formalisms to support reasoning with OWL. Further, we present and survey a number of DL based systems that could be used for this task. Around one of them (Racer) we build our Knowledge Discovery Interface, a web application that can be used to pose intelligent queries to Semantic Web documents in an intuitive manner. As a proof of concept, we then apply the KDI on the CIDOC-CRM reference ontology and discuss our results.	cidoc conceptual reference model;description logic;distributed computing;formal system;hands-on computing;inference engine;prototype;refinement (computing);semantic web;semantics (computer science);upper ontology;web language;web ontology language;web application;web page	Dimitrios A. Koutsomitropoulos;Dimitrios P. Meidanis;Anastasia N. Kandili;Theodore S. Papatheodorou	2006		10.1007/978-3-540-77581-2_24	knowledge representation and reasoning;web modeling;description logic;semantic web rule language;semantic search;ontology inference layer;semantic grid;web standards;computer science;artificial intelligence;semantic web;social semantic web;data mining;semantic web stack;database;knowledge extraction;proof of concept;world wide web;owl-s;information retrieval;semantic analytics	AI	-39.0961572715487	7.3452314713721725	123389
aaa9a5c170947862ba9e4c0874d65abbe263cc93	semantic annotation model definition for systems interoperability	semantic annotation;ontology;models;systems interoperability	Semantic annotation is one of the useful solutions to enrich target’s (systems, models, meta-models, etc.) information. There are some papers which use semantic enrichment for different purposes (integration, composition, sharing and reuse, etc.) in several domains, but none of them provides a complete process of how to use semantic annotations. This paper identifies three main components of semantic annotation, proposes for it a formal definition and presents a survey of current semantic annotation methods. At the end, we present a simple case study to explain how our semantic annotation proposition can be applied.The survey presented in this paper will be the basis of our future research on models, semantics and architecture for enterprises systems interoperability during the product lifecycle.	gene ontology term enrichment;interaction;interoperability;interoperation;ontology (information science);reference model;semiconductor industry;sequence alignment	Yongxin Liao;Mario Lezoche;Hervé Panetto;Nacer Boudjlida	2011		10.1007/978-3-642-25126-9_14	minimum information required in the annotation of models;semantic interoperability;semantic similarity;semantic computing;semantic integration;semantic grid;computer science;data mining;semantic web stack;semantic compression;database;semantic equivalence;semantic technology;temporal annotation;information retrieval	AI	-43.399306891299666	8.442794578315436	123409
fe7d521e5b1b2f8b842223bbd0e055159326d751	ontologies and relational databases meta-schema design: a three-level unified lifecycle		To integrate ontologies and relational databases, in this paper, we propose to build a common meta-schema upon an overall unified lifecycle. Our key idea is to consider that the database is a model for the domain ontology, and conversely. To this end, the ontology-based knowledge and the business-entities are merged with the same conceptual abstractions principles, according to the three-level ANSI/SPARC architecture. Finally, the goal is to show that ontologies' features can be managed by relational databases. A case of study outlines the applicability of our approach.	.net framework;amiga rigid disk block;application programming interface;entity;library (computing);ontology (information science);open api;prototype;relational database;sparc;world wide web	Oumar Sy;Denio Duarte;Guilherme Dal Bianco	2018	2018 5th International Conference on Control, Decision and Information Technologies (CoDIT)	10.1109/CoDIT.2018.8394889	unified modeling language;ontology (information science);architecture;semantics;database;schema (psychology);data modeling;relational database;abstraction;computer science	DB	-38.32622136987151	8.003985130890337	123428
f4c7644a18870dee7c4415c37f7c568089d9fc1b	ontological definition of population for public health databases		This paper discusses available definitions of population and criticise them against some basic rules of ontology. None of the found definition satisfies the requirement of an ontology that supports building consistent public heath databases. Most definitions define population on territorial bases or as reproductive communities. The author argues that populations are systems (its members must be in connection to each other) and individuals forming the population must share a common resource. Proper usage of the definitions may help to build consistent ontology for public health indicators and consistent databases.		György Surján	2005	Studies in health technology and informatics		data science;knowledge management;data mining;ontology;database;public health;population;medicine	HCI	-45.74498593175769	4.235879032767838	123568
3ebb750a997b7522bd0bbacb0e59d358cef16d0e	automatic web service composition using congolog	databases;distributed application;semantic web service;web services semantic web logic programming databases world wide web web sites markup languages automation distributed computing conferences;distributed computing;web service;web service composition;information gathering;theorem prover;logic programming;web services;markup languages;web sites;semantic web;world wide web;logic programs;conferences;automation	Semantic Web Service, nowadays, is emerging as a solution for interoperating distributed applications on the WWW. Web service composition is the task of combining existing web services to yield a new service in order to achieve a desired goal. In this paper, we propose a formal approach to translate OWL-S web service descriptions into primitive and complex actions of ConGolog, a high level logic programming language with sensing actions for web service composition. In addition, in order to support information gathering with search in an open world initial database, we propose an extended version of the middle-ground ConGolog interpreter which relies on a theorem-prover with prime implicates.	automated theorem proving;distributed computing;high-level programming language;owl-s;open world;semantic web service;semantics (computer science);service composability principle;www	Minh Phan;Fumio Hattori	2006	26th IEEE International Conference on Distributed Computing Systems Workshops (ICDCSW'06)	10.1109/ICDCSW.2006.24	web service;web development;web modeling;data web;web standards;computer science;ws-policy;social semantic web;semantic web stack;database;distributed computing;programming language;world wide web;information retrieval	DB	-43.45113615550798	13.630436197117149	123706
e39ee7b76dabd8fb012808d7b493709d272e39f2	database selection and keyword search of structured databases: powerful search for naive users	word frequency;query processing;estimation method;information retrieval;relational database;satisfiability;online front ends;user friendly web based interface database selection keyword searching web user satisfaction distributed databases relational databases information retrieval logical operators web text file searching relational database query processing end user satisfaction web content access distributed structured databases estimation method word frequency information;keyword search;distributed databases web sites relational databases query processing user interfaces online front ends;web sites;distributed databases;keyword search relational databases distributed databases information retrieval informatics prototypes web pages computer science information systems;relational databases;user satisfaction;user interfaces	The main target of the w r k described in this paper is to provide a powerful approach for naiLe users to search structured databases. Such a study is necessary especially to satisfi web users who expect the abilify to access all web contents in a unified way, regardless of the structure of the available information. Given a set of distributed structured databases and a que?y which consists of a set of keywords connected by logical operators, the approach proposed in this paper adapts both web text files search techniques and information retrieval techniques to rank the existing databases based on their relevance to the posed query. For each keyword, the user specifies a level of search, which may be column, record, or table. We developed an estimation method with statistical foundations to estimate the usefulness of individual relational databases. The system gives a hint of what databases might be useful for the user's query, based on wrd-frequency information kept for each database. Some experiments have been conducted to demonstrate the efectiveness of the proposed method in determining promising sources for a given query. As nai've end-user satisfaction is a main target and motive. w developed a prototype system with a user friendly web-based interface that accomplishes our goals in a simple andpowiful w y .	experiment;information retrieval;logical connective;prototype;relational database;relevance;table (database);usability;web application	Mohammad Hassan;Reda Alhajj;Mick J. Ridley;Ken Barker	2003		10.1109/IRI.2003.1251411	search-oriented architecture;sargable;query expansion;web query classification;relational database;computer science;data mining;database;web search query;world wide web;distributed database;information retrieval;query language	DB	-34.1026425360456	4.680696578037777	123820
1cfbb4d63a88c6ddd671b41675040d201f0381e9	prior domain knowledge and ontological overlap in combinations of conceptual models		IS Professionals often use multiple types of models because most information systems are too complex to represent in a single model. When a domain is represented through different models, some features of the envisaged system can feature in multiple models. This situation may lead to overlap between models. The literature is unclear whether overlap is beneficial in understanding multiple models, and how overlap should be designed – or avoided. We propose that understanding multiple models with overlap is a function of both pragmatic and semantic factors. We will execute an experiment to investigate our propositions, which will include eye-tracking to examine how users identify and deal with information overlap between multiple conceptual models. The results of this study can contribute to the existing theories of domain representation by analyzing the interaction between pragmatics and semantics. We can provide practical guidance on how different domains should be presented through multiple models.		Mohammad Jabbari;Jan Recker;Peter F. Green	2018			natural language processing;conceptual model;information system;domain knowledge;pragmatics;ontology;semantics;multiple models;computer science;artificial intelligence	Vision	-45.72618683094135	7.107474006635753	124087
2982f864863ad3f0f362bd5b2ecd1ac00a244142	pattern-based methodology for building the ontologies of scientific subject domains			ontology (information science)	Yury A. Zagorulko;Galina Zagorulko;Olesya Borovikova	2018		10.3233/978-1-61499-900-3-529	systems engineering;ontology (information science);computer science	HPC	-43.920412489004995	5.6563399377530645	124397
4d63afefdbf86db6d29a51e526d596a3df3425d4	the design and application of a domain specific knowledgebase in the tacitus text understanding system		TACITUS is a text understanding system being developed at SRI In­ ternational. One of the main components in the system is a knowledge­ base which contains commonsense and domain specific world knowledge encoded as axioms in a first order predicate calculus language. The prime function o f the knowledgebase is to provide extra-linguistic facts to be used in the resolution of a range of ambiguities such as compound nom­ inal constructions, definite reference, and in drawing conclusions on the basis of the implicatures in the text. The paper discusses the methodology used in building a knowledgebase for analyzing news reports about terror­ ist attacks, and demonstrates how it is used in an application extracting information to be stored in a simulated database.	commonsense knowledge (artificial intelligence);first-order logic;knowledge base	Annelise Bech	1989				AI	-39.32877077762001	4.4846009613657225	124583
9d0abf19a7ef8864a55517a470d44926a7ff13f9	ontoms2: an efficient and scalable ontology management system with an incremental reasoning		We present ONTOMS2, an efficient and scalable ONTOlogy Management System with an incremental reasoning. ONTOMS2 stores an OWL document and processes OWL-QL and SPARQL queries. Especially, ONTOMS2 supports SPARQL Update queries with an incremental instance reasoning of inverseOf, symmetric and transitive properties.	management system;ontology (information science);sparql;scalability;web ontology language	Min-Joong Lee;Jong-Ryul Lee;Sangyeon Kim;Myung-Jae Park;Chin-Wan Chung	2013			computer science;data mining;database;information retrieval	Web+IR	-37.46660237999123	6.637803364482335	124801
8d618e74c81420692e3ff66533b3870de7dd4da4	a broker-based framework for qos-aware web service composition	end to end qos;service composition;qos broker based framework qos aware web service composition in house business service business transaction quality of service;web service;web service composition;online front ends;collection selection;internet;business data processing;transaction processing quality of service online front ends internet business data processing;quality of service;transaction processing;similarity function;web services quality of service business computer science application software system software distributed computing outsourcing systems engineering and theory protocols;business process	Web services are modular web applications that can be independently deployed and invoked by other software or services on the web. This offers enterprises the capability to integrate in-house business services with external Web services to conduct complex business transactions. The integration efficiency and flexibility are critical for services composition. For Web services providing a similar functionality, Quality of Service (QoS) is the main factor to differentiate them. The overall QoS of a business process must meet a user's requirement. In this paper, we propose a broker-based framework to facilitate dynamic integration and adaptation of QoS-aware Web services with end-to-end QoS constraints. The key functions of a dynamic broker include service collection, selection, composition and adaptation. Our study considers both functional and QoS characteristics of Web services to identify the optimal business process solutions.	business process;end-to-end principle;functional programming;quality of service;service composability principle;web application;web service	Tao Yu;Kwei-Jay Lin	2005	2005 IEEE International Conference on e-Technology, e-Commerce and e-Service	10.1109/EEE.2005.1	web service;web application security;web development;web modeling;mobile qos;business process execution language;business service provider;differentiated service;web standards;service delivery framework;marketing;ws-policy;service-oriented architecture;service design;ws-addressing;database;business;services computing;web 2.0;world wide web;universal description discovery and integration	DB	-47.56927163349343	16.795469583262417	125047
12721ce25b491988dcc52c5bab2282032062bcaf	anti-patterns: integrating distributed and heterogeneous data sources in soas	semiconductor optical amplifiers;information resources;collaborative work;information security;anti pattern;distributed processing;data management;soa;industrial projects;service oriented architecture antipatterns heterogeneous data sources distributed data sources industrial projects;data management soa anti pattern;information management;antipatterns;web services;computer science;distributed data sources;service oriented architecture;data models;data security;heterogeneous data sources	In computer science, anti-patterns are specific repeated practices that appear initially to be beneficial, but ultimately result in undesirable consequences that outweigh the expected advantages. In this article we present our experiences from a range of industrial projects where there is a strong need for integrating distributed and heterogeneous data sources. SOA approach is selected to achieve the integration goals. We present the problem and the common solution approach that has been repeatedly observed in the field. This common approach manifests itself as an anti-pattern since in many cases the solution produces unwanted outcomes. We outline the reasons for the failures and introduce a preferred solution to the same problem that we have successfully applied in many cases as an alternative to the so-called common approach.	anti-pattern;computer science	Hakan Hacigümüs	2008	2008 IEEE Congress on Services - Part I	10.1109/SERVICES-1.2008.91	computer science;data mining;database;distributed computing	Robotics	-46.71430382443039	7.091559725980301	125053
33909aec7ceaaac48a2b57c4912d874a4d659a8a	metamodeling architecture of web ontology languages	web ontology language	Recentresearchhasshown thatRDF Schema,asa schemalayerSemanticWeb language, hasa non-standardmetamodelingarchitecture.As a result,it is difficult to understandandlacksclear semantics. Thispaperproposesafixedlayermetamodelingarchitecturefor RDFSchema(RDFS(FA)) anddemonstrateshow the problemsof RDF Schemacanbe solved underRDFS(FA). Basedon this metamodelingarchitecture,a clear model-theoreticsemanticsof RDFS(FA) is given. Interestingly, RDFS(FA) also benefitsDAML+OIL by offering a firm semanticbasisand by solving the “layer mistake” problem.	metamodeling;resource description framework	Jeff Z. Pan;Ian Horrocks	2001			ontology inference layer;computer science;artificial intelligence;data mining;database;programming language;web ontology language;world wide web	PL	-38.19988580929741	7.953669038830737	125127
0d45e56f34d7819c94e21cb9acc819848ee7875b	an ontology framework for knowledge-assisted semantic video analysis and annotation	semantic web service;video object;video analysis;color model;semantic web technology;semantic metadata;football;multimedia data;knowledge representation;semantic analysis	An approach for knowledge assisted semantic analysis and annotation of video content, based on an ontology infrastructure is presented. Semantic concepts in the context of the examined domain are defined in an ontology, enriched with qualitative attributes of the semantic objects (e.g. color homogeneity), multimedia processing methods (color clustering, respectively), and numerical data or low-level features generated via training (e.g. color models, also defined in the ontology). Semantic Web technologies are used for knowledge representation in RDF/RDFS language. Rules in F-logic are defined to describe how tools for multimedia analysis should be applied according to different object attributes and low-level features, aiming at the detection of video objects corresponding to the semantic concepts defined in the ontology. This supports flexible and managed execution of various application and domain independent multimedia analysis tasks. This ontology-based approach provides the means of generating semantic metadata and as a consequence Semantic Web services and applications have a greater chance of discovering and exploiting the information and knowledge in multimedia data. The proposed approach is demonstrated in the Formula One and Football domains and shows promising results.	artificial intelligence;cluster analysis;digital video;domain of discourse;domain-specific language;f-logic;high- and low-level;knowledge representation and reasoning;level of measurement;mpeg-7;norm (social);numerical analysis;ontology (information science);rdf schema;resource description framework;semantic web service;semantic analysis (compilers);video content analysis;web ontology language	Stamatia Dasiopoulou;Vasileios Papastathis;Vasileios Mezaris;Yiannis Kompatsiaris;Michael G. Strintzis	2004			semantic data model;semantic interoperability;semantic computing;semantic integration;semantic web rule language;data web;explicit semantic analysis;semantic search;semantic grid;semantic web;social semantic web;semantic web stack;semantic compression;semantic equivalence;multimedia;semantic technology;world wide web;owl-s;information retrieval;semantic analytics	AI	-42.056069878513426	7.978665091499768	125348
8b9a76d74c94207c39f44f7b4bfc19708bc3c10e	generating explanations for complex biomedical queries		We present a computational method to generate explanations to answers of complex queries over biomedical ontologies and databases, using the high-level representation and efficient automated reasoners of Answer Set Programming. We show the applicability of our approach with some queries related to drug discovery over PHARMGKB, DRUGBANK, BIOGRID, CTD and SIDER.	answer set programming;biogrid;comparative toxicogenomics database (ctd);computation;high- and low-level;ontology (information science);pharmgkb;stable model semantics	Umut Öztok;Esra Erdem	2011			computer science;data mining;database;information retrieval	AI	-38.862196840554844	4.239914740071385	125356
5055bcaba46a5cf4d4ab9ba7f4b5527b569c7b1c	enhancing uddi for grid service discovery by using dynamic parameters	service web;web service;intergiciel publication souscription;grid;intergicial editor suscriptor;rejilla;grid service;grille;information system;080307 operating systems;publish subscribe middleware;systeme information;servicio web;sistema informacion	A major problem for a grid user is the discovery of currently available services. With large number of services, it is beneficial for a user to be able to discover the services that most closely match their requirements. This report shows how to extend some concepts of UDDI such that they are suitable for dynamic parameter based discovery of grid services.	open grid services architecture;programming language;query language;requirement;scalability;service discovery;xml	Brett Sinclair;Andrzej M. Goscinski;Robert Dew	2005		10.1007/11424857_6	web service;semantic grid;computer science;data mining;database;service discovery;ws-i basic profile;grid;world wide web;universal description discovery and integration;information system;grid computing	HPC	-37.5345319348713	12.712557145531024	125486
b81362cce2e1c7a057c705207a60eb27f3483847	integration and querying of genomic and proteomic semantic annotations for biomedical knowledge extraction	mining of semantically annotated biological data management and integration of heterogeneous and distributed biological data biological ontologies querying and retrieval of semantic biological annotations;genomics;biological ontologies;flexible modular multilevel global data schema genomic semantic annotation integration genomic semantic annotation querying proteomic semantic annotation integration proteomic semantic annotation querying biomedical knowledge extraction multiple biomolecular information distributed data sources heterogeneous data sources software architecture genomic and proteomic knowledge base integrated data feature abstraction integrated data feature generalization data content data structure data number integrated biomedical ontologies web interface bioinformatics;semantics bioinformatics ontologies computational biology genomics proteomics;semantics;software architecture bioinformatics data integration data mining data structures genomics medical information systems ontologies artificial intelligence proteomics query processing semantic web;management and integration of heterogeneous and distributed biological data;ontologies;mining of semantically annotated biological data;proteomics;computational biology;querying and retrieval of semantic biological annotations;bioinformatics	Understanding complex biological phenomena involves answering complex biomedical questions on multiple biomolecular information simultaneously, which are expressed through multiple genomic and proteomic semantic annotations scattered in many distributed and heterogeneous data sources; such heterogeneity and dispersion hamper the biologists’ ability of asking global queries and performing global evaluations. To overcome this problem, we developed a software architecture to create and maintain a Genomic and Proteomic Knowledge Base (GPKB), which integrates several of the most relevant sources of such dispersed information (including Entrez Gene, UniProt, IntAct, Expasy Enzyme, GO, GOA, BioCyc, KEGG, Reactome, and OMIM). Our solution is general, as it uses a flexible, modular, and multilevel global data schema based on abstraction and generalization of integrated data features, and a set of automatic procedures for easing data integration and maintenance, also when the integrated data sources evolve in data content, structure, and number. These procedures also assure consistency, quality, and provenance tracking of all integrated data, and perform the semantic closure of the hierarchical relationships of the integrated biomedical ontologies. At  http://www.bioinformatics.deib.polimi.it/GPKB/, a Web interface allows graphical easy composition of queries, although complex, on the knowledge base, supporting also semantic query expansion and comprehensive explorative search of the integrated data to better sustain biomedical knowledge extraction.	biocyc database collection;biological phenomena;closure;data sources;entrez;evaluation;expasy;generalization (psychology);genetic heterogeneity;graphical user interface;interface device component;kegg;knowledge base;modular programming;online mendelian inheritance in man;ontology (information science);proteomics;query expansion;question (inquiry);semantic query;software architecture;trim47 gene;uniprot	Marco Masseroli;Arif Canakoglu;Stefano Ceri	2016	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2015.2453944	semantic data model;idef1x;biology;genomics;semantic computing;semantic integration;computer science;bioinformatics;ontology;data mining;semantics;ontology-based data integration;proteomics;information retrieval	DB	-38.129603007466834	5.074878972442592	125618
01b4c567dd34eb03632c4499c637ac9440f3c2ef	a discount approach to the semantic web	semantic web	The frameworks underlying the Semantic Web have developed and matured greatly over the last years. However, uptake has been patchy, with the majority of SW use based around a small number of popular applications. User testing with SW–based projects highlights a number of issues that may contribute to this effect; principally, these relate to gaps between the user’s mental model and formalism. Similar problems appear in non–SW developments with a strong reliance on a complex data model. Such problems include semantic drift and overload, and the provision of inaccurate or incomplete data. Working from a case study, this paper discusses difficulties with capturing real–world semantics in a large–scale collaborative environment. A preliminary model of user behaviour with respect to shared establishment of semantics, from socially shared cognition, is discussed. We conclude by discussing some possible features of a “discount” model of the Semantic Web, designed to accomodate diverse communities of users, with reference to examples taken from the “small–S” Semantic Web, microformats and free–text tagging.	cognition;data model;heuristic (computer science);mental model;microformat;semantic web;semantics (computer science);shattered world;usability	Emma Tonkin	2006			semantic computing;semantic web rule language;data web;semantic search;semantic grid;web standards;computer science;artificial intelligence;semantic web;social semantic web;semantic web stack;web intelligence;semantic technology;world wide web;owl-s;website parse template;semantic analytics	Web+IR	-46.516679613898305	4.7705426970347675	125658
663ae66fea4208ad72b857eac3d83bd696cb06e3	searching and retrieving legal literature through automated semantic indexing	search and retrieval;web documents;semantic indexing;legal ontologies;service provider;machine learning;design and implementation;indexation;semantic web;document classification;structured data	Access to legal information and, in particular, to legal literature is examined in conjunction with the creation of a Portal to Italian legal doctrine. The design and implementation of services such as integrated access to a wide range of resources are described, with a particular focus on the importance of exploiting metadata assigned to disparate legal material. The integration of structured repositories and Web documents is the main purpose of the Portal: it is constructed on the basis of a federation system with service provider functions, aiming at creating a centralized index of legal resources. The index is based on a uniform metadata view created for structured data by means of the OAI approach and for Web documents by a machine learning approach. Subject searching is a major requirement for legal literature users and a solution based on the exploitation of Dublin Core metadata, as well as the use of legal ontologies and related terms prepared for accessing indexed articles have been implemented.	centralized computing;dublin core;machine learning;ontology (information science);web page	Enrico Francesconi;Ginevra Peruginelli	2007		10.1145/1276318.1276343	service provider;data model;computer science;semantic web;database;world wide web;information retrieval	Web+IR	-41.17008790689015	5.722324896600434	125671
12b380a80ba66e7bb0babc01f0393e27acd57a0a	web interface between users and a centralized information multi-agent system	multi agent system;knowledge management;web interface	Web retrieval becomes more and more important for the knowledge management area, and we think that multi-agent systems are a good answer to this problem. We propose, in this paper, a centralized Information multi-agent system to help actors of technological watch cells (called CIMASTEWA). This system is an evolution of a previous project and is set-up within a n-tiers architecture, which follows the STRUTS framework. This information multiagent system that has been developed to answer to demands from technological watch cells (for example, to securitize the search, notably concerning the survey by spies, we have proposed particular search strategies).	agent-based model;centralized computing;knowledge management;multi-agent system	Emmanuel Adam;René Mandiau	2004			social semantic web;web modeling;web intelligence;web development;web service;web server;computer science;web navigation;multi-agent system;knowledge management	AI	-46.18224652573787	10.23272493258434	125944
c1d01f8f7444678666e7a5ebcf9551a2a4a7c93c	a distributed rule execution mechanism based on mapreduce in sematic web reasoning	distributed system;rule execution;会议论文;sematic web;mapreduce;reasoning;rule	Rule execution is the core step of rule-based semantic web reasoning. However, most existing approaches are centralized, which cannot scale out to reason big semantic web datasets. In this paper, we described a kind of semantic web rule execution mechanism using MapReduce programming model, which not only can handle RDFS and OWL ter Horst semantic rules, but also can be used in SWRL reasoning. Theoretical analysis is present on the scalability of this rule execution mechanism. Result shows that it can scale well as Mapreduce framework.	centralized computing;logic programming;mapreduce;programming model;rdf schema;scalability;semantic web rule language	Haijiang Wu;Jie Liu;Dan Ye;Hua Zhong;Jun Wei	2013		10.1145/2532443.2532457	semantic web rule language;computer science;social semantic web;data mining;database;world wide web	Web+IR	-37.34949533289379	6.48334389076462	126089
e1e55dc9c86691edac7cea46830acbd709e983c5	cooperative question answering for the semantic web		In this paper we propose a Cooperative Question Answering System that takes as input queries expressed in natural language and is able to return a cooperative answer obtained from resources in the Semantic Web, more specifically DBpedia databases represented in OWL/RDF. Moreover, when the DBpedia provides no answer, we use the WordNet in order to build similar questions. Our system resorts to ontologies not only for reasoning but also to find answers and is independent of prior knowledge of the semantic resources by the user. The natural language question is translated into its semantic representation and then answered by consulting the semantics sources of information. If there are multiple answers to the question posed (or to the similar questions for which DBpedia contains answers), they will be grouped according to their semantic meaning, providing a more cooperative and clean answer to the user.	cinema 4d;dbpedia;database;knowledge base;natural language;ontology (information science);parsing;question answering;relevance;semantic web;web resource;wordnet	Dora Melo;Irene Pimenta Rodrigues;Vitor Beires Nogueira	2011			semantic web;social semantic web;semantic web stack;information retrieval;semantic analytics	Web+IR	-36.7667264421885	6.066046922998015	126312
c88fa99480416d6b36d719135624b8334e498f29	validating ontoelect methodology in refining icteri scope ontology		This research work is focused on the experimental validation of our OntoElect approach to ontology engineering in the case study of iterative refinement of the ICTERI Scope Ontology. OntoElect has been used for evaluating the commitment of the domain knowledge stakeholders to the ontological offering measured as positive and negative votes. The analysis of the measures allowed us answering the questions about the fitness of the ontological offering to the implicit requirements of the stakeholders as well as the completeness of the domain model with respect to their implicit expectations. The paper briefly presents the idea of OntoElect as a socially inspired approach for ontology engineering. It further describes the objectives and the set-up of our evaluation experiment. Finally it presents experimental results.		Olga Tatarintseva;Yuriy Borue;Vadim Ermolayev	2012		10.1007/978-3-642-38370-0_12	knowledge management;data mining	Crypto	-45.05915867156863	7.804128333241303	126622
1b25f6d4d02a10354bcd8db275884c2286b50d98	towards the anonymisation of rdf data	informatica	Privacy protection in published data sets is of crucial importance, and anonymisation is one well-known technique for privacy protection that has been successfully used in practice. However, existing anonymisation frameworks have in mind specific data structures (i.e., tabular data) and, because of this, these frameworks are difficult to apply in the case of RDF data. This paper presents an RDF anonymisation framework that has been developed to address the particularities of the RDF specification. Such framework includes an anonymisation model for RDF data, a set of anonymisation operations for the implementation of such model, and a metric for measuring precision and distortion of anonymised RDF data. Furthermore, this paper presents a use case of the proposed RDF anonymisation framework.	algorithm;data structure;distortion;entity;personally identifiable information;privacy;resource description framework;table (information)	Filip Radulovic;Raúl García-Castro;Asunción Gómez-Pérez	2015		10.18293/SEKE2015-167	computer science;data mining;database;world wide web	DB	-36.49281314481596	4.281628041548043	126931
2eff05d8a56e92a8e408500674c37527a27b26ac	discovering and using entity mappings in federated databases	entity mapping;federated databases	In the context of a federated database containing similar information in local component databases, we consider the problem of discovering the different local representations which database designers might use to map the same entity. An extension of the normal SQL system catalog to hold additional domain metadata is discussed. This enables common local mappings to be discovered and allows SQL retrieval queries expressed in terms of the logical federated schema to be transformed to match the appropriate local database schema. The result is a practical means of coping with differing mappings in federated databases without having to resort to special higher order languages or a layer of information mediation middleware. Copyright © 1999 John Wiley u0026 Sons, Ltd.	database;entity–relationship model	P. A. Dearnley;D. J. Smith	1999	Softw., Pract. Exper.	10.1002/(SICI)1097-024X(199901)29:1%3C17::AID-SPE219%3E3.0.CO;2-Q	computer science;data mining;database;database schema;information retrieval	DB	-35.68521352061072	7.1821290709604835	127015
df86b2cb19d29fe0258efb68aa82335ede0cb83a	exploitingweb service semantics: taxonomies vs. ontologies	formal specification;web service;semantic description	Comprehensive semantic descriptions of Web services are essential to exploit them in their full potential, that is, discovering them dynamically, and enabling automated service negotiation, composition and monitoring. The semantic mechanisms currently available in service registries which are based on taxonomies fail to provide the means to achieve this. Although the terms taxonomy and ontology are sometimes used interchangably there is a critical difference. A taxonomy indicates only class/subclass relationship whereas an ontology describes a domain completely. The essential mechanisms that ontology languages provide include their formal specification (which allows them to be queried) and their ability to define properties of classes. Through properties very accurate descriptions of services can be defined and services can be related to other services or resources. In this paper, we discuss the advantages of describing service semantics through ontology languages and describe how to relate the semantics defined with the services advertised in service registries like UDDI and ebXML.	ontology (information science);taxonomy (general)	Asuman Dogac;Gokce Laleci;Yildiray Kabak;Ibrahim Cingil	2002	CoRR		upper ontology;web service;computer science;ontology;data mining;formal specification;database;world wide web	NLP	-44.412153661858156	14.179096503638778	127147
b16901e6add27f3c353f3ec37ffe6861ebb339d5	an intelligent decision support system for digital preservation	archivo electronico;intelligent decision support system;gestion fonds;migration;cultural heritage;service oriented architectures soa;web service;gestion fondos;stock management;decision support systems;web services;digital preservation;preservation metadata;preservation;preservation services;information system;service oriented architecture;preservacion;systeme information;electronic storage;archivage electronique;authenticity;sistema informacion	This paper describes a Service Oriented Architecture (SOA) based on Web services technology designed to assist cultural heritage institutions in the implementation of migration based preservation interventions. The proposed SOA delivers a recommendation service and a method to carry out complex format migrations. The recommendation service is supported by three evaluation components that assess the quality of every migration intervention in terms of its performance (Migration Broker), suitability of involved formats (Format Evaluator) and data loss (Object Evaluator). Throughout the paper the whole workflow between these three components is explained in detail as well as the most relevant tasks that are carried out internally in each of them. The proposed system is also able to produce preservation metadata that can be used by client institutions to document preservation interventions and retain objects’ authenticity. Although the primary goal of this SOA is the implementation of migration based preservation interventions, it can also be used for other purposes such as comparing file formats or evaluating the performance of conversion applications.	intelligent decision support system;interpreter (computing);service-oriented architecture;web service	Miguel Ferreira;Ana Alice Baptista;José Carlos Ramalho	2007	International Journal on Digital Libraries	10.1007/s00799-007-0013-x	web service;intelligent decision support system;computer science;database;multimedia;world wide web	HPC	-47.766343383664584	9.987022334415585	127182
9d38f16a10730c1c06aea149c0b3363fc92cb279	murgroom: multi-site requirement reuse through graph and ontology matching	integration;requirements engineering;matching system;ontology	In the era of globalization, it is strongly recommended for a company when developing a new project to identify companies (at local and international levels) that have already developed similar projects to reuse their experience and reproduce their findings. It should be noticed that the reuse is an eternal grail to increase projects productivity. It may concern all aspects of the life cycle of project development that ranges from user requirements to codes. In this study, we focus on user requirements. In such a context, each company (site) has already expressed its own requirements using different modeling formalisms, conceptualizations and vocabularies and scheduling of tasks. This situation makes the reuse and exploitation of the requirements harder, since the heterogeneity is everywhere. To reduce this heterogeneity, the most important studies assume the existence of pivot models and shared ontologies that allow reducing heterogeneities. Unfortunately, these hypotheses are out of phase with the reality, in which the sites are autonomous and usually multilingual. In this paper, we propose to relax these hypotheses. Firstly, we propose a framework for requirements integration using an ontology-based matching system that identifies and unifies different heterogeneous items. Secondly, a reasoning mechanism is proposed to deduce relationships between integrated requirements. Finally, our proposed framework is evaluated w.r.t its feasibility, results quality and effectiveness. A tool, called MURGROOM implementing the framework is also proposed.	autonomous robot;code;new product development;ontology (information science);ontology alignment;pivot table;requirement;scheduling (computing);user requirements document;vocabulary	Zouhir Djilani;Abderrahmane Khiat;Selma Khouri;Ladjel Bellatreche	2016		10.1145/3011141.3011183	requirements management;computer science;systems engineering;knowledge management;data mining	SE	-43.97489003942192	8.371745701186075	127259
18a57ce51dd179ce63e3e769418252a475336e9c	cleanonto: evaluating taxonomic relationships in ontologies		Consistent ontologies are vital for the growth of the Semantic Web. We describe and appraise the OntoClean methodology and the different implementations available to evaluate taxonomic relationships in ontologies. We propose a new system, CleanONTO, which uses definitions to describe each concept, where definitions are paths from the concept to the root node of the ontology. In the current study, these definitions (paths) have been extracted from WordNet.	ontology (information science);semantic web;tree (data structure);wordnet	Derek H. Sleeman;Quentin Reul	2006			computer science;data mining;database;information retrieval	Web+IR	-39.563157186182984	5.297524437370475	127532
2712ebdefc9c98b7392a9fa9f9af46fd497cf51c	self-serv: a platform for rapid composition of web services in a peer-to-peer environment	rapid composition;peer-to-peer environment;web service;integrated services;profitability	The automation of Web services interoperation is gaining a considerable momentum as a paradigm for effective Business-to-Business collaboration [2]. Established enterprises are continuously discovering new opportunities to form alliances with other enterprises, by offering value-added integrated services. However, the technology to compose Web services in appropriate time-frames has not kept pace with the rapid growth and volatility of available opportunities. Indeed, the development of integrated Web services is often ad-hoc and requires a considerable effort of lowlevel programming. This approach is inadequate given the size and the volatility of the Web. Furthermore, the number of services to be integrated may be large, so that approaches where the development of an integrated service requires the understanding of each of the underlying services are inappropriate. In addition, Web services may need to be composed as part of a short term partnership, and disbanded when the partnership is no longer profitable. Hence, the integration of a large number of Web services requires scalable and flexible techniques, such as those based on declarative languages. Also, the execution of an integrated service in existing approaches is usually centralised, whereas the underlying services are distributed and autonomous. This calls for the investigation of distributed execution paradigms (e.g., peer-to-peer models), that do not suffer of the scalability and availability problems of centralised coordination [3]. Motivated by these concerns, we have developed the SELF-SERV platform for rapid composition of Web	autonomous robot;centralisation;hoc (programming language);integrated services;interoperation;peer-to-peer;programming paradigm;scalability;volatility;web service;world wide web	Quan Z. Sheng;Boualem Benatallah;Marlon Dumas;Eileen Oi-Yan Mak	2002			web modeling;simulation;business process execution language;engineering;operations management;ws-policy;service-oriented architecture;ws-addressing;services computing;world wide web	DB	-47.508052176998845	13.31045504433811	127607
f79252b5b65b012d999b1b966cf610f97c91fca4	towards a conceptual framework for semantic interoperability in service oriented architectures	ontology mapping;semantic mediation;service oriented architectures;semantic interoperability;conceptual framework;semantic web technology;service oriented architecture;domain specificity;semantic web technologies	The application of Semantic Web technologies to service-oriented architectures (SOA) has promised to mitigate the problem of achieving semantic interoperability as the formal definition of semantics paves the way for higher automation in the mediation process between heterogeneous services. Recently, many ontology-based approaches for semantic interoperability have been developed. However, it remains difficult to compare the various approaches because only domain-specific conceptual frameworks for semantic interoperability exist. Moreover, in SOA practice ontology-based approaches are not widely adopted but still XML-based solutions are dominant. This paper targets to fill this gap and presents ongoing work towards a general conceptual framework for semantic interoperability in SOA as a foundation for comparative reflection. Based on the framework selected approaches both academic and industry-driven are compared. Furthermore, an inherent trade-off between efficiency and effectiveness in achieving semantic interoperability is identified and a potential alleviation based on semantic mediation on domain level is outlined.	semantic web;semantic interoperability;service-oriented architecture;service-oriented device architecture;xml	Nils Barnickel;Matthias Flügge	2010		10.1145/1874590.1874602	semantic data model;upper ontology;semantic interoperability;semantic computing;semantic integration;semantic search;semantic grid;computer science;knowledge management;social semantic web;semantic web stack;database;semantic technology;cross-domain interoperability;world wide web;semantic analytics	Web+IR	-43.828743344677086	8.491038750208158	127643
2e32e03c9560d0e02f06c81160b03103ebad82eb	creation of semantic overlay networks based on personal information	p2p system;p2p;user profile;semantic information;son;web 2 0;semantic web;semantic overlay network	In P2P systems, nodes typically connect to a small set of random peers to query them, and they propagate those queries along their own connections. To improve that mechanism, Semantic Overlay Networks influence those connections depending on the content of the peers, clustering peers in overlapped groups (Semantic Overlay Networks). Ontologies are used for describing semantic information of shared items and user profile. Once the peers are grouped by their semantic information, we can take advantage of that distribution to add some new functionalities as recommendation. In this paper we focus on the description and evaluation of SONAtA, a SON classifier algorithm to globally organize peers into semantic groups, executed locally in each peer.	overlay network	Alberto García-Sola;Juan A. Botía Blaya	2009		10.1007/978-3-642-02481-8_11	semantic interoperability;semantic similarity;semantic computing;semantic integration;semantic search;semantic grid;computer science;semantic web;social semantic web;peer-to-peer;semantic web stack;semantic compression;database;semantic technology;web 2.0;world wide web;information retrieval;semantic analytics	NLP	-41.36985541632407	9.233265842432816	127668
3cb75f63dc0d8fd1c84106a34d235509bb06689d	tagnet: using soft semantics to search the web		Semantic annotations are key to efficiently retrieve resources on the Web. On the one hand, ontologies underlying Web resources give rise to linked data. On the other hand, tagging has become increasingly popular to bring order in data across Web applications and social networks. While taxonomies and folksonomies serve the same purpose (i.e. classification), there is a large gap in semantics between uncontrolled keywords used for tagging and hierarchical concepts found in a taxonomy. In this paper we introduce sematags as light-weight ‘soft semantics’ which aim to bridge the gap between ‘no semantics’ and ‘hard semantics’. Sematags define aliases (synonyms) and isas (hypernyms) which overcome the typical issues conventional tags cope with such as ambiguity. Furthermore, we present TagNet, a framework that extracts sematags from lexicons and existing knowledge bases and exploits them to annotate, link and retrieve resources on the Web. We evaluate how soft semantics can be used to semi-automatically map tagged photos in Flickr on DBpedia concepts and vice versa.	align (company);dbpedia;faceted classification;flickr;folksonomy;hard link;lexicon;linked data;lookup table;loose coupling;microsoft outlook for mac;ontology (information science);semiconductor industry;sensitivity and specificity;social network;software release life cycle;tag (metadata);taxonomy (general);uncontrolled format string;vocabulary;web application;web resource;web search engine;web search query;wordnet;world wide web;dialog	Geert Vanderhulst;Lieven Trappeniers	2013			semantic search;web search query	Web+IR	-37.200978172658125	6.020133282629524	127761
3e8eed6aa275078e126e5d601c42d478f1644f10	an architectural style decoupling coordination, computation and data	distributed application;software metrics;computer architecture communication system control protocols control systems telecommunication control distributed computing telecommunication computing concurrent computing application software software architecture;software engineering;software metrics software engineering;telecommunications application architectural style decoupling coordination hard to detect errors token architectures;architectural style	Triple Space Computing is a new communication and coordination paradigm for Semantic Web Services. It has been achieved by extending Tuple Space Computing to support RDF as Triple Space Computing and then use it for communication in Semantic Web Services. Similarly we want to use the emerging Triple Space Computing to enhance the communication of computation and information services of the Semantic Grid. This paper briefly describes how Triple Space Computing can be used for Semantic Web technologies, Web Services, Semantic Web Services and eventually the Semantic Grid. 1. Semantic Web Services Web Services [1] are currently a widely used technology for applications and businesses integration. Their success is due to a shared view on how functionality and behavior should be exposed and invoked by different parties. Applications and resources can be provided as services moving the information society towards a service oriented world. The building blocks of Web Service technologies are protocols like WSDL and SOAP. Using such protocols, the automatization of different task in eWork and eCommerce can be achieved, but only to a very limited extent. A strong human support is needed in order to perform successful service related tasks like discovery, selection, composition, mediation, execution and monitoring. Semantic Web Services are a step forward in the realization of intelligent infrastructure that will enable previous mentioned tasks on services to be realized in a more and more human independent manner. Two technologies are used in realizing Semantic Web Services i.e. Semantic Web and Web Services. The combination of these two allows services to be described in a machine understandable way. The Web Service Modeling Ontology WSMO is one initiative that provides a conceptual model for the description of various aspects of services towards such Semantic Web Services. Four basic elements are defined by WSMO: • Ontologies Ontologies [5] define a common agreed terminology by providing concepts and relationships among concepts. They are then further used in modeling all the other elements involved in Semantic Web Services descriptions. • Goals Goals are key modeling elements that capture the requester perspective and his/her requested functionality. • Web Service descriptions Web Service descriptions contains the specification of the service functionality, behavior and non-functional characteristics. • Mediators Mediators connect different WSMO elements and resolve heterogeneity in data representation, protocols and business processes. By using WSMO semantic descriptions tasks like: discovery, selection, composition, mediation, execution, monitoring on Web Services could be realized in a semantic execution environment. The Web Services Execution Environment (WSMX) [3] is such an execution environment providing the reference implementation for WSMO. WSMX functionality could be summarized as performing discovery, mediation, selection and invocation of Web Services on receiving a user goal specified in WSML [4], the underlying formal language of WSMO. 2. Triple Space Computing Tuple Space Computing is a communication paradigm in which communication is carried out by reading and writing the information to be exchanged in a shared space. Its semantic extension was proposed in [6] and is called as Triple Space Computing in which RDF [2] triples are used to read and write information than that of simple messages. RDF provides the natural link from the Tuple Space Computing paradigm into the Semantic Web. It provides a richer data model than interlinked triples that could also be used to model and retrieve information. The Triple Space shall offer an infrastructure that scales conceptually on an Internet level. Like Web servers publish Web pages for humans to read, Triple Space servers would provide Triple Spaces to publish machine-interpretable data. Providers and consumers could publish and consume triples over a globally accessible infrastructure, i.e., the Internet. Various Triple Space servers could be located at different machines all over the globe and hence every partner in a communication process can target its preferred space, as it is the case for Web. This highlights many advantages for providers and consumers. The providers of data can publish it at any point in time (time autonomy), independent of its internal storage (location autonomy), independent of the knowledge about potential readers (reference autonomy), and independent of its internal data schema (schema autonomy). 3. Triple Space Computing for State-of-the-art technologies As discussed above that Triple Space Computing can be useful in enhancement of semantic enabled communication and coordination. In this section we highlight its use in related technologies i.e. Semantic Web, Web Services, Semantic Web Services and eventually can show its applicability in Semantic Grid. 3.1 For Semantic Web i.e. RDF We are analyzing the existing technologies developed for Semantic Web, particularly concerning RDF in order to bring semantics to the Tuple Space Computing. The extension of Tuple Space Computing to support RDF will form a basis for Triple Space Computing. Also, by bringing semantics into the Triple Space we will overcome the main obstacles of the Tuple Space paradigm in heterogeneous environments making it a basis for a Semantic Space which enables the key concept of strong de-coupling and allows including strong mediation services on top of the Triple Space. 3.2 For Web Services and Semantic Web Services The Triple Space Computing can be used as communication paradigm for Web Services. It provides asynchronous invocation support and hence improves the communication as shown in Figure 1: Figure 1: Asynchronous communication support for Web Services through Triple Space Computing middleware The Triple Space Computing middleware acts as third party among the Web Service clients and Web Services. When a Web Service client sends a request to some Web Service, it should publish data in the Triple Space Computing middleware. Similarly, Web Service can receive the invocation request from client by reading data from the Triple Space middleware. Same applies to semantically described Web Services. We started to investigate that how the Triple Space Computing could be applied to our implementation for Semantic Web Services i.e., The Web Service Modeling Execution Environment. Using Triple Spaces for asynchronous communication between different WSMXs enables and brings them a step closer to their architectural goal i.e., to support greater modularization, flexibility and decoupling in communication of different WSMX nodes. Similarly, it enables WSMX to be highly distributed and easily accessible. Furthermore, being a third party element Triple Space Computing can resolve communication disputes that may arise. 3.3 For Open Grid Service Architecture (OGSA) The OGSA [7] framework provides the conceptual model for Grids. It takes a ServiceOriented approach and defines a set of services that are vital for a Grid environment. It is a major building block for Grid environments and its role will be significant in the realization of Semantic Grid as well. As shown in the figure below, we envision the empowering and extension of OGSA in two directions by using: i. WSMO descriptions for OGSA services ii. Triple Space Computing as a communication and coordination paradigm for OGSA services Figure 2: Web Services communication through Triple Space in OGSA Some initial thoughts on how to employ WSMO to semantically extend OGSA were presented in our previous work [9]. The second extension and improvement of OGSA that we envision is the use of Triple Space Computing as a publication based approach for OGSA services communication and coordination. Using a Triple Space approach features like time, data schemas, location, reference autonomy would be automatically available for communication between OGSA services as well. 3.4 For Semantic Grid Semantic Grid envisions semantics in information and services of existing Grid [10]. The Semantic Grid core services have been proposed in [8]. The Triple Space Computing communication paradigm can also be used in context of Semantic Grid as shown in Figure 3. Figure 3: Triple Space Computing for Semantic Grid Architecture As a first step it would bring asynchronous communication support for Web Services Resource Framework (WSRF) which acts as basis for Semantic Grid architecture. Secondly, it can also be used for enhancement of semantic enabled communication by exchanging RDF objects among knowledge-based data, computation and information services in the Semantic Grid architecture.	autonomy;business process;computation;coupling (computer programming);data (computing);data model;formal language;grid computing;internet;middleware;ontology (information science);open grid services architecture;programming paradigm;reference implementation;resource description framework;soap;semantic web service;semantic grid;spaces;telecommuting;tuple space;wsmo;web services description language;web services resource framework;web page;world wide web	Anssi Karhinen;Juha Kuusela;Tapio Tallgren	1997		10.1109/ICECCS.1997.622297	embedded system;real-time computing;architectural pattern;computer science;component-based software engineering;operating system;software engineering;distributed computing;programming language;software metric;software system	Web+IR	-44.36644327989497	17.154047241208808	127796
b2045ad6869524c0a010221c7e14507941b1def7	object-oriented wrapper for relational databases in the data grid architecture	modelizacion;distributed system;base relacional dato;front end;query language;interfase usuario;modele entreprise;haute performance;systeme reparti;red www;empaqueteur;user interface;grid applications;sql;interrogation base donnee;reseau web;distributed computing;interrogacion base datos;semantics;query optimization;relational database;modelo empresa;semantica;semantique;lenguaje interrogacion;envolvero;grid;modelisation;business model;virtual view;sistema repartido;internet;vue virtuelle;object oriented;rejilla;relational model;base donnee orientee objet;base donnee relationnelle;alto rendimiento;grille;calculo repartido;oriente objet;world wide web;interface utilisateur;modele donnee;langage interrogation;object oriented databases;vista virtual;modeling;orientado objeto;high performance;calcul reparti;database query;data grid;data models;object model;wrapper	The paper presents a solution of the problem of wrapping relational databases to an object-oriented business model in the data grid architecture. The main problem with this kind of wrappers is how to utilize the native SQL query optimizer, which in majority of RDBMS is transparent for the users. In our solution we use the stack-based approach to query languages, its query language SBQL, updateable object-oriented virtual views and the query modification technique. The architecture rewrites the front-end OO query to a semantically equivalent back-end query addressing the M0 object model that is 1:1 compatible with the relational model. Then, in the resulting SBQL query the wrapper looks for patterns that correspond to optimizable SQL queries. Such patterns are then substituted by dynamic SQL execute immediately statements. The method is illustrated by a sufficiently sophisticated example. The method is currently being implemented within the prototype OO server ODRA devoted to Web and grid applications.	1:1 pixel mapping;accessibility;foreign key;grid computing;mathematical optimization;model transformation;object query language;odra (computer);prototype;query language;query optimization;relational database;relational model;sql;select (sql);server (computing);stack-oriented programming language;wrapping (graphics)	Kamil Kuliberda;Jacek Wislicki;Radoslaw Adamus;Kazimierz Subieta	2005		10.1007/11575863_56	business model;data modeling;sargable;query optimization;sql;query expansion;web query classification;the internet;relational model;systems modeling;boolean conjunctive query;object model;relational database;computer science;query by example;front and back ends;operating system;data grid;data mining;database;semantics;rdf query language;language integrated query;programming language;object-oriented programming;user interface;web search query;grid;view;world wide web;query language;object query language;spatial query	DB	-35.70432572085295	11.458361315808089	127797
41aeebd305ff37ec554e0d1e3584ed1738b66b63	extension of the m-gov ontology mapping framework for increased traceability		This paper describes an extension to the M-Gov framework that captures queryable metadata about matcher tools that have been utilized, the users involved, and the discussions of the users, during the generation of alignments. This increases the traceability in an alignment creation process and enables an evaluator to more deeply interpret and evaluate an alignment, e.g. for reuse or maintenance. This requires precise information about the alignments being encoded and the decisions undertaken during their creation. This information is not captured by state of the art approaches in a queryable format. The paper also describes an experiment that was undertaken to examine the effectiveness of our approach in enabling the traceability in the alignment creation process. In the experiment, stakeholders created an alignment between two different datasets. The results indicate that the users were 93% accurate while creating the alignment. The major traceability achievements demonstrated for the test groups were 1) level of participation of various users of a group during alignment creation; 2) most discussed correspondences by users of a group; and 3) accuracy of a group in creating alignment.	conversation threading;dbpedia;e-government;emoticon;interpreter (computing);semantic integration;traceability	Anuj Singh;Christophe Debruyne;Rob Brennan;Alan Meehan;Declan O'Sullivan	2017			traceability;semantic integration;data mining;computer science	HCI	-45.832249712745146	5.546579786044604	127992
287239bf24c060b0f0b92f4ebc24ef1ce31b7a22	saga: a web services architecture for groupware applications	user mobility;modelizacion;distributed system;groupware;business to business;movilidad;systeme reparti;interoperabilite;interoperabilidad;competitividad;mobility;service web;mobilite;web service;orientado servicio;work team;equipe travail;modelisation;notices;internet technology;sistema repartido;internet;distributed environment;team work;web services;notification;competitiveness;equipo trabajo;travail equipe;trabajo equipo;cscw;oriente service;interoperability;competitivite;collecticiel;modeling;empresa hacia empresa b2b;servicio web;service oriented;entreprise a entreprise	To improve their efficiency and competitiveness, organizations are increasingly interested in applications that support team work, usually know as groupware. Beside interoperability, familiarity with the application and users' mobility support, a feature that is of outmost importance in groupware is the notification of events produced by cooperative activities. Web Services have emerged recently to support the exchange of data in distributed environments using common Internet technologies and have been used mainly to build business-to-business applications. However, Web Services have capabilities that make them suitable to meet the requirements posed by groupware applications, a field where little work has been carried out. This article describes a model for developing cooperative applications based on Web Services technology and using asynchronous notification of events, and presents a brief description of the implementation of the support services for that model and of a prototype application that uses them.	collaborative software;web service	Benjamim Fonseca;Eurico Carrapatoso	2006		10.1007/11853862_20	web service;simulation;computer science;operating system;computer-supported cooperative work;database;ws-i basic profile;mobile computing;world wide web;computer security	HPC	-39.20784263350431	15.659256004399442	128013
7d559fc639387dcdd736a88e9efc465d71d8f46f	peer data management system		MAIN TEXT A Peer Data Management System (PDMS) is a distributed data integration system providing transparent access to heterogeneous databases without resorting to a centralized logical schema. Instead of imposing a uniform query interface over a mediated schema, PDMSs let the peers define their own schemas and allow for the reformulation of queries through mappings relating pairs of schemas (see Figure 1). PDMSs typically exploit the schema mappings transitively in order to retrieve results from the entire network.	centralized computing;database;logical data model;management system;pdms	Philippe Cudré-Mauroux	2009		10.1007/978-0-387-39940-9_1233	technical peer review;data management	DB	-36.755425276448086	6.009111081206432	128093
0066314b359c2cfa7ed1939e9fffa56ab6e80280	cloud services representation using sla composition	articulo;sla service level agreements;sla;dynamic composition;cloud services;cloud computing	SLA-aware Cloud platforms need mechanisms to represent, store and retrieve Cloud services. Usually services changes between different platforms, custom models are built to capture this information and ad-hoc implementations used to store and retrieve it. This paper propose a generic methodology for the representation of Cloud services. This methodology uses the WS-Agreement specification for capturing and manipulation arbitrary services using SLA fragments. SLA fragments are composed on the fly in response to user request. A SLA composition algorithm enables a prototype implementation of the methodology in a SLA-aware Cloud platform. This methodology provides the genericity, extensibility and flexibility to unify the modeling of Cloud services. Finally a use case provides a quantitative measure of the utility provided by the methodology from a Cloud user and Cloud provider point of view.	algorithm;cloud computing;extensibility;generic programming;graph coloring;hoc (programming language);on the fly;platform as a service;point of view (computer hardware company);prototype;seamless3d;service-level agreement;virtual reality	Andrés García-García;Ignacio Blanquer	2014	Journal of Grid Computing	10.1007/s10723-014-9295-6	real-time computing;cloud computing;computer science;operating system;database;services computing;world wide web	HPC	-41.983129779661816	13.071022623164259	128097
852b2c2a69c59765b171193fc41f77e06c1ee37a	extracting semantic annotations from moodle data	association rule	The purpose of this paper is to provide a solution which allows automatic reasoning processes over Moodle activities logs, in order to obtain user-personalized recommendations. Activities logs are mined for association rules, which are the translated into Jena Rules. The information is then used by specific learning rules to create recommendations for specific users. Using this technique, additional information is obtained starting from activities database.	association rule learning;automated reasoning;database;logic programming;mined;personalization	Mihai Gabroveanu;Ion-Mircea Diaconescu	2008			data mining;association rule learning;information retrieval;active database;computer science	DB	-39.81498772570018	7.912056508574911	128163
b029c0354ae99f03bc4ca3dbc5b52414434382ed	an adaptive object-oriented approach to integration and access of heterogeneous information sources	database system;information sources;object oriented data model;distributed data management;heterogeneous information;dynamic environment;large scale;intelligent integration of heterogeneous information sources;object oriented approach;interoperability;query mediation	A large-scale interoperable database system operating in a dynamic environment should provide Uniform access to heterogeneous information sources, Scalability to the growing number of information sources, Evolution and Composability of software and information sources,and Autonomy of participants,both information consumers and information producers. We refer to these set of properties as the USECA properties [29]. To address the research issues presented by such systems in a systematicmanner, we introduce the Distributed Interoperable Object Model (DIOM). DIOM promotes an adaptive approach to interoperation via intelligent mediation [46, 47], aimedat enhancing the robustness and scalability of theservices provided for integrating and accessing heterogeneous information sources. DIOM‘s main features include (1) the recursive construction and organizationof information access through a network of application-specificmediators, (2)the explicit use of interface composition meta operations(such as specialization, generalization, aggregation, import and hide) to support the incremental design and construction of consumer‘s domain query model, (3) the deferment of semantic heterogeneity resolution to the query result assembly time instead of before or at the time of query formulation, and (4) the systematic development of the query mediation framework and the procedure of each query processing step from query routing, query decomposition, parallel access planning, query translation to query result assembly.To make DIOM concrete, we outline the DIOM-based information mediation architecture, which includes important auxiliary services such as domain-specific metadatalibrary and catalog functions, object linking databases, and associatedquery services. Several practical examples and application scenarios illustrate the flavor of DIOM query mediation framework and the usefulness of DIOM in multi-database query processing.	assembly language;autonomy;composability;continuous design;database;information access;interoperability;interoperation;partial template specialization;recursion;routing;scalability;semantic heterogeneity	Ling Liu;Calton Pu	1997	Distributed and Parallel Databases	10.1023/A:1008641408566	interoperability;sargable;query optimization;query expansion;web query classification;computer science;data mining;database;world wide web;query language	DB	-34.567165073262906	8.96604181289223	128279
5af26f8fb1f7dd1a1f102f6b755882994cb520c3	semantic web services	978 3 639 35638 0;owl;owl s;art;school and learning;councellor;technology;social sciences;natural;soa;travel;9783639356380;law;sawsdl;semantic web services;3639356381;computer sciences;humanities;medical;web services;semantic web;fiction;economics;service discovery;other;children and youth books;music;ontology;specialized book;wsdl 2 0	The promise of dynamic selection and automatic integration of software components written to Web services standards is yet to be realized. This is partially attributable to the lack of semantics in the current Web service standards. To address this, the Semantic Web community has introduced semantic Web services. By encoding the requirements and capabilities of Web services in an unambiguous and machine-interpretable form, semantics make the automatic discovery, composition and integration of software components possible. This chapter introduces Semantic Web services as a means to achieve this vision. It presents an overview of Semantic Web services, their representation mechanisms, related work and use cases.		Rama Akkiraju	2008		10.1007/978-0-387-78414-4_215	web service;web application security;web development;web modeling;data web;web mapping;web standards;computer science;knowledge management;ws-policy;service-oriented architecture;semantic web;social semantic web;ws-addressing;semantic web stack;multimedia;service discovery;services computing;web intelligence;ws-i basic profile;web 2.0;world wide web;devices profile for web services;universal description discovery and integration	Web+IR	-44.009989518311244	11.410491260565218	128351
77cfad376629ca50097fbb02593531e5a2023bf5	ontology-based approach for semantic description and the discovery of e-learning web services	electronic learning;standards;semantics;web services;semantic web;ontologies	The creation of the e-learning web services are increasingly growing. Therefore, their discovery is a very important challenge. The choice of the e-learning web services depend, generally, on the pedagogic, the financial and the technological constraints. The Learning Quality ontology extends existing ontology such as OWL-S to provide a semantically rich description of these constraints. However, due to the diversity of web services customers, other parameters must be considered during the discovery process, such as their preferences. For this purpose, the user profile takes into account to increase the degree of relevance of discovery results. We also present a modeling scenario to illustrate how our ontology can be used.	artificial intelligence;owl-s;relevance;user profile;web service	Kahina Rabahallah;Faiçal Azouaou;Mohamed Tayeb Laskri	2016	2016 International Conference on Intelligent Networking and Collaborative Systems (INCoS)	10.1109/INCoS.2016.23	web service;web development;web modeling;data web;web mapping;bibliographic ontology;web standards;computer science;knowledge management;ontology;artificial intelligence;ws-policy;semantic web;social semantic web;data mining;semantic web stack;database;semantics;services computing;web intelligence;ws-i basic profile;web 2.0;world wide web;owl-s;universal description discovery and integration	AI	-44.09783933385261	11.870028560129454	128428
ebd2ea21723848599328717c83119d9d64b2d68f	xml structure graphic display based on jsmind		In this paper, a novel graphical display method of XML structure, which can avoid complex technical details and grammatical constraints, is proposed to simplify the complexity of designing XML structure. By adjusting js function and parsing custom XML tags structure of the JSmind framework, the method can adapt the upgrade XML tags set and neglect the downward compatibility of the schema definition. The results show that the method can make the page more friendly and operate more easily when XML tags structure is satisfied.	backward compatibility;infographic;parsing;xml	Yan-Song Cui;Xiao-Min Guo	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393246	xml;web page;machine learning;tree (graph theory);programming language;artificial intelligence;parsing;upgrade;java;ac power;computer science;schema (psychology)	DB	-35.528604782928994	6.840808472900407	128448
2e3327298b3ab882c91307625a25d6144bbedb49	generation of owl ontologies from concept maps in shallow domains	lexical database;collaborative environment;concept map;graphical model;knowledge representation;semantic analysis	A proposal is presented for integration between a graphical model, such as conceptual maps, and ontologies codified in the OWL language. Conceptual maps are a flexible form of knowledge representation, very useful in education-related collaborative environments; OWL is a language of knowledge representation oriented to semantic analysis and processing carried out by machines. Integration consists of a set of formal transformation applied to conceptual maps and the semantic analysis of the relations linking concepts. The proposed method is based on a concept sense-disambiguation procedure, also defined by the authors, and in the WordNet lexical database. It applies to conceptual maps of shallow domains with labels in the Spanish language.	concept map;conceptual schema;directed graph;graphical model;http 404;knowledge representation and reasoning;lexical database;machine learning;ontology (information science);semantic analysis (compilers);structured text;word-sense disambiguation;wordnet	Alfredo Simón-Cuevas;Luigi Ceccaroni;Alejandro Rosete	2007		10.1007/978-3-540-75271-4_27	natural language processing;computer science;data mining;information retrieval	AI	-41.780606235867715	5.392995443438229	128684
7d3ea38eb4f0bb2a428e8f05e675d3e8557257ad	service customization supporting an adaptive information system	engine performance;web service discovery;user preferences;web service;user profile;semantic description;information system;semantic matching;business process	  This work approaches the problem of discovering atomic web services that will realize complex business processes in an adaptive  information system. It is proposed a model for semantic description of web services and user profile and the design of a semantic  recommender engine based on this model. The recommender engine performs, during the web service discovery phase, a ”similarity  evaluation” step in which it can be possible to estimate the similarity between what the service offers and what the user  prefers. A semantic algorithm, that measures distance between concepts in an ontology, is used to rank the results of the  semantic matching between the user profile and a list of web services, suggesting to the user the most suitable services.    	information system	Antonio Caforio;Angelo Corallo;Gianluca Elia;Gianluca Solazzo	2004		10.1007/978-3-540-30134-9_46	web service;data web;differentiated service;computer science;service delivery framework;semantic web;social semantic web;semantic web stack;database;world wide web;information retrieval	Robotics	-44.47381244434346	13.580941672491107	128978
ae9366bbf23accf8a06fbf059c6710649881e07f	a novel approach for service capabilities representation based on statistical study on wsdl	statistical study;semantic web service;google;information systems;semantic web services services capabilities statistical study;xmethods net;wsdl;web and internet services;web service;runtime;semantic web services;large scale;software architecture;statistical analysis;xml semantic web software architecture statistical analysis web services;legacy systems;semantic web services description service capabilities representation wsdl service oriented architecture legacy systems service identification semantic description xmethods net amazon google;web services;semantic description;web services service oriented architecture semantic web buildings large scale systems web and internet services information systems automation open systems runtime;service identification;xml;semantic web;information system;service oriented architecture;services capabilities;open systems;legacy system;service capabilities representation;amazon;buildings;large scale systems;semantic web services description;automation	Web services (WS) are becoming more and more popular nowadays and service-oriented architecture (SOA) have been widely used in the construction of information systems. But most of the SOA applications are not brand new and usually evolved from legacy systems. Our research group is building a service identification framework used for the SOA reengineering of existing large-scale information applications. Service capabilities have become an important issue we care about which are the actions performed or the information delivered by a service. But the semantic description for service capabilities has not been fully addressed in the current approach. In this paper, we analyze the contributions and limitations of current approach. In order to find the practical features of existing web services, we conducted a statistical study on more than four hundred WSDL documents collected from XMethods.net, Amazon and Google. Then we propose a new approach for Semantic Web Services description. Service capabilities are represented by informative entities and standard actions are specified for each informative entity.	angular defect;code refactoring;cohesion (computer science);create, read, update and delete;entity;entropy (information theory);information system;legacy system;requirement;semantic web service;service-oriented architecture;service-oriented device architecture;web services description language	Yan Liu;Mingguang Zhuang;Qingling Wang;Hanli Wang	2008	2008 Third International Conference on Internet and Web Applications and Services	10.1109/ICIW.2008.96	web service;computer science;data mining;database;law;world wide web;legacy system;information system	SE	-44.995363702781745	12.39454525324881	129019
85cc73ede853e8f7d9c1c7371c4b435a80123af3	sparql2git: transparent sparql and linked data api curation via git		In this demo, we show how an effective and application agnostic way of curating SPARQL queries can be achieved by leveraging Git-based architectures. Often, SPARQL queries are hard-coded into Linked Data consuming applications. This tight coupling poses issues in code maintainability, since these queries are prone to change to adapt to new situations; and query reuse, since queries that might be useful in other applications remain inaccessible. In order to enable decoupling, version control, availability and accessibility of SPARQL queries, we propose SPARQL2Git, an interface for editing, curating and storing SPARQL queries that uses cloud based Git repositories (such as GitHub) as a backend. We describe the query editing and management capabilities of SPARQL2Git, its convenience for SPARQL users that lack Git knowledge, and its combination with grlc to easily generate Linked Data APIs.	accessibility;application programming interface;coupling (computer programming);digital curation;hard coding;linked data;sparql;version control	Albert Meroño-Peñuela;Rinke Hoekstra	2017		10.1007/978-3-319-70407-4_27	data mining;maintainability;information retrieval;coupling (computer programming);sparql;linked data;reuse;cloud computing;computer science;database	Web+IR	-40.22854561374978	10.121630114008129	129059
2c6ef6b7d502b82923483cbbcbbab1574bd4e6ff	applying qos-aware service selection on functionally diverse services	current selection algorithm;functionally diverse service;certain task;complex application;invalid genomes;functionality clustering;service-oriented computing;functional invalid configuration;heuristic algorithm;related heuristic algorithm;qos-aware service selection	current selection algorithm;functionally diverse service;certain task;complex application;invalid genomes;functionality clustering;service-oriented computing;functional invalid configuration;heuristic algorithm;related heuristic algorithm;qos-aware service selection	quality of service	Florian Wagner;Fuyuki Ishikawa;Shinichi Honiden	2011		10.1007/978-3-642-31875-7_12	simulation;computer science;operating system;machine learning;data mining;database	ECom	-45.934448706126744	15.19391065808701	129187
d37689983eb6382a865b85a135cb0eab3d6b19db	visualizing ontology mappings to help ontology engineers identify relevant ontologies for their reuse		The importance of ontologies in biomedicine is increasing in the areas such as the standardization of terminology, the verification of data consistency, and the integration of heterogeneous biomedical databases. New ontologies are being built and added to repositories such as BioPortal. These ontologies represent a large network of biomedical concepts where a single ontology connects a group of closely related concepts. When ontology engineers build new ontologies they often search for existing ontologies to avoid redundancy of concepts. When selecting existing ontologies, engineers consider different factors such as ontology domain, the size of the ontology, and also the relations between ontologies and their concepts. In this paper we present a graph that aims to visualize mappings of all BioPortal ontologies. We believe that this graph can help ontology engineers in deciding which ontology to use when selecting existing concepts for building new ontologies.	database;ontology (information science)	Simon Kocbek;Jin-Dong Kim;Jean-Luc Perret;Patricia L. Whetzel	2013			open biomedical ontologies;process ontology;ontology (information science);ontology-based data integration;data mining;ontology components;upper ontology;suggested upper merged ontology;idef5;computer science	AI	-43.762712056445714	6.085954137690189	129321
221f0e812325bb48695963b9d10164b450761410	a context quality model to support transparent reasoning with uncertain context	context aware;uncertain reasoning;pervasive system;context quality;quality measures;quality model;context aware systems;pervasive systems	Much research on context quality in context-aware systems divides into two strands: (1) the qualitative identi cation of quality measures and (2) the use of uncertain reasoning techniques. In this paper, we combine these two strands, exploring the problem of how to identify and propagate quality through the di erent context layers in order to support the context reasoning process. We present a generalised, structured context quality model that supports aggregation of quality from sensor up to situation level. Our model supports reasoning processes that explicitly aggregate context quality, by enabling the identi cation and quanti cation of appropriate quality parameters. We demonstrate the e cacy of our model using an experimental sensor data set, gaining a signi cant improvement in situation recognition for our voting based reasoning algorithm.	aggregate data;algorithm;context-aware pervasive systems;naruto shippuden: clash of ninja revolution 3;sensor;unified modeling language	Susan McKeever;Juan Ye;Lorcan Coyle;Simon A. Dobson	2009		10.1007/978-3-642-04559-2_6	computer science;knowledge management;data mining;database;context model	AI	-42.66755936709696	16.12727027558235	129426
bb34f1b6f93d29b8f9e8f3a3812212231cadc35f	complex data integration based on a multi-agent system	extraction information;base relacional dato;multidisciplinaire;multiagent system;text;oferta;multi agent system;data integrity;red www;offer;information extraction;image databank;integration information;teoria sistema;reseau web;heterogeneous data;service process;texte;relational database;proceso servicio;information integration;appel par service;complex data;internet;senal video;signal video;processus service;systems theory;theorie systeme;almacenamiento;enveloppeur;banco imagen;entreposage;warehousing;banque image;integracion informacion;base donnee relationnelle;video signal;world wide web;multidisciplinary;multidisciplinar;sistema multiagente;texto;offre;extraccion informacion;call by service;systeme multiagent;wrapper	The expansion of the WWW and the growth of data sources lead to the proliferation of heterogeneous data (texts, images, videos, sounds and relational views). We call these data ”complex data”. In order to explore them, we need to carry out their integration into a unified format. Collecting, structuring and storing constitute the different tasks of complex data integration. There exists many approaches for data integration like mediated schemes and wrappers, or warehousing. In this paper, we propose a new approach for complex data integration that uses both classical warehousing approach and multi-agents systems (MAS) technology. We consider the different tasks of the data integration process as services offered by actors called agents. To validate this approach, we have implemented a multi-agent system for complex data integration named SMAIDoC. One of the advantage of The MAS technology is that it provides an evolutive structure to our system.	data mining;diff utility;intelligent agent;multi-agent system;online analytical processing;online search;relational database;unified modeling language;www;web search engine;xml	Omar Boussaïd;Fadila Bentayeb;Amandine Duffoux;Frederic Clerc	2003		10.1007/978-3-540-45185-3_19	computer science;data virtualization;data integration;data mining;database;ontology-based data integration;world wide web;enterprise information integration;system integration	AI	-37.07698241496677	12.036217436142975	129468
f6c84e1336c9660de67e634c5e812d36ef231cb1	heuristic methods for searching and clustering hierarchical workflows	emergency service;heuristic method	Workflows are used nowadays in different areas of application. Emergency services are one of these areas where explicitly defined workflows help to increase traceability, control, efficiency, and quality of rescue missions. In this paper, we introduce a generic workflow model for describing fire fighting operations in different scenarios. Based on this model we also describe heuristics for calculating the similarity of workflows which can be used for searching and clustering.	cluster analysis;heuristic (computer science);traceability	Michael Kastner;Mohamed Wagdy Saleh;Stefan Wagner;Michael Affenzeller;Witold Jacak	2009		10.1007/978-3-642-04772-5_95	simulation;computer science;data mining;world wide web	AI	-39.95168576884098	13.92620156429355	129799
e65802ad1a874eecb62b5bfe0ea6cefe3a4ea818	designing a global information resource for molecular biology	molecular biology	Research in molecular biology is continuously prod ucing an immense amount of data, but this information is spr ead over numerous heterogeneous data repositories. Their integration into a federated information system would drastically reduce the time a biologis t has to spend browsing different WWW sites or databases in search for a pa rticular piece of information. In this study we point out the specific problems th at molecular biology is posing to data integration. We present our approach to cope with these problems. It is based on a mediator architecture an d uses query correspondence assertions (QCA) to describe sources in a flexible yet expressive manner. QCAs both capture content and query capabilities of arbi trary data sources with respect to a federated schema. Based on such QCAs a mediator can answer queries against the federated schema by constructin g semantically equivalent combinations of source queries.	federated database system;information system;matchware mediator;qualitative comparative analysis;www;xml schema	Ulf Leser	1999			computational biology;systems biology;mechanical engineering;biology	DB	-38.01914869429965	4.550256319040823	129814
f0e588ecca53662feadf2b18ad967645767464c5	automating government spatial transactions		The land development approval process between local authorities and government land and planning departments is manual, time consuming and resource intensive. For example, when new land subdivisions, new roads and road naming, and administrative boundary changes are requested, approval and changes to spatial datasets are needed. The land developer submits plans, usually on paper, and a number of employees use rules, constraints and policies to determine if such plans are acceptable. This paper presents an approach using Semantic Web and Artificial Intelligence techniques to automate the decision-making process in Australian jurisdictions. Feedback on the proposed plan is communicated to the land developer in real-time, thus reducing process handling time for both developer and the government agency. The Web Ontology Language is used to represent relationships between different entities in the spatial database schema. Rules on geometry, policy, naming conventions, standards and other aspects are obtained from government policy documents and subject-matter experts and described using the Semantic Web Rule Language. Then when the developer submits an application, the software checks the rules against the request for compliance. This paper describes the proposed approach and presents a case study that deals with new road proposals and road name approvals.	aerial photography;artificial intelligence;database schema;entity;map;ontology (information science);point of interest;real-time clock;semantic web rule language;spatial database;subject matter expert turing test;subject-matter expert;the australian;web ontology language;world wide web	Premalatha Varadharajulu;Geoff A. W. West;David A. McMeekin;Simon Moncrieff;Lesley Arnold	2016		10.5220/0005818901570167	management science;engineering drawing;business;government	AI	-45.672201087776386	8.465547380909111	130052
3a2703c6e9c5a67603022b580e14075d701b8b47	problems, solutions, and semantic computing	search engine;semantic computing;problem solving	"""Semantic Computing extends Semantic Web both in breadth and depth. It bridges, and integrates, several computing technologies into a complete and unified theme. This article discusses the essences of Semantic Computing with a description of SemanticServices.Net, a new paradigm that enables """"Problem-driven"""" search that may offer a new story to the Internet."""	semantic computing	Phillip C.-Y. Sheu;C. V. Ramamoorthy	2009	Int. J. Semantic Computing	10.1142/S1793351X09000781	natural language processing;semantic computing;semantic search;semantic grid;computer science;artificial intelligence;theoretical computer science;social semantic web;semantic web stack;semantic compression;database;semantic technology;world wide web;information retrieval;semantic analytics;search engine	HPC	-40.6887593094631	6.078425995140356	130169
17ba5e3e0e287f0e2b2cb89acf81e07e0d70f63f	distributed constraint management for collaborative engineering databases	engineering design;collaborative engineering;database integration;knowledge sharing	The engineering design process, such asairplane or building design, involves the participation of many independent specialists who need to share information stored in several autonomous design databases. To insure consistency of the final design, high-level constraints need to be defined and enforced on the design objects stored in these pre-existing databases. The constraints need to be verified when the participating databases are updated. In this paper, we present an architecture of a distributed constraint management system that checks design constraints on pre-existing autonomous databases. The architecture emphasizes independence of constraint management from the underlying databases systems to avoid compromising site autonomy. To provide this independence, we present a declarative constraint language for specifying constraints, which facilitates compile time optimizations such as constraint fragmentation and local validation strategies. We also support set-oriented updates that model transactions in engineering applications, and notifications that model interaction between designers. Early detection of constraint violation and appropriate notifications to the participants can avoid expensive project delays and cost overruns. A prototype of the proposed architecture is being implemented using the Starburst database system at Permieeion to copy without fee ell or part of thie materiel ie granted provided that the copiee era not med. or distributed for direct oornrrmrciel advantege, the ACM copyright notice and the tide of the publication end ite dote eppaer, and notice ie given thet copying ie by permieeion of the Aeeocintion for Computing Machinery. To copy otherwiee, or to republieh, requires a fee andlor szmcific permission.	autonomous robot;compile time;compiler;database;engineering design process;fragmentation (computing);high- and low-level;prototype	Ashish Gupta;Sanjai Tiwari	1993		10.1145/170088.170448	system of systems engineering;computer science;systems engineering;knowledge management;data integration;applied engineering;database;requirements engineering;engineering design process	DB	-36.96695624733678	16.501477923119346	130210
5b1e4508b81b24e5bb01b807f4716107f1414ce0	conceptual and spatial footprints for complex systems analysis: application to the semantic web	topic maps;galois lattice;area of interest;navigation;visualization;complex system;clustering;semantic web;ontologies;galois lattices;conceptual footprint;formal concept analysis	This paper advocates the use of Formal Concept Analysis and Galois lattices for complex systems analysis. This method provides an overview of a system by indicating its main areas of interest as well as its level of specificity/generality. Moreover, it proposes possible entry points for navigation by identifying the most significant elements of the system. Automatic filtering of outliers is also provided. This methodology is generic and may be used for any type of complex systems. In this paper, it is applied to the Topic Map formalism which can be used in the context of the Semantic Web to describe any kind of data as well as ontologies. The proposed Conceptual and Spatial Footprints allow the comparison of Topic Maps both in terms of content and structure. Significant concepts and relationships can be identified, as well as outliers; this method can be used to compare the underlying ontologies or datasets as illustrated in an experiment.	algorithm;automatic control;cartography;cluster analysis;complex system;complex systems;computation;distributed computing;formal concept analysis;level of detail;michel hénon;ontology (information science);partial template specialization;programming paradigm;scalability;semantic web;semantics (computer science);sensitivity and specificity;topic maps	Bénédicte Le Grand;Michel Soto;Marie-Aude Aufaure	2009		10.1007/978-3-642-03573-9_9	topic maps;navigation;complex systems;visualization;computer science;formal concept analysis;ontology;artificial intelligence;theoretical computer science;semantic web;data mining;database;cluster analysis;world wide web;information retrieval	Web+IR	-43.86311116315998	5.7297406886194	130230
7a12fb792381a0df12b141eb90a971c29736d260	hera presentation generator	web information system;semantic web technology;rdf s;integrated development environment;design environment;data transformation;semantic web;swis;information system;wis;design methodology	Semantic Web Information Systems (SWIS) are Web Information Systems that use Semantic Web technologies. Hera is a model-driven design methodology for SWIS. In Hera, models are represented in RDFS and model instances in RDF. The Hera Presentation Generator (HPG) is an integrated development environment that supports the presentation generation layer of the Hera methodology. The HPG is based on a pipeline of data transformations driven by different Hera models.	consistency model;integrated development environment;model-driven architecture;model-driven engineering;rdf schema;resource description framework;semantic web	Flavius Frasincar;Geert-Jan Houben;Peter Barna	2005		10.1145/1062745.1062814	web modeling;design methods;computer science;semantic web;social semantic web;semantic web stack;database;data transformation;world wide web;information retrieval;information system;statistics	Web+IR	-39.241959748197445	8.611128043548144	130258
683422744d6a8f5e6bac38756f1f51a8ed16e2e3	spider: flexible matching in databases	design principle;metadata management;data management;model driven development;service oriented architecture	We present a prototype system, SPIDER, developed at AT&T Labs-Research, which supports flexible string attribute value matching in large databases. We discuss the design principles on which SPIDER is based, describe the basic techniques encompassed by the tool and provide a description of the demo.	cosine similarity;database;declarative programming;game demo;prototype;sql;semantic similarity;similarity measure;table (database);tf–idf	Nick Koudas;Amit Marathe;Divesh Srivastava	2005		10.1145/1066157.1066272	data management;computer science;service-oriented architecture;data mining;database;world wide web	DB	-35.68896910044923	7.297487956358952	130439
91256c7c99711d6320beb6ff5517a18642161242	a multi-agent personalized ontology profile based user preference profile construction method	user interfaces multi agent systems ontologies artificial intelligence;ontologies artificial intelligence;user preference profile forgetting function personalized ontology profile sliding time window;ontologies feature extraction computers authentication semantics nickel xenon;multi agent systems;pure forgetting strategy method multiagent personalized ontology profile user preference profile construction method user long term behaviors user short term behaviors user browsing behavior sliding time window update strategy time based forgetting function update strategy dynamic user preference profile pure sliding window method;user interfaces	This paper proposes a multi-agent personalized ontology profile based user preference profile construction method to comprehensively track the users' long-term behaviors and short-term behaviors by comprehensively consider the users' browsing behavior, pace times, as well as using two kinds of different strategies-“Sliding-Time-Window Update Strategy” and “Time-Based-Forgetting Function Update Strategy”, so as to construct a dynamic user preference profile based on the personalized ontology profile. There are four agents in our entire system, with all agents communicating among each other, and each agent having its own responsibility. The simulation result shows that our proposed method has a better performance than the pure sliding window method and the pure forgetting strategy method.	multi-agent system;ontology (information science);personalization;simulation;user (computing);verification and validation;web page	Qian Gao;Su-Mei Xi;Young-Im Cho	2013	IEEE ISR 2013	10.1109/ISR.2013.6695695	computer science;artificial intelligence;multi-agent system;data mining;database;user interface;world wide web	AI	-43.854707870271994	15.9032131255648	130561
2e2d12872203db6623e57d3e4d3202215b04a669	the web in ten years: challenges and opportunities for database research.	xml document;semantic web;information infrastructure;search engine;information retrieval;world wide web;machine learning;knowledge base	In order to evolve into a dependable and ubiquitous information infrastructure, the World Wide Web needs comprehensive quality, performance, and availability guarantees for all kinds of E-services including search engines. To improve the search result quality of search engines and to exploit the Web’s potential as a world-wide knowledge base, intensive research efforts are required that center around the role of the XML document standard. By itself XML is merely syntax, but its momentum for providing semantically meaningful annotations and, to some extent, standardizing domain-specific ontologies provides opportunities toward the widely envisioned ”Semantic Web”. At the same time, the ongoing information explosion in terms of volume and diversity require support for relevance ranking and automatic classification in addition to ontology-based searching. This paper discusses promising research directions along these lines, aiming at the integration of and synergies between database, information retrieval, and machine learning techniques. In particular, the paper sketches the XXL search engine for XML data and the BINGO! focused crawler, both developed at the University of the Saarland.	dependability;e-services;focused crawler;information explosion;information retrieval;knowledge base;machine learning;ontology (information science);relevance;semantic web;synergy;web crawler;web search engine;world wide web;xml	Gerhard Weikum	2002			web service;xml base;web mining;web development;web modeling;data web;web mapping;web design;web standards;computer science;semantic web;web navigation;social semantic web;web page;data mining;semantic web stack;web intelligence;web 2.0;world wide web;information extraction;information retrieval;web server	DB	-41.2583572086211	7.300599835108212	130707
d61be1be8b283b25b6da276eade6306efccce054	semantic search exploiting formal concept analysis, rough sets, and wikipedia		This﻿article﻿describes﻿how﻿the﻿traditional﻿web﻿search﻿is﻿essentially﻿based﻿on﻿a﻿combination﻿of﻿textual﻿ keyword﻿searches﻿with﻿an﻿importance﻿ranking﻿of﻿the﻿documents﻿depending﻿on﻿the﻿link﻿structure﻿of﻿the﻿ web.﻿However,﻿one﻿of﻿the﻿dimensions﻿that﻿has﻿not﻿been﻿captured﻿to﻿its﻿full﻿extent﻿is﻿that﻿of﻿semantics.﻿ Currently,﻿combining﻿search﻿and﻿semantics﻿gives﻿birth﻿to﻿the﻿idea﻿of﻿the﻿semantic﻿search.﻿The﻿purpose﻿ of﻿this﻿article﻿is﻿to﻿present﻿some﻿new﻿methods﻿to﻿semantic﻿search﻿to﻿solve﻿some﻿shortcomings﻿of﻿existing﻿ approaches.﻿Concretely,﻿the﻿authors﻿propose﻿two﻿novel﻿methods﻿to﻿semantic﻿search﻿by﻿combining﻿ formal﻿concept﻿analysis,﻿rough﻿set﻿theory,﻿and﻿similarity﻿reasoning.﻿In﻿particular,﻿the﻿authors﻿use﻿ Wikipedia﻿to﻿compute﻿the﻿similarity﻿of﻿concepts﻿(i.e.,﻿keywords).﻿The﻿experimental﻿results﻿show﻿that﻿ the﻿authors’﻿proposals﻿perform﻿better﻿than﻿some﻿of﻿the﻿most﻿representative﻿similarity﻿search﻿methods﻿ and﻿sustain﻿the﻿intuitions﻿with﻿respect﻿to﻿human﻿judgements. KEyWoRdS Formal Concept Analysis, Rough Sets, Semantic Search, Similarity Reasoning, Wikipedia	formal concept analysis;rough set;semantic search;wikipedia	Yuncheng Jiang;Mingxuan Yang	2018	Int. J. Semantic Web Inf. Syst.	10.4018/IJSWIS.2018070105	information retrieval;data mining;semantic search;computer science;rough set;formal concept analysis	AI	-39.12047402327176	6.258734405424487	130755
0a2ad238e0fb7498bb98fb9bccb9aa1217a96acc	an ontological model for representing pragmatic aspects of collaborative problem solving	pragmatics;groupware;speech act theory ontological model pragmatic aspects representation user interactions web based collaborative problem solving semantic aspects knowledge representation pragmatic web perspective semiotics;ontologies artificial intelligence;internet;collaborative systems;user interfaces computational linguistics groupware internet ontologies artificial intelligence problem solving;pragmatics ontologies collaboration problem solving humans computational modeling semantics;computational linguistics;knowledge representation;problem solving pragmatics collaborative systems ontology knowledge representation;user interfaces;ontology;problem solving	User interactions in Web-based collaborative problem solving generate a huge amount of messages. Such messages can be a source of knowledge to solve future problems, as well as be used for analytical purposes, e.g., for understanding the development of a discussion or for evaluating how a system supports different types of conversations. These possibilities depend on a proper classification of the messages regarding semantic and pragmatic aspects. The construction of a knowledge representation model that considers the pragmatic aspects of the messages is still an open challenge. In this work we argue that such a model must include the association of semantics with illocutionary aspects of the users' communication. This paper proposes a model using Semantic Web standards to associate messages to illocutions and ontology terms, as an integrated representation. This model is grounded in the Pragmatic Web perspective including Semiotics and Speech Act Theory. Two case studies illustrate practical issues and implications.	intentionality;interaction;knowledge representation and reasoning;ontology (information science);pragmatic web;problem solving;refinement (computing);semantic web;semiconductor industry;semiotics;user interface;web ontology language;web standards	Rodrigo Bonacin;Júlio Cesar dos Reis;Heiko Horst Hornung;Maria Cecília Calani Baranauskas	2012	2012 IEEE 21st International Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises	10.1109/WETICE.2012.60	natural language processing;the internet;computer science;knowledge management;computational linguistics;ontology;data mining;database;user interface;management;collaborative software;pragmatics;collaboration	AI	-46.327908667369144	4.739257868706048	130859
8a405164f9f1c411a61a39c2c7f8d9fb38ffd758	internet-based coordination environments and document-based applications: a case study	distributed system;user agent;sistema experto;architecture systeme;systeme reparti;red www;reseau ordinateur;teleinformatica;computer network;teleinformatique;software architecture;sistema repartido;internet;agent intelligent;technology integration;intelligent agent;red ordenador;world wide web;arquitectura sistema;reseau www;agente inteligente;systeme expert;system architecture;remote data processing;expert system	In this paper we discuss a solution to the conference management problem, a case study in designing a groupware application distributed over the WWW. The case study requires supporting the coordination of activities of people engaged in reviewing and selecting papers submitted for a scientific conference. We discuss why such an application is interesting and describe how we designed its software architecture. The architecture we suggest implements what we call an active Web, because it includes agents able to use services offered by WWW infrastructures. A special kind of agents are active documents, which are documents that carry both some content and some code able to manipulate such a content. Users, agents, and active documents can interoperate using a set of basic services for communication and synchronization. The active Web implementation we describe here is based on coordination technology integrated with Java.		Davide Rossi;Fabio Vitali	1999		10.1007/3-540-48919-3_19	embedded system;user agent;software architecture;the internet;simulation;computer science;artificial intelligence;operating system;world wide web;expert system;intelligent agent	HCI	-39.08790524923326	17.108647263283128	131267
0ff8ca1d4beb959df17815186cfc984ab3ab6032	optimization of multi-domain queries on the web	multi domain queries;multi domain;search computing	Where can I attend an interesting database workshop close to a sunny beach? Who are the strongest experts on service computing based upon their recent publication record and accepted European projects? Can I spend an April weekend in a city served by a low-cost direct flight from Milano offering a Mahler’s symphony? We regard the above queries as multi-domain queries, i.e., queries that can be answered by combining knowledge from two or more domains (such as: seaside locations, flights, publications, accepted projects, conference offerings, and so on). This information is available on the Web, but no general-purpose software system can accept the above queries nor compute the answer. At the most, dedicated systems support specific multi-domain compositions (e.g., Google-local locates information such as restaurants and hotels upon geographic maps). This paper presents an overall framework for multi-domain queries on the Web. We address the following problems: (a) expressing multi-domain queries with an abstract formalism, (b) separating the treatment of “search” services within the model, by highlighting their differences from “exact” Web services, (c) explaining how the same query can be mapped to multiple “query plans”, i.e., a well-defined scheduling of service invocations, possibly in parallel, which complies with their access limitations and preserves the ranking order in which search services return results; (d) introducing crossdomain joins as first-class operation within plans; (e) evaluating the query plans against several cost metrics so as to choose the most promising one for execution. This framework adapts to a variety of application contexts, ranging from end-user-oriented mash-up scenarios up to complex application integration scenarios.	emoticon;general-purpose markup language;list of google products;mash-1;map;program optimization;scheduling (computing);seaside;semantics (computer science);services computing;software system;symphony;web service;world wide web	Daniele Braga;Stefano Ceri;Florian Daniel;Davide Martinenghi	2008	PVLDB	10.14778/1453856.1453918	web query classification;computer science;data mining;database;programming language;world wide web	DB	-40.827665261696886	15.327179398146543	131326
2b3723b521968348a71f91d6f7e463a7c5b3fefb	a geographic standards based metamodel to formalize spatio-temporal knowledge in remote sensing applications	spatio temporality;geographic standards;remote sensing interpretation	Spatio-Temporal Metamodel Relations The Spatial Relation Package abc 3 sub-classes defined by E.Clementini → Most elements are derived from previous works Egenhofer, RCC8, Frank, . . . C. Pierkot (Espace-Dev) Geographic standards based metamodel Secogis 2012 18 / 25 SpatialRelationPackage AngleRelation DistanceRelation OrientationRelation CardinalRelation -Reference : String = {intrinsic, extrinsic, deitic} SpatialRelation ProjectiveRelation MetricRelat ion TopologicalRelation CBMRelation RCC8Relation C o m p le m e n tO f E q iv a le n tT o EhRelation RelationPackage Relat ion SpatialRelation C o m p le m e n tO f E q iv a le n tT o < < re fi n e > > Examples How to use this metamodel ? 1) For each viewpoints, create a model derived from the metamodel a) Define Spatio-Temporal features b) Add Thematic, Spatial and Temporal Characteristics c) Add Semantic, Spatial and Temporal Relationships → Spatio-Temporal concepts and Relations are derived from the Metamodel → Thematic Properties and Semantic Relations are specified by the model C. Pierkot (Espace-Dev) Geographic standards based metamodel Secogis 2012 19 / 25 Examples How to use this metamodel ? 2) Make matching between viewpoints → Some correspondances are obvious (e.g spatial relations) → Others must be explicitly defined (e.g. thematic dimensions) C. Pierkot (Espace-Dev) Geographic standards based metamodel Secogis 2012 20 / 25 Examples Experiments on Amazonian Littoral Field Point of View Model → Features classification with spatial and temporal relations C. Pierkot (Espace-Dev) Geographic standards based metamodel Secogis 2012 21 / 25 Examples Experiments on Amazonian Littoral Image Point of View Model → Image segment thematic characteristics C. Pierkot (Espace-Dev) Geographic standards based metamodel Secogis 2012 22 / 25 Examples results Preliminary Results → For details, please contact samuel.andres@ird.fr C. Pierkot (Espace-Dev) Geographic standards based metamodel Secogis 2012 23 / 25 Calibrated Image Vegetation Mangrove Beach Conclusions Conclusions Modular conceptual ST metamodel based on normalized approaches Support image interpretation according to the associated point of view Easy to use by remote sensing and thematic experts Preliminary interesting results Perspectives Formalize all concepts into framework and domain ontologies Define more accurate matching Use description logics to enable complex reasoning Refine preliminary results C. Pierkot (Espace-Dev) Geographic standards based metamodel Secogis 2012 24 / 25	description logic;espace;emoticon;image segmentation;metamodeling;ontology (information science);point of view (computer hardware company);region connection calculus;software framework;view model	Christelle Pierkot	2012		10.1007/978-3-642-33999-8_36	knowledge management;data mining;database;management science	AI	-42.322288902498215	4.985456993759292	131412
bdf5bb991bf7f034218a7e7ca9091b40a986023e	dealing with elements of medical encounters: an approach based on the ontological realism		Electronic health records (EHRs) serve as repositories of documented data collected in a health care encounter. An EHR records information about who receives, who provides the health care and about the place where the encounter happens. We also observe additional elements relating to social relations in which the healthcare consumer is involved. To provide a consensus representation of common data and to enhance interoperability between different EHR repositories we have created a solution grounded in formal ontology. Here, we present how an ontology for the obstetric and neonatal domain deals with these general elements documented in health care encounters. Our goal is to promote the interoperability of information among EHRs created in different specialties. To develop our ontology, we used two main approaches: one based on ontological realism, the other based on the principles of the OBO Foundry, including reuse of reference ontologies.	formal ontology;global variable;interoperability;obo foundry;ontology (information science);upper ontology	Fernanda Farinelli;Maurício Barcellos de Almeida;Peter L. Elkin;Barry Smith	2016				Web+IR	-45.35785719142722	4.35660951209447	131516
fe1538240c14fcf0de2507c9d6271fbaf38f22d5	frankenstein: a platform enabling reuse of question answering components		Recently remarkable trials of the question answering (QA) community yielded in developing core components accomplishing QA tasks. However, implementing a QA system still was costly. While aiming at providing an efficient way for the collaborative development of QA systems, the Frankenstein framework was developed that allows dynamic composition of question answering pipelines based on the input question. In this paper, we are providing a full range of reusable components as independent modules of Frankenstein populating the ecosystem leading to the option of creating many different components and QA systems. Just by using the components described here, 380 different QA systems can be created offering the QA community many new insights. Additionally, we are providing resources which support the performance analyses of QA tasks, QA components and complete QA systems. Hence, Frankenstein is dedicated to improve the efficiency within the research process w.r.t. QA.	composability;ecosystem;frankenstein complex;ibm thinkpad 380;machine learning;mathematical optimization;pipeline (computing);pipeline (software);population;question answering	Kuldeep Singh;Andreas Both;Arun Sethupat Radhakrishna;Saeedeh Shekarpour	2018		10.1007/978-3-319-93417-4_40	data mining;computer science;reuse;question answering;reusability	HCI	-47.73443301325781	11.491471935458808	131536
95e35334d4efb8505704d2ee462806bafcf28296	applying a service-oriented approach for developing a distributed multi-agent system for healthcare	geriatrics;distributed system;alzheimer patients;multiagent system;raisonnement base sur cas;razonamiento fundado sobre caso;systeme reparti;healthcare;agent based systems;multi agent system;service orientation;salud publica;geriatria;service oriented architectures;healthcare technology;intelligence artificielle;soa;orientado servicio;patient care;distributed mas;geriatrie;geriatric residences;multi agent systems;planificacion;old peoples homes;sistema repartido;case based planning;cbr;sante publique;artificial intelligence;planning;oriente service;inteligencia artificial;planification;case based reasoning;sistema multiagente;service oriented architecture;security;public health;systeme multiagent;service oriented	This paper presents a service-oriented architecture that allows a more efficient distribution of resources and functionalities. The architecture has been used to develop a multi-agent system aimed at enhancing the assistance and healthcare for Alzheimer patients living in geriatric residences. Most of the system functionalities have been modelled as independent and distributed services, including reasoning, planning and security mechanisms. The results obtained after testing the architecture in a real healthcare scenario demonstrate that a service-oriented approach is far more robust and has better performance than a centralised one.	automated planning and scheduling;centralisation;multi-agent system;service-orientation;service-oriented architecture;service-oriented device architecture	Dante I. Tapia;Juan Francisco de Paz;Sara Rodríguez;Cristian Pinzón;Rosa Cano;Javier Bajo;Juan Manuel Corchado	2010	IJCAT	10.1504/IJCAT.2010.036027	simulation;computer science;engineering;artificial intelligence;service-oriented architecture;multi-agent system;computer security	ML	-38.65619660082884	14.979021189121454	131667
0b30496fa3315b7902e4ad61018b079d7a4404b7	an ontology-learning knowledge support system to keep e-organization's knowledge up-to-date: a university case study	complementary knowledge source;knowledge acquisition;knowledge management systems;olekss process;ontology-learning knowledge support system;university case;university case study;knowledge user;ontology-learning knowledge support;task-intensive knowledge requirement;knowledge support;general component	e-Organizational users can apply semantic engineering solutions to deal with decision-making and task-intensive knowledge requirements supported by Knowledge Management Systems (KMSs). Such optional engineering strategies consider some system types to meet knowledge users' need, aligned with the e-services and e-management qualities required for them. Particularly, in the Knowledge Support System (KSS) field, developers have adopted some Ontology-based technologies to support user's task-knowledge system functionalities. In this paper, an Ontology-Learning Knowledge Support System (OLeKSS) model is proposed as a general component of e-organizations, to keep the ontologies associated with this kind of KMS updated and enriched. Relational Databases (RDBs) are considered complementary knowledge source for Knowledge Acquisition (KA) through a OLeKSS Process (as a subsystem component) based on methodologies for Ontology Learning (OL). In a University case, we had applied a Systemic Methodology for OL (SMOL) from a RDB to update the correspondent host-ontology associated to the University's KSS during this OLeKSS process.	ontology learning	Richard Gil;Maria J. Martín-Bautista	2011		10.1007/978-3-642-22961-9_20	knowledge integration;computer science;knowledge management;artificial intelligence;body of knowledge;knowledge-based systems;open knowledge base connectivity;data mining;procedural knowledge;knowledge extraction;personal knowledge management;knowledge value chain;world wide web;domain knowledge	NLP	-44.972708161632625	5.707621802255205	131812
9891d4e02e9eb8056cf25f57964e36bd60713095	a semantic approach to a framework for business domain software systems	gestion integrada;extraction information;documento electronico;anotacion;gestion integree;logica booleana;entreprise;ontologie;sistema experto;productivite;document analysis;information extraction;computational techniques;information source;source information;processus metier;rule based;web semantique;empresa;knowledge extraction;software systems;semantics;arquitectura logicial;integrated management;annotation;intelligence artificielle;rule learning;semantica;semantique;productividad;document electronique;software architecture;analyse documentaire;business applications;semantic processing;web semantica;enterprise information system;firm;separacion senal;extraction connaissances;semantic web;architecture basee modele;analisis documental;artificial intelligence;extraccion conocimiento;logique booleenne;proceso oficio;ontologia;separation source;inteligencia artificial;domain analysis;extraction rules;information system;systeme expert;productivity;source separation;boolean logic;ontology;extraccion informacion;business rules;model driven architecture;systeme information;architecture logiciel;business process;electronic document;fuente informacion;arquitectura basada modelo;sistema informacion;expert system	In enterprise firms, enormous amounts of electronic documents are generated by business analysts and other business domain application users. Applications that use these documents are often driven by business logic that is hard-coded together with application logic. One approach to the separation of business logic from applications is to create and maintain business and information extraction rules in an external, user-friendly format. The drawback of such an externalization is that the business rules, usually, do not have machine interpretable semantics. This situation often leads to misinterpretation of domain analysis documents, which can inhibit the productivity of computer-assisted analytical work and the effectiveness of business solutions. This paper proposes an ontology and rule-based framework for the development of business domain applications, which includes semantic processing of externalized business rules and to some extent externalization of application logic. The creation of external information extraction rules by the business analyst is a cumbersome and time consuming task. In order to overcome this problem, the framework also includes a rule learning system to semi-automate the generation of information extraction rules from source documents with the help of manual annotations. The main idea behind the work presented in this paper is to re-engineer very large enterprise information systems to adapt to Semantic Web computing techniques. The work presented in this paper is inspired by an industrial project.	business domain;software system	Yevgen Biletskiy;Girish R. Ranganathan	2010	Computers in Industry	10.1016/j.compind.2010.05.004	domain analysis;business rule management system;boolean algebra;business object;software architecture;productivity;business logic;semantics of business vocabulary and business rules;business domain;business requirements;semantic memory;computer science;artifact-centric business process model;business process management;artificial intelligence;semantic web;ontology;business case;data mining;database;semantics;business process model and notation;knowledge extraction;business process;business software;business process discovery;business rule;business process modeling;expert system;information extraction;information system;business activity monitoring;enterprise information system;software system;business architecture	SE	-37.57493037444781	13.122672629420563	131938
ce9ad0696f184825f20d329766bd33a0f6131f29	an ontology solution for language interoperability between negotiation agents		Traditional negotiation systems have been implemented using agent architectures, where agents communicate exchanging negotiation primitives generated by each system, based on particular language definitions implicitly encoded, giving different syntax and semantics to their messages. In this paper we address the problem of communicating heterogeneous negotiation agents in a Web-based environment, considering differences in their message implementations. Our research is based in the development of a shared ontology for publishing definitions of negotiation primitives, and a translator module, which is executed only when a misunderstanding occurs. We implemented a service-oriented architecture for executing negotiations and conducted experiments incorporating heterogeneous agents. The results of the tests show that the proposed solution improves communication between negotiation agents.	agent architecture;experiment;language interoperability;run time (program lifecycle phase);service-oriented architecture;service-oriented device architecture;systems architecture	Maricela Bravo;José Velázquez	2006			computer science;knowledge management;database;distributed computing	AI	-41.997833406901265	17.837716471117986	132045
634d3a3f7377b0b2e7feabfed73c4fd30fdc587f	enquiring and reasoning over diagrams using the semantic web			diagram;semantic web	Zaineb Ben Fredj	2010				AI	-39.442806299087074	6.21334786136899	132135
367f2ee0b3d24070999eb4cfe057821d473d3b36	modeling interactive web sources for information mediation	navegacion informacion;red www;navigation information;integration information;information browsing;model complexity;systeme conversationnel;information integration;interactive system;integracion informacion;base donnee orientee objet;sistema conversacional;world wide web;reseau www;object oriented databases;information system;user interaction;systeme information;sistema informacion	"""We propose a method for modeling complex Web sources that have active user interaction requirements. Here """"active"""" refers to the fact that certain information in these sources is only reachable through interactions like filling out forms or clicking on image maps. Typically, the former interaction can be automated by wrapper software (e.g., using par2imeterized urls or post commands) while the latter cannot and thus requires explicit user interaction. We propose a modeling technique for such interactive Web sources and the information they export, based on so-called interaction diagrams. The nodes of an interaction diagram model sources and their exported information, whereas edges model transitions and their interactions. The paths of a diagram correspond to sequences of interactions emd allow to derive the various query capabilities of the source. Based on these, one can determine which queries sire supported by a source and derive query plans with minimal user interaction. This technique can be used offline to support design and implementation of wrappers, or at runtime when the mediator generates query plans against such sources."""	diagram;formal language;image map;input/output;interaction technique;interoperation;lazy evaluation;mix;microsoft outlook for mac;requirement;run time (program lifecycle phase);sql;standard streams;unified modeling language;web page	Bertram Ludäscher;Amarnath Gupta	1999		10.1007/3-540-48054-4_19	computer science;information integration;operating system;data mining;database;distributed computing;world wide web;information system	DB	-35.94939986967102	11.103510083504746	132259
379eeb7c35e17717e64bff3c491392c1cfeba16d	a linked data perspective for collaboration in mashup development	groupware;information retrieval;enterprise linked data collaborative mashup design expert search enterprise 2 0 linked data integration enterprise mashup;web site expert search linked data set knowledge integration enterprise mashup development linksman approach linked data supported mashup collaboration web mashup application development web api software component catalogue mashape programmableweb;cataloguing;enterprise 2 0;linked data integration;web sites application program interfaces cataloguing data integration groupware information retrieval software development management web services;mashups collaboration organizations vocabulary resource description framework knowledge engineering;collaborative mashup design;application program interfaces;enterprise linked data;web services;web sites;enterprise mashup;software development management;expert search;data integration	Web mashup is becoming an approach more and more popular for developing Web applications both for general and enterprise purposes. Mashup development is fueled by Web sites, as Programmable Web and Mashape, offering large, ever growing, catalogues of software components accessible through Web APIs. Developing Web mashup applications requires specialized knowledge about Web APIs, technologies and the way to combine them in a meaningful way. This kind of knowledge is often available but distributed among different experts. In this paper we introduce the LINKSMAN (LINKed data Supported MAshup collaboratioN) approach for expert search in enterprise mashup development. The approach is based on integrating knowledge both internal and external to enterprises. The result is then published as linked data set. We show how typical collaboration patterns among mashup developers can be formalized and implemented on this linked data set to support expert search. A prototype application is also described.	component-based software engineering;linked data;mashup (web application hybrid);prototype;resource description framework;vocabulary;web api;web application	Devis Bianchini;Valeria De Antonellis;Michele Melchiori	2013	2013 24th International Workshop on Database and Expert Systems Applications	10.1109/DEXA.2013.20	web service;web modeling;computer science;data integration;data mining;database;world wide web;mashup	Web+IR	-45.906791097189	9.242071661415396	132819
0bc4a1ae9f88d012109c20a32c91de0e8f9ca6f5	an automatic configuration algorithm for reliable and efficient composite services		Reusability is a central concept of Web services as it allows for the construction of composite services. Thus, an existing composite service can be combined with other composite services to form more complex nested or hierarchical services. Reliability and efficiency are the main requirements of composite services construction. The reliability requirements are rigorously defined by designers using the accepted termination states concept. The efficiency requirements are tightly related to a set of quality-of-service (QoS) constraints that are required by customers. In this paper, we first developed a hierarchical model for composite services. Based on this model, we developed a recursive procedure for the automatic computation of the transactional reliability and QoS of composite services. Second, we proposed a new concept, called required efficiency level, to offer more flexibility to the customers to specify their needs in terms of QoS. Third, we developed a new composite service configuration (CSC) algorithm for the construction and adaptation of composite services while considering the reliability and efficiency requirements. The originality of the CSC algorithm consists in a new recursive global QoS constraint decomposition procedure. Finally, we conducted a set of experiments to evaluate the benefits of the proposed CSC algorithm in comparison with the related work. These experiments confirm that our CSC algorithm is able to generate, in a timely fashion, reliable, and efficient composite services.	algorithm;computation;experiment;hierarchical database model;quality of service;recursion;requirement;service-oriented modeling;web service	Imed Abbassi;Mohamed Graiet	2018	IEEE Transactions on Network and Service Management	10.1109/TNSM.2017.2785360	quality of service;web service;computer science;distributed computing;algorithm;recursion;computation;reusability;hierarchical database model;composite number	DB	-45.945828080726784	16.876979573097763	132844
287afc317c2e5ff80717fa3acbfe16da530edc3c	semantic exchange of medicinal data: a way towards open healthcare systems	resource description format;information systems;semantic interoperability medicinal data semantic exchange healthcare sector xml based technologies medicinal information systems open healthcare information systems;semantic interoperability;medical services information systems standards organizations xml pharmaceuticals standardization ontologies programming paper technology blood;medical services;engines;healthcare system;medical information systems;web services;xml;ontologies;information system;healthcare information system;xml electronic data interchange health care medical information systems;electronic data interchange;health care	Recently several organizations in the healthcare sector have produced standards and representation forms using XML. However, the introduction of these XML-based technologies is not enough to provide a means to interpret the semantics of the exchanged messages. Instead communicating medicinal information systems are hard-coded to only work together. As a result extending the systems by new parties as well as introducing new message types is inconvenient typically requiring long lasting standardization processes. How to replace hard-coded medicinal information systems by the open healthcare information systems that support semantic interoperability, are extensible and maintainable is the topic of this paper. In particular, we described our work on using RDF-statements in exchanging messages. In such a setting, the semantics of the exchanged data is available with the data, which makes the data machine processable. We also describe how communicating medicinal information systems can use XSLT-transformations in picking out their needed information from the exchanged messages.	archive;hard coding;information system;interoperation;semantic web;semantic interoperability;xml;xslt	Juha Puustjärvi;Leena Puustjärvi	2009	2009 Third International Conference on Digital Society	10.1109/ICDS.2009.59	computer science;data mining;database;law;world wide web;information system	DB	-46.24119434247445	8.272056144508058	132882
1e8606c2698ec0077b2e0a57b85aa289730bbcdf	reporters, editors and presenters: using embodied agents to report on online computer games	owl s;multi agent system;nets within nets;application software;clocks;tv broadcasting;prototypes;information filtering;unreal tournament;web service;data mining;renew;permission;fellows;reference nets;pervasive game;high level petri nets;workflow;prototypes application software scalability permission computer science fellows clocks tv broadcasting data mining information filtering;scalability;computer science;business process;computer game;embodied agent	We present a multi-agent framework to generate reports of playersý activities within multi-player computer games. We describe an initial implementation of our framework as an extension to the Capture the Flag game within Unreal Tournament, and sketch future applications of this work including other genres of games, the emergence of games as a spectator sport, implications for pervasive games as well as non-gaming applications.	capture the flag;embodied agent;emergence;multi-agent system;pc game;pervasive informatics;unreal tournament	Dan Fielding;Mike Fraser;Brian Logan;Steve Benford	2004	Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.	10.1109/AAMAS.2004.228	video game design;web service;workflow;application software;scalability;simulation;4x;embodied agent;turns, rounds and time-keeping systems in games;computer science;artificial intelligence;emergent gameplay;game mechanics;multi-agent system;prototype;multimedia;business process;world wide web;owl-s	AI	-47.79904449450848	12.6311354085454	132920
5b84251151a0dd60f4ffd82e0408258be233693e	a framework for feeding linked data to complex event processing engines	linked data;ontology mapping;complex event processing;rule based reasoning	A huge volume of Linked Data has been published on the Web, yet is not processable by Complex Event Processing (CEP) or Event Stream Processing (ESP) engines. This paper presents a framework to bridge this gap, under which Linked Data are first translated into events conforming to a lightweight ontology, and then fed to CEP engines. The event processing results will also be published back onto the Web of Data. In this way, CEP engines are connected to the Web of Data, and the ontological reasoning is integrated with event processing. Finally, the implementation method and a case study of the framework are presented.	complex event processing;event stream processing;lightweight ontology;linked data;resource description framework;sparql;world wide web	Dong Liu;Carlos Pedrinaci;John Domingue	2010			computer science;complex event processing;data mining;database;world wide web	Web+IR	-38.921014445832185	6.963686718140893	133140
bccf7d2792c1a1e6dcfe1926f0185dcd360a26dd	a teaching strategies engine using translation from swrl to jess	ontologie;computer assisted teaching;hoja estilo;systeme tutoriel intelligent;format fichier;intelligent tutoring system;xslt;teaching strategies;web semantique;interrogation base donnee;service web;interrogacion base datos;ensenanza asistida por computador;intelligence artificielle;web service;user assistance;assistance utilisateur;file format;web semantica;feuille style;asistencia usuario;intelligent tutoring systems;semantic web;metaprogrammation;artificial intelligence;ontologia;formato fichero;procedural knowledge;inteligencia artificial;educacion;metaprogramming;meta programming;metaprogramacion;ontology;database query;enseignement assiste ordinateur;servicio web	Within an intelligent tutoring system framework, the teaching strategy engine stores and executes teaching strategies. A teaching strategy is a kind of procedural knowledge, generically an if-then rule that queries the learner’s state and performs teaching actions. We develop a concrete implementation of a teaching strategy engine based on an automatic conversion from SWRL to Jess. This conversion consists of four steps: (1) SWRL rules are written using Protégé’s SWRLTab editor; (2) the SWRL rule portions of Protégé’s OWL file format are converted to SWRLRDF format via an XSLT stylesheet; (3) SweetRules converts SWRLRDF to CLIPS/Jess format; (4) syntax-based transformations are applied using Jess meta-programming to provide certain extensions to SWRL syntax. The resulting rules are then added to the Jess run-time environment. We demonstrate this system by implementting a scenario with a set of learning contents and rules, and showing the run-time interaction with a learner.	clips;encode;interaction;jess;metaprogramming;ontology (information science);protégé;runtime system;semantic web rule language;upper ontology;web ontology language;xslt	Eric Wang;Yong Se Kim	2006		10.1007/11774303_6	metaprogramming;computer science;artificial intelligence;operating system;ontology;database;multimedia;programming language;world wide web;computer security	Web+IR	-36.83069438610154	11.150703815797494	133273
2cdac59e8f70be8e93f91ec2a21a98f3c415df1a	the rule responder distributed reputation management system for the semantic web		Online Reputation management systems compute, manage, and provide reputation about entities which act on the Web. An important research question is how such a reputation management system can be build for the Semantic Web. In this paper we contribute with a reputation management system based on distributed rule agents, which uses Semantic Web rules for implementing the reputation management functionalities as rule agents and which uses Semantic Web ontologies for representing simple or complex multi-dimensional reputations. We demonstrate a Semantic Web reputation management system by means of an e-Commerce and a Social Semantic Web 3.0 use case, which is using an ontology based reputation object model and a rule-based inference service agent middleware, called Rule Responder. As contribution our solution ensures efficient automation, semantic interpretability and interaction, openness in ownership, fine-grained privacy and security protection and easy management of semantic reputation data on the Web.	agent-based model;e-commerce;entity;knowledge representation and reasoning;logic programming;management system;middleware;network architecture;ontology (information science);openness;reputation management;social semantic web;world wide web	Adrian Paschke;Rehab Alnemr	2010			semantic web;social semantic web;data mining;ontology (information science);object model;management system;middleware;inference;computer science;reputation	Web+IR	-40.841334750612326	14.179657974048773	133276
1b8fa8ac27075d331133fe2c48a88e89a6441793	context-based free-form annotation in xml documents	rule based approach;context information;free form marking correction;rule based;annotation;general methods;xml;xml document;markup language;annotation interface	When creating annotation information in a free-form environment, ambiguity arises during the analysis stage between geometric information and the annotations. This needs to be resolved so that the accurate creation of annotation information in a free-form annotation environment is possible. This paper identifies and analyses the ambiguities, specifying methods that are tailored to each of the various contexts that can cause conflicts with free-form marking in a XML-based annotation environment. The proposed general method is based on context which includes various textual and structure information between free-form marking and the annotations themselves. The context information used is expressed in context-based annotation markup language (CAML), a language defined within the paper. The results are printed and shared through a system specifically implemented for this study. The results from the implementation of the proposed method show that the annotated areas included in the free-form marking information are more accurate, achieving more accurate exchange results amongst multiple users in a heterogeneous document environment.	xml	Won-Sung Sohn;Jae-Kyung Kim;Seung-Kyu Ko;Soon-Bum Lim;Yoon-Chul Choy	2003	Int. J. Hum.-Comput. Stud.	10.1016/S1071-5819(03)00013-2	rule-based system;xml;computer science;database;programming language;temporal annotation;world wide web;information retrieval	Web+IR	-36.718302466405405	7.5987257147475775	133452
2b6350b50859768a110c62bb9a51f0cf5e76cb61	a survey on ontology creation methodologies	information systems;knowledge management;methodologies;knowledge engineer;km;is;ontologies;top down ontology;domain ontologies;semantic models	In the current literature of knowledge management and artificial intelligence, several different approaches to the problem have been carried out of developing domain ontologies from scratch. All these approaches deal fundamentally with three problems: (1) providing a collection of general terms describing classes and relations to be employed in the description of the domain itself; (2) organizing the terms into a taxonomy of the classes by the ISA relation; and (3) expressing in an explicit way the constraints that make the ISA pairs meaningful. Though a number of such approaches can be found, no systematic analysis of them exists which can be used to understand the inspiring motivation, the applicability context, and the structure of the approaches. In this paper, we provide a framework for analyzing the existing methodologies that compares them to a set of general criteria. In particular, we obtain a classification based upon the direction of ontology construction; bottom-up are those methodologies that start with some descriptions of the domain and obtain a classification, while top-down ones start with an abstract view of the domain itself, which is given a priori. The resulting classification is useful not only for theoretical purposes but also in the practice of deployment of ontologies in Information Systems, since it provides a framework for choosing the right methodology to be applied in the specific context, depending also on the needs of the application itself.		Matteo Cristani;Roberta Cuel	2005	Int. J. Semantic Web Inf. Syst.	10.4018/jswis.2005040103	upper ontology;idef5;open biomedical ontologies;ontology alignment;kilometer;ontology components;bibliographic ontology;computer science;knowledge management;ontology;artificial intelligence;knowledge-based systems;methodology;data mining;ontology-based data integration;is;world wide web;information retrieval;process ontology;information system;suggested upper merged ontology	Web+IR	-44.172653625930074	6.1104013590129345	133461
77a60a793089ce411fe9fbc4a8bf80027a78daeb	efficient and scalable filtering of graph-based metadata	subscribe;publish subscribe system;graph matching;content distribution;publish subscribe;information dissemination;content management system;experimental evaluation;publish;content based routing;data dissemination;rdf	RDF Site Summaries constitute an application of RDF on the Web that has considerably grown in popularity. However, the way RSS systems operate today limits their scalability. Current RSS feed arregators follow a pull-based architecture model, which is not going to scale with the increasing number of RSS feeds becoming available on the Web. In this paper we introduce G-ToPSS, a scalable publish/subscribe system for selective information dissemination. G-ToPSS only sends newly updated information to the interested user and follows a push-based architecture model. G-ToPSS is particularly well suited for applications that deal with large-volume content distribution from diverse sources. G-ToPSS allows use of an ontology as a way to provide additional information about the data disseminated. We have implemented and experimentally evaluated G-ToPSS and we provide results demonstrating its scalability compared to alternative approaches. In addition, we describe an application of G-ToPSS and RSS to a Web-based content management system that provides an expressive, efficient, and convenient update notification dissemination system.	class hierarchy;content management system;digital distribution;experiment;multiple inheritance;prototype;publish–subscribe pattern;query language;rdf schema;rss;resource description framework;scalability;selective dissemination of information;time complexity;web ontology language;web content;world wide web	Haifeng Liu;Milenko Petrovic;Hans-Arno Jacobsen	2005	J. Web Sem.	10.1016/j.websem.2005.09.006	computer science;rdf;database;internet privacy;publish–subscribe pattern;world wide web;dissemination;matching	Web+IR	-34.14322552675103	4.291053041459061	133490
89ba68b963d0e9c7e6c86e590c583593fa16eba7	ontology-based analysis of event-related potentials		We describe recent progress in the development and application of NEMO (Neural ElectroMagnetic Ontology), a formal ontology for the event-related potentials (ERP) domain. The ontology encodes knowledge about patterns that are commonly seen in ERP studies. The patterns are defined using equivalent class descriptions, which specify the spatial, temporal, and functional constraints that must be satisfied for an ERP instance, or datum, to belong to a particular pattern class. The data themselves are represented in RDF, using N-triples that link the data to the ontology. Our analysis pipeline automatically generates these RDF data. We then apply a reasoner, such as Hermit, to classify the data. By creating this pipeline, we have enabled our consortium partners to compare results across experiment paradigms using a common knowledge base and to refine that base (i.e., to add or adjust pattern descriptions) based on cross-lab study results. We discuss implications for ERP meta-anlaysis, discovery of new knowledge, and resolution of current controversies in the ERP literature.	erp;formal ontology;geodetic datum;knowledge base;semantic reasoner	Gwen A. Frishkoff;Robert M. Frank;Paea LePendu	2011			ontology;data mining;computer science;event-related potential	AI	-38.20737349078428	4.987369302842853	134114
37bb5a4d2813a88a5884913a320e6cde72071ce9	scalable oriented-service architecture for heterogeneous and ubiquitous iot domains		Internet of Things (IoT) grows quickly, and 50 billion of IoT devices will be interconnected by 2020. For the huge number of IoT devices, a high scalable discovery architecture is required to provide autonomous registration and look-up of IoT resources and services. The architecture should enable dynamic updates when new IoT devices are incorporated into Internet, and changes are made to the existing ones. Nowadays in Internet, the most used discovery architecture is the Domain Name System (DNS). DNS offers a scalable solution through two distributed mechanisms: multicast DNS (mDNS) and DNS Service Directory (DNS-SD). Both mechanisms have been applied to discover resources and services in local IoT domains. However, a full architecture has not still been designed to support global discovery, local directories and a search engine for ubiquitous IoT domains. Moreover, the architecture should provide other transversal functionalities such as a common semantic for describing services and resources, and a service layer for interconnecting with M2M platforms and mobile clients. This paper presents an oriented-service architecture based on DNS to support a global discovery, local directories and a distributed search engine to enable a scalable looking-up of IoT resources and services. The architecture provides two lightweight discovery mechanisms based on mDNS and DNS-SD that have been optimized for the constraints of IoT devices to allow autonomous registration. Moreover, we analyse and provide other relevant elements such semantic description and communications interfaces to support the heterogeneity of IoT devices and clients. All these elements contribute to build a scalable architecture for the discovery and access of heterogeneous and ubiquitous IoT domains.	access control;authorization;autonomous robot;centralized computing;context awareness;distributed web crawling;elasticsearch;geolocation;hypertext transfer protocol;ipso alliance;internet of things;interoperability;lookup table;mobile phone;multicast;privacy;radio-frequency identification;scalability;secure digital;sensor;service layer;smart tv;web search engine;web service	Pablo Lopez;David Fernández;Rafael Marin-Perez;Antonio J. Jara;Antonio F. Gómez-Skarmeta	2013	CoRR		internet privacy;world wide web;computer network	Mobile	-45.544851606730916	11.600201365698222	134136
5e2ff58aae9ad584c798b64efa6e4d9c5a7eada3	configuration knowledge representations for semantic web applications	semantic web;knowledge representation	Today’s economy exhibits a growing trend toward highly specialized solution providers cooperatively offering configurable products and services to their customers. This paradigm shift requires the extension of current standalone configuration technology with capabilities of knowledge sharing and distributed problem solving. In this context a standardized configuration knowledge representation language with formal semantics is needed in order to support knowledge interchange between different configuration environments. Languages such as Ontology Inference Layer ~OIL! and DARPAAgent Markup Language ~DAML 1OIL! are based on such formal semantics ~description logic! and are very popular for knowledge representation in the Semantic Web. In this paper we analyze the applicability of those languages with respect to configuration knowledge representation and discuss additional demands on expressivity. For joint configuration problem solving it is necessary to agree on a common problem definition. Therefore, we give a description logic based definition of a configuration problem and show its equivalence with existing consistency-based definitions, thus joining the two major streams in knowledge-based configuration ~description logics and predicate logic0constraint based configuration !.	darpa agent markup language;description logic;dyadic transformation;free variables and bound variables;graphical user interface;knowledge representation and reasoning;knowledge-based configuration;markup language;ontology (information science);ontology inference layer;problem solving;programming paradigm;requirement;semantic web;semantics (computer science);turing completeness;unified modeling language;well-formed formula	Alexander Felfernig;Gerhard Friedrich;Dietmar Jannach;Markus Stumptner;Markus Zanker	2003	AI EDAM	10.1017/S0890060403171041	natural language processing;knowledge representation and reasoning;configuration management database;description logic;computer science;knowledge management;artificial intelligence;theoretical computer science;body of knowledge;semantic web;open knowledge base connectivity	AI	-41.66122978350232	13.302053801050267	134486
6f10c765096a04e6332fc38f5f6d339427a523e6	composing data-providing web services	web service	Today, more than ever, modern enterprises are using Web services for data sharing within and across the ent erprise's boundaries. We call this kind of Web service as Dat a Providing Web services. In this paper, we present our approac h to automatically compose primitive data providing Web services for the purpose of creating data integration applicatio ns. Our approach exploits existing mature works done in dat a integration systems. Specifically, data providing services are modeled as RDF Parameterized Views over mediated ontologies. Then, an RDF oriented query rewriting algorithm is used to compo se services for answering received queries. The composition is then optimized and deployed as a new Web service accessible on top of he Web.	algorithm;demoscene compo;exploit (computer security);ontology (information science);resource description framework;rewriting;web service;world wide web	Mahmoud Barhamgi;Djamal Benslimane	2009			web service;web application security;web development;web modeling;data web;web mapping;web-based simulation;web standards;computer science;ws-policy;web navigation;social semantic web;web page;data mining;ws-addressing;database;services computing;web intelligence;ws-i basic profile;web 2.0;world wide web	Web+IR	-40.784218556899155	9.335665551801034	134595
8f7394f7f6b0cb31634b199392dda7bb30907d71	a semantic-based architecture for electronic money system and multi-channel value-added services	protocols;vas;semantics;e payment;consumer electronics;user modeling;user profiling;semantic modeling;business;mobile communication;interoperability;recommender systems;data models;service delivery	The evolution of microchips and embedded technologies and those related to the mobile world has led to the growth and proliferation of devices and systems for electronic payment used by end users to buy products or services. Over the last few years there has been a tendency to use these devices and systems to provide users a way to benefit of services that are accessories to the payment (e.g. Value Added Services - VAS) and can be used before or after a transaction takes place. The heterogeneity of users' devices and instruments, linked to the heterogeneity of acceptance devices and service platforms, also makes life harder to new service providers and to users (e.g. merchants). This paper describes the solution defined in the context of TITAN project which objective is to create an integrated and innovative system for managing Electronic Money and Value Added Services offering an integrated, multichannel and customer-centric view through the integration of perspectives of Business Partners, Customers and Service Providers by defining a business partner network (modeled semantically) and supporting marketing policies (facilitated by exploiting users' profiles). This paper is focused on the architecture of TITAN platform and on the models used to represent semantic services and the user profiling.	acquiring bank;change request;embedded system;functional requirement;integrated circuit;owl-s;ontology (information science);recommender system;usability	Giuseppe D'Aniello;Alfonso De Vivo;Anna Chiara De Rosa;Antonio Donatiello;Daniela Greco;Francesca Pettinati;Valeria Viserta;Ciro Bologna;Antonio de Donato;Giuseppe Di Santo	2014	2014 International Conference on Intelligent Networking and Collaborative Systems	10.1109/INCoS.2014.105	data modeling;communications protocol;interoperability;user modeling;mobile telephony;computer science;service delivery framework;operating system;data mining;database;semantics;services computing;world wide web;computer security;computer network	Mobile	-48.006517775661116	14.14422042487606	134610
82f724d2387a24a56485fd0efa266461e2e3d307	xoperator - an extensible semantic agent for instant messaging networks	personal information management;context aware;instant messaging;semantic technologies;semantic web technology;heterogeneous information;large scale;semantic web;overlay network;information agent	Instant Messaging is in addition to Web and Email the most popular service on the Internet. With xOperator we demonstrate the implementation of a strategy which deeply integrates Instant Messaging networks with the Semantic Web. The xOperator concept is based on the idea of creating an overlay network of collaborative information agents on top of social IM networks. It can be queried using a controlled and easily extensible language based on AIML templates. Such a deep integration of semantic technologies and Instant Messaging bears a number of advantages and benefits for users when compared to the separated use of Semantic Web technologies and IM, the most important ones being context awareness as well as provenance and trust. Our demonstration showcases how the xOperator approach naturally facilitates enterprise and personal information management as well as access to large scale heterogeneous information sources.	instant messaging	Sebastian Tramp;Jörg Unbehauen;Sören Auer	2008		10.1007/978-3-540-68234-9_59	semantic computing;semantic integration;overlay network;semantic grid;computer science;semantic web;social semantic web;personal information management;semantic web stack;database;internet privacy;semantic technology;world wide web;semantic analytics	AI	-41.56095398402001	9.075476013967027	134827
b6b07f241736f4a7c875de03025bd08acd063e9d	rfc format framework		In order to improve the readability of RFCs while supporting their archivability, the canonical format of the RFC Series will be transitioning from plain-text ASCII to XML using the xml2rfc version 3 vocabulary; different publication formats will be rendered from that base document. With these changes comes an increase in complexity for authors, consumers, and the publisher of RFCs. This document serves as the framework that provides the problem statement, lays out a road map of the documents that capture the specific requirements, and describes the transition plan.	complexity;mac address;requirement;vocabulary;xml	Heather Flanagan	2016	RFC	10.17487/RFC7990	computer science;data mining;augmented backus–naur form;database;world wide web	DB	-35.057672268739445	7.22870130665948	134941
54b7ce1812fb2be01de4108375b3170fdc6f8ab0	increasing the efficiency of ontology alignment by tracking changes in ontology evolution		In this paper we present a development of our ontology alignment framework based on varying semantics of attributes. Emphasising the analysis of explicitly given descriptions of how attributes change meanings they entail while being included within different concepts have been proved useful. Moreover, we claim that it is consistent with the intuitive way how people see the real world and how they find similarities and correspondences between its elements. In this paper we concentrate on the issue of tracking changes that may occur within aligned ontologies and how these potential changes can influence the process of finding new mappings or validating ones that have already been found.	ontology alignment	Marcin Pietranik;Ngoc Thanh Nguyen;Cezary Orlowski	2014		10.1007/978-3-319-11289-3_40	bioinformatics;data mining;ontology-based data integration;information retrieval	AI	-45.25186336454795	6.688394763634208	135104
54c8d599b1820adfd9c64f0046c0c42001b6256f	using statistics, visualization and data mining for monitoring the quality of meta-data in web portals	content management;web portals;meta data quality;web data analysis;quality of process	The goal of many web portals is to select, organize and distribute content in order to satisfy its users/customers. This process is usually based on meta-data that represent and describe content. In this paper we describe a methodology and a system to monitor the quality of the meta-data used to describe content in web portals. The methodology is based on the analysis of the meta-data using statistics, visualization and data mining tools. The methodology enables the site’s editor to detect and correct problems in the description of contents, thus improving the quality of the web portal and the satisfaction of its users. We also define a general architecture for a system to support the proposed methodology. We have implemented this system and tested it on a Portuguese portal for management executives. The results validate the methodology proposed.	data mining;portals	Marcos Aurélio Domingues;Carlos Soares;Alípio Mário Jorge	2013	Inf. Syst. E-Business Management	10.1007/s10257-012-0209-5	web mining;web development;web modeling;enterprise portal;content management;web standards;computer science;data mining;database;world wide web;mashup	DB	-44.83723236163301	8.721978449897938	135162
0bd4d7f6f20e02b2ab230278ce59de8715df2ca5	xqse: an xquery scripting extension for the aqualogic data services platform	libraries logic service oriented architecture xml shape java middleware proposals buildings application software;bea;query processing;query languages;xml distributed databases middleware query languages query processing;integration logic xquery scripting extension aqualogic data services platform bea middleware platform multiple heterogeneous data sources xquery language aldsp 3 0 declarative logic procedural logic;xml;distributed databases;middleware	The AquaLogic Data Services Platform (ALDSP) is a BEA middleware platform for creating services that access and manipulate information drawn from multiple heterogeneous sources of data. The integration logic for read services is specified declaratively using the XQuery language. ALDSP 3.0, available in December 2007, includes a new XQuery-based Scripting Extension - XQSE - that enables developers to write procedural as well as declarative logic without leaving the XQuery world. In this paper, we describe the XQSE extensions to XQuery and show how they help to support important new classes of data services in ALDSP 3.0.	middleware;xquery;zorba (xquery processor)	Vinayak R. Borkar;Michael J. Carey;Daniel Engovatov;Dmitry Lychagin;Till Westmann;Warren Wong	2008	2008 IEEE 24th International Conference on Data Engineering	10.1109/ICDE.2008.4497532	xml;computer science;middleware;database;programming language;world wide web;distributed database;query language	DB	-34.19834811934171	8.694601772636396	135342
3393939eba9404f53c223913c0a4a1910b6bc005	ontology-based economic models for bioenergy and biofuel projects	chp;ontology based economic model;process and energy engineering;biological system modeling;semantics;h800 chemical;economic model;semantic web biofuel decision support systems ontologies artificial intelligence renewable energy sources renewable materials;bioenergy ontology;biofuels;investment;bchp;bioenergy bioenergy ontology economic model semantic economic model ontology based economic model combined heat and power chp bchp biomass;combined heat and power;biomass;semantic economic model;economics;europe;biological system modeling economics europe semantics biofuels investment;bioenergy;interreg projects ontology based economic models bioenergy biofuel projects renewable energy biomass hydrocarbon fuel decision support tools programming code semantic web technologies biofuel economics economic calculations	Bioenergy is a renewable energy generated from biomass, while biofuel is a hydrocarbon fuel that is produced from biomass. Recently, bioenergy and biofuel projects are encouraged and supported by many governments and organizations in various ways such as providing incentives, technical supports, information, and decision support tools. Economic model is one of the decision support tools, which helps to estimate the costs and earnings involved in a project. It is constructed with various elements such as concepts, relations, logics, constants and equations. In current economic models, all the elements are hard coded into some programming code, which makes the model less reusable and extendable. To address the issue, we present an ontology-based economic model in this paper. In particular, we have leveraged the Semantic Web technologies to represent the knowledge about the bioenergy and biofuel economics and inferred the equations and other values required for economic calculations. The case study has been carried out in two of the INTERREG Projects and found promising results.	axiomatic system;decision support system;extensibility;http 404;hard coding;semantic web	Krishna Sapkota;Pathmeswaran Raju;William J. Byrne;Craig Chapman	2015	Proceedings of the 2015 IEEE 9th International Conference on Semantic Computing (IEEE ICSC 2015)	10.1109/ICOSC.2015.7050839	biomass;investment;economic model;semantics;linguistics;bioenergy	SE	-47.33490897909269	5.435450613140286	135536
6bf24c930915228d173e4989a3171c5aa369083d	a trust-based provider recommender for mobile devices in semantic environments		Semantic web services have been studied during last years as an extension of Service-Oriented Computing on the Web 2.0. A lot of effort has been made to address some problems such as service discovery, matchmaking or service composition. Nevertheless, there is not much work in the literature about how to integrate trust into the process of selecting service providers. In this paper, an abstract architecture with a trust-based recommender module is presented, and a case study is put forward explaining how to apply the architecture to implement an agent in a mobile device.	recommender system	Helena Cebrián;Álvaro Higes;Ramón Hermoso;Holger Billhardt	2010		10.1007/978-3-642-14883-5_9	computer science;knowledge management;multimedia;world wide web	AI	-46.49917191586203	13.994176798499252	135775
0a6f127723c6c980865b3d491cb3294a09920567	federated query processing for the semantic web	informatica;peerreviewed;tesis;info eu repo semantics doctoralthesis	The recent years have witnessed a constant growth in the amount of RDF data available on the Web. This growth is largely based on the increasing rate of data publication on the Web by different actors such governments, life science researchers or geographical institutes. RDF data generation is mainly done by converting already existing legacy data resources into RDF (e.g. converting data stored in relational databases into RDF), but also by creating that RDF data directly (e.g. sensors). These RDF data are normally exposed by means of Linked Data-enabled URIs and SPARQL endpoints. Given the sustained growth that we are experiencing in the number of SPARQL endpoints available, the need to be able to send federated SPARQL queries across them has also grown. Tools for accessing sets of RDF data repositories are starting to appear, differing between them on the way in which they allow users to access these data (allowing users to specify directly what RDF data set they want to query, or making this process transparent to them). To overcome this heterogeneity in federated query processing solutions, the W3C SPARQL working group is defining a federation extension for SPARQL 1.1, which allows combining in a single query, graph patterns that can be evaluated in several endpoints. In this PhD thesis, we describe the syntax of that SPARQL extension for providing access to distributed RDF data sets and formalise its semantics. We adapt existing techniques for distributed data access in relational databases in order to deal with SPARQL endpoints, which we have implemented in our federation query evaluation system (SPARQL-DQP). We describe the static optimisation techniques that we implemented in our system and we carry out a series of experiments that show that our optimisations significantly speed up the query evaluation process in presence of large query results and optional operators.	data access;experiment;federated search;linked data;mathematical optimization;relational database;resource description framework;sparql;semantic web;sensor;speedup;world wide web	Carlos Buil Aranda	2014			rdf/xml;cwm;named graph;turtle;computer science;sparql;linked data;data mining;database;rdf query language;information retrieval;rdf schema	Web+IR	-35.38250719728951	4.929717461791491	135917
14a05832b00e52db77d2591aaef26009461ba6ca	ittalks: a case study in the semantic web and daml	daml oil;hypermedia markup languages;information resources;daml;multi agent system;case studies;information infrastructure;information retrieval;darpa agent markup language;information technology;data management;semantics;intelligent online service semantic web markup languages automated information gathering automated information processing multiagent systems ittalks system daml oil;semantic web technology;computer aided software engineering semantic web microstrip markup languages ontologies intelligent agent portals scheduling information retrieval information technology;online front ends;rdfs;multi agent systems;internet;semantic web;web based system;online front ends information resources multi agent systems information retrieval hypermedia markup languages;markup language;knowledge based systems;programming languages;rdf	Effective use of the vast quantity of information now available on the web will require the use of “Semantic Web” markup languages such as the DARPA Agent Markup Language (DAML). Such languages will enable the automated gathering and processing of much information that is currently available but insufficiently utilized. Effectively, such languages will facilitate the integration of multi-agent systems with the existing information infrastructure. As part of our exploration of Semantic Web technology, and DAML in particular, we have constructed ITTALKS, a web-based system for automatic and intelligent notification of information technology talks. In this paper, we describe the ITTALKS system, and discuss the numerous ways in which the use of Semantic Web concepts and DAML extend its ability to provide an intelligent online service to both the human community and the agents assisting them.	darpa agent markup language;multi-agent system;online service provider;semantic web;web application	R. Scott Cost;Timothy W. Finin;Anupam Joshi;Yun Peng;Charles K. Nicholas;Harry Chen;Lalana Kagal;Filip Perich;Youyong Zou;Sovrin Tolia	2001		10.1109/5254.988447	information infrastructure;semantic computing;the internet;semantic web rule language;data web;ontology inference layer;computer science;artificial intelligence;semantic web;rdf;social semantic web;multi-agent system;semantic web stack;database;markup language;world wide web;owl-s;information retrieval;semantic analytics;rdf schema	AI	-45.222977302156224	9.490746321684622	135952
38eec81417202f2575b3041e33e94b0b59373b28	discovering executable semantic mappings between ontologies	relational data;ontology mapping;data exchange;information integration;semantic mapping;query translation;ontology matching	Creating executable semantic mappings is an important task for ontology-based information integration. Although it is argued that mapping tools may require interaction from humans (domain experts) for best accuracy, in general, automatic ontology mapping is an AI-Complete problem. Finding matchings (correspondences) between the concepts of two ontologies is the first step towards solving this problem but matchings are normally not directly executable for data exchange or query translation. This paper presents an systematic approach to combining ontology matching, object reconciliation and multi-relational data mining to find the executable mapping rules in a highly automatic manner. Our approach starts from an iterative process to search the matchings and do object reconciliation for the ontologies with data instances. Then the result of this iterative process is used for mining frequent queries. Finally the semantic mapping rules can be generated from the frequent queries. The results show our approach is highly automatic without losing much accuracy compared with human-specified mappings.	ai-complete;database schema;executable;iteration;iterative method;matching (graph theory);ontology (information science);ontology alignment;relational data mining;semantic integration;semantic mapping (statistics)	Han Qin;Dejing Dou;Paea LePendu	2007		10.1007/978-3-540-76848-7_56	upper ontology;ontology alignment;computer science;ontology;data mining;database;ontology-based data integration;information retrieval;process ontology;data mapping	AI	-36.38650039784661	6.240313963090642	136086
a2a2edafdf7677f1789f704d72fe72504dfc2dec	a framework for ontology-based service selection in dynamic environments	service selection;satisfiability;dynamic environment	Previous approaches to service selection are mainly based on capturing and exchanging the ratings of consumers to providers. However, ratings reflect tastes of the raters. Therefore, service selection using ratings may mislead the consumers having a taste different than that of the raters. We propose to use experiences instead of the ratings. Experiences are the representation of what is requested by a consumer and what is received at the end. Unlike ratings, experiences do not reflect the opinion of the others, but the actual story between consumers and providers concerning a service demand. Using experiences, the consumer models the services of a provider for a specific service demand and selects the provider that is expected to satisfy the consumer the most. Our simulations show that proposed approach significantly increases the overall satisfaction of the service consumers.	experience;simulation	Murat Sensoy	2007			service level requirement;service level objective;differentiated service;knowledge management;satisfiability	HCI	-45.06496878114742	15.793803639565489	136176
26125e6cde91bea12f8ff73476cec6417349997a	interactive ontology-based user knowledge acquisition: a case study	content management;sistema interactivo;distributed system;unfolding;museo;ontologie;multiagent system;systeme reparti;conceptualization;musee;base de connaissances;behavioral analysis;deploiement;cultural heritage;web semantique;customization;despliegue;service web;personnalisation;recommandation;automatisation;gestion contenido;intelligence artificielle;acquisition connaissances;web service;automatizacion;systeme conversationnel;conceptualizacion;patrimoine culturel;sistema repartido;interactive system;web semantica;knowledge acquisition;patrimonio cultural;analyse comportementale;comportement utilisateur;personalizacion;semantic web;gestion contenu;recomendacion;artificial intelligence;base conocimiento;recommendation;ontologia;analisis conductual;museum;inteligencia artificial;user behavior;adquisicion de conocimientos;sistema multiagente;use case;ontology;conceptualisation;comportamiento usuario;servicio web;user model;systeme multiagent;knowledge base;automation	On the Semantic Web personalization technologies are needed to deal with user diversity. Our research aims at maximising the automation of acquisition of user knowledge, thus providing an effective solution for multi-faceted user modeling. This paper presents an approach to eliciting a user’s conceptualization by engaging in an ontology-driven dialog. This is implemented as an OWL-based domain-independent diagnostic agent. We show the deployment of the agent in a use case for personalized management of learning content, which has been evaluated in three studies with users. Currently, the system is being deployed in a cultural heritage domain for personalized recommendation of museum resources.	apache jena semantic web framework;conceptualization (information science);digital library;emoticon;faceted classification;formal specification;interoperability;knowledge acquisition;library (computing);online banking;personalization;resource description framework;semantic web;software architecture;software deployment;taxonomy (general);user experience;user modeling;web ontology language;dialog	Lora Aroyo;Ronald Denaux;Vania Dimitrova;Michael Pye	2006		10.1007/11762256_41	use case;web service;conceptualization;knowledge base;user modeling;content management;computer science;knowledge management;cultural heritage;artificial intelligence;automation;semantic web;ontology;multimedia;world wide web	AI	-38.097783323753895	14.013239142800966	136282
3b18f5eaceba49104b3b63b5823d34328a22b84d	tuml: a method for modelling temporal information systems	application development;modelizacion;base donnee temporelle;temporal information;modelisation;unified modelling language uml;unified modelling language;temporal databases;information system;analisis semantico;analyse semantique;modeling;systeme information;information system development;semantic analysis;object model;sistema informacion	Time is a very important aspect of the real world. The importance of time in database applications has led to work on the extension of existing database technologies in order to handle time efficiently. Nevertheless, little work has been done in the area of temporal application development methods. In this paper we present the Temporal Unified Modelling Language (TUML) method. The TUML method extends the Unified Modelling Language (UML) with the temporal semantics defined in the TAU Temporal Object Model and the process of the Booch method to enable the capturing of the temporal semantics of information during information system development.	information system;unified modeling language	Marianthi Svinterikou;Babis Theodoulidis	1999		10.1007/3-540-48738-7_38	natural language processing;unified modeling language;systems modeling;object model;computer science;data mining;database;temporal database;programming language;rapid application development;management;information system	DB	-35.06054451379392	12.594098189721523	136373
fb786aae4874c6ebb4bf27d42c2c7a86c4014d27	towards transaction-based reliable service compositions	protocols;reliability;transaction management;service composition;web services automotive engineering proposals protocols prototypes context aware services transaction databases computer applications application software sun;formal specification;automotive industry;prototypes;web service;commerce;data mining;reliability service compositions transaction management bpel;transaction based reliable service compositions;web services commerce formal specification transaction processing user interfaces;business;web services;bpel specifications transaction based reliable service compositions user requests service environment web services transaction management;user requests;bpel;transaction processing;bpel specifications;user interfaces;context;business process;service environment;service compositions	When a user requests the execution of a process in a service environment, often she desires transactional behavior in executing. However, current standards for Web services execution do not offer support for transactions. We overcome this shortcoming by presenting an extension proposal for integrating transaction management into the BPEL framework. The proposal has been demonstrated by a drop-dead order system in the context of the automotive industry. We test the business behavior described by BPEL specifications with and without transaction management when failures occur. Experimental results show that the proposed extension to BPEL framework is a viable solution for incorporating transactions into BPEL processes in order to implement reliable business processes.	business process execution language;business architecture;functional requirement;java;late binding;long-lived transaction;middleware;non-functional requirement;prototype;transaction processing;ws-federation;web service	Chang-ai Sun	2009	2009 33rd Annual IEEE International Computer Software and Applications Conference	10.1109/COMPSAC.2009.35	web service;real-time computing;business process execution language;computer science;database;law;world wide web	DB	-47.31697486158956	18.094966271890737	136553
b0839eb403e2dd386ea3b588f0a60f8195216303	knowledge sharing using ontology graph-based: application in plm and bio-imaging contexts		Data resources in PLM (Product Lifecycle Management) systems are becoming more and more huge and complex. The heterogeneity of data type and the dependencies among technical information make difficult for users in database exploitation: to query and to share the data. In this paper, we present an ontology-based approach as a promising solution to overcome this issue. An ontology graph-based query interface has been developed with the aim to enhance the knowledge sharing among different types of users (non-technical or coming from diverse expert domains) and then to facilitate the database exploitation. An example in Bio-Imaging domain will be presented as an application field.		Pham Cong Cuong;Alexandre Durupt;Nada Matta;Benoît Eynard	2015		10.1007/978-3-319-33111-9_22	ontology	HCI	-43.643902796630854	6.32033794756532	136615
f76286063b0b53225df46c16212f9848837f00c5	towards a semantic social network	semantic web semantic social network knowledge advanced technologies distributed systems web services complex web applications interoperability service composition techniques;web services open systems semantic web social networking online;web services;social networking online;semantic web;semantics web services social network services ontologies logic gates engines;open systems	Semantic Web is the application of the knowledge advanced technologies for the Web and the distributed systems. One of the most novel technologies for the Web are the Web services. Web services are a simple solution to develop complex Web applications as social networks that they need the interoperability of many technologies. The interoperability has been got in this work by using service composition techniques. In this paper we propose a strategy to develop a semantic social network taking advantage of composition Web Services. Our proposal is concentrated in the annotation process. We have developed an annotation tool that produces an annotated post from a simple post, this process will be described in the subsection II-A.	distributed computing;interoperability;semantic web;service composability principle;social network;web application;web service;world wide web	R. Pablo Camarille;Abraham Sánchez López;R. David Núñez	2013	CONIELECOMP 2013, 23rd International Conference on Electronics, Communications and Computing	10.1109/CONIELECOMP.2013.6525762	web service;web application security;web development;web modeling;data web;web mapping;web design;web standards;computer science;knowledge management;artificial intelligence;ws-policy;semantic web;web navigation;social semantic web;web page;semantic web stack;database;web intelligence;open system;ws-i basic profile;web 2.0;world wide web;semantic analytics	Web+IR	-43.843976883283986	12.673948680903143	136811
420693812aa1b7e1671003f873fc58a97e29b12c	hubble: an advanced dynamic folder technology for xml	query language;data exchange;data model;semantic information;xml document	A significant amount of information is stored in computer systems today, but people are struggling to manage their documents such that the information is easily found. XML is a de-facto standard for content publishing and data exchange. The proliferation of XML documents has created new challenges and opportunities for managing document collections. Existing technologies for automatically organizing document collections are either imprecise or based on only simple criteria. Since XML documents are self describing, it is now possible to automatically categorize XML documents precisely, according to their content. With the availability of the standard XML query languages, e.g. XQuery, much more powerful folder technologies are now feasible. To address this new challenge and exploit this new opportunity, this paper proposes a new and powerful dynamic folder mechanism, called Hubble. Hubble fully exploits the rich data model and semantic information embedded in the XML documents to build folder hierarchies dynamically and to categorize XML collections precisely. Besides supporting basic folder operations, Hubble also provides advanced features such as multi-path navigation and folder traversal across multiple document collections. Our performance study shows that Hubble is both efficient and scalable. Thus, it is an ideal technology for automating the process of organizing and categorizing XML documents.	categorization;data model;embedded system;organizing (structure);query language;scalability;xml;xquery	Ning Li;Joshua Hui;Hui-I Hsiao;Kevin S. Beyer	2005			well-formed document;xml catalog;data exchange;xml validation;binary xml;xml encryption;xml base;simple api for xml;xml;xml schema;data model;streaming xml;computer science;document type definition;document structure description;xml framework;xml database;xml schema;database;xml signature;world wide web;xml schema editor;cxml;information retrieval;query language;efficient xml interchange	DB	-36.519933050331346	8.922599405266704	136828
b397be25f318df1d71a1da482314aa8df3891fed	folkview: a multi-agent system approach to modeling folksonomies	multi agent system;dynamic views;folksonomy;zz structures;authoring	Folksonomies contain semantic information on data, and represent a meaningful mean for identifying similarities among users, resources and tags. Their strong potential is often reduced by the lack in social tagging systems of specialized functionalities for managing and modifying them, and of specific tools for generating customized and dynamic views on them. The aim of this paper is to present Folkview, an innovative way to conceive a folksonomy in terms of a multi-agent system. Each element (tag, user, resource) become an active entity and the folksonomy transforms itself from a traditional passive container of data into a computational agent, provided of a set of procedural and distributed skills. The agents actively collaborate in order to generate dynamic and customized views and supporting users in the updating, managing and modifying her personomy, and the same folksonomy.	folksonomy;mathematical model;multi-agent system;personalization;prototype	Antonina Dattolo;Emanuela Pitassi	2011		10.1007/978-3-642-28509-7_19	computer science;data mining;world wide web;information retrieval	AI	-42.5613954618982	8.575733428295006	136846
2a723f7b0c80b17b2ae0a2576b030ff9b5b9baaf	metadata creation in socio-semantic tagging systems: towards holistic knowledge creation and interchange	bottom up;social bookmarking;topic maps;semantic network;knowledge creation	Fuzzzy.com, a social bookmarking website has been developed to study collaborative creation of semantics. In a shared online space, users of Fuzzzy continuously create metadata bottom-up by categorizing (tagging) favourite hyperlinks (bookmarks). The semantic network of tags created by users evolves into a people’s fuzzy common ontology (“folktology”). We discuss several social and cognitive aspects of Topic Maps technology and scalability by analyzing the use of the system. We further argue that holistic knowledge creation and interchange is highly needed. Our results from Fuzzzy suggest that this cause will be rightly served by connecting distributed and welldefined communities of dedicated users within specific domains, having a clear knowledge-centric purpose.	categorization;folksonomy;holism;hyperlink;information overload;relevance;scalability;semantic network;tag (metadata);topic maps;dialog	Roy Lachica;Dino Karabeg	2007		10.1007/978-3-540-70874-2_16	topic maps;computer science;knowledge management;artificial intelligence;top-down and bottom-up design;semantic network;world wide web;information retrieval	Web+IR	-42.46885296684071	8.624711390430274	137136
b065c20da081fd9f2f4bdadb225b0ed0e57ff7e8	knowledge-based semantic annotation and retrieval of multimedia content	semantic annotation;information retrieval;ontologies;multimedia content analysis;knowledge base	— aceMedia is a 4 year EC part-funded FP6 Integrated Project, ending in December 2007. The project has developed tools to enable users to manage and share both personal and purchased content across PC, STB and mobile platforms. Knowledge-based analysis and ontologies have been successfully exploited in an end-to-end system to enable automated semantic annotation and retrieval of multimedia content. The paper briefly describes the objectives of aceMedia and the application of knowledge-based analysis in the project.	end system;end-to-end encryption;integrated project support environment;mobile device;ontology (information science);personal computer;set-top box	Giorgos Akrivas;Georgios Th. Papadopoulos;Matthijs Douze;Johannes Heinecke;Noel E. O'Connor;Carsten Saathoff;Simon Waddington	2007			knowledge base;content management;computer science;ontology;artificial intelligence;multimedia;world wide web;information retrieval	Web+IR	-41.56495275775394	5.5299283963670876	137202
90a80ec8d647ce4191a96dfe2c7d81282193bdbb	communities and media-towards a reconstruction of communities on media	electronic commerce;multi agent system;humans electronic commerce internet software agents multiagent systems constitution logic knowledge management information representation communications technology;multimedia systems electronic commerce multi agent systems internet;reference model;electronic commerce communities artificial agents media design media concept media model multi agent systems media reference model e commerce knowledge management interrelation communities multi agent system;multimedia systems;multi agent systems;col;internet;other research area	Media are explored as model to envision, to design, to formalize and to implement platforms for communities. We consider communities of both natural and artificial agents and aim at designing media which facilitate collaboration within such a community. Our approach is based on the media concept and the media model. The media concept envisions media as platforms for multi-agent systems and the media reference model determines the main components of a medium and guides its application as, e.g., for ECommerce or Knowledge Management. We present a formalization of those models that facilitates artificial agents to act according to the description given in this formalization. We explore the notion of community and various interrelations communities and their media. We discuss the representation of a community on a platform and how technology enables and influences the constitution of communities. We reconstruct communities on media and explore formalization, redesign and reconsideration of aspects of communities.	e-commerce;intelligent agent;knowledge management;multi-agent system;reference model	Ulrike Lechner;Beat F. Schmid	2000		10.1109/HICSS.2000.926817	the internet;reference model;computer science;knowledge management;artificial intelligence;multi-agent system;multimedia;world wide web	AI	-46.83327605620762	11.908648992511743	137344
790bec3288a7b0b640dbba62c08efb0650003f54	the ace theorem for querying the web of data	web of data;query processing;ace properties	Inspired by the CAP theorem, we identify three desirable properties when querying the Web of Data: Alignment (results up-to-date with sources), Coverage (results covering available remote sources), and Efficiency (bounded resources). In this short paper, we show that no system querying the Web can meet all three ACE properties, but instead must make practical trade-offs that we outline.	ace;cap theorem;data structure alignment;semantic web;world wide web	Jürgen Umbrich;Claudio Gutiérrez;Aidan Hogan;Marcel Karnstedt;Josiane Xavier Parreira	2013		10.1145/2487788.2487852	web modeling;computer science;semantic web;data mining;database;world wide web;information retrieval	DB	-39.22319305063332	5.539376777452864	137403
9b421598e5b667178cb20c4b78d21d410e224aaa	a semantic learning approach for mapping unstructured query to web resources	resource selection;resource selection semantic learning approach mapping unstructured query web resource xml data formal vocabulary data dictionary;t technology;query formulation;ontologies artificial intelligence;semantic web;meta data;semantic web learning artificial intelligence meta data ontologies artificial intelligence query formulation;learning artificial intelligence;xml vocabulary ontologies dictionaries resource description framework gallium nitride computer science information technology natural languages data mining	The search that involves structured Web resources like XML data, services is still lagging of its own method and relying on contemporary search systems. This paper presents a method that learns semantics from structured information of these resources. Instead of committing the semantic meaning of resources to strict and formal vocabularies like ontology or data dictionary, we are interested to interpret the meaning based on the natural context of the resources. The semantics are used in search process, i.e. query reasoning and resource selection, to provide better answer in terms of context relevancy and clearer result description	data dictionary;relevance;vocabulary;web resource;xml	Gan Keng Hoon;Keat Keong Phang;Enya Kong Tang	2006	2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06)	10.1109/WI.2006.24	natural language processing;query expansion;web query classification;semantic search;semantic grid;computer science;artificial intelligence;semantic web;concept search;social semantic web;data mining;semantic web stack;web intelligence;web search query;metadata;world wide web;information retrieval;semantic analytics	DB	-40.68958986192096	7.29404277580173	137612
d2c6d614029fca1a32c49895d9aae3eec9bd83a0	mapping relational databases to the semantic web with original meaning	relational data;software agent;relational database;semantic web	Most of nowadays Web content is stored in relational data-bases. It is important to develop ways for representation of this information in the Semantic Web to allow software agents to process it intelligently. The paper presents an approach to translation of data, schema and the most important constraints from relational databases into the Semantic Web without any extensions to the Semantic Web languages.	relational database;semantic web	Dmitry V. Levshin	2009		10.1007/978-3-642-10488-6_5	semantic data model;semantic computing;relational model/tasmania;web modeling;semantic web rule language;data web;web mapping;relational calculus;semantic search;semantic grid;web standards;relational database;computer science;artificial intelligence;semantic web;social semantic web;data mining;semantic web stack;database;web intelligence;semantic technology;information retrieval;semantic analytics	Web+IR	-38.195036497458844	7.305971641094656	137721
61285934e25af98fbbb24f752d6532828cbedc5c	security preferences specification and implementation in a service-based workflow	esb;security preferences specification;agile process support;service composition;service provider;software prototyping;risk analysis;authentication;partner companies;soa;business process support;service composition soa security risk analysis esb;software architecture;it security;internet;xml;web 2 0;workflow management software;authorization;service oriented architecture authorization xml authentication risk analysis;it applications;service based workflow;service oriented architecture;security;software reliability;service bus esb;security of data;business process;service provider security preferences specification service based workflow web 2 0 agile process support business process support service oriented architecture it applications partner companies service composition service bus esb;workflow management software internet security of data software architecture software prototyping software reliability	The development of web 2.0 increases the call for agile and simple Business process support. SOA (Service oriented Architecture) provides companies with a new model to build their IT applications around their business processes and combine them dynamically and flexibly with the services of partner companies. In this open and distributed context, it is required to implement an appropriate security at each service. So, during the composition of service, it will be good for user to specify the security preference to associate to each service. In this article we describe in a first step, the difficulty of using analytical risk methods such as EBIOS, Mehari and OCTAVE to specify the constraints of security to associate with services. Then we present the SOA and its security component, therefore start the service bus ESB will act as an intermediary between the client and service provider. In a second step, we develop our method that can lead to the specification of security and describe how it would be possible to specify these security constraints during the service composition.	agile software development;business process;enterprise service bus;gnu octave;service composability principle;service-oriented architecture;web 2.0	Wendpanga Francis Ouedraogo;Frédérique Biennier;Nicolas Salatgé	2010	2010 Sixth International Conference on Information Assurance and Security	10.1109/ISIAS.2010.5604047	computer security model;cloud computing security;service level requirement;service level objective;sherwood applied business security architecture;security information and event management;business service provider;differentiated service;service delivery framework;service design;database;security service;business;world wide web;computer security	DB	-47.84197196204319	17.33968263627138	137739
5407520783eb3152e9267b24b789ddf765db6317	a proposal for a flexible validation method for exchange of metadata between heterogeneous systems by using the concept of microschema		A new method to solve the validation problem that arises when exchanging information between heterogeneous systems is proposed. The problem of validation is addressed by introducing the concepts of MicroSchema, used in a namespace environment.		Jens Vindvad;Erlend Øverby	2002			computer science;knowledge management;data mining;database	EDA	-43.61643438414011	9.616075845556896	137939
bc810a37156b1ad6f65cf5c72506c87d0191b194	a fuzzy extension of owl for vague knowledge	official w3c recommendation;vague knowledge;multimedia application;fuzzy set theory;ontologies artificial intelligence;description logic ontology web language vague knowledge official w3c recommendation semantic web multimedia application fuzzy logic;fuzzy logic;web ontology language;owl fuzzy logic semantic web ontologies uncertainty fuzzy set theory computer science automatic logic units knowledge representation humans;semantic web;semantic web fuzzy logic fuzzy set theory ontologies artificial intelligence;description logic;ontology web language	OWL web ontology language is an official W3C recommendation in ontology language that has recently been developed by the W3C. Although OWL has a powerful expressive ability on knowledge, it has no capability to represent the uncertainty and imprecise information. In the context of Semantic Web and multimedia applications, concepts are rather vague than precise and there are increasing needs to deal with vague knowledge. This paper proposes an extension of OWL DL with fuzzy logic by giving the formal syntax and semantics. The feature of the extension is that it has the ability of capture the vague concepts with the defined modifying operators.	formal grammar;fuzzy logic;ontology (information science);semantic web;vagueness;web ontology language	Dexin Zhao;Zhiyong Feng;Qing Yu;Guangquan Xu	2007	Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)	10.1109/FSKD.2007.34	fuzzy logic;knowledge representation and reasoning;f-logic;description logic;semantic web rule language;ontology inference layer;computer science;ontology;artificial intelligence;semantic web;social semantic web;data mining;semantic web stack;database;fuzzy set;ontology language;web ontology language;owl-s;information retrieval	AI	-38.14028937599258	7.7323590340880015	137968
bb463807c0d71cda1e5012686bb8234bb7974ea8	describing bibliographic references in rdf		In this paper we present two ontologies, i.e., BiRO and C4O, that allow users to describe bibliographic references in an accurate way, and we introduce REnhancer, a proof-of-concept implementation of a converter that takes as input a raw-text list of references and produces an RDF dataset according to the BiRO and C4O ontologies.	bibliographic record;entity;linked data;ontology (information science);prototype;resource description framework;scientific literature;vocabulary;way to go;web ontology language	Angelo Di Iorio;Andrea Giovanni Nuzzolese;Silvio Peroni;David M. Shotton;Fabio Vitali	2014			information retrieval;rdf;ontology (information science);computer science	NLP	-40.17064210497701	4.5892187514434175	138026
6f248871d33bc798a51b95dff92b39d2d37b56ce	the research on hierarchical construction method of domain ontology	hierarchical identification domain ontology hierarchical construction method class meta information;owl;owl hierarchical construction method domain ontology semantic web knowledge organization semantic description method knowledge representation;knowledge management;semantics;information services;ontologies artificial intelligence;domain knowledge;knowledge representation languages;semantic description method;hierarchical identification;semantic web knowledge representation languages ontologies artificial intelligence;cognition;semantic description;semantic web;hierarchical construction method;ontologies;knowledge representation;class meta information;domain ontology;organizational structure;user interfaces;ontologies owl buildings semantics cognition knowledge based systems;knowledge based systems;buildings;spatial information;knowledge organization	The proposal and development of semantic web make ontology, as a new knowledge organization and semantic description method, get attention in various fields. The building of ontology is a basic and important task in semantic web. The construction method and organizational structure of ontology will directly affect the application of ontology. Based on the analysis of hierarchical characters existing in domain knowledge representation and organization and also the process of ontology building, this paper puts forwards a hierarchical construction method of ontology and gives the hierarchical construction principles in different ontology levels, which pays much attention to the hierarchical relationships in every stage of ontology building. And then based on this hierarchical construction method, this paper designs and builds a three-level geographical ontology applied in spatial information service. And also, this paper presents a description mechanism of ontology class metainformation to extend the description capability of OWL, which can help to well represent the difference and relationship between concept and term. The hierarchical identification in class meta-information can help to realize quick retrieving and matching of concepts based on hierarchical relationship.	extensibility;knowledge organization;knowledge representation and reasoning;ontology (information science);ontology engineering;semantic web;tree structure;web ontology language	Shengtao Sun;Dingsheng Liu;Guoqing Li;Wenyang Yu;Lv Pang	2010	2010 Sixth International Conference on Semantics, Knowledge and Grids	10.1109/SKG.2010.31	natural language processing;organizational structure;upper ontology;ontology alignment;cognition;ontology components;bibliographic ontology;ontology inference layer;computer science;knowledge management;ontology;artificial intelligence;semantic web;data mining;ontology chart;semantics;spatial analysis;ontology-based data integration;user interface;owl-s;process ontology;information system;domain knowledge;suggested upper merged ontology	AI	-42.562421484475365	6.543289468998272	138077
a97d2a8067e33e8367a082f0b691f03597443f0c	service search strategy based on graph in grid environment	semantic similarity;search problems graph theory grid computing;search algorithm;search strategy;grid service discovery service search strategy distributed grid environment grid service matching graph semantic similarity function parameters search algorithm;grid service;service discovery	Service discovery is a key concept in a distributed Grid environment. The first step towards Grid implement is the discovery of services. In this paper, we build a new Grid service matching graph based on the semantic similarity of function parameters and then an efficient search algorithm is presented to accomplish Grid service discovery. The experiment results show the algorithm proposed in this paper achieves good recall and precision.	precision and recall;search algorithm;semantic similarity;service discovery	Zilin Song;Weihua Ai;Yi Wang;Liang Wu	2006		10.1109/SKG.2006.95	semantic similarity;semantic grid;computer science;operating system;data mining;database;service discovery;best-first search;information retrieval;search algorithm	HPC	-43.972186865180184	13.035164798773378	138190
179a2fa69b3e0804c283eb15b67ad4f93d758f0b	transformation list for sgml application	standard generalized markup language	SGML (Standard Generalized Markup Language) is an ISO standard for document description (ISO 8879). The main idea in SGML is to specify document both by text and by the document’s structure without reference to a particular processing system. This kind of document description puts the document interchange into fact. But there are very few systems of SGML that have friendly interface and are portable in many applications. In this paper, various approaches to implementing SGML are assessed and the transformation list for SGML application is introduced. This approach is not limited to specific application fields. It is suitable to any application domain and is friendly to users. Users can understand it without any training and can use it as easily as doing their routine work. It will accelerate the development of the document interchange.	application domain;standard generalized markup language	Hong Gao	1995	Journal of Computer Science and Technology	10.1007/BF02948341	numeric character reference;processing instruction;computer science;document type definition;document type declaration;database;world wide web;information retrieval;sgml;sgml entity	NLP	-36.73728000838359	8.327965466666358	138322
da77e5a6d1f4c91a89eccaff30c7a15de7a30240	automated web services composition with iterated services		In the last decade there has been a proliferation of web services based application systems. In some applications (e.g., e-commerce, weather forecast) a web service is invoked many times with different actual parameters to obtain a composed service. In this paper we introduce the notion of iterated services that are obtained from given atomic services by iteration. The iterated services provide compact and elegant solutions to such complex composition problems that are unsolvable using the existing approaches. We define a new service dependency graph model to capture web services with sets of objects as input/output. We give a translation of the web services composition problem to a planning problem. Finally, we transform a plan to a composed web service. We have implemented our approach using the BlackBox planner.	iterated function;web service	Alfredo Milani;Rajdeep Niyogi	2017		10.1007/978-3-319-60438-1_19	iterated function;data mining;computer science;ws-addressing;web service;dependency graph;composition (visual arts);ws-i basic profile;services computing;ws-policy	ECom	-45.157663521049166	16.378784932726354	138332
0619a688bfc741de57f4cf41a7e17949485b389d	a semantic web based model for product cooperative design	groupware;design process;intelligent retrieval;design engineering;inference mechanisms;maintenance engineering;semantic inference;pharmaceutical technology;design optimization;ontologies artificial intelligence;process design;semantic web based model;groupware product design semantic web software tools web design ontologies artificial intelligence software reusability inference mechanisms;web design tool;design workflow;web design;product cooperative design;cscw semantic web based model product cooperative design engineering knowledge web design tool design resource reuse design resource ontology intelligent retrieval semantic inference design workflow;software reusability;engineering knowledge;semantic web;cscw;semantic web product design design engineering ontologies design optimization process design maintenance engineering knowledge engineering grid computing pharmaceutical technology;ontologies;software tools;product design;design resource ontology;grid computing;cooperative design;design resource reuse;knowledge engineering	A semantic Web based model for product cooperative design is proposed by enabling engineering knowledge to be published, discovered, reused, and integrated. Three features of the model are described in detail. The first is the tools for Web design, which enable a design process to be published as a service and then to be reused as a design resource in other design processes; the second is the ontology of design resources, which helps the implementation of intelligent retrieval by semantic inference; the third is design workflows, which enable designers to construct complex design with the existing simple design instances.	correctness (computer science);engineering design process;semantic web;web design	Yanning Xu;Lin Lv;Wei Ren;Xiangxu Meng	2005	Proceedings of the Ninth International Conference on Computer Supported Cooperative Work in Design, 2005.	10.1109/CSCWD.2005.194356	maintenance engineering;iterative design;process design;user-centered design;web modeling;multidisciplinary design optimization;design process;web design;computer science;knowledge management;computer-automated design;ontology;artificial intelligence;computer-supported cooperative work;semantic web;knowledge engineering;social semantic web;semantic web stack;database;design education;product design;design technology;high-level design;grid computing;generative design	EDA	-43.077288105459026	9.991790064505484	138375
81ef06380e6d95c42fb9e42b5621e5d2251908ad	agent-based model to web service composition		Traditional centralized Web Service Composition (WSC) approaches suffer from critical problems such as performance bottleneck and single point of failure. However, the analogy between Service and Agent Computing paradigms suggests that incorporating both technologies will likely lead to a more effective hybrid service model. In this work, we adopt an agent-based approach to WSC, in which Service Dependency Graph (SDG) is constructed as an AND/OR graph, distributed among the agent community members. Upon receiving a user composition request, agents perform internal reasoning and corporate through a communication protocol attempting to find a solution. Experiments are conducted on a public dataset and results show the effectiveness of the proposed approach in terms of communication cost.	agent-based model;algorithm;centralized computing;communications protocol;experiment;knowledge base;reliability engineering;single point of failure;web service;world sudoku championship	Abdullah Abdullah;Xining Li	2016	2016 IEEE International Conference on Services Computing (SCC)	10.1109/SCC.2016.74	engineering;knowledge management;data mining;world wide web	SE	-43.55297661461286	16.682334732306995	138642
a2dab26af598fcea9482fbc41c260fbf00dbbefc	towards savvy adoption of semantic technology: from published use cases to category-specific adopter readiness models	information systems;semantic technology;readiness;critical success factors;clustering	The decision of organizations to invest (or not) into a semantic application is, currently, often based on vague considerations and personal feelings. What is lacking is a model that would help determine whether semantic approaches would be adequate, given the aspects of the particular business and concrete adopter. Such model would however need to take into account the heterogeneity of di fferent applications that exhibit semantic features. We present a thorough exercise, and a prototypical methodology abstracted from it, for proceeding in multiple steps, from loosely sorted and purely textual descriptions of semantic applications to structured and instructive adopter readiness models. The whole process relies on expert-level manual analysis of textual descriptions, automatic cluster analysis (leading to plausible categories of semantic applications), critical factor analysis, questionnaire survey addressing the developers of applications, and adaptation of principles known from building multi-layer Capability Maturity Models. Although the overall approach relies to a large degree on (potentially subjective) manual analysis, very lightweight quantitative evaluation was also made for relevant steps in the process.	capability maturity model;cluster analysis;factor analysis;layer (electronics);semantic web;vagueness	Marek Nekvasil;Vojtech Svátek	2013	J. Web Sem.	10.1016/j.websem.2013.05.003	semantic grid;computer science;knowledge management;artificial intelligence;data science;data mining;database;cluster analysis;semantic technology;critical success factor;world wide web;information system	Web+IR	-45.869630748796496	7.243029485405628	138741
31d17d646f6c0873ea0ac4494c1be5811939e7e0	ontology integration approaches: a systematic mapping.		Ontology integration is an important topic of interest in Ontology Engineering. Integrating ontologies is a complex process that involves finding suitable ontologies, interpreting them and integrating them into a new integrated ontology. This paper presents a systematic mapping that investigated ontology integration approaches and provides a panorama of this topic. The results revealed a limited use of semantic relationships to map ontologies and a lack of concern with the integration scope, the goals the integrated ontology must achieve, search and selection of the ontologies to be integrated, and integration-based development processes.	ontology (information science);ontology engineering;web ontology language	Jordana Sarmenghi Salamon;Cássio Chaves Reginato;Monalessa Perini Barcellos	2018			data mining;ontology;computer science	AI	-43.60684514873187	6.1172710907753824	138818
a995447c7f673b1cfa7fe747d94e42424e2b7e18	kgol: a knowledge grid operating language	application development;query language;interfase usuario;red www;programming environment;user interface;sql;web;xml language;interrogation base donnee;reseau web;service web;interrogacion base datos;web service;lenguaje interrogacion;knowledge grid;medio ambiente programacion;internet;design and implementation;xml;analizador sintaxico;world wide web;interface utilisateur;langage interrogation;parser;analyseur syntaxique;database query;langage xml;lenguaje xml;grid operating language;environnement programmation	This paper presents the design and implementation of a Knowledge Grid operating language and the programming environment KGOL. It provides not only a friendly user interface for end-users to easily access and manage the Knowledge Grid resources but also a programming environment for application developers to implement the Knowledge Grid applications. The KGOL programming environment consists of a parser, an interpreter, an execution engine, and a result generator. Comparisons between the KGOL, the SQL, and the XML-based query languages XQL and LOREL show the distinguished advantages of the KGOL.	integrated development environment;interpreter (computing);query language;sql;user interface;xml;xquery	Hai Zhuge;Jie Liu	2003	SIGPLAN Notices	10.1145/844091.844101	xml;computer science;database;programming language;world wide web	HPC	-35.887831033764925	11.402677750440002	139058
49a957ae92305a491c96c711032f739632e8a86c	query processing of integrated xml databases	xml database	Query processing of integrated XML databases has recently received considerable attention. It helps users to extract information from an integrated XML databases system. A flexible and efficient framework for such query processing is proposed. Through a single request, without concern about their internal structures and geographical distribution, it allows decomposition of global queries into sub-queries conformed simultaneously to local formats. In order to reduce the redundancies of mappings, low-level metadata processing has been introduced; it yields bidirectional mappings for query decomposition and data conversion from the original metadata. It is also shown how to extract data and resolve conflicts between them to provide user-friendly results. XML Declarative Description [15] is applied to all components of the framework as their underlying model.	high- and low-level;usability;xml database	Le Thi Thu Thuy;Vilas Wuwongse	2003			streaming xml;data mining;database;xml framework;information retrieval;computer science;xml encryption;xml schema editor;xml validation;xml database;efficient xml interchange;xml base	DB	-35.72459077488835	7.1761684432560395	139191
381bd3b6104d6addff2968ba5e80b350f8252839	real-world restful service composition: a transformation-annotation-discovery approach		Existing Web API search engines allow for only category-based browsing and keyword or tag-based searches for RESTful services without offering the capability of discovering and composing real-world RESTful services from the viewpoint of application developers. Therefore, we propose a novel approach, referred to as TAD (Transformation-Annotation-Discovery), to address the above issue. TAD firstly transforms OpenAPI (Swagger) documents of RESTful services into the graph structure in graph database, and provides an annotation engine to automatically annotate the semantic concepts on each graph node by using LDA (Latent Dirichlet Allocation), and WordNet. Next, TAD conducts service composition based on the user requirement by the two modules, service discovery chain and pipeline-based composition. The service discovery chain checks service interface compatibility and retrieves supported and aided services to bridge the gap between the user requirement and the current discovered services based on the Hungarian algorithm. The pipeline-based composition module finds services that semantically fit the user's required tasks based on the annotated graph database and sends candidate services to service discovery chains to simultaneously seek for multiple possible composition solutions fitting the user's composition requirement. Experimental results show that the proposed approach is with good performance under the precision metric.	ahead-of-time compilation;application programming interface;graph (discrete mathematics);graph database;hungarian algorithm;konica minolta openapi;lambda lifting;latent dirichlet allocation;open api;openapi specification;oracle application server;pipeline (computing);quantum dot;representational state transfer;response time (technology);service composability principle;service discovery;user requirements document;web api;web search engine;wordnet	Shang-Pin Ma;Hsuan-Ju Lin;Ci-Wei Lan;Wen-Tin Lee;Ming-Jen Hsu	2017	2017 IEEE 10th Conference on Service-Oriented Computing and Applications (SOCA)	10.1109/SOCA.2017.9	web api;distributed computing;computer science;quality of service;latent dirichlet allocation;web service;information retrieval;graph database;service discovery;user requirements document;wordnet	DB	-45.02974573171444	13.105772284944718	139345
3e8afb18fcaf2861860cb56769260f4eaa5b45fd	chemical markup language: a simple introduction to structured documents			chemical markup language	Peter Murray-Rust	1997	World Wide Web Journal		world wide web;ruleml;computer science;style sheet language;document definition markup language;pcdata;collaborative application markup language;markup language;xhtml;html	ML	-38.860372693093126	8.263435773923183	139419
113fe6a4582e8b01ca5a773c1fc03c1c9696098b	boundary-based module extraction in weakly acyclic el++ ontologies: theory foundation and preliminary evaluation			directed acyclic graph;ontology (information science)	Jun Fang;Lei Guo	2009			discrete mathematics;ontology (information science);mathematics	AI	-37.818593187585925	6.468124850208615	139631
5247f2d46756778ca3fcf03ebe1060e53c140149	haley: a hierarchical framework for logical composition ofweb services	planning technique;state space methods;web services formal logic;uncertainty;logic;logic web services process planning state space methods uncertainty costs calculus computer science explosions supply chains;web service;haley framework web service logical composition planning technique symbolic technique;supply chains;web service logical composition;calculus;state space;web services;formal logic;symbolic technique;haley framework;explosions;computer science;process planning;first order logic	Prevalent approaches for automatically composing Web services (WSs) into Web processes predominantly utilize planning techniques to achieve the composition. However, many of the planning methods do not scale efficiently to large processes. In addition, they lack the capability to operate directly on the WS descriptions, and specifically on the preconditions and effects which may be represented using methods ground and propositionalize the higher level logic resulting in exponentially many more states. In this paper, we present a new framework for composing Web services into processes, called Haley, that exploits the natural hierarchy often found in Web processes. Haley uses symbolic techniques that operate directly on first order logic based representations of the state space to obtain the compositions. In addition to providing an approach that handles the uncertainty inheret in Web services, Haley guarantees cost-based optimality and offers an approach potentially scalable to large real world processes.	first-order logic;precondition;scalability;state space;web service	Haibo Zhao;Prashant Doshi	2007	IEEE International Conference on Web Services (ICWS 2007)	10.1109/ICWS.2007.95	web service;computer science;knowledge management;theoretical computer science;programming language;law;logic	SE	-43.48626482038372	14.543432624080605	139748
807467cc6670bf0b9c246ed6bffb359c9af03334	a monolithic approach to automated composition of semantic web services with the event calculus	semantic web service;optimal solution;composite web service;semantic annotation;owl s;event calculus;functional properties;execution of composite web services;web service;automatic web service composition;web service composition;input output;semantic web services;quality of service;ai planning	In this paper, a web service composition and execution framework is presented for semantically -annotated web services. A monolithic approach to automated web service composition and execution problem is chosen, which provides some benefits by separating composition and execution phases. An AI planning method using a logical formalism, namely Abductive Event Calculus, is chosen for the composition phase. This formalism allows one to generate a narrative of actions and temporal orderings using abductive planning techniques given a goal. The functional properties of services, namely input/output/precondition/effects (IOPE) are taken into consideration in the composition phase and non-functional properties, namely Quality of Service (QoS) parameters are used in selecting the most appropriate solution to be executed. The repository of OWL-S semantic web services are translated to the Event Calculus axioms and the resulting plans found by the Abductive Event Calculus Planner are converted to graphs. These graphs can be sorted according to a score calculated using the defined quality of service parameters of the atomic services in the composition to determine the optimal solution. The selected graph is converted to an OWL-S file which is executed consequently.	event calculus;semantic web service	Cagla Okutan;Nihan Kesim Cicekli	2010	Knowl.-Based Syst.	10.1016/j.knosys.2010.02.006	automated planning and scheduling;web service;input/output;web modeling;semantic web rule language;data web;quality of service;web standards;computer science;artificial intelligence;ws-policy;social semantic web;data mining;ws-addressing;semantic web stack;database;event calculus;world wide web;owl-s	AI	-43.97096715090251	14.317083652804358	139770
34a2e3fc43c84c561649d1e9c4b57fa28197cd9c	a planning approach for message-oriented semantic web service composition	semantic web service;service composition;data processing;web service;satisfiability;web service composition;semantic model;semantic information	"""In this paper, we consider the problem of composing a set of web services, where the requirements are specified in terms of the input and output messages of the composite workflow. We propose a semantic model of messages using RDF graphs that encode OWL ABox assertions. We also propose a model of web service operations where the input message requirements and output message characteristics are modeled using RDF graph patterns. We formulate the message-oriented semantic web service composition problem and show how it can be translated into a planning problem. There are, however, significant challenges in scalably doing planning in this domain, especially since DL reasoning may be performed to check if an operation can be given a certain input message. We propose a two-phase planning algorithm that incorporates DLP reasoning and evaluate the performance of this planning algorithm. An important class of web services consists of those that either do data processing or provide information, i.e. they take in messages containing input data, process them in some manner, and produce messages containing output data or results. In this paper we propose a novel way of associating rich semantic information with messages and web service operations. Our model describes messages using RDF graphs that encode OWL ABox assertions. It also describes the input message requirement and the output message description of each operation using RDF graph patterns. The terms used in these patterns are defined in OWL ontologies that describe the application domain. The main motivation behind this model is to allow automatic composition of workflows that process, transform or analyze data to produce some desired information. In such workflows, it is necessary to have expressive models of messages and of the data processing capabilities of services so as to compose services that are semantically compatible and to create workflows that produce the desired information. We formulate the message oriented service composition problem as one of producing messages that satisfy certain semantic conditions, from certain initial input messages. We show how this problem can be cast as a planning problem. Copyright c © 2007, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Many existing service models (like OWL-S (Martin et al 2004)) do not allow expressions with variables in describing inputs and outputs. OWL-S describes inputs and outputs using concepts in an ontology. Similarly, SA-WSDL (Akkiraju et al 2005), which allows linking semantic annotations to WSDL files, is also typically used to associate inputs and outputs with concepts in an ontology. Our model describes inputs and outputs using logical expressions with variables, and can relate the semantics of outputs to the semantics of inputs. These expressions describe constraints on data instances present in a message. In this sense, it is similar to WSML (de Bruijn 2005) which also allows specifying axioms with variables in the preand post-conditions of a web service capability. Our model also has a semantic description of messages. It explicitly defines the elements in a message and describes the semantics of these elements using an RDF graph. This feature is useful during composition by AI planning, when the planner needs to decide what kind of messages can be given as input to a web service and how it can construct these messages from other messages that have been produced by other services in a partial plan. There are many differences between our messageoriented composition problem and the problem tackled by many existing works, e.g. (Sirin & Parsia 2004; Traverso & Pistore 2004; Narayanan & McIlraith 2002), where the requirements are specified in terms of the initial and goal states of the world. The difference is that in our model, the world consists of a number of mutually independent messages, and new messages may be produced when operations are invoked. In this paper we explain further differences. One challenge with planning concerns the use of description logic reasoning for checking if two components can be connected. Since a planner needs to perform a large number of such checks while building a plan, this results in a large number of calls to a reasoner during plan-building. We describe a planner that overcomes this challenge by using a two-phase approach where pre-reasoning is performed once offline, and plan-building is performed online for each new request, without the use of reasoning. The pre-reasoning is done based on DLP (Description Logic Programs) (Grosof et al. 2003), and it produces additional inferred facts based on the descriptions of operations. The plan-building process makes use of the original and inferred facts when checking compatibility of operations and messages. The key contributions of this paper are the investigation of message-oriented model of services and service composition, the identification of some of the challenges that arise in using planning for composition, and the design of a twophase planner that tackles these challenges. We also present evaluation results of the planner. Workflow and Service Model A workflow is a graph G(V,E) where G is a DAG (Directed Acyclic Graph). Each vertex vi ∈ V is a web service operation. Each edge e(u, v) represents a logical flow of messages from u to v. If there is an edge e(u, v), then it means that an output message produced by the operation u is used to create an input message to the operation v. If v has many incoming edges of the form e1(u1, v), e2(u2, v), . . . en(un, v), it means that the output messages of u1, u2, . . . un are all used to create an input message to v. This is done by copying data elements from the output messages of u1, u2, . . . un into the input message to v. Note that an edge only represents a logical flow of messages, and not necessarily an actual flow. In the case of a BPEL workflow, the flow may occur through the coordinating workflow process, which gets the output message from the first operation, and copies some or all of the data elements from this output message into the input message to the second web service operation. Typically, a whole workflow can be viewed as one request-response operation on a WSDL portType defined for the workflow, i.e. there is one partner web service that makes a request to the workflow and receives a reply from it. We model this partner web service using two vertices in the DAG. One vertex called the source makes the request to the workflow. The source vertex has no incoming edges. Another vertex called the sink receives the reply from the workflow. The sink has no outgoing edges. All the remaining vertices in the DAG represent other web services that the workflow may call using a request-response paradigm. One of the main reasons why we restrict the workflow model to a DAG is that it is extremely difficult to create plans with cycles. Most AI planners create partially ordered plans, which can be represented as DAGs. For now, we assume that the definition of a message in the WSDL description of a web service includes only simple atomic XML types or complex types that are sequences. This restriction allows us to model the collection of elements in a message as a set. We now describe the model of messages. WSDL describes elements in a message using types in XML Schema. We extend this model to describe additional semantics associated with the elements. The semantics of the elements are described as an RDF graph consisting of OWL facts. The semantics describe a message that may be produced as output by a web service operation in a workflow. This message is of the form M(E,R) where • E is the set of elements in the message. • R is an RDF graph that describes the semantics of the data elements in the message. The RDF graph consists of a set of OWL facts (ABox assertions). One challenge with such a description is that different messages produced as output by a web service operation may have different values of the elements. Hence, we provide a layer of abstraction over the actual values and define the data elements of an exemplar message that contains exemplar individuals or exemplar literals. An exemplar individual is represented in OWL as belonging to a special concept called Exemplar. An exemplar literal is of a user defined type called xs:exemplar, which is derived from xs:string. Exemplar individuals and literals act as existentially quantified variables. In a particular message, they may be substituted by a value that belongs to the set of non-exemplar individuals (i.e. an OWL individual not belonging to the concept Exemplar) or non-exemplar literals (i.e. a literal that is not of type xs:exemplar). In this paper, we represent all exemplar individuals and literals with a preceding “ ”. For example, consider a message described by the complex type corpProfile: <xs:complexType name=""""corpProfile""""> <xs:sequence> <xs:element name=""""name"""" type=""""xs:string""""/> <xs:element name=""""address"""" type=""""xs:string""""/> <xs:element name=""""ticker"""" type=""""xs:string""""/> </xs:sequence> </xs:complexType> While the XML schema definition provides the general format of a message, messages produced by an operation in a workflow have more specific semantics. For example, the corporation profile may be of a specific corporation, XYZ, and the ticker from the New York Stock Exchange. An example description, in an N3-based syntax is below. import http://www.example.com/corporation.owl; Sequence corpProfile ContainsElements name, address, ticker WithSemanticDescription { corpProfile a Exemplar; a CorporationRecord;"""	abox;abstraction layer;algorithm;application domain;artificial intelligence;automated planning and scheduling;business process execution language;consistency model;data element;de bruijn graph;description logic;digital light processing;directed acyclic graph;encode;emoticon;existential quantification;graph (discrete mathematics);input/output;literal (computer programming);literal (mathematical logic);narayanan shivakumar;owl-s;object composition;online and offline;ontology (information science);partial-order planning;planner;programming paradigm;r language;request–response;requirement;resource description framework;semantic web service;semantic reasoner;service composability principle;two-phase commit protocol;two-phase locking;type conversion;web ontology language;web services description language;xml schema;xyz file format	Zhen Liu;Anand Ranganathan;Anton Riabov	2007			semantic data model;web service;semantic computing;web modeling;semantic web rule language;data web;data processing;semantic grid;web standards;computer science;ws-policy;semantic web;social semantic web;linked data;semantic web stack;database;world wide web;information retrieval;semantic analytics;satisfiability	Web+IR	-42.09859993120194	13.95494277832267	139784
0acf678f25c89402fe01cc3953258f84fea4894f	an architecture of a distributed semantic social network	application framework;distributed social networks;best practice;semantic pingback;social network;webid;semantic web;social semantic web;evaluation;online social network;architecture	Online social networking has become one of the most popular services on the Web. However, current social networks are like walled gardens in which users do not have full control over their data, are bound to specific usage terms of the social network operator and suffer from a lock-in effect due to the lack of interoperability and standards compliance between social networks. In this paper we propose an architecture for an open, distributed social network, which is built solely on Semantic Web standards and emerging best practices. Our architecture combines vocabularies and protocols such as WebID, FOAF, Semantic Pingback and PubSubHubbub into a coherent distributed semantic social network, which is capable to provide all crucial functionalities known from centralized social networks. We present our reference implementation, which utilizes the OntoWiki application framework and take this framework as the basis for an extensive evaluation. Our results show that a distributed social network is feasible, while it also avoids the limitations of centralized solutions.	application framework;best practice;centralized computing;coherence (physics);distributed social network;foaf (ontology);interoperability;ontowiki;reference implementation;semantic web;standards-compliant;vocabulary;web standards;webid;world wide web	Sebastian Tramp;Philipp Frischmuth;Timofey Ermilov;Saeedeh Shekarpour;Sören Auer	2014	Semantic Web	10.3233/SW-2012-0082	semantic computing;data web;computer science;knowledge management;social semantic web;data mining;semantic web stack;world wide web;social computing	Web+IR	-41.666829494126894	9.338408533236013	139955
95361f38da77f58d508373aba4b22017defcd09d	institution morphisms for relating owl and z	knowledge representation;semantic web;formal semantics	Checking for properties of Web ontologies is important for the development of reliable Semantic Web systems. Software specification and verification tools can be used to complement the Knowledge Representation tools in reasoning about Semantic Web. The key to this approach is to develop sound transformation techniques from Web ontology to software specifications so that the associated verification tools can be applied to check the transformed specification models. Our previous work has demonstrated a practical approach to translating Web ontologies to Z specifications. However, from a sound engineering point of view, the translation is lacking the theoretical work that can formally relate the respective underlying logical systems of OWL and Z. In this paper, we take the advantage that the logics underlying OWL and Z can be represented as institutions and we show that the institution comorphism provides a formal semantic foundation for the transformation from OWL to Z.	audio engineer;correctness (computer science);knowledge representation and reasoning;ontology (information science);semantic web;semantics (computer science);shapley–folkman lemma;software engineering;web ontology language	Dorel Lucanu;Yuan-Fang Li;Jin Song Dong	2005			programming language;systems engineering;social semantic web;web modeling;owl-s;semantic web;semantic web rule language;theoretical computer science;data web;ontology (information science);semantic web stack;computer science	Web+IR	-41.903828724343995	13.657659646549922	140131
a6ec6b129364edba0327033f5c9407a995418c12	olinda: uma abordagem para decomposição de consultas em federações de dados interligados		One of the main challenges to be faced in order to provide an integrated view of data distributed across multiple data sources is the query decomposition problem. In mediatorbased data integration systems, this problem consists in decomposing a query defined according to the mediation schema into one or more subqueries to be submitted on a set of local data sources. In order to produce the subqueries, the query decomposition process uses a set of mappings that specify the relationships between concepts of the mediation schema and concepts of the local schemas. Considering that the local schemas can be complex and heterogeneous, one of the major bottlenecks of conventional solutions for data integration is to discover these mappings. In general, this is a manual or semi-automatic process that has a high startup cost. This work deals with the query decomposition problem in the context of data integration on the Web of Data, where local data sources are RDF datasets (Resource Description Framework) and can be accessed through SPARQL queries (SPARQL Protocol and RDF Query Language), besides, data sources may be associated with an ontology that, in general, plays the role of the schema of the data source. It is important to note that, given the dynamic and heterogeneous nature of the Web of Data, and also because of scalability issues, it becomes really difficult to implement a query decomposition strategy that may be applied on the entire Web of Data. In this way, this work proposes a solution for query decomposition over Linked Data Federations available on theWeb of Data, i.e., sets of RDF data sources published according to the principles of Linked Data. In this context, if an application needs to query different datasets without having to make a new query for each one of them, then solutions are required to deal with the query decomposition problem between different and heterogeneous datasets. To solve this problem, this work proposes an approach for query decomposition over Linked Data Federations, which is divided into three main activities. Among each of these activities, the main contribution lies in the definition and implementation of a process for decomposing queries, considering that the data sources have structurally distinct ontologies, which describe their schemas. In this approach, the queries are expressed in SPARQL language and heterogeneous mappings are used with the purpose to treat aspects related to structural differences between ontologies. In order to evaluate the proposed approach, a prototype was implemented and some experiments were performed.	experiment;linked data;ontology (information science);power-on reset;prototype;sparql;unified model;world wide web	Danusa R. B. Cunha;Bernadette Farias Lóscio	2014			database;computer science	DB	-39.329063258183425	7.849556358711673	140369
42d123b1f52d2c953c75b8b8ba83906b2acf3355	managing technological knowledge of patents: hcontology, a semantic approach	owl;ipc classification;patent;ontology	Patent data provide technological information essential to define strategies and decisions in the context of firm innovative processes. At present, information regarding patents is usually represented and stored in large databases. Information from these databases is commonly retrieved in the form of files with a CSVor XML-based codification but with little semantics that enable the inference of further relationships among patents. In these databases, each patent is associated with a technological field by a code. Although the codes assume a hierarchical classification approach, inclusion/subsumption relationships are not explicitly specified such that computers can process them automatically. In recent years, ontologies have been proven to facilitate the exchange of information between people and systems. In this context, the Web Ontology Language (OWL), whose formal semantics are based on description logics, has become the most widely used language for the representation of ontologies. Certain patent ontologies have already been developed in OWL to benefit from the semantics of patent information. However, none have fully exploited the information that can be derived from the formal representation of patent code classification hierarchies through description-logics-based reasoning. This paper presents an approach to automatically translate the hierarchies found in the patent classification codes into concept hierarchies. This proposal also enables the automatic inference of implicit knowledge based on reclassification techniques and relationships between different application domains without changing the applications that make use of patent information. Several examples are presented to illustrate the applicability of the proposal and how it can assist firms in patent information management. Copyright Elsevier B.V. Reproduced with permission. Autor Bermudez-Edo, Maria; Hurtado, Maria V.; Noguera, Manuel; Hurtado-Torres, Nuria Institution University of Surrey, Guildford, GB; Universidad de Granada, ES Quelle Computers in Industry * Band 72 (2015) Seite 1-13 (13 Seiten, 37 Quellen)	code;computer;database;description logic;granada;information management;ontology (information science);semantics (computer science);subsumption architecture;web ontology language;world wide web;xml	María Bermúdez-Edo;María Visitación Hurtado;Manuel Noguera;Nuria Hurtado-Torres	2015	Computers in Industry	10.1016/j.compind.2015.03.010	patent visualisation;computer science;knowledge management;artificial intelligence;ontology;data mining;database;web ontology language	AI	-42.46311328467951	5.473170266574465	140524
0202f2a9581880810fc1c8d9f3c79c159abd2345	graph transformation for the semantic web: queries and inference rules	data sharing;context aware;pervasive computing;software systems;resource description framework;graph transformation;semantic web technology;context model;inference rule;web ontology language;semantic web	Pervasive computing becomes an important characteristic of today's IT systems. In particular, context awareness is a core technology to enhance pervasive computing functionalities [1]. In recent years, Semantic Web (SW) is widely used in the development of mainstream software systems; it becomes increasingly important and feasible for improving conventional web data sharing process by providing machine-readable information and metadata expressed in various semantic web technologies such as Web Ontology Language (OWL) and Resource Description Framework (RDF). SW technology itself has already indicated the solutions to context modeling and sharing. However, the context querying and inferencing methodologies are still missing which leads to 2 major challenges.	graph rewriting;semantic web	HongQing Yu;Yi Hong	2008		10.1007/978-3-540-87405-8_49	web service;semantic computing;web development;web modeling;semantic web rule language;data web;web mapping;semantic search;ontology inference layer;semantic grid;web standards;computer science;semantic web;rdf;social semantic web;linked data;data mining;semantic web stack;database;context model;web intelligence;web ontology language;web 2.0;world wide web;owl-s;ubiquitous computing;semantic analytics;rule of inference;software system	DB	-42.99130336795645	11.1604461522279	140537
6b89e8eb15a572b80cefc2fc21c67d71dd979ee4	meteor-s web service annotation framework with machine learning classification	semantic web service;semantic annotation;bayesian classifier;web service discovery;selected works;web service;schema matching;machine learning;automatic annotation;bepress	Researchers have recognized the need for more expressive descriptions of Web services. Most approaches have suggested using ontologies to either describe the Web services or to annotate syntactical descriptions of Web services. Earlier approaches are typically manual, and capability to support automatic or semi-automatic annotation is needed. The METEOR-S Web Service Annotation Framework (MWSAF) created at the LSDIS Lab at the University of Georgia leverages schema matching techniques for semi-automatic annotation. In this paper, we present an improved version of MWSAF. Our preliminary investigation indicates that, by replacing the schema matching technique currently used for the categorization with a Naïve Bayesian Classifier, we can match web services with ontologies faster and with higher accuracy.	algorithm;bayesian network;categorization;computer science;imperative programming;information processing and management;information engineering;information system;interoperation;machine learning;meteor;naive bayes classifier;ontology (information science);semantic web service;semiconductor industry;web services description language;world wide web	Nicole Oldham;Christopher J Thomas;Amit P. Sheth;Kunal Verma	2004		10.1007/978-3-540-30581-1_12	web service;web modeling;naive bayes classifier;data web;web mapping;web design;image retrieval;web standards;computer science;semantic web;social semantic web;data mining;semantic web stack;database;web intelligence;web 2.0;world wide web;information retrieval	Web+IR	-41.057464421230506	6.8943854906361395	140661
6b8a4f704ab684aebdebf34e1ef2585458c9171d	combining configuration and query rewriting for web service composition	directed graphs;semantic web service;directed acyclic graph;composition;query processing;semantics;user preferences;web service;query preference configuration query rewriting semantic web service composition user query service description discovery stage directed acyclic graph business rule orchestration stage semantic ranking algorithm user preference classification stage;web service composition;configuration semantic web services composition query rewriting;semantic web services;rewriting systems;business;cognition;web services;semantic web;ranking algorithm;planning;ontologies;web services ontologies concrete semantics business cognition planning;configuration;configuration management;query rewriting;business rules;web services configuration management directed graphs query processing rewriting systems semantic web;concrete	In this paper, we investigate the combination of configuration and query rewriting for semantic Web service composition. Given a user query and a set of service descriptions, we rely on query rewriting to find services that implement the functionalities expressed in the user query (discovery stage). Then, we use configuration to capture dependencies between services, and to generate a set of composed Web services described as a directed acyclic graph, while maintaining validity with respect to business rules (orchestration stage).Finally, we propose a semantic ranking algorithm to rank results according to user preferences (classification stage).The techniques used in our approach take into account the semantics of concepts utilized to describe the elements (services, business rules, query and user preferences) involved in the composition process. We provide a formal approach and its implementation, together with experiments on Web services from different application domains.	algorithm;application domain;directed acyclic graph;experiment;ontology (information science);requirement;rewriting;semantic web service;service composability principle;user (computing)	Amin Mesmoudi;Michael Mrissa;Mohand-Said Hacid	2011	2011 IEEE International Conference on Web Services	10.1109/ICWS.2011.26	web service;sargable;query optimization;query expansion;web query classification;ranking;computer science;database;semantics;web search query;law;world wide web;information retrieval;query language	DB	-44.34656470955445	14.165516055920456	140665
5382c42ddbd7a54d3072b4cea9c4840505dc1ecc	leveraging genetic algorithm to compose web services in a context-aware environment	context aware;genetic algorithm service similarity measurement service similarity tree method sst method service conflict optimal service composition multiobjective optimization problem service representation user preference representation situation representation context representation context space model service composition method service discovery adaption mechanism context aware pervasive environment web service composition;trees mathematics;web services;service similarity web services context aware genetic algorithm;ubiquitous computing;context genetic algorithms context modeling web services biological cells genetics conferences;genetic algorithm;genetic algorithms;web services genetic algorithms trees mathematics ubiquitous computing;service similarity	In a context-aware pervasive environment, it is crucial to provide an efficient adaption mechanism to rapidly discover appropriate services and compose them to achieve user desired situation when context change. In this paper, we propose a GA(Genetic Algorithm) based novel service composition method in a context-aware environment. We first use Context Space model to represent context, situation, user preference and services in a unified formal way. Then we transform the service composition problem into a multi-objective optimization problem and utilize GA to find the optimal service composition based on user's preference. To affiliate the connection between context and services, we use a vector to represent each service to reflect which context types a service can change so as to utilize GA to find the best service composition. Confliction of services may happen during the GA process. In order to resolve the conflicts, we propose a SST(Service Similarity Tree) method to measure the similarity among services to find out the best alternative. Finally, we design and implement a simulation experiment to verify our method. The results shows that our method can leverage GA to synthesis appropriate services to achieve user desired goal in an efficient way.	crossover (genetic algorithm);dijkstra's algorithm;experiment;genetic algorithm;mathematical optimization;multi-objective optimization;optimization problem;pervasive informatics;requirement;service composability principle;simulation;software release life cycle;unified model;web service	Zhichao Zhang;Shaoqiu Zheng;Weiping Li;Ying Tan;Zhonghai Wu;Wei Tan	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.147	genetic algorithm;computer science;artificial intelligence;machine learning;data mining;database;world wide web;ubiquitous computing	Robotics	-45.130169326362875	14.830849571606649	140842
432132862cd9803264fddac53c0c7b55f939aff7	qos-driven service selection optimization model and algorithms for composite web services	composite web service;owl;owl s;grounding;application software;service selection;service selection optimization model;web service;global constraint;application integration;software engineering;similar functionality;web service composition;knowledge representation languages;simulation experiment;web services knowledge representation languages quality of service;mathematical model quality of service service selection optimization model web service composition resource sharing application integration similar functionality owl s;resource sharing;web services;mathematical model;software algorithms;ontologies;scalability;web services quality of service mathematical model application software software algorithms ontologies grounding software engineering owl scalability;quality of service;similarity function;optimization model	Web service composition has been recognized as a flexible way for resource sharing and application integration. For Web services providing similar functionality, quality of service (QoS) is the main factor to differentiate them. In this paper, the issue of QoS driven service selection in dynamic Web service composition is investigated. OWL-S is extended with a flexible QoS model to better facilitate service selection process. Based on different global constraints users set, the QoS-based service selection problems for composite services are divided into three categories. For each category, the mathematical model and its corresponding solving algorithm are put forward separately. Simulation experiments are conducted to show the validity and efficiency of the algorithms.	algorithm;experiment;mathematical model;owl-s;quality of service;service composability principle;simulation;web service	Wentao Zhang;Yan Yang;Shengqun Tang;Lina Fang	2007	31st Annual International Computer Software and Applications Conference (COMPSAC 2007)	10.1109/COMPSAC.2007.175	web service;mobile qos;differentiated service;computer science;knowledge management;artificial intelligence;service delivery framework;ws-policy;database;world wide web	Web+IR	-45.013494527750936	14.815633547229432	140875
1ed79e60a2794e53e7f8c0fc6c2f7c65285f9e81	mapmerge: correlating independent schema mappings	different schema;target schema;larger schema mapping;schema mapping composition;source schema;new operator;independent schema mapping;mapmerge algorithm;schema mapping;schema matcher;complex mapping;schema correspondence	One of the main steps toward integration or exchange of data is to design the mappings that describe the (often complex) relationships between the source schemas or formats and the desired target schema. In this paper, we introduce a new operator, called MapMerge, that can be used to correlate multiple, independently designed schema mappings of smaller scope into larger schema mappings. This allows a more modular construction of complex mappings from various types of smaller mappings such as schema correspondences produced by a schema matcher or pre-existing mappings that were designed by either a human user or via mapping tools. In particular, the new operator also enables a new “divide-and-merge” paradigm for mapping creation, where the design is divided (on purpose) into smaller components that are easier to create and understand and where MapMerge is used to automatically generate a meaningful overall mapping. We describe our MapMerge algorithm and demonstrate the feasibility of our implementation on several real and synthetic mapping scenarios. In our experiments, we make use of a novel similarity measure between two database instances with different schemas that quantifies the preservation of data associations. We show experimentally that MapMerge improves the quality of the schema mappings, by significantly increasing the similarity between the input source instance and the generated target instance. Finally, we provide a new algorithm that combines MapMerge with schema mapping composition to correlate flows of schema mappings.	algorithm;experiment;modular design;programming paradigm;similarity measure;synthetic intelligence	Bogdan Alexe;Mauricio A. Hernández;Lucian Popa;Wang Chiew Tan	2010	The VLDB Journal	10.1007/s00778-012-0264-z	data exchange;semi-structured model;computer science;conceptual schema;theoretical computer science;data integration;star schema;data mining;database;database schema	DB	-36.6182588040186	4.964197561497984	141083
35a1a05fd3eb419f15ce50c2bb7421f1072d6cd4	a customer-centric trust evaluation model for personalized service selection		Trust is a very important criterion when service customers select desired Web services from a cluster of Web services with the same function. Most existing trust models cannot effectively implement personalized service selection with regard to consumer preferences and expectations. This paper designs a novel trust management method based on peer-to-peer network and presents a customer-centric trust evaluation model for personalized service selection. The trust evaluation model firstly maintains consumer-to-consumer trust values that are calculated according to preference similarity between customers, secondly gathers ratings on services submitted by other consumers, then synthesizes customer-to-customer trust and these ratings to generate personalized consumer-to-service trust, and finally selects the desired services according to the expected trust levels presented by customers. This paper conducts some experiments to demonstrate the details of service selection. Experimental results show that this model has good applicability to implement personalized service selection. The proposed model well simulates the reality.		Junwei Zhang;Deyu Li;Xiaoqin Fan	2018	Scientific Programming	10.1155/2018/4819195	data mining;speech recognition;computer science;web service	HCI	-45.186330314571194	15.807002743404906	141326
cda270b345d9fefd6eebf404b4d1c5c2772050d9	uniting formal and informal descriptive power: reconciling ontologies with folksonomies	quality assurance;gestion informacion;ontologie;information aggregation;folksonomy;web information management;information organization;web classification schemes;standardisation;organizacion informacion;information management;organisation information;westminster business school;ontologia;ontologies;information modelling;folksonomie;gestion information;folksonomies;ontology	Ontologies and folksonomies are currently the most prominent web content classification schemes. While their roles are similar, their engineering is different. In an attempt to combine and harness their distinct eb classification schemes nformation modelling eb information management powers, web and information scientists are attempting to integrate them, merging the flexibility, collaboration and information aggregation of folksonomies with the standardisation, automated validation and interoperability of ontologies. This paper explores the basics of web information classification engineering, identifies the strengths and weaknesses of the existing methodologies, assesses their effectiveness and investigates a number of key quality issues. It then investigates the existing methods for integrating ontologies and folksonomies and examines the integration requirements. It finally proposes a common ion o framework for reconciliat	folksonomy;information management;information scientist;interoperability;ontology (information science);requirement;web content	Fefie Dotsika	2009	Int J. Information Management	10.1016/j.ijinfomgt.2009.02.002	quality assurance;computer science;knowledge management;ontology;ontology;data mining;information management;world wide web;standardization	Web+IR	-43.850578299647744	6.289258195157888	141379
adcfdfa3a292b3fee183cd815000c9c111c2d553	domain independent learning of ontology mappings	databases;protocols;game theory;ontology mapping;decision making under uncertainty;information retrieval;collaboration;independent learning;opponent modeling;domain knowledge;learning systems;standards development;learning methods;internet;permission;communication standards;artificial intelligence;ontologies;biomedical informatics;languages;computing methodologies;ontologies databases protocols learning systems collaboration communication standards standards development internet permission biomedical informatics	This paper proposes a domain independent method for handling interoperability problems by learning a mapping between ontologies. The learning method is based on exchanging instances of concepts that are defined in the ontologies. The method starts with identifying pairs of instances of concepts denoting the same entity in the world using information retrieval techniques, followed by proposing and evaluating mappings between the ontologies using the pairs of instances. For each step of this method, the likelihood that a decision is correct is taken into account. Important benefits of the method are that (a) no domain knowledge is required, and (b) the structures of ontologies between which a mapping must be established, play no role.	emoticon;information retrieval;interoperability;ontology (information science)	Floris Wiesman;Nico Roos	2004	Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.	10.1109/AAMAS.2004.110	game theory;idef5;communications protocol;the internet;semantic integration;computer science;knowledge management;ontology;artificial intelligence;data mining;information retrieval;domain knowledge;collaboration	AI	-42.63662415607152	13.52147356792487	141454
6bb13f41fb6575a6476126ed2b3231e9bd001e9b	stateful web services -- auto modeling and composition	electronic commerce;standards;composition;wsdl;behavioral modeling;web service;stateful web services web service behavioral modeling composition wsdl;business;unified modeling language;web services;xml;stateful web services wsdl document wsdl file amazon e commerce web service service composition framework web services description language service behavior extraction service vendors informal documentation;communities;web services unified modeling language xml standards documentation business communities;stateful web services;documentation;xml electronic commerce web services	The web service community has introduced many techniques to cope with the inability of WSDL to describe a service's behavior. Those techniques range from embedding more XML tags in WSDL, to generate formal behavioral models on top of WSDL. Apart from the efficiency of these techniques, a common problem is that they require manual efforts to model the behavior of a service, and often need informal documentation from service vendors to do so. In this paper, we propose a solution for the above problem by automatically extracting a service's behavior, directly from its WSDL document. Our approach is based on the utilization of particular WSDL elements, which are usually ignored by bottom-up approaches while generating a WSDL file. We illustrate our process in steps by taking a scenario from Amazon E-commerce Web Service. We also survey the issues with extracting the behavioral models, from the WSDLs of existing web services. Finally, we tested our automatically generated models by composing them together using our service composition framework.	behavioral modeling;bottom-up parsing;documentation;e-commerce;service composability principle;state (computer science);stateful firewall;web services description language;web service;world wide web;xml	Syed Adeel Ali;Partha S. Roop;Ian Warren	2013	2013 IEEE 20th International Conference on Web Services	10.1109/ICWS.2013.46	e-commerce;web service;computer science;database;internet privacy;ws-i basic profile;law;world wide web	DB	-46.18844589202501	17.3489039022394	141475
125630040eb2d3a7d05b08ce9b99b6294e6c27e6	adaptive composition of conversational services through graph planning encoding	user needs;service composition;service oriented computing	Service-Oriented Computing supports description, publication, discovery and composition of services to fulfil end-user needs. Yet, service composition processes commonly assume that service descriptions and user needs share the same abstraction level, and that services have been pre-designed to integrate. To release these strong assumptions and to augment the possibilities of composition, we add adaptation features into the service composition process using semantic structures for exchanged data, for service functionalities, and for user needs. Graph planning encodings enable us to retrieve service compositions efficiently. Our composition technique supports conversations for both services and user needs, and it is fully automated thanks to a tool, pycompose, which can interact with state-of-the-art graph planning tools.	abstraction layer;algorithm;information needs;interoperability;pervasive informatics;requirement;service composability principle;service-oriented software engineering;software deployment;web service	Pascal Poizat;Yuhong Yan	2010		10.1007/978-3-642-16561-0_11	differentiated service;computer science;service delivery framework;service-oriented architecture;service design;database;multimedia;world wide web	HCI	-46.4836490219649	14.79900684180424	141586
1a7959705cd919124299189f6641d327c1076e36	scalable semantic brokering over dynamic heterogeneous data sources in infosleuthtm	facilitation;multibrokering;heterogeneous systems;robustness scalability collaboration query processing information retrieval peer to peer computing computer architecture computer networks ontologies algebra;information agents;multiagents;information discovery;peer to peer system;computer networks;computer networks distributed object management multi agent systems computational linguistics;multi agent systems;design and implementation;agent based system;distributed object management;computational linguistics;heterogeneous systems scalable semantic brokering dynamic heterogeneous data sources infosleuth agent based system information discovery information retrieval dynamic open environment matchmaking process distributed multibroker design explicitly advertised information agent capabilities collaborating agents peer to peer system collective response semantic matching information agents;semantic matching;heterogeneous data sources	InfoSleuth is an agent-based system for information discovery and retrieval in a dynamic, open environment. Brokering in InfoSleuth is a matchmaking process, recommending agents that provide services to agents requesting services. This paper discusses InfoSleuth’s distributed multibroker design and implementation. InfoSleuth’s brokering function combines reasoning over both the syntax and semantics of agents in the domain. This means the broker must reason over explicitly advertised information about agent capabilities to determine which agent can best provide the requested services. Robustness and scalability issues dictate that brokering must be distributable across collaborating agents. Our multibroker design is a peer-to-peer system that requires brokers to advertise to and receive advertisements from other brokers. Brokers collaborate during matchmaking to give a collective response to requests initiated by nonbroker agents. This results in a robust, scalable brokering system.		Marian H. Nodine;Anne H. H. Ngu;Anthony R. Cassandra;William Bohrer	2003	IEEE Trans. Knowl. Data Eng.	10.1109/TKDE.2003.1232266	facilitation;computer science;computational linguistics;database;distributed computing;world wide web	AI	-41.515396094312784	17.443802135690927	141792
2518408471e302c6f7085fe750a77c0b448619c7	how to compose a complex document recognition system	design principle;document analysis;uncertainty;pattern recognition;variability;design principles	The technical challenges in document analysis and recognition have been to solve the problems of uncertainty and variability. From our experiences in developing OCRs, business form readers, and postal address recognition engines, we would like to present design principles to cope with these problems of uncertainty and variability. When the targets of document recognition are complex and diversified, the recognition engine needs to solve many different kinds of pattern recognition problems, which are a reflection of uncertainty and variability. Inevitably, the engine becomes complex, raising a question of how to combine its subcomponents, which are not perfect in their accuracies. The design principles will be explained with examples in postal address recognition.	pattern recognition;postal;spatial variability	Hiromichi Fujisawa	2006		10.1145/1364742.1364759	speech recognition;computer science;artificial intelligence;data mining	Vision	-46.64396908720797	5.542352581316471	141829
c49b8b757af03dc9ad382c57a635ebb7eb9482f0	an infrastructure for e-government based on semantic web services	semantic web service;public information systems;electronic government semantic web web services prototypes collaborative work distributed computing computer science automation information systems communications technology;web service discovery;collaborative work;e government;web service;satisfiability;semantic web;open systems semantic web government data processing public information systems;information system;open systems;government data processing;service composition e government semantic web services governmental information system automated web service discovery	Incomplete automation and lack of collaborative work can be frequently found in the traditional governmental information system. When using the system, a citizen has to discover the involved governmental services to satisfy his requirement and contacts with each of them. Semantic Web services (SWS) in e-government can allow the automated Web service discovery, selection and execution making the automatic composition of governmental services a reality. A citizen can gain the response conveniently after submitting the request to the platform of SWS. In this paper, SWS requirements in e-government are presented, and a conceptual architecture for SWS in e-government is proposed, furthermore a prototype system is designed to implement SWS in e-government.	e-government;information system;prototype;requirement;semantic web service;service discovery;sinewave synthesis	Liuming Lu;Guojin Zhu;Jiaxun Chen	2004	IEEE International Conference onServices Computing, 2004. (SCC 2004). Proceedings. 2004	10.1109/SCC.2004.1358048	web service;web application security;web development;web modeling;data web;web mapping;web design;web standards;computer science;ws-policy;semantic web;web navigation;social semantic web;web page;ws-addressing;semantic web stack;database;web intelligence;web 2.0;world wide web;information retrieval;universal description discovery and integration;semantic analytics	Robotics	-44.3999082097468	13.742149195140017	142044
537dfda03218de8f2ce50afb25fabf79f4d93ca6	building up a class hierarchy with properties by refining and integrating japanese wikipedia ontology and japanese wordnet	japanese wordnet;doddle owl;japanese wikipedia ontology;ontology learning;ontology alignment	Previously, we constructed the Japanese Wikipedia Ontology (JWO) via a semi-automatic process using the Japanese Wikipedia, but it had problems due to a lack of upper classes and appropriate definitions of properties. Thus, the aim of the current study was to complement the upper classes in JWO by refining and integrating JWO and Japanese WordNet (JWN) to build a class hierarchy with defined properties based on the considerations of property inheritance. To achieve this, we developed tools that help users to refine the class-instance relationships, to identify the JWO classes that need to be aligned with JWN synsets, and to align the JWO classes with the JWN synsets via user interaction. We also integrated JWO and JWN using a domain ontology development environment, DODDLE-OWL. We also propose a method for building a class hierarchy with defined properties by elevating the common properties defined in sibling classes to higher classes in JWO.	align (company);class hierarchy;ontology (information science);semiconductor industry;synonym ring;wikipedia;wordnet	Takeshi Morita;Yuka Sekimoto;Susumu Tamagawa;Takahira Yamaguchi	2014	Web Intelligence and Agent Systems	10.3233/WIA-140293	ontology alignment;computer science;data mining;world wide web;information retrieval	AI	-41.90301771813894	5.913477797836626	142104
4c8adecbc2a3201e4aaac062f565d957b0e81688	ontology-based feature mapping and verification between cad systems	computer aided design;semantic web rule language;feature;interoperability;ontology web language	Data interoperability between computer-aided design (CAD) systems remains a major obstacle in the information integration and exchange in a collaborative engineering environment. The use of CAD data exchange standards causes the loss of design intent such as construction history, features, parameters, and constraints, whereas existing research on feature-based data exchange only focuses on class-level feature definitions and does not support instance-level verification, which causes ambiguity in data exchange. In this paper, a hybrid ontology approach is proposed to allow for the full exchange of both feature definition semantics and geometric construction data. A shared base ontology is used to convey the most fundamental elements of CAD systems for geometry and topology, which is to both maximize flexibility and minimize information loss. A three-branch hybrid CAD feature model that includes feature operation information at the boundary representation level is constructed. Instance-level feature information in the form of the base ontology is then translated to local ontologies of individual CAD systems during the rule-based mapping and verification process. A combination of the Ontology Web Language (OWL) and Semantic Web Rule Language (SWRL) is used to represent feature classes and properties and automatically classify them by a reasoner in the target system, which requires no knowledge about	boundary representation;cad data exchange;computer-aided design;feature model;interoperability;logic programming;mental representation;ontology (information science);semantic web rule language;semantic reasoner;web language;web ontology language	Sean Tessier;Yan Wang	2013	Advanced Engineering Informatics	10.1016/j.aei.2012.11.008	natural language processing;upper ontology;interoperability;semantic web rule language;bibliographic ontology;feature;ontology inference layer;computer science;ontology;computer aided design;data mining;database;ontology-based data integration;owl-s;process ontology	AI	-38.19024398978162	7.361032501886215	142154
e5365a1c8131a83093fea642c4055d1f16438788	a classification of trust systems	modelizacion;distributed system;virtual network;reseau social;confiance;systeme reparti;psychologie sociale;trust model;classification;modelisation;social network;confidence;sistema repartido;internet;confianza;psicologia social;social psychology;information system;modeling;clasificacion;systeme information;red social;red virtual;reseau virtuel;sistema informacion	Trust is a promising research topic for social networks, since it is a basic component of our real-world social life. Yet, the transfer of the multi-facetted concept of trust to virtual social networks is an open challenge. In this paper we provide a survey and classification of established and upcoming trust systems, focusing on trust models. We introduce a set of criteria as basis of our analysis and show strengths and short-comings of the different approaches.	context-sensitive language;facet (geometry);hoc (programming language);regret (decision theory);social network;software propagation;trust management (information system)	Sebastian Ries;Jussi Kangasharju;Max Mühlhäuser	2006		10.1007/11915034_114	the internet;systems modeling;biological classification;artificial intelligence;confidence;computational trust;information system;social network	AI	-37.87766928904133	16.870250017297906	142171
7244818f65ac449f606340c2ac0430edf54bdc27	documentation for aircraft maintenance based on topic maps	xml topic maps;topic maps;semantic information	This document is the property of SNOW consortium ; no part of it shall be reproduced or transmitted without the express prior written authorization of SNOW consortium, and its contents shall not be disclosed.	authorization;documentation;topic maps	Kay Kadner;David Roussel	2006		10.1007/978-3-540-71945-8_6	topic maps;computer science;artificial intelligence;database;world wide web;information retrieval	NLP	-40.611913355125075	4.782127686783136	142205
255a2c0605372b45f22eba2aa7f14d9c9f404cc5	using properties for uniform interaction in the presto document system	information management system;document interfaces;interaction models;interaction model;document properties;document management	Most document or information management systems rely on hierarchies to organise documents (e.g. files, email messages or web bookmarks). However, the rigid structures of hierarchical schemes do not mesh well with the more fluid nature of everyday document practices. This paper describes Presto, a prototype system that allows users to organise their documents entirely in terms of the properties those documents hold for users. Properties provide a uniform mechanism for managing, coding, searching, retrieving and interacting with documents. We concentrate in particular on the challenges that property-based approaches present and the architecture we have developed to tackle them.	encode;email;information management;interaction;presto;prototype	Paul Dourish;W. Keith Edwards;Anthony LaMarca;Michael Salisbury	1999		10.1145/320719.322583	well-formed document;document clustering;computer science;document management system;management information systems;data mining;database;world wide web;vision document;design document listing	Web+IR	-42.30535376272536	8.417008001789084	142646
9d9947d1ea15e9e7d6225c36ebff3d478b21eb46	settlebot: a negotiation model for the agent based commercial grid		Market-driven sharing of distributed computational resources requires coordination support that can be provided by distributed problem solving (software agent technology). Multiple-issue negotiation among autonomous software agents allows the efficient alignment of resource consumers’ demand profiles and the service capabilities of resource providers. To address the inefficiencies of negotiations on imperfect markets, the negotiation model suggested by the SettleBot research effort includes both self-interested negotiations driven by a heuristic strategy and a joint-gains approach to win/win-negotiations. While finding joint gains under imperfect information is a well-known problem with approaches relating to simulated annealing as common approximate solutions, self-interested negotiations in a dynamically evolving environment require intelligent agents that retrieve, process and leverage knowledge about the world state. Superior strategy solutions in given market scenarios are identified using a genetic learning algorithm.		Florian Lang	2005			computer science	AI	-42.33347894387925	16.064905733696868	142701
19541a30bf678a8b1dbe3a182956c4938e91174a	automated evaluation of building alignments in generalized maps	building alignments;automated evaluation of generalized data;data matching;map requirement formalization;building pattern recognition;automated generalization	Evaluation is a key step to examine the quality of generalized maps with respect to map requirements. Map generalization facilitates the recognition of pattern generating processes by preserving and highlighting the patterns at smaller scales. This article focuses specifically on the evaluation of building patterns in topographic maps that are generalized from large to mid scales. Currently, there is a lack of knowledge and functionality on automatically evaluating how these patterns are generalized. The issues of the evaluation range from missing formal map requirements on building alignments to missing automated evaluation techniques. This article firstly analyses the requirements constraints related to the generalization of building alignments. Then, it focuses on three more specific constraints, i.e. on existence, orientation of alignments and spatial distribution of composing buildings. Later, a three-step approach is proposed to 1 recognize and 2 match alignments from source and generalized datasets and 3 evaluate building alignments in generalized datasets. Besides, many-to-many and partial matching between initial and target alignments is a side effect of generalization, which reduces the reliability of the evaluation results. This article introduces a confidence indicator to document the reliability and to inform intended users e.g. cartographers and/or systems about the reliability of evaluation decisions. The effectiveness of our approach is demonstrated by evaluating the alignments in both interactively manually generalized maps and automated generalized maps. Finally, we discuss how our approach can be used to control automated generalization and identify further improvements.		Xiang Zhang;Jantien E. Stoter;Tinghua Ai;Menno-Jan Kraak;Martien Molenaar	2013	International Journal of Geographical Information Science	10.1080/13658816.2012.758264	theoretical computer science;machine learning;data mining;mathematics	Robotics	-47.89974307475261	6.016751838090796	142711
d3d22badc7249d8d1cf421d05ba46738049c1736	a conditional lexicographic approach for the elicitation of qos preferences		In a service-oriented environment, clients can usually choose between several web services offering the same functionality. The web service selection can be automated by allowing clients to specify nonfunctional requirements such as quality of service. Clients should also be able to indicate how to make tradeoffs when some of these requirements cannot be met. The ability to capture tradeoff preferences is critical for selecting the best fitting web service. In this paper, we propose a method of expressing non-functional preferences, which requires minimal effort on the part of the clients, but offers great flexibility in managing tradeoffs. This method leads to a simple algorithm for selecting web services, which does not require sophisticated multicriteria decision techniques.	algorithm;lexicographical order;lexicography;non-functional requirement;open-source software;prototype;quality of service;service-oriented device architecture;sourceforge;specification language;web service;world wide web	Raluca Iordache;Florica Moldoveanu	2012		10.1007/978-3-642-33606-5_12	theoretical computer science;database;programming language	Web+IR	-46.78012694182261	16.423944392007375	142847
c628a58cad18d190acac026f1707ebb7de3a9148	a high performance semantic web query answering engine	semantic web;query language	We present an (extensively revised) semantic web query language called nRQL as well as a working high performance implementation of this language in the RacerPro system. We present the features of this query answering engine.	query language;question answering;semantic web;web ontology language	Michael Wessel;Ralf Möller	2005			query expansion;semantic web;web search query;social semantic web;web query classification;information retrieval;database;semantic web stack;sargable;query optimization;computer science	AI	-37.70552309551713	6.61525231860711	143035
6355a5eb7d075cdf707da50b0833637228d9118e	c minor: a semantic publish/subscribe broker for the internet of musical things		Semantic Web technologies are increasingly used in the Internet of Things due to their intrinsic propensity to foster interoperability among heterogenous devices and services. However, some of the IoT application domains have strict requirements in terms of timeliness of the exchanged messages, latency and support for constrained devices. An example of these domains is represented by the emerging area of the Internet of Musical Things. In this paper we propose C Minor, a CoAP-based semantic publish/subscribe broker specifically designed to meet the requirements of Internet of Musical Things applications, but relevant for any IoT scenario. We assess its validity through a practical use case.		Fabio Viola;Luca Turchet;Francesco Antoniazzi;György Fazekas	2018	2018 23rd Conference of Open Innovations Association (FRUCT)	10.23919/FRUCT.2018.8588087	latency (engineering);world wide web;musical;semantics;semantic web;interoperability;the internet;publication;computer science;internet of things	AI	-44.36669028996643	10.87393507578992	143052
96f8554b71eb3145e7eb95c98a51e85ce7fe2558	role of models in knowledge transfer during oo software development	knowledge transfer;efficient knowledge transfer mechanism;semi-formal transfer;conceptual model;hemisphere model;business process model;problem domain knowledge;better utilization;oo software development;formal transfer;problem domain	Formal and semi-formal transfer of knowledge in the conventional object-oriented software development is seriously impaired. This occurs because it is impossible to assure both completeness and consistency of the initial body of knowledge from which the knowledge transfer may commence. In this paper we propose that better utilization of the formal transfer of knowledge requires the focus on the functional aspects of the problem domain to a much higher degree than it is currently customary. We propose that the two hemisphere model based approach, where the problem domain knowledge is expressed in terms of the business process model and the conceptual model, offers effective as well as efficient knowledge transfer mechanism, if sufficiently complete and consistent, explicit and structured, problem domain knowledge is available.	software development	Oksana Nikiforova;Marite Kirikova;Wita Wojtkowski	2005			computer science;systems engineering;knowledge management;body of knowledge;knowledge-based systems;data mining	SE	-45.26166072141154	6.524595348607317	143064
833dd81e290a4ff678cfec77bbfe2f89c07e416f	supporting systematic usage of context in web applications.	context aware;web performance;conceptual model;context model;domain knowledge;web based system;user interaction;domain ontology	Context can be seen as a paradigm aiming to improve user interaction with software. For Web applications in particular, the issues of content explosion and technological constraints need to be addressed better. This paper discusses an approach for engineering contextaware, adaptive Web applications. This approach integrates context knowledge with domain ontologies, and allows adaptation to be specified for each aspect of application generation. We further describe CATWALK, our implemented framework, which makes use of the context models to enable adaptivity in applications.	ontology (information science);programming paradigm;web application	J. Wolfgang Kaltz;Jürgen Ziegler	2006			web service;web application security;web development;web modeling;data web;web analytics;web design;web standards;computer science;knowledge management;artificial intelligence;conceptual model;semantic web;web navigation;social semantic web;semantic web stack;database;context model;web intelligence;world wide web;domain knowledge	SE	-43.33423726754479	11.071089627714425	143127
46faf194c1fb40320843a9a4a9c7469bfa158fab	mapping discovery in p2p databases: a query-based approach	p2p	In this paper, we investigate a query-based approach to mapping discovery in peer to peer databases. The approach we propose does not require additional information about schemas of peers. The main idea behind our design is that even if the schemas are structurally and syntactically di erent, queries can be similar in the sense that they return semantically related objects. The discovery process is the following: given two queries Q1 and Q2 over schemas S1 and S2, respectively, (1) we analyze relationships between attributes appearing in Q1 and Q2, and (2) we propagate the relationships over the remaining attributes of S1 and S2.	database;database schema;experiment;peer-to-peer;synthetic intelligence	Lyes Limam;Salima Benbernou;Mohand-Said Hacid;Arantza Illarramendi;Aris M. Ouksel	2008			computer science;business process discovery;data mining;information retrieval;peer-to-peer;database;schema (psychology)	DB	-35.01662176088509	4.826496231509122	143471
7d235af19fe99d0e2f50c3b18cd098b661dd60d4	sifter-ii: a heterogeneous agent society for information filtering	heterogeneous agents;information filtering;collaboration;information overload;agents;wide area network	A direct consequence of today’s interconnected world is the phenomenal growth in the amount of available and accessible information. This information overload is going to increase as more machines will be connected to wide area networks. Thus, an effective and efficient information filtering system is needed, which will weed out unwanted information and present the relevant information in a rank-ordered fashion. This paper describes one such distributed information filtering system consisting of collaborating heterogeneous agents.	information filtering system;information overload;interconnectedness	Rajeev R. Raje;Mingyong Qiao;Snehasis Mukhopadhyay	2001		10.1145/372202.372292	computer science;software agent;information filtering system;information overload;data mining;multimedia;management;world wide web;collaboration	ML	-46.10910764587797	10.333275836872811	143875
fd822fcaad4559cc3a661b5cb810e20da01d902c	shark - a system for management, synchronization and exchange of knowledge in mobile user groups	topic maps;knowledge management;shared knowledge;semantic web;communication protocol;information system;peer to peer;knowledge work;mobile user	New wireless protocols like W-LAN and Bluetooth allow establishing spontaneous networks and peer-to-peer exchange of information. At the same time standards like Semantic Web and Topic Maps gain acceptance that add semantics to information. This paper introduces Shark. Shark is an acronym and stands for “Mobile Shared Knowledge”. Shark organizes knowledge with help of Topic Maps, synchronizes knowledge inside closed user groups but also enables a peer-to-peer exchange of knowledge by means of Bluetooth. This paper gives an overview of the system and its communication protocols.	bluetooth;communications protocol;map;peer-to-peer;shark;semantic web;spontaneous order;topic maps	Thomas Schwotzer;Kurt Geihs	2002	J. UCS	10.3217/jucs-008-06-0644	topic maps;communications protocol;knowledge base;computer science;knowledge management;artificial intelligence;operating system;knowledge-based systems;semantic web;open knowledge base connectivity;knowledge extraction;personal knowledge management;world wide web;information system;domain knowledge	Web+IR	-43.759300200423134	10.043854384687597	143948
23864093ff244b3725c5713bc170b1553768e9ee	provenance trails in the wings/pegasus system		Our research focuses on creating and executing large-scale scientific workflows that often involve thousands of computations over distributed, shared resources. We describe an approach to workflow creation and refinement that uses semantic representations to 1) describe complex scientific applications in a data-independent manner, 2) automatically generate workflows of computations for given data sets, and 3) map the workflows to available computing resources for efficient execution. Our approach is implemented in the Wings/Pegasus workflow system and has been demonstrated in a variety of scientific application domains. This paper illustrates the application-level provenance information generated Wings during workflow creation and the refinement provenance by the Pegasus mapping system for execution over grid computing environments. We show how this information is used in answering the queries of the First Provenance Challenge.	application domain;computation;grid computing;mathematical optimization;pegasus;refinement (computing);software propagation	Jihie Kim;Ewa Deelman;Yolanda Gil;Gaurang Mehta;Varun Ratnakar	2008	Concurrency and Computation: Practice and Experience	10.1002/cpe.1228		HPC	-36.56152457884159	17.6627896622603	143960
b901ffca63868cc3093d4fffe2a3de5567d53406	neurogrid: semantically routing queries in peer-to-peer networks	multiagent system;reseau pair;red www;metadata;peer to peer network;routing;reseau web;routage;semantics;semantica;semantique;peer to peer p2p;internet;metadonnee;distributed search;world wide web;metadatos;sistema multiagente;systeme multiagent;open source;enrutamiento	NeuroGrid is an adaptive decentralized search system. NeuroGrid nodes support distributed search through semantic routing (forwarding of queries based on content), and a learning mechanism that dynamically adjusts metadata describing the contents of nodes and the files that make up those contents. NeuroGrid is an open-source project, and prototype software has been made available at http://www.neurogrid.net/ NeuroGrid presents users with an alternative to hierarchical, folder-based file organization, and in the process offers an alternative approach to distributed search.	distributed web crawling;john d. wiley;justin (robot);neurogrid;open-source software;peer-to-peer;prototype;raph levien;routing;tf–idf	Sam Joseph	2002		10.1007/3-540-45745-3_18	routing;the internet;computer science;database;distributed computing;semantics;metadata;world wide web	OS	-38.69432893642254	12.115086469675957	143976
f42df062cc4cb43368a429de6df11a8bc488973d	i18n of semantic web applications	regional difference;semantic technologies;semantic web technology;semantic web;knowledge base	Recently, the use of semantic technologies has gained quite some traction. With increased use of these technologies, their maturation not only in terms of performance, robustness but also with regard to support of non-latin-based languages and regional differences is of paramount importance. In this paper, we provide a comprehensive review of the current state of the internationalization (I18n) of Semantic Web technologies. Since resource identifiers play a crucial role for the Semantic Web, the internatinalization of resource identifiers is of high importance. It turns out that the prevalent resource identification mechanism on the Semantic Web, i.e. URIs, are not sufficient for an efficient internationalization of knowledge bases. Fortunately, with IRIs a standard for international resource identifiers is available, but its support needs much more penetration and homogenization in various semantic web technology stacks. In addition, we review various RDF serializations with regard to their support for internationalized knowledge bases. The paper also contains an in-depth review of popular semantic web tools and APIs with regard to their support for internationalization.	align (company);internationalization and localization;knowledge engineer;rdf/xml;resource description framework;robustness (computer science);semantic web;semantic publishing;serialization;traction teampage;utf-8;uniform resource identifier;web application;xml	Sören Auer;Matthias Weidl;Jens Lehmann;Amrapali Zaveri;Key-Sun Choi	2010		10.1007/978-3-642-17749-1_1	knowledge base;semantic computing;web modeling;semantic integration;data web;semantic search;semantic grid;web standards;computer science;knowledge management;artificial intelligence;semantic web;social semantic web;linked data;data mining;semantic web stack;database;web resource;web intelligence;semantic technology;world wide web;semantic analytics	Web+IR	-41.29217289342235	7.832436634965075	143977
6e8c4534daf23e372559f6d0201875fb2f3bc290	socially targeted mobile services: towards an upper level ontology of social roles for mobile environments	social roles;ontologie;content provisioning;semantic web;mobile services	The Semantic Web has recently attracted the attention of both researchers and practitioners in the information systems field. In this paper we explore an application of Semantic Web in mobile context. In particular we focus on the development of advanced models of mobile service provision by contextualizing users’ interaction characteristics through an upper level annotation ontology. The Semantic Web enables richer representations of context, which combined with the interaction and communication modalities attached to the device enrich user experience. We propose an ontological representation of user stereotypes through the concept of social roles. This representation can be used in further research on content negotiation and deployment of different interaction styles that are subject to the stereotype. Key-Words: Semantic Web; Social Roles; Mobile Services; Content Provisioning; Ontologies.	content negotiation;information system;ontology (information science);provisioning;semantic web;software deployment;upper ontology;user experience	Nikolaos Korfiatis;Ioanna D. Constantiou	2006			computer science;knowledge management;role;semantic web;social semantic web;semantic web stack;multimedia;world wide web;owl-s	Web+IR	-43.94183772421817	10.469564158600253	144360
948636f838ec1ba5fc2cb7f245cad4ae58516415	incomplete preference-driven web service selection	web service selection;total order;oceans;qualitative graphical representation tool;cp nets;history;cp nets web service selection qualitative graphical representation tool;service selection;information technology;distributed computing;user preferences;web service;preference services selection;web service composition;software architecture;graphical representation;software reusability;web services;artificial intelligence;incomplete preferences;preference;computer science;services selection;context modeling	Selecting services from those available according to user preferences plays a important role in Web services composition. Current solutions for service selection focus on selecting services quantitatively. In many domains it is desirable to assess such preference in a qualitative rather than quantitative way. Furthermore, in many practical situations, if the user is reluctant to provide complete preference information or to totally order the values of each attributes in every possible context, it is natural to ask how to model in these incompletely specified cases. This paper uses a qualitative graphical representation tool, called CP-nets, to describe qualitative preference relations in a relatively compact, intuitive, and structured manner under conditional ceteris paribus (all else being equal) preference statements. Our goal is to select the optimal set of services available according to user preference with this technique. Experiment results demonstrate the effectiveness and performance of this approach.	petri net;user (computing);web service	Hongbing Wang;Junjie Xu;Peicheng Li	2008	2008 IEEE International Conference on Services Computing	10.1109/SCC.2008.49	preference learning;computer science;knowledge management;data mining;world wide web	Robotics	-44.608223461546245	15.469694343946085	144707
a987210d48d8374c9947246014921064c277fec8	squall: the expressiveness of sparql 1.1 made available as a controlled natural language		The Semantic Web (SW) is now made of billions of triples, which are available as Linked Open Data (LOD) or as RDF stores. The SPARQL query language provides a very expressive way to search and explore this wealth of semantic data. However, user-friendly interfaces are needed to bridge the gap between end-users and SW formalisms. Navigation-based interfaces and natural language interfaces require no or little training, but they cover a small fragment of SPARQL's expressivity. We propose SQUALL, a query and update language that provides the full expressiveness of SPARQL 1.1 through a flexible controlled natural language (e.g., solution modifiers through superlatives, relational algebra through coordinations, filters through comparatives). A comprehensive and modular definition is given as a Montague grammar, and an evaluation of naturalness is done on the QALD challenge. SQUALL is conceived as a component of natural language interfaces, to be combined with lexicons, guided input, and contextual disambiguation. It is available as a Web service that translates SQUALL sentences to SPARQL, and submits them to SPARQL endpoints (e.g., DBpedia), therefore ensuring SW compliance, and leveraging the efficiency of SPARQL engines.	controlled natural language;sparql	Sébastien Ferré	2014	Data Knowl. Eng.	10.1016/j.datak.2014.07.010	natural language processing;named graph;turtle;computer science;sparql;data mining;database;rdf query language;programming language	DB	-36.011564040727926	5.850625847204361	144821
d04209e355044b366bb777f9281573ffe7e905ba	a publish/subscribe framework: push technology in electronic commerce	commerce electronique;electronic commerce;systeme intelligent;navegacion informacion;comercio electronico;e commerce;navigation information;sistema inteligente;resource manager;information browsing;object oriented;publish subscribe;intelligent system;base donnee orientee objet;object oriented databases;information system;systeme information;electronic trade;sistema informacion	Publish/Subscribe (P/S) technology, conceptually divides resource managers (RM) and applications within a transaction tree into two categories: resource producers and resource consumers. Publishers enqueue information and the P/S system pushes it to the subscribers. Applying the P/S paradigm to the Internet-based Electronic Commerce (sometimes known as Webcasting), the RM's that produce and consume information can be real people. In this case, customers subscribe voluntarily or unknowingly to provide merchants their own information. Based on these customer profiles which may include information about their household or buying patterns, companies make decisions to push products and marketing messages to the relevant prospects electronically. In this paper, OMM, an object-oriented organizational model, is presented as an underlying model to support P/S in E-Commerce. Firstly, it allows companies to flexibly model both the consumers and the materials to be consumed. Secondly, it allows users to specify business policies in SQL-like queries to match the consumers with the materials. Every time a new material is published, OMM automatically pushes it to the right audiences. The paper also discusses our experience of applying the research prototype, OMM/PS, to support the E-Commerce strategy of a commercial insurance firm.	e-commerce;push technology	Edward C. Cheng	1999		10.1007/978-3-540-46652-9_56	e-commerce;computer science;artificial intelligence;operating system;database;distributed computing;publish–subscribe pattern;object-oriented programming;world wide web;computer security;information system	DB	-37.455174162781695	14.694384005334522	144829
750a1afff7a266f34cc6d5076d304b4d11705da2	explaining structured queries in natural language	databases;directed graphs;sql database management systems directed graphs natural languages query processing;graph edges;textual query descriptions natural language naive users database accessing structured query language user interactions query translation problem directed graphs graph edges template mechanism graph traversal strategies;query processing;database management systems;sql;semantics;natural languages;query translation problem;usa councils;user interactions;database accessing;natural languages database languages visual databases speech analysis computer science informatics telecommunications qualifications resource description framework automatic speech recognition;structured query language;textual query descriptions;directed graph;natural language;template mechanism;joining processes;graph traversal strategies;query translation;user interaction;database languages;containers;naive users	Many applications offer a form-based environment for naïve users for accessing databases without being familiar with the database schema or a structured query language. User interactions are translated to structured queries and executed. However, as a user is unlikely to know the underlying semantic connections among the fields presented in a form, it is often useful to provide her with a textual explanation of the query. In this paper, we take a graph-based approach to the query translation problem. We represent various forms of structured queries as directed graphs and we annotate the graph edges with template labels using an extensible template mechanism. We present different graph traversal strategies for efficiently exploring these graphs and composing textual query descriptions. Finally, we present experimental results for the efficiency and effectiveness of the proposed methods.	abstraction layer;algorithm;database schema;directed graph;entity;experiment;graph traversal;interaction;naivety;natural language;query language;sql;tree traversal	Georgia Koutrika;Alkis Simitsis;Yannis E. Ioannidis	2010	2010 IEEE 26th International Conference on Data Engineering (ICDE 2010)	10.1109/ICDE.2010.5447824	natural language processing;sql;directed graph;computer science;data mining;database;semantics;natural language;programming language;graph database;query language;spatial query	DB	-34.35792448385353	5.2460722076240875	145025
bcec24999e9dc03ecdc9b83c366b3834b57ec3f1	a computational intelligence method for traversing dynamically constructed networks of knowledge	self organisation;knowledge network;network simulation;computational intelligence;network simulator;self adaptation;autonomic system;autonomous;knowledge;large scale;network;knowledge organization	A core goal for autonomous systems such as proposed here is automated collaboration in order to perform tasks or share information. The system is always distributed by default and frequently on a large-scale. It can be argued that robustness and economy demand the deployment of a tested autonomic supporting infrastructure whenever possible. A knowledge network is a generic structure that organises distributed knowledge of any format into a system that will allow it to be retrieved efficiently. The rationale of the knowledge network is to act as a middle layer that connects to a multitude of sources, organises them based on various concepts and finally provides well-structured, pre-organised knowledge to individual services and applications. To use the knowledge network we need a querying mechanism to be able to retrieve information. The knowledge network will organise itself in an autonomous manner and it is possible to use the querying mechanism also as part of the knowledge organization mechanism, to autonomously create temporary views that reflect the use of the system. This paper is an attempt to investigate the peculiarities of node behaviour in traversing such a knowledge network. We investigate a variety of methods of traversing a knowledge network.		Kevin Curran;Matthias Baumgarten;Maurice D. Mulvenna;Chris D. Nugent;Kieran Greer	2009	Telecommunication Systems	10.1007/s11235-008-9128-7	organizational network analysis;knowledge base;network formation;computer science;knowledge management;artificial intelligence;mathematical knowledge management;knowledge-based systems;computational intelligence;open knowledge base connectivity;data mining;network simulation;knowledge extraction;computer security;domain knowledge;computer network	AI	-37.52315235049427	15.54910841851691	145047
524b0fc9fdb1fbc855d791e376466414a7f1b37d	role-based access control for model-driven web applications	shdm;access control model;rbac;semantic web;ontology	The Role-based Access Control (RBAC) model provides a safe and efficient way to manage access to information of an organization, while reducing the complexity and cost of security administration in large networked applications. However, Web Engineering frameworks that treat access control models as first-class citizens are still lacking so far. In this paper, we integrate the RBAC model in the design method of Semantic Web applications. More specifically, this work presents an extension of the SHDM method (Semantic Hypermedia Design Method), where these access control models were included and seamlessly integrated with the other models of this method. The proposed model allows the specification of semantic access control policies. SHDM is a model-driven approach to design Web applications for the Semantic Web. This extension was implemented in the Synth environment, which is an application development environment that supports designs using SHDM	freedom of information laws by country;hypermedia;model-driven architecture;model-driven integration;role-based access control;run time (program lifecycle phase);semantic web;software development kit;web application;web engineering	Mairon Belchior;Daniel Schwabe;Fernando Silva Parreiras	2012		10.1007/978-3-642-31753-8_8	web modeling;computer science;semantic web;ontology;social semantic web;role-based access control;data mining;semantic web stack;database;world wide web	SE	-43.409158022324085	11.388526317299975	145096
7a24e5ca42a35a71eac9696bb564b49187c8c853	learning knowledge rich user models from the semantic web	user modelling;red www;reseau web;semantics;intelligence artificielle;semantica;semantique;internet;comportement utilisateur;semantic web;artificial intelligence;world wide web;inteligencia artificial;user behavior;comportamiento usuario;user model;structured data	The Semantic Web [2] is a vision in which today’s Web will be extended with machine readable content, and where every resource will be marked-up using machine readable metadata. The intention is that documents on the Semantic Web will convey real meaning by using structured data-formats and by referring to common ontologies. In our research we wish to explore the impact such a Semantic Web would have on personalisation and user modelling. We investigate how a language with a well-defined semantic data model could be used for describing instances for learning user preferences, how this affects the learning process, and also how it could be used for describing the learned user models.	human-readable medium;ontology (information science);personalization;semantic web;semantic data model;user (computing)	Gunnar Aastrand Grimnes	2003		10.1007/3-540-44963-9_60	semantic computing;web modeling;the internet;user modeling;semantic web rule language;data web;semantic search;semantic grid;data model;web standards;computer science;artificial intelligence;semantic web;web navigation;social semantic web;semantic web stack;database;semantics;multimedia;world wide web;semantic analytics	Web+IR	-37.84564369618967	11.841354057763288	145195
b5dc68e17b10efd3e1ce0c66a8f3079d09a6271c	modelo ontológico para representar información sobre la práctica profesional en una institución educativa		This article presents the design of an ontological model for the search of information within a higher education institution. This model aims to provide answers about the processes of enrollment and release of professional practices, that is, the requirements that must be carried out by the student to perform this type of procedures in the educational institution. In this work the design phases of the methodology proposed by Grninger and Fox’s for the manual creation of the ontology are followed; also includes the proposal of a scenario, questions of competence, the definition of classes, properties, formalization and evaluation through the answers obtained through the SPARQL query language.	linear algebra;ontology (information science);query language;requirement;sparql;unique name assumption	Juan Carlos Flores Molina;Mireya Tovar;Ana Patricia Cervantes Márquez	2017	Research in Computing Science			SE	-44.474094163166214	4.301485115286513	145551
628d581c5939591b708c524792b884a6a04545e4	ontology to represent similarity relations between public web services	automated ontology population;similarity relations discovery;web services;structural similarity measures;inference	Currently Internet is largely populated with Web services offered by different providers and published in various Web repositories. However, public available Web services still suffer from problems that have been widely discussed, such as the lack of functional semantics. This lack of semantics makes very difficult the automatic discovery and invocation of public Web services, even when the system integrator can obtain a copy of the WSDL file. This paper describes an ontological approach for discovering similarity relations between public Web services. The objective of this work is to extract relevant data that is coded into service descriptions, calculate similarity measures between them, represent discovered similarities in an ontological form, and execute inference. Experimental results show that the overall process towards the automation of public Web services discovery based on ontology population and structural similarity measures is feasible.	web service	Maricela Bravo	2011		10.1007/978-3-642-25126-9_55	web modeling;data web;web mapping;web standards;computer science;ws-policy;data mining;ws-addressing;web intelligence;ws-i basic profile;world wide web;owl-s;information retrieval	NLP	-44.45303487278446	13.582905443602584	145715
f08ae550d102c559a50e8ead1cf14517dc58c0e2	metadata representation and risk management framework for preservation processes in av archives	hd61 risk management;qa75 electronic computers computer science	This paper proposes an approach to assessing risks related to audiovisual (AV) preservation processes through gathering and representing the metadata needed for performing this assessment. We define a model for process metadata, which is interoperable with business process models and other preservation metadata formats. We propose a risk management framework to help key decision makers to plan and to execute preservation processes in a manner that reduces the risk of ‘damage’ to audiovisual content. The framework uses a plan, do, check, act cycle to continuously improve the process. The process metadata serves as the interface between the steps in the framework and enables a unified approach to data gathering from the heterogeneous tools and devices used in an audiovisual preservation workflow.	business process;interoperability;risk management framework;risk measure	Werner Bailer;Martin Hall-May;Galina Veres	2014			data management plan;computer science;data mining;database;world wide web;metadata repository	HPC	-47.82313612295007	10.014367071472037	145730
8e27143b03531796810a23e10946c98fc21accc2	towards a systematic approach for heterogeneous web service messages		Establishing semantic interoperability between the heterogeneous messages of Web services has been a critical issue since the emergence of Web service. So far, many approaches that are aimed at solving semantic conflicts to achieve semantic interoperability of heterogeneous messages of Web service have been proposed. However, despite the significant contributions of the current approaches, semantic conflicts remain the critical problem that prevents exchanging the data between heterogeneous messages seamlessly. In this paper, we propose a new systematic approach to solve semantic conflicts of heterogeneous messages. The proposed approach revolves around three steps; semantic conflict identification, detection and solution.	web service	Ibrahim Ahmed Al-Baltah;Abdul Azim Abdul Ghani;Wan Nurhayati Wan Ab. Rahman;Rodziah Binti Atan	2012		10.1007/978-3-319-11629-7_7	web service	Crypto	-43.914924700051074	9.230413166457124	146007
0b37f8be930c50ae39aa6469bdbb0e485e9e4c9d	an agent-based search engine based on the internet search service on the corba	information resources;search engine;agent based;search engines;corba;internet search service;web search engine;software agents;search engines web and internet services web sites world wide web ear electrical capacitance tomography programming profession computer networks helium web search;heterogeneous search engine agent;distributed object management;heterogeneous search engine agent agent based search engine internet search service corba world wide web web search engines multi engine search service meta broker common object service specification application programs queried yahoo altavista;software agents search engines information resources distributed object management;world wide web	Search service is an important tool in the World Wide Web. In general, these standard web search engines are far from idea. Many researchers have therefore implemented Multi-Engine Search Service (MESS) using meta-broker. However these MESS are difficult to integrate a new search engine. On the other hand, the applications that need search service ability are also difficult using these MESS. In this paper, we propose an Internet Search Service (ISS) based on the CORBA. We follow the style of Common Object Service Specification to define the interface of ISS, so that it is not only easily to integrate any search engine into multi-search services, but also can be queried by application programs. In addition, two search engine agents are implemented in our project, one is for Yahoo and the other is for AltaVista. Programmers can use this interface to coding their search engine agent or to query search service in their applications. Finally, we build a heterogeneous search engine agent based on this	agent-based model;common object request broker architecture;programmer;web search engine;world wide web	Yue-Shan Chang;Hsin-Chun Hsieh;Shyan-Ming Yuan;Winston Lo	1999		10.1109/DOA.1999.793979	content farm;search-oriented architecture;cloaking;web service;search engine indexing;database search engine;online presence management;metasearch engine;web search engine;semantic search;computer science;spamdexing;web crawler;data mining;database;search analytics;web search query;world wide web;link farm;search engine	Web+IR	-41.24278061307284	13.205754270952857	146248
8509fac03454ea34ed22404eeab70e845f162212	design of a qos-aware service composition and management system in peer-to-peer network aided by devs	computers;rt devs model qos aware service composition peer to peer network qos aware service management service oriented architecture soa web services p2p network distributed network technology jxta flat based service composition flat based service management;qos aware service management;service composition;pediatrics;management system;peer to peer network;peer to peer computing service oriented architecture technology management disaster management computer simulation real time systems research and development management engineering management environmental management information technology;distributed networks;rt devs model;service management;qos aware service composition;p2p;soa;web service;satisfiability;software architecture;p2p network;computer network management;web services;jxta;success rate;xml;mathematical model;distributed network technology;p2p networks;flat based service composition;peer to peer computing;quality of service;service oriented architecture;peer to peer;flat based service management;web services computer network management peer to peer computing quality of service software architecture;real time systems	QoS-aware service management and composition has become an interesting research topic with the rapid development of service oriented architecture (SOA). Differently with Web-services based systems, the emergence of peer-to-peer (P2P) based distributed network technology brings more challenge to the QoS-aware service management and composition. In this paper, we propose our design of a QoS-aware hierarchical service composition and management system in a context of JXTA-enabled P2P network. We conducted a comparison experiment of our design with commonly used flat-based service composition and management, and found that our design outperforms the flat-based one in terms of a higher success rate for satisfying the user's QoS requirement. Furthermore, we used a RT-DEVS model based formal approach to validate our design, and believe that it can be a promising technology in aiding the design of an efficient QoS-aware service composition and management system.	devs;emergence;jxta;management system;peer-to-peer;quality of service;sp-devs;service composability principle;service-oriented architecture	Hengheng Xie;Azzedine Boukerche;Ming Zhang;Bernard P. Zeigler	2008	2008 12th IEEE/ACM International Symposium on Distributed Simulation and Real-Time Applications	10.1109/DS-RT.2008.40	element management system;computer science;database;world wide web;computer network	EDA	-47.04793470942381	17.302526117489467	146491
f9b268e86bdd1cafee7110fb7a670dac8d45f7ec	are all classes created equal? increasing precision of conceptual modeling grammars		Recent decade has seen a dramatic change in the information systems landscape that alters the ways we design and interact with information technologies, including such developments as the rise of business analytics, user-generated content, and NoSQL databases, to name just a few. These changes challenge conceptual modeling research to offer innovative solutions tailored to these environments. Conceptual models typically represent classes (categories, kinds) of objects rather than concrete specific objects, making the class construct a critical medium for capturing domain semantics. While representation of classes may differ between grammars, a common design assumption is what we term different semantics same syntax (D3S). Under D3S, all classes are depicted using the same syntactic symbols. Following recent findings in psychology, we introduce a novel assumption semantics-contingent syntax (SCS) whereby syntactic representations of classes in conceptual models may differ based on their semantic meaning. We propose a core SCS design principle and five guidelines pertinent for conceptual modeling. We believe SCS carries profound implications for theory and practice of conceptual modeling as it seeks to better support modern information environments.	business analytics;category theory;contingency (philosophy);database;information system;nosql;relevance;user-generated content	Roman Lukyanenko;Binny M. Samuel	2017	ACM Trans. Management Inf. Syst.	10.1145/3131780	conceptual model (computer science);business analytics;information system;conceptual model;nosql;syntax;machine learning;semantics;natural language processing;computer science;rule-based machine translation;artificial intelligence	SE	-46.90004824050056	6.325304941871272	146543
7d2298844fccdcf2f79d66eefb6c36b23a773c89	algorithms for extracting adjacency matrix based on formal concept analysis (fca)		The desire to achieve a holistic representation of Information Retrieval (IR) with the aim for a human-oriented form of representation has spurred the growth of concept-based IR search techniques such as the Semantic Web technology. However, Semantic Web calls for the use of ontologies for many domains. Although meaningful and important, ontology development presents great challenges to the developers especially in terms of conceptual dynamics.. This paper is based on a study that attempts to provide an alternative to ontology lookup for Semantic information retrieval. However, the focus of the paper is on a method proposed to extract adjacency matrix from concepts obtained from the theory of Formal Concept Analysis (FCA) using two consecutive algorithms called the Relatedness Algorithm and Adjacency Matrix Algorithm. Consequently, the adjacency matrices obtained could be used in a similarity measure process based on graph theory. The proposed method offers an alternative to specific domain ontology look-up where results from the measure can further be used in concept-based IR process.	adjacency matrix;algorithm;formal concept analysis;graph theory;holism;information retrieval;lookup table;ontology (information science);semantic web;similarity measure	Hasni Hassan;Mohd Isa Awang;Mokhairi Makhtar;Aznida Hayati Zakaria;Rohana Ismail;Fadhilah Ahmad	2017		10.1145/3018896.3018952	adjacency matrix;graph theory;ontology (information science);ontology;semantic web;similarity measure;computer science;formal concept analysis;algorithm	Web+IR	-39.32521196341284	5.141272598709002	146715
f2a671b4204922727a23b644d8458f4bd3c5eba4	extending the methods for computing the importance of entity types in large conceptual schemas	conceptual schema	Visualizing and understanding large conceptual schemas requires the use of specific methods. These methods generate clustered, summarized, or focused schemas that are easier to visualize and understand. All of these methods require computing the importance of each entity type in the schema. In principle, the totality of knowledge defined in the schema could be relevant for the computation of that importance but, up to now, only a small part of that knowledge has been taken into account. In this paper, we extend seven existing methods for computing the importance of entity types by taking into account more relevant knowledge defined in the structural and behavioural parts of the schema. We experimentally evaluate the original and extended versions of these methods with three large real-world schemas. We present the two main conclusions we have drawn from the experiments.	cardinality (data modeling);computation;computer cluster;conceptual schema;entity;experiment;fixed-point iteration;global motion compensation;link analysis;olami–feder–christensen model;pagerank;partial template specialization;universal product code	Antonio Villegas;Antoni Olivé	2010	J. UCS	10.3217/jucs-016-20-3138	computer science;knowledge management;conceptual schema;data mining;database	DB	-36.81752862503189	4.386108170153863	146916
718c8fde0e8cebb13a325162ba9eb0188683ed74	architecture oriented towards the management of learning objects repositories (lor@)	objects repository;learning objects repositories;unified modeling language;different layer;research group;different standard;formal specification;least squares approximation;uml;data structures;relational databases;environmental management;computer science;computer architecture;data models	This paper describes an architecture oriented towards the usage of learning objects repositories (LOR@). LOR@ are logically organized through different layers constituted by classes, services, components and diagrams which are specified through a Unified Modeling Language (UML). The design is highly flexible, based on a data distributed model and oriented towards the management of different standards. It can be reused freely and enriched by institutions or research groups which develop applications for organizing and managing learning objects repositories	diagram;organizing (structure);unified modeling language	Leonel Iriarte Navarro;Manuel Marco Such;Daniel Morón Martin;Pedro Pernías Peco	2006	Sixth IEEE International Conference on Advanced Learning Technologies (ICALT'06)	10.1109/ICALT.2006.80		Robotics	-43.15122489850468	9.692313114937436	146921
62b16325944bb43b974005ec898f2a2c75feacd7	phosphorus: a task-based agent matchmaker	e commerce;middle agents;trust model;phosphorus;facilitator;declarative languages;kqml;description logic;domain ontology	PHOSPHORUS is an agent matchmaking service that exploits domain ontologies, description logic, and a highly declarative language to reason about task-related agent capabilities. PHOSPHORUS uses the EXPECT goal language to represent the tasks that agents are capable of accomplishing, as well as requests to locate agents with a required capability. PHOSPHORUS supports matching through subsumption, reverse subsumption, and several kinds of goal reformulation.	declarative programming;description logic;expect;ontology (information science);subsumption architecture	Yolanda Gil;Surya Ramachandran	2001		10.1145/375735.376015	e-commerce;phosphorus;description logic;computer science;knowledge management;artificial intelligence	AI	-42.59987335250963	14.310790028581247	146980
ce9f59a823917d319ac91ec6f8ac438e79651a01	a tool for producing structured interoperable data from product features on the web	web scraping;information extraction;protege;rich snippets;ontology;goodrelations	This paper introduces a tool that produces structured interoperable data from product features, i.e., attribute name–value pairs, on the web. The tool extracts the product feaproduct types and their features. The final output of the tool is GoodRelations snippets, which contain product features encoded in RDFa or Microdata. These snippets can be embedded into existing static and dynamic web pages in a way accessible to major search engines like Google and Yahoo, mobile applications, and browser extensions. This increases the visibility of your products and services in the latest generation of search engines, recommender systems, and other novel applications. & 2015 Elsevier Ltd. All rights reserved.	application programming interface;browser extension;database;dynamic web page;e-commerce;embedded system;entity;google chrome;html;input/output;interoperability;json;java;microdata (html);microformat;mobile app;plug-in (computing);portable document format;protégé;rdfa;recommender system;semiconductor industry;software agent;table (information);template processor;universal product code;web search engine;yet another	Tugba Özacar	2016	Inf. Syst.	10.1016/j.is.2015.09.002	microdata;static web page;web modeling;computer science;semantic web;ontology;web page;data mining;database;world wide web;information extraction;information retrieval	Web+IR	-40.18531386402352	9.956846460635225	146998
522c7795ce25699691c664ae89da281a1c01832f	accessing multilingual, heterogeneous data sources in wide area networks - requirements and approach			requirement	Ralf Kramer;Ralf Nikolai	1996			data mining;database;computer science;wide area network	ML	-38.96295077872323	6.6510849973948325	147199
f1316ac396d0030381844fbf495b60f50465a804	development of a model for manipulation of interoperable semi-structured & unstructured data over distributed networks			interoperability;semiconductor industry	Alison Hull	2003			interoperability;database;computer science;distributed design patterns;distributed computing;unstructured data	DB	-37.82883478180404	8.728149393554874	147673
103327e9dc742e1cde52448c7f7904d6bd720af2	an interaction instance oriented approach for web application integration in portals	information resources;portals;web pages;application software;web application integration;application integration;software engineering;computer applications;portal stems;html;internet;aggregates;portals internet;portals web pages application software software engineering internet information resources standardization html aggregates computer applications;interaction instance oriented approach;web pages interaction instance oriented approach web application integration portal stems portlet;standardization;portlet	Currently, the significance of a portal stems not only from being a handy way to access data but also from being the means of facilitating the integration with Web applications. This paper proposes an interaction instance oriented approach for integrating web applications in portals. The key aspect is to enable a user to have the same interaction experiences from a portal that he/she accesses the web application directly. The approach is thus focused on the description of the presentation layer of the interaction instances of a web application and a portlet, which defines an interaction instance as consecutive web pages or fragments. Web application integration is then transformed to the problem that how to translate all web pages of an interaction instance of a Web application to certain view equivalence regions, which form the interaction instance of a portlet. Experiments show that the approach is effective and efficient.	handy board;portals;portlet;turing completeness;web application;web page	Jingyu Song;Jun Wei;Shuchao Wan;Hua Zhong	2007	31st Annual International Computer Software and Applications Conference (COMPSAC 2007)	10.1109/COMPSAC.2007.67	web service;ajax;web application security;application software;web development;web application;web modeling;the internet;data web;web mapping;web-based simulation;html;web design;web standards;computer science;web navigation;social semantic web;web page;semantic web stack;database;computer applications;web 2.0;world wide web;information retrieval;standardization;mashup	Web+IR	-43.14974795631844	12.496931607654874	147748
8ef50e0d55fa7c5ea4ba6161db7f05e767fdd763	using clustering to support the migration from static to dynamic web pages	web pages data mining production quality control yarn software engineering software quality advertising transaction databases memory;hypermedia markup languages;yarn;web pages;data mining;software engineering;hypermedia markup languages web design reverse engineering;html pages;transaction databases;web design;clustering;clustering dynamic web pages web sites html pages browser;web sites;production;quality control;browser;dynamic web pages;software quality;memory;reverse engineering;advertising	Web sites of the first generation consist typically of a set of purely static Web pages. Content and presentation are not separated, and a same page structure is replicated every time a similar organization of the information is devised. Such a practice poses several problems to the evolution of these sites. It is not easy to update the content, and each time the HTML structure is modified, the same changes have to be propagated to all replications. In this paper, an approach is proposed for the identification of the Web pages that are more amenable to be migrated into a dynamic version, in that they share a similar structure, filled in with a content organized according to a common scheme. Clustering is used for this purpose: a common template is extracted from the pages in the same cluster, and the variable information of the pages matching the template is migrated to a database. A server side program extracts the requested information from the data base, and generates dynamically the HTML pages to be displayed	cluster analysis;computation;database;dynamic web page;effective method;html;server (computing);server-side;web application	Filippo Ricca;Paolo Tonella	2003		10.1109/WPC.2003.1199204	quality control;static web page;web design;computer science;operating system;dynamic web page;web page;database;cluster analysis;memory;hits algorithm;world wide web;information retrieval;web server;software quality;reverse engineering	DB	-38.67962917175062	10.593093594758876	147837
d661127ea0b97db055fe3960fea7f5f719e7fbf7	alert monitoring cloud: psychological health analysis of web text in cloud	web text mining;web text ming cloud computing workflow;history;semantic representation;text mining;resource management;rdf alert monitoring cloud psychological health analysis psychological status web text mining cloud architecture workflow customization workflow execution resource description frame;text analysis;alert monitoring cloud;psychology;workflow execution;workflow customization;data mining;resource description frame;computer architecture;psychological health analysis;web text ming;internet;psychology clouds cloud computing computer architecture resource management history service oriented architecture;clouds;cloud architecture;text analysis data mining health care internet meta data psychology;psychological status;workflow;meta data;service oriented architecture;rdf;cloud computing;health care	Alert monitoring of psychological health for web text is to evaluate and forecast the unhealthy psychological status of web users by the technology of web text mining. Based on the idea of “Service Distributed and Management Centralized”, this paper proposes a cloud architecture for psychological health analysis and describes the service component representation, workflow customization and workflow execution. With the excellent semantic representation ability of resource description frame (RDF), a service component metadata representation method is presented. Through querying the metadata of service component, end-users could choose the service components and customize the logic workflow on demand. In the workflow execution, the workflow case database is constructed. Through analyzing the similarity between history case and user workflow, a series of workflow reuse strategies are proposed. To validate the performance of the cloud architecture for psychological health analysis, a set of experiments are prepared to test the viability, the results demonstrate that the cloud architecture has higher precision, reuse and scalability for psychological health analysis of web text.	centralized computing;cloud computing;data-intensive computing;experiment;resource description framework;scalability;text mining	Yu Weng;Changjun Hu;Xiaoming Zhang;Huayu Li	2010	2010 Fifth Annual ChinaGrid Conference	10.1109/ChinaGrid.2010.45	workflow;computer science;data mining;database;windows workflow foundation;world wide web;workflow management system;workflow engine;workflow technology	Web+IR	-44.93456830473947	8.723476054913391	148029
6f8d051434d12a95943fd39b198acbadcebc706b	foundational aspects of semantic web optimization	tuple generating dependencies;conjunctive queries;counting complexity;redundancy elimination;complexity;semantic web;optimization;sparql;conjunctive queries cqs;rdf	The goal of the semantic web is to make the information available on the web easier accessible. Its idea is to provide machine readable meta-data to enable the development of tools that support users in finding the relevant data.  The goal of the thesis is to shed some light onto different foundational aspects of optimization tasks occurring in the field of the Semantic Web. Examples include the redundancy elimination in RDF data or static query analysis of (well-designed) SPARQL queries. Towards this goal, we already contributed several results.	human-readable medium;mathematical optimization;sparql;search engine optimization;semantic web;web performance	Sebastian Skritek	2012		10.1145/2213598.2213611	complexity;named graph;semantic web rule language;computer science;sparql;semantic web;rdf;social semantic web;linked data;data mining;semantic web stack;database;conjunctive query;information retrieval;semantic analytics	Web+IR	-34.88682494481531	4.344069249899605	148094
9ca893a5f3a32fdabee6b6b37889c0510b52f632	trends and challenges in formal specification and verification of services composition in ambient assisted living applications	theorem proving	Emerging Ambient Assisted Living (AAL) applications, as a part of AmI applications, deal essentially with healthcare related applications such as assistance to the elderly and handicapped persons, emergency services. Several approaches and techniques have been proposed, providing formal languages modeled with ontologies (e.g. OWL-S, WSMO) that describe in semantic way the environment. In this paper, relevant challenges of the current AAL application development, with a focus on the formal specification and verification are discussed. A formal system which enable to specify a semantic model represented by an upper ontology is presented. The innovative aspect of the proposed model concerns the use of a constructive description logic. c © 2011 Published by Elsevier Ltd.	atm adaptation layer;correctness (computer science);critical systems thinking;description logic;formal language;formal specification;formal system;formal verification;isabelle;owl-s;ontology (information science);upper ontology;wsmo	Mohamed Hilia;Abdelghani Chibani;Karim Djouani	2013		10.1016/j.procs.2013.06.072	simulation;computer science;data mining;formal specification;database	SE	-41.17975968802792	14.99256852228647	148246
ca8d2534de6219dffb35e4a5b7470af7bafc4f6e	rmx: the architecture of rule-based mailing system	rule based;rmx;e mail;mailing list	Mailing lists are widespread tools to communicate and share information with each other. Especially, organizations maintain so many of them for collaborative works. Because of conventional mailing schemes, it requires constant administration from its initiation to its maintenance. In this paper, we propose a rule-based mailing system called RMX where e-mail is delivered based on rules and parameters, instead of recipients' bare e-mail address or manually maintained mailing lists. By using this rule-based mailing approach, the administrator need not manage mailing lists since it guarantees a single point of administration by involving the organization's database and rules defined in SQL.	email;logic programming;rmx;sql	Yohei Matsumoto;Satoru Matsuzawa;Motomichi Toyama	2015		10.1145/2790755.2790792	rule-based system;computer science;data mining;database;world wide web	ML	-35.46660953957437	16.662493221512804	148351
97a381f0911fb2ceb8f0401be9b2a9160aa5f775	the linked data appstore - a software-as-a-service platform prototype for data integration on the web			linked data;prototype;software as a service;world wide web	Dumitru Roman;Claudia Daniela Pop;Roxana I. Roman;Bjørn Magnus Mathisen;Leendert W. M. Wienhofen;Brian Elvesæter;Arne-Jørgen Berre	2014		10.1007/978-3-319-13817-6_37	world wide web;linked data;software as a service;data integration;computer science	DB	-39.59521133003115	6.778505030140741	148565
57ce46093d73503aea8d9fca20b7c2685072ffe7	overcoming barriers to evaluation of terminological systems	evaluation.;terminology	Evaluation of terminological systems has been demonstrated to be a complicated task. This is due to the broad range of terminological systems, their application, and the clinical contexts in which they can be applied. We propose an evaluation framework that explicitly distinguishes an application-independent description of terminological systems, methods to determine application-dependent requirements, and methods to assess the applicability. In order to support a systematic application-independent description of terminological systems, we present a categorization of characteristics, including explicit questions. The answers to these questions can be mapped to the requirements for a certain application of a terminological system. This framework aims at reducing the efforts for determining which terminological system is applicable for a certain clinical setting.	categorization;requirement;software requirements specification	Ronald Cornet;Nicolette F. de Keizer;Danielle G. T. Arts	2004	Studies in health technology and informatics	10.3233/978-1-60750-949-3-497	knowledge management;data mining;categorization;computer science	AI	-47.51636287477949	6.208646846912462	148593
0ed87ba47a54d7215b5ff4c0075d706a36a792da	mobile cloud computing system components composition formal verification method based on space-time pi-calculus		To build different mobile cloud computing system (MCS) business applications, how to design open system architecture is essential. First, a service-oriented architecture is put forward. In this architecture, MCS components are expressed as the form of interoperable MCS services, which are combined to achieve complex business needs. Second, a formal method is proposed based on space-time (S-T) Pi-calculus, in order to verify validity of MCS service composition model. Finally, the case study shows that how to apply the model and method. The experiment result shows that they are feasible.	formal verification;lambda calculus;mobile cloud computing;π-calculus	Ling Yang;Guo Wen Li	2015		10.1007/978-3-319-38904-2_16	theoretical computer science;database;distributed computing	EDA	-45.52236056266908	17.009833884127982	148630
854776d7621c7dfad2e50990a42ef9626c090cc2	semantic enrichment on cultural heritage collections: a case study using geographic information		Cultural heritage institutions have recently started to explore the added value of sharing their data, using Linked Open Data to integrate and enrich metadata of their collections. The catalogue of the Biblioteca Virtual Miguel de Cervantes contains about 200,000 records which were transformed to RDF triples employing basically the RDA vocabulary (Resource Description and Access) to describe the entities. However, further refinements are needed for the recognition and extraction of implicit relationships expressed in natural language, such as geographic locations and dates. In order to facilitate literature researchers in carrying out sophisticated location-based searches, a web-based prototype is proposed. The usability of this web interface were evaluated in a user-based study, the results of which are also reported. The methods applied for the automation of the enrichment process, which build upon open-source software components, are described here.	component-based software engineering;entity;gene ontology term enrichment;linked data;location-based service;natural language;open-source software;prototype;remote database access;usability;user interface;vocabulary;web application	Gustavo Candela Romero;Maria Pilar Escobar Esteban;Manuel Marco Such	2017		10.1145/3078081.3078090	linked data;resource description and access;rdf;usability;semantic web;information retrieval;metadata;cultural heritage;added value;computer science	HCI	-41.62642152507392	5.5938172738163745	148700
330e8b8e16a0f818449598ee82200ae45a2ff798	xet as a rule language for consistency maintenance in uml	distributed system;ontologie;systeme reparti;mise a jour;maintenance;lenguaje uml;xml language;web semantique;langage modelisation unifie;lenguaje marcacion;actualizacion;sistema repartido;web semantica;unified modelling language;semantic web;mantenimiento;xml document;ontologia;consistency maintenance;markup language;ontology;langage xml;lenguaje xml;updating;langage marquage	Although XET is a powerful rule language, a mechanism to provide an automatic update on an XML document according to monitored events cannot be realized easily under the common use of XET. Proposed in the paper is a simple XML-expression transformation by XET enabling applications to perform update actions on an XML document when an event is detected. As a case study, it will be shown that XET is capable of maintaining consistency between UML diagrams. The capabilities include inconsistency detection according to user changes and automatic resolution process.	diagram;simple xml;unified modeling language	Nimit Pattanasri;Vilas Wuwongse;Kiyoshi Akama	2004		10.1007/978-3-540-30504-0_17	xml;computer science;ontology;database;programming language;world wide web	Web+IR	-36.95438625537914	12.900021613387752	148851
a52dafc2baf036892e75bf0beb324f2fa5193782	multi-agent and workflow-based web service management model	web service discovery;agent based;service management;web service	The coordination between agent service and Web service is the key factor for intelligent Web service management in the multi-agent based Web service framework. In view of the drawbacks of existing coordination approaches for agent service and Web service, this paper proposed a multiagent and workflow-based Web service management model. Through analyzing the interaction relations between agent service and Web service in the logical action-based task environment, a uniform task view for intelligent Web service is built. And based on such task view, a workflow towards special task is designed to realize intelligent Web service discovery and cooperation and composition. This model provides a more flexible Web service management.	agent-based model;business process execution language;multi-agent system;service discovery;task view;web framework;web service	Wenjia Niu;Quansheng Dou;Xu Han;Xinghua Yang;Zhongzhi Shi	2010		10.1007/978-3-642-16327-2_7	web service;web application security;service level requirement;web development;web modeling;data web;service product management;differentiated service;web standards;knowledge management;service delivery framework;ws-policy;service design;web navigation;database;business;web 2.0;world wide web;universal description discovery and integration	Web+IR	-44.355209930134365	13.960087968804086	148876
b9cfb73bfb29eff066b843ba27409691fe86cc54	context-driven and service oriented semantic mediation in daas composition		In this paper we present a context driven approach for automatically inserting appropriate mediation services in Data-as-a-Service (DaaS) compositions to carry out data conversion between interconnected services. We propose a context model expressed over Conflicting Aspect Ontology to describe more accurately the semantics of DaaSs. Based on the context model, we specify the mediation services that perform the transformation of DaaS parameter values between contexts. Then, we develop an efficient algorithm to detect and resolve the semantic conflicts between services in DaaS composition. An implementation demonstrates the applicability of our proposal.	data as a service;service-oriented architecture	Idir Amine Amarouche;Karim Benouaret;Djamal Benslimane;Zaia Alimazighi;Michael Mrissa	2012		10.1007/978-3-642-30567-2_8	data mining;semantics;computer science;ontology;mediation (marxist theory and media studies);context model;data conversion;composition (visual arts)	AI	-43.236129039578564	13.265865016814006	149456
49e0a6e7fd43c1272a63f6643901b63c94eab7e0	principles and practice of semantic web reasoning	semantic web	This paper discusses subtyping of tree-structured data encountered on the Web, e.g. XML and HTML data. Our long range objective is to define a type system for Web and/or Semantic Web query languages amenable to static type checking. We propose a type formalism motivated by XML Schema and accommodating two concepts of subtyping: inclusion subtyping (corresponding to XML Schema notion of type restriction) and extension subtyping (motivated by XML Schema’s type extension). We present algorithms for checking both kinds of subtyping. The algorithms are polynomial if certain conditions are imposed on the type definitions; the conditions seem natural and not too restrictive.	algorithm;html;polynomial;query language;semantic web;semantics (computer science);type system;world wide web;xml schema	Josef Kittler;Moni Naor	2004		10.1007/b100228	natural language processing;knowledge representation and reasoning;semantic computing;semantic web rule language;semantic search;semantic grid;web standards;computer science;knowledge management;model-based reasoning;semantic web;social semantic web;semantic web stack;reasoning system;information retrieval;semantic analytics	Web+IR	-35.50087737684576	8.420963535268909	149803
a547e7822e1b5547f50ae79fe5649a86e7dce518	g2st: a novel method to transform gml to svg	transforming language;geography markup language;gml;svg;scalable vector graphics	Geography Markup Language (GML) has been adopted as de facto standard for geo-referenced information storing and exchanging, while Scalable Vector Graphics (SVG), also a W3C-recommended XML standard, is appearing as an ideal format for rendering maps. Usually, Extensible Stylesheet Language Transformations (XSLT) is used to transform GML documents toSVG documents. Considering the complexity and variety of GML documents, however, designing XSLT rules is not a easy task; even worse, such a method cannot guarantee that the generated SVG document is valid. In this paper, we present a novel method to transform GML documents to SVG documents, while guaranteeing that the result SVG documents conform to a certain DTD or Schema pre-specified by the users. With this new method, it is convenient for the users to construct transformation rules from GML to SVG guided by the pre-specified SVG DTD or schema. We propose three algorithms to implement the new method. We also build a prototype and conduct preliminary experiments, which validate the effectiveness of the new method.	algorithm;database schema;experiment;geography markup language;high availability;interoperability;map;prototype;scalable vector graphics;synthetic data;world wide web;xml;xslt	Zhimao Guo;Shuigeng Zhou;Zhengchuan Xu;Aoying Zhou	2003		10.1145/956676.956698	computer science;scalable vector graphics;database;world wide web;information retrieval;remote sensing	Web+IR	-35.88845558720826	7.779232753104639	150184
fc13e9c264b4a1d3fd5b8e6edbd465bd9cbef1fd	distribution modeling and evaluation of product design problems	distributed system;intercambio informacion;computer aided design;multidisciplinaire;architecture systeme;systeme reparti;reseau ordinateur;developpement produit;integrated design;concepcion integrada;computer networks;computer network;distributed objects;sistema repartido;object oriented;design environment;multidisciplinario;echange information;information exchange;distributed models;mathematical model;conception assistee;communication protocol;oriente objet;multidisciplinary;arquitectura sistema;ingenierie simultanee;information system;product design;product quality;collaborative design;system architecture;development time;orientado objeto;conception integree;systeme information;desarrollo producto;concurrent engineering;evaluation framework;sistema informacion;product development	This paper proposes a framework for the modeling and evaluation of product design problems in a computer network-oriented design environment. The framework is intended to integrate designer-specified mathematical models for multidisciplinary and multiobjective design problems. The goal is to provide the ability to rapidly construct integrated design problem models to facilitate collaborative design work, improve product quality and reduce development time. Ultimately, it should allow specialized engineering applications and design problem models to operate under a common design environment. A product design problem is modeled in terms of interacting objects, called modules, each representing a specific aspect of the problem. Modules interact with one another through services that allow the exchange of information. Modules can encapsulate engineering models and data or software applications. The method is extended using a standard network communication protocol to create a distributed object-based modeling and evaluation framework for design problems.		Francis Pahng;Nicola Senin;David Wallace	1998	Computer-Aided Design	10.1016/S0010-4485(98)00005-0	iterative design;communications protocol;simulation;probabilistic design;information exchange;idef4;computer science;systems engineering;engineering;computer-automated design;artificial intelligence;object-oriented design;computer aided design;mathematical model;multidisciplinary approach;distributed design patterns;product design;design technology;object-oriented programming;information system;high-level design;new product development;concurrent engineering;generative design;product engineering	EDA	-39.62573610062072	18.088570838104467	150366
0513684ec0d92428523e1626aa09551b14f56201	knowledge-based semantic clustering	content based networking;conference paper;internet of things;publish subscribe;ontologies;computer science;knowledge base	"""Users of the web are increasingly interested in tracking the appearance of new postings rather than locating existing knowledge. Coupled with this is the emergence of the Web 2.0 movement (where everyone effectively publishes and subscribes), and the concept of the """"Internet of Things"""". These trends bring into sharp focus the need for efficient distribution of information. However to date there has been few examples of applying ontology-based techniques to achieve this. Knowledge-based networking (KBN) involves the forwarding of messages across a network based not just on the contents of the messages but also on the semantics of the associated metadata. In this paper we examine the scalability problems of such a network that would meet the needs of Internet-scale semantic-based event feeds. This examination is conducted by evaluating an implemented extension to an existing pub-sub content-based networking (CBN) algorithm to support matching of notification messages to client subscription filters using ontology-based reasoning. We also demonstrate how the clustering of ontologies leads to increased efficiencies in the subscription forwarding tables used, which in turn results in increased scalability of the network."""	algorithm;automated reasoning;cluster analysis;emergence;internet of things;knowledge-based systems;ontology (information science);scalability;web 2.0;world wide web	John Keeney;Dominic Jones;Dominik Roblek;David Lewis;Declan O'Sullivan	2008		10.1145/1363686.1363801	knowledge base;computer science;ontology;artificial intelligence;data mining;database;publish–subscribe pattern;world wide web;computer security;internet of things	Web+IR	-41.995952566522256	8.278412034185319	150383
4d4c5353b6f88b368be70c2777c43eae722a10eb	integrating order and distance relationships from heterogeneous maps	heterogeneous maps;distance relationships;integrating order	There is no automatic mechanism to integrate information between heterogeneous genome maps. Currently, integration is a difficult, manual process. We have developed a process for knowledge base design, and we use this to integrate order and distance relationships between genetic linkage, radiation hybrid, and physical maps. Until now, the only way to develop a persistent, knowledge-intensive application was to either develop a new knowledge base from scratch or coerce the application to fit an existing knowledge base. This was not from lack of interest by the knowledge base or database community, but merely from a lack of theoretical tools powerful enough to tackle the problem. We import formalisms from knowledge representation, natural language semantics, programming language research, and databases. These form a strong, theoretical foundation for knowledge base design upon which we have implemented the knowledge base design tool called WEAVE.	aspect weaver;database;design tool;genetic heterogeneity;hereditary diseases;knowledge base;knowledge representation and reasoning;linkage (software);mind map;natural language;persistent data structure;programming language theory	Mark Graves	1993	Proceedings. International Conference on Intelligent Systems for Molecular Biology		knowledge base;computer science;bioinformatics;knowledge management;artificial intelligence;knowledge-based systems;machine learning;open knowledge base connectivity;data mining;mathematics	DB	-38.02998121688432	5.835356258758771	150679
4f76c6fc94442ede3964dd9007f64df10af664a5	conceptual xml schema evolution - the codex approach for design and redesign	xml schema	Most available approaches for XML schema evolution specify the evolution steps for an XML schema or a DTD. This article will show that schema evolution can also be realized on a conceptual model. Schema evolution always requires propagating the changes to the XML documents that are already associated to the schema. This article suggests a method for conceptual schema evolution concerning all these subtasks. It is implemented in a tool called CoDEX (Conceptual Design and Evolution of XML schemas).	conceptual schema;schema evolution;xml schema	Meike Klettke	2007			document schema definition languages;xml schema (w3c);document structure description;schema evolution;relax ng;database;data mining;xml schema;xml validation;xml schema editor;computer science	DB	-33.938414506579214	10.99986502562721	150781
143494718e67a19dbc83002acc67fcb1ec6e07a0	an xml based electronic medical record integration system	historia clinica;documento electronico;extensible markup language;architecture systeme;medical record;aplicacion medical;sql;integration information;integrable system;interrogation base donnee;interrogacion base datos;document electronique;information integration;dossier medical;integracion informacion;xml;xml document;arquitectura sistema;medical application;information system;electronic medical record;system architecture;database query;systeme information;domain specificity;electronic document;application medicale;sistema informacion	In a domain specific information integration system CareHaven, a top-to-down integration strategy is adopted and XML is used as integration media. The main functional components of CareHaven and the disposal of user queries are fully explained in this paper. In the fulfilling of converting XML based query request to SQLs understood by data sources, we decompose the query request to sub requests that are easier to convert. Each sub request is converted with the aid of the mapping data. And the results of all sub requests is combined into one XML documents conformed to a certain DTD.	display resolution;independent computing architecture;information system;internet relay chat;mathematical optimization;multitier architecture;os-tan;object-relational database;query optimization;relational database management system;sql;xml	Hongyan Li;Shiwei Tang;Dongqing Yang;Yi Yu	2001		10.1007/3-540-47714-4_15	xml validation;xml encryption;xml;streaming xml;computer science;document structure description;operating system;xml framework;data mining;xml database;xml schema;database;xml signature;world wide web;xml schema editor;cxml;efficient xml interchange;systems architecture	DB	-35.71559380393656	11.149270749440731	150959
8f5ed9a2912375ca9cb03a195c29cdabde203fae	collaborative knowledge evaluation with a semantic wiki - wikidesign		We will present in this paper how to ensure a knowledge evaluation and evolution in a knowledge management system by using a Semantic Wiki approach. We describe a Semantic Wiki called WikiDesign which is a component of a Knowledge Management system. Currently WikiDesign is use in engineering department of companies to emphasize technical knowledge. In this paper, we will explain how WikiDesign ensures the reliability of the knowledge base thanks to a knowledge evaluation process. After explaining the interest of the use of semantic wikis in knowledge management approach we describe the architecture of WikiDesign with its semantic functionalities. At the end of the paper, we prove the effectiveness of WikiDesign with a knowledge evaluation example for an industrial project.	knowledge base;knowledge management;management system;wiki	Davy Monticolo;Samuel Gomes;Vincent Hilaire;Abder Koukam	2010			computer science;knowledge management;semantic web stack;world wide web	AI	-44.95724905881791	5.688186511803854	150975
f7bd462c296869b118ea0e757d3318f677820182	using ontology meta data for data warehousing.	data warehousing	One of the most complex issues of the integration and transformation interface is the case where there are multiple sources for a single data element in the enterprise Data Warehouse (DW). There are many facets due to the number of variables that are needed in the integration phase. However we are interested in the integration of temporal and spatial problem due to the nature of DWs. This paper presents our ontology based DW architecture for temporal integration on the basis of the temporal and spatial properties of the data and temporal characteristics of the data sources. The proposal shows the steps for the transformation of the native schemes of the data sources into the DW scheme and end user scheme and the use of an ontology model as the common data model.	canonical model;data element;data model;dreamwidth;global variable;reference architecture;requirement;web ontology language	Alberto Salguero;Francisco Araque;Cecilia Delgado	2008			computer science;data warehouse;data mining;database	DB	-34.85800557328476	10.693895645083044	151009
89a0f959877900d3ec89341a1e9f297b696e95c4	encoding and presenting interlinear text using xml technologies	formal specification;xml encoding;interlinear text;conference paper	Interlinear text is a common presentational format for linguistic information, and its creation and management have been greatly facilitated by the development of specialised software. In earlier work we developed a four-level model and corresponding formal specification for interlinear text. Here we describe a suitable XML representation for the model and show how it can be rendered into a variety of convenient presentational formats. We conclude by discussing architectural extensions, an application programming interface for interlinear text, and prospects for embedding the interlinear model into existing applications.	application programming interface;formal specification;lineage (evolution);pervasive informatics;text corpus;xml	Baden Hughes;Steven Bird;Catherine Bow	2003			xml validation;computer science;database;programming language;xml schema editor;information retrieval;efficient xml interchange	SE	-36.578380174202664	7.5004067447841996	151010
7c52324754fba94867a3e902dadaa9c589204219	towards a managed extensible control plane for knowledge-based networking	distributed system;unfolding;analyse amas;event service;ontologie;systeme reparti;correspondance ontologie;systeme grande taille;base de connaissances;ontology mapping;deploiement;routing;gestion red;interrogation base donnee;despliegue;routage;interrogacion base datos;semantics;large scale system;semantica;semantique;classification;sistema reactivo;sistema repartido;cluster analysis;gestion reseau;reactive system;systeme reactif;base conocimiento;ontologia;analisis cluster;network management;computer science;ontology;correspondencia ontologia;database query;clasificacion;content based routing;sistema gran escala;knowledge base;enrutamiento	This paper proposes an open, extensible control plane for a global event service, based on semantically rich messages. This is based on the novel application of control plane separation and semantic-based matching to Content-Based Networks. Here we evaluate the performance issues involved in attempting to perform ontology-based reasoning for content-based routing. This provides us with the motivation to explore peer-clustering techniques to achieve efficient aggregation of semantic queries. The clustering of super-peers using decentralized policy engineering will deliver the incremental deployment of new peer-clustering strategies.	automated reasoning;cluster analysis;control plane;peer-to-peer;routing;software deployment	David Lewis;John Keeney;Declan O'Sullivan;Song Guo	2006		10.1007/11907466_9	network management;knowledge base;routing;semantic integration;biological classification;reactive system;computer science;ontology;data mining;database;semantics;cluster analysis;world wide web;computer network	Networks	-37.870339779991895	12.47628306542277	151248
d9b01c18d778792f58b2dcd94fb3ab704dece400	a rich-variant architecture for a user-aware multi-tenant saas approach		Software as a Service cloud computing model favorites the Multi-Tenancy as a key factor to exploit economies of scale. However Multi-Tenancy present several disadvantages. Therein, our approach comes to assign instances to multi-tenants with an optimal solution while ensuring more economies of scale and avoiding tenants hesitation to share resources. The present paper present the architecture of our user-aware multi-tenancy SaaS approach based on the use of rich-variant components. The proposed approach seek to model services functional customization as well as automation of computing the optimal distribution of instances by tenants. The proposed model takes into consideration tenants functional requirements and tenants deployment requirements to deduce an optimal distribution using essentially a specific variability engine and a graph-based execution framework.		Houda Kriouile;Bouchra El Asri	2018	CoRR	10.5281/zenodo.1346047	computer science;personalization;economies of scale;software deployment;systems engineering;cloud computing;architecture;functional requirement;exploit;software as a service	HPC	-47.297080932687976	16.289526029571096	151265
f13d3cec911c609142cff1adb804b81f2e726b80	computed knowledge base for description of information resources of water spectroscopy		We develop the addition to the W@DIS information system that allows one to load solutions of the quantitative spectroscopy problems. These solutions are supplied with calculated semantic annotations that characterise properties of the solutions. In addition to the typical properties (e.g. represented in Dublin Core) the solution reliability properties are also determined. Every solution is represented in a knowledge base as an individual of the quantitative spectroscopy ontology, which contains more that 106 axioms. In the paper we present the knowledge base structure and describe two classes of the information sources classification tasks, together with the solutions using querying OWL ontologies.	dublin core;information system;knowledge base;ontology (information science);reliability (computer networking);web ontology language	Alexey Privezentsev;Alexander Z. Fazliev;Dmitry Tsarkov;Jonathan Tennyson	2010			ontology;data mining;computer science;knowledge base;dublin core;spectroscopy;axiom;information system;ontology (information science)	AI	-39.861123476667785	5.342550137987067	151813
f445e44b57adca4aba7983bfca2aefa87f613aa7	toward ontology-driven omics data integration in current database management systems	owl;data integrity;ontology driven omics data integration;database management systems;semantic web database management systems ontologies artificial intelligence;resource description framework;semantic web technology;ontologies artificial intelligence;oracle database server ontology driven omics data integration database management system semantic web technology;emergent semantics;oracle database server;proteins;semantic web;ontologies;proteomics;web technology;database management system;ontologies database systems resource description framework xml proteomics semantic web owl proteins genomics bioinformatics;benchmark testing	To fully realize the benefit of ontology-driven “Omics” data integration, it is essential that database management systems (DBMSs) provide persistent storage and efficient concurrent access in a manner that leverages the emerging Semantic Web technologies. In this paper, we explore the current capabilities of existing DBMSs, such as Oracle’s database server, to meet the needs generated by the Semantic Web technologies. Our primary consideration is the new features provided by existing DBMSs to incorporate RDF, OWL, and SPARQL. We present a framework for this evaluation and lay the groundwork for subsequent benchmarking of the semantic-enabled DBMSs.	concurrency control;database server;management system;omics;persistence (computer science);resource description framework;sparql;semantic web;server (computing);web ontology language	John A. Springer;Jake Yue Chen	2009	2009 International Conference on Network-Based Information Systems	10.1109/NBiS.2009.80	benchmark;database server;computer science;ontology;semantic web;rdf;data integrity;data mining;database;proteomics;world wide web	DB	-37.79904990346481	4.970261352758126	152019
8ee7fbf1b59a3bc093f121c949863b2ec9fb3fe7	inbenta semantic search engine: a search engine inspired by the meaning-text theory	search engine;lexicon;semantics;meaning text theory;lexical function;natural language processing	Due to the widespread digitalization of documents, the need to build sophisticated search engines that can adapt to the particular way users ask questions and provide quick and efficient access to information is becoming increasingly relevant.  To cope with this reality, INBENTA has developed an intelligent search engine, called the Inbenta Semantic Search Engine (ISSE). ISSE's two main tasks are analyzing users' queries and finding the most appropriate answer to those questions in a knowledge-base. To carry out these tasks, INBENTA's software solution relies on Meaning-Text Theory, which focuses on the lexicon and semantics.	freedom of information laws by country;lexicon;meaning–text theory;security engineering;semantic search;web search engine	Manon Quintana	2016		10.1145/2912845.2912877	natural language processing;search engine indexing;metasearch engine;semantic search;computer science;data mining;search analytics;information retrieval;search engine	Web+IR	-40.805411360521624	6.99644504951361	152038
0d1fa0b6e8dd59889b78e7b450f2876635043d98	dynamic information retrieval optimization using mobile agents	information retrieval;mobile agents;heterogeneous data;query optimization;agent communication;dynamic information;earth science data;retrieval optimization;mobile agent;data retrieval;local area network;data transfer;dynamic optimization	"""Mobile agents have the potential to substantially improve the speed and efficiency with which distributed and heterogeneous data is retrieved. By moving the computation to the data, retrieval times can be reduced by the elimination of unnecessary data transfer. One way to improve a mobile agent system's retrieval efficiency is to incorporate various query optimization techniques (Das et. al., 2002). These methods involve re-writing of the query execution graph so each mobile agent retrieves its requested data in an optimized order, thus minimizing total data transfer size. While these query re-writing methods can be highly effective in reducing both retrieval times and data transfer sizes, they are generally """"static"""", in that the mobile agents retrieve data in a particular order based on an itinerary that is fixed at the time the plan is generated. We have developed a system by which the advantages of mobile agents are leveraged to optimize data retrieval by dynamically optimizing the retrieval strategy as it is carried out. This strategy equips each spawned agent with the full query execution graph and necessary code to execute the retrieval plan at any data site in the network. The spawned agents communicate and collaborate with each other to dynamically decide where to migrate, send data, and perform necessary computations. These decisions depend on retrieval factors such as network speed, data size, and the computational capabilities of the data servers involved in the retrieval. The feasibility of the approach has been demonstrated within a local area network environment using Earth Science data and we present some experimental results in this context."""	computation;data retrieval;information retrieval;mathematical optimization;mobile agent;query optimization;server (computing);virtual data room	Subrata Kumar Das;Kurt Shuster;Curt Wu	2003		10.1145/860575.860749	local area network;query optimization;query expansion;computer science;theoretical computer science;data mining;mobile agent;database;data retrieval	AI	-37.16133975427529	15.976069752396283	152124
91de0fdcc54206cc22ddbd665a40cef6921b0ada	seal - a framework for developing semantic web portals	site web;portail web;representacion conocimientos;navegacion informacion;red www;maintenance;navigation information;interrogation base donnee;information browsing;interrogacion base datos;semantics;base connaissance;information access;semantica;semantique;internet;web portal;acces information;mantenimiento;world wide web;base conocimiento;acceso informacion;reseau www;sitio web;knowledge representation;representation connaissances;database query;web site;knowledge base	The core idea of the Semantic Web is to make information accessible to human and software agents on a semantic basis. Hence, web sites may feed directly from the Semantic Web exploiting the underlying structures for human and machine access. We have developed a generic approach for developing semantic portals, viz. SEAL (SEmantic portAL), that exploits semantics for providing and accessing information at a portal as well as constructing and maintaining the portal. In this paper, we discuss the role that semantic structures make for establishing communication between different agents in general. We elaborate on a number of intelligent means that make semantic web sites accessible from the outside, viz. semantics-based browsing, semantic querying and querying with semantic similarity, and machine access to semantic information at a semantic portal. As a case study we refer to the AIFB web site — a place that is increasingly driven by Semantic Web technologies.	conceptualization (information science);darpa agent markup language;description logic;existential quantification;inference engine;information extraction;knowledge base;ontology (information science);ontology learning;personalization;portals;programming paradigm;provisioning;scalability;seal (emblem);semantic web;semantic similarity;software agent;viz: the computer game;web mining;web page	Alexander Maedche;Steffen Staab;Nenad Stojanovic;Rudi Studer;York Sure-Vetter	2001		10.1007/3-540-45754-2_1	semantic data model;semantic interoperability;knowledge base;semantic computing;the internet;semantic integration;semantic web rule language;data web;semantic search;semantic grid;web standards;computer science;artificial intelligence;semantic web;social semantic web;linked data;semantic web stack;semantic compression;database;semantics;semantic technology;world wide web;owl-s;information retrieval;semantic analytics	AI	-37.89423603245535	11.877042000524275	152238
99cb6b71de7f4aa684f9e39630e5f9b38bdf95ee	automatic composition of semanticweb services	multistep narrowing algorithm;web services constraint handling formal specification semantic web;web service discovery;constraint logic programs;formal specification;web service discovery problem;web services search engines semantic web automation logic programming context aware services computer science application software scalability internet;application software;search engines;formal semantics;web service;semantic web services;internet;logic programming;formal semantic descriptions;semantics based automatic service discovery;service oriented computing;web services;semantic description;semantic web;constraint handling;scalability;computer science;service discovery;constraint logic programming technology semantics based automatic service composition semantic web services service oriented computing formal semantic descriptions web service discovery problem semantics based automatic service discovery multistep narrowing algorithm;semantics based automatic service composition;constraint logic programming technology;context aware services;automation	Service-oriented computing is gaining wider acceptance. For Web services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose and synthesize services automatically. For this automation to be effective, formal semantic descriptions of Web services should be available. In this paper we formally define the Web service discovery and composition problem and present an approach for automatic service discovery and composition based on semantic description of Web services. We also report on an implementation of a semantics-based automated service discovery and composition engine that we have developed. This engine employs a multi-step narrowing algorithm and is efficiently implemented using the constraint logic programming technology. The salient features of our engine are its scalability, i.e., its ability to handle very large service repositories, and its extremely efficient processing times for discovery and composition queries. We evaluate our engine for automated discovery and composition on repositories of different sizes and present the results.	algorithm;constraint logic programming;scalability;semantics (computer science);service discovery;service-oriented software engineering;web service;world wide web	Srividya Kona Bansal;Ajay Bansal;Gopal Gupta	2007	IEEE International Conference on Web Services (ICWS 2007)	10.1109/ICWS.2007.52	web service;computer science;data mining;database;service discovery;services computing;law;world wide web;universal description discovery and integration	DB	-44.36782069863648	14.291736703108567	152347
6a5ebbf2ba6faa10b206eed0c62f616080b3905d	semantic information retrieval in the compass location system	busqueda informacion;sensibilidad contexto;utilisation information;distributed system;sistema multiple;uso informacion;systeme reparti;context aware;informatique mobile;information sources;location based service;metadata;distributed hash table;information use;information retrieval;par a par;pervasive computing;xml language;localization;web semantique;service web;semantics;multiple system;localizacion;resource description framework;web service;peer to peer system;semantica;semantique;intergiciel publication souscription;informatica difusa;semantic information;sistema repartido;localisation;poste a poste;intergicial editor suscriptor;informatique diffuse;recherche information;web semantica;metadonnee;specification rdf;semantic web;metadatos;information system;sensibilite contexte;mobile computing;peer to peer;publish subscribe middleware;systeme information;langage xml;lenguaje xml;rdf;servicio web;sistema informacion;systeme multiple	In our previous work, we have described the COMPASS location system that uses multiple information sources to determine the current position of a node. The raw output of this process is a location in geo-coordinates, which is not suitable for many applications. In this paper we present an extension to COMPASS, the so called Translator, that can provide facts about the location like city name, address, room number, etc. to the application. These facts are represented in the Semantic Web RDF/XML language and stored on distributed Geo RDF Servers. The main focus of this paper is a location-based service discovery mechanism which allows a node to find all services that can provide facts about its current location. This discovery service is built upon a structured Peer-to-Peer system implementing a distributed hash table.	compass;cpu cache;distributed hash table;geographic coordinate system;information retrieval;location-based service;microsoft outlook for mac;peer-to-peer;prototype;rdf/xml;semantic web;server message block;service discovery;vocabulary;xml	Frank Kargl;Günter Dannhäuser;Stefan Schlott;Jürgen Nagler-Ihlein	2006		10.1007/11890348_11	computer science;artificial intelligence;operating system;rdf;data mining;database;semantics;mobile computing;world wide web;ubiquitous computing	Web+IR	-37.14456356210168	11.537647246417066	152499
0feb8db73fd7900286970c81856e2ffd0142d437	an agent mediated approach to dynamic change in coordination policies	dynamic change;distributed system;decision support;multiagent system;systeme reparti;policy change;agent based;systeme aide decision;e commerce;cambio;analysis and design;agente;sistema ayuda decision;change;agent;decision support system;sistema repartido;distributed information system;changement;coordinacion;information system;sistema multiagente;systeme information;systeme multiagent;coordination;sistema informacion	Distributed information systems for decision-support, logistics, and e-commerce involve coordination of autonomous information resources and clients according to specific domain independent and domain dependent policies. A major challenge is handling dynamic changes in the priorities, preferences, and constraints of the clients and/or the resources. Addressing such a challenge requires solutions to two problems: a) Reasoning about the need for dynamic changes to coordination policies in response to changes in priorities, preferences, and constraints. b) Coordinating the run-time assembly of policy changes in a dependable manner. This paper introduces the NAVCo approach to address these problems. The approach involves exploiting negotiation-based coordination to address the first problem and model-based change coordination to address the second problem. These two key features of the approach are well suited for realization using an agent-based architecture. The paper describes the architecture with specific emphasis on the analysis and design of the agent specifications for negotiation and change coordination.	agent-based model;autonomous robot;e-commerce;emoticon;information system;logistics;regular expression	Prasanta K. Bose;Mark G. Matthews	2000		10.1007/3-540-45263-X_11	decision support system;computer science;knowledge management;artificial intelligence;information system	AI	-39.673320687475886	16.665986074478326	152544
4afeb8a13b06b7aab4355567549a41e9e5c3aee3	using semantic web techologies for the generation of domain templates to support clinical study meta-data		The Biomedical Research Integrated Domain Group (BRIDG) model is a formal domain analysis model for protocol-driven biomedical research, and serves as semantic foundation for application and message development in the standards developing organizations (SDOs). The increasing sophistication and complexity of the BRIDG model requires new approaches to the management and utilization of the underlying semantics to harmonize domain-specific standards. The objective of this study was to develop and evaluate a semantic web-based approach that integrates the BRIDG model with ISO 21090 data types to generate domain templates to support clinical study meta-data standards development. In it we developed a template generation and visualization system based on an open-source Resource Description Framework (RDF) store backend, a SmartGWT-based web user interface, and a “mind map” based tool for the visualization of generated domain templates. We also developed a RESTful web service for access to the generated domain templates in a Clinical Information Modeling Initiative (CIMI)-compliant format. A preliminary usability study is performed to evaluate the system in terms of the ease of use and the capability for meeting the requirements using a selected use case.	domain analysis;domain-specific language;information model;mind map;open-source software;representational state transfer;requirement;resource description framework;semantic web;usability testing;user interface;web application;web service	Guoqian Jiang;Julie Evans;Cory M. Endle;Harold R. Solbrig;Christopher G. Chute	2013			semantic web;data mining;metadata;template;computer science;semantic web stack	SE	-41.65686470350258	4.535246478843437	152553
053f34a1f21eba78fbada6a7f96aa37c3aaf2752	semantic bms: ontology for analysis of building automation systems data	semantic sensor network ontology;intelligent buildings;building management systems;building automation systems;semantic web;computer aided facility management;ontology;data integration	Building construction has gone through significant change with the emerging spread of ICT during last decades. “Intelligent buildings” are equipped with building automation systems (BAS) that can be remotely controlled and programmed and that are able to communicate and collaborate. However, BAS aim to facilitate operation of the building and do not provide sufficient support for strategic level decision support. This article presents adaptation of Semantic Sensor Network ontology for use in the field of building operation analysis. The Semantic BMS ontology enriches the SSN with model of building automation datapoints that gather operation data and describe the interconnections between BAS devices, algorithms and influenced or monitored properties of a building. Proposed ontology allows facility managers to query BAS systems in a way that is convenient for tactical and strategic level planning and that is unavailable in current state of the art systems.	automation;bridge management system	Adam Kucera;Tomás Pitner	2016		10.1007/978-3-319-31165-4_5	building management system;building automation;computer science;knowledge management;ontology;artificial intelligence;data integration;semantic web;ontology;data mining;database;ontology-based data integration	Robotics	-45.100906810981414	4.755171714974637	152567
9af01c9f6b14d2eba64ac68ddce23b570a2adc56	requests management for smartphone-based matching applications using a multi-agent approach		We present a new multi-agent approach to managing how requests are sent between users of smartphone-based applications for reaching bi-lateral agreements. Each agent is modelled as having a selfish behaviour based on his preferences and an altruist behaviour with respect to the links between the agent and his neighbours. The objective is to maximise the likelihood of an acceptable match while minimising the burden on the users due to unnecessary messaging. We provide a dynamic algorithm using this architecture and we present an empirical evaluation with various mathematical models of user behaviour and altruism. The evaluation shows that our approach can reduce the risks of rejections and the number of requests while increasing the likelihood of acceptable matches.	multi-agent system;requests;smartphone	Gilles Simonin;Barry O'Sullivan	2016		10.1007/978-3-319-50349-3_12	knowledge management;distributed computing;management science	AI	-43.196351459104136	16.75324190438234	152587
ecfc46fd7ac0aefb0321ff816348e21356c46ca3	data quality in xml databases - a methodology for semi-structured database design supporting data quality issues	xml database;data quality;database design	As the use of XML as a technology for data exchange has widely spread, the need of a new technology to store semi-structured data in a more efficient way has been emphasized. Consequently, XML DBs have been created in order to store a great amount of XML documents. However, like in previous data models as the relational model, data quality has been frequently left aside. Since data plays a key role in organization efficiency management, its quality should be managed. With the intention of providing a base for data quality management, our proposal address the adaptation of a XML DB development methodology focused on data quality. To do that we have based on some key area processes of a Data Quality Maturity reference model for information management process definition.	capability maturity model;data model;data quality;database design;information management;reference model;relational model;semi-structured data;semiconductor industry;xml database	Eugenio Verbo;Ismael Caballero;Eduardo Fernández-Medina;Mario Piattini	2007			data modeling;xml base;database theory;data quality;data transformation;computer science;data administration;database model;data mining;xml database;database;view;database schema;information retrieval;database testing;database design;efficient xml interchange;spatiotemporal database;component-oriented database	DB	-34.52605391373222	10.449252907458348	152624
2574a13b882b6fca4dce4c0c1a8fc62f00d9899c	using a middleware agent to bridge standalone cad systems for distributed and collaborative applications	encapsulation;distributed system;concepcion asistida;computer aided design;systeme reparti;collaborative application;cooperation;logicial personalizado;encapsulacion;cooperacion;intergiciel;sistema repartido;team work;conception assistee;travail equipe;trabajo equipo;middleware;ingenierie simultanee;ingenieria simultanea;concurrent engineering	A method to bridge standalone CAD systems is presented. The method uses a middleware agent to wrap the standalone CAD systems. Both inside and outside encapsulations are constructed inside the agent. The online bridge has been initially implemented to integrate commercial CAD systems for distributed and collaborative applications.	computer-aided design;middleware	Bin Liao;Fazhi He;Jun Chen;Yong Ma	2005		10.1007/11686699_31	embedded system;simulation;teamwork;encapsulation;computer science;operating system;computer aided design;middleware;cooperation;concurrent engineering	EDA	-39.37894223754572	17.58470277050937	152641
79542b6a8a85ce39af4a5e7f10532750d881e7f6	specifying open agent systems: a survey	dispute resolution;electronic markets;satisfiability;open system;agent systems	Electronic markets, dispute resolution and negotiation protocols are three types of application domain that can be viewed as open agent systems. Members of such systems are developed by different parties and have conflicting goals. Consequently, they may choose not to, or simply fail to, conform to the norms governing their interactions. It has been argued that many practical applications in the future will be realised in terms of open agent systems of this sort. Not surprisingly, recently there is a growing interest in open systems. In this paper we review and compare four approaches for the specification of open systems, pointing out the extent to which they satisfy a set of requirements identified in the literature.	application domain;electronic markets;interaction;open system (computing);requirement	Alexander Artikis;Jeremy V. Pitt	2008		10.1007/978-3-642-02562-4_2	computer science;knowledge management;operations management;computer security	SE	-43.907142770440394	17.75858482347245	152730
5a1a168a46458b2541aa56078e687c4af5dac33c	automatic generation of java/sql based inference engines from rdf schema and ruleml	query language;search engine;buscador;red www;sql;reseau web;semantics;semantica;semantique;lenguaje interrogacion;automatic generation;internet;relational database system;world wide web;langage interrogation;moteur recherche	This paper describes two approaches for automatically converting RDF Schema and RuleML sources into an inference engine and storage repository. Rather than using traditional inference systems, our solution bases on mainstream technologies like Java and relational database systems. While this necessarily imposes some restrictions, the ease of integration into an existing IT landscape is a major advantage. We present the conversion tools and their limitations. Furthermore, an extension to RuleML is proposed, that allows Java-enabled reaction rules, where calls to Java libraries can be performed upon a rule firing. This requires hosts to be Java-enabled when rules and code are moved across the web. However, the solution allows for great engineering flexibility.	java;rdf schema;ruleml;sql	Andreas Eberhart	2002		10.1007/3-540-48005-6_10	sql;relational database management system;the internet;computer science;operating system;data mining;database;semantics;programming language;world wide web;algorithm;query language;search engine	SE	-36.05083532580071	10.94378222843856	152796
4fb792d6c855dc5f5d038a200f4d8bce19ac53a1	exploiting information relationships for access control in pervasive computing	corresponding author;pervasive computing;complexity analysis;semantics;performance analysis;distributed access control;information relationships;access control;information service	Many information services in pervasive computing offer rich information, which is information that includes other types of information. For example, the information listed in a person’s calendar entry can reveal information about the person’s location or activity. To avoid rich information from leaking its included information, we must consider the semantics of the rich information when controlling access to this information. Other approaches that reason about the semantics of information (e.g., based on Semantic Web rule engines) are based on a centralized design, whose drawback is a single point of failure. In this paper, we exploit information relationships for capturing the semantics of information. We identify three types of information relationships that are common and important in pervasive computing and integrate support for them in a distributed, certificate-based access control architecture. In the architecture, individuals can either define their own information relationships or refer to relationships defined by a standardization organization. In our approach, access control is fully distributed while sophisticated rule engines can still be used to deal with more complex access control cases. To demonstrate the feasibility of our design, we give a complexity analysis of the architecture and a performance analysis of a prototype implementation. c © 2006 Elsevier B.V. All rights reserved.	access control;analysis of algorithms;business rules engine;centralized computing;prototype;reliability engineering;semantic web;semantic reasoner;single point of failure;ubiquitous computing	Urs Hengartner;Peter Steenkiste	2006	Pervasive and Mobile Computing	10.1016/j.pmcj.2006.05.001	information technology architecture;information exchange;computer science;access control;information integration;information filtering system;personal information management;management information systems;data mining;information processor;database;semantics;information quality;automated information system;world wide web;computer security;ubiquitous computing;information architecture;computer network	AI	-40.19071863206721	14.46235992754142	152926
46de73d0c203b83307e6054d640f08c8a11ac9de	towards solving the interdisciplinary language barrier problem	information retrieval;world wide web;computers and society;human activity;problem solving	This work aims to make it easier for a specialist in one field to find and explore ideas from another field which may be useful in solving a new problem arising in hi s practice. It presents a methodology which serves to represent the relationships that exist b etween concepts, problems, and solution patterns from different fields of human activity in the form o f a graph. Our approach is based upon generalization and specialization relationships and prob lem solving. It is simple enough to be understood quite easily, and general enough to enable coheren t int gration of concepts and problems from virtually any field. We have built an implementation whi c uses the World Wide Web as a support to allow navigation between graph nodes and collabo rative development of the graph.	coherence (physics);email;graph (discrete mathematics);partial template specialization;problem solving;reference implementation;world wide web	Sébastien Paquet	2001	CoRR		human–computer interaction;computer science;artificial intelligence;theoretical computer science;world wide web	AI	-45.86411995867804	4.888720830366171	153078
b7b1950114912ca487a4a222eab5e818ce183a1a	maggis: a mobile-agent and gml based distributed geographic information system	geographic information system;geographic information;distributed computing;distributed gis;information integration;web mapping;gml;svg;distributed computing environment;mobile agent;mobile internet;spatial information;information publishing	The rapidly emerging of Mobile Internet and the constantly increasing of wireless subscribers' number bring new opportunities and challenges to geographic information sharing and accessing. Current Web GISs, which are accessed by using connection based approaches, are very inefficient in fulfilling the requirements of GIS applications under open, dynamic, heterogeneous and distributed computing environments such as (Mobile) Internet. In this paper, we propose a new system for accessing and sharing distributed geographic information by using mobile agent and GML technologies, in which mobile agents are used to overcome the limitations of traditional distributed computing paradigms in (mobile) Internet context and GML is adopted as the common format for spatial information wrapping and mediation, while SVG is used as a web‐map publishing format that can be processed and displayed in Web browser. A prototype is implemented, which demonstrates the effectiveness and feasibility of the proposed method.	geographic information system;mobile agent	Jihong Guan;Jiaogen Zhou;Shuigeng Zhou	2006	IJWIS	10.1108/17440080780000304	distributed gis;mobile search;mobile web;web mapping;mobile database;computer science;information integration;data mining;scalable vector graphics;mobile agent;database;spatial analysis;geographic information system;mobile computing;world wide web;distributed computing environment	Robotics	-36.0801795751711	14.392512273397912	153213
f8c8df623907353331ac2300959247d3a99acea4	an efficient way to accelerate service discovery and invocation	classification based service discovery method service oriented architecture search space service query bayes based method;query processing;search space;bayes based method;web services pattern classification query processing software architecture;service query;software architecture;web services;pattern classification;classification based service discovery method;service discovery;acceleration service oriented architecture web services taxonomy iso standards code standards memory management software architecture network servers companies;service oriented architecture	Searching a target service to invoke in an efficient way is one of the most important issues in the implementation and deployment of SOA (service-oriented architecture). In order to retrieve the target services quickly from a repository according to clients' request, how to reduce the search space is important. Moreover, in order to accelerate the frequent service invocation by a client, one key is to reduce the times of service query on the repository by the client. Thus, this paper proposes a Bayes-based method to classify services and determine the classification to which the target services belong. On retrieving a target service, this paper also proposes a mechanism to buffer the service to accelerate the multiple invocation of the same service. Experiments show that the classification-based service discovery method get better efficiency and the buffer mechanism also accelerate the service invocation.	client (computing);service discovery;service-oriented architecture;service-oriented device architecture;software deployment;the times	Gexin Li;Wenjie Zhang;Huxiong Li;Junfang Guo	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4413625	web service;software architecture;web query classification;differentiated service;computer science;service delivery framework;service-oriented architecture;data mining;database;service;service discovery;data as a service;world wide web;universal description discovery and integration	Mobile	-46.636153158828364	15.367558043777537	153314
dffcf3f77589e109697614e3e0b00d1a13960d10	intersect: a general purpose hypertext system based on an object oriented database		Most complicated documentation maintenance environments require sophisticated computer support systems for maintaining relationships between document types and document instances. Unfortunately, existing approaches are not sufficient to achieve this aim. Hypertext technology gives us the possibility to easily browse document contents but it lacks a mechanism for declaring the overall structure of the document and its relationships. Database management systems can model complex real world situations, but do not have “the single coherent interface to the database which is the hallmark of hypertext”[1]. In this paper, we propose a general hypertext-based documentation support system, InterSect, which uses an object oriented database as an information repository to support both the definition and manipulation of complex document structures. This prototype provides both hypertext and database views of documents, together with a versioning mechanism driven by a user defined schema for the overall document structure.	hypertext	Bing Wang;Peter Hitchcock	1991			computer science;database;programming language;world wide web;database schema;database design	DB	-34.027066641820596	9.85528001812813	153352
d99e4058394ccfd062fcda0043293de3f50972d9	query processing in the wassit mediation framework	query processing in the wassit mediation framework;owl;xquery;information systems;cost function;web information system;software libraries;query processing;articulo;query formulation;query execution plan;query optimization;data engineering;xml internet knowledge representation languages ontologies artificial intelligence query formulation query languages query processing;owl ontology;ontologies artificial intelligence;query languages;knowledge representation languages;internet;algebra;query optimization query processing wassit mediation framework web information system data source query rewriting owl ontology query execution plan xquery;mediation;warehousing;xml;cities and towns;ontologies;peer to peer computing;data source;query processing mediation warehousing peer to peer computing software libraries data engineering information systems owl ontologies cost function;high level architecture;query rewriting;cost model;wassit mediation framework;data models;heterogeneous data sources	As more and more data is available through the Web, mediation of information from multiple autonomous and heterogeneous data sources becomes a crucial task for future Web information systems. We describe the features of our mediation framework WASSIT (frameWork d'intégrAtion de reSSources de données par la médIaTion), which provides the user with an integrated view of the underlying data sources. In particular, we describe the high level architecture of WASSIT and present how user's queries are processed focusing on query rewriting and query optimization. For query rewriting, we use OWL ontology to describe the mappings between global schemas and local schemas and propose an approach to generate query execution plans for XQuery queries. To optimize queries over sources with limited query capabilities, we adopt a formalism that describes data sources capabilities and propose an algorithm for producing equivalent query plans. We present also our cost model for choosing the optimal plan.	algorithm;analysis of algorithms;apache jena semantic web framework;autonomous robot;context-free grammar;high-level architecture;information system;java;linear algebra;mathematical optimization;ontology (information science);query optimization;response time (technology);rewriting;schema evolution;semantics (computer science);web ontology language;world wide web;xquery	Faouzia Wadjinny;Lahcen Gounbark;Laila Benhlima;Dalila Chiadmi;Ahmed Moujane	2009	2009 IEEE/ACS International Conference on Computer Systems and Applications	10.1109/AICCSA.2009.5069387	sargable;query optimization;query expansion;web query classification;information engineering;computer science;artificial intelligence;data mining;database;rdf query language;web ontology language;web search query;information retrieval;query language;spatial query	DB	-35.00614062514317	8.662449283376436	153463
01385d896a56fb8b7aac0203eaffb77bcabb1cfc	online semantic knowledge management for product design based on product engineering ontologies	linked data;knowledge management;exhibit;semantic web;ontology query reasoning;sparql;product design;engineering ontology;javascript object notation json	"""This paper formulates an approach to use the semantic web for knowledge management in the product design domain to provide enhanced capabilities of authoring/updating, querying/reasoning, searching, and visualization of information. Engineering has unique challenges, due to the pervasive use of CAD models and underlying interoperability and integration issues. The authors propose a distributed model composed of a host hybriddata repository, external public linked data sources, a semantic data management engine, and a web-based user interface layer. The hybrid-data repository consists of ontologies to preserve knowledge for the product design domain and a conventional product data base to utilize legacy design data. Near full integration with a web based environment is achieved. The importance of accessing product related CAD data that has been instantiated in ontology models, querying them, and then displaying the data on a web interface in real time with other legacy data, such as hand sketches and notes that have been scanned and relevant information from conventional rational databases public linked data sites, is a useful and transformational capability. The system clearly facilitates design and information management beyond traditional CAD capabilities and creates a foundation for important capability improvements in the domain. DOI: 10.4018/jswis.2011100102 International Journal on Semantic Web and Information Systems, 7(4), 36-61, October-December 2011 37 Copyright © 2011, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. plications, interoperability, and the widespread use of the CAD model as the master model. The issue is compounded because the engineers are often distributed globally. The CAD models are local-based and sharing of that data in near real time over the internet is a big challenge. Although engineers are now working with these multiple systems and multiple applications in a distributed manner, there needs to be progress in parallel in terms of using new approaches and technologies to support the management and sharing of this engineering data. Traditionally, the process to build applications to share data has depended on hand-tuned and ad hoc techniques to integrate information (Seagram, 2009). Recently, ontology-based approaches have been used for managing product data. An ontology is a data model that represents a domain and is used to reason about the objects in that domain and relations between them. Using description languages, such as Resource Description Framework (RDF) and Web Ontology Language (OWL), data can be made explicit with semantic relationships, and thus there are powerful possibilities for mitigating miscommunication between engineers because everyone will interpret an ontology the same way. Using these approaches, ontology modeling allows engineers in a specific domain to represent design knowledge in a relatively flexible manner along with a formal and machine manipulatable standard in a context-dependent way. Machines can now be used to solve relatively complex problems that require application of domain knowledge. However, in order to realize the full potential of ontologies and create knowledge-contextual design environments, we need to extend the scope of sharing and reusing product design knowledge that has been captured in existing ontology models. The Semantic Web provides a common framework that allows data to be shared and reused across applications, enterprises, and community boundaries (Herman, 2009). Currently Semantic Web applications are used for personal information management, travel planning, and exchange of data in telecommunication and legal domains (Sheth, 2010). A few Semantic Web applications have been developed for product design in the engineering domain, but more powerful and germane applications would serve to streamline knowledge sharing, and ultimately enhance individual and group contributions to the field of engineering. The monolithic and “silo” nature of CAD data in each CAD system and the great difficulty in viewing this data in any way other than in a CAD system or as a “dumb” model is definitely a major concern in many small and large design companies. If the Semantic Web can help alleviate this problem and allow real time querying and display of this data, it essentially creates a “lightweight” version of the CAD system and this information is now on the web, can be shared, can be dynamically collated, and does not need the CAD system after the initial instantiation process. This research aims to explore a product design semantic knowledge management system (PD-SKMS) and realize a knowledgecontextual design environment for product engineers by integrating design knowledge with Semantic Web technologies. This paper makes four significant contributions that differentiate it from other research groups in this domain: 1. It constructs a knowledge-contextual design environment based on concepts of semantic knowledge management; 2. It provides applications with the capability to query/reason and author/update on a host hybrid-data repository; 3. It develops a novel approach to present rich, dynamic visualizations of product data semantics through the Web; 4. It provides for the extension of knowledge sources by linking to external public linked data sources in a seamless manner. Through these contributions this work seeks to provide a completed and implemented framework and solution to publish product data semantics on the Web. Relevant technologies/ tools related to the semantic web are identified, integrated, and implemented for the product 24 more pages are available in the full version of this document, which may be purchased using the """"Add to Cart"""" button on the product's webpage: www.igi-global.com/article/online-semantic-knowledgemanagement-product/63644?camid=4v1 This title is available in InfoSci-Journals, InfoSci-Journal Disciplines Computer Science, Security, and Information Technology, InfoSci-Computer Systems and Software Engineering eJournal Collection, InfoSci-Networking, Mobile Applications, and Web Technologies eJournal Collection, InfoSci-Journal Disciplines Engineering, Natural, and Physical Science, InfoSci-Select. Recommend this product to"""	computer-aided design;database;information management;interoperability;linked data;ontology (information science);pervasive informatics;product engineering;research data archiving;semantic web;semantic knowledge management;user interface;web application	Lijuan Zhu;Uma Jayaram;Okjoon Kim	2011	Int. J. Semantic Web Inf. Syst.	10.4018/jswis.2011100102	computer science;sparql;semantic web;social semantic web;linked data;data mining;semantic web stack;database;ontology-based data integration;product design;world wide web	DB	-43.15320334874348	5.384622386978649	153624
e0f93ec68434e89e5d927cd08b9710b74386711b	multidimensional indexing and querying of xml in digital libraries and relational database systems		The rise of XML (Extensible Markup Language) as a new universal data format created the demand to store XML with established relational database technology. This became even more important as application fields of relational databases like digital library systems also turned towards a massive use of XML. This work presents an approach to map XML data to a relational schema. The mapping uses an approach based on simplified Multidimensional Hierarchical Clustering (MHC) which was originally designed for data warehousing. On the one hand we analyze the performance of the developed mapping in the context of navigation in XML documents, on the other hand we also examine which indexing technique for the relational schema is especially powerful for typical XML queries. XML and the storage in relational database management systems are techniques which are utilized in the second part of this work which describes the digital library system OMNIS/2. The system allows to access several remote systems via one user interface, to work with the documents of the remote system (annotation and linking of documents), to upload user-defined XML documents and to store them in the system. OMNIS/2 offers this additional functionality without changing the original systems but instead extends the systems in a meta layer.	database schema;digital library;hierarchical clustering;library (computing);library classification;markup language;model of hierarchical complexity;relational database management system;upload;user interface;xml	Michael G. Bauer	2004			xml framework;streaming xml;database;cxml;data mining;document structure description;xml schema editor;xml database;computer science;efficient xml interchange;xml validation	DB	-33.89749802493185	7.672893365736359	153977
15a944cb26f1713b1528c7933d857a1fa643bebc	une approche algébrique pour la réutilisation et l'orchestration de services dans les sysèmes d'information	busqueda informacion;anotacion;semantic annotation;ontologie;service composition;information retrieval;reutilizacion;salud publica;web semantique;service web;semantics;annotation;web service;semantica;semantique;orientado servicio;reuse;qualite service;intergiciel publication souscription;intergicial editor suscriptor;recherche information;web semantica;decouverte connaissance;semantic web;sante publique;descubrimiento conocimiento;ontologia;coordinacion;oriente service;information system;service discovery;ontology;publish subscribe middleware;service description algebra;systeme information;service quality;public health;reutilisation;servicio web;coordination;service oriented;calidad servicio;sistema informacion;knowledge discovery	In order to deal with business evolutions, it is imp ortant today to enable fast developments of new functionalities on the basis of existing Services reuse. In the context of information search within a federation of independen t systems, we address the problem of automated discovery and orchestration of Services. We propose an approach based on a domain ontology, and on the Services semantics capt ure by the mean of ontology properties and algebraic expressions. We present an algorithm t at generates orchestration plans, with characteristics of optimality regarding QoS. The ap proach has been validated by a prototype and an evaluation in the case of an Health Informat ion System. MOTS-CLÉS : ontologie, web sémantique, annotation sémantique, algèbre de description de service, composition de services, découverte de ser vice.	algorithm;linear algebra;ontology (information science);prototype	Yann Pollet	2010	Ingénierie des Systèmes d'Information	10.3166/isi.15.5.63-88	web service;public health;computer science;semantic web;ontology;data mining;reuse;database;semantics;service discovery;law;world wide web;service quality;information system	Web+IR	-38.54404213428637	12.696648971769202	154088
266fe82a4e754d160a793fe1a2085aca589769bc	eliciting concepts from the brazilian access law using a combined approach	text mining;knowledge extraction;non functional requirement;nfr framework;transparency;requirement pattern	Lately, organizations have been subject to regulation promoting information transparency; one example of this is the Brazilian Information Access Law. This paper presents a novel way of performing requirements elicitation using both the law and a Non-Functional Requirements Patterns catalog as the information sources. Since organizations must follow the law, its information systems must also implement the law as requirements. Our process is guided by pattern matching, text mining and grounded analysis. We examine the special case of the Brazilian Access Law using our approach, which compares a previously encoded transparency knowledge base with the law.	functional requirement;information access;information system;knowledge base;non-functional requirement;pattern matching;requirements elicitation;text mining	Priscila Engiel;Claudia Cappelli;Julio Cesar Sampaio do Prado Leite	2014		10.1145/2554850.2555057	text mining;computer science;knowledge management;data mining;database;knowledge extraction;transparency;computer security;non-functional requirement	Mobile	-43.12622761384126	5.974416330161964	154407
92d4ebce12fa310de96c4b84623cca940709d076	distributed reactive xml	process calculi;data processing;bigraphs;process calculus;concurrency control;xml;optimistic concurrency control;reactive system;xml document;middleware;reactive systems;model of computation;theoretical foundation;coordination model;peer to peer;coordination middleware	XML-centric models of computation have been proposed as an answer to the demand for interoperability, heterogeneity and openness in coordination models. We present a prototype implementation of an open XML-centric coordination middleware called Distributed Reactive XML. The middleware has as theoretical foundation a general distributed extensible process calculus inspired by the theory of Bigraphical Reactive Systems. The calculus is extensible just as XML is extensible, in that its signature and reaction rules are not fixed. It is distributed by allowing both the state of processes as well as the set of reaction rules to be distributed (or partly shared) between different clients. The calculus is implemented by representing process terms as XML documents stored in a value-oriented, peer-to-peer XML Store and reaction rules as XML transformations performed by the clients. The formalism does not require that only process terms are stored—inside process terms one may store application specific data as well. XML Store provides transparent sharing of process terms between all participating peers. Conflicts between concurrent reaction rules are handled by an optimistic concurrency control. The implementation thus provides an open XMLbased coordination middleware with a formal foundation that encompasses both the shared data, processes and reaction rules.	abstract machine;ambient calculus;bigraph;bisimulation;concurrency (computer science);interoperability;join-calculus;join-pattern;lambda calculus;location-based service;metamodeling;middleware;model of computation;openness;optimistic concurrency control;peer-to-peer;process calculus;prototype;semantics (computer science);ucph department of computer science;xdr schema;xml schema;xpath;xquery	Thomas T. Hildebrandt;Henning Niss;Martin Olsen;Jacob W. Winther	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.12.024	xml validation;xml encryption;xml namespace;simple api for xml;process calculus;xml;data processing;xml schema;reactive system;streaming xml;computer science;document structure description;xml framework;xml schema;database;distributed computing;xml signature;programming language;xml schema editor;cxml;efficient xml interchange	DB	-33.740929248878956	14.17484411128	154442
ba3013cf72a7a7c1dd006d8b0510c1bb33c49827	business and enterprise ontology management with symontox	representacion conocimientos;ontologie;red www;reseau web;semantics;semantica;semantique;internet;world wide web;knowledge representation;representation connaissances;ontology	Ontologies are emerging as a key solution for knowledge sharing in co-operative business environment. From the technology point of view, the growth of Internet use has favoured the development of environments devoted to collaborative and distributed work, allowing different communities to increase flexibility and effectiveness in their work. From the representation point of view, an ontology management system represents a powerful tool to create common and shareable knowledge repositories. The goal of this work is to present SymOntoX, a web-based ontology management system. It is an open source environment supporting collaborative and distributed ontology construction and maintenance.	information repository;internet;ontology (information science);open-source software;point of view (computer hardware company);web application	Michele Missikoff;Francesco Taglino	2002		10.1007/3-540-48005-6_38	upper ontology;the internet;bibliographic ontology;epistemology;ontology inference layer;computer science;knowledge management;ontology;artificial intelligence;ontology;data mining;database;semantics;linguistics;ontology-based data integration;world wide web;owl-s;process ontology;suggested upper merged ontology	AI	-38.32778336097978	12.318159244385674	154450
3e2e3bb6074b6c7ef40ea6cf34727a281a4f8ce4	constraint-based livespaces configuration management	user needs;constraint reasoning;constraint satisfaction;livespaces;ubiquitous workspaces;configuration;configuration management	In this paper, we describe use of constraint-based methods for configuring ubiquitous workspaces. A declarative representation allows succinct, easily maintainable definitions of the dependencies inherent in setting up a meeting, and permits the use of general constraint reasoners for various standard tasks such as setting up meeting interfaces, switching between setting for different meetings, and saving and restoring settings. Personalisation techniques can be used for intelligently adapting the workspace to individual user needs.	configuration management;constraint programming;personalization;workspace	Markus Stumptner;Bruce H. Thomas	2006		10.1145/1111449.1111536	constraint satisfaction;computer science;knowledge management;database;configuration management;configuration	HCI	-41.43462152516477	10.426056336947548	154606
7d5b2e1988a9fb6afc7d83a38c704379daf8ffd6	towards the application of reinforcement learning techniques for quality-based service selection in automated service composition	software quality learning artificial intelligence markov processes quality of service;service composition;learning;service selection;reinforcement learning;learning markov processes context process control optimization conferences globalization;process control;optimization;on the fly computing;markov processes;markov decision process;learning artificial intelligence;quality of service;on the fly computing service composition service selection reinforcement learning markov decision process;globalization;context;markov decision process reinforcement learning quality based service selection automated service composition on the fly computing project functional requirements;software quality;conferences	A major goal of the On-The-Fly Computing project is the automated composition of individual services based on services that are available in dynamic markets. Dependent on the granularity of a market, different alternatives that satisfy the requested functional requirements may emerge. In order to select the best solution, services are usually selected with respect to their quality in terms of inherent non-functional properties. In this paper, we describe our idea of how to model this service selection process as a Markov Decision Process, which we in turn intend to solve by means of Reinforcement Learning techniques in order to control the underlying service composition process. In addition, some initial issues with respect to our approach are addressed.	functional requirement;markov chain;markov decision process;reinforcement learning;service composability principle	Alexander Jungmann;Bernd Kleinjohann	2012	2012 IEEE Ninth International Conference on Services Computing	10.1109/SCC.2012.76	computer science;knowledge management;machine learning;management science	Robotics	-46.08941008568872	15.94885233568047	154644
a9a10bbbcdc653f8de250e527a00f16f93bf954e	towards a semantic-driven and scalable publish/subscribe framework	open sharing;publish subscribe;multimedia sharing;scalability;p2p networks;dht;ontology	Heterogeneous networks, including personal, home and collaborative enterprise networks are aimed at interconnecting diverse communities of users. In the case of large communities of mobile, distributed and collaborating users sharing their multimedia content, scalable and semantic approaches are needed to facilitate the efficient publication and discovery of data. In this work, we propose an approach aimed at efficiently providing the required functionalities for dynamically managing the resources and the multimedia content of interconnected home networks, as well as providing the required community sharing capabilities. Our solution consists in combining the benefits of both ontologies and peer-to-peer approaches, by taking advantage of the semantic capabilities of ontologies at local domain level and the scalable P2P solutions at inter-domains level. Our proposed framework is founded on a new architecture based on peer-to-peer event-based communication system and using mainly a home box entity based on a global multimedia ontology. This domain ontology allows publishers and subscribers to use a common semantic space to characterise production and consumption of resources and services. Our approach can also be applied for similar large interconnected systems scenarios such as topic discovery with JNDI and service exchange with UDDI.	autonomic computing;distributed hash table;entity;inter-domain;java naming and directory interface;ontology (information science);peer-to-peer;publish–subscribe pattern;routing;scalability;semantic network;service-oriented architecture;user research;web services discovery	Amina Chaabane;Codé Diop;Wassef Louati;Mohamed Jmaiel;Jorge R. Gomez-Montalvo;Ernesto Exposito	2013	IJIPT	10.1504/IJIPT.2013.055472	scalability;computer science;ontology;data mining;database;publish–subscribe pattern;world wide web;computer network	DB	-46.06614035625793	11.391357523427805	154741
01864080a6883225102870c9d127a4c0ff51b3fb	semantic representation of information objects for digital resources management	intelligent web user interfaces;personal information management;ontology based user interaction;semantic technologies;ontology based access to digital resources	Current users of digital devices have to face the management of a huge amount of heterogeneous digital resources, the switch between activity contexts, and the interaction with many different applications and services. This situation leads to a very fragmented interaction experience, which poses a great cognitive overload for the users and risks to cause lack of efficiency and loss of information. Starting from the limitations of both traditional mechanisms for Personal Information Management (based on the notions of files and hierarchical folders) and new proposals (tagging and folksonomies), in this paper we present Semantic T++, a system supporting users in collaboratively handling digital resources, based on the notion of “tables” (thematic Web-based collaborative workspaces), populated by “objects” (shared digital resources). Semantic T++ exploits a formal semantic representation of such objects to support users in organizing, selecting and using them. Its core is represented by an ontology which models table objects as “information elements” having properties and relations mainly (but not only) related to their content. Reasoning techniques can be applied to infer knowledge useful to provide users with a flexible access to table objects, based on different criteria, which can be defined and combined by the user on the basis of her needs. In order to evaluate our model, we demonstrated its technical feasibility by developing a proof-of-concept prototype, and we showed its advantages in the access to personal and shared resources by discussing the results of a user test.	folksonomy;ontology (information science);organizing (structure);personal information management;population;prototype;tag (metadata);workspace	Anna Goy;Diego Magro;Giovanna Petrone;Marino Segnan	2014	Intelligenza Artificiale	10.3233/IA-140070	computer science;knowledge management;personal information management;data mining;semantic technology;world wide web	HCI	-42.419098191286054	8.38486503005679	155084
d27ffcc2daa43770bd70a9950c0dadbc1c636941	approach to manage semantic informations from ugc		The purpose of this work is to face the issue of classification variety and non-homogeneity, especially in Web 2.0, for User Generated Content coming from popular digital platforms. The solution offered to this problem is an approach based on an ontology that can represent information, typically associated with UGCs, integrated with a unique mapping technique amongst ontology contents and UGCs contents coming from other platforms. Regarding standard information and information shared by many of these objects, existing relations are exploited through mapping, when possible; otherwise new ones are created when it is deemed necessary. Such an ontology can represent, as embedded information, folksonomies and all nonstandard information. That kind of information, despite being unclassifiable by means of standard schemas like the UGC ones, can be mapped. Rather than representing all properties of digital content, we were concerned with having an ontology that could associate semantic value to every tag, standard and not.	categorization;digital recording;dublin core;embedded system;exif;extensible metadata platform;folksonomy;synergy;user-generated content;web 2.0	Maria Ilaria Lunesu;Filippo Eros Pani;Giulio Concas	2011			user-generated content;data mining;computer science;ontology;knowledge management;knowledge base	Web+IR	-41.20012339316699	6.636545868432086	155242
9377d3aa61d8ad93c927166ce5d7807ae4a5c733	towards an open repository environment	informacion documentacion;ciencias sociales	Repositories used to be fairly monolithic systems, with a single object store, a tailored content model, and a dedicated application on top. First steps for searching across repositories (e.g. Z39.50, first drafted in 1988; OAI-PMH for content aggregation, first released 2001) are an exception to this. However, we are still far away from an ”open repository environment“, in which repositories interact on all levels with other agents (e.g. other repositories, added-value services, registries). This paper creates a more fine-grained view on repository federation and analyses existing approaches by decomposing them into a physical, a logical, and a conceptual layer for both the object and the system. Among these attributes, the most evident gap pertains to interaction ”patterns“ between agents. In particular, the notification pattern is more immediate and directed than existing query and harvesting mechanisms, enabling new federation scenarios and laying the grounds for open repository environments. Prototypes of the concepts presented here are being implemented in the scope of the project Dariah, which establishes an e-Infrastructure for the humanities.	aggregate data;archive;blog;chart;cyberinfrastructure;document classification;federated identity;high- and low-level;in the beginning... was the command line;outsourcing;portals;prototype;semantic interoperability;trackback;virtual artifact	Andreas Aschenbrenner;Tobias Blanke;Marc Wilhelm Küster;Wolfgang Pempe	2010	J. Digit. Inf.		computer science;data mining;database;world wide web	Web+IR	-40.677147623965936	8.921417573914388	155274
d60bcbd6add66cc20e190e065fb773bda859befa	representing annotations in xml document using string-trees model	representation;annotations;xml	The flexibility of XML allows document to be annotated easily. However, these annotations come from different sources like Wordnet thesaurus, POS, DTD, semantic roles etc. These annotations can either be combined in the same document or captured separately in different document. The former, though richer in annotations, may look messy and requires more parsing time. The latter needs control of document consistency. This paper proposes a string-trees model to represent XML document for multiple sources of annotations. This model extends the existing string-tree structure for linguistic content in order to support structured contents of XML document. In this paper, we describe how this model is refined and applied on XML document.	parsing;point of sale;thesaurus;tree structure;wordnet;xml	Gan Keng Hoon	2014		10.1145/2684200.2684311	well-formed document;xml catalog;xml validation;simple api for xml;xml;xml schema;streaming xml;computer science;document type definition;document definition markup language;document structure description;xml framework;document type declaration;xml database;xml schema;database;document schema definition languages;xml signature;world wide web;xml schema editor;information retrieval;efficient xml interchange;design document listing	Web+IR	-36.294665093794436	7.445081312487572	155440
a1d90182373309fa68f7c93853746a0e70386886	time management for web service composition	web services time management;web services time factors conference management disaster management web and internet services collaborative work educational institutions computer science environmental management companies;time management web service composition;web service;time management;time management model;web service composition;web services;time management model web service composition	Recently Web service composition is a hot research spot, but there is no any transitional management issues concerned, including the time management. The time management is a crucial issue in web service composition. This paper presents a time management model for the environment. In this model a service should provide necessary time value,. In this way Web service definition language will be expanded, and time management in web service composition can be simplified. On the other hand, it preserves the autonomy of the web services.	application lifecycle management;autonomy;loose coupling;run time (program lifecycle phase);semantic grid;service composability principle;time value of money;web service	Fuwei Fan;Ying Li;Shuiguang Deng	2007	2007 11th International Conference on Computer Supported Cooperative Work in Design	10.1109/CSCWD.2007.4281495	web service;web application security;web development;web modeling;data web;web analytics;web design;service product management;web accessibility initiative;web standards;postback;knowledge management;ws-policy;web navigation;multimedia;internet presence management;web intelligence;web 2.0;law;world wide web	DB	-48.265670348362725	15.290735999529318	155669
3cbfc367de8595a2b35876e3617830e432b5c1a9	technical perspective: attacking the problem of consistent query answering		Inconsistent data refers to data that do not adhere to one or more constraints. The term constraints refers to conditions that need to be imposed on the data. Constraints often arise from organizational requirements or business logic, such as the requirement that every employee in the database must be uniquely identified by the employee id, or every employee must work on some project, or the expenses cannot exceed the credit limit, or even a desired designated format for storing phone numbers. The need to manage inconsistent data arises in many settings. Quite typically, when one integrates data from different sources, the integrated data can be inconsistent data even when the data sources may be individually consistent. Another scenario where inconsistency in data can arise is when data and/or schema evolves, for example, through the addition or removal of data, changes in schema, or knowledge of new constraints.	business logic;constraint (mathematics);database schema;requirement;telephone number	Wang Chiew Tan	2016	SIGMOD Record	10.1145/2949741.2949745	data mining;computer science	DB	-37.21097302608226	10.198521661651059	155790
6fbf2276f907721df75804417456337e56c04f0b	service maps in xml	specification;matchmaking;web service;service map	Increasing number of services available on web makes service discovery a difficult problem. Existing webservice search techniques follow search by names or inputs, outputs. Here we propose a new method to search web services on the basis of service elements and the maps they make. Service maps are also found useful in exploring alternatives and business promotions associated with. The proposed concept is being implemented in jUDDI an open source software for service registry.		Supriya Vaddi;Hrushikesha Mohanty;R. K. Shyamasundar	2012		10.1145/2381716.2381838	web service;service level requirement;web mapping;application service provider;differentiated service;computer science;service delivery framework;ws-policy;service design;data mining;database;service discovery;data as a service;world wide web;web coverage service	Web+IR	-45.06627979332799	13.40055175774238	155791
6b1f033bd31c577b87c07d309fb3f9e3d982c86d	a three-level schema architecture for the conceptual design of web-based information systems: from web-data management to integrated web-data and web-process management	e commerce;data management;web based information system;integrated design;process management;conceptual design;web page design;object oriented approach;business process	It has been recognized only recently that, like databases, web sites need models and schemes. Data-intensive web sites are best developed using a multi-level design approach proceeding from data design via navigation design to web-page design. Modern web-based information systems are no longer static in nature. Rather they are dynamic. Besides querying, they support workflow tasks and e-commerce transactions. The design of such systems needs to consider the underlying business process next to the data. Their integrated design has been mainly treated in an ad-hoc way so far. In this paper, we present a three-level schema architecture for the conceptual design of dynamic web-based information systems. We employ an object-oriented approach that integrates data and process management and complements previous approaches for the design of data-intensive web sites.	business process;computer architecture;data-intensive computing;database;database design;distributed database;e-commerce;e-commerce payment system;fragmentation (computing);graphical user interface;html;hoc (programming language);information engineering;information system;java servlet;level design;machine translation;relevance;semiconductor industry;web application;web engineering;world wide web;xml database	Günter Preuner;Michael Schrefl	2000	World Wide Web	10.1023/A:1019281629747	e-commerce;web service;web application security;web development;web modeling;data web;web mapping;web design;data management;web standards;computer science;knowledge management;three schema approach;web navigation;social semantic web;conceptual design;database;design education;business process;web intelligence;web engineering;world wide web;web design program;systems design	DB	-39.7195328519549	11.20754925567533	155796
b71c8545ccbabe9bcce1a7c4d60ae16b6db7e508	constructing virtual documents for ontology matching using mapreduce	iterative process;large ontology;virtual document technique;mapreduce framework;ontology matching;oaei dataset;run time;virtual document;simple process;ontology matchers	Ontology matching is a crucial task for data integration and management on the Semantic Web. The ontology matching techniques today can solve many problems from heterogeneity of ontologies to some extent. However, for matching large ontologies, most ontology matchers take too long run time and have strong requirements on running environment. Based on the MapReduce framework and the virtual document technique, in this paper, we propose a 3-stage MapReduce-based approach called V-Doc+ for matching large ontologies, which significantly reduces the run time while keeping good precision and recall. Firstly, we establish four MapReduce processes to construct virtual document for each entity (class, property or instance), which consist of a simple process for the descriptions of entities, an iterative process for the descriptions of blank nodes and two processes for exchanging the descriptions with neighbors. Then, we use a word-weight-based partition method to calculate similarities between entities in the corresponding reducers. We report our results from two experiments on an OAEI dataset and a dataset from the biology domain. Its performance is assessed by comparing with existing ontology matchers. Additionally, we show how run time is reduced with increasing the size of cluster.	algorithm;balance theory;blank node;computation;disk partitioning;dynamic web page;entity;entity–relationship model;experiment;iteration;mapreduce;ontology (information science);ontology alignment;parallel computing;precision and recall;preprocessor;requirement;run time (program lifecycle phase);runtime system;semantic web;speedup	Hang Zhang;Wei Hu;Yuzhong Qu	2011		10.1007/978-3-642-29923-0_4	computer science;data mining;database;ontology-based data integration;world wide web;process ontology	Web+IR	-37.07248265191625	5.841075511164775	155931
7fca92f36cfdbc2aa4bc7c04c5df38ce4ed02f7b	position paper: description schemes for mathematical web services	web service		web service	Mike Dewar;David Carlisle;Olga Caprotti	2002			web modeling;web standards;position paper;web service;theoretical computer science;database;ws-addressing;ws-i basic profile;ws-policy;web application security;computer science	Vision	-42.49824681533404	12.394374787898613	155985
c4000e0eb20002f4f39caeba5004078475a7e64a	analyzing web service resource compatibility	сети петри;модули потоков работ;веб сервисы;бездефектность;совместимость	In this work we consider modeling of services with workflow modules, which are a subclass of Petri nets. The service compatibility problem is to answer the question, whether two Web services fit together, i.e. whether the composed system is sound. We study complementarity of service produced/consumed resources, that is a necessary condition for the service compatibility. Resources, which are produced/consumed by a Web service, are described as a multiset language. We define an algebra of multiset languages and present an algorithm for checking the conformance of resources for two given structured workflow modules.	algorithm;complementarity theory;conformance testing;petri net;regular expression;sensor;software incompatibility;time complexity;web service	Irina A. Lomazova;Ivan V. Romanov	2012			computer science;theoretical computer science;database;world wide web	DB	-45.41251910150031	17.607969122792813	156026
e7b6cde7219185252cc58bcd2bccdf4a8f548d34	a semantic description approach for telecommunications network capability services	semantic web service;telecommunication network capability service;owl s;context aware;semantic parlay;web service;semantic gap;semantic description;semantic parlay x;telecommunication service domain ontology;service oriented architecture;domain ontology;ontology;telecommunication networks;dynamic service composition	The provision of pervasive services in the ubiquitous convergent network presents a great challenge in light of well known problems like context-awareness, semantic service description, accurate service query and dynamic service composition. In order to eliminate the semantic gap of Telecommunication Network and Internet in the service layer, we propose the provision of the semantic Telecommunications Network Capability Services (TNCS). By comparing the differences between TNCS and the plain Web Services, we present a semantic description method of TNCS by extending OWL-S. Then using this approach, we demonstrate a service profile description case for semantic ParlayX. In this way, the ontology-based accurate query, matching, automatic composition and invocation for telecommunication network capability services can be supported. The semantic convergence of telecommunication network and Internet in the service layer could be facilitated.	telecommunications network	Xiuquan Qiao;Xiaofeng Li;Tian You	2008		10.1007/978-3-540-88623-5_34	web service;semantic computing;semantic web rule language;ontology inference layer;semantic grid;differentiated service;computer science;knowledge management;service-oriented architecture;ontology;social semantic web;semantic web stack;database;semantic technology;law;world wide web;owl-s;semantic analytics;semantic gap	Vision	-44.3857967777949	12.729213885429438	156080
27357d4d655c2bd26736d7d2ddf42313ad2c044f	multidimensional semistructured data: representing context-dependent information on the web	a frame;base donnee;modele echange donnee n dimensions;red www;reseau web;database;base dato;pregunta documental;multidimensional data;estructura triangulada plana;question documentaire;semistructured data;structure triangulee plane;semi structured data;internet;modelo n dimensiones;dato semi estructurado;multidimensional model;contexto;query;contexte;world wide web;context dependent;echange donnee informatise;context;electronic data interchange;modele n dimensions;cambio dato electronico;donnee semistructuree	In this paper, we address a problem common in the frame of WWW, namely, representing information that assumes different facets under different contexts (sets of worlds). For expressing context-dependent (or multidimensional) data, we introduce Multidimensional Semistructured Data, where context is defined through (dimension, value) pairs. An extension of OEM called Multidimensional Object Exchange Model (MOEM) is introduced, for representing multidimensional data. We discuss the properties of MOEM and define a transformation that, for any given world, reduces MOEM to a conventional OEM holding under that world. As a case study, we show how MOEM can be used to represent changes over time in an OEM database.	context-sensitive language;object exchange model;www;world wide web	Yannis Stavrakas;Manolis Gergatsoulis	2002		10.1007/3-540-47961-9_15	semi-structured data;the internet;computer science;context-dependent memory;electronic data interchange;data mining;database;world wide web	DB	-36.19994831085979	11.652315771834349	156097
2d41a554395b600685123355d222b0188be0bcf6	an agent-based method and tool to add collaborative capabilities based on standalone cad systems for grid computing environment	agent based;grid computing environment;collaborative application;data management;group awareness;consistency maintenance;grid computing;software agents cad data encapsulation database management systems grid computing groupware;homogeneous standalone cad applications agent based method agent based tool collaborative capabilities standalone cad systems grid computing environment grid computing applications internal encapsulation external encapsulation legacy standalone cad applications consistency maintenance group awareness data management prototype tools heterogeneous standalone cad applications	The importance of collaboration to typical grid computing applications is becoming widely recognized. This paper explores how to add collaborative capabilities to standalone CAD applications in grid computing environment. The method uses internal encapsulation and external encapsulation to wrapper legacy standalone CAD applications. The basic issues for collaborative applications, such as consistency maintenance, group awareness and data management, are discussed. The proposed method is implemented with case study of prototype tools to integrate both homogeneous and heterogeneous standalone CAD applications for collaborative applications.	computer-aided design;grid computing	Bin Liao;Xunyong Pan	2006		10.1109/SKG.2006.20	embedded system;data management;computer science;database;distributed computing;grid computing	HPC	-39.856159897533026	18.05347866571454	156102
6a4220a31a57c0b58278260d4d42407062383f70	transformation of the common information model to owl	common information model;web ontology language;semantic web;open standard;structured data	Managing an IT environment requires the exchange of structured data between different agents. The Common Information Model (CIM) is a comprehensive open standard that specifies how managed elements in an IT environment are modelled as a set of common objects and relationships between them. It has however limited support for knowledge interoperability and aggregation, as well as reasoning. By converting the existing CIM model into a format that can be processed by semantic web tools, these limitations can be overcome. This paper describes how CIM can be converted into a Web Ontology Language (OWL) ontology including constructs for which no obvious direct conversion exists, such as CIM qualifiers.	computer-integrated manufacturing;consortium;entity;field (computer science);information model;interoperability;lecture notes in computer science;object constraint language;ontology (information science);pl/i;programming language;sql;scala;semantic web rule language;springer (tank);unified modeling language;web ontology language;world wide web	Andreas Textor;Jeanne Stynes;Reinhold Kröger	2010		10.1007/978-3-642-16985-4_15	open standard;data model;computer science;ontology;artificial intelligence;semantic web;data mining;semantic web stack;database;web ontology language;world wide web;owl-s	Web+IR	-37.981636939347034	7.135940249458017	156135
8fe88f7f5455454b331d33c7d53fd2affea9fe01	meta-model based information mediation	meta model	INTRODUCTION Information mediation is a research area that deals with integrating information from different, usually heterogeneous, information sources, including regular databases, XML source, record files, email systems, etc. The software that handles or masks data heterogeneity from end users is called a mediator. ABSTRACT Information mediation is one of the major approaches to solve interoperability problems related to heterogeneous information integration. This paper first discusses the concept of information mediation and typical mediation architecture. Two major mediation research projects, TSIMMIS and MIX, and their limitations, are discussed. Meta-model, a way for exchanging metadata, is then introduced for the purpose of improving information mediation. Finally, a meta-model based mediation approach is proposed. 701 E. Chocolate Avenue, Hershey PA 17033-1240, USA Tel: 717/533-8845; Fax 717/533-8661; URL-http://www.idea-group.com IDEA GROUP PUBLISHING	database;email;fax;interoperability;mix;matchware mediator;metamodeling;metaobject;pa-risc;uniform resource identifier;xml	Luyin Zhao;Keng Siau	2003			data mining;sql;information model;metamodeling;data model;mediation (marxist theory and media studies);computer science;user interface	DB	-37.25809461774291	8.604304765457442	156154
617f43e69f43888235fa8c42bb438f3b280ded57	coordinating knowledge elicitation to support context construction in cooperative information systems	databases;scopes;control systems;groupware;nonmonotonic process;electronic mail;information systems;information systems databases control systems optical wavelength conversion electronic mail space exploration decision making force control communication system control data models;heterogeneous information systems;coordination mechanisms;cooperative information system;space exploration;context construction;coordination mechanism;heterogeneous information;summary schema model;semantic coordinator over parallel exploration spaces;cooperative systems;concept hierarchy model;cooperative information systems;knowledge acquisition;semantic knowledge;distributed databases;knowledge elicitation coordination;concept hierarchy;context dependent;knowledge elicitation;semantic reconciliation techniques;optical wavelength conversion;communication system control;data models;coordination mechanism knowledge elicitation coordination context construction cooperative information systems semantic reconciliation techniques concept hierarchy model summary schema model heterogeneous information systems nonmonotonic process semantic knowledge semantic coordinator over parallel exploration spaces scopes;force control	Two popular semantic reconciliation techniques namely, the Concept Hierarchy Model and the Summary Schema Model, are evaluated for their strengths and limitations in facilitating cooperation between heterogeneous information systems. These techniques fail to recognize that reconciliation is context dependent, and context construction is a nonmonotonic process which requires coordinating multiple sources of semantic knowledge pertinent to a specific request. We show how their limitations can be overcome, while exploiting their strengths, in the SCOPES (Semantic Coordinator Over Parallel Exploration Spaces) architecture. SCOPES (A. Ouksel and C. Naiman, 1994) basically provides the coordination mechanism that elicits the knowledge necessary to build the context for reconciliation.	information system	Aris M. Ouksel;Iqbal Ahmed	1996		10.1109/COOPIS.1996.554952	data modeling;semantic memory;computer science;knowledge management;control system;artificial intelligence;space exploration;context-dependent memory;data mining;database;distributed database;information system	HCI	-46.33566593390028	7.990101808953421	156174
cbb17d327ce8b74d685a0104b51079e5e06c7d36	semantic document model to enhance data and knowledge interoperability	semantic annotation;document model;satisfiability;document management system;semantic web	In order to enable document data and knowledge to be efficiently shared and reused across application, enterprise, and community boundaries, desktop documents should be completely open and queryable resources, whose data and knowledge are represented in a form understandable to both humans and machines. At the same time, these are the requirements that desktop documents need to satisfy in order to contribute to the visions of the Semantic Web. With the aim of achieving this goal, we have developed the Semantic Document Model (SDM), which turns desktop documents into Semantic Documents as uniquely identified and semantically annotated composite resources, that can be instantiated into human-readable (HR) and machine-processable (MP) forms. In this paper, we present the SDM along with an RDF and ontology-based solution for the MP document instance. Moreover, on top of the proposed model, we have built the Semantic Document Management System (SDMS), which provides a set of services that exploit the model. As an application example that takes advantage of SDMS services, we have extended MS Office with a set of tools that enables users to transform MS Office documents (e.g., MS Word and MS PowerPoint) into Semantic Documents, and to search local and distant semantic document repositories for document content units (CUs) over Semantic Web protocols.	desktop computer;discoverability;human-readable medium;information retrieval;intelligent agent;interoperability;management system;microsoft word for mac;nepomuk;ontology (information science);openoffice basic;plug-in (computing);portable document format;recommender system;requirement;resource description framework;sparql;semantic web;software agent;solid-state drive;transformer	Sasa Nesic	2009		10.1007/978-1-4419-1219-0_6	semantic data model;well-formed document;semantic interoperability;semantic computing;semantic web rule language;explicit semantic analysis;document clustering;semantic search;semantic grid;semantic web;social semantic web;document management system;semantic web stack;semantic compression;database;semantic equivalence;semantic technology;world wide web;information retrieval;semantic analytics	Web+IR	-40.436031540413126	6.726477266727504	156274
8c270384176cfc6a71caa3f0c0d13f3a582dccc9	caveat mercator in electronic commerce: an update	ecommerce;outil logiciel;commerce electronique;software tool;tratamiento transaccion;electronic commerce;decision support tool;mise a jour;seller beware;comercio electronico;contre mesure electronique;systeme aide decision;software agent;logicial personalizado;sistema ayuda decision;agent logiciel;interface agent;intergiciel;software agents;actualizacion;human interface;decision support system;contra medida electronica;middleware;electronic countermeasure;transaction processing;herramienta software;electronic trade;updating;traitement transaction	Article history: Received 24 October 2007 Received in revised form 4 June 2008 Accepted 19 June 2008 Available online 6 July 2008 Conway and Koehler presented a new type of software agent that converted merchant interfaces into middleware that enabled a user to bring to bear powerful decision support tools in eCommerce transactions. They called them Interface Agents. These agents operated directly through the human interface and were largely indistinguishable from a human user. They illustrated their ideas with an agent that could play optimal Blackjack at the then emerging online casinos. They discussed possible merchant countermeasures. In this paper we look back at this setting and see what evolved and how such agents have fared. We reassess their proposed countermeasures and update them based on the ever evolving cat-and-mouse game between such agents and merchants. © 2008 Elsevier B.V. All rights reserved.	blackjack;conway's game of life;countermeasure (computer);decision support system;e-commerce;middleware;software agent;universal transverse mercator coordinate system;user interface	Daniel G. Conway;Gary J. Koehler	2008	Decision Support Systems	10.1016/j.dss.2008.06.009	e-commerce;simulation;computer science;artificial intelligence;operations management;software agent;computer security	AI	-39.63338465509473	16.150954965615057	156289
2fb2cd1deeee4f1fb323c4350fd6317f23ff7bbb	querying the web of data with xsparql 1.1		On the Web and in corporate environments there exists a lot of data in various formats. XQuery and XSLT serve as query and transformation languages for XML. But as RDF also becomes a mainstream format for Web of data, transformations languages between these formats are required. XSPARQL is a hybrid language that provides an integration framework for XML, RDF, but also JSON and relational data by partially combining several languages such as XQuery, SPARQL and SQL. In this paper, we present the latest open source release of the XSPARQL engine, which is based on standard software components (Jena and Saxon) and outline possible applications of XSPARQL 1.1 to address Web data integration use cases.	component-based software engineering;json;open-source software;resource description framework;sparql;sql;semantic web;transformation language;world wide web;xml;xquery;xslt	Daniele Dell'Aglio;Axel Polleres;Nuno Lopes;Stefan Bischof	2014			data exchange;web service;rdf/xml;cwm;web modeling;turtle;xslt;computer science;sparql;rdf;database;world wide web;information retrieval	Web+IR	-36.50683855215609	8.12875529431397	156488
b2fa186ba9e69309cb61c5be3f5134a52481700f	petri net representation of ontologies for indoor location- based services		This paper presents a Petri net representation of an ontology for indoor location-based services. The ontology is described within the Resource Description Framework (RDF). Therefore, this paper proposes a method for transforming the RDF model into a CPN (colored Petri net). The proposed method represents the semantics of the RDF model in the CPN by mapping the classes and properties of the RDF model onto CPN places and representing the relationships between those classes and properties as token transitions in the CPN. To represent an RDF statement in the CPN, the method introduces a transition that produces a (complex) token composed of two ordered (simple) tokens: one corresponding to the subject and the other corresponding to the object of the statement. Applying the proposed method, we build a sample CPN for an RDF model and perform simulations using the model to answer RDF queries. This paper also introduces a simple ontology for an indoor location- based service. Using the proposed transformation method, we transform the ontology into a CPN. Finally, we introduce our inference algorithm for the CPN and a prototype database system for demonstrating the practicality of our method. The results indicate that the proposed database system can resolve the semantic ambiguities in the query by using the ontology.	ontology (information science);petri net	Jaegeol Yim;Jaehun Joo;Gyeyoung Lee	2011		10.1007/978-3-642-27180-9_52	rdf;petri net;security token;ontology (information science);location-based service;ontology;machine learning;inference;artificial intelligence;semantics;computer science	HCI	-35.95910353248302	7.818407549361852	156587
b7631e24e12e6913cc120483d6299fcd705a2963	real-time log query interface for large datasets using apache spark		1University of California, Los Angeles {sandha,xinxu129,yuexin,lizhehan}@cs.ucla.edu ABSTRACT Log Query Interface is an interactive web application that allows users to query the very large data logs of MobileInsight easily and efficiently. With this interface, users no longer need to talk to the database through command line queries, nor to install the MobileInsight client locally to fetch data. Users can simply select/type the query message through our web based system which queries the database very efficiently and responds back to user. While testing on 6GB of datasets our system takes less than 1 seconds to respond back, the similar queries on traditional MySql database takes more than 60 seconds. The system gives user the capability to execute all the queries using sql query language. User can perform complex join operations on very large tables. The query response time is hugely improved by the server side Spark clusters, which stores the big datasets in a distributed system and execute the query in parallel on multiple machines.	apache spark;command-line interface;distributed computing;mysql;query language;real-time transcription;response time (technology);sql;server (computing);server-side;table (database);web application	Sandeep Sandha;Xin Xu;Yue Xin;Zhehan Li	2016	CoRR		web application;sql;server-side;query language;spark (mathematics);database;response time;computer science	DB	-35.16321627163231	16.59976340605546	156752
dca50325340d58bf04688a7fe5ab106d6fcaa312	a semantic mediation approach for problems in computational molecular biology	biology computing;programming language semantics;high level languages;mediation biology computing computational biology proteins xml engines bioinformatics web pages relational databases information filtering;formal languages;information access;formal semantics;computational method;information integration problems semantic mediation approach computational molecular biology life sciences information access synergic combination high level programmable planner engine formal semantics;computational molecular biology;information integration;high level languages molecular biophysics biology computing programming language semantics formal languages;molecular biology;molecular biophysics;life sciences	As great amount of data are being produced and accumulated in Life Sciences, information access and consequent integration in analytical and computational methods become critical issues in order to conduct research. In this work we describe how the synergic combination of a high-level programmable planner engine and formal semantics are suitable to resolve many information integration problems in molecular biology.	computation;high- and low-level;information access;semantics (computer science);synergy	Monica Chagoyen;M. Erdem Kurul;Pedro A. de Alarcón;Simone Santini;Bertram Ludäscher;José María Carazo;Amarnath Gupta	2003		10.1109/CSB.2003.1227339	natural language processing;biochemistry;formal language;computer science;bioinformatics;information integration;theoretical computer science;machine learning;formal semantics;high-level programming language;computational semantics;molecular biophysics	Comp.	-35.184640393198244	8.36260792327188	156833
09974548805f6e7daa40c64d299e2a1a5a2fccee	advanced owl 2.0 ontology visualization in owlgred				Karlis Cerans;Julija Ovcinnikova;Renars Liepins;Arturs Sprogis	2012		10.3233/978-1-61499-161-8-41	open biomedical ontologies;database;ontology;owl-s;visualization;web ontology language;computer science	HCI	-39.37011998570196	6.687661547245691	156864
d8f149a894f946e1dd62a678dddb7055f2579dbe	short-lived ontology approach for agent/hla federated enterprise interoperability	collaborative framework;distributed system;enterprise interoperability federated approach;mda;hla;application software;implementation;information technology;distributed processing;collaboration;ontologies artificial intelligence;short lived ontology enterprise modeling enterprise interoperability federated approach agent approach mda mdi hla collaborative framework distributed simulation implementation;software agents;agent approach;computer architecture;software architecture;multi agent systems;internet;data privacy;mdi;business data processing;information exchange;military standards;short lived ontology approach;artificial intelligence;enterprise modeling;ontologies;software standards;distributed enterprise models short lived ontology approach high level architecture federation oriented enterprise interoperability concept artificial agent;distributed simulation;open systems;high level architecture;federation oriented enterprise interoperability concept;software architecture artificial intelligence business data processing distributed processing multi agent systems ontologies artificial intelligence open systems;distributed enterprise models;short lived ontology;artificial agent;ontologies military standards computer architecture software agents collaboration application software internet information technology software standards data privacy	This paper aims at proposing an implementation of the Federation oriented Enterprise Interoperability concept, using the rising notion of Short-Lived Ontology. We give first, a review of ongoing researches on Enterprise Interoperability. Then, we recall on Artificial Agent Concept and HLA Standard that appear to be adequate to support execution of the studied con-cept. Indeed, on the one hand Agent dialogue fits the concept of information exchange in a federated enterprise interoperability approach, on the other hand the HLA standard, initially designed for military M&S purpose, can be transposed for enterprise interoperability at the implementation level, reusing the years of experiences in distributed systems. From these post-ulates, we propose the first Agent/HLA framework Short-Lived Ontology based to implement distributed enterprise models from the conceptual level of federated enterprise interoperability approach.	distributed computing;enterprise interoperability;fits;information exchange;intelligent agent;naruto shippuden: clash of ninja revolution 3	Gregory Zacharewicz;David Chen;Bruno Vallespir	2009	2009 International Conference on Interoperability for Enterprise Software and Applications China	10.1109/I-ESA.2009.27	semantic interoperability;interoperability;enterprise systems engineering;enterprise software;computer science;knowledge management;database;enterprise integration;cross-domain interoperability;world wide web;enterprise information system	DB	-43.41678030923979	17.537345309648916	156884
35f316fed3e42d3419585164ea62cf1837960326	synthesizing the mediator with jabc/abc.		In this paper we show how to apply a tableau-based software composition technique to automatically generate the mediator’s service logic. This uses an LTL planning (or configuration) algorithm originally embedded in the ABC and in the ETI platforms. The algorithm works on the basis of the existing jABC library of available services (SIB library) and of an enhanced description of their semantics given in terms of a taxonomic classification of their behaviour (modules) and abstract interfaces/messages (types). 1 The SWS Challenge Mediator The ongoing Sematic Web Service Challenge [19] proposes a number of increasingly complex scenarios for workflow-based service mediation and service discovery. We use here the technology presented in [10] to synthesise a process that realizes the communication layer for the Challenge’s initial mediation scenario. In this scenario, a customer (technically, a client) initiates a Purchase Order Request specified by a special message format (RosettaNet PIP3A4) and waits for a corresponding Purchase Order Confirmation according to the same RosettaNet standard. The seller however does not support this standard. Its backend system or server awaits an order in a proprietary message format and provides appropriate Web Services to serve the request in the proprietary format. As client and server here speak different languages, there is a need for a mediation layer that adapts both the data formats and also the granularity. Of course we can easily define the concrete process within our jABC modelling framework, as we have shown in the past [11, 6, 7]. To provide a more flexible solution framework, especially to accommodate later declarative specification changes on the backend side or on the data flow, we synthesize the whole mediator using the synthesis technology introduced in [10]. We proceed here exactly along the lines already presented in that paper. In the following, we show in Sect. 2 how to use the SLTL synthesis methodology to generate the mediator workflow based on a knowledge base that expresses the semantics Fig. 1. The SWS Challenge Mediator Type Taxonomy Fig. 2. The SWS Challenge Mediator Module Taxonomy of the concrete types from the SWS mediator scenario, then in Sect. 3 we add a more business-level-like abstraction to the knowledge base, and in Sect. 4 we show how this leads to a looser solution, and how this solution can be stepwisely refined towards the first solution by adding business-level knowledge to the problem definition, in a declarative way. Sect. 5 describes how to work with the synthesis tool. Finally, Sect. 6 discusses related work and Sect. 7 draws some conclusions and sketches ongoing work. 2 The Concrete Mediator Workflow 2.1 Abstract Semantics: Taxonomies for Modules and Types Table 1 shows the modules identified within the system. They represent at the semantic level the collection of basic services available for the mediator. In order to produce a running solution as demonstrated in Stanford in November they are then bound (grounded) to the concrete SIBs that in the jABC constitute the running services. How module name input type output type description Mediator Maps RosettaNet messages to the backend startService {true} PurOrderReq Receives a purchase order request message obtCustomerID PurOrderReq SearchString Obtains a customer search string from the req. message createOrderUCID CustomerObject CustomerID Gets the customer id out of the customer object buildTuple OrderID Tuple Builds a tuple from the orderID and the POR sendLineItem Tuple LineItem Gets a LineItem incl. orderID, articleID and quantity closeOrderMed SubmConfObj OrderID Closes an order on the mediator side confirmLIOperation OrderConfObj PurOrderCon Receives a conf. or ref. of a LineItem and sends a conf. Moon The backend system searchCustomer SearchString CustomerObject Gets a customer object from the backend database createOrder CustomerID OrderID Creates an order addLineItem LineItem SubmConfObj Submits a line item to the backend database closeOrderMoon OrderID TimeoutOut Closes an order on the backend side confRefLineItem Timeout orderConfObj Sends a conf. or ref. of a prev. subm. LineItem Table 1. The SWS mediation Modules this happens is sketched in [17]. This information about the single modules is complemented by simple ontologies that express in terms of is-a and has-a relations properties over the types and the modules of the scenario. We call these relations Taxonomies. The taxonomies regarding the mediation scenario are shown in Fig. 1 (Type Taxonomy) and Fig. 2 (Module Taxonomy). This information is expressed in a Prolog-like fashion in a concrete knowledge base which feeds then the synthesys algorithm. 2.2 The Concrete Knowledge Base The synthesis tool takes as input a textfile with the definitions of the taxonomies (module and type taxonomy), the module descriptions, and some documentation. The first line of the file declares a name for the knowledge base: $program(sws_challenge). The file contains statements (one per line) of facts in the following three forms: – tax(type, output, customerObject). – tax(module, mediator, sendLineItem). – module(searchCustomer, searchString, customerObject). The two first statements show how to specify the type and module taxonomy: – The first line declares customerObject as a subtype of the output type. – The second line declares module sendLineItem to be a mediator module. The third statement form is used to specify the relation between input and output types for particular modules. It describes the module definition as already presented in Table 1: the searchCustomer module takes a searchString as input type and produces a customerObject output type. This way it is possible to concisely represent the taxonomies of Fig. 1 and 2 as well as the module description of Table 1 in one single file. 2.3 Semantic Linear-time Temporal Logic The loose specification language supported by the synthesis is the Semantic Linear-time Temporal Logic (SLTL)[14], a temporal (modal) logic comprising the taxonomic specifications of types and activities. This lifts the classical treatment of types and activities in terms of actions and propositions to a semantical level in a way typical today in the context of the semantic Web. Definition 1 (SLTL). The syntax of Semantic Linear-time Temporal Logic (SLTL) is given in BNF format by: Φ ::= type(tc) | ¬Φ | (Φ ∧ Φ) | <ac> Φ | G(Φ) | (ΦUΦ) where tc and ac represent type and activity constraints, respectively, formulated as taxonomy expressions. SLTL formulas are interpreted over the set of all legal coordination sequences, i.e. alternating type correct sequences of types and activities1, which start and end with types. The semantics of SLTL formulas is now intuitively defined as follows2: – type(tc) is satisfied by every coordination sequence whose first element (a type) satisfies the type constraint tc. – Negation ¬ and conjunction ∧ are interpreted in the usual fashion. – Next-time operator <> : <ac> Φ is satisfied by coordination sequences whose second element (the first activity) satisfies ac and whose continuation3 satisfies Φ. In particular, <tt> Φ is satisfied by every coordination sequence whose continuation satisfies Φ. – Generally operator G: G(Φ) requires that Φ is satisfied for every suffix4 satisfies Φ. 1 During the description of the semantics, types and activites will be called elements of the orchestration sequence. 2 A formal definition of the semantics can be found online. 3 This continuation is simply the coordination sequence starting from the third element. 4 According to the difference between activity and type components, a suffix of a coordination sequence is any subsequence which arises from deleting the first 2n elements (n any natural number). Fig. 3. (a) The synthesised SWS mediator (standard) and (b) Using loose types: the new solution – Until operator U: (ΦUΨ) expresses that the property Φ holds at all type elements of the sequence, until a position is reached where the corresponding continuation satisfies the property Ψ . Note that ΦUΨ guarantees that the property Ψ holds eventually (strong until). The definitions of continuation and suffix may seem complicated at first. However, thinking in terms of path representations clarifies the situation: a subpath always starts with a node (type) again. Users should not worry about these details: they may simply think in terms of pure activity compositions and not care about the types, unless they explicitly want to specify type constraints. The online introduction of derived operators supports a modular and intuitive formulation of complex properties. 2.4 Declarative LTL Specification for the Concrete Mediator For the mediator, we look for a workflow (a service coordination) that satisfies the following requirement: The mediator service should produce a Purchase Order Confirmation. The corresponding formal specification formulated in SLTL is simple: we need to start the service (module startService) and reach the result PurOrderCon (a type). We may simply write: (startService < PurOrderCon) where the symbol < denotes a derived operator meaning before or preceeds and is defined as f1 < f2=df F(f1 ∧ F(f2)) The jABC process model shown in Fig. 3(a) resembles very closely the expected required solution. If we adopt the very fine granular model of the types shown in Table 1, a natural choice given the SWS Challenge problem description, this is in fact the only solution. In this setting, we use abstract type names in the taxonomy to model de facto almost the concrete operational semantics: we distinguish for instance an OrderID from an OrderConfObject, modelling the described application domain at the concrete level of datatypes and objects a direct rendering of what happens at the XML level, or for programs in the memory and in the heap. This is however already a technical view, and it corresponds to lifting the concrete, programming-level granul	abstract type;adobe flash builder;application domain;beta normal form;client (computing);continuation;database;dataflow;documentation;embedded system;emoticon;formal specification;function composition (computer science);graphical user interface;input/output;is-a;knowledge base;lambda lifting;linear model;linear temporal logic;logic programming;loose coupling;matchware mediator;mediator pattern;method of analytic tableaux;modal logic;ontology (information science);operational semantics;piaget's theory of cognitive development;process modeling;prolog;rosettanet;semantic web;server (computing);service discovery;sinewave synthesis;specification language;string searching algorithm;taxonomy (general);usability;waits;web service;xml;xfig	Tiziana Margaria	2008			database;message format;proprietary format;software;semantics;web service;computer science	DB	-41.73781734123874	14.233202038525386	156926
ecc76ea77a8d89126620bfcd2efb945f120a3f82	challenges for service providers when importing metadata in digital libraries	service provider;digital library			Marilyn McClelland;David McArthur;Sarah Giersch;Gary Geisler	2002	D-Lib Magazine	10.1045/april2002-mcclelland	service provider;service level requirement;digital library;business service provider;computer science;service delivery framework;service design;database;internet privacy;world wide web	HPC	-48.18353777645135	13.713383214624882	156937
f1ceec4891e172608fb4511ed355be905208315f	a workbench for prototyping xml data exchange	data exchange	This paper describes a prototype software which is the outcome of a research carried at the Ca’ Foscari University of Venice in the framework of the project Data-X1. The software is a workbench for “data engineers”, integrating several tools which assist the user in all the tasks of integration and exchange of data with a standard format, for instance for application integration, generation of content-rich web portals, building of virtual information systems, etc. The system described here is a first step towards the construction of a “data hub”: with this term, we intend a comprehensive tool which, like a network hub device, connects several information sources and consumers through standard ports, allowing the designer to dynamically interconnect these ports in different ways, and performing appropriate translations. Such a tool would be of great help for the tasks above mentioned, and its existence is made possible by the nowadays wide acceptance of XML. Data exchange on the WWW has received a lot of attention due to the rapid diffusion of the proposal of XML as standard for information description by the W3 Consortium. The complete flexibility and generality of the XML markup mechanism as well as its platform-independence, allows its use in many contexts as a language for describing data of any kind, not only for documents to be published on WWW. What is missing is either a tool or a linguistic level to denote the meaning and type of data, since the XML markup mechanism only denotes the logic structure of data. Currently, XML allows neither description of the semantics nor that of the internal representation of data, with respect to applications. The DTD (Document Type Declaration) is of little help to address semantics issues, because it only describes the structural scheme of the document parts composition. The internal representation of data cannot be used since XML is text-based and platform independent, therefore data should be translated in a standard alphanumeric coding by other means. When exchanging data between different data sources, or between data sources and applications by using an XML based mechanism, this omission may limit the possibility of verifying data coherency. This involves both the formalization aspects (type) and the semantics aspects (meaning). This problem is a big roadblock in many application areas.	authentication;consortium;data hub;declaration (computer programming);ethernet hub;information system;markup language;portals;prototype;text-based (computing);usb hub;www;workbench;xml	Renzo Orsini;Augusto Celentano	2002			xml;software;markup language;information system;database;semantics;data exchange;data hub;document type declaration;computer science	DB	-36.81074667557756	8.317653405426798	157155
d0ae439b2bd27c19ecca99797b735c983803dbdc	enterprise modeling with conceptual xml	developpement logiciel;modelizacion;entity relationship model;entreprise;gestion memoire;base donnee;modele entreprise;integration information;storage management;xml language;empresa;conceptual analysis;database;base dato;conceptual model;modelo entidad relacion;modele entite relation;satisfiability;modelo empresa;analisis conceptual;modelisation;business model;avalanche;large scale;gestion memoria;information integration;desarrollo logicial;firm;software development;integracion informacion;avalancha;enterprise modeling;information system;analyse conceptuelle;modeling;systeme information;langage xml;lenguaje xml;sistema informacion	An open challenge is to integrate XML and conceptual modeling in order to satisfy large-scale enterprise needs. Because enterprises typically have many data sources using different assumptions, formats, and schemas, all expressed in—or soon to be expressed in—XML, it is easy to become lost in an avalanche of XML detail. This creates an opportunity for the conceptual modeling community to provide improved abstractions to help manage this detail. We present a vision for Conceptual XML (C-XML) that builds on the established work of the conceptual modeling community over the last several decades to bring improved modeling capabilities to XML-based development. Building on a framework such as C-XML will enable better management of enterprise-scale data and more rapid development of enterprise applications.	enterprise modelling;enterprise software;xml	David W. Embley;Stephen W. Liddle;Reema Al-Kamha	2004		10.1007/978-3-540-30464-7_13	business model;binary xml;xml;systems modeling;enterprise modelling;entity–relationship model;computer science;artificial intelligence;conceptual model;information integration;software development;data mining;database;xml schema editor;cxml;information system;satisfiability	Web+IR	-35.24408357973142	12.734288242790802	157273
be9c4aef21cbdd9d366ffc5b2be68ff55904e49e	static/semi-dynamic and dynamic composition of services in distributed systems	distributed system;ontology composition of services metadata for service description trader;metadata for service description;ontologies object oriented modeling automata runtime context aware services context modeling protocols dynamic compiler;composition of services;ontology;trader	In an earlier context, we have proposed a metadata model for service description. It describes service by three levels of properties: static properties (like its provider, its location and so on), behavior and interface. We have used ontologies in order to index and store the properties provided by this model. We have developed and implement tools which constitute a trader based on ontologies to discover a service by querying its three levels of description from ontologies. Based on the metadata model that we have proposed, we propose in this paper, an approach which allows the client to compose and to combine services by using our trader. This approach defines three models of composition : static, semi_dynamique and dynamique. These models allow the client to benefit from the functionalities of more than one service and to combine them at run-time and at compile-time to get novel services which provide novel functionalities.	compile time;compiler;distributed computing;ontology (information science);semiconductor industry;traders	Oussama Kassem Zein;Yvon Kermarrec	2006	Advanced Int'l Conference on Telecommunications and Int'l Conference on Internet and Web Applications and Services (AICT-ICIW'06)	10.1109/AICT-ICIW.2006.180	computer science;ontology;data mining;database;world wide web	HPC	-42.53250421785229	14.57731360386858	157427
23180686f56394730cbd0d5ec6c11f34815be8af	a rdf-based data integration framework	distributed data;query reformulation;data integrity;service provider;my publications;satisfiability;semantic web;description logic	Data integration is one of the main problems in distributed data sources. An approach is to provide an integrated mediated schema for various data sources. This research work aims at developing a framework for defining an integrated schema and querying on it. The basic idea is to employ recent standard languages and tools to provide a unified data integration framework. RDF is used for integrated schema descriptions as well as providing a unified view of data. RDQL is used for query reformulation. Furthermore, description logic inference services provide necessary means for satisfiability checking of concepts in integrated schema. The framework has tools to display integrated schema, query on it, and provides enough flexibilities to be used in different application domains.	application domain;data model;description logic;lambda calculus;programmer;rdf query language;relational database;resource description framework;sql;xml	Amineh Amini;Hadi Saboohi;Nasser Bakhsh Nemat	2012	CoRR		service provider;data exchange;description logic;logical schema;computer science;three schema approach;conceptual schema;data integration;semantic web;data integrity;data mining;database;rdf query language;information retrieval;satisfiability	DB	-34.07524994990753	9.487351514551271	157464
c6fe25d6ed7fdbbb450abd47602c3285b55d5eda	composing web services on the basis of natural language requests	human computer interaction interface web service composition natural language requests semantic web paradigm service oriented architectures service selection user request interpretation;semantic web paradigm;human computer interaction;user request interpretation;natural languages semantic web human computer interaction user interfaces;service selection;natural language requests;vocabulary;service oriented architectures;natural languages;portfolios;web service;web service composition;natural language;web services;semantic web;terminology;ontologies;human computer interaction interface;service oriented architecture;web services natural languages ontologies semantic web vocabulary telecommunications service oriented architecture natural language processing portfolios terminology;user interfaces;natural language processing;telecommunications	The introduction of the semantic Web paradigm in service-oriented architectures enables explicit representation and reasoning about services, via a semantically rich description of their operations. We propose an approach towards service selection and composition based upon the interpretation of user requests expressed through an informal human-computer interaction interface that employs (restricted) natural language.	human–computer interaction;natural language;programming paradigm;semantic web;service-oriented architecture;service-oriented device architecture;web service	Alessio Bosca;Andrea Ferrato;Fulvio Corno;Ilenia Congiu;Giuseppe Valetto	2005	IEEE International Conference on Web Services (ICWS'05)	10.1109/ICWS.2005.35	natural language processing;web service;computer science;service-oriented architecture;database;natural language;law;world wide web	Robotics	-45.22044965078836	13.869587969036012	157581
066b5d17753dbacfb6d44447f02d5093a3224f84	xicomas_q: an xml-based information content oriented multi-agent system for qos management in telecommunications networks		In this paper we propose XICOMASQ, an XML-based multi-agent system for the Information Content oriented management of QoS in telecommunications networks. XICOMASQ is characterized by the following features: ( i) it is strongly subjective, i.e. it handles network management activities according to user preferences represented by means of a user profile; ( ii) it can operate on a large variety of telecommunications networks; ( iii) it is semi-automatic; ( iv) it is XML-based, i.e. it exploits XML for guaranteeing a light, versatile and standard mechanism for information management and exchange. The paper reports also various experimental results as well as a comparison between XICOMASQ and other related QoS management systems already presented in the literature.	artificial intelligence;e-services;emoticon;exploit (computer security);information management;information system;multi-agent system;quality of service;semiconductor industry;telecommunications network;user (computing);user profile;xml	Pasquale De Meo;Jameson Mbale;Giorgio Terracina;Domenico Ursino	2004	Web Intelligence and Agent Systems			AI	-47.984376205412346	14.808631158411456	157666
2e2760a1a5b1c300266bf8c800becb6b198ea1f1	toward dynamic adoption for a user's situation information in a context-aware workflow system	context aware;workflow system;context aware service;ubiquitous computing environment	Recently, there are many studies to adopt the workflow model, which has been successively applied to traditional computing environments, into ubiquitous computing environments for context-aware and autonomous services. A service in the ubiquitous computing environments must be executed according to a user’s situation information, which is generated dynamically from sensors. However, such existing workflow systems as FollowMe and uFlow to support context-aware services through workflow models, can’t immediately adopt changes of a user’s situation into a already on-going service workflow. In this paper, we propose a context-aware workflow system, for ubiquitous computing environments, which can apply changes of user’s service demand or situation information into an on-going workflow without breaking its operation. To do this, the proposed system represents contexts described in a workflow as a RDF-based DItree (Document Instance tree). The system uses the tree information to recognize a exact position to be changed in the on-going workflow for user’s situation changes, and to reconstruct only the position under the influence of the changes in the DItree. Therefore, the suggested system can quickly and efficiently apply a change of user’s new situation into a on-going workflow without a lot loss of the time and the space, and can offer a context-aware service continuously, according to a new workflow.	algorithm;autonomous robot;context awareness;context-aware network;experiment;interrupt;resource description framework;sensor;tree (data structure);ubiquitous computing	Yongyun Cho;Kyoungho Shin;Jongsun Choi;Jaeyoung Choi	2007		10.1007/978-3-540-72588-6_41	workflow;knowledge management;database;world wide web;workflow management system;workflow engine;workflow technology	HCI	-41.63174602453093	15.46704028439936	157672
58f15e6bbd8253a0687ae4eac977dd2e1132e984	an integrated gis database server for malaysian mapping, cadastral and location-based systems (lbs)	information storage cartography database management systems file servers geographic information systems;local cadastral information;file servers;location based system;geographic information system;design and development;database management systems;global position system;geodetic reference system;geographic information systems databases global positioning system ellipsoids data engineering design engineering computer science information systems coordinate measuring machines position measurement;ecadastre coordinated cadastral system;ellipsoids;data mining;geographical information system;information storage;reference systems;global positioning system;geographic information systems;retrieval process;spatial databases;integrated gis database server;cartography;local geodetic measurements;gsm;retrieval process integrated gis database server malaysian mapping location based system geographical information system global positioning system geodetic reference system local geodetic measurements local cadastral information ecadastre coordinated cadastral system cellular coordinate information;cellular coordinate information;malaysian mapping	This paper discusses the design and development of a Malaysian Geographical Information System (GIS) database server for location data sources available in Malaysia, such as the Global Positioning System (GPS), WGS84, Geocentric Datum for Malaysia (GDM2000), Geodetic Reference System 1980 (GRS80), local geodetic measurements such as Malayan Revised Triangulation 1948 (MRT48 or KERTAU 48), Borneo Triangulation 1948 (TIMBALAI 1948), Malayan Revised Triangulation 1968(MRT68), Borneo Triangulation 1968 (BT68), local cadastral information such as the ECadastre Coordinated Cadastral System (CCS) system developed by JUPEM and cellular coordinate information based on Cell-based ID, E-OTD etc It describes the pros and cons of the abovementioned systems, analyze the accuracies and tolerance of the data source. It also highlights the disparities between different systems, and proposes fast methods to correlate and transform these data into a unified, reliable data source. Lastly it will cover the storage and retrieval process for the textual and graphical GIS data.	database server;geodetic datum;geographic information system;global positioning system;graphical user interface;location-based service;server (computing);world geodetic system	Teoh Chee Hooi	2009	2009 WRI World Congress on Computer Science and Information Engineering	10.1109/CSIE.2009.938	gsm;file server;global positioning system;computer science;data mining;database;ellipsoid;geographic information system	DB	-35.46288835508522	14.734285770497927	157787
5e278cc5de9fb9d5607d8910259d4104d7b59529	semantic web service discovery and selection: a test bed scenario	semantic web service;functional properties;test bed;settore ing inf 05 sistemi di elaborazione delle informazioni;semantic matching	The Semantic Web Service Challenge is one of the major initiative dedicated to work on Semantic Web Service (SWS) discovery and selection. It represents an effective manner for evaluating the functionality of SWS technologies. In this paper, we provide a new SWS-Challenge scenario proposal with new interesting problems to be solved on the basis of an analysis of a real world shipment scenario in the logistic operators domain. In the discussion a number of aspects concerning the discovery and selection processes are emphasized. In particular, we focus on the problem of considering the heterogeneity between the provider and the requester perspectives, and on the differences between functional and non functional specifications both on the requester and provider side.	business process;functional specification;semantic web service;service discovery;sinewave synthesis	Alessio Carenini;Dario Cerizza;Marco Comerio;Emanuele Della Valle;Flavio De Paoli;Andrea Maurino;Matteo Palmonari;Matteo Sassi;Andrea Turati	2008			computer science;data mining;database;world wide web	Web+IR	-46.73860858349628	14.412126844811441	157805
5ddd0d6181d38e831cd73ab51380e7de415b5097	the problem of constructing general-purpose semantic search engines	search engine;semantic web;semantic search	This work proposes the basic ideas to achieve a really semantic search. For this, first the number of semantic web sites must be increased which, on the one hand, maintain compatibility with the current Web and, on the other, offer different interpretations (now with semantics) of the same information according to different ontologies. Thus, the design of tools is proposed that facilitate this translation of HTML contents into OWL contents, as we say, possibly, according to different ontologies. The article continues by analysing the possible functionalities that we consider a semantic search engine based on the Semantic Web paradigm must support and by presenting a general-purpose search engine prototype on the two structures, current and semantic.	general-purpose markup language;learning to rank;semantic search	Luis Criado Fernández;Rafael Martínez-Tomás	2009		10.1007/978-3-642-02264-7_38	semantic interoperability;semantic similarity;semantic computing;semantic integration;semantic web rule language;data web;semantic search;semantic grid;computer science;semantic web;social semantic web;semantic web stack;semantic compression;database;semantic equivalence;semantic technology;world wide web;owl-s;information retrieval;semantic analytics;search engine	ML	-40.32437291454002	7.1493275611203995	158184
7e3e5e5b07533790f6fece58b36f4a9760ebe3fc	concept analysis and terminology: a knowledge-based approach to documentation	knowledge base;concept analysis;knowledge engineering	The central concern of terminology, a component of the general documentation process, is concept analysis, an activity which is becoming recognized as fundamental as term banks evolve into knowledge bases. We propose that concept analysis can be facilitated by knowledge engineering technology, and describe a generic knowledge acquisition tool called CODE (Conceptually Oriented Design Environment) that has been successfully used in two terminology applications: 1) a bilingual vocabulary project with the Terminology Directorate of the Secretary of State of Canada, and 2) a software documentation project with Bell Northern Research. We conclude with some implications of computerassisted concept analysis for terminology. 1. TERMINOLOGY AND CONCEPT	formal concept analysis;knowledge acquisition;knowledge engineering;software documentation;vocabulary	Douglas R. Skuce;Ingrid Meyer	1990			knowledge base;computer science;formal concept analysis;knowledge management;artificial intelligence;knowledge engineering;data mining;domain knowledge	SE	-43.7658646711412	4.413218452722293	158315
b6e0769b735507c9d3b6bca91dfac277d275e342	a web annotation system with access control and evolutional restructuring capabilities	access control		access control	Tetsuo Saito;Katsumi Tanaka	2002			world wide web;access control;restructuring;database;computer science;web annotation	Crypto	-39.46463749264395	7.700081128851156	158439
6ec4592291ffcdc2cecc045041476a462498c1e4	dynamic composition of semantically annotated web services through qos-aware htn planning algorithms	semantic annotation;owl s;semantically annotated web services;recovery mechanism;performance evaluation;hierarchical task network semantically annotated web services qos aware htn planning algorithms dynamic composer semantic descriptions owl s trip planning use case quality of service execution time service preconditions recovery mechanism flight services performance tests scalability tests trip booking scenarios complex trip planning service;dynamic composer;execution time;htn planning;hierarchical task network;service preconditions;web service;satisfiability;web services humanities performance evaluation quality of service;performance tests;books;qos;trip booking scenarios;engines;trip planning use case;technology and engineering;dynamic composition;humanities;complex trip planning service;heuristic algorithms;impedance matching;semantic descriptions;web services;web services quality of service semantic web software agents service oriented architecture artificial intelligence web and internet services technology planning information technology costs;semantic description;semantic web;scalability tests;planning;quality of service;qos aware htn planning algorithms;planning algorithms;use case;flight services;semantic web dynamic composition planning algorithms qos owl s	This paper presents a Dynamic Composer for Web services. The services are enriched with semantic descriptions in OWL-S, based on which the Composer automatically creates a combination of services reaching a specified goal. As an example, a trip planning use case is chosen where the goal ranges from booking of a single flight to planning of an entire trip including flight, hotel, transport, etc. The composition is achieved using local and global algorithms satisfying specific quality of service (QoS) constraints and requirements such as the execution time or cost of the invoked Web services. At the same time a more extended HTN planning algorithm is discussed, matching not only service outputs to inputs but also satisfying service preconditions through effects. In addition to the automatic composition, the paper also proposes a recovery mechanism in case of unavailable services. When executing the composition of flight services, unavailable services are dynamically replaced by equivalent services or a new composition achieving the needed result. The presented platform and planning algorithms are put through extensive performance and scalability tests for typical trip booking scenarios, in which basic services are composed to a complex trip planning service.	algorithm;anna-brita stenström;automated planning and scheduling;composer;national fund for scientific research;owl-s;open architecture;precondition;quality of service;requirement;run time (program lifecycle phase);scalability;unavailability;web service;world wide web	Anna Hristoskova;Bruno Volckaert;Filip De Turck	2009	2009 Fourth International Conference on Internet and Web Applications and Services	10.1109/ICIW.2009.62	web service;simulation;quality of service;computer science;artificial intelligence;database;services computing;law;world wide web	Robotics	-44.04958878454227	14.28014052213611	158480
1c3ff383fe41cc18887cf1ac43028104051847e0	a bayesian knowledge engineering framework for service management	belief networks;information resources;bayesian network;bayesian methods knowledge engineering knowledge management engineering management technology management information resources environmental management resource management power system management data mining;unstructured information resource;resource management;knowledge management;service management;change impact analysis service management bayesian networks knowledge engineering problem determination;bayesian methods;data mining;technology management;internet;change impact analysis bayesian knowledge engineering framework service management unstructured information resource bayesian network problem determination;internet belief networks;bayesian knowledge engineering framework;power system management;engineering management;change impact analysis;it service management;environmental management;knowledge modeling;geographic distribution;problem determination;bayesian networks;knowledge engineering	Service management is becoming more and more important within the area of IT service management. How to efficiently manage and organize service in complicated IT environments with frequent changes is a challenging issue. Service and the related information from different sources are characterized as diverse, incomplete, heterogeneous, and geographically distributed. It is hard to consume these complicated data without knowledge assistant. To address this problem, a knowledge engineering framework is proposed to tackle the challenges of acquisition, structuring and refinement of structured knowledge regarding existing different unstructured information resource, and the Bayesian network is utilized as the knowledge model. This framework can be successfully applied on key tasks in service management, such as problem determination and change impact analysis. And a real example of Cisco VoIP system is introduced to show the usefulness of this method.	bayesian network;business software;graphical model;hidden markov model;knowledge engineering;knowledge representation and reasoning;markov chain;refinement (computing)	Wei Wang;Hao Wang;Bo Yang;Liang Liu;Peini Liu;Guosun Zeng	2008	NOMS 2008 - 2008 IEEE Network Operations and Management Symposium	10.1109/NOMS.2008.4575210	computer science;knowledge management;technology management;resource management;service design;knowledge engineering;bayesian network;data mining;management science;personal knowledge management;management	DB	-47.658533931286726	14.954017060302965	158509
c5b4083d3529e081251c6da6d39a9950f01c231d	a multi-agent system for information semantic sharing		AOIS is a multi-agent system that supports the sharing of information among a community of users connected through the Internet. In respect to Web search engines, this system enhances the search through domain ontologies, avoids the burden of publishing the information on the Web and guaranties a controlled and dynamic access to the information. The use of agent technologies has made the realization of three of the main features of the system straightforward: i) filtering of information coming from different users, on the basis of the previous experience of the local user, ii) pushing of new information that can be of interest for a user, and iii) delegation of access capabilities, on the basis of a reputation network, built by the agents of the system on the community of its users.	internet;multi-agent system;ontology (information science);software agent;web search engine;world wide web	Agostino Poggi;Michele Tomaiuolo	2011			semantic analytics;social semantic web;semantic integration;semantic grid;search engine;information filtering system;multi-agent system;distributed computing;computer science;semantic web stack	AI	-41.299058999440604	8.835596234656458	158584
ff1df176c7e04727715ab91707a943a8aa320b09	change management in data integration systems	data integrity;change management	In this paper, we present a flexible architecture allowing applications and functional users to access heterogeneous distributed data sources. Our proposition is based on a multi-agent architecture and a domain knowledge model. The objective of such an architecture is to introduce some flexibility in the information systems architecture. This flexibility can be in terms of the ease to add or remove existing/new applications but also the ease to retrieve knowledge without having to know the underlying data sources structures. We propose to model the domain knowledge with the help of one or several ontologies and to use a multi-agent architecture maintain such a representation and to perform data retrieval tasks. The proposed architecture acts as a single point of entry to existing data sources. We therefore hide the heterogeneity allowing users and applications to retrieve data without being hindered by changes in these data sources.	agent architecture;data retrieval;enterprise architecture;information system;knowledge representation and reasoning;multi-agent system;ontology (information science);semantic interoperability;systems architecture	Rahee Ghurbhurn;Philippe Beaune;Hugues Solignac	2007			change control;change management;computer science;change management;data integrity;database;management	AI	-44.08510513033106	8.931462397434965	158767
