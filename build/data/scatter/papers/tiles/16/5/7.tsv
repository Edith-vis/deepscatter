id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
2f12c20c362fc680660f9e24bb1c0be7b3d6e19f	protege-2000: a plug-in architecture to support knowledge acquisition, knowledge visualization, and the semantic web	biomedical research;bioinformatics	Background. Prot6ge-2000 is the latest in a series of tools developed in our laboratory to assist developers in the construction of large electronic knowledge bases [1]. The direct-manipulation user interface allows developers to create and edit domain ontologies that represent the salient concepts and relationships among concepts in an application area. From the ontology, the system automatically constructs a graphical knowledge-acquisition system that allows application specialists to enter the content knowledge required for specific applications.	direct manipulation interface;graphical user interface;knowledge acquisition;ontology (information science);semantic web	Mark A. Musen;Ray W. Fergerson;Natalya Fridman Noy;Monica Crubézy	2001			architecture;database;protégé;knowledge acquisition;data mining;semantic web;visualization;plug-in;ontology (information science);user interface;computer science	HCI	-39.93768155373036	3.3396515942351064	44453
5b19f3ac0f5130ce9e39215f568bfc5bbadb0129	a data model for content modelling of temporal media	surveillance system;content modelling;sensor network;data model;temporal information;natural language interface;common event model;ontology	This paper describes a data model for content representation of temporal media in an IP based sensor network. The model is formed by introducing the idea of semantic-role from linguistics into the underlying concepts of formal event representation with the aim of developing a common event model. The architecture of a prototype system for a multi camera surveillance system, based on the proposed model is described. The important aspects of the proposed model are its expressiveness, its ability to model content of temporal media, and its suitability for use with a natural language interface. It also provides a platform for temporal information fusion, as well as organizing sensor annotations by help of ontologies.		Behrang Q. Zadeh;Ian M. O'Neill;Philip Hanna;Darryl Stewart	2009		10.1007/978-3-642-02472-6_18	natural language processing;computer science;theoretical computer science;data mining;logical data model	Vision	-36.141371261629104	-3.6014022638816754	44746
fd57319f1f6ea2151530ed22783ece6ac1df0c13	collected wisdom: some cross-domain issues of collection level description	collected wisdom;collection level description;cross-domain issues			Paul Miller	2000	D-Lib Magazine	10.1045/september2000-miller	data science;data mining;management science	NLP	-40.315847485716404	-7.184365253946106	44941
804971a191b10da070365f625294c9819e60ec0c	modeling temporal aspects of sensor data for mongodb nosql database	data mining and knowledge discovery;database management;mathematical applications in computer science;computational science and engineering;information storage and retrieval;communications engineering networks	Proliferation of structural, semi-structural and no-structural data, has challenged the scalability, flexibility and processability of the traditional relational database management systems (RDBMS). The next generation systems demand horizontal scaling by distributing data over autonomously addable nodes to a running system. For schema flexibility, they also want to process and store different data formats along the sequence factor in the data. NoSQL approaches are solutions to these, hence big data solutions are vital nowadays. But in monitoring scenarios sensors transmit the data continuously over certain intervals of time and temporal factor is the main property of the data. Therefore the key research aspect is to investigate schema flexibility and temporal data integration aspects together. We need to know that: what data modelling should we adopt for a data driven real-time scenario; that we could store the data effectively and evolve the schema accordingly during data integration in NoSQL environments without losing big data advantages. In this paper we explain a middleware based schema model to support the temporal oriented storage of real-time data of ANT+ sensors as hierarchical documents. We explain how to adopt a schema for the data integration by using an algorithm based approach for flexible evolution of the model for a document oriented database, i.e, MongoDB. The proposed model is logical, compact for storage and evolves seamlessly upon new data integration.	algorithm;big data;data modeling;document-oriented database;image scaling;middleware;mongodb;need to know;next-generation network;nosql;real-time data;real-time locating system;real-time transcription;relational database management system;scalability;semiconductor industry;sensor	Nadeem Qaisar Mehmood;Rosario Culmone;Leonardo Mostarda	2017	Journal of Big Data	10.1186/s40537-017-0068-5	data science;idef1x;computer science;elasticity (data store);data mining;nosql;data modeling;database;data warehouse;database model;view;document-oriented database	DB	-35.64665785528422	-0.9979708166165993	45164
ec389268a4be76bf2ea5519b635f85b53ec753ef	machine learning and knowledge discovery in databases		The next decade will see a deep transformation of industrial applications by big data analytics, machine learning and the internet of things. Industrial applications have a number of unique features, setting them apart from other domains. Central for many industrial applications in the internet of things is time series data generated by often hundreds or thousands of sensors at a high rate, e.g. by a turbine or a smart grid. In a first wave of applications this data is centrally collected and analyzed in Map-Reduce or streaming systems for condition monitoring, root cause analysis, or predictive maintenance. The next step is to shift from centralized analysis to distributed in-field or in situ analytics, e.g in smart cities or smart grids. The final step will be a distributed, partially autonomous decision making and learning in massively distributed environments. In this talk I will give an overview on Siemens’ journey through this transformation, highlight early successes, products and prototypes and point out future challenges on the way towards machine intelligence. I will also discuss architectural challenges for such systems from a Big Data point of view. Bio.Michael May is Head of the Technology Field Business Analytics & Monitoring at Siemens Corporate Technology, Munich, and responsible for eleven research groups in Europe, US, and Asia. Michael is driving research at Siemens in data analytics, machine learning and big data architectures. In the last two years he was responsible for creating the Sinalytics platform for Big Data applications across Siemens’ business. Before joining Siemens in 2013, Michael was Head of the Knowledge Discovery Department at the Fraunhofer Institute for Intelligent Analysis and Information Systems in Bonn, Germany. In cooperation with industry he developed Big Data Analytics applications in sectors ranging from telecommunication, automotive, and retail to finance and advertising. Between 2002 and 2009 Michael coordinated two Europe-wide Data Mining Research Networks (KDNet, KDubiq). He was local chair of ICML 2005, ILP 2005 and program chair of the ECML PKDD Industrial Track 2015. Michael did his PhD on machine discovery of causal relationships at the Graduate Programme for Cognitive Science at the University of Hamburg. Machine Learning Challenges at Amazon	artificial intelligence;autonomous robot;big data;business analytics;causality;centralized computing;cognitive science;data mining;data point;database;ecml pkdd;information system;international conference on machine learning;internet of things;mapreduce;point of view (computer hardware company);sensor;smart city;time series	Hendrik Blockeel;Kristian Kersting;Siegfried Nijssen Filip;Zelezny	2016		10.1007/978-3-319-46131-1	data science;pattern recognition;data mining;knowledge extraction	ML	-38.90340621042792	-7.646924473373006	45167
d18f6cc7c1bc459d5a08a6cab93c5230fdd5b4cb	special issue on exploiting semantic technologies with particularization on linked data over grid and cloud architectures	relevant perspective;cloud architectures;cloud computing;semantic technology;special issue;cutting-edge development;web new development;linked data;cloud architecture	The Web has changed the way users get information, but its success and exponential growth have made it increasingly difficult to find, access, present, and maintain such information for a wide variety of users [1]. However, during its existence the World Wide Web has evolved allowing the transformation of the traditional Web to Semantic Web – called also ‘‘the Web of Data’’ [2]. The Semantic Web (and more recently, Semantic Technologies) is described by Berners-Lee, Hendler and Lassila [3] as an extension of the current Web in which information is given a well-defined meaning, better enabling computers and people to work collaboratively in a more meaningful way. As particularization of the semantic technologies the term LinkedDatawas coined,which refers to a set of best practices for publishing and connecting structured data on theWeb [4]. In the last few years, these best practices have been applied by a number of data providers, thus significantly increasing the amount of LinkedData published, creating as outcome a new trend in the field of knowledge engineering. As a consequence of this, there is a vast amount of Linked Data that has been generated during the last few years, and for hence, there grows a necessity of storing, accessing and processing this data in the new models of services providers such as Cloud Computing. There are several works which have exploited the use of Semantic Technologies in Cloud Computing settings [5–8]. Thus in this field, semantic technologies have been pointed out as a way to articulate resource discovery mechanisms in portable cloud architectures [9] or in distributed infrastructures like Grids or Clouds [10] and as an access control model in multi-tenancy cloud systems [11]. On the other side, the use of Linked Data approaches is beginning to gain momentum [12] in spite of the many uncovered challenges for the adoption of Linked Data in corporate scenarios including technological aspects as well as social and legal implications [13]. In this field, literature reports recent and relevant initiatives like, for instance, the integration of distributed environmental and ecological data [14]. However, the current literature still lacks new architectures for the exploiting of the most recent advances in Semantic Technologies, and more concretely in the exploiting of Linked Data capabilities such as the generation and consumption of Linked Data and its articulation in cloud environments. The aim of this special issue is to collect innovative and highquality research contributions regarding the exploitation of the Semantic Technologies through the use of Cloud Computing and Grid Computing architectures and techniques. Editors received a considerable number of submissions that were peer-reviewed by top experts in the field. Based on the reviews and our reading of the	access control;best practice;biconnected component;cloud computing;computer;grid computing;knowledge engineering;linked data;multitenancy;ora lassila;semantic web;time complexity;world wide web	Ricardo Colomo Palacios;Vladimir Stantchev;Alejandro Rodríguez González	2014	Future Generation Comp. Syst.	10.1016/j.future.2013.10.021	semantic computing;semantic grid;computer science;data science;social semantic web;data mining;information retrieval	Web+IR	-44.55165080491089	2.7165266241087602	45378
d13c18c1c1d08617991ea23d706323e47f7ce2c2	efficient techniques for multipolynomial resultant algorithms	efficient technique;multipolynomial resultant algorithm	Dinesh Manocha and John Canny Computer Science Division University of California	algorithm;canny edge detector;computer science;resultant	Dinesh Manocha;John F. Canny	1991		10.1145/120694.120706	computer science;discrete mathematics	Theory	-47.11301281073156	-9.690045026734088	45496
59e31cf4d12551e88cb7a1b280f1d85a9783ce78	middleware for plug and play integration of heterogeneous sensor resources into the sensor web	ogc puck protocol;open geospatial consortium;sensor web enablement;sensorml;interoperability;plug and play;sensor integration	The study of global phenomena requires the combination of a considerable amount of data coming from different sources, acquired by different observation platforms and managed by institutions working in different scientific fields. Merging this data to provide extensive and complete data sets to monitor the long-term, global changes of our oceans is a major challenge. The data acquisition and data archival procedures usually vary significantly depending on the acquisition platform. This lack of standardization ultimately leads to information silos, preventing the data to be effectively shared across different scientific communities. In the past years, important steps have been taken in order to improve both standardization and interoperability, such as the Open Geospatial Consortium's Sensor Web Enablement (SWE) framework. Within this framework, standardized models and interfaces to archive, access and visualize the data from heterogeneous sensor resources have been proposed. However, due to the wide variety of software and hardware architectures presented by marine sensors and marine observation platforms, there is still a lack of uniform procedures to integrate sensors into existing SWE-based data infrastructures. In this work, a framework aimed to enable sensor plug and play integration into existing SWE-based data infrastructures is presented. First, an analysis of the operations required to automatically identify, configure and operate a sensor are analysed. Then, the metadata required for these operations is structured in a standard way. Afterwards, a modular, plug and play, SWE-based acquisition chain is proposed. Finally different use cases for this framework are presented.	architecture as topic;community;consortium;dna integration;data acquisition;digital archive;genetic heterogeneity;hl7publishingsubsection <operations>;information silo;interoperability;laryngeal web;middleware;plug (physical object);plug and play;sensor web;sensor (device)	Enoc Martínez;Daniel Mihai Toma;Simon Jirka;Joaquín del Río	2017		10.3390/s17122923	geospatial analysis;electronic engineering;computer engineering;engineering;sensor web;standardization;interoperability;sensorml;data acquisition;middleware;plug and play	Visualization	-40.532310072903364	-0.9170198890606558	45769
cc317bf0b81e8cf1e09894f24c5c948fba06f23a	applications of computational science: data-intensive computing for student projects	us government agencies equations fitting vectors mathematical model scientific computing data mining;very large databases computer science education educational courses research and development;student project;computer aided instruction;data mining;us government agencies;scientific computing environmental data mining linear model fitting smoothing;fitting;computer science education;research and development;vectors;smoothing;educational courses;linear model;environmental data mining;scientific computing;mathematical model;data intensive computing;very large databases;further education;parallel processing;large data;publicly available datasets computational science data intensive computing student projects research course large computations data intensive computations	A research course for juniors and seniors has been designed to offer students a chance to work with the tools of computational science for large, data-intensive computations on publicly available datasets.	computation;computational science;data-intensive computing	Jessica Howard;Omar Padron;Patricia Morreale;David A. Joiner	2012	Computing in Science & Engineering	10.1109/MCSE.2012.18	computational science;parallel processing;further education;computer science;data science;linear model;data-intensive computing;mathematical model;data mining;statistics;smoothing	HPC	-45.12378954797116	-2.8711628018332234	45796
47014e2bbaa7a63f332181641d0183f535370566	interoperability in costal zone monitoring systems: resolving semantic heterogeneities through ontology driven middleware	oceans;weather forecasting;protection;semantic heterogeneity;data analysis;monitoring system;conceptual schema;coastal zone;monitoring;global earth observation system of systems;underwater communication;middleware;ontologies;query answering;sea measurements;monitoring ontologies middleware sea measurements oceans data analysis global earth observation system of systems weather forecasting protection underwater communication	Ontologies are widely recommended as a means of rectifying semantic heterogeneity. The advantage of using ontologies is that they can provide a conceptual schema regardless of a data set’s format, structure, or size MSU is developing an ontological framework for resolving semantic heterogeneity problems in coastal zone data. This type of framework will provide the capability to (a) link the users to the knowledge, making integrated visualizations available; (b) provide search and query answering facilities; and (c) gather information at different levels of granularity, from the subcategory to the specific data level. Issues related to coupling such a system to models will also be discussed.	conceptual schema;emoticon;interoperability;msu lossless video codec;middleware;ontology (information science);rectifier;semantic heterogeneity	Surya S. Durbha;Roger L. King	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1525853	meteorology;underwater acoustic communication;semantic interoperability;weather forecasting;computer science;conceptual schema;ontology;global earth observation system of systems;middleware;data mining;coast;database;data analysis;remote sensing	SE	-38.765740618091954	-0.6670999299859319	45863
e32c8d67469217ff0b1a78093429571a0bd644ed	the xsede blast gateway: leveraging campus development for the xsede community	diagrid;hubzero;xsede;xsede computing resources;blast;htc job submission;blaster	This paper describes an XSEDE Extended Collaboration Support Service (ECSS) effort on scaling a campus-developed online BLAST service (BLASTer) into an XSEDE gateway to help bridge the gap between genomic researchers and advanced computing and data environments like those found in the Extreme Science and Engineering Discovery Environment (XSEDE) network. Biologists and geneticists all over the world use the suite of Basic Local Alignment Search Tools (BLAST) developed by the National Center for Biotechnology Information (NCBI) throughout the full spectrum of genomic research. It has become one of the de facto bioinformatics applications used in all variety of computing environments. BLASTer allows researchers to achieve those tasks faster and without expert computing knowledge by converting BLAST jobs to parallel executions. It handles all of the details of computation submission, execution, and database access for users through an intuitive web-based interface provided by the unique features of the HUBzero gateway platform. This paper details the core development of BLASTer for campus computing resources at Purdue University, some of its successes among the user community, and the current efforts by an ECSS scientific gateways project from XSEDE to include data-intensive use of resources like Wrangler at the Texas Advanced Computing Center (TACC) in the XSEDE network. The lessons learned from this project will be used to bring other XSEDE computing resources to BLASTer in the future and other programs like BLASTer to XSEDE users.	blast;bioinformatics;computation;data-intensive computing;hubzero;image scaling;virtual community;web application	Christopher S. Thompson;Steven M. Clark;Carol X. Song	2016		10.1145/2949550.2949653	simulation;engineering;data science;world wide web	HPC	-44.06345199182872	-2.202511188655421	45868
dfaabc730878bcba0d6f54e6eedd50ce85ef00dd	southeastern universities research association (sura) coastal ocean observing and prediction (scoop)	ocean and marine science geoscience cyberinfrastructure data education scoop;oceans;oceanography distributed sensors education geophysics computing;data;information technology;collaboration;distributed sensors;marine science southeastern universities research association sura coastal ocean observing and prediction program distributed collaborative research educational tools;educational institutions sea measurements oceans information technology collaboration computer networks application software sensor systems collaborative tools educational technology;geoscience;ocean and marine science;geophysics computing;lessons learned;cyberinfrastructure;ontologies;scoop;educational tool;distributed collaboration;oceanography;sea measurements	The SURA coastal ocean observing and prediction (SCOOP) program is developing a network of sensors and linked computers as a part of fully integrating several observing systems in the southern region of the United States. This session is focused on highlighting lessons learned in the development of distributed collaborative research applications and gaining a better understanding of the needs for educational tools and capabilities for ocean and marine science.	computer;scoop;sensor	Sandra L. Harper	2008	2008 IEEE Fourth International Conference on eScience	10.1109/eScience.2008.74	environmental science;oceanography;data science;remote sensing	Robotics	-41.07558751916482	-2.027663342888113	45904
33b85647a382ecc4d6f6d73638f5b3f871927a67	specifying olap cubes on xml data	extensible markup language;business to business;information sources;data integrity;olap;multidimensional database;data format;data representation;information organization;data model;data analysis;special needs;data warehousing;xml;world wide web;data integration;multidimensional databases;on line analytical processing;b2b e commerce	On-Line Analytical Processing (OLAP) enables analysts to gain insight about data through fast and interactive access to a variety of possible views on information, organized in a dimensional model. The demand for data integration is rapidly becoming larger as more and more information sources appear in modern enterprises. In the data warehousing approach, selected information is extracted in advance and stored in a repository, yielding good query performance. However, in many situations a logical (rather than physical) integration of data is preferable. Previous web-based data integration efforts have focused almost exclusively on the logical level of data models, creating a need for techniques focused on the conceptual level. Also, previous integration techniques for web-based data have not addressed the special needs of OLAP tools such as handling dimensions with hierarchies. Extensible Markup Language (XML) is fast becoming the new standard for data representation and exchange on the World Wide Web. The rapid emergence of XML data on the web, e.g., business-to-business (B2B) e-commerce, is making it necessary for OLAP and other data analysis tools to handle XML data as well as traditional data formats. Based on a real-world case study, this paper presents an approach to specification of OLAP DBs based on web data. Unlike previous work, this approach takes special OLAP issues such as dimension hierarchies and correct aggregation of data into account. Also, the approach works on the conceptual level, using Unified Modeling Language (UML) as a basis for so-called UML snowflake diagrams that precisely capture the multidimensional structure of the data. An integration architecture that allows the logical integration of XML and relational data sources for use by OLAP tools is also presented.	cubes;data (computing);data model;diagram;e-commerce;emergence;list of concept- and mind-mapping software;markup language;olap cube;ole db for olap;online analytical processing;prototype;relevance;semantics (computer science);unified modeling language;web application;world wide web;xml database;xml schema	Mikael R. Jensen;Thomas H. Møller;Torben Bach Pedersen	2001	Journal of Intelligent Information Systems	10.1023/A:1012814015209	data exchange;xml;online analytical processing;computer science;data warehouse;data mining;database;logical data model;information retrieval	DB	-36.14563079535465	1.8880596263525937	45920
a7a716d12928b78a4efcd4afe556b98dfadfb2d6	metadata for the certificates of energy efficiency of buildings in smart cities		The SusCity project, an MIT Portugal project, falls within the scope of smart cities. One of its tasks aims to research and develop metadata artefacts to be used in the scope of a Linked Open Data platform for the project’s data. In this article, we report the process and results associated with the development of the following metadata artifacts: an application profile, a metadata schema and four controlled vocabularies, one of which is independent of the application profile. The application field is the certification of the energy efficiency of buildings. For the development of the application profile, we inspired ourselves in the Me4MAP method although we did not use it thoroughly. The creation of the metadata schema and controlled vocabularies was resorted to the use of wikidata, so all new terms (properties and concepts) are wikidata terms. The results include the application profile, the metadata schema and the controlled vocabularies, and are all already open to the community for use and reuse. The application profile has 11 properties, four of which are new. The controlled vocabulary on measures for energy efficiency has 22 new terms spread over four­­­ levels. Information about all new terms will be included in the Linked Open Vocabularies service.	smart city	Ana Alice Baptista;Sara Catarina Silva	2017			database;linked data;certification;metadata;controlled vocabulary;schema (psychology);reuse;efficient energy use;application profile;computer science	EDA	-44.05391621507478	1.6317463860163408	45982
5a95a6752a07fae90a141a8394c496cf94e793d0	photonic quantum computers and communication systems	electrical engineering and computer science;thesis	Thesis: Ph. D., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2015.	computer;quantum computing	Jacob Mower	2015			electrical engineering technology;mechatronics;computer science;electrical engineering;computer engineering;mechanical engineering	Theory	-46.92797008844625	-6.928473168496971	45983
5df7ddee44fd899b1fe0cf7ac54caf99b6869469	"""differences in """"meaningful learning strategies of navigation: an empirical model"""	semantic map;navigational path;concepts maps usage;map usage;winds project;empirical model;meaningful learning strategies;hypertextual structure;personal semantic map;strategies construction;concepts map;navigational data;terminology;databases;distributed environment;frequency;space technology;semantic web;collaboration;navigation;continuing professional development;concept map;statistical significance	"""In this study, the empirical detection of ways of generating semantic maps using navigational data and data driven approaches is addressed. It is hypothesized that the hypertextual structure of distributed environments could encourages the construction of personal semantic maps, expressed by navigational paths. Then this kind of usage is compared with the one of concepts maps in the dataset provided by the WINDS Project. It is shown that: a) the two strategies are in most cases mutually exclusive; b) the differences in usage of the two strategies are statistically significant; c) map usage is more scarce than the one of hypertextual structure; d) concepts maps usage is clustered around the same course. The potential of distributed technologies in supporting """"meaningful learning"""" is discussed; moreover, the importance of other variables (e.g. learner-teacher interaction) in strategies construction is underlined"""	emoticon;map;semantic mapper	Silvia Rita Viola;Alberto Giretti;Tommaso Leo	2006	Sixth IEEE International Conference on Advanced Learning Technologies (ICALT'06)	10.1109/ICALT.2006.142	concept map;navigation;computer science;knowledge management;artificial intelligence;data science;continuing professional development;frequency;semantic web;data mining;database;statistical significance;space technology;terminology;world wide web;empirical modelling;distributed computing environment;collaboration	Robotics	-42.1038294661819	-1.7550122459742548	46098
8faecc3fd638db5c3c821e25349d038480245cf7	reminiscences on influential papers		Rarely, a paper comes along that summarizes the current literature and brings it all together in a single place, concisely written, and clearly argued and these papers form the basis point for both future product development and further research. The ARIES paper featured here achieved that goal with unusual clarity yet goes a step further by not just summarizing the current research but in also documenting in detail how leading commercial relational databases had implemented logging, concurrency control, buffer management, and logging at a time when there was almost no information available describing the internals of these systems. The paper is massive in scope covering logging and recovery methods, providing a taxonomy of different approaches to buffer management, concurrency control and lock hierarchies with intent locking, fined grained (row) locking techniques and the impact on recovery, write ahead logging, and recovery techniques include the value of an log analysis phase and some of the different optimizations possible. Some of the concepts discussed where brought together from the existing literature, some of the techniques where from commercial systems where the implementation hadn’t been publicaly discussed prior to the publication of this work, and some of approaches were inventions of the authors first presented in this paper.	concurrency (computer science);concurrency control;lock (computer science);log analysis;new product development;relational database management system;server log;software documentation;write-ahead logging	Kenneth A. Ross	2005	SIGMOD Record	10.1145/1083784.1083801		DB	-33.917551712721526	-0.19605892864033675	46126
238a69fa764c0a0de874bb6498539f2b754836e0	developing constraint-based recommenders	content based filtering;financial services;satisfiability;collaborative filtering	Recommender systems provide valuable support for users who are searching for products and services in e-commerce environments. R e earch in the field long focused on algorithms supporting the recommendation o f quality & taste products such as news, books, or movies. Nowadays, the scope of th ose systems is extended to complex product domains such as financial services or lectronic consumer goods. Constraint-based recommenders are particula rly well suited as they support effective product and service selection processes in such domains. In this chapter, we characterize constraint-based recommendatio problems and provide an overview of major technologies that support the developmen t of knowledge bases for constraint-based recommenders which is of high importa nce for a successful application in commercial settings. Thereafter we give an o verview of intelligent interaction mechanisms which are supported by constraintbased recommender applications, discuss scenarios where constraint-based rec ommenders have been successfully applied, and provide a discussion of different so luti n approaches. Finally, this chapter is concluded with an outline of open research is sue . Alexander Felfernig Graz University of Technology e-mail: alexander.felferni g@ist.tugraz.at Gerhard Friedrich University Klagenfurt e-mail: gerhard.friedrich@uni-kl u.ac.at Dietmar Jannach TU Dortmund e-mail: dietmar.jannach@tu-dortmund.de Markus Zanker University Klagenfurt e-mail: markus.zanker@uni-klu.ac .at	algorithm;book;e-commerce;email;enea ose;open research;recommender system	Alexander Felfernig;Gerhard Friedrich;Dietmar Jannach;Markus Zanker	2011		10.1007/978-0-387-85820-3_6	computer science;knowledge management;collaborative filtering;data mining;database;recommender system	AI	-45.70430137342741	0.8855630053418254	46153
1f9f4d26a125e02adee02201cd49e1eaa3a31b0b	the relational model: beginning of an era	relational database management systems relational model annals special issue;history;sql;chris date;history of computing;computational modeling history relational databases database systems;relvars;computational modeling;sql history of computing relational databases software industry ted codd chris date relations relvars;relations;database systems;software industry;relational databases;ted codd	Serving as an informal technical introduction to this Annals special issue on relational database management systems, this article gives an introductory overview of the relational model and discusses the value of Edgar F. (Ted) Codd's model. Then, after providing an account of Chris Date's contributions, the author assesses the relational model's effect on the industry and how it might affect future developments.	relational database management system;relational model;ted	Hugh Darwen	2012	IEEE Annals of the History of Computing	10.1109/MAHC.2012.50	sql;relational model/tasmania;relational database management system;relational model;entity–relationship model;relational database;computer science;artificial intelligence;data science;database model;data mining;database;alpha;computational model;relvar;algorithm;database design;codd's 12 rules	DB	-47.73552731521786	-4.606803023327214	46249
46d41aef7578934aa63ccc844018aa37a6bb8464	workflows and extensions to the kepler scientific workflow system to support environmental sensor data access and analysis	engineering;sensors;data stream;scientific workflow;sensor network;scientific workflows;data analysis;near real time data access;life sciences;data access;terrestrial ecology;environmental science;data exploration;near real time;use case;oceanography;data archive	a National Center for Ecological Analysis and Synthesis, 735 State Street, Suite 300, Santa Barbara, CA, USA b San Diego Supercomputer Center, University of California San Diego, 10100 Hopkins Drive, La Jolla, CA, 92093-0505, USA c OPeNDAP, Inc., 165 Dean Knauss Dr., Narragansett, RI, 02882, USA d Graduate School of Oceanography, University of Rhode Island, South Ferry Road, Narragansett, RI, 02882, USA e Department of Zoology, Oregon State University, Corvallis, OR, 97331, USA f Wildlife Trust, 460 West 34th Street, 17th Floor, New York, NY, 10001, USA	data access;jolla;kepler scientific workflow system;rs-232;san diego supercomputer center	Derik Barseghian;Ilkay Altintas;Matthew B. Jones;Daniel Crawl;Nathan Potter;James Gallagher;Peter Cornillon;Mark Schildhauer;Elizabeth T. Borer;Eric W. Seabloom;Parviez R. Hosseini	2010	Ecological Informatics	10.1016/j.ecoinf.2009.08.008	use case;data access;wireless sensor network;computer science;sensor;data science;data mining;database;data analysis	ML	-44.71194879828682	-9.82046509410751	46782
f456843e7550bcc0d791888a4cc6a504eb43ae3c	semakode: hybrid system for knowledge discovery in sensor-based smart environments	linked data;sensors;knowledge discovery system	This article describes a conceptual hybrid architecture for a knowledge discovery system, able to automatically annotate, reason, classify and operate with sensor data. The adoption of semantic web technologies to enrich sensor and link data represents an adequate methodology that facilitates the processes of reasoning, classification and other types of automation. We discussed a system deployment scenario in the context of e-health.	hybrid system;smart environment	Stefan Negru	2012		10.1007/978-3-642-31753-8_41	computer science;sensor;data science;linked data;data mining;world wide web	Robotics	-38.79466050917388	0.944610939535136	46971
968bb4053a4b2bea487f3c6ab864e87428448a51	presenting the past: a framework for facilitating the externalization and articulation of user activities in desktop environment	context aware application;context awareness;knowledge management context aware services humans history computer science information systems monitoring environmental management context awareness information retrieval;information systems;history;information retrieval;tacit knowledge;knowledge management;work environment;temporal information;monitoring;humans;computer science;environmental management;context aware services	Work processes are conducted in various contexts and they involve different tasks, interruptions, activities and actions. In all of these, tacit knowledge plays a part. Some part of that tacit knowledge can be externalized and articulated by continuously monitoring the user’s activities. Because the desktop environment is an integral part of almost any office work context, we chart the demands the unstructured and discontinuous nature of work puts on the management of desktop working context. We discuss possibilities to augment the user’s awareness of his/her desktop working environment by providing a context-aware application that can act as a map-like resource for the user’s past activities on the desktop. We propose using temporal information to couple personal experiences with representational, more objective aspects of the context in order to make it possible for the user to express and retrieve subjectively significant activities with a minimal effort. We present an abstract model for designing an application for this purpose.	biconnected component;desktop computer;floor and ceiling functions;knowledge management	Kimmo Wideroos;Samuli Pekkola	2006	Proceedings of the 39th Annual Hawaii International Conference on System Sciences (HICSS'06)	10.1109/HICSS.2006.385	human–computer interaction;computer science;knowledge management;database;multimedia;world wide web;information system	HCI	-40.784199484997956	-9.703578870518415	47084
171b0969d0dce2aac6a79a83f3a916cb13f4c6d0	modelling of distributed control for repetitive production flow prototyping	dispatching rules;performance evaluation;repetitive production flow prototyping;distributed control;steady state	Modelling of distributed control for repetitive production flow prototyping B Skołud a & ZA Banaszak b a Department of Engineering Processes Automation and Integrated Manufacturing Systems , Silesian University of Technology , 18a Konarskiego Str, 44-100, Gliwice, Poland b Laboratory of Knowledge and Artificial Intelligence , Systems Research Institute of Polish Academy of Sciences , 75 Podwale, Str, 50-449, Wroclaw, Poland E-mail: Published online: 19 Feb 2007.	academy;artificial intelligence;automation;distributed control system;mail (macos);prototype;silesian library;zonealarm	Bozena Skolud;Zbigniew Antoni Banaszak	2005	Int. J. Computer Integrated Manufacturing	10.1080/09511920500081486	control engineering;real-time computing;engineering;operations management;steady state	Robotics	-44.34726717776755	-9.049314158238257	47129
d5035f10e5189fe66108c793e8d891e5a0a248fc	embedded real-time control via matlab, simulink, and xpc target	real time control	1 The MathWorks, Inc., Simulink Development, 3 Apple Hill Dr., Natick, MA 01760-2098, U.S.A. 2 The MathWorks, Inc., Application Engineering, 39555 Orchard Hill Place, Novi, MI 48375-5374, U.S.A. 3 The MathWorks, Inc., xPC Target Development, 3 Apple Hill Dr., Natick, MA 01760-2098, U.S.A. 4 The MathWorks, Inc., Technical Marketing, 39555 Orchard Hill Place, Novi, MI 48375-5374, U.S.A. 5 The MathWorks, Inc., Application Engineering, 3 Apple Hill Dr., Natick, MA 01760-2098, U.S.A. 6 The MathWorks, Inc., Technical Marketing, 3 Apple Hill Dr., Natick, MA 01760-2098, U.S.A.	embedded system;matlab;orchard;real-time transcription;simulink	Pieter J. Mosterman;Sameer Prabhu;Andrew Dowd;John Glass;Tom Erkkinen;John Kluza;Rohit Shenoy	2005			embedded system;real-time computing;operating system	ML	-46.04786242239418	-8.397063266178396	47214
7c5f7e116109f3f5802fd172645ecc133d7c5cd4	demonstrating open science for modeling & simulation research	simulation;open data;open access;e infrastructure;modeling;open science	Most conference and journal publications present contributions based on research artefacts (data, results, software, etc.) that are difficult for researchers to access. Open science aims to promote open access to research presented in academic works. Ideally, the software, data and results presented in a scientific article should be available for other scientists to use, validate and build upon for their own research. This is particularly true in some Modeling & Simulation (M&S) research where in addition to the above access might also be required to complex models. Using a case study based on an Infection Model, this demonstration shows how Open Science approaches based on Digital Object Identifiers, Researcher Registries, Open Access Data Repositories, Scientific Gateways and e-Infrastructures project can support M&S research.	scientific literature;simulation	Simon J. E. Taylor;Adedeji O. Fabiyi;Anastasia Anagnostou;Roberto Barbera;Mario Torrisi;Rita Ricceri;Bruce Becker	2016	2016 IEEE/ACM 20th International Symposium on Distributed Simulation and Real Time Applications (DS-RT)	10.1109/DS-RT.2016.35	library science;computer science;world wide web	HPC	-43.718563852740616	-1.0657702757422136	47348
403e57d642397d4bb64719aca3873dba830b3cf9	using data grid technology to build modis data management and distribution system based on spatial information grid	distributed system;data sharing;cross border data computation;image resolution;spatial data;modis technology management geoscience distributed computing grid computing remote sensing buildings costs remote monitoring conference management;earth;data grid technology;earth observation system;moderate resolution imaging spectroradiometer;data management;modis data catalogue service data grid technology modis data management distribution system spatial information grid moderate resolution imaging spectroradiometer earth observation system remote sensing data modis data receive station data interoperation xml based spatial data access language heterogeneous data integration cross border data computation;modis data management;integration;system performance;spatial information grid;layered structure;geophysical signal processing;xml geophysical signal processing grid computing image resolution open systems remote sensing;remote sensing data;remote sensing;spatial databases;xml;distributed databases;heterogeneous data integration;modis;modis data receive station;modis data catalogue service;earth observing system;open systems;grid computing;buildings;spatial information;data grid;data service;data interoperation;distribution system;spatial resolution;xml based spatial data access language	MODIS (Moderate Resolution Imaging Spectroradiometer) is an advance earth observation system, which is the world's best remote sensing data of free receiving. However, there is a serious problem about duplicated establishment of MODIS data receive station in China, and it has brought a tremendous waste of money. Utilizing data grid technology could take advantages to integrate and share heterogeneous and distributed MODIS data sources. This paper presents research work on building MODIS data management and distribution system based on Spatial Information Grid (SIG). We studied and implemented a three-layer structure for data interoperation, and designed the extensible XML-based spatial data access language to support heterogeneous data integration and cross-border data computation. Furthermore, we developed the MODIS data catalogue service to facilitate users and improve system performance. The research helps to release conflict between duplicated establishment of MODIS receiving station and less effectively data sharing, and enables researcher to focus on science, not on problems of data location, computing ability and data management.	computation;data access language;interoperation;multitier architecture;xml	Yi Zeng;Dingsheng Liu;Guoqing Li;Zhenchun Huang	2008	2008 Seventh International Conference on Grid and Cooperative Computing	10.1109/GCC.2008.108	image resolution;data management;computer science;data mining;database;distributed computing;spatial analysis	HPC	-38.48650593982912	-1.7476445687396818	47354
f501398f05c97f6df28313551120d8212826740b	spectral library: a proposal for data model		In remote sensing, the spectral behavior of targets (also called spectral signature) related to the interaction process between matter and incident electromagnetic radiation (EMR) allows the identification of several materials. Organizing all this information in spectral libraries has its importance in the most diverse areas of scientific investigation and is carried out by various organs and institutions. However, as there is no model for structuring the data, each responsibly organizes according to their need. In this context, this work after analyzing the data of some libraries, proposes a data model to facilitate standardization and interoperability.	data model;excalibur: morgana's revenge;interoperability;library (computing)	Carlos Alberto Stelle;Francisco Javier Ariza-López;Manuel Antonio Ureña-Cámara	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518247	data mining;remote sensing;interoperability;standardization;data model;reflectivity;data modeling;computer science;spectral signature	Embedded	-39.92988257916875	-1.0343384990592195	47450
1b41aad8cf4131d6529855c670d78418fc937642	entity linkage for heterogeneous, uncertain, and volatile data.		A plethora of collections is nowadays created by merging data from a variety of different applications and information sources. These sources often use different identifiers for data that describe the same real world object, for example an artist, a conference, an organization. The large number of existing entity linkage approaches are not designed for the characteristics of modern applications and Web data. These includes data heterogeneity that is due to the lack of uniform standards, uncertainty resulting in imperfections in the extraction process or the reliability of the sources, and the volatile nature of the data due to constant modifications through interactions with users or external applications. This dissertation introduces a novel methodology to address the entity linkage problem for heterogeneous, uncertain, and volatile data. The methodology is based on a probabilistic linkage database, which is able to simultaneously capture the entities from the original collection data and the possible linkages between entities, as these are generated by a number of the existing entity linkage techniques. The probabilistic linkage database consists of two main components. The first is related to efficient query processing. The proposed query mechanism does not only consider the entities and the probabilistic linkages, but it also handles the uncertainty present in them. The second component is related to the processing of the entity data for generating probabilistic linkages between entities. In order to handle the heterogeneity and the volatile nature of the data, this part focuses on incremental and adaptive techniques that consider not only the available textual information but also their inferred semantics. Both effectiveness and efficiency of the introduced algorithms are illustrated through an experimental evaluation that involves real world data.	algorithm;database;entity;identifier;interaction;linkage (software);volatile memory	Ekaterini Ioannou	2011			data mining;database	DB	-36.16004847117945	2.8453718980793505	47459
07dd454622cbf6bc4d3947d267da125a46763191	an analytical framework for classifying software tools and systems dealing with cultural heritage spatio-temporal information		Abstract This paper presents an analytical framework for classifying software tools and systems dealing with spatio-temporal information developed for applications in the cultural heritage field. These can be numerous and quite different from one another, depending for the purpose they were developed for. In order to assess if one of the already existing software tools and systems can be used or modified to fit our research goals, a list of needs was established. Starting from those, requirements were defined and the software tools and systems dealing with spatio-temporal information are then compared and evaluated based on those characteristics. This analytical framework is an important step in our research since using a tool that is not appropriate for the study object will not be able to provide valuable information to answer the main research questions.	bim;castles;citygml;formal specification;requirement;scottish rite masonic museum & library;test drive	Andrea Luczfalvy Jancsó;Benoît Jonlet;Patrick Hoffsummer;Emmanuel Delye;Roland Billen	2017		10.1007/978-3-319-63946-8_50	study object;data mining;computer science;software;semantics;cultural heritage	SE	-41.61250455262246	0.3035802214671434	47535
7ca37f500f413a9ec9aceecbe907a47bef2e8b5a	a study: from electronic laboratory notebooks to generated queries for literature recommendation		Relating one’s research to the vast body of scientific knowledge is a difficult task; the sheer volume of literature makes it difficult to keep up-to-date with scientific developments. Particularly when research is on-going, keeping track of related work is especially important to avoid an unintended duplication of effort. We outline a novel approach to this problem that uses the text in an Electronic Laboratory Notebook (ELN) as a representation of an experimental context in the field of Chemistry. The contribution of this work is to situate the literature recommendation task within the context of the user’s experimental information needs. We find that our approach to transform the ELN text into queries for use with PubMed is able to recover a subset of user bibliographies. We find that alternative methods for query generation that capture both scientific terminology and salient terms in the ELN complement each other.	bibliographic index;information needs;pubmed;situated cognition;vaxeln	Oldooz Dianat;Cécile Paris;Stephen Wan	2013			human–computer interaction;computer science;multimedia;world wide web	Web+IR	-44.23016962424188	0.9599257373366311	47776
dd4401d20e668d4cdbc23541b63d516479c89d2e	querying graph databases with the gsql query language		This talk presents GSQL, a recent addition to the spectrum of query languages for expressing graph analytics. GSQL is a high-level yet still Turing-complete language whose syntax is inspired by SQL in order to reduce the learning curve for SQL programmers, while simultaneously supporting a Map-Reduce interpretation that is preferred by NoSQL developers and that is conducive to massively parallel evaluation. The talk will also provide some context on the graph query language landscape represented in modern systems. Alin Deutsch is a professor of Computer Science and Engineering at UC San Diego. His research is motivated by the data management challenges raised by database-powered applications. Alin's interests include query language design and optimization for various data models ranging from text to the relational and post-relational models (with particular emphasis on graph data). He also works on cross-model data integration and on automatic verification of business processes. Alin earned his PhD in Computer Science from the University of Pennsylvania, an MSc degree from the Technical University of Darmstadt (Germany) and a BSc degree from the Polytechnic University Bucharest (Romania). He is the recipient of the 2018 ACM PODS Test of Time Award, a Jean D’Alembert Fellowship from the University Paris-Saclay, the Alfred P.Sloan Fellowship, the ACM SIGMOD 2006 Top-3 Best Paper Award, and an NSF CAREER award. 2018 SBC 33rd Brazilian Symposium on Databases (SBBD) August 25-26, 2018 Rio de Janeiro, RJ, Brazil	business process;computer engineering;computer science;data model;graph database;high- and low-level;ibm notes;jean;mapreduce;mathematical optimization;nosql;programmer;query language;registered jack;sql;turing completeness;uc browser;winsock	Alin Deutsch	2018			database;graph database;query language;computer science	DB	-48.147588503718644	-5.94633506214596	47979
bd032373b0e85c17ee8a73194db0187c5b01917e	future internet technologies for environmental applications	institute;environmental informatics;image;info eu repo semantics article;future internet;init;internet of things;environmental observation web;big data;environmental specific enablers;crowdtasking;articles in periodicals and books;imaging;new;technologies;volunteered geographic information;cloud computing	This paper investigates the usability of Future Internet technologies (aka “Generic Enablers of the Future Internet”) in the context of environmental applications. The paper incorporates the best aspects of the state-of-the-art in environmental informatics with geospatial solutions and scalable processing capabilities of Internet-based tools. It specifically targets the promotion of the “Environmental Observation Web” as an observation-centric paradigm for building the next generation of environmental applications. In the Environmental Observation Web, the great majority of data are considered as observations. These can be generated from sensors (hardware), numerical simulations (models), as well as by humans (human sensors). Independently from the observation provenance and application scope, data can be represented and processed in a standardised way in order to understand environmental processes and their interdependencies. The development of cross-domain applications is then leveraged by technologies such as Cloud Computing, Internet of Things, Big Data Processing and Analytics. For example, “the cloud” can satisfy the peak-performance needs of applications which may occasionally use large amounts of processing power at a fraction of the price of a dedicated server farm. The paper also addresses the need for Specific Enablers that connect mainstream Future Internet capabilities with sensor and geospatial technologies. Main categories of such Specific Enablers are described with an overall architectural approach for developing environmental applications and exemplar use cases.		Carlos Granell;Denis Havlik;Sven Schade;Zoheir A. Sabeur;Conor Delaney;Jasmin Pielorz;Thomas Usländer;Paolo Mazzetti;Katharina Schleidt;Michael Kobernus;Fuada Havlik;Nils Rune Bodsberg;Arne-Jørgen Berre;José Lorenzo	2016	Environmental Modelling and Software	10.1016/j.envsoft.2015.12.015	big data;cloud computing;computer science;engineering;data science;image;data mining;volunteered geographic information;new;world wide web;internet of things;technology	Mobile	-40.383315939581095	-1.2301540280528485	48174
ab75fd72b8aec67c205e65c6aef9c8b730de5467	a distributed agent-based simulation environment for interference detection and resolution	interference detection;navio;distributed system;aplicacion militar;systeme reparti;application militaire;distributed agents;simulation;simulacion;agent logiciel;software agents;sistema repartido;military application;ship;simulation environment;radar;navire	S. Ramaswamy Software Automation and Intelligence Laboratory Computer Science Department Tennessee Technological University Cookeville TN 38505 Email: srini@acm.org K. Srinivasan, P. K. Rajan, S. Krishnamurthy Electrical and Computer Engineering Department Tennessee Technological University Cookeville TN 38505 Email: pkrajan@tntech.edu R. MacFadzean Naval Surface Warfare Center Dalhgren, Virginia Email: macfad@sirinet.net	automation;computer engineering;computer science;email;interference (communication);simulation;twisted nematic field effect	Srini Ramaswamy;K. Srinivasan;P. K. Rajan;S. Krishnamurthy;Robert MacFadzean	2001	Simulation	10.1177/003754970107600604	embedded system;simulation;telecommunications;computer science;engineering;electrical engineering;software agent;radar	AI	-45.527420243262746	-8.673041842455898	48290
3d27a2e0712bd632c721db50ee9cfb7f8d9c69f6	interlinking multimedia: how to apply linked data principles to multimedia fragments	linked data	In this paper, we introduce interlinking multimedia (iM), a pragmatic way to apply the linked data principles to fragments of multimedia items. We report on use cases showing the need for retrieving and describing multimedia fragments. We then introduce the principles for interlinking multimedia in the Web of Data, discussing potential solutions which sometimes highlight controversial debates regarding what the various representations of a Web resource span. We finally present methods for enabling a widespread use of interlinking multimedia.	linked data;web resource;world wide web	Michael Hausenblas;Raphaël Troncy;Tobias Bürger;Yves Raimond	2009			linked data;mathematics	Web+IR	-43.3110491886153	2.5320381609910956	48444
b7452b57375e990fe51ccf2eaf2daea340cba4e4	granularity as a qualitative concept for gir	spatial data quality;spatial semantics;granularity;geographic information retrieval	We examine the notion of granularity for qualitative thinking about geospatial data and location references. Granularity can be understood as an abstraction of level of detail or spatial resolution. Pure coordinates, which may exhibit strong overprecision for some entities, can be combined with not only hierarchical gazetteer information, but also with derived semantic data about extent of places and thus help in correct interpretations without necessarily more accuracy.	entity;false precision;image resolution;level of detail	Dirk Ahlers	2015		10.1145/2837689.2837704	granularity;computer science;data mining;database;information retrieval;corporate taxonomy	AI	-37.71998764617981	-0.8537865284113391	48621
403a052ff6d7a963643408fcea51039388785eb4	experiments on using lod cloud datasets to enrich the content of a scientific knowledge base		This paper describes the issues arising when employing the Linked Open Data (LOD) cloud datasets to enrich the content of a scientific knowledge base as well as the approaches to solving them. The experiments are carried out with the help of a toolkit intended to simplify the analysis and integration of data from different datasets. The toolkit comprises several tools for application-specific visualization. The dataset of the Open Archive of the Russian Academy of Sciences and several bibliographic datasets are used as test examples.	knowledge base	Zinaida Apanovich;Alexander G. Marchuk	2013		10.1007/978-3-642-41360-5_1	data mining;world wide web;information retrieval	HPC	-42.45971372274876	3.61516370804225	48862
b5f5296b36cc77978793025ba1f0595df13612d0	exploration of massive crime data sets through data mining techniques	institute for integrated and intelligent systems;faculty of science environment engineering and technology;exploratory analysis;data mining;journal article;association rule mining;080109;temporal pattern;pattern recognition and data mining	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	data mining;francis;primary source	Ickjai Lee;Vladimir Estivill-Castro	2011	Applied Artificial Intelligence	10.1080/08839514.2011.570153	concept mining;web mining;text mining;association rule learning;computer science;data science;machine learning;data mining;data stream mining;world wide web;k-optimal pattern discovery	Robotics	-40.03890406454672	-9.063393622912885	49003
6caeaff833927e850fe4efa58a864b8491273fe1	using dataspace archives to support long-term stewardship of remote and distributed data	web service	In this note, we introduce DataSpace Archives. DataSpace Archives are built on top of DataSpace’s DSTP servers [2] and are designed not only to provide a long term archiving of data, but also to enable the archived data to be discovered, explored, integrated and mined. DataSpace Archives are based upon web services. Web services’ UDDI and WSDL mechanisms provide a simple means for any web service client to discover relevant archived data [7]. In addition, data in DataSpace Archives can carry a variety of XML metadata, and the DSTP servers which underly the DataSpace Archives provide direct access to this metadata. Unfortunately, web services today do not provide the scalabilty required to work with large remote data sets. For this reason, DataSpace Archives employ a scalable web service we have developed called SOAP+. As the amount of data grows, the ability to explore and browse remote and distributed archived data will become more and more important. For this reason, a requirement of DataSpace Archives is that they support direct browsing of the data they contain, without the necessity of first retrieving the data and then opening a local application. DataSpace Archives also support a type of distributed database keys, which are described below and which enable data sets in different DataSpace Archives to be easily integrated. Finally, DataSpace Archives use emerging internet storage platforms, such as IBP [1] and OceanStore [6], as a basis for providing long term storage, long past the demise of any individual disk or server.	archive;browsing;cloud storage;dataspaces;distributed database;mined;random access;scalability;server (computing);web services description language;web services discovery;web service;xml	Robert L. Grossman;David Hanley;Xinwei Hong;Parthasarathy Krishnaswamy	2004			world wide web;data web;web service;database;computer science	Web+IR	-37.563603960060945	1.3578979616075926	49008
327c2b06d2326591369e1a019a177f88ddf5959b	poster: designing reusable user-interfaces for browsing a collection of neuroscience ontologies	user interfaces graph theory knowledge representation languages medical computing neurophysiology;class hierarchy;graph theory;ontologies owl neurons materials neuroscience;owl;owl reusable user interfaces neuroscience ontologies web ontology language semantic data class hierarchy;user interface;reusable user interfaces;semantic data;re usability;materials;medical computing;neuroscience;biological data representation;knowledge representation languages;neuroscience ontologies;web ontology language;semantic web;user interface design;ontologies;neurons;neurophysiology;biological data;graph theory biological data representation semantic web user interface design re usability;user interfaces	This poster examines the problem of generating effective, reusable web interfaces for searching and browsing neuroscience data represented in the Web Ontology Language (OWL). The goal of this work is to design interfaces that are reusable across different ontologies without the need for extensive customizations. In order to achieve this re-usability, we view the underlying semantic data in the ontology as a graph and are currently exploring the use of different graph properties to infer the structure of the class hierarchy in the ontology.	class hierarchy;graph property;ontology (information science);usability;user interface;web ontology language;world wide web	Akshaye Dhawan;Alison Nolan	2011	2011 IEEE 1st International Conference on Computational Advances in Bio and Medical Sciences (ICCABS)	10.1109/ICCABS.2011.5729894	upper ontology;open biomedical ontologies;ontology components;computer science;ontology;graph theory;theoretical computer science;database;ontology-based data integration;web ontology language;user interface;world wide web;owl-s;neurophysiology;process ontology	DB	-37.97608754628519	3.599456405529179	49029
6e181440d9dafe66f3bd62c128f06df1d10476a3	development of an information portal for the lake winnipeg basin initiative		The Lake Winnipeg Basin Initiative was created as part of Canada’s Action Plan on clean water. Its focus is to deal with excessive lake inputs of nutrients from surface runoff and municipal wastewater. Understanding the dynamics of nutrient loading, the associated algal blooms and resulting changes in fish populations, beach closures, and ecosystem imbalance requires access to various sources of data, information, knowledge, expertise and tools. Such critical components are delivered through the Lake Winnipeg Basin Initiative Information Portal, which integrates multiple geospatial and non-geospatial datasets of information pertaining to the basin and serves as a data, information and modelling portal. With data coming from many disparate sources, the Canadian Geospatial Data Infrastructure standards are applied to ensure interoperability. The “Community of Models” allows the modellers to post their model and results, and also allows the portal users to comment on the results to ensure a healthy dialogue.	data infrastructure;ecosystem;interoperability;population;typset and runoff	Isaac W. S. Wong;William G. Booty;Phil Fong;Sarah Hall	2011		10.1007/978-3-642-22285-6_72	environmental engineering	HPC	-40.794745020960804	-2.845315155003666	49089
1f5fdd27efd765c49097916cb249c6bc37f6ab8f	reasoning with large scale owl 2 el ontologies based on mapreduce		OWL 2 EL, which is underpinned by the description logic (mathcal {EL}), has been used to build terminological ontologies in real applications, like biomedicine, multimedia and transportation. On the other hand, there have been techniques that allow developers and users acquiring large scale ontologies by automatically extracting data from different sources or integrating different domain ontologies. Thus the issue of handling large scale ontologies has to be tackled. In this short paper, we report our work on classification of OWL 2 EL ontologies using MapReduce, which is a distributed computing model for data processing. We discuss the main problems when we use MapReduce to handle OWL 2 EL classification and how we address these problems. We implement the algorithm using Hadoop, and evaluate it on a cluster of machines. The experimental results show that our prototype system achieves a linear scalability on large scale ontologies.		Zhangquan Zhou;Guilin Qi;Chang Liu;Raghava Mutharaju;Pascal Hitzler	2016		10.1007/978-3-319-45817-5_40	computer science;data science;data mining;database	AI	-35.86855120685559	-0.049573291593586526	49230
7fec8989969547642a9c8e3c130066ee82ae89a9	an open science platform for the next generation of data		Imagine an online work environment where researchers have direct and immediate access to myriad data sources and tools and data management resources, useful throughout the research lifecycle. This is our vision for the next generation of the Dataverse Network: an Open Science Platform (OSP). For the first time, researchers would be able to seamlessly access and create primary and derived data from a variety of sources: prior research results, public data sets, harvested online data, physical instruments, private data collections, and even data from other standalone repositories. Researchers could recruit research participants and conduct research directly on the OSP, if desired, using readily available tools. Researchers could create private or shared workspaces to house data, access tools, and computation and could publish data directly on the platform or publish elsewhere with persistent, data citations on the OSP. This manuscript describes the details of an Open Science Platform and its construction. Having an Open Science Platform will especially impact the rate of new scientific discoveries and make scientific findings more credible and accountable. Keywords: open science, open data, data reuse, data citation, big data, data science, provenance, data privacy, real-time data analysis, data sharing 1    Primary    contact:    Latanya    Sweeney,    latanya@fas.harvard.edu.        Manuscript    date    April    2013.    This    paper    is drawn    from    an    unfunded    proposal    that    included    Latanya    Sweeney,    Merce    Crosas,    Gary    King,    Margo    Seltzer, Todd    Mostak,    Benjamin    Lewis,    Michael    Bar-­‐Sinai,    James    Waldo,    Francesca    Dominici,    Alyssa    Goodman, Hanspeter    Pfister,    Seymour    Knowles-­‐Barley Sweeney and Crosas An Open Science Platform for the Next Generation of Data	big data;citizen science;computation;confidentiality;data science;graphml;information privacy;json;mysql;next-generation network;open research;personally identifiable information;postgresql;prototype;r language;real-time data;real-time transcription;spss;shapefile;solr;stata;tiff;workspace;edx	Latanya Sweeney;Mercè Crosas	2015	CoRR		big data;data quality;computer science;data science;operating system;data warehouse;data mining;world wide web;computer security	DB	-42.24373169139696	-0.8625490034134785	49485
55939434be089ef2b864b77c6ca38fe9d5363b4c	a secure mutual authentication scheme with non-repudiation for vehicular ad hoc networks	vehicular ad hoc networks;mutual authentication;non repudiation;security;privacy	Department of Computer Science and Information Engineering, Chaoyang University of Technology 168, Jifeng E. Rd., Wufeng District, Taichung, 41349 Taiwan, R.O.C. 2 College of Information, Changchun University of Technology [e-mail: clc@mail.cyut.edu.tw] Department of Information and Communication Engineering, Chaoyang University of Technology 168, Jifeng E. Rd., Wufeng District, Taichung, 41349 Taiwan, R.O.C. [e-mail: {mlchiang, goudapeng}@cyut.edu.tw] Department of Computer Science and Information Engineering, Chaoyang University of Technology 168, Jifeng E. Rd., Wufeng District, Taichung, 41349 Taiwan, R.O.C. [e-mail: silencedence@hotmail.com] Information Engineering Institute, Changchun University of Technology 1699, Donghua Street, Shuangyang District, Changchun City, Jilin Province, China [e-mail: sqr1979@21cn.com]	computer science;email;hoc (programming language);information engineering;mutual authentication;non-repudiation;robbins v. lower merion school district;the wall street journal	Chin-Ling Chen;Mao-Lun Chiang;Chun-Cheng Peng;Chun-Hsin Chang;Qing-Ru Sui	2017	Int. J. Communication Systems	10.1002/dac.3081	vehicular ad hoc network;mobile ad hoc network;non-repudiation;computer science;information security;ad hoc wireless distribution service;internet privacy;privacy;computer security;computer network	DB	-44.74897626345109	-7.83224405196251	49623
60a21bb69f855b5c8e01cb42e3d2225b77ea03ab	geometric programming	prototypes;operations research;functional programming;polynomials;mathematical programming;solid modeling;book reviews;conferences;optimization methods	Geometric Programming (GP) is a special form of NLP that has many interesting features, and finds a lot of use in engineering design. We motivate its study through an example.... 3 of gravel is to be ferried across a river on a barge. A box (with the top open) is to be designed for this purpose	engineering design process;geometric programming;natural language processing	Thomas R. Jefferson;Carlton H. Scott	2008		10.1002/9780470050118.ecse537	mathematical optimization;functional reactive programming;computer science;theoretical computer science;prototype;procedural programming;symbolic programming;solid modeling;inductive programming;programming language theory;functional programming;algorithm;polynomial	HCI	-47.86645206395782	-4.530716423875846	49680
4ecd26718f3398bf776e09b09d1a2a9147581418	annotating data to support decision-making: a case study	remote sensing image;semantic annotation;remote sensing image classification;geospatial standards;geospatial data	Georeferenced data are a key factor in many decision-making systems. However, their interpretation is user and context dependent so that, for each situation, data analysts have to interpret them, a time-consuming task. One approach to alleviate this task, is the use of semantic annotations to store the produced information. Annotating data is however hard to perform and prone to errors, especially when executed manually. This difficulty increases with the amount of data to annotate. Moreover, annotation requires multi-disciplinary collaboration of researchers, with access to heterogeneous and distributed data sources and scientific computations. This paper illustrates our solution to approach this problem by means of a case study in agriculture. It shows how our implementation of a framework to automate the annotation of geospatial data can be used to process real data from remote sensing images and other official Brazilian data sources.	computation	Carla Geovana N. Macário;Jefersson Alex dos Santos;Claudia Bauzer Medeiros;Ricardo da Silva Torres	2010		10.1145/1722080.1722106	computer science;geospatial analysis;data mining;world wide web;information retrieval;cartography;remote sensing	HPC	-37.29882700408245	-0.39182571957522605	49917
73ad12bb57dec6ebf1cf6464eb8eb2ed16d5daad	performing entity facts		In a theatre play, persons appear as playwright, director, actors, etc. The play may have several performances with changing casts while actors may contribute to other plays and the role of a contributor may vary. Persons and the character of their contribution are a major focus in the performing arts domain. In order to create a domain specific and comprehensive research portal, current indexing techniques are combined with linked data methods giving access to person related information. The Specialised Information Service Performing Arts aggregates numerous metadata sources documenting the holdings of German-speaking cultural heritage institutions. This information source is extended by links to the recently established service Entity Facts by the German National Library that provides details to the persons related to the metadata records. The portal is based on the VuFind framework with index files that cover the metadata of all data providers and the cached data of all related authority records from Entity Facts. In order to achieve this, the standard model of VuFind MARC21 has been replaced by the Europeana model EDM. This allows for modeling all data – metadata and authority data – following linked data principles. While the respective mappings could be re-used new record drivers and indexing rules had to be defined.	data model;europeana;gene ontology term enrichment;identifier;information source;json;json-ld;linked data;linked data page;performance;semantic web;software documentation;type conversion	Julia Beck;Michael Büchner;Stephan Bartholmei;Marko Knepper	2016	Datenbank-Spektrum	10.1007/s13222-016-0241-6	computer science;data mining;database;multimedia;world wide web;information retrieval;metadata repository	DB	-42.43286233992871	2.471678394184619	50178
57807ccb9a2b2fbb52b5b8dd4fc710d9f36acc13	restifying real-world systems: a practical case study in rfid		As networked sensors become increasingly connected to the Internet, RFID or barcode-tagged objects are likely to follow the same trend. The EPC Network is a set of standards to build a global network for such electronically tagged goods and objects. Amongst these standards, the Electronic Product Code Information Service (EPCIS) specifies interfaces to capture and query RFID events from external applications. The query interface, implemented via SOAP-based Web services, enables business applications to consume and share data beyond companies borders and forms a global network of independent EPCIS instances. However, the interface limits the application space to the rather powerful platforms which understand WS-* Web services. In this chapter we introduce tools and patterns for Webenabling real-world information systems advertising WS-* interfaces. We describe our approach to seamlessly integrate RFID information systems into the Web by designing a RESTful (Representational State Transfer) architecture for the EPCIS. In our solution, each query, tagged object, location or RFID reader gets a unique URL that can be linked to, exchanged in emails, browsed for, bookmarked, etc. Additionally, this enables Web languages such as HTML and JavaScript to directly use RFID data to fast-prototype light-weight applications such as mobile applications or Web mashups. We illustrate these benefits by describing a JavaScript mashup platform that integrates with various several services on the Web (e.g., Twitter, Wikipedia, Dominique Guinard ETH Zurich, Switzerland, MIT Auto-ID Labs, USA and SAP Research, Switzerland e-mail: dguinard@ethz.ch Mathias Mueller Software Engineering Group, University of Fribourg, Switzerland Vlad Trifa Institute for Pervasive Computing, ETH Zurich, Switzerland The original publication is available at www.springerlink.com published in the book: REST: From Research to Practice, edited by E. Wilde and C. Pautasso.	application programming interface;auto-id labs;barcode;dashboard;electronic product code;email;global network;html;information system;javascript;mashup (web application hybrid);mobile app;open-source software;performance;prototype;radio-frequency identification;representational state transfer;soap;sensor;software engineering;switzerland;tracing (software);ubiquitous computing;universal product code;web service;wikipedia;world wide web;world-system	Dominique Guinard;Mathias Mueller;Vlad Trifa	2011		10.1007/978-1-4419-8303-9_16	the internet;mashup;world wide web;radio-frequency identification;javascript;representational state transfer;web service;epcis;soap;computer science	Web+IR	-42.648707345664306	-1.743971561036403	50222
46a740d341244ef3c82b5e2cb9fc16589d64ba6a	small data making big impact: an edw story from ohiohealth				Mrunal Shah;Rick Snow;Jyoti Kamal	2013			data science;small data;business	NLP	-38.80181237915182	-6.808390695757823	50324
496944e32352895a0d02c3d546cc478d57b397af	data warehousing and data mining as a tool to gain a competitive edge in healthcare institutes	data mining;data warehousing		data mining	B. Lavi;A. Armoni;Z. Rotstein;U. Gabay	2000			computer science;data science;data mining;database	ML	-39.70997107608281	-5.402270782772771	50483
7f7ac45d20b9d1987e0a8468e4ca114724e42a06	multimedia internetworking		Stephen Pink received a Bachelor's degree from the University of Minnesota, a Master's degree from Cornell University and a PhD from the Royal Institute of Technology in Sweden. He is the manager of the Computer and Network Architecture research group at the Swedish Institute of Computer Science and a Professor of Computer Science and Engineering at Luieä University in Sweden. His major interests are in protocol design, high speed and wireless networking, multimedia, and distributed operating systems.	communications protocol;computer engineering;distributed operating system;internetworking;network architecture;swedish institute of computer science	Stephen Pink	1995	it+ti - Informationstechnik und Technische Informatik	10.1524/itit.1995.37.4.26		Theory	-45.38460602678136	-7.152632530102508	50513
7fb0f69515638ae55e62e7d9cfd04b8ed8dfa95d	updating geographic databases using multi-source information.	geographic database			Sylvie Servigne;Robert Laurini	1995			computer science;geospatial analysis	DB	-38.0664352373793	-3.8100187059398603	50536
b1ca6def0d392c2fbea177ea0bddbd6851f8abbe	csg 94: set-theoretic solid modelling: techniques and applications, winchester, uk, april 1994		Programme Committee Susan Bloor University of Leeds, UK Adrian Bowyer University of Bath, UK Wim Bronsvoort Technical University of Delft, Netherlands Stephen Cameron University of Oxford, UK BK Choi KAIST, Korea Yukinori Kakazu Hokkaido University, Japan YT Lee Nanyang Technological Institute, Singapore Jarek Rossignac IBM Research, USA Moshe Shpitalni Technion, Israel ST Tan University of Hong Kong Bob Tilove GM Research Laboratories, USA Stephen Todd IBM UK Scientific Centre Don Vossler Electronic Data Systems, USA Tony Woo University of Michigan, USA	constructive solid geometry;data system;ibm research;os-tan;solid modeling;theory	S. Bloor;Adrian Bowyer	1994	Computer-Aided Design	10.1016/0010-4485(94)90036-1		Crypto	-46.41416813488766	-9.754455088649948	50569
9cd1d5bbbbf68614b85e9b4199a7d626bf7dd96e	ontoabsolute as a ontology evaluation methodology in analysis of the structural domains in upper, middle and lower level ontologies	query language;query evaluation;semantic web	Ontology researchers struggle to propose various methods and methodology in ontology evaluation as reference sources for ontology engineering and evaluating process. In this paper, we intend to explain a new ontology evaluation methodology (i.e., OntoAbsolute) which it is proposed to evaluate structural domains in semantic relations and concepts in upper, middle and lower ontologies. In 2007, a modern method was proposed in quantitative evaluation of the structural domains in knowledge organizations based on Kant's Epistemology. OntoAbsolute relies on the modern quantitative evaluation method to evaluate simplicity and unity concepts (i.e., meta-properties) in the structural domains in concepts and their relations via proposed criteria and related measures. OntoAbsolute has a capacity to develop new criteria and measures to access cognitive results in ontology as a whole. OntoAbsolute is similar to OntoClean in terms of domain-independent feature and meta-properties usage and also OntoAbsolute consists of the same structure to classify the characteristics (i.e., multi-level framework) with OntoMetric.	cognition;ontology (information science);ontology engineering;workbench	Kevin Vlaanderen;Francisco Valverde;Oscar Pastor	2008	2011 International Conference on Semantic Technology and Information Retrieval	10.1007/978-3-642-00670-8_3	rdf/xml;named graph;turtle;xslt;xml schema;streaming xml;computer science;sparql;xpath 2.0;artificial intelligence;semantic web;xml database;database;rdf query language;xml signature;programming language;web search query;information retrieval;rule interchange format;query language;rdf schema	Web+IR	-36.90376934003804	3.4378094373051784	50638
fd515735cd195e068d9cba40aa06ad482066d1e2	sensing a changing world	health research;uk clinical guidelines;biological patents;water management;europe pubmed central;citation search;wireless sensor network;uk phd theses thesis;temporal scale;remote sensing;life sciences;n a;sensor web;uk research reports;medical journals;air quality;europe pmc;biomedical research;bioinformatics	The workshop “Sensing a Changing World” was held in Wageningen, The Netherlands, from November 19–21, 2008. The main goal of the workshop was to explore and discuss recent developments in sensors and (wireless) sensor networks for monitoring environmental processes and human spatial behavior in a changing world. The challenge is then to develop concepts and applications that can provide timely and on-demand knowledge to end-users in different domains over a range of different spatial and temporal scales. During this workshop over 50 participants, representing 15 countries, presented and discussed their recent research. The workshop provided a broad overview of state-of-the-art research in a broad range of application fields: oceanography, air quality, biodiversity and vegetation, health, tourism, water management, and agriculture. In addition the workshop identified the future research challenges. One of the outcomes of the workshop was a special issue in the journal Sensors with contributions presented at the workshop. This editorial of the special issue aims to provide an overview of the discussions held during the workshop. It highlights the ideas of the authors and participants of the workshop about directions of future research for further development of sensor-webs for “sensing” spatial phenomena. The “big” question was are we already able to sense a changing world? And if the answer is positive, then what are we going to sense and for what?	editorial;oceanography;spatial analysis;sensor (device)	Arend Ligtenberg;Lammert Kooistra	2009		10.3390/s90906819	sensor web;embedded system;air quality index;wireless sensor network;computer science;engineering;data mining;nanotechnology;operations research	HCI	-41.9560194608256	-7.9922771728214155	50671
db332631c697b17c3e771843c531d919c0f8d43b	a partial implementation of a relational data-base management system as an extension to fortran	relational data;management system;fortran	In general applications of a Data Base Management System (DBMS) for centralised monitoring of a medium sized data base, a Relational approach is often said to be preferred to a Hierarchical or a Network approach, because of its simplicity and ease of usage. An experimental Relational DBMS has been implemented at the Computer Centre of the Indian Institute of Technology, Kanpur, India. This system provides most of the facilities required for a Data Model Definition, Data Sub-language, and Data Manipulation Language. The FORTRAN language was selected as the host, because of its popularity and the available system software. The resulting DBMS has been tested on live data and the results have been encouraging.	database;fortran	R. K. Bagga;V. Rajaraman	1979	Information & Management	10.1016/0378-7206(79)90023-5	data independence;sql;economics;relational database;computer science;data mining;management system;database;programming language;management;world wide web	DB	-43.26971199771795	-3.540854157771806	50680
64d424f06d50a6d2149325dcb75846abdf0807d7	the crowd is the territory: assessing quality in peer-produced spatial data during disasters		ABSTRACTToday, disaster events are mobilizing digital volunteers to meet the data needs of those on the ground. One form of this crowd work is Volunteered Geographic Information. This peer-produced spatial data creates the most up-to-date map of the affected region; maintaining the accuracy of these data is therefore a critical task. Accuracy is one aspect of data quality, a relative concept requiring standards to measure against. The field of Geographic Information Sciences has developed standards for this comparison, achieving widespread acceptance. However, the peer production model of spatial data presents new opportunities—and challenges—to traditional methods of quality assessment. Through analysis of the OpenStreetMap database, we show that temporal editing patterns and contributor characteristics can provide additional means of understanding spatial data quality. Drawing upon experiences from Wikipedia, we offer and evaluate three intrinsic quality metrics of peer-produced spatial data to assess t...		T. Jennings Anderson;Robert Soden;Brian Keegan;Leysia Palen;Kenneth M. Anderson	2018	Int. J. Hum. Comput. Interaction	10.1080/10447318.2018.1427828	multimedia;data mining;peer production;spatial analysis;computer science;volunteered geographic information;information science;data quality	HCI	-42.10928960340923	-4.625042410008361	51052
42ff5eb731a97853861442ca350cc31f757397ae	sko types: an entity-based scientific knowledge objects metadata schema	semantic annotation;metadata schema;entity oriented;scientific knowledge object	During the past fifty years, many metadata schemas have been developed in a variety of disciplines. However, current scientific metadata schemas focus on describing data, but not entities. They are descriptive, but few of them are structural and administrative. SKO Types is an entity-oriented theory for representing and linking Scientific Knowledge Objects by defining entities, relationships between entities, and attributes of each entity in the scientific domain. In SKO management, SKO Types serve as the basis for relating entities, entity components, aggregated entities, relationships and attributes to various tasks, e.g. linked entity, rhetorical structuring, strategic reading, semantic annotating, etc., that users may perform when consulting ubiquitous SKOs.	entity;simple knowledge organization system	Hao Xu;Fausto Giunchiglia	2015	J. Knowledge Management	10.1108/JKM-11-2014-0452	computer science;knowledge management;data mining;entity linking;database;weak entity	DB	-42.49866138670485	2.8005389747903804	51209
8c741a24397fda15ac4aa4b1b1d1fa620984eb9f	a survey of association rule hiding methods for privacy		Data and knowledge hiding are two research directions that investigate how the privacy of raw data, or information, can be maintained either before or after the course of mining the data. By focusing on the knowledge hiding thread, we present a taxonomy and a survey of recent approaches that have been applied to the association rule hiding problem. Association rule hiding refers to the process of modifying the original database in such a way that certain sensitive association rules disappear without seriously affecting the data and the non-sensitive rules. We also provide a thorough comparison of the presented approaches, and we touch upon hiding approaches used for other data mining tasks. A detailed presentation of metrics used to evaluate the performance of those approaches is also given. Finally, we conclude our study by enumerating interesting future directions in this research body.	algorithm;association rule learning;data mining;database;emergence;privacy;video synopsis	Vassilios S. Verykios;Aris Gkoulalas-Divanis	2008		10.1007/978-0-387-70992-5_11	data mining;internet privacy;computer security	DB	-36.60044249261579	-6.612014204230619	51295
3fa5345d5d63a954ef41aabe2eadebf0ed71fc0b	multimedia digital libraries handling: the organic mmir perspective		This paper focuses on new retrieval methods and tools applicable to the management of multimedia documents in Digital Libraries (DL). These matters merge in the organic methodology of MultiMedia Information Retrieval (MMIR). A paper’s goal is to demonstrate the operating limitations of a generic Information Retrieval (IR) system, restricted only to textual language. MMIR offers a better alternative, whereby every kind of digital document can be analyzed and retrieved with the elements of language appropriate to its own nature, directly handling the concrete document content. The integration of this content-based conception of information processing with the traditional semantic conception, can offer the advantages of both systems in accessing of information and documents managed in actual multimedia digital libraries.		Roberto Raieli	2013		10.1007/978-3-642-54347-0_19	computer science;theoretical computer science;multimedia;computer graphics (images)	Vision	-44.682596379822805	1.9806660422099893	51458
49e97be86f3763c0906fb7ce45dab834c8289b3f	on the need of graph support for developer identification in software repositories.		Software repositories from open-source projects provide a rich source of information for a wide range of tasks. However, one issue to overcome in order to make this information useful is the accurate identification of developers. This is a particular challenge, as developers usually use different IDs in different repositories of one project, but usually there is no kind of dictionary or similar available to map the different IDs to realworld persons. Often, they even use different IDs in the same repository. We show that the few methods suggested so far are not always appropriate to overcome this problem. Further, we highlight related techniques from other areas and discuss how they can be applied in this context. We particularly focus on the idea of applying graph-based methods and argue for the benefits we expect from that.	dictionary;information source;open-source software;software repository	Aftab Iqbal;Marcel Karnstedt	2010			systems engineering;database;world wide web	SE	-43.6632685308573	-0.00012063197996755031	51539
9df2fe9effc52e82e2fcf82d5216d58ffe168486	data blending in manufacturing and supply chains	databases;filtering;measurement;supply chains;big data;manufacturing;data integration	Big Data revolution has transformed business models of many organizations to include the usage of big data analytics. Big Data are believed to be the key basis of competition and growth in today's world whereby huge amounts of data are created daily. One of the main challenges of Big Data is not mainly about the storage of the data but how to blend the different varieties or sources of data together and turn them into values. As the nature of supply chain is complex and dynamic, data are stored in various forms or managed independently. The data have their own naming convention as the data from the different nodes in the supply chain seldom communicate with each other. Some of the challenges of data blending are the lack of unique identifiers to merge the data together and the lack of training data or domain knowledge to understand the criteria to blend the data. In this paper, an automatic filtering and sorting similarity metric, Term Frequency-Inverse Document Frequency (TF-IDF) Ratcliff/Obershelp is proposed. The method is able to handle the issue of same entity with different naming conventions and allow word filtering. The experiment results show that the proposed TF-IDF Ratcliff/Obershelp is able to improve the performance of the data blending.	alpha compositing;big data;sorting;tf–idf;unique identifier	B. Y. Ong;Rong Wen;Allan N. Zhang	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7841047	data quality;computer science;data science;data mining;database;data efficiency	DB	-36.83106475304484	-0.17543113155103882	51541
0f9d42d7d2b628b52605716fef7cdfb3f5e882e0	mining farmers problems in web-based texual database application				Said Mabrouk;Mahmoud Rafea;Ahmed A. Rafea;Samhaa R. El-Beltagy	2010			data science;data mining	DB	-39.72841152598181	-6.558203490844272	51554
249d6aff4fed717c220a0ce316eeb9d0bc76d697	visual exploration of large data sets	journal_article;large data sets	level view of a five-dimensional, 16,000-record remote-sensing data set. Lines indicate cluster centers and bands indicate the extent of the clusters in each dimension. Data represents five channels—spot, magnetics, and three involving radiometrics—focusing on potassium, thorium, and uranium from the Grant's Pass region of Australia. Data courtesy of Peter Ketelaar, Commonwealth Scientific and Industrial Research Organization, Australia. Image generated using XmdvTool, a public-domain multivariate data visualization package; courtesy Matthew Ward, Worcester Polytechnic Institute, Worcester, MA.	data visualization;peter gutmann (computer scientist)	Daniel A. Keim	2001	Commun. ACM	10.1145/381641.381656	computer science;data science;data mining;information retrieval	Visualization	-42.67577119325998	-9.438994580490238	51593
8aa5fd2a5dbbe589c73b2db8b57221a137e601bb	making the web of data available via web feature services		Interoperability is the main challenge on the way to efficiently find and access spatial data on the web. Significant contributions regarding interoperability have been made by the Open Geospatial Consortium (OGC), where web service standards to publish and download spatial data have been established. The OGCs GeoSPARQL specification targets spatial data on the Web as Linked Open Data (LOD) by providing a comprehensive vocabulary for annotation and querying.While OGC web service standards are widely implemented in Geographic Information Systems (GIS) and offer a seamless service infrastructure, the LOD approach offers structured techniques to interlink and semantically describe spatial information. It is currently not possible to use LOD as a data source for OGC web services. In this chapter we make a suggestion for technically linking OGC web services and LOD as a data source, and we explore and discuss its benefits. We describe and test an adapter that enables access to geographic LOD datasets from within OGC Web Feature Service (WFS), enabling most current GIS to access the Web of Data. We discuss performance tests by comparing the proposed adapter to a reference WFS implementation. J. Jones · S. Scheider Institute for Geoinformatics, University of Münster, Münster, Germany e-mail: jim.jones@uni-muenster.de S. Scheider e-mail: simon.scheider@uni-muenster.de W. Kuhn Center for Spatial Studies University of California, Santa Barbara, USA e-mail: kuhn@geog.ucsb.edu C. Keßler (B) CARSI, Department of Geography, Hunter College, City University of New York, New York, USA e-mail: carsten.kessler@hunter.cuny.edu J. Huerta et al. (eds.), Connecting a Digital Europe Through Location and Place, 341 Lecture Notes in Geoinformation and Cartography, DOI: 10.1007/978-3-319-03611-3_20, © Springer International Publishing Switzerland 2014	cartography;consortium;download;email;geosparql;geographic information system;geoinformatics;interoperability;jones calculus;linked data;seamless3d;semantic web;springer (tank);switzerland;vocabulary;web feature service;web service;world wide web	Jim Jones;Werner Kuhn;Carsten Keßler;Simon Scheider	2014		10.1007/978-3-319-03611-3_20	web service;sensor web;distributed gis;web processing service;web development;web modeling;data web;web mapping;web design;web standards;computer science;ws-policy;data mining;database;web intelligence;ws-i basic profile;world wide web;observations and measurements;web feature service;web coverage service	Web+IR	-42.38889063990376	-1.7367943376095174	51843
214c966d1f9c2a4b66f4535d9a0d4078e63a5867	brainwash: a data system for feature engineering		A new generation of data processing systems, including web search, Google’s Knowledge Graph, IBM’s Watson, and several different recommendation systems, combine rich databases with software driven by machine learning. The spectacular successes of these trained systems have been among the most notable in all of computing and have generated excitement in health care, finance, energy, and general business. But building them can be challenging, even for computer scientists with PhD-level training. If these systems are to have a truly broad impact, building them must become easier. We explore one crucial pain point in the construction of trained systems: feature engineering. Given the sheer size of modern datasets, feature developers must (1) write code with few effective clues about how their code will interact with the data and (2) repeatedly endure long system waits even though their code typically changes little from run to run. We propose brainwash, a vision for a feature engineering data system that could dramatically ease the ExploreExtract-Evaluate interaction loop that characterizes many trained system projects.	computer scientist;data system;database;feature engineering;knowledge graph;machine learning;recommender system;thomas j. watson research center;waits;web search engine	Michael R. Anderson;Dolan Antenucci;Victor Bittorf;Matthew Burgess;Michael J. Cafarella;Arun Kumar;Feng Niu;Yongjoo Park;Christopher Ré;Ce Zhang	2013			watson;recommender system;data science;database;computer science;data processing system;software;ibm;health care;feature engineering;graph	DB	-38.231792946365566	-8.71061240419272	52005
380e408ecaf9dcb0ceed281f2cfbbab16bd87d98	computational modeling of objects presented in images. fundamentals, methods, and applications		Computational modeling of objects presented in images: fundamentals, methods and applications Paolo Di Giamberardino, Daniela Iacoviello, R.M. Natal Jorge & Joao Manuel R.S. Tavares a Department of Computer, Control and Management Engineering Antonio Ruberti, Sapienza, Università di Roma, Italy b Departamento de Engenharia Mecânica, Faculdade de Engenharia, Instituto de Ciência e Inovação em Engenharia Mecânica e Engenharia Industrial, Universidade do Porto, Porto, Portugal Published online: 23 Jun 2015.	adaptive sampling;aliasing;coefficient;computation;computational model;data general eclipse mv/8000;experiment;leaflet;mv-algebra;markov switching multifractal;multiscale modeling;sparse matrix;video-in video-out	Yongjie Jessica Zhang;João Manuel R. S. Tavares	2014		10.1007/978-3-319-09994-1		Robotics	-46.67999394136543	-9.5745233705826	52171
9547277c7fd3c7da87e326c30608877a2158519d	semantic web for the working ontologist: effective modeling in rdfs and owl	semantic web	Semantic Web models and technologies provide information in machine-readable languages that enable computers to access the Web more intelligently and perform tasks automatically without the direction of users. These technologies are relatively recent and advancing rapidly, creating a set of unique challenges for those developing applications. #R##N##R##N#Semantic Web for the Working Ontologist is the essential, comprehensive resource on semantic modeling, for practitioners in health care, artificial intelligence, finance, engineering, military intelligence, enterprise architecture, and more. Focused on developing useful and reusable models, this market-leading book explains how to build semantic content (ontologies) and how to build applications that access that content.#R##N##R##N#New in this edition: #R##N##R##N##R##N#Coverage of the latest Semantic Web tools for organizing, querying, and processing information - see details in TOC below#R##N##R##N#Detailed information on the latest ontologies used in key web applications including ecommerce, social networking, data mining, using government data, and more#R##N##R##N##R##N##R##N##R##N#Updated with the latest developments and advances in Semantic Web technologies for organizing, querying, and processing information, including SPARQL, RDF and RDFS, OWL 2.0, and SKOS #R##N#Detailed information on the ontologies used in today's key web applications, including ecommerce, social networking, data mining, using government data, and more #R##N#Even more illustrative examples and case studies that demonstrate what semantic technologies are and how they work together to solve real-world problems#R##N##R##N#Table of Contents#R##N##R##N#1?What Is The Semantic Web?#R##N#2?Semantic Modeling#R##N#3?RDF - The Basis of the Semantic Web#R##N#4?SPARQL - The Query Language for RDF#R##N#5?Semantic Web Application Architecture#R##N#6?RDF And Inferencing#R##N#7?RDF Schema Language#R##N#8?RDFS-Plus#R##N#9?SKOS - the Simple Knowledge Organization System#R##N#10?Ontologies in the Wild: Linked Open Data and the Open Graph Project#R##N#11?Basic OWL#R##N#12?Counting and Sets In OWL #R##N#13?MORE Ontologies in the Wild: QUDT, GoodRelations, and OBO Foundry#R##N#14?Good and Bad Modeling Practices #R##N#15?OWL 2.0 Levels and Logic#R##N#16?Conclusions#R##N#17?Frequently Asked Questions	rdf schema;semantic web;web ontology language	Dean Allemang;James A. Hendler	2011			cwm;semantic computing;web modeling;semantic integration;semantic web rule language;data web;semantic search;semantic grid;web standards;computer science;sparql;simple knowledge organization system;semantic web;social semantic web;linked data;data mining;semantic web stack;web intelligence;world wide web;owl-s;information retrieval;semantic analytics;rule interchange format	Web+IR	-45.475656529237845	1.0436891826928092	52188
1b83d8fa4d5fc7a33110055222e901af6b651234	estimating aboveground biomass in interior alaska with landsat data and field measurements	alaska;landsat;spectral vegetation index;aboveground biomass;yukon flats ecoregion;lidar	aASRC Research and Technology Solutions, contractor to the U.S. Geological Survey (USGS) Earth Resources Observation and Science (EROS) Center, Sioux Falls, SD 57198-0001, work performed under USGS contract 08HQCN0007, lji@usgs.gov; bUSGS EROS Center, Sioux Falls, SD 57198-0001, wylie@usgs.gov, jrover@usgs.gov; cBoreal Ecology Cooperative Research Unit, University of Alaska Fairbanks, Fairbanks, AK 99775-6780, drnossov@alaska.edu; dUSGS, Menlo Park CA, 94025, mwaldrop@usgs.gov; eBoreal Ecology Cooperative Research Unit, Pacific Northwest Research Station, USDA Forest Service, Fairbanks, AK 99775-6780, tnhollingsworth@alaska.edu	eros (microkernel);ecology	Lei Ji;Bruce K. Wylie;Dana R. Nossov;Birgit Peterson;Mark P. Waldrop;Jack W. McFarland;Jennifer Rover;Teresa N. Hollingsworth	2012	Int. J. Applied Earth Observation and Geoinformation	10.1016/j.jag.2012.03.019	lidar;geography;geology;ecology;physics;remote sensing	HPC	-45.17642365695468	-9.167384739346922	52199
adf5634984b5c6cfc2e91e3ea9001e99f1e89cb5	a large scholarly corpus: a bird's-eye view		In this paper we present a new, very large, rich, Comprehensive Scholarly Corpus (CompScholarCorp) as a platform and data source for future research. Our corpus contains records of 1,044,454 papers, 472,365 unique authors, and substantial publication meta-data for each record. We have integrated the data we collected from 276 publishers using a uniform and consistent XML data format within the corpus. The corpus is designed to be compatible with DBLP enabling existing research to utilise our new corpus directly. As an initial analysis of the corpus, we present a number of visualisations of the corpus to better understand the data, provide some analytics of the data, and present a rule-of-thumb we have observed for citations.	bird's-eye view;dbl-browser;xml	Yashar Najaflou;Kris Bubendorfer	2017	2017 IEEE 13th International Conference on e-Science (e-Science)	10.1109/eScience.2017.75	data mining;xml;data visualization;visualization;computer science;corpus linguistics;cloud computing;analytics	DB	-42.493790998198584	1.2610654086455064	52501
6185abec8ff575ad1edb8deb3095b9c43aeb0f1f	interoperable multimedia annotation and retrieval for the tourism sector		The Atlas Metadata System (AMS) employs semantic web annotation techniques in order to create an interoperable informa- tion annotation and retrieval platform for the tourism sector. AMS adopts state-of-the-art metadata vocabularies, annotation techniques and semantic web technologies. Interoperability is achieved by reusing several vocabularies and ontologies, including Dublin Core, PROV-O, FOAF, Geonames, Creative commons, SKOS, and CiTO, each of which provides with orthogonal views for annotating different aspects of digital assets. Our system invests a great deal in managing geospatial and tem- poral metadata, as they are extremely relevant for tourism-related appli- cations. AMS has been implemented as a graph database using Neo4j, and is demonstrated with a dataset of more than 160000 images down- loaded from Flickr. The system provides with online recommendations, via queries that exploit social networks, spatiotemporal references, and user rankings. AMS is offered via service-oriented endpoints using public vocabularies to ensure reusability.	interoperability	Antonios Chatzitoulousis;Pavlos S. Efraimidis;Ioannis N. Athanasiadis	2015		10.1007/978-3-319-24129-6_6	image retrieval;computer science;database;world wide web;information retrieval	Web+IR	-41.97727816257662	3.2247547276723	52735
9c0f865ff742d08662c7b12a524138a10024e76e	an intelligent metadata extraction approach based on programming by demonstration	rule based system;metadata extraction;demonstration by programming	Metadata extraction is an important prerequisite for remote sensing images management and sharing. Business users urgently need to extract metadata automatically and quickly. However, existing metadata extraction applications extract only one single type of image metadata. In this paper, we proposed a generic and extensible metadata extraction approach based on Programming by Demonstration (PbD). Data owners can specify the metadata items to be extracted and the extraction methods in an interactive and visual user interface. Such knowledge will be stored in the rule base in the form of rules. The advantage of this approach is that users do not need any programming knowledge, but they can handle new types of images themselves.	programming by demonstration	Binge Cui;Jie Zhang	2012		10.1007/978-3-642-33469-6_84	rule-based system;metadata modeling;computer science;artificial intelligence;marker interface pattern;data mining;database;world wide web;data element;meta data services;information retrieval;metadata repository	NLP	-38.76497292268868	3.2391257116615195	52955
6e487ce2e1bafdc96ea00a1bfd5c9b347d0bcdfe	experiences in cobol compiler validation	errors;syntax;standards;cobol;specifications;machine coding;department of defense;computer program documentation;compilers;federal government;software development;information processing;validation;national bureau of standards;automatic data processing;computer program verification;programming languages;subroutines	The Federal COBOL Compiler Testing Service (FCCTS) is an activity of the Software Development Division of the Department of the Navy, Automatic Data Processing Equipment Selection Office (ADPESO). Since July 1, 1972, all COBOL compilers brought into the Federal Government have to be identified as implementing one of the levels of the Federal COBOL Standard. The National Bureau of Standards, which has the responsibility for the development and maintenance of Federal ADP Standards, has delegated to the Department of Defense, and thereby to ADPESO, the responsibility for the operation of a Government-wide COBOL Compiler Testing Service. This responsibility is discharged by the FCCTS through the implementation and maintenance of the COBOL Compiler Validation System, a comprehensive set of routines used to test COBOL compilers for compliance with the Federal COBOL Standard as prescribed in Federal Information Processing Standards Publication 21 (FIPS PUB 21), published by the National Bureau of Standards.	ccvs;cobol compilers;compiler;conformance testing;information processing;object code;programming language specification;requirement;software development;software engineering;software quality assurance;usb on-the-go	George N. Baird;Margaret M. Cook	1974		10.1145/1500175.1500263	computer science;software engineering;database;cobol;programming language	SE	-46.663779994852824	-4.77732286710326	52981
f55f35691687ad93ab97fcd7c2d91f46a91b98c7	applications of gps technology in the land transportation system	avl;thematic maps;geographic information system;geographic information;global position system;transport system;gps;gis;transportation;rail	The global positioning system (GPS) allows the accurate positioning of an object using satellite signals. There are a lot of applications of this technology in many scientific fields all over the world. In recent years, the rapid increase in the development of the geographic information system technology (GIS) has led to the development of GPS/GIS applications. Therefore, the geometric and geographic information obtained by the use of GPS can be introduced to GIS database and thus thematic maps can be produced. In the framework of this paper, a short overview of applications in the area of transportation in Greece and abroad is presented. Emphasis is placed on an ongoing application in railway mapping, through the presentation of its pilot phase in Greece. The use of modern technologies, the problems identified and the results produced are presented and discussed. 2003 Elsevier B.V. All rights reserved.	automotive navigation system;expectation–maximization algorithm;gis applications;geographic information system;global positioning system;informatics;mobile phone;multimodal interaction;real-time data;real-time locating system;thematic map	G. Mintsis;S. Basbas;P. Papaioannou;C. Taxiltaris;Ilias N. Tziavos	2004	European Journal of Operational Research	10.1016/S0377-2217(03)00032-8	enterprise gis;transport;global positioning system;gis and public health;geographic information system;thematic map	Robotics	-46.88459164321656	-1.7431809924396402	53193
215b63ec84ebca41e47944498eaaf61e7ef0eeea	a linked data approach to sharing workflows and workflow results	linked data;text mining;concept web;data analysis;digital preservation;semantic web;provenance;biological systems;workflow	A bioinformatics analysis pipeline is often highly elaborate, due to the inherent complexity of biological systems and the variety and size of laboratory publications would be highly beneficial to bioinformatics, for evaluating evidence and examining data across related experiments, while introducing the potential to find associated resources and integrate them as data and services. We present initial steps towards preserving bioinformatics design of a data analysis pipeline, and RDF to link the workflow, its component services, run-time provenance, and a personalized biological interpretation of the results. An example shows the reproduction of the unique graph of an analysis procedure, its results, provenance, and personal interpretation of a text mining experiment. It links data from Taverna, myExperiment.org, BioCatalogue.org, and ConceptWiki.org. The approach is relat unobtrusive to bioinformatics users.	bioinformatics;biological system;experiment;graph (discrete mathematics);linked data;personalization;resource description framework;text mining;unobtrusive javascript;myexperiment;taverna	Marco Roos;Sean Bechhofer;Jun Zhao;Paolo Missier;David R. Newman;David De Roure;M. Scott Marshall	2010		10.1007/978-3-642-16558-0_29	concept map;workflow;text mining;computer science;semantic web;linked data;data mining;data analysis;world wide web;workflow management system;information retrieval;workflow engine;workflow technology	Comp.	-40.53198599546356	1.7772025663795317	53527
492ca1d12b2b95aab728721be906efd4a3bae051	the semantic data dictionary approach to data annotation & integration		A standard approach to describing datasets is through the use of data dictionaries: tables which contain information about the content, description, and format of each data variable. While this approach is helpful for a human readability, it is difficult for a machine to understand the meaning behind the data. Consequently, tasks involving the combination of data from multiple sources, such as data integration or schema merging, are not easily automated. In response, we present the Semantic Data Dictionary (SDD) specification, which allows for extension and integration of data from multiple domains using a common metadata standard. We have developed a structure based on the Semanticscience Integrated Ontology’s (SIO) high-level, domain-agnostic conceptualization of scientific data, which is then annotated with more specific terminology from domain-relevant ontologies. The SDD format will make the specification, curation and search of data much easier than direct search of data dictionaries through terminology alignment, but also through the use of “compositional” classes for column descriptions, rather than needing a 1:1 mapping from column to class.	1:1 pixel mapping;conceptualization (information science);data dictionary;digital curation;high- and low-level;ontology (information science)	Sabbir M. Rashid;Katherine Chastain;Jeanette A. Stingone;Deborah L. McGuinness;Jim McCusker	2017			semantic computing;data annotation;data dictionary;ontology-based data integration;data mapping;semantic data model;information retrieval;machine-readable dictionary;computer science	DB	-41.03953550007671	3.263303537960452	53600
d932eecc37c8bc9b7043ceb8edbb7e78fab8096c	quality-driven geospatial data integration	urban planning;data integrity;large scale;quality criteria;experimental evaluation;query answering;quality of data;geospatial data;geospatial data quality;quality driven query answering	Accurate and efficient integration of geospatial data is an important problem with applications in areas such as emergency response and urban planning. Some of the key challenges in supporting large-scale geospatial data integration are automatically computing the quality of the data provided by a large number of geospatial sources and dynamically providing high quality answers to the user queries based on a quality criteria supplied by the user. We describe a framework called the Quality-driven Geospatial Mediator (QGM) that supports efficient and accurate integration of geospatial data from a large number of sources. The key contributions of our framework are: (1) the ability to automatically estimate the quality of data provided by a source by using the information from another source of known quality, (2) representing the quality of data provided by the sources in a declarative data integration framework, and (3) a query answering technique that exploits the quality information to provide high quality geospatial data in response to user queries. Our experimental evaluation using over 1200 real-world sources shows that QGM can accurately estimate the quality of geospatial sources. Moreover, QGM provides better quality data in response to the user queries compared to the traditional data integration systems and does so with lower response time.	display resolution;response time (technology)	Snehal Thakkar;Craig A. Knoblock;José Luis Ambite	2007		10.1145/1341012.1341034	data quality;computer science;data integrity;data mining;urban planning;database;information retrieval	DB	-35.770574208530455	2.8001342586219016	54182
ddc977b57e9a711cf1c84596564d4f5a84ef5fd1	managing large data volumes from scientific facilities				Shaun De Witt;Richard Sinclair;Andrew Sansum;Michael Wilson	2012	ERCIM News		database	HPC	-40.7873751496218	-4.858263438288603	54252
552c333f538e6aea867e9de0b7c4cd205b909e0a	"""preserving an evolving collection: """"on-the-fly"""" solutions for the chora of metaponto publication series"""			horde application framework	Jessica Trelogan;Maria Esteva;Lauren S Jackson	2015			on the fly;data mining;data science;computer science	ML	-41.2243793084432	0.08904026467056662	54375
a8d4e3d2322b1cef0c26869a64669011b35c1e06	the dark web portal project: collecting and analyzing the presence of terrorist groups on the web	information overload;web portal;social network analysis;prediction model	1 Department of Management Information Systems, The University of Arizona, Tucson, AZ 85721, USA {ednareid, qin, yiluz, hchen}@bpa.arizona.edu 2 Department of Systems and Industry Engineering, The University of Arizona, Tucson, AZ 85721, USA guanpi@email.arizona.edu 3 The Solomon Asch Center For Study of Ethnopolitical Conflict, University of Pennsylvania, St. Leonard’s Court, Suite 305, 3819-33 Chestmut Street, Philadelphia, PA 19104, USA sageman@sas.upenn.edu	asch conformity experiments;assignment zero;dark web;management information system;world wide web	Jialun Qin;Yilu Zhou;Guanpi Lai;Edna Reid;Marc Sageman;Hsinchun Chen	2005		10.1007/11427995_78	social network analysis;data web;web analytics;computer science;information overload;data mining;predictive modelling;internet privacy;world wide web	ML	-43.53371557644406	-8.14615461428342	54389
54e621f43a4e42db2cf5f8ec5cd8954d30a1a356	rehabilitation robotics	robotic approach;active task;lower extremity robotic device;rehabilitation practice;general importance;robotic device interaction;closing section;added value;robotic device;rehabilitation robotics	1 Department of Human and Information Systems, Faculty of Engineering, Gifu University, 1-1 Yanagido, Gifu 501-1193, Japan 2 Mechanical Engineering, School of Engineering, University of North Florida, 1 UNF Drive, Jacksonville, FL 32224, USA 3 Department of Mechanical Engineering, Sogang University, Mapoku, Seoul 121-742, Republic of Korea 4 Institut des Systèmes Intelligents et de Robotique, Université Pierre et Marie Curie, 75005 Paris, France	information system;rehabilitation robotics	Haruhisa Kawasaki;Daniel Cox;Doyoung Jeon;Ludovic Saint-Bauzel;Tetsuya Mouri	2011	J. Robotics	10.1155/2011/937875	biological engineering	Robotics	-46.544327149797375	-9.234309254042298	54462
95f366de4169ca7db5d7e61cd31b7f46d44677f3	an active database system for receiving broadcast data	active database system	In recent years, many broadcast and communication satellites have been launched to start various new services, which attracts a great deal of attention on data broadcasting system using satellites. In such systems, the volume and variety of data being broadcast will increase significantly. This results in a growing demand on a system that stores a large volume of data and presents the necessary information for users in an efficient way. Highly motivated by such requirements, we propose the super active database(SADB) system which is a data receiving system based on an active database in this paper. Since the use of active database can increase the flexibility in describing the system behavior, the proposed system is well suited to receive and process many types of data. On SADB, the receiving operation management and the received data processing are separated from the rule processing unit. As a result, the SADB can process a great deal of data faster than the conventional active		Tsutomu Terada;Masahiko Tsukamoto;Shojiro Nishio	2002			engineering;data mining;database;world wide web;database testing	DB	-34.18879618392827	-1.6002491051210725	54618
e77be6c43cae8f01fdda6f8fd92d7f8c0880beb0	multiplexing methods for fiber optic local communication networks	electrical engineering and computer science;thesis	Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1985.	multiplexing;optical fiber;telecommunications network	Stuart S. Wagner	1985			telecommunications;computer science;electrical engineering	Theory	-46.73918380432167	-6.838936438640509	54769
b723ee5f786338695cf2db9f477135f4b9d51517	provenance-based workflow composition with virtual simulation objects technology	computational experiments design provenance based workflow composition virtual simulation objects technology scientific workflows scientific experiment design semantic compliant provenance data processing virtual simulation objects toolbox vso toolbox intelligent problem solving environment;computational modeling ontologies software data models semantics cognition data mining;escience open provenance model composite application workflow virtual simulation objects;natural sciences computing digital simulation	Accumulated provenance data about correctly constructed and successfully executed scientific workflows provides a valuable source of knowledge, which may be used and reused in new scientific experiments' design. In this paper we propose a provenance-based workflow composition approach - the opposite to package-based one, presented earlier. Processing semantic-compliant provenance data makes available automatic search of already executed workflows (or their parts) in order to reuse them for newly designed applications. This helps the user to be aware only about input and output parameters and variants of intermediate process will be found in provenance and offered for choose. The idea was implemented into Virtual Simulation Objects (VSO) [1] toolbox which provides an intelligent problem solving environment for computational experiments design. Experimental use-case showing the proof of approach hypothesis is also presented.	computation;experiment;input/output;microsoft visual studio;problem solving environment;simulation	Pavel A. Smirnov;Sergey V. Kovalchuk	2014	2014 11th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2014.6980964	computer science;data science;database;world wide web;workflow engine;workflow technology	DB	-39.48750653490178	-0.348110279736627	54862
a94ef79ed46ffe8e9c978caf054088d617522e94	designing an intelligent operating system consultant and teacher (abstract)	software documentation;software engineering;group effort;operating system;paperless enviroments;on line documentation	In this pape: ~e shall describe the design of, and the progress made on, our research project concerned with construction of OSCAT, an expert system capable of acting as an intelligent Operating System Consultant And ~eacher in a training environiaent. OSCAT's natural language interface produces an internal representation of the dialogue semantics using expectation based parsing. ~his internal representation is then utilized in creation of the models constructed by OSCAT: the user model, the user's machine ~[~del and the dialogue model. The user model together with appropriate ~:~tricu is needed to allow the choice of correct level of responses to be generated by OSCAT in order to guide the teaching process; the machine r,~del allows enforcement of pragmatic constraints; the dialogue nDdel keeps focus and context of discourse.	expert system;natural language user interface;operating system;parsing;wilhelm pape	Alex Bykat	1986		10.1145/324634.325353	software engineering;software documentation;software system	AI	-34.71410729248714	-9.088365244294815	55672
abcf4148fe996d77fe0342d08cda27d4bbf176cc	extreme data mining	numerical method;data mining;data center;machine learning;data mining algorithm;high throughput;sigmod invited talk	At Google, the quality and speed of statistical data mining algorithms directly affects the usefulness of our search results and the relevance of our targeted advertising. One of the things that makes planet-wide, high throughput, 24/7 data mining so interesting is that all parts of the software stack are involved. This talk will walk up the stack, from the physical machines in warehouse-sized data centers, through networking and secondary storage abstractions to the distributed numerical methods and high throughput training and serving algorithms needed to support online logs processing and machine learning. We will also discuss the significant infrastructure and algorithmic impacts of batch versus online training: both data mining modes have essential roles in Google.	algorithm;auxiliary memory;computer data storage;data center;data mining;machine learning;numerical method;relevance;throughput	Sridhar Ramaswamy	2008		10.1145/1376616.1376617	high-throughput screening;data center;text mining;numerical analysis;computer science;data science;data mining;database;data stream mining	ML	-38.05389156580166	-7.634685551971831	55792
fb2a78693562075a5c0a94d155f9f7ccc417d608	political aspects of spatial data infrastructures	administracion electronica;geographic information;real time;political aspect;administration publique;spatial database;aspecto politico;spatial data infrastructure;base donnee spatiale;administration electronique;electronic government;spatial data structures;civil service;base dato especial;administracion publica;aspect politique;structure donnee spatiale	The geographic information is the information that describes phenomena associated directly or indirectly with a location with respect to the Earth surface. Nowadays, there are available large amounts of geographic data that have been gathered with different purposes by different institutions and companies. Furthermore, the volume of this information grows day by day thanks to important technology advances in high-resolution satellite remote sensors, GPS, databases and geoprocessing software notwithstanding an increasing interest by individuals and institutions. Even more, it is possible to georeference complex collections of a broad range of resource types, including textual documents, real-time acquired observations, legacy databases of tabular historical records, multimedia components such as audio and video, and scientific algorithms. Additionally, in most cases, data that are collected for a particular project are useful for other projects. This fact is even more pertinent with the recent ”commoditisation” of data and information. The costs involved with data collection are taken into account in project planning, along with attempting to maximize the use of the data from a project. Furthermore, it should be also realized that some data required for particular decisions are transient and may no longer be able to be collected when required. An example of this occurs when decisions concerning agricultural practices must be made. These decisions will often require environmental data spanning over several years. This data must be collected when they are available, even if the need for them is not present at the time of collection, otherwise it is not possible to collect the data for past years when they are later needed. Thus there is a need to store this type of data in databases and make them accessible to others. These databases become a shared resource, which must be maintained continuously. Furthermore, one might be interested in the interoperation of those resources, which are maintained at the state or national level, and sometimes by private corporations. In such cases, coordinating authorities are needed to assign custodianship and usage privileges for subsets of the data to different users (which may be agencies). Users in the general community are then able to expect the data to be available, and with	algorithm;database;file spanning;geoprocessing;global positioning system;image resolution;interoperation;real-time locating system;relevance;sensor;table (information)	F. Javier Zarazaga-Soria;Javier Nogueras-Iso;Rubén Béjar;Pedro R. Muro-Medrano	2004		10.1007/978-3-540-30078-6_65	spatial data infrastructure;computer science;operations research;spatial database	DB	-41.05000029623149	-3.307391493983145	56149
4b0fa329f61e30ec5a686c9355d96ec49d781d31	estimation of delay using sensor data for reporting through business intelligence		This study proposes a simple method to estimate delay using sensor data with the final objective of processing and reporting the information through Business Intelligence. The method involves three main tasks: determination of the Peak Period, definition of seasons used by FAST, and the calculation of delay. A small portion of the Las Vegas Roadway network is used to illustrate results. Functional requirements for Business Intelligence are proposed.	delay calculation;las vegas algorithm;requirement	Victor Molano;Alexander Paz	2016		10.5220/0005865200990103	computer science;data science;data mining;database	Robotics	-39.77052692546508	-5.473855337389784	56241
2c7117a79e54c97eb8c3af5eb80d04328e4b4d98	formal specification of image schemata -- a steptowards interoperability in geographic informationsystems	formal specification;image schemata;spatial cognition;geographic information system;potential difference;spatial relations;formal specifications;data exchange;large scale;spatial relation	The formal specification of spatial objects and spatial relations is at the core of geographic data exchange and interoperabilit y for geographic information systems (GIS). It is necessary that the representation of such objects and relations comes close to how people use them in their everyday lives, i.e., that these specifications are built upon elements of human spatial cognition. Image schemata have been suggested as highly abstract and structured mental patterns to capture spatial and similar physical as well as metaphorical relations between objects in the experiential world. We assume that image-schematic details for large-scale (geographic) space are potentially different from image-schematic details for small -scale (table-top) space. This paper reviews methods for the formal description of spatial relations, integrates them in a categorical view, and applies the methods arrived at to formally specify image schemata for large-scale (LOCATION, PATH, REGION, and BOUNDARY) as well as small -scale (CONTAINER, SURFACE, and LINK) space. These specifications should provide a foundation for further research on formalizing elements of human spatial cognition for interoperabilit y in GIS.	cognition;formal specification;geographic information system;interoperability;schematic	Andrew U. Frank;Martin Raubal	1998	Spatial Cognition & Computation	10.1023/A:1010004718073	spatial relation;computer science;theoretical computer science;data mining;formal specification;database;mathematics;geographic information system;remote sensing	DB	-36.2847850512485	-2.7113023208921567	56280
713ec657e33fb336432500d4e8f53208b5694f14	recent applications of web semantics in elifescience		Originating as heavily descriptive sciences, biology and biomedicine have a long tradition of specialized terminologies, classification systems, and controlled vocabularies. Therefore, we should not be surprised at the enthusiasm with which bioand bio-medical informaticians have developed and made use of semantic technologies (e.g. see [1, 2]). Ontologies have a natural use in the domain. The National Center for Biomedical Ontology (http://bioportal.bioontology.org) now provides access to over 400 ontologies with over 6 million classes [3, 4].	british informatics olympiad;controlled vocabulary;national center for biomedical ontology;ontology (information science)	Timothy W Clark;Marco Roos	2014	J. Web Sem.	10.1016/j.websem.2014.11.005	web application security;web modeling;web accessibility initiative;web standards;computer science;data science;semantic web;social semantic web;data mining;web intelligence;web engineering	Web+IR	-41.5748419718546	1.8534789789229544	56465
205b08247c5814fc33e005a6a07edb5b2405125c	location intelligence : a decision support system for business site selection		............................................................................................................................................ 3 ACKNOWLEDGEMENTS ......................................................................................................................... 4 TABLE OF CONTENTS ............................................................................................................................. 5 INDEX OF TABLES .................................................................................................................................10 INDEX OF FIGURES ...............................................................................................................................11 GLOSSARY ............................................................................................................................................13 ABBREVIATIONS ...................................................................................................................................15	decision support system;glossary;index (publishing)	Patrick Weber	2011				DB	-41.722043857714844	-6.857909503437151	56552
040b85e83a491ecbc5242ef48f1d136f4957c604	ambientdb: complex query processing for p2p networks	data management	This paper describes the project AmbientDB done at CWI which is the focus of my PhD track. The goal of the AmbientDB project is to provide a data management platform for multiple devices interconnecting with each other in ad-hoc ways. The platform is intended to be a crucial building block to Ambient Intelligent (AmI) applications. In this paper I will give an overview of AmbientDB project and present my PhD work associated with it in terms of what progress has been made so far and directions for further research.	experiment;hoc (programming language);peer-to-peer;prototype;relational database	Caspar Treijtel	2003			database;web search query;query expansion;web query classification;data mining;rdf query language;query language;sargable;query optimization;query by example;computer science	DB	-34.0690457415047	2.4132863044492967	56716
ecd7f6fadb383def01a13f614562f23de840ff09	an information system for real-time online interactive applications	mysql;real time;online interaction;relational database;real time online interactive applications;relational databases;information system;information system design	The edutain@grid European project [1] is developing a support platform for deployment, management and execution of Real-Time Online Interactive Applications (ROIA) on Grid. In this paper we present an information system designed by the edutain@grid project which provides support for ROIA deployment and monitoring, and offers a generic frontend for ROIA-specific optimisations. We conduct a variety of experiments that justify various decisions of our design, and investigate the performance and scalability of our system with respect to various types of queries.	information system;interactivity;real-time computing;real-time transcription	Vlad Nae;Herbert Jordan;Radu Prodan;Thomas Fahringer	2008		10.1007/978-3-642-00955-6_41	relational database;computer science;operating system;data mining;database;distributed computing;world wide web	HCI	-34.73258612508788	-1.2399202882444191	56928
b778ce0d492d9586bdcac2f4447f2b69983c2956	social information retrieval with agents		With the widespread adoption of online social networks as a crucial means for communication, social information retrieval is becoming one of the most interesting areas of research in terms of the large number of–theoretical and practical–issues that it encompasses. We argue that agent technology is central in supporting the decentralization of next generation online social networks and the synergistic pairing of agents and social networks is evident, if nothing else, because members of a social network interact as agents do in a multiagent system. In this paper we investigate the possibilities that agent technology can offer to social information retrieval and we emphasize the role that agents and multi-agent systems can play by presenting Blogracy, an agent-based online social network	agent-based model;information retrieval;multi-agent system;next-generation network;social network;synergy	Federico Bergenti;Agostino Poggi;Michele Tomaiuolo	2014			human–computer information retrieval;adversarial information retrieval;relevance (information retrieval);information retrieval;cognitive models of information retrieval;social network;computer science	AI	-46.96986792013589	2.332146105509401	56958
8e4a662703b0cd2dc0e82754ead06690a493f228	argos: an ontology and web service composition infrastructure for goods movement analysis	economic analysis;urban planning;data integrity;data processing;web service;web service composition;data analysis;social science;movement analysis;data access;cost effectiveness;economic development;los angeles	Many scientific problems can be modeled as computational workflows that integrate data from heterogeneous sources and process such data to derive new results. These data analysis problems are pervasive in the physical and social sciences, as well as in government practice. Therefore, techniques that facilitate the creation of such computational workflows are of critical importance. In our work on the Argos project we are developing techniques to automatically create computational workflows in response to user data requests. As a unifying paradigm, we represent both data access and data processing operations as web services. We focus on a domain representative of many economic analysis problems: good movement analysis in a large metropolitan area. This domain is of great interest to both social scientists and government practitioners, since understanding the patterns of freight flow is crucial for urban planning and forecasting the economic development of a region. Previous approaches to this type of analysis, mainly surveys, are prohibitively expensive. In Argos we propose to integrate data from secondary sources to estimate the flow of commodities in the Los Angeles metropolitan area, following Gordon and Pan [2001]. The Argos approach presents several advantages. First, it is significantly more cost-effective since the problem is now reduced to automated data integration and processing, instead of labor-intensive surveys. Second, many more questions can be posed and answered, since different data sources and data operations can be composed to derive a variety of novel data for the given domain. Finally, our system produces results based on the latest data, which is obtained live from the data sources whenever possible. In this system demo, we will showcase our initial design for automatic composition of web services for goods movement analysis. Our approach consists of three steps: 1. Define an ontology of the domain to model data processed throughout a workflow. 2. Describe data sources and operations using this ontology. 3. Automatically compose a workflow in response to a user request based on mediator techniques. First, we have developed an ontology of the goods movement domain to describe the data provided by the sources and the data participating as input and output of the processing operations (see Figure 1). We consider each data item as a measurement that has values along a set of dimensions, such as geographic area (e.g., Long Beach), type of flow (e.g., imports), type of product (e.g., vehicles and parts), time interval (e.g., January 2001), value and unit (e.g., 1108 thousand short tons). Some of the values of the dimensions have a hierarchical structure, either type or part-of hierarchies. For example, California is a geographical area that is part of the US. The part-of hierarchies are used for value aggregation. For example, we could approximate a measurement for all of the US if we have data for its states and territories. To facilitate the knowledge acquisition task we used the Protégé ontology editor [Noy et al., 2001] (see Figure 1). As a common syntax for the ontology and the data we adopt the Resource Description Framework (RDF) [Lassila & Swick, 1999]. RDF represents labeled graphs (i.e., semantic networks). RDF also includes edge labels with predefined class/subclass semantics. We considered more	approximation algorithm;attribute–value pair;box counting;data access;data item;geographic coordinate system;input/output;knowledge acquisition;matchware mediator;ontology (information science);ora lassila;pervasive informatics;programming paradigm;protégé;resource description framework;secondary source;semantic network;tagged union;web service	José Luis Ambite;Genevieve Giuliano;Peter Gordon;Stefan Decker;Andreas Harth;Karanbir Jassar;Qisheng Pan;LanLan Wang	2004			computer science;data mining;database;world wide web	DB	-37.656667414863726	1.4624047724355946	56965
2106ee3318b14c8c6e7424f2bdef3dde52e86e3c	objectglobe: open distributed query processing services on the internet		In this paper, we provide a brief overview of the Propel Distributed Services Platform, the infrastructure component of the recently announced Propel Commerce System. The Propel Distributed Services Platform is Internet infrastructure software that provides an integrated set of core services for developers of mission-critical, datacentric, e-business applications. It is designed to provide the foundation for future as well as current Propel product offerings. We list the current problems and key requirements that have driven the design of the Propel Platform and then describe the resulting system architecture and highlight its feature set.	electronic business;internet;mission critical;propel;requirement;systems architecture	Reinhard Braumandl;Markus Keidl;Alfons Kemper;Donald Kossmann;Stefan Seltzsam;Konrad Stocker	2001	IEEE Data Eng. Bull.		web search query;world wide web;database;data mining;web query classification;query expansion;the internet;query language;computer science;distributed database;view;query optimization	DB	-48.06177295412687	3.0130338743666503	57357
d28a90744f89ab29cb903edd9f5ffc0a110b6eab	object reuse and exchange for publishing and sharing workflows	collaboration;open archive initiative;workflow metadata;qa75 electronic computers computer science;escience workflows;information exchange;reproducibility of results;web 2 0;object reuse and exchange;workflow sharing;use case	The workflow paradigm can provide the means to describe the complete functional pipeline for a scientific experiment and therefore expose the underlying scientific processes for enabling the reproducibility of results. However, current means for exposing such information are tied closely to the individual workflow engines and there is no existing method that provides a common way to share this information.  In this paper, we discuss a lightweight approach that can be used to expose such information, using the Open Archives Initiative Object Reuse and Exchange (ORE) standard, to provide a common format for representing and sharing workflows and their associated metadata required for their execution.  We describe how workflows can be mapped to the ORE format using RDF and how they can be stored using bundles for sharing with others. We discuss tooling we have developed that provides a mechanism for existing workflow engines to conveniently export workflows as ORE bundles. We present three use cases for Triana, ASKALON and MOTEUR, where such integration has already been undertaken, and conclude the paper by providing a short study showing that the overhead implications of adopting the proposed ORE bundling format are minimal.	experiment;ore algebra;overhead (computing);product bundling;programming paradigm;workflow engine	Andrew Harrison;Ian Harvey;Andrew Jones;David H. Rogers;Ian J. Taylor	2011		10.1145/2110497.2110506	use case;information exchange;computer science;operating system;data mining;database;distributed computing;management;web 2.0;world wide web;collaboration;workflow technology	DB	-42.89294243370201	3.6739864813718808	57462
a1412162efc8ae1b96ff6d589e83c9bfc3ac0821	data mining aspects of a dam monitoring project	agent based;computer model;data mining;open source	When creating computational models of engineering structures, it can be very helpful to have insights into the behavior and composition of the physical system. Such insights can be gleaned using data mining techniques, where often unexpected relationships between physical quantities can be made evident. In the context of an agent-based research project to monitor dams, a number of sensors supply a large amount of data over an extended period of time. This data can naturally be used for data mining purposes to discover new and interesting aspects about the engineering structure. In this paper, an open source data mining tool (Weka) is briefly introduced and presented to show how data mining techniques can be applied in the handling of engineering tasks.	agent-based model;computation;computational model;data mining;open-source software;sensor;simulation;source data;weka	Karlheinz Lehner;Ingo Mittrup;Dietrich Hartmann	2005		10.1007/3-540-32391-0_79	engineering;data science;data mining;database	ML	-34.186432493142306	-6.505907638121457	57673
8a2041e7c6a28fde0b773a3010ee4c58cacc5572	implementing interactive analysis of attack graphs using relational databases	interaction analysis;relational database;attack graph	Lingyu Wang a,∗, Chao Yao b, Anoop Singhal c and Sushil Jajodia d a Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, H3G 1M8, Canada E-mail: wang@ciise.concordia.ca b Center for Secure Information Systems, George Mason University, Fairfax, VA 22030-4444, USA E-mail: cyao@gmu.edu c Computer Security Division, National Institute of Standards and Technology, Gaithersburg, MD 20899, USA E-mail: anoop.singhal@nist.gov d Center for Secure Information Systems, George Mason University, Fairfax, VA 22030-4444, USA E-mail: jajodia@gmu.edu	chao (sonic);computer security;decision support system;information security;intrusion detection system;mason;mail (macos);mathematical optimization;relational database;relational model;systems engineering;yao graph	Lingyu Wang;Chao Yao;Anoop Singhal;Sushil Jajodia	2008	Journal of Computer Security	10.3233/JCS-2008-0327	statistical relational learning;relational database;computer science;theoretical computer science;data mining;database;conjunctive query;computer security	Security	-44.25457011279446	-7.942875540500731	57688
d8ef95812b802271af7a116a532189683e035660	combining geographical information and traditional plots: the checkerplot	geographical information;thematic maps;geographic information;health insurance;health system;optimization problem;loss function;linear programming;linear program;member states;statistical graphics;r;health care	The checkerplot is implemented in the R-package sparkTable Kowarik, Meindl, and Templ, 2012.  sparkTable: sparklines and graphical tables for tex and html  [online]. R package version 0.9.3 [Accessed 7 February 2012] and can be freely downloaded from the CRAN repository. It is implemented in a flexible manner and not restricted to the examples given in this contribution.		Matthias Templ;Beat Hulliger;Alexander Kowarik;Karin Fürst	2013	International Journal of Geographical Information Science	10.1080/13658816.2012.684386	optimization problem;geography;computer science;linear programming;data science;machine learning;data mining;database;mathematics;thematic map;cartography;health care;statistics;loss function	DB	-42.06120981651192	-9.271313808252687	57851
0d69ad95c0f7cd9a318f9fa0de77834ecf4680d4	data mining for the internet of things: literature review and challenges	期刊论文	The massive data generated by the Internet of Things (IoT) are considered of high business value, and data mining algorithms can be applied to IoT to extract hidden information from data. In this paper, we give a systematic way to review data mining in knowledge view, technique view and application view, including classification, clustering, association analysis, time series analysis, outlier analysis, etc. And the latest application cases are also surveyed. As more and more devices connected to IoT, large volume of data should be analyzed, the latest algorithms should be modified to apply to big data also reviewed, challenges and open research issues are discussed. At last a suggested big data mining system is proposed.	algorithm;big data;cluster analysis;data mining;internet of things;open research;time series	Feng Chen;Pan Deng;Jiafu Wan;Daqiang Zhang;Athanasios V. Vasilakos;Xiaohui Rong	2015	IJDSN	10.1155/2015/431047	computer science;data science;data mining;data stream mining;world wide web	ML	-37.40743843348542	-6.416443547103109	58037
adc697b809e87ba4530fe03971f84333d4034255	personal data analytics for well-being oriented life support: experiment and feasibility study	personal data;data analytics;well being oriented life support			Seiji Kasuya;Xiaokang Zhou;Shoji Nishimura;Qun Jin	2016		10.3233/978-1-61499-637-8-172	analytics;data science;data mining;database;software analytics	HCI	-38.971443234820875	-7.130973547102125	58107
2fc491dbaad43fa5877b0e8540a30b812b9bdf89	heterodimensional cycle bifurcation with orbit-flip	local moving frame;heterodimensional cycle;orbit flip;heteroclinic orbit;periodic orbit;homoclinic orbit	QIUYING LU†,¶, ZHIQIN QIAO‡, TIANSI ZHANG§ and DEMING ZHU‡ †Department of Mathematics, Zhejiang Sci-Tech University, Xiasha Economic Development Area Hangzhou City, Zhejiang Province 310018, P. R. China ‡Department of Mathematics, East China Normal University, Shanghai 200241, P. R. China §College of Science, University of Shanghai for Science and Technology, Shanghai 200093, P. R. China ¶qiuyinglu@163.com	bifurcation theory;flip-flop (electronics)	Qiuying Lu;Zhiqin Qiao;Tiansi Zhang;Deming Zhu	2010	I. J. Bifurcation and Chaos	10.1142/S0218127410025569	classical mechanics;blue sky catastrophe;homoclinic orbit;topology;homoclinic bifurcation;mathematics;geometry;heteroclinic bifurcation;physics;heteroclinic orbit	AI	-46.13578018153152	-8.487067484178679	58122
7218ccafefed92e9549589658adcf9b75a326fdf	a spatio-temporal graph model for marine dune dynamics analysis and representation		Defining a model for the representation and the analysis of spatio-temporal dynamics remains an open domain in geographical information sciences. In this article we investigate a spatio-temporal graph-based model dedicated to managing and extracting sets of geographical entities related in space and time. The approach is based on spatial and temporal local relations between neighboring entities during consecutive times. The model allows us to extract sets of connected entities distant in time and space over long periods and large spaces. From GIS concepts and qualitative reasoning on space and time, we combine the graph model with a dedicated spatial database. It includes information on geometry and geomorphometric parameters, and on spatial and temporal relations. This allows us to extend classical measurements of spatial parameters, with comparisons of entities linked by complex relations in space and time. As a case study, we show how the model suggests an efficient representation of dunes dynamics on a nautical chart for safe navigation.	entity;geographic information system;information science;john d. wiley;mathematical morphology;nautical chart;row (database);spatial analysis;spatial database;von neumann neighborhood	Rémy Thibaud;Géraldine Del Mondo;Thierry Garlan;Ariane Mascret;Christophe Carpentier	2013	Trans. GIS	10.1111/tgis.12006	geography;artificial intelligence;theoretical computer science;data mining;database;mathematics;cartography	ML	-36.08738124599869	-3.007509545622943	58257
39b55344e9283f4e478d6c0174b6e7a504d94f9f	linkeddatalens: linked data as a network of networks	web of data;linked data;network analysis;workflow system;linked open data;knowledge capture;knowledge base	"""With billions of assertions and counting, the Web of Data represents the largest multi-contributor interlinked knowledge base that ever existed. We present a novel framework for analyzing and using the Web of Data based on extracting and analyzing thematic subsets of it. We view the Web of Data as a """"network of networks"""" from which to extract meaningful subsets that can be converted them into self-contained networks to be further analyzed and reused. These extracted networks can then be analyzed through network analysis and discovery algorithms, and the results of these analyses can be published back on the Web of Data. We describe LinkedDataLens, an implementation of this framework that uses the Wings workflow system to represent multi-step network extraction and analysis processes."""	algorithm;data web;knowledge base;linked data;network theory;semantic web;world wide web	Yolanda Gil;Paul T. Groth	2011		10.1145/1999676.1999720	knowledge base;web modeling;data web;computer science;artificial intelligence;linked data;data mining;database;world wide web	Web+IR	-39.13012535115412	3.370780792874698	58576
36a06db3f1d311149066a97cf79670de378377f8	future communication services: application enablement and the role of software agent architectures	whole area;wiley periodicals;major role;multi-agent system;application enablement;software agent architecture;communication services research;novel flexible network architecture;current research;future communication service;ongoing work;communication service	The emerging area of Application Enablement addresses challenging issues by trying to identify novel flexible network architectures and by conceiving novel conceptual and practical tools for the design, development, and execution of such applications. The aim here is to outline the possible relationship between Application Enablement in communication services research and ongoing work in the area of multi-agent systems. In particular, this letter shows that current research into software agents and multi-agent systems can effectively extend the dichotomy between self-adaptive and self-organizing approaches and potentially play a major role in inspiring and advancing the whole area of Application Enablement in communication services.	multi-agent system;organizing (structure);self-organization;software agent	Abdulsalam Yassine	2011	Bell Labs Technical Journal	10.1002/bltj.20516	systems engineering;engineering;knowledge management;management science	AI	-47.59283618393797	2.335798713658751	58616
2aaebacf6bd77fd284f9c70aa9b5816909fd5f08	sydney owl syntax - towards a controlled natural language syntax for owl 1.1	semantic web;english language	This paper describes a proposed new syntax that can be used to write and read OWL ontologies in Controlled Natural Language (CNL): a well-defined subset of the English language. Following the lead of Manchester OWL Syntax in making OWL more accessible for non-logicians, and building on the previous success of Schwitter’s PENG (Processable English), the proposed Sydney OWL Syntax enables two-way translation and generation of grammatically correct full English sentences to and from OWL 1.1 functional syntax. Used in conjunction with OWL tools, it is designed to facilitate ontology construction and editing by enabling authors to write an OWL ontology in a defined subset of English. It also improves readability and understanding of OWL statements or whole ontologies, by enabling them to be read as English sentences. It is hoped that by providing the option of an intuitive, easy to use English syntax which requires no specialized knowledge, the broader community will be far more likely to develop and benefit from Semantic Web applications. This paper is a discussion paper covering the scope, design, and examples of Sydney OWL Syntax in use, and the authors invite feedback on all aspects of the proposal via email to krr.sydneysyntax@cse.unsw.edu.au. Working drafts of the full specification are available at http://www.ics.mq.edu.au/~rolfs/sos.	abox;affiliate (commerce);compute node linux;controlled natural language;email;field (computer science);green paper;language technology;ontology (information science);parsing;predictive text;semantic web;tbox;text editor;the australian;web ontology language;web application	Anne Cregan;Rolf Schwitter;Thomas Andreas Meyer	2007			owl-s;web ontology language;natural language programming;syntax;universal networking language;computer science;ontology (information science);natural language processing;semantic web rule language;programming language;controlled natural language;artificial intelligence	Web+IR	-41.15482126780972	4.152557349914188	58645
7d302d0f1e4aea26d2dbc381e76fa42241502e87	extracting conceptual relationships from specialized documents	document model;usability evaluation;user study;knowledge management;conceptual model;automatic extraction;modeling semi structured data;it value;semi structured data;scientific research;structured data	Conceptual modeling has been fundamental to the management of structured data. However, its value is increasingly being recognized for knowledge management in general. In trying to develop suitable conceptual models for unstructured information, issues such as the level of representation and complexity of processing techniques arise. Here, we investigate the use of a conceptual model that is simple enough to allow efficient automatic extraction from two kinds of documents--scientific research papers and patents. Our model focused on the problem-solution relationship that is central to the analysis of scientific papers, while allowing supporting relationships such as methods and claims. We evaluated the utility of the approach by building a prototype system and carrying out experiments that assessed the accuracy level of the techniques used in building the model and the acceptability of the model through preliminary user studies. The feedback from these experiments shows promising results that support our choice in the tradeoffs between the granularity of the model and the processing techniques used. We discuss a variety of issues that arouse from this project and describe several directions for future work.		Bowen Hui;Eric Kai-Hsiang Yu	2005	Data Knowl. Eng.	10.1016/j.datak.2004.10.001	conceptual model;semi-structured data;scientific method;data model;computer science;knowledge management;conceptual schema;conceptual model;data mining;database;management science	DB	-36.813658857457774	3.1201986464642775	58807
102ee76c561e3c8db1eac7a4d2776da5cb31ba3d	the state of databases and data warehouses in slovene organizations	data warehouse		database	Jurij Jaklic;Mojca Indihar Stemberger;Talib Damij;Janez Grad;Miro Gradisar;Andrej Kovacic;Gortan Resinovic;Tomaz Turk	1999			computer science;data science;data warehouse;data mining;database	DB	-39.75870693134264	-5.336042789959026	58901
b29b72246e58538214219894a799a51051deb688	a best of both worlds approach to complex, efficient, time series data delivery		Point time series are a key data-type for the description of real or modelled environmental phenomena. Delivering this data in useful ways can be challenging when the data volume is large, when computational work (such as aggregation, subsetting, or re-sampling) needs to be performed, or when com- plex metadata is needed to place data in context for understanding. Some as- pects of these problems are especially relevant to the environmental domain: large sensor networks measuring continuous environmental phenomena sam- pling frequently over long periods of time generate very large datasets, and rich metadata is often required to understand the context of observations. Neverthe- less, timeseries data, and most of these challenges, are prevalent beyond the en- vironmental domain, for example in financial and industrial domains. A review of recent technologies illustrates an emerging trend toward high performance, lightweight, databases specialized for time series data. These da- tabases tend to have non-existent or minimalistic formal metadata capacities. In contrast, the environmental domain boasts standards such as the Sensor Obser- vation Service (SOS) that have mature and comprehensive metadata models but existing implementations have had problems with slow performance. In this paper we describe our hybrid approach to achieve efficient delivery of large time series datasets with complex metadata. We use three subsystems within a single system-of-systems: a proxy (Python), an efficient time series da- tabase (InfluxDB) and a SOS implementation (52 North SOS). Together these present a regular SOS interface. The proxy processes standard SOS queries and issues them to the either 52 North SOS or to InfluxDB for processing. Res- ponses are returned directly from 52 North SOS or indirectly from InfluxDB via Python proxy where they are processed into WaterML. This enables the scala- bility and performance advantages of the time series database to be married with the sophisticated metadata handling of SOS. Testing indicates that a recent version of 52 North SOS configured with a Postgres/PostGIS database performs well but an implementation incorporating InfluxDB and 52 North SOS in a hy- brid architecture performs approximately 12 times faster.	time series	Benjamin Leighton;Simon J. D. Cox;Nicholas J. Car;Matthew P. Stenson;Jamie Vleeshouwer;Jonathan Hodge	2015		10.1007/978-3-319-15994-2_37	computer science;engineering;operations management;software engineering;data mining;database;world wide web	DB	-42.361588923991995	-2.2711044305698826	58979
7ca88c45d0ea6905a609416ee15d4ffc6d6586ac	scientific and statistical database management	artificial intelligent;information system;database management	Some science domains have the advantage that the bulk of the data comes from a single source instrument, such as a telescope or particle collider. More commonly, big data implies a big variety of data sources. For example, the Center for Coastal Margin Observation and Prediction (CMOP) has multiple kinds of sensors (salinity, temperature, pH, dissolved oxygen, chlorophyll A & B) on diverse platforms (fixed station, buoy, ship, underwater robot) coming in at different rates over various spatial scales and provided at several quality levels (raw, preliminary, curated). In addition, there are physical samples analyzed in the lab for biochemical and genetic properties, and simulation models for estuaries and near-ocean fluid dynamics and biogeochemical processes. Few people know the entire range of data holdings, much less their structures and how to access them. We present a variety of approaches CMOP has followed to help operational, science and resource managers locate, view and analyze data, including the Data Explorer, Data Near Here, and topical “watch pages.” From these examples, and user experiences with them, we draw lessons about supporting users of collaborative “science observatories” and remaining challenges.	big data;biogeochemistry;experience;large hadron collider;sensor;simulation;spatial scale;statistical database	Josef Kittler	2012		10.1007/978-3-642-31235-9	database theory;database server;intelligent database;computer science;theoretical computer science;personal information management;data mining;data retrieval;information retrieval;information system;database testing;database design	ML	-39.52210906821686	-2.9519510589875657	59253
2b74d96467e6306b6e01a0602a10aa8b3d3c1205	content-based ontology matching for gis datasets	geographic information system;schema matching;artificial intelligent;semantic heterogeneity;geographic information systems;mutual information;ontology;dataset;ontology alignment;ontology matching	The alignment of separate ontologies by matching related concepts continues to attract great attention within the database and artificial intelligence communities, especially since semantic heterogeneity across data sources remains a widespread and relevant problem. In particular, the Geographic Information System (GIS) domain presents unique forms of semantic heterogeneity that require a variety of matching approaches.  Our approach considers content-based techniques for aligning GIS ontologies. We examine the associated instance data of the compared concepts and apply a content-matching strategy to measure similarity based on value types based on N-grams present in the data. We focus special attention on a method applying the concepts of mutual information and N-grams by developing 2 separate variations and testing them over GIS dataset including multi-jurisdictions. In order to align concepts, first we find the appropriate columns. For this, we will exploit mutual information between two columns based on the type distribution of their content. Intuitively, if two columns are semantically same, type distribution should be very similar. We justify the conceptual validity of our ontology alignment technique with a series of experimental results that demonstrate the efficacy and utility of our algorithms on a wide-variety of authentic GIS data.	algorithm;align (company);artificial intelligence;column (database);field (computer science);geographic information system;grams;mutual information;ontology (information science);ontology alignment;semantic heterogeneity	Jeffrey Partyka;Neda Alipanah;Latifur Khan;Bhavani M. Thuraisingham;Shashi Shekhar	2008		10.1145/1463434.1463496	upper ontology;enterprise gis;ontology alignment;computer science;ontology;data mining;database;mathematics;geographic information system;ontology-based data integration;information retrieval;statistics;remote sensing	AI	-38.575315339363364	3.6640842736222337	59304
b98ea7677158e2f9063984a6bffb65ab0ea80124	developing sustainable data services in cyberinfrastructure for higher education: requirements and lessons learned	network attached storage;sustainable e science infrastructure sustainable data services higher education university of california san diego research cyberinfrastructure program uc san diego rci long term quality services centralized storage rcids team rci data services organized research units orus human genomic sequences marine natural products cosmological simulations network attached storage nas sustainable business model cloud computing technology cloud based data services;educational institutions memory interviews business hardware biology computational modeling;data cloud;computer aided instruction;further education cloud computing computer aided instruction;higher education;biology;computational modeling;business;interviews;research cyberinfrastructure;network attached storage higher education research cyberinfrastructure data cloud sustainable data services;further education;memory;sustainable data services;cloud computing;hardware	The University of California, San Diego (UC San Diego) Research Cyber infrastructure (RCI) program provides long-term quality services in centralized storage, colocation, computing, data curation, networking and technical expertise. To help define the data storage needs and set priorities, the RCI data services (RCIDS) team conducted a series of interviews with faculty and senior staff members between September 2012 and February 2013. A total of 50 groups from 29 separate departments and organized research units (ORUs) participated in the interviews, representing more than 600 UC San Diego researchers. From human genomic sequences, marine natural products, to cosmological simulations, their diverse datasets are shared with hundreds of thousands of users worldwide. The top 10 requirements on data services and the top 5 existing challenges and risks as reported by UC San Diego researchers have been identified. Based upon these requirements, the RCIDS team recommends a Network Attached Storage (NAS) data service to be first deployed with a sustainable business model. Additional services will be developed through further discussion with the research community and in view of emerging cloud computing technologies. An extensive discussion is provided on the implementation plan, cloud-based data services, and the lessons learned in building sustainable e-science infrastructure for higher education research.	centralized computing;cloud computing;colocation centre;computer data storage;cyberinfrastructure;data curation;digital curation;e-science;network-attached storage;requirement;simulation;uc browser	Wilfred W. Li;Richard Lee Moore;Matthew Kullberg;Brian Battistuz;Steve Meier;Ronald Joyce;Richard P. Wagner;Tad Reynales;Qian Liu	2013	2013 IEEE 9th International Conference on e-Science	10.1109/eScience.2013.46	simulation;engineering;data science;services computing;world wide web	Visualization	-44.77467373904499	-2.6182452535848912	59505
6dd4171e350db50e1340956afc31160db1e9aa85	big data fusion in internet of things			big data;internet of things	Zheng Yan;Jun Liu;Laurence Tianruo Yang;Nitesh Chawla	2018	Information Fusion	10.1016/j.inffus.2017.04.005	internet privacy;world wide web	Robotics	-39.56793114031451	-6.711084515367778	59845
87c320a2781df605cc578e69f5620ed87fb5e035	spatial data storage model based on map calculus for online map generalization	spatial data;semi structured spatial data;online automatic map generalization;relational database;map calculus;spatial database;data model;online map indefinite scales automatic generalization spatial data storage model webgis map calculus storage model geographic cognitive model;redundancy;geographic information systems;web sites cartography geographic information systems;calculus;spatial databases;web sites;spatial databases data models relational databases calculus redundancy adaptation models;semi structured spatial data online automatic map generalization map calculus;cartography;relational databases;adaptation models;data models	Online map generalization has appeared as a vast and intricate research domain with growth of WEBGIS. The aim of this paper is to propose a “map calculus” storage model of spatial data for online automatic map generalization which is to change the traditional online automatic map generalization storage model of certain scales LOD on the basis of geographic cognitive model of domain. This model grounds on single large scale spatial data to accomplish the indefinite scales zoom of online maps which is based on map calculus and spatial nesting relations with utilizing the nesting character of semi-structured spatial data and provides a new way to explore the further implementation of online map indefinite scales automatic generalization. A simplified case study in the application domain of online cadastre involving is examined closely.	application domain;cartographic generalization;cognitive model;computer data storage;map;semiconductor industry;storage model;web mapping	Yang Jun	2011	Proceedings 2011 IEEE International Conference on Spatial Data Mining and Geographical Knowledge Services	10.1109/ICSDM.2011.5969032	relational database;computer science;machine learning;data mining;database;geographic information system;cartographic generalization	Robotics	-36.72454441817444	-1.843167779171726	60168
efb2f8fa673ef7c1234d277500eec3a1fc5467f8	visual data mining of graph based data		"""Acknowledgments This work has profited very much by suggestions from colleagues and by co-operations with other researchers. I especially wish to thank Professor Dr. H. Kleine Büning, my thesis advisor, for supporting me and my work. Furthermore , I thank Professor Dr. F. Meyer auf der Heide for evaluating this thesis. I owe Dr. Benno Stein many thanks for setting the standard for this work and for reminding me that there's no such thing as a free lunch. The discussions with Dr. Theo Lettmann, Sven Meyer zu Eissen, Andrè Schulz, and all my other colleagues and their constant support were very important to me during the last years. Michael Lappe from the Structural Genomics Group of the European Bioin-formatics Institute in Cambridge (UK) provided me with many interesting problems from the field of biology and also helped me to solve some of them. I also thank Jens Tölle from the University of Bonn (Germany) for answering all those questions about computer networks. Finally, I would like to express my deepest gratitude to my wife Petra. She made this work possible. The field of data visualization has reached a point of inflection. The rapidly growing amount of data does not anymore allow for a presentation of all given data items. Furthermore, the complexity of many datasets surpasses the user's ability to identify the gist or the underlying concepts. One solution to these problems is to analyze the given data and confront the user with an abstract view of the information. The following examples may help to illustrate the breadth and complexity of current datasets: """" For instance, Wal-Mart, the chain of over 2000 retail stores, every day uploads 20 million point-of-sale transactions to an AT&T massively parallel system with 483 processors running a centralized database. At corporate headquarters, they want to know trends down to the last Q-Tip. """" (in [63]) Visualizing traffic in computer networks is one of six applications described in this work. Currently several million IPs (computer addresses) are used in the internet (from [118]). Even if not all of these addresses have to be visualized at the same time, most realistic scenarios still comprise several thousand computers which interact in highly complex ways. SGI describes customers for its data visualization software package from the telecommunication market as follows: """" According to the Technology Research Institute, 26 percent of the carriers it interviewed had warehouses of 500GB or …"""	central processing unit;centralized computing;computer;data mart;data mining;data visualization;gist;point of sale;sven jaschan;tip (unix utility);upload;visualization software;write-ahead logging	Oliver Niggemann	2001				DB	-43.76543623629429	-4.216960560210398	60186
f5f07f6562307a1ef4d39bd203fffa48ff951900	integrated data analysis and management for the problem solving environment	data analysis;problem solving environment	Abstract   Large quantities of application oriented data (e.g. measurement data, planning data, etc.) have been accumulated or are still under development in science and industry. The detailed analysis and non-routine usage of such data collections by typical endusers (scientists, engineers, economists—with little or no programming skill and ambitions) has heavy demand for   1.   —a powerful interactive language   2.   —a flexible data base management system   3.   —easy embedding of application specific programs   4.   —facilities for graphical data presentation   5.   —online usage-information on programs and data   6.   —application oriented description of data and programs   #N# The Integrated Data Analysis and Management System (IDAMS) which combines several components to satisfy all the above requirements has been largely implemented in APL. IDAMS offers access to data (managed by a data base system) and to programs (from a library of APL and non APL programs) with a simple but powerful query language. The attributes of data and programs are specified in a dictionary and their meaning is described in an information network of the interactive user guidance component.  User guidance and adaptivity of the IDAMS interface to the user's skill are the key to easy and comfortable usage of IDAMS for problems solving by non DP professionals.	problem solving environment	R. Erbe;R. Hartwig;H. Lehmann;G. Mueller;Ulrich Schauer	1980	Inf. Syst.	10.1016/0306-4379(80)90074-5	computer science;artificial intelligence;theoretical computer science;operating system;machine learning;data mining;database;data analysis;programming language;computer security;statistics	DB	-34.3300394984786	-7.407075095023272	60370
e76c9a40adf1941741bd5cb22c9e8687a36da2f9	qos‐aware energy‐efficient resource allocation in ofdm‐based heterogenous cellular networks	energy efficiency;resource allocation;heterogenous networks;qos aware	1College of Electronic Science and Engineering, National University of Defense Technology, Changsha, China 2Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada 3College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China 4Science and Technology on Information Transmission and Dissemination in Communication Networks Laboratory, Shijiazhuang, China	automation;columbia (supercomputer);computer engineering;mechatronics;quality of service;telecommunications network	Li Zhou;Chunsheng Zhu;Rukhsana Ruby;Xiaofei Wang;Xiaoting Ji;Shan Wang;Jibo Wei	2017	Int. J. Communication Systems	10.1002/dac.2931	real-time computing;resource allocation;distributed computing;efficient energy use;computer network	DB	-45.1917316457502	-7.4166951886728825	60405
209bf967e15b99c6b3faf0dd044ec26b4ea453b0	beyond abstract data types: giving life to objects	programming language;abstract data type;software engineering;object oriented	The most recent buzzword in software engineering is object-orientation (OO), a trend encouraged (if not inspired) by the Ada programming language. We have all witnessed OO principles gain undisputed popularity in the computing society. Also, we have all experienced the challenges of the move from the platonic world of objects, where all objects are implicitly given their own flow of control, their own state, resources and behavior, to the real world of computing where none of these luxuries can be taken for granted. The main topic of this article is the search for an Ada design that can provide the illusion of the ideal world of objects at an acceptable price.	abstract data type	Drasko M. Sotirovski;Slobodan S. Jovanovic;Philippe Kruchten	1994		10.1007/3-540-58822-1_93	first-generation programming language;type safety;computer science;object;theoretical computer science;component-based software engineering;software development;object-oriented design;database;programming language;object-oriented programming;object-orientation;abstract data type	Logic	-47.63098400485719	-2.3734645281316227	60414
3111201aab8e9e2a0cbd86bd93339c6a9615b7e8	towards a marketplace for the scientific community: accessing knowledge from the computer science domain	informacion documentacion;grupo a;ciencias sociales	"""As scientific output is constantly growing, it is getting more and more important to keep track not only for researchers but also for other scientific stakeholders such as funding agencies or research companies. Each stakeholder values different types of information. A funding agency, for instance, might be rather interested in the number of publications funded by their grants. However, information extraction approaches tend to be rather researcher-centric indicated, for example, by the type of named entities to be recognized. In this paper we account for additional perspectives by proposing an ontological description of one scientific domain � the computer science domain. We accordingly annotated a set of 22 computer science papers by hand and make this data set publicly available. In addition, we started to apply methods to automatically extract instances and report preliminary results. Taking various stakeholders' interests into account as well as automating the mining process represent prerequisites for our vision of a """"Marketplace for the Scientific Community"""" where stakeholders can exchange not only information but also search concepts or annotated data."""	computer science	Mark Kröll;Stefan Klampfl;Roman Kern	2014	D-Lib Magazine	10.1045/november14-kroell	computer science;knowledge management;data mining;world wide web	Theory	-44.86490752330165	2.612372890375624	60720
a89740427489d2e22b039ce2e9f1b47ef9d87b9c	photogrammar: organizing visual culture through geography, text mining, and statistical analysis	selected works;bepress		organizing (structure);text mining	Lauren Tilton;Peter Leonard;Taylor Arnold	2014			library science;text mining;geography;data science;data mining	HCI	-41.072026532776334	-7.703860689961874	61140
782f627747507340de249f9e83727b5607c296c2	detecting drdos attacks by a simple response packet confirmation mechanism	distinguishing attack;securite;detection;safety;denial of service;robust method;distributed reflection dos;seguridad;response confirmation;denegacion de servicio;deni service	a Tohoku Institute of Technology, Yagiyama Kasumicho 35-1, Taihaku-ku, Sendai 982-8577, Japan b Cyber Solutions Inc., Minami Yoshinari 6-6-3, Aoba-ku, Sendai 989-3204, Japan c NTT DATA Corporation, Toyosu Center Buildings, 3-3, Toyosu 3-chome, Koto-ku, Tokyo 135-6033, Japan d Advanced Networking Laboratory, Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ 07102-1982, USA e Graduate School of Information Sciences, TOHOKU University, Aramaki Aza Aoba 6-6-05, Aoba-ku, Sendai 980-8579, Japan	computer engineering;denial-of-service attack;experiment;firewall (computing);ku band;network packet;next-generation firewall;norm (social);sensor;state management;stateful firewall;tracing (software)	Hiroshi Tsunoda;Kohei Ohta;Atsunori Yamamoto;Nirwan Ansari;Yuji Waizumi;Yoshiaki Nemoto	2008	Computer Communications	10.1016/j.comcom.2008.05.033	real-time computing;computer science;distributed computing;computer security;denial-of-service attack	Mobile	-44.413058151153656	-7.710118999496924	61180
21265b10a853040e3c1fe602e4b47e4dbe27bd57	optimized seamless integration of biomolecular data	biology computing;information resources;low cost query evaluation plans optimized seamless biomolecular data integration internet multiple remote heterogeneous data sources multiple local heterogeneous data sources visualization analysis scientific discovery digital library metadata decision support cost based knowledge semantic knowledge;scientific data;scientific discovery;bioinformatics costs data visualization software libraries data warehouses mediation educational institutions internet data mining visual databases;data visualisation molecular biophysics biology computing scientific information systems information resources meta data data analysis;data visualisation;data analysis;data extraction;molecular biophysics;meta data;technical report;scientific information systems	Today, scientific data is inevitably digitized, stored in a variety of heterogeneous formats, and is accessible over the Internet. Scientists need to access an integratedview of multiple remote or local heterogeneous data sources. They then integrate the results of complex queries and apply further analysis and visualization to support the task of scientific discovery. Building a digital library for scientific discovery requires accessing and manipulating data extracted from flat files or databases, documents retrieved from the Web, as well as data that is locally materialized in warehouses or is generated by software. We consider several tasks to provide optimized and seamless integration of biomolecular data. Challenges to be addressed include capturing and representing source capabilities; developing a methodology to acquire and represent metadata about source contents and access costs; and decision support to select sources and capabilities using cost based and semantic knowledge, and generating low cost query evaluation plans.	database;decision support system;digital library;internet;seamless3d;world wide web	Barbara A. Eckman;Zoé Lacroix;Louiqa Raschid	2001		10.1109/BIBE.2001.974408	information visualization;computer science;bioinformatics;technical report;data science;data warehouse;data mining;database;data analysis;metadata;data element;statistics;data;data mapping;molecular biophysics	DB	-37.83385099221476	1.8526338428709244	61235
63e3c568fa5f7fdacc9800b4c35b552bc6c32e37	enhancing dataset quality using keys		The Linked Data principles provide a decentral approach for publishing structured data in RDF on the Web. A consequence of this architectural choice is a high variance in the quality of the RDF datasets which constitute the Linked Data cloud. In this demo paper, we address a particular aspect of quality, i.e., the discriminability of resources. During our demo, we will present our simple three-step approach and interface, which allows data publishers to detect the resources in their dataset that are indistinguishable with respect to a given set of properties. Our approach is highly scalable as it relies on ROCKER, a novel algorithm for key discovery. Our evaluation on DBpedia suggests that even very commonly-used data sources are still in need to significant improvement to abide by the discriminability criterion.	algorithm;dbpedia;linked data;resource description framework;scalability;tag cloud;world wide web	Tommaso Soru;Edgard Marx;Axel-Cyrille Ngonga Ngomo	2015			data mining;linked data;scalability;cloud computing;rdf;data model;computer science;key (lock);publishing	AI	-36.19182332697408	4.035676273471897	61240
5208c132a10a6a993047cad99ff7b2ac49f0b525	committees and chairs	general;miscellaneous	Organizing Committee Members Amiram Yehudai, Tel Aviv University, Israel Orit Hazzan, Technion, Israel Orna Berry, Venture Partner, Gemini Israel Funds, Israel Oded Cohn, IBM Research Lab, Haifa, Israel Moshe Salem, Iltam, Israel Yossi Tsuria, NDS, Israel Michael Winokur, Israel Aircraft Industries, Israel Udi Ziv, SAP, Israel Gil Amisar, GE Healthcare, Israel Amos Maggor, Software Together, Israel	amos;ibm research;orna berry;salem	Oded Cohn	2005		10.1109/SWSTE.2005.10	distributed computing;polyamide;barium;aluminium;computer science;caproamide;mold;rigidity (psychology);composite material	Crypto	-46.40312336936977	-9.462466921521012	61258
6e1ec111a36221b741279a859f5b93e3c708c694	a lifecycle management solution to manage mississippi's data lake and big data analytics platform				Denise D. Krause	2015			data science;big data;data mining;engineering;application lifecycle management	DB	-39.16125557088295	-5.578790872389335	62043
1c7197e34af8248e8a3bc7e060b1cabd2dfdad3e	a framework for customizable generation of hypertext presentations	requirements summarization;powerful declarative language;portable cross-platform;different level;weather forecasting;object modeling;hypertext presentation generator;system description;customizable generation	In this paper, we present a framework, PRESENTOR, for the development and customization of hypertext presentation generators. PRESENTOR offers intuitive and powerful declarative languages specifying the presentation at different levels: macro-planning, micro-planning , realization, and formatting. PRESENTOR is implemented and is portable cross-platform and cross-domain. It has been used with success in several application domains including weather forecasting, object modeling, system description and requirements summarization. 1 I n t r o d u c t i o n Presenting information through text and hypertext has become a major area of research and development. Complex systems must often deal with a rapidly growing amount of information. In this context, there is a need for presentation techniques facilitating a rapid development and customization of the presentations according to particular standards or preferences. Typically, the overall task of generating a presentation is decomposed into several subtasks including: macro-planning or text planning (determining output content and structure), microplanning or sentence planning (determining abstract target language resources to express content, such as lexical items and syntactic constructions and aggregating the representations), realization (producing the text string) and formatting (determining the formatting marks to insert in the text string). Developing an application to present the information for a given domain is often a time-consuming operation requiring the implementation from scratch of domain communication knowledge (Kittredge et al., 1991) required for the different generation subtasks. In this technical note and demo we present a new presentation framework, PRESENTOR, whose main purpose is to facilitate the development of presentation applications. PRESENTOR has been used with success in different domains including object model description (Lavoie et al., 1997), weather forecasting (Kittredge and Lavoie, 1998) and system requirements summarization (Ehrhart et al., 1998; Barzilay et al., 1998). PRESENTOR has the following characteristics, which we believe are unique in this combination: • PRESENTOR modules are implemented in Java and C + + . It is therefore easily portable cross-platform. • PRESENTOR modules use declarative knowledge interpreted at run-time which can be customized by non-programmers without changing the modules. • PRESENTOR uses rich presentation plans (or exemplars) (Rambow et al., 1998) which can be used to specify the presentation at different levels of abstraction (rhetorical, conceptual, syntactic, and surface form) and which can be used for deep or shallow generation. In Section 2, we describe the overall architecture of PRESENTOR. In Section 3 to Section 6, we present the different specifications used to define domain communication knowledge and linguistic knowledge. Finally, in Section 7, we describe the outlook for PRESENTOR. 2 PRESENTOR A r c h i t e c t u r e The architecture of PRESENTOR illustrated in Figure 1 consists of a core generator with several associated knowledge bases. The core generator has a pipeline architecture which is similar to many existing systems (Reiter, 1994): an incoming request is received by the generator interface triggering sequentially the macroplanning, micro-planning, realization and fi-	compiler;complex systems;hypertext;java;knowledge base;microsoft outlook for mac;pipeline (computing);principle of abstraction;programmer;requirement;string (computer science);system requirements;word lists by frequency	Benoit Lavoie;Owen Rambow	1998			simulation;object model;weather forecasting;computer science;database;programming language	NLP	-37.29627309845672	2.49691136750013	62699
d7699bac182bda1bfa759dac748e6c2ad358779c	enabling wireless technologies for green pervasive computing	signal image and speech processing;information systems applications incl internet;pervasive computing;wireless technology;communications engineering networks	1 Department of Computer Science and Computer Engineering, La Trobe University, Melbourne, Victoria 3086, Australia 2 Department of Computer Science and Information Technology, University of the District of Columbia 4200, Connecticut Avenue, N.W. Washington, DC 20008, USA 3 School of Electrical and Information Engineering, University of Sydney, NSW 2006, Australia 4 Department of Computer Science and Engineering, University Texas at Arlington, P.O. Box 19015, Arlington, TX 76019, USA	columbia (supercomputer);computer engineering;computer science;information engineering;linear algebra;ubiquitous computing;victoria (3d figure)	Naveen K. Chilamkurti;Sherali Zeadally;Abbas Jamalipour;Sajal K. Das	2009	EURASIP J. Wireless Comm. and Networking	10.1155/2009/230912	context-aware pervasive systems;computer science;theoretical computer science;distributed computing;ubiquitous computing;computer network	DB	-45.06708464271889	-7.786206936166047	62723
de8b5db196f76146c43da9b4f952d36dd7ecf4c6	configuring of supply chain networks based on constraint satisfaction, genetic and fuzzy game theoretic approaches	constraint satisfaction;genetics;supply chain network	Alexander V. Smirnov, Leonid B. Sheremetov, Nikolai Chilov, Jose Romero Cortes St.Petersburg Institute for Informatics and Automation of the Russian Academy of Sciences SPIIRAS, 39, 14 Line, St.Petersburg, 199178, Russia e-mail: smir@.iias.spb.su 2 Computer Science Research Center of National Technical University (CIC-IPN), Av. Juan de Dios Batiz esq. Othon de Mendizabal s/n Col. Nueva Industrial Vallejo, México, D.F., C.P. 07738, Mexico e-mail: sher@cic.ipn.mx 3 Mexican Oil Institute, Av. Lazaro Cardenas, 152, Col.san Bartolo Atepehuacan, México, D.F., CP 07730 ,	academy;automation;computer science;constraint satisfaction;email;game theory;informatics;interplanet	Alexander V. Smirnov;Leonid Sheremetov;Nikolay Shilov;José C. Romero Cortés	2002		10.1007/978-0-387-35613-6_20	mathematical optimization;computer science;knowledge management;operations management	Theory	-44.065584133821346	-9.24438898353787	62743
c52fc66e651fef6d2c838e33a430300031b403c0	incomplete data and technological progress in energy storage technologies		Energy storage is an important topic as many countries are seeking to increase the amount of electricity generation from renewable sources. An open and accessible online database on energy storage technologies was created, incorporating a total of 18 energy storage technologies and 134 technology pages with a total of over 1,800 properties. In this database information on technical maturity, technology readiness level and forecasting is included for a number of technologies. However, since the data depends on various sources, it is far from complete and fairly unstructured. The chief challenge in managing unstructured data is understanding similarities between technologies. This in turn requires techniques for analyzing local structures in high dimensional data. This paper approaches the problem through the use and extension of t-stochastic neighborhood embedding (t-SNE). t-SNE embeds data that originally lies in a high dimensional space in a lower dimensional space, while preserving characteristic properties. In this paper, the authors extend the t-SNE technique with an expectation-maximization method to manage incompleteness in the data. Furthermore, the authors identify some technology frontiers and demonstrate and discuss design trade-offs and design voids in the progress of energy storage technologies. Conference Topic Mapping and visualization	big data;byte;capability maturity model;data point;database;entropy maximization;expectation–maximization algorithm;experiment;feature extraction;feature selection;nonlinear dimensionality reduction;principal component analysis;requirements analysis;t-distributed stochastic neighbor embedding;technometrics	Sertaç Oruç;Scott W. Cunningham;Christopher Davis;Bert van Dorp	2015			technological change;process engineering;energy storage;computer science	EDA	-38.01057943787752	-7.735977607673923	63071
2e8fae69defe5f58279b720efdbc095f44163cf1	beyond the next 700 lot platforms		The emergence of the Internet of Things (IoT) brings us connected devices that are an integral part of the physical world. Everyday devices and things will have considerable computing and communication capabilities, and will turn the real world programmable in a very literal sense. The dawn of the Programmable World poses significant challenges above and beyond data acquisition, analytics and other common research topics in the IoT area. In this paper we take a look at the software development issues associated with the Programmable World, highlighting technical topics that deserve developer education and deeper investigation by the research community.	data acquisition;emergence;floor and ceiling functions;iswim;integrated circuit;internet of things;literal (mathematical logic);runtime system;software development;software release life cycle;space: above and beyond;virtual machine	Antero Taivalsaari;Tommi Mikkonen	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8123178	computer engineering;artificial intelligence;machine learning;computer science;cloud computing;software development;internet of things;data acquisition;analytics	Robotics	-47.48006220805751	-1.1146849186919159	63270
cc51b2b3eda7ae8b353d5c3151bbd8425cec9449	i/o problems in preparing data for data warehousing and data mining, part 1.	data mining;data warehousing			Won Young Kim	1999	JOOP		computer science;data science;data mining;database	ML	-39.79380153301134	-5.466006192084247	63323
0b9899862ccf33a91226c08c6d84bf893dd99cfe	building an application framework for monitoring the environment	web service description language;workflow management;application framework;business process execution language;data processing;web service;xml environmental management geographic information systems geophysics computing information networks;monitoring system;real world application;geophysics computing;information networks;geographic information systems;seasonality;forest fire;software framework application framework environmental monitoring envimon project information systems workflow management xml web service description language business process execution language;xml;cost efficiency;software framework;information system;traffic monitoring;environmental management;monitoring application software buildings web services software prototyping prototypes information systems data analysis engines computer architecture	In the Envimon project we are building prototype information systems for diverse real-world environment monitoring applications. The applications are built onto a common software framework, which will be designed and implemented in the project. The framework can be used for developing diverse applications in a straightforward and cost- efficient way. It enables employing diverse data sources, preprocessing, analyzing and preparing the data into proper products, and delivering them at the right time, in the right form and via the appropriate channel to different end users that use diverse terminal equipment. The core of the framework consists of workflow management. A workflow description written in a workflow language represents the data process used in an application, and the workflow engine manages the execution of the process. The workflow tasks are implemented as web services, and the framework architecture utilizes XML-based standards like WSDL (Web service description language) and BPEL (business process execution language). The feasibility of the concept introduced by the software framework is verified by implementing several environment monitoring systems for the real-world applications in the areas of disaster monitoring, forestry, forest fire, maritime, traffic monitoring, disposal site monitoring, and season monitoring for tourism.	application framework;business process execution language;data sources;disposal;forestry;information systems;information system;preparation;preprocessor;prototype;software framework;web service definition language;web services description language;workflow engine;xml;standards characteristics	Ville Kotovirta;J. Kanniainen;Teppo Veijonen;Seppo Neuvonen	2006	2006 IEEE International Symposium on Geoscience and Remote Sensing	10.1109/IGARSS.2006.556	web service;workflow;xml;business process execution language;data processing;computer science;software framework;data mining;database;windows workflow foundation;world wide web;workflow management system;information system;workflow engine;seasonality;cost efficiency;workflow technology	OS	-39.239690137803535	-1.684850000862816	63375
7add09253b193b81cf64773af653b0a9026749f1	a data fusion system for spatial data mining, analysis and improvement	homogenization;heterogeneous geospatial data;data conflation;data fusion;vector data;data integration	The availability of multiple spatial data and differences in their geometric and thematic qualities require the development of a new system with a new approach to data mining. This should include the processes of the analysis and improvement of heterogeneous spatial data structures. This paper describes the concept, architecture and functionality of such a system: the spatial Data Fusion System (DAFU). DAFU allows efficient use of heterogneous spatial information and creates the possibility to individual use of geo-data.	data mining	Silvija Stankute;Hartmut Asche	2012		10.1007/978-3-642-31075-1_33	homogenization;computer science;data science;data integration;data mining;database;sensor fusion;spatial database;information retrieval	DB	-37.68274994662219	-2.8400347478140855	63513
513040c63600ee05020750f2dfb158887a5a0d76	ddi-lifecycle migration, curation and dissemination production systems at the danish data archive			archive;digital curation;research data archiving	Jannik V. Jensen;Anne Sofie	2013			data archive;data mining;danish;business	OS	-41.226310489718145	-4.560861687391847	63581
3da9887ae153ef728290460d68cd9891effabaf8	securing cognitive wireless sensor networks: a survey	engineering and technology;teknik och teknologier	Alexandros Fragkiadakis, Vangelis Angelakis and Elias Z. Tragos, Securing Cognitive Wireless Sensor Networks: A Survey, 2014, International Journal of Distributed Sensor Networks, 393248. International Journal of Distributed Sensor Networks is available online at informaworldTM: http://dx.doi.org/10.1155/2014/393248 Copyright: Hindawi Publishing Corporation / Taylor & Francis (Routledge) http://www.hindawi.com/	francis;hindawi programming system;sensor	Alexandros G. Fragkiadakis;Vangelis Angelakis;Elias Z. Tragos	2014	IJDSN	10.1155/2014/393248	telecommunications;computer science;key distribution in wireless sensor networks;computer security;computer network	Mobile	-45.0065483613415	-7.707620531546287	64079
5ef5ec7f6abcb925e70446257214c9b9fe3e76c0	key issues regarding digital libraries: evaluation and integration	libraries;metadata;digital;information	This is the second book based on the 5S (Societies, Scenarios, Spaces, Structures, Streams) approach to digital libraries (DLs). Leveraging the first volume, on Theoretical Foundations, we focus on the key issues of evaluation and integration. These cross-cutting issues serve as a bridge for those interested in DLs, connecting the introduction and formal discussion in the first book, with the coverage of key technologies in the third book, and of illustrative applications in the fourth book. These two topics have central importance in the DL field, allowing it to be treated scientifically as well as practically. In the scholarly world, we only really understand something if we know how to measure and evaluate it. In the Internet era of distributed information systems, we only can be practical at scale if we integrate across both systems and their associated content. Evaluation of DLs must take place atmultiple levels,so we can address the different entities and their associated measures. Thus, for digital objects, we assess accessibility, pertinence, preservability, relevance, significance, similarity, and timeliness. Other measures are specific to higher-level constructs like metadata, collections, catalogs, repositories, and services. We tie these together through a case study of the 5SQual tool, which we designed and implemented to perform an automatic quantitative evaluation of DLs. Thus, across the Information Life Cycle, we describe metrics and software useful to assess the quality of DLs, and demonstrate utility with regard to representative application areas: archaeology and education. Though integration has been a challenge since the earliest work on DLs, we provide the first comprehensive 5S-based formal description of the DL integration problem, cast in the context of related work. Since archaeology is a fundamentally distributed enterprise, we describe ETANADL, for integrating Near Eastern Archeology sites and information. Thus, we show how 5S-based modeling can lead to integrated services and content. While the first book adopts a minimalist and formal approach to DLs, and provides a systematic and functional method to design and implement DL exploring services, here we broaden to practical DLs with richer metamodels, demonstrating the power of 5S for integration and evaluation. Table of Contents: Evaluation / Integration / Bibliography	digital library;digital rights management;library (computing)	Ina Fourie	2014	Online Information Review	10.1108/OIR-06-2014-0143	information;computer science;database;metadata;world wide web;information retrieval	AI	-45.47188842483795	1.8685470523031258	64341
ace4ad2abc3080583b4d2dd5122037aee22ac48e	fca-cia: an approach of using fca to support cross-level change impact analysis for object oriented java programs	impact factor;change impact analysis;formal concept analysis;lattice of class and method dependence	0950-5849/$ see front matter 2013 Elsevier B.V. All rights reserved. http://dx.doi.org/10.1016/j.infsof.2013.02.003 q This work is supported partially by National Natural Science Foundation of China under Grant No. 60973149, partially by the Open Funds of State Key Laboratory of Computer Science of Chinese Academy of Sciences under Grant No. SYSKF1110, partially by Doctoral Fund of Ministry of Education of China under Grant No. 20100092110022, and partially by the Scientific Research Foundation of Graduate School of Southeast University under Grant No. YBJJ1102. ⇑ Corresponding author at: School of Computer Science and Engineering, Southeast University, Nanjing, China. Tel.: +86 25 5209089; fax: +86 25 52090879. E-mail addresses: bx.li@seu.edu.cn (B. Li), xbsun@yzu.edu.cn (X. Sun), Jacky. Keung@cityu.edu.hk (J. Keung). Bixin Li , Xiaobing Sun a,b,⇑, Jacky Keung d	academy;computer science;experiment;fax;formal concept analysis;jripples;java;mail (macos);project merrimac;regression testing;software maintenance;sun one;xiaoice	Bixin Li;Xiaobing Sun;Jacky W. Keung	2013	Information & Software Technology	10.1016/j.infsof.2013.02.003	computer science;formal concept analysis;engineering;data mining;engineering drawing;algorithm;change impact analysis	Logic	-45.77724658572341	-9.261005505166839	64450
a9b33e640bba91940d131c731107b30a5ab3c83b	biocomputer information management in the biological system human			biological system;information management	Adam Adamski	2010	Bio-Algorithms and Med-Systems		information management;biological system;environmental science;biocomputer	AI	-41.22113823940111	-6.617765886563622	64563
1060a4ab5ce94804ddd66184717de0ff229f5989	a call quality performance measure for handoff algorithms	performance measure;performance evaluation;wireless;swinburne;cellular networks;optimal handoff;handoff;handover;retrial model;handoff cost	1ARC Special Research Center for Ultra-Broadband Information Networks, Department of Electrical and Electronic Engineering, The University of Melbourne, Melbourne, VIC 3010, Australia 2Center for Advanced Internet Architectures, Faculty of I.C.T., Swinburne University of Technology, PO Box 218, Hawthorn, VIC 3122, Australia 3Electronic Engineering Department, City University of Hong Kong, Kowloon, Hong Kong SAR	algorithm;electronic engineering;hysteresis;john d. wiley;mos technology vic-ii;performance evaluation;signal-to-noise ratio	Malka N. Halgamuge;Hai Le Vu;Kotagiri Ramamohanarao;Moshe Zukerman	2011	Int. J. Communication Systems	10.1002/dac.1159	real-time computing;telecommunications;computer science;handover;operating system;computer network	DB	-45.26396103838488	-7.260074635248205	64621
469f1cb56f6877ca3ff65452dfbfa33792a669c8	semantic preventive conservation of cultural heritage collections		Semantic knowledge has been proven to be rather efficient on the data management of Culture Heritage domain. Conservation is an important aspect of museum management cycle aiming to preserve cultural heritage objects in the best possible condition for future generations. Since cultural objects are susceptible to environmental changes, sensor data could be of significant importance in automatic environmental monitoring and possible conservation issues. Recently, many approaches have included the SSN (Semantic Sensor Network) ontology in their domain knowledge representation and relevant applications. In this work, we merge the SSN using the CORE (Conservation Reasoning) ontology, an ontology which is based on empirical analysis, scientific knowledge and existing vocabularies of the conservation domain. Incorporating many of the existing properties of both ontologies and proposing additional ones, we integrate the majority of SSN classes in the CORE ontology, creating a new merged ontology that combines conservation procedures data and rules with sensor and environmental information. Furthermore, we create ontology-based rules, using the SWRL (Semantic Web Rule Language), in order to express preventive conservation guidelines and rules based on sensor and object current data.	context-aware network;core ontology;dmz (computing);knowledge representation and reasoning;ontology (information science);open-source software;protégé;real-time web;recommender system;requirement;semantic sensor web;semantic web rule language;subscriber identity module;vocabulary;web ontology language	Efthymia Moraitou;John Aliprantis;George Caridakis	2018				AI	-39.35096892098125	0.7652232391889159	64768
2668675b864036d6488163033d32f51f91bcd567	effective software reuse in an embedded real-time system	message processing;design process;radio equipment;development strategy;ada programming language;surveillance;buffers;real time;embedded real time systems;computer communications;ocean surveillance;time;division;strategy;test and evaluation;naval command control and ocean surveillance center;system development;warfare;control;software reuse;naval warfare;submarines;los angeles	The Submarine Message Buffer (SMB) is a ml time embedded message processing system developed at the Naval Command, Control and Ocean Surveillance Center, Research, Development, Test and Evaluation Division (NRaD). The SMB is sponsored by the Space and Naval Warfare Systems Command (SPAWAR) to support modernization of SSN (Los Angels and Seawolf class submarines) radio rooms. The development strategy adopted for the SMB concentrated on the reuse of Ada software. This paper will focus on the design, processes and methodologies, which were used in the development of this system. Metrics will also be provided showing why this system has been identified as an Ada success story by the Ada Joint Program Office among others [1,2].	ada;code reuse;dmz (computing);embedded system;process (computing);real-time operating system;real-time transcription	B. Barlin;John M Lawler	1992		10.1145/143557.143743	embedded system;real-time computing;simulation;engineering	Embedded	-45.41936261937826	-6.044400110369739	64819
a4097e096303146edab67e89e49ede8760e2387d	overview of simbig 2015: 2nd annual international symposium on information management and big data		Big Data is a popular term used to describe the exponential growth and availability of both structured and unstructured data. The aim of the symposium is to present the analysis methods for managing large volumes of data through techniques of artificial intelligence and data mining. Bringing together main national and international actors in the decision-making field to state in new technologies dedicated to handle large amount of information.	artificial intelligence;big data;data mining;information management;time complexity	Juan Antonio Lossio-Ventura;Hugo Alatrista Salas	2015			information management;emerging technologies;big data;data mining;unstructured data;computer science	Arch	-38.21844192240307	-6.818605291199607	64944
6078595623fedd38dccaef9021ae8bd3fe3114b6	data mapping framework in a digital library with computational epidemiology datasets	simulation;epidemiology;digital library	Computational epidemiology employs computer models and informatics tools to reason about the spatio-temporal spread of diseases. The diversity of models, data sources, data representations, and modalities that are collected, used, and modified motivate the development of a digital library (DL) framework to support computational epidemiology. The heterogeneous content includes metadata, text, tables, spreadsheets, experimental descriptions, and large result files. There is no accepted framework that allows unified access to such content. We propose a framework for a digital library system tailored to such datasets to support computational network epidemiology.	artificial neural network;computational epidemiology;computer simulation;digital library;informatics;library classification;spreadsheet;table (database)	S. M. Shamimul Hasan;Sandeep Gupta;Edward A. Fox;Keith R. Bisset;Madhav V. Marathe	2014	IEEE/ACM Joint Conference on Digital Libraries		data modeling;digital library;epidemiology;computer science;bioinformatics;data science;rdf;data mining;computational model;world wide web	HPC	-37.92633388754538	2.7869855907210694	65023
b438b42e374305297e1464e33659af57ceefdc0f	integrating knowledge representation and reasoning in geographical information systems	settore inf 01 informatica;constraint logic programs;geographic information system;programming environment;knowledge representation and reasoning;middleware;knowledge representation;temporal reasoning	We propose a formalism and a programming environment in which sophisticated spatio-temporal reasoning can be performed, while keeping the capabilities of manipulating and presenting large amounts of geographical data, typical of commercial Geographical Information Systems (GISs). The spatio-temporal knowledge representation language, named MuTACLP+, is based on constraint logic programming and is integrated via a middleware of commands and translation features with a commercial GIS. The paper presents the language, the architecture of the environment, and a few examples of its use in the field of event planning.	geographic information system;knowledge representation and reasoning	Paolo Mancarella;Alessandra Raffaetà;Chiara Renso;Franco Turini	2004	International Journal of Geographical Information Science	10.1080/13658810410001672908	knowledge representation and reasoning;computer science;knowledge management;artificial intelligence;theoretical computer science;model-based reasoning;middleware;data mining;database;reasoning system	DB	-36.48448884258071	-0.8857929400052865	65298
3d35e86f4bcc98b9d824d155d113491e7be98a0c	canary: an information extraction platform for clinical researchers			information extraction	Shervin Malmasi;Nicolae L. Sandor;Naoshi Hosomura;Matt Goldberg;Stephen Skentzos;Alexander Turchin	2017			data mining;information extraction;computer science	NLP	-40.876136304079544	-8.547293245322765	65339
a9685cc75e49e130ca10b65088a80b00f691f3c9	towards ontology-enabled biocontexts for bioinformatics research	biomedical ontologies;text annotation;software engineering bioinformatics internet ontologies artificial intelligence;web 2 0 applications biomedical ontologies text annotation;web 2 0 applications;ontologies biology semantics bioinformatics software knowledge based systems portals;ontology enabled biocontexts bcnotes software environment biomedical researchers web technologies bioinformatics research	The evolution of web technologies seems to characterize a new scenario for biomedical researchers who take advantage from sophisticated on to logies offered by scientific portals. This paper introduces the concept of BioContext, a software environment which combines services for the fully exploitation of several ontologies whose knowledge is captured, and managed in a local repository. We illustrate our ideas pragmatically by presenting BCnotes, a BioContext which enables the functional annotation of biomedical texts in order to discover important relationships among genes.	bioinformatics;ontology (information science);portals;usb on-the-go	Nicoletta Dessì;Giuliano Ferrentino;Emanuele Pascariello;Barbara Pes	2015	2015 IEEE 24th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises	10.1109/WETICE.2015.17	idef5;open biomedical ontologies;computer science;bioinformatics;data mining;world wide web;information retrieval	Visualization	-41.01254756885557	2.661124247539951	65459
9050c8376df5cab2ebf88db9c2162cee8cd0d60f	optimal multi-robot task planning: from synthesis to execution (and back)		[1] F. Leofante, E. Ábrahám, T. Niemueller, G. Lakemeyer and A. Tacchella: On the Synthesis of Guaranteed-Quality Plans for Robot Fleets in Logistics Scenarios via Optimization Modulo Theories. IEEE IRI. (2017) [2] F. Leofante, E. Ábrahám, T. Niemueller, G. Lakemeyer and A. Tacchella: Integrated Synthesis and Execution of Optimal Plans for Multi-Robot Systems in Logistics. Information Systems Frontiers. (2018) [3] T. Niemueller, G. Lakemeyer, F. Leofante, E. Ábrahám Towards CLIPS-based Task Execution and Monitoring with SMT-based Planning and Optimization. PlanRob@ICAPS. (2017) [4] T. Niemueller, G. Lakemeyer, and A. Ferrein: The RoboCup Logistics League as a Benchmark for Planning in Robotics. PlanRob@ICAPS. (2015) [5] T. Niemueller, E. Karpas, T. Vaquero and E. Timmons: Planning Competition for Logistics Robots in Simulation. PlanRob@ICAPS. (2016)	benchmark (computing);clips;information systems;logistics;program optimization;robot;robotics;simulation;simultaneous multithreading	Francesco Leofante	2018		10.24963/ijcai.2018/829	artificial intelligence;computer science;machine learning;robot	Robotics	-45.08363126620869	-9.095687870739342	65780
58147816571bc69feb2ef62272d6d247bf863a2f	keeping track of the big event	high energy physics instrumentation computing;high energy physics babar detector b meson factory stanford linear accelerator laboratory anti matter big bang silicon vertex detector raw data c java fortran coding standards computing environment online system offline system;data handling high energy physics instrumentation computing computerised control c language fortran java;detectors electron beams silicon mesons production facilities linear accelerators laboratories collaboration europe north america;north america;computerised control;silicon vertex detector;c language;lines of code;linear accelerator;fortran;data handling;big bang;java	The BaBar detector at the B meson factory at the Stanford Linear Accelerator Laboratory (SLAC) is physically large. It represents a collaboration of more than 600 researchers at 73 institutions in Europe and North America. The experiment is primarily designed to answer a really big question: why more matter than anti matter survived a fraction of a second after the Big Bang. Several times a person's height and weighing more than 1000 tons, the BaBar detector sits at the intersection point of the electron and positron beams. A silicon vertex detector, comprising strips of silicon spaced 100 microns apart, provides 150000 channels of raw data. The software that controls and processes data from this experiment is also large; an estimated three million lines of code primarily written in C++, with small amounts of Java and Fortran. Because of the complexity, coding standards and tools for managing the code have been developed. The computing environment for BaBar is divided into two parts: the online system and the offline system.		David I. Lewin	2001	Computing in Science and Engineering	10.1109/5992.947102	parallel computing;real-time computing;simulation;computer science;operating system;group method of data handling;big bang;programming language;java;source lines of code;linear particle accelerator;physics	DB	-44.104145674732074	-4.149786992209045	65959
f067392537a926535d04703d1077d644cf1e05a1	towards cyberphysical digital libraries: integrating iot smart objects into digital libraries		Digital libraries are distributed software infrastructures that aim at collecting, managing, preserving, and using digital objects (or resources) for the long term, and providing specialized services on such resources to its users. Service provision should be of measurable quality and performed according to codified policies. Currently, modern digital libraries include a wide range of conventional digital objects: text document, image, audio, video, software, etc. In the emerging domain of the Internet of Things (IoT), cyberphysical smart objects (or simply smart objects) will play a central role in providing new (smart) services to both humans and machines. It is therefore challenging to include smart objects, the newest type of digital objects, into digital libraries as novel first-class objects to be collected, managed, and preserved. However, their inclusion poses critical issues to address and many research challenges to deal with. This paper aims at paving the way towards such a novel inclusion that will enable effective discovery, management and querying of smart objects, so establishing cyberphysical digital libraries. In particular, our approach is based on a metadata model purposely defined to describe all the cyberphysical characteristics (geophysical, functional, and non-functional) of smart objects. The metadata model is then used for a seamless integration of smart objects into digital libraries compliant with the digital library reference model proposed by the DL.org community. The proposed approach is also exemplified through a simple yet effective case study.	digital library;smart objects	Giancarlo Fortino;Anna Rovella;Wilma Russo;Claudio Savaglio	2016		10.1007/978-3-319-26869-9_7	computer science;software;computer security;computer engineering;reference model;metadata;metadata modeling;digital library;internet of things;smart objects	EDA	-43.98394793545433	2.019796626406614	66039
552b4881e090a7114743b51e7eae99d2f796e2ef	a hybrid ict-solution for smart meter data analytics	ict solution;big data;smart meter data;data analytics	Smart meters are increasingly used worldwide. Smart meters are the advanced meters capable of measuring energy consumption at a fine-grained time interval, e.g., every 15 minutes. Smart meter data are typically bundled with social economic data in analytics, such as meter geographic locations, weather conditions and user information, which makes the data sets very sizable and the analytics complex. Data mining and emerging cloud computing technologies make collecting, processing, and analyzing the socalled big data possible. This paper proposes an innovative ICT-solution to streamline smart meter data analytics. The proposed solution offers an information integration pipeline for ingesting data from smart meters, a scalable platform for processing and mining big data sets, and a web portal for visualizing analytics results. The implemented system has a hybrid architecture of using Spark or Hive for big data processing, and using the machine learning toolkit, MADlib, for doing in-database data analytics in PostgreSQL database. This paper evaluates the key technologies of the proposed ICT-solution, and the results show the effectiveness and efficiency of using the system for both batch and online analytics.	adobe streamline;algorithm;apache hive;batch processing;big data;cloud computing;data mining;hybrid system;in-database processing;machine learning;postgresql;preprocessor;real-time clock;spark;scalability;smart meter;synthetic data;systems architecture	Xiufeng Liu;Per Sieverts Nielsen	2016	CoRR	10.1016/j.energy.2016.05.068	analytics;web analytics;big data;data mining;business intelligence;data analysis;software analytics	DB	-36.25163762103875	-4.829255864412926	66119
4b917229af10809610f5994a3105de3a09c8b7a2	an improved identification code for city components based on discrete global grid system		City components are important elements of a city, and their identification plays a key role in digital city management. Various identification codes have been proposed by different departments and systems over the years, however, their application has been partly hindered by the lack of a unified coding framework. The use of a code identifying a city component for unified management and geospatial computation across systems is still problematic. In this paper, we put forward an improved identification code for city components based on the discrete global grid system (DGGS). According to their spatial location, city components were identified with one-dimensional integer codes. The results illustrated that this identification code could express the location information of city components explicitly, as well as indicate the spatial distance relationship and the spatial direction relationship between different components. The experiment showed that this code performed better than traditional codes in data query and geospatial computation. Therefore, we concluded that this improved identification code was conducive to the more efficient management of city components, and hence might be used to improve digital city management.	computation;digital rights management;grid (spatial index);qr code;smart city	Kun Qi;Chengqi Cheng;Yi'na Hu;Huaqiang Fang;Yan Ji;Bo Chen	2017	ISPRS Int. J. Geo-Information	10.3390/ijgi6120381	geospatial analysis;coding (social sciences);grid;data mining;computation;code (cryptography);geography	SE	-37.01791320280476	-3.370785940082325	66506
c7762f5fb7aa340df298fe67834738cac600364a	integration and automation of data preparation and data mining	sensors;semantics;data mining;global positioning system;data mining data models semantics discrete fourier transforms sensors ontologies global positioning system;pattern classification data integration data mining data preparation meta data;data mining algorithms data preparation automation data mining automation data mining tools raw data detailed metadata data sources learned classifier;ontologies;discrete fourier transforms;data models	Data mining tasks typically require significant effort in data preparation to find, transform, integrate and prepare the data for the relevant data mining tools. In addition, the work performed in data preparation is often not recorded and is difficult to reproduce from the raw data. In this paper we present an integrated approach to data preparation and data mining that combines the two steps into a single integrated process and maintains detailed metadata about the data sources, the steps in the process, and the resulting learned classifier produced from data mining algorithms. We present results on an example scenario, which shows that our approach provides significant reduction in the time in takes to perform a data mining task.	algorithm;automation;data mining;data pre-processing	Shrikanth (Shri) Narayanan;Ayush Jaiswal;Yao-Yi Chiang;Yanhui Geng;Craig A. Knoblock;Pedro A. Szekely	2014	2014 IEEE International Conference on Data Mining Workshop	10.1109/ICDMW.2014.44	concept mining;data modeling;text mining;global positioning system;data transformation;computer science;sensor;ontology;data virtualization;data science;data warehouse;data mining;database;semantics;data pre-processing;data stream mining;data element;data mapping	DB	-37.70040817740753	-0.0667547344866086	66541
d2d14a0b77145a5d708ea9c24d0293d2ac2c5278	characterization of the structure and function of the normal human fovea using adaptive optics scanning laser ophthalmoscopy	ophthalmology;optics;fovea;berkeley austin roorda putnam;nicole marie;biomedical engineering;imaging;vision science;optics characterization of the structure and function of the normal human fovea using adaptive optics scanning laser ophthalmoscopy university of california;scanning laser ophthalmoscopy;adaptive optics;psychophysics	.......................................................................................................................................... 1 TABLE OF CONTENTS........................................................................................................................ i LIST OF FIGURES .............................................................................................................................. iv LIST OF ABBREVIATIONS ................................................................................................................. vi ACKNOWLEDGEMENTS .................................................................................................................. vii CHAPTER 1: IMAGING THE HUMAN FOVEA .................................................................................... 1 The Anatomical Structure of the Fovea ...................................................................................... 1 Imaging and Resolution Limits .................................................................................................... 2 Adaptive Optics ........................................................................................................................... 3 AO Scanning Laser Ophthalmoscopy .......................................................................................... 4 Fixational eye movements and AOSLO image stabilization ........................................................ 5 OVERVIEW OF THE STUDIES IN THIS DISSERTATION: ................................................................. 6 Chapter 2: Modeling the Foveal Cone Mosaic Imaged with Adaptive Optics Scanning Laser Ophthalmoscopy ..................................................................................................................... 6 Chapter 3: Comparison of Anatomical and Functional Measures of the Normal Human Fovea using Adaptive Optics Scanning Laser Ophthalmoscopy .............................................. 7 Chapter 4: The Role of Natural and Manipulated Fixational Eye Movements in Foveal Vision ................................................................................................................................................. 7 A. Measurement of Motion Detection Thresholds ............................................................... 8 B. Measurement of Contrast Thresholds and the Role of Fading ........................................ 9 CHAPTER 2: MODELING THE FOVEAL CONE MOSAIC IMAGED WITH ADAPTIVE OPTICS SCANNING LASER OPHTHALMOSCOPY ......................................................................................... 10 ABSTRACT .................................................................................................................................. 10................................................................................................................................. 10 INTRODUCTION ......................................................................................................................... 10 METHODS .................................................................................................................................. 11 Where does the light come from in an AOSLO image of a cone? ......................................... 11 OCT measurements of foveal cone reflectance .................................................................... 12 The impact of source coherence ........................................................................................... 15	ambient occlusion;catastrophic interference;cone;interference (communication);ncsa mosaic;optics algorithm;simulation;vii;zhi-li zhang	Nicole Marie Putnam	2012			computer vision;peripheral vision;geography;optics;computer graphics (images)	Vision	-47.27338131961584	-9.396249259397964	66697
3a4ce1a4832755c0aa4c7fcdad42356e3513170e	abstraction versus implementation: issues in formalizing the niehs application profile		The National Institute of Environment Health Sciences (NIEHS), is an Institute of the National Institutes of Health (NIH), which is a component of the U.S. Department of Health and Human Services (DHHS). As with many governmental organizations, the NIEHS website contains a rich and growing body of important resources for both employees and the general public. The NIEHS library has spearheaded an organization-wide metadata project to provide better access to these resources. Dublin Core was elected for the NIEHS metadata project because it supports semantic interoperability and the potential for data sharing, and because the schema is simple enough to support author-generated metadata. This paper and corresponding poster document issues in formalizing the NIEHS Application Profile, specifically the changes implemented between Version 1 to Version 2, which were influenced by revisions made to the NIEHS metadata form supporting author-generated metadata. Version 1 was comprised of twenty metadata elements, the majority of which were drawn from the basic Dublin Core Metadata Element Set version 1.1, and the expanded set of Dublin Core Qualifiers (Robertson et al. 2001). NIEHS’ Version 1 corresponded quite closely with the Dublin Core semantics. Exceptions included merging the Creator element with the Contributor element, incorporating an Audience element from the Gateway to Educational Materials (GEM) namespace. Version 1 of the NIEHS Metadata Schema was an application profile, in the rough sense of the word, but discrepancies in the formal schema and the public schema, which supported the NIEHS metadata form for author metadata creation, delayed official formalization, until version 2 of this schema.	dublin core;semantic interoperability	Corey A. Harper;Jane Greenberg;W. Davenport Robertson;Ellen M. Leadem	2002			engineering;data mining;database;world wide web	NLP	-45.347802523091666	1.4948599326448722	66769
49a31ab5bd237c4645401981669ec3f833f3894f	big-data management use-case: a cloud service for creating and analyzing galactic merger trees	cloud service;myria;parallel data management;astronomy	We present the motivation, design, implementation, and preliminary evaluation for a service that enables astronomers to study the growth history of galaxies by following their `merger trees' in large-scale astrophysical simulations. The service uses the Myria parallel data management system as back-end and the D3 data visualization library within its graphical front-end. We demonstrate the service at the workshop on a ~5TB dataset.	big data;computational astrophysics;data hub;data visualization;graphical user interface;simulation;terabyte;visualization library	Sarah Loebman;Jennifer Ortiz;Lee Lee Choo;Laurel Orr;Lauren Anderson;Daniel Halperin;Magdalena Balazinska;Thomas Quinn;Fabio Governato	2014		10.1145/2627770.2627774	simulation;computer science;data science;world wide web	HPC	-36.06514610793509	-5.38404181804729	67146
31e63fa912f1477ec9444c0362159fe79d0b734d	semantic mediation in the national geologic map database (us)		Controlled language is the primary challenge in merging heterogeneous databases of geologic information. Each agency or organization produces databases with different schema, and different terminology for describing the objects within. In order to make some progress toward merging these databases using current technology, we have developed software and a workflow that allows for the “manual semantic mediation” of these geologic map databases. Enthusiastic support from many state agencies (stakeholders and data stewards) has shown that the community supports this approach. Future implementations will move toward a more Artificial Intelligence-based approach, using expertsystems or knowledge-bases to process data based on the training sets we have developed manually.	artificial intelligence;controlled natural language;data steward;database schema	David Percy;Stephen Richard;David Soller	2008			data mining;database;information retrieval	DB	-43.26715716415128	4.081956003511937	67502
249a912579271bb231a3ada6a5e87f4e7abedbe6	ontology based data access and integration for improving the effectiveness of farming in nepal	web based data;box office data;agriculture resource description framework production ontologies meteorology soil shape;ticket sale prediction;resource description framework;data mining;ontologies artificial intelligence data integration farming information retrieval;shape;machine learning;production;agriculture;ontologies;soil;meteorology;data publication process ontology based data access data integration farming effectiveness nepal rdf data re use data conversion data linking	Is widely accepted that food supply and quality are major problems in the 21st century. Due to the growth of the world's population, there is a pressing need to improve the productivity of agricultural crops, which hinges on different factors such as geographical location, soil type, weather condition and particular attributes of the crops to plant. In many regions of the world, information about those factors is not readily accessible and dispersed across a multitude of different sources. One of those regions is Nepal, in which the lack of access to this knowledge poses a significant burden for agricultural planning and decision making. Making such knowledge more accessible can boot up a farmer's living standard and increase their competitiveness on national and global markets. In this article, we show how we converted several available, although not easily accessible, datasets to RDF, thereby lowering the barrier for data re-usage and integration. We describe the conversion, linking, and publication process as well as use cases, which can be implemented using the farming datasets in Nepal.	booting;data access;linker (computing);location (geography);population;resource description framework	Suresh Pokharel;Mohamed Ahmed Sherif;Jens Lehmann	2014	2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)	10.1109/WI-IAT.2014.114	geography;knowledge management;data science;data mining	DB	-39.546892838998005	-3.2969288513699686	67642
96cc346729a8aba935d9f1567d111d100961e377	geographic knowledge discovery from web 2.0 technologies for advance collective intelligence		Collective intelligence is currently a hot topic within the Web and Geoinformatics communities. Research into ways of producing advances with collective intelligence is becoming increasingly popular. This article introduces a novel approach to collective intelligence with the use of geographic knowledge discovery to determine spatially referenced patterns and models from the Geospatial Web which are used for supporting decisions. The article details the latest Web 2.0 technologies which make geographic knowledge discovery from the Geospatial Web possible to produce advanced collective intelligence. The process is explored and illustrated in detail, and use cases demonstrate the potential usefulness. Finally, potential pitfalls are discussed.	collective intelligence;geoinformatics;geoweb;web 2.0;world wide web	Ickjai Lee;Christopher Torpelund-Bruin	2011	Informatica (Slovenia)		computer science;knowledge management;data science;data mining;web intelligence	ML	-37.419746825948884	-5.202845875202029	67674
4f1ef349faa3b6e9b7cb580362f2017985de29cb	a structured representation of word-senses for semantic analysis	semantic knowledge base;ibm scientific center;structured representation;semantic analysis;text understanding project;knowledge base data structure;semantic verification algorithm;concept type;semantic knowledge;deep knowledge;italian text understanding;knowledge base;data structure;conceptual graph	A framework for a structured representation of semantic knowledge (e.g. word-senses) has been defined at the IBM Scientific Center of Roma, as part of a project on Italian Text Understanding. This representation, based on the conceptual graphs formalism [SOW84], expresses deep knowledge (pragmatic) on word-senses. The knowledge base data structure is such as to provide easy access by the semantic verification algorithm. This paper discusses some important problem related to the definition of a semantic knowledge base, as depth versus generality, hierarchical ordering of concept types, etc., and describes the solutions adopted within the text understanding project.	accessibility;algorithm;concept map;conceptual graph;data structure;knowledge base;representation oligonucleotide microarray analysis;semantics (computer science)	Maria Teresa Pazienza;Paola Velardi	1987			semantic data model;natural language processing;conceptual graph;knowledge base;semantic similarity;semantic computing;multinet;explicit semantic analysis;data structure;semantic grid;computer science;knowledge-based systems;open knowledge base connectivity;data mining;database;linguistics;semantic technology	AI	-42.76362571468838	3.8988930839712173	67778
b84f51a460f001d625f66ceecdca0fc3bbb32821	on implementing a text-database-as-a-service	hybrid data service;web service;filtering data analysis algorithm design and analysis indexes data models content management;text database;text database web service hybrid data service dbaas;text analysis big data query processing relational databases sql;dbaas;top k word algorithm text database as a service heterogeneous big data hybrid data service text data services content management system text data query relational data json data dbms big text data text analytics hds ibm research tdbaas sql	The emergence of heterogeneous big data in the last decade calls for a hybrid data service that can manage all different kinds of data, including relational data, JSON data, and text data in a unified way. Among them, text data play an important role in many fields such as Internet-of-Things, biology, social network, and etc. For example, a smart meter application detecting the anomaly of the electricity use might want to link each anomaly of a certain area to a meaningful social event mined from the news in plain text. As a result, text data services have raised more and more attentions by the research community. Most of such services are implemented based on a content management system such as ElasticSearch and Solr. However, we found that the mere content management capabilities are not enough. On one hand, text data query often requires join operations to relational data or JSON data in an existing DBMS. On the other hand, users often have to pull the big text data out to an independent system or service for further text analytics. In this paper, we present our Text-DataBase-as-a-Service (TDBaaS), which is built on top of the Hybrid Data Service (HDS) from IBM Research. The TDBaaS is designed to manage the text data together with relational data and JSON data in a single service. Basic text analytics can be conducted directly inside the database in the form of general SQLs. Moreover, the extensible framework allows the service to have abundant text analytic capabilities with high performance. As a case study, we investigate in the implementation of the top-k word algorithm, and show how the common computations are shared across different tenants in the TDBaaS. The experimental results demonstrate the high performance of the TDBaaS on both text data management and text data analytics.	algorithm;anomaly detection;big data;cloud database;computation;content management system;elasticsearch;emergence;holographic data storage;ibm research;json;mined;sensor;smart meter;social network;solr;text corpus;text mining	Yaoliang Chen;Chang Liu;Jianfeng Zhang;Zhenying He;Xiaoyang Sean Wang;Sheng Huang;Xiaoyan Chen	2016	2016 IEEE International Conference on Web Services (ICWS)	10.1109/ICWS.2016.30	web service;data modeling;analytics;text mining;semi-structured data;data quality;data transformation;computer science;database model;data warehouse;noisy text analytics;data mining;database;law;world wide web;database design	DB	-36.61015622627946	-4.973854240386789	68125
71896eee4c0c3ed069092e01b4078c10514c2b0a	big data processing: big challenges and opportunities	distributed processing;data management;journal;big data;cloud computing	With the rapid growth of emerging applications like social network, semantic web, sensor networks and LBS (Location Based Service) applications, a variety of data to be processed continues to witness a quick increase. Effective management and processing of large-scale data poses an interesting but critical challenge. Recently, big data has attracted a lot of attention from academia, industry as well as government. This paper introduces several big data processing techniques from system and application aspects. First, from the view of cloud data management and big data processing mechanisms, we present the key issues of big data processing, including definition of big data, big data management platform, big data service models, distributed file system, data storage, data virtualization platform and distributed applications. Following the MapReduce parallel processing framework, we introduce some MapReduce optimization strategies reported in the literature. Finally, we discuss the open issues and challenges, and deeply explore the research directions in the future on big data processing in cloud computing environments.	big data;cloud computing;clustered file system;computer data storage;distributed computing;location-based service;mapreduce;mathematical optimization;parallel computing;semantic web;social network	Changqing Ji;Yu Li;Wenming Qiu;Yingwei Jin;Yujie Xu;Uchechukwu Awada;Keqiu Li;Wenyu Qu	2012	Journal of Interconnection Networks	10.1142/S0219265912500090	big data;cloud computing;data management;computer science;data science;operating system;data mining;world wide web	DB	-37.311003656769294	-5.665802148766078	68421
e107f516d50ccf0b562e4650d8b505bc4ea6fc41	understanding crowdsourcing systems from a multiagent perspective and approach		Crowdsourcing has recently been significantly explored. Although related surveys have been conducted regarding this subject, each has mainly consisted of a review of a single aspect of crowdsourcing systems or on the application of crowdsourcing in a specific application domain. A crowdsourcing system is a comprehensive set of multiple entities, including various elements and processes. Multiagent computing has already been widely envisioned as a powerful paradigm for modeling autonomous multi-entity systems with adaptation to dynamic environments. Therefore, this article presents a novel multiagent perspective and approach to understanding crowdsourcing systems, which can be used to correlate the research on crowdsourcing and multiagent systems and inspire possible interdisciplinary research between the two areas. This article mainly discusses the following two aspects: (1) The multiagent perspective can be used for conducting a comprehensive survey on the state of the art of crowdsourcing, and (2) the multiagent approach can bring about concrete enhancements for crowdsourcing technology and inspire future research directions that enable crowdsourcing research to overcome the typical challenges in crowdsourcing technology. Finally, this article discusses the advantages and disadvantages of the multiagent perspective by comparing it with two other popular perspectives on crowdsourcing: the business perspective and the technical perspective.	acclimatization;agent-based model;application domain;autonomous robot;behavior;computation (action);control system;crowdsourcing;entity;game theory;multi-agent system;numerous;programming paradigm;social welfare;systems theory	Jiuchuan Jiang;Bo An;Yichuan Jiang;Donghui Lin;Zhan Bu;Jie Cao;Zhifeng Hao	2018	TAAS	10.1145/3226028	application domain;management science;distributed computing;computer science;multi-agent system;crowdsourcing	AI	-47.75120220953893	2.2384324161107942	68482
4f4cdcdf73dd9fa3648a0784e2cf6b2d4a74b87f	what can agent-based computing offer service-oriented architectures, and vice versa?	panel discussion;industry collaborative research;agent-based computing;agent-based computing offer;agent-based system;basic question;industry representative;agent-based computing paradigm;service-oriented architecture;key research;industry issue	This article serves as a record of a panel discussion held at PRIMA in November, 2010. The panel consisted of two academic and three industry representatives, and thus provided a rare opportunity to discuss the relationship between agent-based computing and service-oriented architectures from both points of view. The basic question for the panel was to identify the key research and industry issues that arise in the deployment of systems based on service-oriented architectures, and in particular to address whether the agent-based computing paradigm offers any resolution of those issues. The question was also posed whether applications based on service-oriented architectures provide a suitable platform for implementing agent-based systems, which are presently limited in application by comparison. This summary is presented with the aim of stimulating further academic and industry collaborative research in this fast growing area which potentially has wide-ranging practical application.	service-oriented architecture;service-oriented device architecture	Wayne Wobcke;Nirmit Desai;Frank Dignum;Aditya K. Ghose;Srinivas Padmanabhuni;Biplav Srivastava	2010		10.1007/978-3-642-25920-3_1	computer science;artificial intelligence;data mining	HPC	-47.418170702861886	0.48620116769459826	68599
914b199d0e2ae4564ac69a02e902d6934763225a	ontology and semantic wiki for an intangible cultural heritage inventory	ontology collaborative enrichment semantic based catalog chilean ich inventory ontowiki instance controlled social enrichment rigid data models data acquisition unesco globalization intangible cultural heritage inventory semantic wiki;history;cataloguing;ontologies artificial intelligence;semantic web;cultural differences electronic publishing internet encyclopedias silicon compounds materials;semantic web cataloguing history ontologies artificial intelligence;heritage computing semantic wikis social enrichment intangible cultural heritage ontologies	The increasing globalization has made the preservation of the Intangible Cultural Heritage (ICH) an urgent need, and the UNESCO's states parties have compromised to make collaborative inventories of ICH. Many traditional inventories become obsolete quickly because they present rigid data models and/or because data adquisition from scarce specialists are required. This article proposes to introduce a participative inventory as a semantic wiki, combining the familiarity of the audience with the free text, the expressive power of ontologies, and the benefits of wikis for the controled social enrichment. The scope has been validated in a pilot inventory with public access, that combine an Ontowiki instance, a new ontology based on UNESCO's clasification, and the data of an existent Chilean ICH inventory. Preliminary results indicate that the semantic-based catalog is much more flexible than existing traditional catalogs. Using these collaborative enrichment will enable active involvement to citizenship in recording their immaterial heritage.	bibliothèque de l'école des chartes;cidoc conceptual reference model;data model;gene ontology term enrichment;handbook;hypertext transfer protocol;iso 8601;informatics;information system;inventory;java annotation;knowledge systems laboratory;knowledge acquisition;knowledge modeling;language technology;linear algebra;linked data;naruto shippuden: clash of ninja revolution 3;os-tan;ontowiki;ontology (information science);ontology engineering;power-on reset;sparql;springer (tank);tom gruber;unique name assumption;web ontology language;wiki	Renzo Stanley;Hernán Astudillo	2013	2013 XXXIX Latin American Computing Conference (CLEI)	10.1109/CLEI.2013.6670653	engineering;knowledge management;data mining;world wide web	AI	-44.26008649128031	4.09803385954159	68656
cc008485a8f0488c0e62e8d9992bf161017abc7e	intelligent audio, speech, and music processing applications	signal image and speech processing;acoustics;mathematics in music;engineering acoustics	1 Digital Signal Processing Laboratory, School of Electrical and Electronic Engineering, Nanyang Technological University, Nanyang Avenue, Singapore 639798 2 Department of Electrical Engineering, Northern Illinois University, Dekalb, IL 60115, USA 3 Center for Robust Speech Systems (CRSS), Department of Electrical Engineering, Erik Jonsson School of Engineering and Computer Science, University of Texas at Dallas, Richardson, TX 75083, USA	computer science;digital signal processing;electrical engineering;electronic engineering;richardson number;sound card	Woon-Seng Gan;Sen M. Kuo;John H. L. Hansen	2008	EURASIP J. Audio, Speech and Music Processing	10.1155/2008/854716	speech technology;speech recognition;acoustics;acoustical engineering;computer science;speech processing;computer music;physics	ML	-47.09738709877711	-8.503926671075375	68683
4ef13fef7bf9f89577aac6e2b3a330319e260f5a	"""""""vipar"""" libraries to support distribution and processing of visualization datasets"""	software tool;application visualization system;data exploration;parallel architecture;epsrc	The aims of the “Visualization in Parallel” (Vipar) project is to produce a comprehensive environment for the development of parallel visualization modules in systems such as the Application Visualization System (AVS), Iris Explorer, IBM Data Explorer (DX) and Khoros. This paper presents an overview of the project and describes the libraries developed to support the first phase of the work which is a tool to describe parallel visualization modules. This work is funded as part of the EPSRC project (GR/K40390) Portable Software Tools for Parallel Architectures (PSTPA).	display resolution;library (computing)	Steve Larkin;Andrew J. Grant;W. Terry Hewitt	1996		10.1007/3-540-61142-8_618	information visualization;bioinformatics;data science;data mining	HPC	-42.903209910918754	-4.484088803876737	68700
b47c985585381b11efe2245208f1b3f44e3a27a8	publishing a disease ontologies as linked data	disease ontology;linked data;ontological engineering;ontology	Publishing open data as linked data is a significant trend in not only the Semantic Web community but also other domains such as life science, government, media, geographic research and publication. One feature of linked data is the instance-centric approach, which assumes that considerable linked instances can result in valuable knowledge. In the context of linked data, ontologies offer a common vocabulary and schema for RDF graphs. However, from an ontological engineering viewpoint, some ontologies offer systematized knowledge, developed under close cooperation between domain experts and ontology engineers. Such ontologies could be a valuable knowledge base for advanced information systems. Although ontologies in RDF formats using OWL or RDFS can be published as linked data, it is not always convenient to use other applications because of the complicated graph structures. Consequently, this paper discusses RDF data models for publishing ontologies as linked data. As a case study, we focus on a disease ontology in which diseases are defined as causal chains.	linked data;ontology (information science)	Kouji Kozaki;Yuki Yamagata;Takeshi Imai;Kazuhiko Ohe;Riichiro Mizoguchi	2013		10.1007/978-3-319-06826-8_10	upper ontology;idef5;open biomedical ontologies;ontology components;bibliographic ontology;computer science;ontology;simple knowledge organization system;linked data;data mining;ontology-based data integration;web ontology language;world wide web;information retrieval;process ontology;rdf schema	AI	-41.027749684225306	2.578636202092818	68837
527b016af7553457d24e02e1b242cd25a1b18497	formalizing knowledge and evidence about potential drug-drug interactions	drug drug interactions;linked data;knowledge bases;nanopublications;mi cropublications;evidence bases	Potential drug-drug interactions (PDDI) are a significant source of preventable drug-related harm. One contributing factor is that there is no standard way to represent PDDI knowledge claims and associated evidence in a computable form. The research we present in this paper addresses this problem by creating a new version of the Drug Interaction Knowledge Base, with scalable, interlinkable repositories for PDDI evidence and PDDI knowledge claims.	computable function;interaction;knowledge base;scalability	Jodi Schneider;Mathias Brochhausen;Samuel Rosko;Paolo Ciccarese;William R. Hogan;Daniel C. Malone;Yifan Ning;Timothy W Clark;Richard D. Boyce	2015			computer science;knowledge management;linked data;data mining;world wide web	NLP	-39.93220179834397	2.062069799672227	68895
51bf8f1d4b8fa36d3d811914d595f8e5d132571e	single event upset rate determination for 65 nm sram bit-cell in leo radiation environments		a Department of Electrical Engineering, COMSATS Institute of Information Technology, Islamabad, Pakistan b Skobeltsyn Institute of Nuclear Physics, Moscow State University, Moscow, Russia c Department of Electronics Engineering, UFMG, Belo Horizonte, Brazil d University of Lahore, Islamabad Campus, Islamabad, Pakistan e Department of Electrical and Computers Engineering, University of Auckland, New Zealand f China Academy of Space Technology (CAST), Beijing, China g Department of Physics, International Islamic University, Islamabad, Pakistan	academy;bit cell;cmos;dynamical simulation;electrical engineering;electronic engineering;entity–relationship model;flops;jing;ke software;linux;msu lossless video codec;onera;single event upset;static random-access memory;technical support;traffic collision avoidance system	Muhammad Sajid;N. G. Chechenin;Frank Sill;Usman Ali Gulzari;Muhammad Usman Butt;Zhu Ming;E. U. Khan	2017	Microelectronics Reliability	10.1016/j.microrel.2017.07.084	static random-access memory;robustness (computer science);electronic engineering;engineering;single event upset;radiation;real-time computing;bit cell;electromagnetic shielding;cmos	DB	-45.46899390402358	-7.499836912643265	68943
4e79438ee5a816021ad37fd654a80d152b2d5d8b	what constitutes successful format conversion? towards a formalization of 'intellectual content'		Recent work in the semantics of markup languages may offer a way to achieve more reliable results for format conversion, or at least a way to state the goal more explicitly. In the work discussed, the meaning of markup in a document is taken as the set of things accepted as true because of the markup’s presence, or equivalently, as the set of inferences licensed by the markup in the document. It is possible, in principle, to apply a general semantic description of a markup vocabulary to documents encoded using that vocabulary and to generate a set of inferences (typically rather large, but finite) as a result. An ideal format conversion translating a digital object from one vocabulary to another, then, can be characterized as one which neither adds nor drops any licensed inferences; it is possible to check this equivalence explicitly for a given conversion of a digital object, and possible in principle (although probably beyond current capabilities in practice) to prove that a given transformation will, if given valid and semantically correct input, always produce output that is semantically equivalent to its input. This approach is directly applicable to the XML formats frequently used for scientific and other data, but it is also easily generalized from SGML/XMLbased markup languages to digital formats in general; at a high level, it is equally applicable to document markup, to database exchanges, and to ad hoc formats for high-volume scientific data. Some obvious complications and technical difficulties arising from this approach are discussed, as are some important implications. In most real-world format conversions, the source and target formats differ at least somewhat in their ontology, either in the level of detail they cover or in the way they carve reality into classes; it is thus desirable not only to define what a perfect format conversion looks like, but to quantify the loss or distortion of information resulting from the conversion. 1 This paper is based on the paper given by the authors at the 6th International Digital Curation Conference, December 2010; received December 2010, published March 2011. The International Journal of Digital Curation is an international journal committed to scholarly excellence and dedicated to the advancement of digital curation across a wide range of sectors. ISSN: 1746-8256 The IJDC is published by UKOLN at the University of Bath and is a publication of the Digital Curation Centre. 154 What Constitutes Successful Format Conversion?		C. M. Sperberg-McQueen	2011	IJDC	10.2218/ijdc.v6i1.179	ruleml;computer science;data mining;database;world wide web;information retrieval	DB	-44.605965877492004	3.513415064188578	68963
4289f8cf361020e5d51075e4689734d9c4f1f597	swapit: a multiple views paradigm for exploring associations of texts and structured data	relational data;document maps;customer relationship management;text mining;visual interaction;information space;multiple views;ontologies;business intelligence;visual interfaces;structured data	Visualization interfaces that offer multiple coordinated views on a particular set of data items are useful for navigating and exploring complex information spaces. In this paper we address the problem of mining text information which is associated with structured data from relational data sources. We present a multi-view paradigm that closely integrates the analysis of unstructured text data with related structured data sets. Our concept brings together views on text similarity, text categories, and associated relational attributes for application fields like customer relationship management or business intelligence. A prototype is presented that exemplarily implements our multi-view framework.	customer relationship management;data model;programming paradigm;prototype;text corpus	Andreas Becks;Christian Seeling	2004		10.1145/989863.989894	text mining;data model;relational database;computer science;ontology;data science;data mining;database;business intelligence;world wide web	DB	-36.140209725165484	2.4866134152356008	68976
b62defa0a891b2c2131d93907697cab97a340a7f	single machine problem with multi-rate-modifying activities under a time-dependent deterioration		1 College of Information Science and Engineering, Northeastern University, and State Key Laboratory of Synthetical Automation for Process Industries (Northeastern University) Shenyang, Wenhua Road, Heping District, No. 11, Lane 3, Shenyang, Liaoning 110819, China 2Department of Management and Marketing, The Hong Kong Polytechnic University, Hong Kong 3 Department of Industrial and Systems Engineering, The Hong Kong Polytechnic University, Hong Kong	automation;information science;systems engineering	Min Huang;Huaping Wu;Vincent Cho;Andrew W. H. Ip;Xingwei Wang;C. K. Ng	2013	J. Applied Mathematics	10.1155/2013/135610	mathematical optimization;mathematics;branch and bound;algorithm;branch and cut	DB	-44.320190858766395	-8.99495338275108	69041
b054c8d77a416a929687662d0a91cb249e322176	active management of scientific data	mesoscale meteorology data driven applications scientific computing web based data management grid services metadata catalog;gestion informacion;dato observacion;personal information management;red www;scientific data;data management;reseau web;distributed computing;service web;information space;cataloguing;grid services;geophysics computing internet scientific information systems meta data cataloguing meteorology grid computing;web service;gestion donnee scientifique;data distribution;internet;geophysics computing;environnement relie pour decouverte scientifique;internet weather forecasting new products catalog meteorology predictive models atmosphere educational institutions portals meteorological radar doppler radar;mesoscale meteorology;scientific data management data distribution schemes mesoscale meteorology mylead personalized information management tool globus metadata catalog service distributed computation environments grid service web services;web based data management;information management;grid service;scientific computing;calculo repartido;world wide web;meta data;metadata catalog;modele donnee;distributed computing environment;donnee observation;gestion information;grid computing;meteorology;calcul reparti;data driven applications;observation data;scientific information systems;servicio web;data models	Sophisticated data-distribution schemes and recent developments in sensors and instruments that can monitor the lower kilometers of the atmosphere at high levels of resolution have rapidly expanded the quantity of information available to mesoscale meteorology. The myLEAD personalized information-management tool helps geoscience users make sense of this vastly expanded information space. MyLEAD extends the general globus metadata catalog service and leverages a well-known general and extensible schema. Its orientation makes it an active player in large-scale distributed computation environments characterized by interacting grid and Web services.	computation;distributed computing;interaction;personalization;sensor;web service	Beth Plale;Jay Alameda;Robert Wilhelmson;Dennis Gannon;Shawn Hampton;Albert L. Rossi;Kelvin Droegemeier	2005	IEEE Internet Computing	10.1109/MIC.2005.4	web service;data modeling;the internet;mesoscale meteorology;data management;computer science;personal information management;data mining;database;distributed computing;information management;metadata;law;world wide web;grid computing;distributed computing environment;data	HPC	-40.9788360746594	-2.278371599712077	69105
4af38bf640b5d433774b02972faa09a23af44741	pddl4j: a planning domain description library for java		PDDL4J (Planning Domain Description Library for Java) is an open source toolkit for Java cross-platformdevelopersmeant (1) to provide state-of-theart planners based on the PDDL language, and (2) to facilitate research works on new planners. In this article, we present an overview of the Automated Planning concepts and languages. We present some planning systems and their most significant applications. Then, we detail the PDDL4J toolkit with an emphasis on the available informative structures, heuristics and search algorithms. ARTICLE HISTORY Received 5 December 2016 Accepted 9 July 2017	automated planning and scheduling;heuristic (computer science);information;java;open-source software;planning domain definition language;search algorithm	Damien Pellier;Humbert Fiorino	2018	J. Exp. Theor. Artif. Intell.	10.1080/0952813X.2017.1409278	natural language processing;machine learning;computer science;business system planning;heuristic;heuristics;planning domain definition language;java;search algorithm;artificial intelligence	AI	-45.951204941060844	-2.0740389101811547	69305
55f9813baf0af06c6049d064b64b9d86d853ca55	knova: a reference architecture for knowledge-based visual analytics		The increase in storage capacity and the progress in information technology today lead to a rapid growth in the amount of stored data. In increasing amounts of data, gaining insight becomes rapidly more difficult. Existing automatic analysis approaches are not sufficient for the analysis of the data. The problem that the amount of stored data increases faster than the computing power to analyse the data is called information overload phenomenon. Visual analytics is an approach to overcome this problem. It combines the strengths of computers to quickly identify re-occurring patterns and to process large amounts of data with human strengths such as flexibility, intuition, and contextual knowledge. In the process of visual analytics knowledge is applied by expert users to conduct the analysis. In many settings the expert users will apply the similar knowledge continuously in several iterations or across various comparable analytical tasks. This approach is time consuming, costly and possibly frustrating for the expert users. Therefore a demand for concepts and methods to prevent repetitive analysis steps can be identified. This thesis presents a reference architecture for knowledge-based visual analytics systems, the KnoVA RA, that provides concepts and methods to represent, extract and reapply knowledge in visual analytic systems. The basic idea of the reference architecture is to extract knowledge that was applied in the analysis process in order to enhance or to derive automated analysis steps. The objective is to reduce the work-load of the experts and to enhance the traceability and reproducibility of results. The KnoVA RA consist of four parts: a model of the analysis process, the KnoVA process model, a meta data model for knowledge-based visual analytics systems, the KnoVA meta model, concepts and algorithms for the extraction of knowledge and concepts and algorithms for the reapplication of knowledge. With these concepts, the reference architecture servers as a blueprint for knowledge-based visual analytics systems. To create the reference architecture, in this thesis, two real-world scenarios from different application domains (automotive and healthcare) are introduced. These scenarios provide requirements that lead to implications for the design of the reference architecture. On the example of the motivating scenarios the KnovA RA is implemented in two visual analytics applications: TOAD, for the analysis of message traces of in-car bus communication networks and CARELIS, for the aggregation of medical records on an interactive visual interface. These systems illustrate the applicability of the KnoVA RA across different analytical challenges and problem classes.	algorithm;application domain;blueprint;computer;data model;information overload;iteration;metamodeling;process modeling;reference architecture;requirement;scarab of ra;toad;telecommunications network;traceability;tracing (software);traffic collision avoidance system;visual analytics	Stefan Flöring	2013			data science;knowledge extraction;visual analytics;reference architecture;metadata modeling;data mining;information processing;cultural analytics;analytics;computer science;phenomenon	Visualization	-34.62667800138369	-4.609494073304895	69316
4203a924ac542c76d03f50ffba0cf633bbec79e0	automatic climate classification of environmental science literature		Climate type is one of the potentially most relevant pieces of metadata for identifying studies in evidence-based environmental management. In this paper, we propose a method for automatically predicting the climate type in environmental science literature using NLP techniques, relative to a pre-existing set of climate type categories. Our main approaches combine toponym detection and resolution using two different resources with support vector machines. The results show great promise, but also further challenges, for using NLP to extract information from the vast and rapidly growing collection of environmental sciences literature.	environmental resource management;image resolution;natural language processing;support vector machine	Jared Willett;David Martínez;J. Angus Webb;Timothy Baldwin	2013			environmental science;climate classification;environmental resource management;management science	NLP	-40.489874796407044	-2.440063293741208	69436
64a9ec97fa879aed397cc5852997f73429e70f29	research data reusability: conceptual foundations, barriers and enabling technologies	data understandability;metadata;tacit knowledge;data reuse;data representation;data discoverability;explicit knowledge;relational thinking;data abstraction;data publishing	High-throughput scientific instruments are generating massive amounts of data. Today, one of the main challenges faced by researchers is to make the best use of the world’s growing wealth of data. Data (re)usability is becoming a distinct characteristic of modern scientific practice. By data (re)usability, we mean the ease of using data for legitimate scientific research by one or more communities of research (consumer communities) that is produced by other communities of research (producer communities). Data (re)usability allows the reanalysis of evidence, reproduction and verification of results, minimizing duplication of effort, and building on the work of others. It has four main dimensions: policy, legal, economic and technological. The paper addresses the technological dimension of data reusability. The conceptual foundations of data reuse as well as the barriers that hamper data reuse are presented and discussed. The data publication process is proposed as a bridge between the data author and user and the relevant technologies enabling this process are presented.	meteorological reanalysis;throughput;usability	Costantino Thanos	2017	Publications	10.3390/publications5010002	data modeling;data proliferation;data quality;computer science;knowledge management;data warehouse;data mining;database	HPC	-37.656884912445115	-1.371862204536697	69792
288f95798823d2a9e157480b955b0ab06d70a486	research of big data space-time analytics for clouding based contexts-aware iov applications		With the rapid development of Internet of Things and Big Data analysis, the computing mode of the 21st century is undergoing profound reform. But these technologies bring great challenges such as more multiple-dimensional and more numerous information with wide-area and heterogeneous sensor networks to classical context-aware frameworks at the same time. The IOV (Internet of Vehicles) applications is one kind of the typical IOT (Internet of Things) applications and the data involved in them are more and more big which need more complex querying or analyzing methods. Therefore, we have researched the big data problems in IOV applications and put forward the clouding based big data space-time analytics methods for contexts storing and contexts querying to improve the analysis efficiency of the systems. By these methods, we can improve the capability of the complex IOV applications for dealing the numerous contexts.	big data;dataspaces;giove;internet of things	Di Zheng;Kerong Ben;Hongliang Yuan	2014	2014 Second International Conference on Advanced Cloud and Big Data	10.1109/CBD.2014.26	computer science;data mining;internet privacy;world wide web	DB	-37.33678107208291	-5.6805358270970165	70046
be79c3be0f7419febf43ed701e5abeb553d0d86d	developing a general purpose olap client prototype using xml for analysis		"""Today's information systems mostly rely on relational databases for transactional data storage and manipulation. Throughout the years, massive amounts of data have been gathered. Those data, if properly analyzed, could serve as a basis for strategic decisions. For the purposes of analysis data is transformed from relational model (database) into dimensional model and stored in a data warehouse. Informations gained from gathered data provide an advantage that, in the conditions of sharp business competition, could be distinguishing. Followed by intense hardware development, that was the main reason for rapid data warehousing development and acceptance in the past years. However, data warehousing platform includes more than relational database holding data in a dimensional model: exhaustive data warehousing platform comprises of data warehouse server, OLAP (On-Line Analytical Processing) server and some data mining functionality. OLAP stands for a type of application that attempts to facilitate multidimensional (i.e., data that has been aggregated into various categories or """"dimensions"""") analysis. OLAP should help a user synthesize enterprise information through comparative, personalized viewing as well as through analysis of historical and projected data. The main characteristic of OLAP systems is providing multi-dimensional view of the data and answers to queries at high response rates. Unlike relational databases that show some degree of uniformity through the SQL and ODBC standards, there are no generally accepted standards for communicating with OLAP databases. Such situation makes development of OLAP client tools more complicated and expensive. This results in platform specific and more expensive products that are, therefore, less acceptable to the end users. In this article two major initiatives for OLAP standardizations are discussed. Also, an OLAP client tool based on the XML for Analysis (XMLA) protocol with a sufficient set of functionalities was developed."""	circuit complexity;computer data storage;data mining;dynamic data;information system;online analytical processing;open database connectivity;personalization;prototype;relational database;relational model;sql;server (computing);xml for analysis	Igor Mekterovic;Mirta Baranovic	2005			online analytical processing;world wide web;business logic;database;web service;dimensional modeling;xml for analysis;data access;data warehouse;data structure;computer science	DB	-34.6670236704051	-0.08446392835981965	70427
eaf85cd2480c69314bb6839521ed83ae6566c190	a retrospective on dose: an interpretive approach to structure editor generation	text editor;structure editor generation;compilateur;programming environment;implementation;ingenieria logiciel;compiler;software engineering;medio ambiente programacion;ejecucion;interpreteur;editor texto;genie logiciel;editeur texte;computer science;interpreter;lnaguage based editor;interprete;compilador;environnement programmation	"""DOSE is unique among structu.re editor generators in its interpretive appro臼h. This approach leads to v町y fast turn-around time for changes and provides multi-language facilities for no additional effort or cost. This article comp缸es the interpretive approach to the comp i1ation approach of other structu.re editor generators. It describes some of the design and implementation decisions made and remade during this project and the lessons learned. It emphasizes 由e advantages and disadvantages of DoSE wi由自S归ct to other structure editing systems. Prof. Kaiser is supported in part by grants from AT&T, mM, Siemens and Sun, in part by the Center of Advanced Technology and by the Center for Telecommunications Re回arch. and in part by a DEC Faculty Award. The DOSE sys臼m was developed at Siemens Research and Technology Laboratory in Princeton, NJ. *Carnegie Mellon University, Software Engineering Institute, 5创)() Forbes Avenue, Pittsburgh, PA 15213. Dr. Feil町 is supported by the Department of Defense. +AT&T Bell Laboratories, 600 Mountain Avenue, Murray Hil1, NJ 07974. =Xerox Corporation, Joseph Wi1son Research Center, 8∞ Phillips Road. Webst町, NY 14580. SOFT飞飞'ARE-PRλCTICE 飞\D EXPERIE :'IJ CE. \'OL 18(8). ï3J-ì48 (:\L'GCS丁 1(188) A Retrospective on DOSE: An Interpretive l\pproach to Structure Editor Generation GAIL E. KAISER Co/umbia L""""ni'l.'ersity , Department 01 ('omputer Scie71ce , 450 Computer Science Rui/di咽, .飞 ezι: )ork, _飞飞 • J002ï, C.S--\. PETER H. FEILER Camegre .\Je/lon L""""nit'ersity , 8呐饥:are E71gineen'lIg lnstitute , 5000 For古时 .. h'enue , Pittsburgh , P.4. J 521J, C.S--\. FAHI l\1EH ]ALlLI AT&T Re /l Laboraton'es , 600 .\lou71tain .4.u71ue, .\Jurray Hill, _飞J 07974, CSλ."""	computer science;grace murray hopper award;ijustine;ork;software engineering institute;structure editor	Gail E. Kaiser;Peter H. Feiler;Fahimeh Jalili;Johann H. Schlichter	1988	Softw., Pract. Exper.	10.1002/spe.4380180803	compiler;interpreter;computer science;engineering;software engineering;programming language;implementation;algorithm	SE	-46.669021107037565	-4.712337550377183	70433
6cc321cb883bb5f022fd115350a4ff435451ed60	semantic rule-based equipment diagnostics		Industrial rule-based diagnostic systems are often data-dependant in the sense that they rely on specific characteristics of individual pieces of equipment. This dependence poses significant challenges in rule authoring, reuse, and maintenance by engineers. In this work we address these problems by relying on Ontology-Based Data Access: we use ontologies to mediate the equipment and the rules. We propose a semantic rule language, sigRL, where sensor signals are first class citizens. Our language offers a balance of expressive power, usability, and efficiency: it captures most of Siemens data-driven diagnostic rules, significantly simplifies authoring of diagnostic tasks, and allows to efficiently rewrite semantic rules from ontologies to data and execute over data. We implemented our approach in a semantic diagnostic system, deployed it in Siemens, and conducted experiments to demonstrate both usability and efficiency.	data access;experiment;expressive power (computer science);first-class function;logic programming;ontology (information science);rewrite (programming);usability	Gulnar Mehdi;Evgeny Kharlamov;Ognjen Savkovic;Guohui Xiao;Elem Güzel Kalayci;Sebastian Brandt;Ian Horrocks;Mikhail Roshchin;Thomas A. Runkler	2017		10.1007/978-3-319-68204-4_29	data mining;computer science;rule-based system;expressive power;database;reuse;ontology (information science);usability;data access;first class	AI	-35.270764507346115	3.7203309370811817	70531
16ad60210cccad43666040132fe98f5993fd212d	a framework for gis and imagery data fusion in support of cartographic updating	hypothesis generation;complex objects;information sources;decision level fusion;semantic network;data fusion;aerial image;image interpretation;semantic net;gis;levels of abstraction;knowledge based image interpretation;image analysis;quality control;knowledge base	The task to update map databases regularly using images calls for automation. Within this process GIS and image data have to be combined. The different nature and content of these information sources prevent a direct comparison. In this paper an approach for the combination of GIS data and aerial images is proposed. We make use of a knowledge base, modelling the objects which are expected to be found. This leads to a refined image interpretation process and enables a revision of the GIS data. The focus is thereby on settlement and industrial areas which are detected automatically from images. The use of a semantic network allows the formulation of such complex objects expected in the image and supports decisions on a high level of abstraction. Reasoning is supported by using the existing GIS information for hypothesis generation. The decisions combine different clues which evolve during the image analysis process. The analysis generates a hierarchical scene description for the area of study and evaluates the correctness of the GIS data.	cartography;geographic information system	Martin Weis;Sönke Müller;Claus-E. Liedtke;Martin Pahl	2005	Information Fusion	10.1016/j.inffus.2004.08.001	enterprise gis;computer vision;knowledge base;quality control;image analysis;computer science;artificial intelligence;data science;machine learning;data mining;sensor fusion;semantic network	Robotics	-35.4133015624282	-3.84187695424346	70757
b2437fe631196f0bd6c2d8cec4254129440ed26c	pds program committee	program committee	R. Arpaci-Dusseau, University of Wisconsin, USA S. Bagchi, Purdue University, USA C. Borcea, New Jersey Institute of Technology, USA P. Buchholz, University of Dortmund, Germany G. Ciardo, University of California Riverside, USA C. Constantinescu, Intel, USA M. Cukier, University of Maryland, USA M. Dacier, Eurecom, France T. Dohi, University of Hiroshima, Japan J. Dugan, University of Virginia, USA P. Ezhilchevan, University of Newcastle, UK J. da Silva Fraga, UFSC Florianopolis, Brazil S. Garg, Avaya Labs, USA R. German, University of Erlangen, Germany S. Gokhale, University of Connecticut, USA K. Goseva-Popstojanova, West Virginia University, USA B. Haverkort, University of Twente, Netherlands G. Heijenk, University of Twente, Netherlands S. Jarvis, University of Warwick, UK Z. Kalbarczyk, University of Illinois Urbana-Champaign, USA V. Kalogeraki, University of California Riverside, USA K. Kanoun, LAAS-CNRS, France K. Keeton, Hewlett Packard Labs, USA W. Knottenbelt, Imperial College, UK B. Littlewood, City University of London, UK M. Massink, CNUCE, Italy J.-P. Martin-Flatin, UQAM, Canada R. Maxion, Carnegie Mellon University, USA V. Mendiratta, Lucent Bell Labs, USA B. Murphy, Microsoft Research, UK P. Murray, Hewlett Packard Labs, UK D. Nicol, University of Illinois Urbana-Champaign, USA A. Somani, Iowa State University, USA W. Sanders, University of Illinois Urbana-Champaign, USA J. Silva, University of Coimbra, Portugal L. Spainhower, IBM, USA A. Tai, IA Tech Inc., USA D. Tang, Sun Microsystems, USA A. Tanner, IBM Research, Switzerland M. Telek, University of Budapest, Hungary K. Wolter, Humboldt University Berlin, Germany	ibm research;local area augmentation system;microsoft research;nicol prism;switzerland;tanner graph	Saurabh Bagchi;Peter Buchholz;Cristian C Constantinescu	2006		10.1109/DSN.2006.57	computer science	ML	-46.05910739327275	-9.681110688197183	70773
6cc42e9bedfc68339db197ac0d216fa5c5407556	semantic middleware for e-science knowledge spaces	content management;metadata management;e science;computer model;data management;contextual information;relational database;semantic web technology;distributed environment;semantic web;middleware;scientific communication;institutional repository	"""The Tupelo semantic content management middleware implements Knowledge Spaces that enable scientists to locate, use, link, annotate, and discuss data and metadata as they work with existing applications in distributed environments. Tupelo is built using a combination of commonly-used Semantic Web technologies for metadata management, content management technologies for data management, and workflow technologies for management of computation, and can interoperate with other tools using a variety of standard interfaces and a client and desktop API. Tupelo's primary function is to facilitate interoperability, providing a Knowledge Space """"view"""" of distributed, heterogeneous resources such as institutional repositories, relational databases, and semantic web stores. Knowledge Spaces have driven recent work creating e-Science cyberenvironments to serve distributed, active scientific communities. Tupelo-based components deployed in desktop applications, on portals, and in AJAX applications interoperate to allow researchers to develop, coordinate and share datasets, documents, and computational models, while preserving process documentation and other contextual information needed to produce a complete and coherent research record suitable for distribution and archiving."""	ajax (programming);application programming interface;archive;coherence (physics);computation;computational model;desktop computer;documentation;e-science;interoperability;knowledge space;middleware;portals;relational database;semantic web;spaces	Joe Futrelle;Jeff Gaynor;Joel Plutchak;James D. Myers;Robert E. McGrath;Peter Bajcsy;Jason Kastner;Kailash Kotwani;Jong Sung Lee;Luigi Marini;Rob Kooper;Terrence M. McLaren;Yong Liu	2009		10.1145/1657120.1657124	semantic grid;computer science;social semantic web;data mining;semantic web stack;database;world wide web	HPC	-41.200720806013145	0.6943402300234357	70892
173094daf6506ca89c7a527a2e5f2805bb0764cb	a semantic model for movement data warehouses	social media posts;settore inf 01 informatica;analytical queries;multidimensional model;semantic annotations;lod;moving object trajectories	Despite recent progresses in methods for processing data about the movement of objects in the geographic space, some fundamental issues remain unresolved. One of them is how to describe movement segments (e.g., semantic trajectories, episodes like stops and moves) and diverse movement patterns (e.g., moving clusters, hotel-restaurant-shop-hotel), with formal semantic descriptions. Another issue is how to arrange descriptive data and measures in a Movement Data Warehouse (MDW) for powerful information analyses and reasonable performance. This paper introduces general definitions for movement segments, movement patterns, their categories and hierarchies. The proposed constructs are semantically enriched with references to concepts (categories) and/or instances of these concepts (objects) arranged in distinct hierarchies. Based on these constructs, we propose a semantic multidimensional model for MDW. A case study illustrates the expressiveness of the proposal for analyzing movement data collected via social media and semantically enriched with Linked Open Data (LOD).	linked data;semantics (computer science);social media	Renato Fileto;Alessandra Raffaetà;Alessandro Roncato;Juarez A. P. Sacenti;Cleto May;Douglas Klein	2014		10.1145/2666158.2666180	computer science;level of detail;data mining;database	ML	-35.82976072271153	-3.105237686545822	71213
4aed4f43d64583ed0d917d3b1dcaefca509bf601	innovative approaches for efficiently warehousing complex data from the web	olap;complex data;data warehouse	Research in data warehousing and OLAP has produced important technologies for the design, management and use of information systems for decision support. With the development of Internet, the availability of various types of data has increased. Thus, users require applications to help them obtaining knowledge from the Web. One possible solution to facilitate this task is to extract information from the Web, transform and load it to a Web Warehouse, which provides uniform access methods for automatic processing of the data. In this chapter, we present three innovative researches recently introduced to extend the capabilities of decision support systems, namely (1) the use of XML as a logical and physical model for complex data warehouses, (2) associating data mining to OLAP to allow elaborated analysis tasks for complex data and (3) schema evolution in complex data warehouses for personalized analyses. Our contributions cover the main phases of the data warehouse design process: data integration and modeling and user driven-OLAP analysis.	data mining;decision support system;information system;internet;online analytical processing;personalization;schema evolution;world wide web;xml	Fadila Bentayeb;Nora Maïz;Hadj Mahboubi;Cécile Favre;Sabine Loudcher;Nouria Harbi;Omar Boussaïd;Jérôme Darmont	2016	CoRR		data quality;dimensional modeling;online analytical processing;computer science;data science;data warehouse;data mining;database;complex data type	DB	-36.10509582871149	-0.5530418350323889	71247
4c7c9a46f7f21a7c10243e60a58255fb3a961d3e	adapting the electronic laboratory notebook for the semantic era	data transmission;scientific information systems internet middleware;general and miscellaneous mathematics computing and information science;data repository;department of energy;document types electronic laboratory notebooks scientific annotation middleware collaboration semantic data management;collaboration;data management;scientific annotation middleware;closed system;computer networks;computer architecture;semantic model;standardized terminology;web based notebook system;internet;notebook service;semantic data management;system design;open source electronic laboratory notebook;semantic model open source electronic laboratory notebook collaborative notebook system web based notebook system data repository scientific annotation middleware notebook service;collaborative notebook system;middleware;electronic laboratory notebooks;laboratories middleware problem solving collaborative work instruments resource description framework collaboration intellectual property protection displays;problem solving environment;scientific information systems;open source;pacific northwest	The open source electronic laboratory notebook (ELN) is a collaborative, distributed, Web-based notebook system, designed to provide researchers with a means to record and share their primary research notes and data. As with most electronic notebook (EN) systems, the ELN was originally designed as a closed system with its own data repository and implicit semantics. The scientific annotation middleware (SAM) project, a Department of Energy (DOE)-funded effort at Pacific Northwest and Oak Ridge National Laboratories, envisions a new model in which ENs are simply one application contributing to a much richer and semantically explicit record. Such a record would include, for example, data provenance, descriptive metadata, and annotations from a wide range of applications, problem solving environments, and agents. This paper reports the adaptation of the ELN client to use SAM and the development of an initial set of SAM-based notebook services and semantic model, and then discusses the advantages of such an architecture in creating federated, human- and machine-interpretable, electronic research records	closed system;file spanning;instrument control;laboratory information management system;middleware;open-source software;problem solving;requirement;research data archiving;semantic web;semantic data model;server (computing);user interface;vaxeln;wiki;world wide web	Tara D. Talbott;Michael Peterson;Jens Schwidder;James D. Myers	2005	Proceedings of the 2005 International Symposium on Collaborative Technologies and Systems, 2005.	10.1109/ISCST.2005.1553305	semantic data model;the internet;human–computer interaction;data management;computer science;operating system;middleware;database;closed system;information repository;world wide web;data transmission;systems design;collaboration	OS	-41.47370024485886	-1.9703934463698347	71379
28e8e10dd7c362227ee72ea8ff058f23cf3673c0	isrl: intelligent search by reinforcement learning in unstructured peer-to-peer networks	peer to peer network;unstructured peer to peer networks;reinforcement learning;index terms hint based search;indexing terms;artificial intelligent;performance improvement;p2p networks;cumulant;peer to peer;peer to peer networks	International Journal of Parallel, Emergent and Distributed Systems Publication details, including instructions for authors and subscription information: http://www.informaworld.com/smpp/title~content=t713729127 ISRL: intelligent search by reinforcement learning in unstructured peer-to-peer networks Xiuqi Li a; Jie Wu a; Shi Zhong b a Department of Computer Science and Engineering, Florida Atlantic University, Boca Raton, FL, USA b SDS-Data Mining Research, Yahoo! Inc., Sunnyvale, CA, USA	computer science;data mining;emergent;jie wu;peer-to-peer;reinforcement learning	Xiuqi Li;Jie Wu;Shi Zhong	2008	IJPEDS	10.1080/17445760701442176	index term;computer science;machine learning;data mining;distributed computing;world wide web;reinforcement learning;cumulant	ML	-42.91906942015993	-8.750158413648995	71404
011b16294646c1c1a99a11031af83b51cb0981d0	frequent pattern queries for flexible knowledge discovery	knowledge discovery			Francesco Bonchi;Fosca Giannotti;Dino Pedreschi	2004			knowledge extraction;k-optimal pattern discovery;data mining;computer science	ML	-38.201896242243045	-5.418853119489456	71539
293f36473d6b8acef60fc1b5d1c448abcd866619	collection management system (cms).	management system			David Bearman	1990	Archives and Museum Informatics	10.1007/BF02770073	computer science;management system	DB	-41.06137976390121	-4.775912854028471	71696
3c5154d3aa120eb8d597af7db512bef1068882e6	using visualizations to monitor changes and harvest insights from a global-scale logging infrastructure at twitter	log visualization;funnel analysis;interactive visualizations change monitoring global scale logging infrastructure twitter user activity logging data analysis internet products unified logging infrastructure information visualization log event monitoring visual funnel analysis;log analysis;session analysis;information visualization;funnel analysis information visualization visual analytics log analysis log visualization session analysis;data visualization twitter radar monitoring visualization vegetation data analysis;visual analytics;social networking online data analysis data visualisation	Logging user activities is essential to data analysis for internet products and services. Twitter has built a unified logging infrastructure that captures user activities across all clients it owns, making it one of the largest datasets in the organization. This paper describes challenges and opportunities in applying information visualization to log analysis at this massive scale, and shows how various visualization techniques can be adapted to help data scientists extract insights. In particular, we focus on two scenarios: (1) monitoring and exploring a large collection of log events, and (2) performing visual funnel analysis on log data with tens of thousands of event types. Two interactive visualizations were developed for these purposes: we discuss design choices and the implementation of these systems, along with case studies of how they are being used in day-to-day operations at Twitter.	anomaly detection;apache hadoop;batch processing;client (computing);client-side;data aggregation;data mining;data quality;data science;database;dictionary;event monitoring;exploit (computer security);gibbs sampling;information retrieval;information visualization;interactive visualization;interactivity;internet;iteration;log analysis;path analysis (computing);radar;sensor;synergy	Krist Wongsuphasawat;Jimmy J. Lin	2014	2014 IEEE Conference on Visual Analytics Science and Technology (VAST)	10.1109/VAST.2014.7042487	visual analytics;information visualization;computer science;data science;data mining;world wide web	Visualization	-35.818546968656385	-5.3966141336821085	71730
dca565f97128408679e17607216198442e8bd8b0	the design and realization of the spatial database for emergency earthquake damage assessment based on rs and gis	remote sensing technique;earthquake damage assessment;emergency response;damage assessment;chaotic communication;integrated database management system;disaster management;geographic information system;spatial data;image databases;database management systems;geographic information system technique;gis spatial database remote sensing images earthquake damage assessment;earthquakes;data mining;spatial database;spatial databases earthquakes geographic information systems image databases relational databases disaster management engines data mining remote sensing chaotic communication;spatial data engine;emergency earthquake damage assessment;engines;gis;visual databases database management systems earthquakes geographic information systems remote sensing;geographic information systems;remote sensing;spatial databases;integrated database management system spatial database emergency earthquake damage assessment remote sensing technique geographic information system technique seismic damage loss relational database management system spatial data engine china;relational database management system;it management;relational databases;seismic damage loss;china;images;visual databases	While an earthquake occurs especially in depressed areas or mountainous regions, the survey of damage loss becomes arduous because of chaos and awful communication condition. A solution is proposed that using RS (Remote Sensing) and GIS (Geographic Information System) techniques assesses seismic damage loss for emergency, which need establish a spatial database to store the necessary data. In this paper, the design of the spatial database is introduced. The database depended upon RDBMS (Relational Database Management System) and SDE (Spatial Data Engine), including images, vector files and tables which are sorted by types and organized by every earthquake events, divides into several sub-databases. According to the integrated database and its management system, emergency earthquake damage assessment could be realized. The constructive work provides much faster response and data service and is even more economical to a certain extent. A similar spatial database has been deployed at national commanding center for earthquake emergency response of China.	arcsde;chaos theory;geographic information system;reed–solomon error correction;relational database management system;spatial database	Long Wang;Xiaoqing Wang;Xiang Ding;Aixia Dou	2009	2009 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2009.5417834	relational database management system;computer science;data mining;database;geographic information system;computer security;spatial database;china;remote sensing	DB	-37.45377130656811	-3.649442173882701	71801
00f8c68571dc6408b07c0e605abcd9a3cb94a249	a survey on ontology-driven geographic information systems	storage management data integration decision making geographic information systems ontologies artificial intelligence open systems;ontologies geospatial analysis iso standards interoperability context semantics;iso standards;semantics;geospatial analysis;ontology driven geographic information systems gis domain decision making geospatial data reuse geospatial data sharing information integration information processing information sharing data management gis interoperability heterogeneity problem applications sharing data geographic data storage geographic data capture gis technological tools;ontologies;interoperability;context;data integration geographic information systems ontology	Geographic Information Systems (GIS) have become widely used for capturing, storing, integrating, analyzing and displaying various types of geospatial data. They have been applied to several fields such as transportation, education, business, tourism, urban planning, etc. Nowadays, with the advancement of GIS technological tools in capturing and storing geographic data, it leads to the challenge in managing data from diverse sources and storing in different formats at different levels of detail. In another word, two or more applications sharing data that represent the same thing may cause different types of heterogeneity problems. To enable GIS interoperability and data management, ontologies which are considered as powerful tools for information sharing, information processing and information integration are utilized in this context. This paper discusses how ontologies can be employed to resolve heterogeneities in order to enhance interoperability and support geospatial data sharing and reuse as well as facilitate decision making in GIS domain.	geographic information system;information processing;interoperability;level of detail;ontology (information science)	Waralak V. Siricharoen;Udsanee Pakdeetrakulwong	2014	2014 Fourth International Conference on Digital Information and Communication Technology and its Applications (DICTAP)	10.1109/DICTAP.2014.6821679	local information systems;interoperability;enterprise gis;geospatial metadata;geospatial pdf;geoportal;telecommunications;computer science;knowledge management;ontology;geospatial analysis;gis and public health;data mining;database;semantics;geographic information system;geographic information systems in geospatial intelligence	DB	-37.07131065278967	-2.062586224201416	72239
10889b5c5fdffdd545277cb30ce8073d34f9fa79	teaching an old elephant new tricks	elephants;data warehousing;data structure	In recent years, column stores (or C-stores for short) have e merged as a novel approach to deal with read-mostly data warehousin g applications. Experimental evidence suggests that, for cert ain types of queries, the new features of C-stores result in orders of mag nitude improvement over traditional relational engines. At the sa me time, some C-store proponents argue that C-stores are fundamenta lly different from traditional engines, and therefore their benefi ts cannot be incorporated into a relational engine short of a complete rewrite. In this paper we challenge this claim and show that many of the benefits of C-stores can indeed be simulated in traditional e gin s with no changes whatsoever. We then identify some limitatio ns of our “pure-simulation” approach for the case of more compl ex queries. Finally, we predict that traditional relational e ngines will eventually leverage most of the benefits of C-stores nativel y, as is currently happening in other domains such as XML data.	c-store;computer emergency response team;happened-before;mag technology co.;relational database;rewrite (programming);simulation;xml	Nicolas Bruno	2009	CoRR		data structure;computer science;data science;data warehouse;data mining;database	DB	-34.24130121022261	0.3479495053709689	72354
be84ad0745aa8422370f339fdbb76fdba8f2a135	arqiologist: an integrated decision support tool for lead optimization		This paper describes ArQiologist, a Web-based tool that integrates chemical, analytical, biological, and computational data to facilitate decision support for lead optimization at ArQule. It features an easy-to-use graphical query builder that allows queries to be saved, reused, and shared by researchers. Query results can be viewed with built-in data browsers or exported with structures to external applications such as Microsoft Excel or Spotfire for further analysis.		Atipat Rojnuckarin;Daniel A. Gschwend;Sergio H. Rotstein;David S. Hartsough	2005	Journal of chemical information and modeling	10.1021/ci049880h	bioinformatics;data science;data mining;decision support system;computer science	DB	-35.77560388724068	-0.3835774505917482	72486
96a3f301cb3f9720f57b36e5517f178d6876e07e	a parallel library for social media analytics		Social media analysis is a fast growing research area aimed at extracting useful information from huge amounts of data generated by social media users. This work presents a Java library, called ParSoDA (Parallel Social Data Analytics), which can be used for developing parallel data analysis applications based on the extraction of useful knowledge from large dataset gathered from social networks. The library aims at reducing the programming skills necessary to implement scalable social data analysis applications. To reach this goal, ParSoDA defines a general structure for a social data analysis application that includes a number of configurable steps, and provides a predefined (but extensible) set of functions that can be used for each step. The paper describes the ParSoDA library and presents two case studies to assess its usability and scalability.	cloud computing;data acquisition;flickr;high-level programming language;java;parallel computing;scalability;social data analysis;social media analytics;social network;speedup;usability	Loris Belcastro;Fabrizio Marozzo;Domenico Talia;Paolo Trunfio	2017	2017 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCS.2017.105	social media analytics;data mining;data science;data modeling;social data analysis;usability;social media;social network;data analysis;analytics;computer science	HPC	-35.72294654185594	-5.023261002478231	73006
13a05696d1e225477231cc7cd51fefff53e76c2b	towards a wiki interchange format (wif)		Wikis tend to be used more and more in world-wide, intranet and increasingly even in personal settings. Current wikis are data islands. They are open for everyone to contribute, but closed for machines and automation. In this paper we define a wiki interchange format (WIF) that allows data exchange between wikis and related tools. Different from other approaches, we also tackle the problem of page content and annotations. The linking from formal annotations to parts of a structured text is analysed and described.	intranet;microsoft outlook for mac;prototype;requirement;structured text;wiki	Max Völkel;Eyal Oren	2006			acoustics;continuous wave;rf probe;interrogation;computer science	DB	-40.66605402840511	3.595446763591134	73467
0bdb2d18234cbd4c8ef62632288fdf80a6098cbc	curvature flow based 3d surface evolution model for polyp detection and visualization in ct colonography	mean curvature flow;ct colonography;colon cancer;computerized tomography	1 Computer Vision & Image Processing (CVIP) Laboratory Department of Electrical & Computer Engineering, University of Louisville Louisville, Kentucky 40292, USA {dqchen,farag}@cvip.louisville.edu 2 Department of Medical Imaging, Jewish Hospital & St. Mary’s Healthcare Louisville, Kentucky 40202, USA 3 Division of Gastroenterology/Hepatology, Department of Medicine, University of Louisville Louisville, Kentucky 40292, USA	algorithm;ct scan;colon classification;computer engineering;computer vision;concave function;evolution;image processing;medical imaging;sensor;synthetic intelligence;virtual colonoscopy	Dongqing Chen;Aly A. Farag;M. Sabry Hassouna;Robert Falk;Gerald W. Dryden	2008		10.1007/978-3-540-70778-3_8	computer vision;mathematics;optics;engineering drawing	Vision	-47.04147402381818	-9.378643058918206	73545
e0b0f9dcfd0639c3be50663666f955396ba60538	call for papers - special issue of management science: business analytics: submission deadline: september 16, 2012 expected publication date: first quarter 2014		"""Organizations today have access to enormous data sets, and sophisticated business analytical tools are needed to harness the tremendous potential of these data to improve day-to-day decision making. To encourage further developments in this rapidly growing area of interdisciplinary research, Management Science will publish a special issue dedicated to business analytics. Applications of business analytics extend to nearly all managerial functions in an organization. Using microlevel data, mathematical models can become more realistic, be validated more easily, and match more closely true behavioral phenomena. Through business analytics, historical data can be used to discover relationships among important elements in an organization's environment, make more formal statistical inferences regarding the structure or strength of these relationships, and improve actions taken by the organization. Because these relationships are often not static, the entire cycle of identification, estimation, prediction, and decision making is repeated frequently and on an ongoing basis, leading to real-time and highly targeted data-driven decisions. We envision business analytics applied to many domains, including, but surely not limited to: digital market design and operation; network and social-graph analysis; pricing and revenue management; targeted marketing and customer relationship management; fraud and security; sports and entertainment; retailing to healthcare to financial services to many other industries. We seek novel modeling and empirical work which includes, among others, probability modeling, structural empirical models, and/or optimization methods. Submission: Please submit your manuscript online via <ext-link ext-link-type=\""""uri\"""" xmlns:xlink=\""""http://www.w3.org/1999/xlink\"""" xlink=\""""http://mc.manuscriptcentral.com/ms\""""http://mc.manuscriptcentral.com/ms</ext-link> You must select \""""Special Issue\"""" as the Manuscript Type in Step 1 and select one of the issue's four coeditors in Step 5. Manuscripts will generally be assigned to one of the Associate Editors for this issue (see below; please recommend three in Step 4), but you may also recommend guest Associate Editors."""		Dimitris Bertsimas;Eric T. Bradlow;Noah Gans;Alok Gupta	2012	Management Science	10.1287/mnsc.1120.1596		Theory	-37.21087069386364	-9.67115233920733	73548
60d4d7ccea50e05b1d7e019e1285234bc04486a6	arminer: a data mining tool based on association rules	interestingness;data mining;negative item;association rule;concept hierarchy;system architecture;data transfer	In this paper, ARMiner, a data mining tool based on association rules, is introduced. Beginning with the system architecture, the characteristics and functions are discussed in details, including data transfer, concept, hierarchy generalization, mining rules with negative items and the re-development of the system. An example of the tool’s application is also shown. Finally, some issues for future research are presented.	association rule learning;data mining;systems architecture	Haofeng Zhao;Jianqiu Zhu;Yangyong Zhu;Baile Shi	2002	Journal of Computer Science and Technology	10.1007/BF02948827	concept mining;association rule learning;computer science;data science;data mining;information retrieval;systems architecture	ML	-34.18882209361466	-3.856239195081474	73612
3ec58a7606d625485e520af5ded15fe0a43f5f73	virtual observatory publishing with dachs	virtual observatory;publication tools	The Data Center Helper Suite DaCHS is an integrated publication package for building VO and Web services, supporting the entire workflow from ingestion to data mapping to service definition. It implements all major data discovery, data access, and registry protocols defined by the VO. DaCHS in this sense works as glue between data produced by the data providers and the standard protocols and formats defined by the VO. This paper discusses central elements of the design of the package and gives two case studies of how VO protocols are implemented using DaCHS’ concepts.	data access;data center;debian;documentation;virtual observatory	Markus Demleitner;Margarida Castro Neves;Florian Rothmaier;Joachim Wambsganss	2014	CoRR	10.1016/j.ascom.2014.08.003	computer science;data mining;database;world wide web;physics	DB	-41.50754551599312	0.1423528856518516	73679
8a80c13287560040816f0d520c20a5101ab68b93	an ontology model for narrative image annotation in the field of cultural heritage		In the task of tagging narrative images, traditional event or story models are not suitable for temporal-spatial information modeling. These models are too coarse-grained to represent plots and actions information sufficiently in the particular field of culture heritage. In this paper, we design a narrative image annotation ontology (NIAO) model and a tool (NIA) to address these issues, by using ontology design patterns and other relevant models for reusability. The annotation model, combining the OAC (Open Annotation Collaboration) framework and regarding the Plot as a core element, makes a mapping between annotated image regions and high-level image semantics. It has been embedded in NIA, which we successfully use in the task of annotating narrative paintings. This tool can record annotation region pixels and related property values according to NIAO, and these annotation data can be stored as various formats such as csv, json, and rdf. We have built a SPARQL endpoint, in which end users can make semantic queries based on these annotation data, and visualize the results with pictures rather than tables.	automatic image annotation;communication endpoint;design pattern;embedded system;high- and low-level;information model;json;neural impulse actuator;object action complex;pixel;resource description framework;sparql	Xu Lei;Albert Meroño-Peñuela;Zhisheng Huang;Frank van Harmelen	2017			information retrieval;automatic image annotation;rdf;information model;sparql;software design pattern;semantics;narrative;computer science;annotation	AI	-41.613758060239995	0.9233846176398368	73737
9d506ba5a8ca9135e18e80089e6a3efeebd18de7	on the construction of eastweb framework — a plug-in framework for processing earth observation data streams	java programming language earth observation data streams data streams processing earth science datasets eastweb plugin framework plugin factory;earth observation data streams software architectural design software framework;data processing remote sensing diseases databases modis earth abstracts;java data handling geophysics computing internet	The EASTWeb system has been developed to facilitate access to earth science datasets for public health research and applications. In order to exploit the usage of the system, there is a need to expand the types of the data archives that the EASTWeb can process. The EASTWeb plugin framework introduces novel plugins and a plugin factory to allow the users to customize it to access and process different types of the earth observation data streams. This paper reviews the EASTWeb system, upon which the EASTWeb framework is built, and presents the top-level architectural design of the framework in detail. The framework is currently being implemented in the JAVA programming language according to the design presented in the paper.	archive;download;java;plug-in (computing);programming language;research data archiving	Yi Liu;Michael C. Wimberly;Jiameng Hu	2014	IEEE International Conference on Electro/Information Technology	10.1109/EIT.2014.6871809	computer science;data science;data mining;database	Robotics	-39.27757035394735	-1.7978048588187627	73821
83345e37e22dcff954511dfb845210d8f12e5952	odix: a rapid hypotheses testing system for origin-destination data ieee vast challenge award for excellence in spatio-temporal graph analytics		In this paper, we present our solution to the VAST Challenge 2017 Mini Challenge 1. We discuss challenges posed by data set and tasks and introduce ODIX, a custom rapid hypotheses testing system tailored to origin-destination data as provided by the challenge. We show findings made with ODIX and illustrate how we apply sequential pattern mining to explore common traffic patterns.		Juri Buchmüller;Wolfgang Jentner;Dirk Streeb;Daniel A. Keim	2017	2017 IEEE Conference on Visual Analytics Science and Technology (VAST)	10.1109/VAST.2017.8585686	data mining;data science;computer science;task analysis;statistical hypothesis testing;excellence;sequential pattern mining;graph;analytics	Visualization	-38.5685880285046	-7.384729587526464	74239
4a74f649e62a63c66f696d4989fe93e8e95d3e85	forced oscillations, bifurcations and stability of a molecular system part 2: resonances		Forced oscillations, bifurcations and stability of a molecular system Part 2: Resonances P. YU a , U. TANERI b & K. HUSEYIN a a Department of Systems Design Engineering, Faculty of Engineering , University of Waterloo , Waterloo, Ontario, N2L 3G1, Canada b Quantum Theory Group, Department of Applied Mathematics, Faculty of Mathematics , University of Waterloo , Waterloo, Ontario, N2L 3G1, Canada Published online: 16 May 2007.	bifurcation theory;quantum mechanics;systems design;systems engineering	Pei Yu;Ufuk Taneri;Koncay Huseyin	1996	Int. J. Systems Science	10.1080/00207729608929341		Theory	-46.15974951753658	-9.377159443808365	74262
7ad37ff0aedd2d91e5276ccf2ebd45b2c111ab1a	a computation-oriented multimedia data streams model for content-based information retrieval	continuous queries;multimedia data streams model;information retrieval;data stream;continuous query;multimedia application;relational database;functional dependency;sensor network;content based information retrieval;multimedia dependency theory;multimedia data;multimedia database	Multimedia applications nowadays are becoming prevalent. In the past the relational database model was generalized to the multimedia database model. More recently the relational database model was generalized to the data streams model, as the technology advanced and data became bulky and unbounded in size due to the utilization of sensor networks. In this paper we take one more step of generalization by providing a multimedia data streams model. The objective is to furnish a formal framework to design multimedia data streams (MMDS) schema for efficient content based information retrieval. We also extend the functional dependency theory and the normalization framework to handle multimedia data streams. Finally we present algorithmic methods of generating continuous multimedia queries along with examples for illustration.	computation;database model;functional dependency;information retrieval;multichannel multipoint distribution service;relational database;relational model	Shi-Kuo Chang;Lei Zhao;Shenoda Guirguis;Rohit Kulkarni	2009	Multimedia Tools and Applications	10.1007/s11042-009-0372-y	wireless sensor network;intelligent database;relational database;computer science;data mining;database;functional dependency;information retrieval;database design	DB	-34.478704247710674	2.022447808501182	74283
38c85852eb48e67e36efcb7652f227c4ee08e134	unifying data exploration and curation		"""Recent years have seen a surge in """"self-service"""" business intelligence tools. These tools primarily focus on supporting decision-making by non-technical """"end users"""", through data exploration -- the querying of data and inspection of results.  Exploration, however, is only part of the story. Curation is its complement. Curation is the ability to organize data into structures that are meaningful for a particular problem domain and convenient for building further explorations upon. Curation is also the ability to modify data, as well as creating new data through rules and constraints, in order to support what-if's, forecasting, and planning for the future. Exploration and curation often need to interleave in the decision-making process of an end-user.  In this talk, we discuss the LogicBlox Modeler, a unifying environment that provides support for both exploration and curation. We motivate the need for a unifying environment through applications in government, major financial institutions, and large global retailers. We discuss our language -- in its visual and textual representations -- that supports not only querying, but also the creation and modification of schema and data. We discuss the challenges imposed on the database runtime by the use cases of exploration and curation at scale and aspects of the LogicBlox database designed to meet these challenges."""	database schema;digital curation;interleaving (disk storage);problem domain	Shan Shan Huang	2016		10.1145/2948674.2948680	data curation;computer science;data science;data mining;database;world wide web	DB	-34.94262134591466	1.5887198132021794	74351
443d8e6e300f6c2ddd72449786875cec5dede628	reproducible research in computational harmonic analysis	software packages electronic publishing harmonic analysis mathematics computing;software publishing;computers;software;scientific publication reproducible research scientific computing computational science;image coding;mathematics computing;scientific publication;sensors;information science;scientific method;technical communication;computational science;computational modeling;classification algorithms;scientific computing;reproducible research;software publishing computational harmonic analysis reproducible research scientific publication;electronic publishing;harmonic analysis computational modeling computer errors large scale systems logic testing data mining laboratories image recognition information management scientific computing;algorithm design and analysis;software packages;computational harmonic analysis;harmonic analysis	Scientific computation is emerging as absolutely central to the scientific method. Unfortunately, it's error-prone and currently immature-traditional scientific publication is incapable of finding and rooting out errors in scientific computation-which must be recognized as a crisis. An important recent development and a necessary response to the crisis is reproducible computational research in which researchers publish the article along with the full computational environment that produces the results. In this article, the authors review their approach and how it has evolved over time, discussing the arguments for and against working reproducibly.	cognitive dimensions of notations;computation;computational science;scientific literature	David L. Donoho;Arian Maleki;Inam Ur Rahman;Morteza Shahram;Victoria Stodden	2009	Computing in Science & Engineering	10.1109/MCSE.2009.15	computational science;algorithm design;scientific method;technical communication;information science;computer science;sensor;theoretical computer science;harmonic analysis;data mining;electronic publishing;computational model	HPC	-46.81635347097615	-1.1703248912684903	74374
15f59cfde569805ed75bae1affccb0fb938b2ad5	towards data and data quality management for large scale healthcare simulations - position paper		The approach of ProHTA (Prospective Health Technology Assessment) is to understand the impact of medical processes and technologies as early as possible. Therefore, simulation techniques are utilized to estimate the effects of innovative health technologies and find potentials of efficiency enhancement within the supply chain of healthcare. Data management for healthcare simulations is required as heterogeneous data is needed both as simulation input data and for validation purposes. The main problem is the heterogeneity of the data and the initially unknown and continuously changing demands of the simulation. Also, data quality considerations are necessary to quantify the reliability of simulation output. A solution has to consider all of these aspects and must be extensible to cope with changing requirements. As the structure of the data is not known in advance, a generic database schema is required. This paper proposes an approach to store heterogeneous statistical data in an RDF-triplestore. Semantic annotations based on conceptual models are utilized to describe the datasets. Additionally, a special query language helps loading the data into the simulation. The feasibility of the approach has been demonstrated in a prototype implementation. We discuss the benefits of this approach as well as remaining challenges and issues.	box counting;computer simulation;data quality;data structure;database schema;prospective search;prototype;query language;rdf schema;requirement;resource description framework;triplestore	Philipp Baumgärtel;Richard Lenz	2012				DB	-35.561281923109654	-0.35623327130305504	74378
a610c9a150dd7eca8395433313390495e094c17b	selected spatio-temporal data types and operations for a 3d/4d geological information system	distributed spatiotemporal geoinformation systems spatiotemporal data types spatiotemporal data operations 3d geological information system 4d geological information system 3d models data volume data complexity 3d objects;geology information systems object oriented modeling database systems spatiotemporal phenomena collaborative work educational institutions computer science environmental management geographic information systems;data type;3d model;spatio temporal data;geophysics computing;temporal databases geographic information systems spatial data structures geophysics computing visual databases;geographic information systems;spatial data structures;temporal databases;information system;visual databases	The management of the evolution in time of large and complex 3D-models is not a trivial task and may largely surpass the data volume and complexity of conventional GIS. 3D objects changing in time have to be retrieved, inserted, processed and updated. It requires the development of application-specific spatiotemporal data types and operations. In this paper we present the design and the realisation of spatiotemporal data types and operations to be used in a typical 3D/4D-geological information system, and we give an outlook on our further research in the field of distributed spatiotemporal geoinformation systems.	geographic information system;microsoft outlook for mac;spatiotemporal database	Jörg Siebeck;Serge S. Shumilov;Armin B. Cremers;Martin Breunig;Andreas Thomsen	2004	Proceedings. 16th International Conference on Scientific and Statistical Database Management, 2004.	10.1109/SSDBM.2004.62	data type;computer science;data science;data mining;database;geographic information system;temporal database;programming language;information system;spatiotemporal database	DB	-37.30100743262683	-2.146262657164412	74387
8dd27a4303f8090c7ae911de8bd420150f425080	3d visualisation of continuous, multidimensional, meteorological satellite data		Ten years of earth observation by the MIPAS instrument aboard the European environmental satellite Envisat [1] led to a considerable amount of measurement and processed data, which has to be managed and visualized by powerful and flexible software tools. This paper gives an overview about new concepts, tools and methods for the 3D visualisation of continuous, multidimensional, meteorological Satellite Data, which are realised with WISA (Wissenschaftliches Informationssystem für die Atmosphärenforschung – Scientific Information System for atmospheric Research). A major goal of the visualisation is the recognition of correlations between different trace gas concentrations (e.g. ozone, chlorine nitrate, chlorofluorocarbons, etc.) and the visualisation of trace gas distributions where the main focus was laid on the polar areas.	chart;computer data storage;data structure;graphical user interface;information system;java 3d;power supply unit (computer);shading;visualization (graphics);zooming user interface	Richard Lutz	2014			visualization;trace gas;satellite;chlorine nitrate;remote sensing;geography;earth observation	DB	-38.68016391639401	-3.021908887719632	74667
6560e28f9015b72977509e98eb95bdffb41ca216	fuzzy spatial relation ontology driven detection of complex geospatial features in a web service environment		Remote sensing images have been widely used by intelligence analysts to discover geospatial features. The overwhelming volume of remote sensing imagery requires automated methods or systems for feature discovery. Existing research focuses on automatic extraction of isolated or elementary features, such as buildings and roads. It is rather understudied to discover complex geospatial features, which is spatially composed of elementary features. From the e-Science perspective, service computing technologies have shown great promise for widespread automation of data analysis and computation. The discovery of complex features would benefit from service computing technologies by computing spatial relations and their fuzziness among elementary features using geoprocessing services. The discovery process can be automated using an ontology approach. The paper presents how ontologies for complex geospatial features, enriched with fuzzy sets of spatial relations, can automate the workflow generation. Spatial computation functions, fuzzy membership functions, and mathematical fuzzy logical operators, are provided as services, and plugged into workflows on demand to enjoy the benefits of service computing technologies. A prototype system demonstrates on-demand uncertainty-aware detection of complex geospatial features in a geoprocessing service environment.	web service	Lianlian He;Peng Yue;Liangcun Jiang;Mingda Zhang	2015	Earth Science Informatics	10.1007/s12145-014-0186-y	computer science;data science;data mining;database;world wide web	Web+IR	-35.56204523014294	-3.6970875102917997	74752
02686401a1f4f3a342a4e8ced5047d3bbabebbe3	modeling, exploring and recommending music in its complexity	eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;ontology frbroo musical metadata schema org recommendation	Knowledge models that are currently in-use for describing music metadata are insufficient to express the wealth of complex information about creative works, expressions, performances, publications, authors and performers. In this research, we aim to propose a method for structuring the classical music information coming from different heterogeneous librarian repositories. In particular, we research and implement an appropriate music ontology based on existing models, controlled vocabularies and tools for converting and visualizing the metadata. Moreover, we research how this data can be consumed by end-users, through the development of a web application for exploring the data and a recommendation system that takes advantage of the richness of the data.	communication endpoint;controlled vocabulary;level of detail;librarian;performance;recommender system;sparql;schema.org;web application;word-sense disambiguation	Pasquale Lisena	2016		10.1007/978-3-319-58694-6_41	computer science;knowledge management;multimedia;world wide web	Web+IR	-42.65117359391826	1.898165135629142	74951
27c5478b8884540dbc46d1237093bd595aa5ace9	pav ontology: provenance, authoring and versioning	swan;biological patents;biomedical journals;linked data;drug discovery;text mining;europe pubmed central;web;citation search;annotation;data mining and knowledge discovery;citation networks;journal article;computational biology bioinformatics;research articles;attribution;abstracts;open access;life sciences;clinical guidelines;semantic web;provenance;authoring;algorithms;full text;combinatorial libraries;versioning;computer appl in life sciences;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search;resource	"""BACKGROUND Provenance is a critical ingredient for establishing trust of published scientific content. This is true whether we are considering a data set, a computational workflow, a peer-reviewed publication or a simple scientific claim with supportive evidence. Existing vocabularies such as Dublin Core Terms (DC Terms) and the W3C Provenance Ontology (PROV-O) are domain-independent and general-purpose and they allow and encourage for extensions to cover more specific needs. In particular, to track authoring and versioning information of web resources, PROV-O provides a basic methodology but not any specific classes and properties for identifying or distinguishing between the various roles assumed by agents manipulating digital artifacts, such as author, contributor and curator.   RESULTS We present the Provenance, Authoring and Versioning ontology (PAV, namespace http://purl.org/pav/): a lightweight ontology for capturing """"just enough"""" descriptions essential for tracking the provenance, authoring and versioning of web resources. We argue that such descriptions are essential for digital scientific content. PAV distinguishes between contributors, authors and curators of content and creators of representations in addition to the provenance of originating resources that have been accessed, transformed and consumed. We explore five projects (and communities) that have adopted PAV illustrating their usage through concrete examples. Moreover, we present mappings that show how PAV extends the W3C PROV-O ontology to support broader interoperability.   METHOD The initial design of the PAV ontology was driven by requirements from the AlzSWAN project with further requirements incorporated later from other projects detailed in this paper. The authors strived to keep PAV lightweight and compact by including only those terms that have demonstrated to be pragmatically useful in existing applications, and by recommending terms from existing ontologies when plausible.   DISCUSSION We analyze and compare PAV with related approaches, namely Provenance Vocabulary (PRV), DC Terms and BIBFRAME. We identify similarities and analyze differences between those vocabularies and PAV, outlining strengths and weaknesses of our proposed model. We specify SKOS mappings that align PAV with DC Terms. We conclude the paper with general remarks on the applicability of PAV."""	align (company);assumed;bibframe;class;community;core needle biopsy;description;digital artifact;dublin core;eap protocol;entity name part qualifier - adopted;general-purpose markup language;interoperability;lightweight ontology;morphologic artifacts;ontology (information science);polycythemia vera;published comment;requirement;scientific publication;simple knowledge organization system;vocabulary tests;weakness;web resource	Paolo Ciccarese;Stian Soiland-Reyes;Khalid Belhajjame;Alasdair J. G. Gray;Carole A. Goble;Timothy W Clark	2013		10.1186/2041-1480-4-37	text mining;computer science;bioinformatics;software versioning;attribution;semantic web;linked data;data mining;world wide web;information retrieval;drug discovery;resource	Web+IR	-41.290806068864576	2.0705353005640443	75183
d96b86388f1b6fbffc004e1e08bb1fb882504b2b	from terabytes to insights	tecnologia electronica telecomunicaciones;computacion informatica;grupo de excelencia;ciencias basicas y experimentales;tecnologias;high performance	Note: One terabyte is 1,000,000,000,000 bytes. There are many more byte sizes, large and small, to derive insights from, as well.	cyberinfrastructure;ibm notes;terabyte	Rita R. Colwell	2003	Commun. ACM	10.1145/792704.792724	theoretical computer science;data science;cyberinfrastructure;computer science;terabyte	Graphics	-42.27889180466495	-6.6412536048360336	75186
1f1477cb3d72e507140bb22abdc65e7a57afabb3	designing and implementing knowledge bases for narrative animations system	animation system;semantic web;knowledge base	The current technology such as semantic web and ontology allows everyone to build rapidly vast knowledge bases for a specific domain. A development of story animation knowledge bases is presented. The knowledge bases are used to produce movie animation given a story. Databases are expressed in the form of ontology by extracting textual entities and parsing each sentence of the story. Rules are obtained by analyzing the process of manually produced movie animation.		David Ramamonjisoa	2007		10.1145/1280720.1280722	natural language processing;knowledge base;computer science;knowledge management;artificial intelligence;semantic web;multimedia	Web+IR	-39.69525294752493	3.395540465498929	75494
f8535bef790bba5fea2010e1c44e569fb869ad0b	a domain independent framework for extracting linked semantic data from tables	probability;linked data;numbers;entity linking;resources;semantics;data processing;linkages;graphs;value;graphical models;machine learning;coding;tables data;semantic web;mapping;language;reasoning;tables;constants;models;extraction;rdf	Vast amounts of information is encoded in tables found in documents, on the Web, and in spreadsheets or databases. Integrating or searching over this information benefits from understanding its intended meaning and making it explicit in a semantic representation language like RDF. Most current approaches to generating Semantic Web representations from tables requires human input to create schemas and often results in graphs that do not follow best practices for linked data. Evidence for a table’s meaning can be found in its column headers, cell values, implicit relations between columns, caption and surrounding text but also requires general and domain-specific background knowledge. Approaches that work well for one domain, may not necessarily work well for others. We describe a domain independent framework for interpreting the intended meaning of tables and representing it as Linked Data. At the core of the framework are techniques grounded in graphical models and probabilistic reasoning to infer meaning associated with a table. Using background knowledge from resources in the Linked Open Data cloud, we jointly infer the semantics of column headers, table cell values (e.g., strings and numbers) and relations between columns and represent the inferred meaning as graph of RDF triples. A table’s meaning is thus captured by mapping columns to classes in an appropriate ontology, linking cell values to literal constants, implied measurements, or entities in the linked data cloud (existing or new) and discovering or and identifying relations between columns.	best practice;column (database);constant (computer programming);database;entity;graphical model;linked data;literal (mathematical logic);resource description framework;semantic web;spreadsheet;table cell;tag cloud;world wide web	Varish Mulwad;Timothy W. Finin;Anupam Joshi	2012		10.1007/978-3-642-34213-4_2	computer science;data mining;database;information retrieval	Web+IR	-37.552303479506016	4.174767501924115	75714
482a9935ed859a026123b70816c2079f1ff459d4	multidimensional data warehousing and mining - an approach for managing multiple reservoir ecosystems	sediments data mining data warehouses ecology hydrocarbon reservoirs ontologies artificial intelligence petroleum industry;the australian standard research classification 210000 science general;hydrocarbon reservoirs;sedimentary basin digital ecosystem ontology datawarehousing data mining;reservoirs petroleum ecosystems ontologies rocks seals data mining;ecology;data mining;sediments;ontologies artificial intelligence;conference paper;data mining sedimentary basin digital ecosystem ontology datawarehousing;digital ecosystem;petroleum industry;sedimentary basin;data warehouses;sedimentary basin ontology multidimensional data warehousing multidimensional data mining multiple reservoir ecosystem management oil fields oil producing wells gas producing wells gas fields multiple reservoir pay zones atomic dimensions nondivisible dimensions petroleum ecosystem ontology based cardinalities business rules multidimensional data models cardinality consistency cardinality integrity reservoir connectivity;datawarehousing;ontology	Many sedimentary basins comprise of numerous oil and gas fields. Each field has multiple oil and gas producing wells and each drilled well has multiple reservoir pay zones, with each pay zone having different fluids - either oil or gas and both. From a sedimentary basin scale, a super-type dimension is distinguished into its atomic and non-divisible dimensions, such as reservoir and structure. In database terminology, cardinality is representative of the set of elements-, and attributes and their relationships. Here, each element is interpreted as a dimension, narration of multiple dimensions for multiple elements within the context of a petroleum ecosystem. Ontology based cardinalities are described for designing constraints and business rules among multidimensional data models, to maintain integrity and consistency of the cardinalities. For the purpose of analyzing petroleum ecosystem and its reservoir connectivity, ontologies based cardinalities are described. Though sedimentary-basin ontology narrates, connectivity among structures, reservoirs, seals, source and other processes, such as migration and timing of occurrence or existence of these elements, but we focus on an approach exploring connections among multiple reservoirs and traps within a petroleum ecosystem. This approach minimizes the ambiguity during interpretation and management of reservoir ecosystems' limits or boundaries.	aquatic ecosystem;cardinality (data modeling);data model;data modeling;interpretation (logic);ontology (information science)	Shastri L. Nimmagadda;Heinz Dreher;Olga Shtukert;Nikita Zolotoi	2013	2013 11th IEEE International Conference on Industrial Informatics (INDIN)	10.1109/INDIN.2013.6622940	environmental science;petroleum engineering;hydrology;data mining	DB	-37.536122934065766	-1.7551978262720378	75737
d16b665977a3464245dac312e8a7e247af6526f5	on the challenges and opportunities in visualization for machine learning and knowledge extraction: a research agenda		We describe a selection of challenges at the intersection of machine learning and data visualization and outline a subjective research agenda based on professional and personal experience. The unprecedented increase in the amount, variety and the value of data has been significantly transforming the way that scientific research is carried out and businesses operate. Within data science, which has emerged as a practice to enable this data-intensive innovation by gathering together and advancing the knowledge from fields such as statistics, machine learning, knowledge extraction, data management, and visualization, visualization plays a unique and maybe the ultimate role as an approach to facilitate the human and computer cooperation, and to particularly enable the analysis of diverse and heterogeneous data using complex computational methods where algorithmic results are challenging to interpret and operationalize. Whilst algorithm development is surely at the center of the whole pipeline in disciplines such as Machine Learning and Knowledge Discovery, it is visualization which ultimately makes the results accessible to the end user. Visualization thus can be seen as a mapping from arbitrarily high-dimensional abstract spaces to the lower dimensions and plays a central and critical role in interacting with machine learning algorithms, and particularly in interactive machine learning (iML) with including the human-in-the-loop. The central goal of the CDMAKE VIS workshop is to spark discussions at this intersection of visualization, machine learning and knowledge discovery and bring together experts from these disciplines. This paper discusses a perspective on the challenges and opportunities in this integration of these discipline and presents a number of directions and strategies for further research.	algorithm;computation;data science;data visualization;data-intensive computing;interaction;machine learning;while	Cagatay Turkay;Robert S. Laramee;Andreas Holzinger	2017		10.1007/978-3-319-66808-6_13	data visualization;end user;visualization;knowledge extraction;operationalization;scientific method;data management;machine learning;artificial intelligence;knowledge management;data science;computer science	ML	-36.65622197279538	-7.945988366962532	75751
26ef0a7f38abbc245420cf95dc1ac21cb924dac3	creating a large topic map by integrating wandora and ontopia	topic maps;ontopia;integration;interoperability;data handling;usefulness;wandora	Purpose – The purpose of this paper is to present conceptual and technical knowledge about creating a large topic map by integrating the strengths of two topic maps creation tools (i.e. Ontopia and Wandora).Design/methodology/approach – This study is focused on the testing of the usefulness of the two topic map creation tools. Each tool is used to create a topic map with dozens of topics in order to find out the strengths and weaknesses of each tool and the interoperability of the two tools.Findings – When creating a large topic map, a developer may have many requirements of a desired topic maps creation tool, but may not be able to find a single tool that meets all the requirements. If multiple such tools implement the topic maps standard, there is some interoperability between the tools, and the developer may integrate these tools to meet the requirements.Practical implications – Although this paper presents the strengths, weakness, and interoperability of two topic maps creation tools (i.e. Ontopia and...	topic maps	Yejun Wu;David J. Dunaway	2013	Library Hi Tech	10.1108/07378831311303930	topic maps;interoperability;telecommunications;computer science;data science;group method of data handling;data mining;world wide web	AI	-43.218073498446316	-0.7954538271458464	76053
5ba0c4ef650b7d6fbfa25ef887447ba9ed90933f	location-based distributed group key agreement scheme for vehicular ad hoc network	sensor networks;management	1 Shenzhen Graduate School, Harbin Institute of Technology, HIT Campus, Shenzhen University Town, Xili, Shenzhen 518055, China 2 Shenzhen Key Laboratory of Internet Information Collaboration, Shenzhen Graduate School, Harbin Institute of Technology, HIT Campus, Shenzhen University Town, Xili, Shenzhen 518055, China 3Harbin Institute of Technology, 92 West Dazhi Street, Nan Gang District, Harbin 150001, China	computation;group key;hoc (programming language);key-agreement protocol;nan;overhead (computing)	Eric Ke Wang;Yuming Ye;Xiaofei Xu	2014	IJDSN	10.1155/2014/759601	vehicular ad hoc network;mobile ad hoc network;distributed computing;computer security;computer network	Crypto	-44.828244076065474	-7.817105300782525	76140
602ab24e8134815630da74c746405b3693836d6e	a survey of data mining and deep learning in bioinformatics		The fields of medicine science and health informatics have made great progress recently and have led to in-depth analytics that is demanded by generation, collection and accumulation of massive data. Meanwhile, we are entering a new period where novel technologies are starting to analyze and explore knowledge from tremendous amount of data, bringing limitless potential for information growth. One fact that cannot be ignored is that the techniques of machine learning and deep learning applications play a more significant role in the success of bioinformatics exploration from biological data point of view, and a linkage is emphasized and established to bridge these two data analytics techniques and bioinformatics in both industry and academia. This survey concentrates on the review of recent researches using data mining and deep learning approaches for analyzing the specific domain knowledge of bioinformatics. The authors give a brief but pithy summarization of numerous data mining algorithms used for preprocessing, classification and clustering as well as various optimized neural network architectures in deep learning methods, and their advantages and disadvantages in the practical applications are also discussed and compared in terms of their industrial usage. It is believed that in this review paper, valuable insights are provided for those who are dedicated to start using data analytics methods in bioinformatics.	academia (organization);algorithm;architecture as topic;artificial neural network;bioinformatics;clinical informatics;cluster analysis;data mining;data point;deep learning;informatics (discipline);linkage (software);machine learning;neural network simulation;preprocessor;tree accumulation;statistical cluster	Kun Lan;Dan-tong Wang;Simon Fong;Liansheng Liu;Kelvin Kian Loong Wong;Nilanjan Dey	2018	Journal of Medical Systems	10.1007/s10916-018-1003-9	automatic summarization;data mining;domain knowledge;cluster analysis;biological data;artificial neural network;deep learning;data analysis;analytics;bioinformatics;artificial intelligence;medicine	ML	-37.306792339775484	-7.56464039952968	76155
32aa1fb453ec06cf506c987de012e7d2a42897ff	the emerging field of semantic scientific knowledge integration	semantic scientific knowledge integration;special issues and sections;information retrieval;e science;information technology;scientific data;knowledge management;semantic technologies;knowledge enhanced scientific data retrieval;artificial intelligence ontologies geology collaborative software software tools informatics knowledge representation collaborative work intelligent systems collaborative tools;semantic scientific knowledge integration e science semantic web ontologies knowledge enhanced scientific data retrieval;next generation;semantic web;ontologies;knowledge integration;knowledge representation;natural sciences computing;scientific publishing;grid computing;natural sciences computing knowledge management;scientific knowledge;deep data integration semantic scientific knowledge integration grid computing knowledge enhanced scientific data retrieval next generation information technology semantic technologies semantic e science	As science informatics and e-Science blossom around the world, teams of collaborating researchers are fi nding needs for next-generation cyberinfrastructure along with knowledge and tool support for data-intensive scientifi c research. M any geosciences researchers are taking advantage of the emergence of virtual repositories and observatories, such as those in astronomy, heliophys-ics, environmental science, hydrology, and solar-terrestrial physics, where distributed and often heterogeneous collections of scientifi c data are made available transparently. 2 But geoscience is but one of many fi elds leading the way, as is evident in this issue. Such efforts strive to present a scientifi c research environment that provides software tools and interfaces to interoperating data archives and associated services. The initial goals for these efforts focused more on traditional database considerations and relatively simple uses of AI techniques. Recently, there has been substantial adoption of mainstream AI techniques, and the medium-and long-range goals for these efforts call for full-scale semantic integration of scientifi c data and associated knowledge. Consequently, they present interesting motivations for and tests of existing and emerging AI techniques. Alongside next-generation information technology for science is a growing demand for semantic technologies. Knowledge representation languages and environments continue to evolve. Importantly for science applications, several of the languages have become stable international standards or recommendations. As a result, a number of academic and commercial tools are now available for use from new as well as established organizations and companies. The e-Science community has been an early adopter of many of these tools, and in turn has provided essential feedback to semantic-technology developers, driving infrastructure evolution. As e-Science projects mature, many new challenges are appearing for both AI and IT, stimulating new research directions that in turn are motivating development of the next generation of scientifi c cyberinfrastructure. To date, growth in e-Science and in semantic technologies has been largely independent, but the need to coalesce the two communities is becoming increasingly apparent. One such cross-disciplinary forum was the Semantic Scientifi c Knowledge Integration Workshop held in March 2008 (www. ksl.stanford.edu/people/dlm/sss08) as part of the AAAI's Spring Symposium Series. One goal of that workshop 3 and subsequently for this special issue was to help identify and catalyze the semantic scientifi c knowledge integration community into collaborative efforts. That workshop, this special	archive;artificial intelligence;cyberinfrastructure;data-intensive computing;e-science;emergence;full scale;informatics;knowledge integration;knowledge representation and reasoning;null (sql);research data archiving;semantic integration;terrestrial television;world wide web	Deborah L. McGuinness;Peter Fox;Boyan Brodaric;Elisa F. Kendall	2009	IEEE Intelligent Systems	10.1109/MIS.2009.19	knowledge integration;semantic grid;computer science;knowledge management;ontology;artificial intelligence;data science;semantic web;semantic technology;information technology;sociology of scientific knowledge;information retrieval;grid computing;data	AI	-44.743513275791976	0.7283573521598988	76278
dc6bde16eec576c8e126aa86ec8828c0eb3baf00	knowledge-driven wireless networks with artificial intelligence: design, challenges and opportunities		This paper discusses technology challenges and opportunities to embrace artificial intelligence (AI) era in the design of wireless networks. We aim to provide readers with motivation and general methodology for adoption of AI in the context of next-generation networks. First, we discuss the rise of network intelligence and then, we introduce a brief overview of AI with machine learning (ML) and their relationship to self-organization designs. Finally, we discuss design of intelligent agent and it's functions to enable knowledge-driven wireless networks with AI.	artificial intelligence;intelligent agent;machine learning;network intelligence;next-generation network;self-organization	Haris Gacanin	2018	CoRR		wireless network;intelligent agent;computer science;network intelligence;artificial intelligence	AI	-47.565450666475094	2.321197829120087	76442
d8d25152bc35086828f02b086addf94d442dffdb	message from the mmns workshop co-chairs	mmns workshop co-chairs	2005-2008 Université du Québec à Montréal  Ph.D. in Computer Science. Dissertation: “Specification, Verification and Satisfiability of Hybrid Constraints by Reduction to Temporal Logic.” Advisors: Omar Cherkaoui, Roger Villemaire. Research project in collaboration with the CANARIE consortium, funded by a postgraduate scholarship from the Natural Sciences and Engineering Research Council (NSERC). 2003-2005 Université du Québec à Montréal  M.Sc. in Mathematics and Computer Science. Advisor: Roger Villemaire. Research project in collaboration with Cisco Systems inc., funded by a scholarship from Institut des Sciences Mathématiques (ISM). 2002 Université Laval  Certificate in Computer Science 1999-2002 Université Laval  B.Sc. in Mathematics 1997-1999 Petit Séminaire de Québec  International Baccalaureate (IB) in Pure Sciences, Bilingual Diploma (French/English)	computer science;diploma;robo-advisor;sfiaplus;temporal logic	Michiaki Katsumoto;Wu-chi Feng	1999		10.1109/ICPP.1999.10003		Logic	-46.60271594640501	-8.050391095578904	76779
c5f618fea429770988d894e8929056e720247e95	computational challenges for sentiment analysis in life sciences	social networks life sciences sentiment analysis;sentiment analysis visualization affective computing twitter life sciences mood;social networking online biology computing health care parallel processing sentiment analysis;social networks;sentiment analysis;life sciences;life sciences sentiment analysis social networks healthcare high performance computing	Data and connectivity between users form the core of social networks. Every status, post, friendship, tweet, re-tweet, tag or image generates a massive amount of structured and unstructured data. Deriving meaning from this data and, in particular, extracting behavior and emotions of individual users, as well as of user communities, is the goal of sentiment analysis and affective computing and represents a significant challenge. Social networks also represent a potentially infinite source of applications for both research and commercial purposes and are adaptable to many different areas, including life science. Nevertheless, collecting, sharing, storing and analyzing social networks data pose several challenges to computer scientists, such as the management of highly unstructured data, big data, and the need for real-time computation. In this paper we give a brief overview of some concrete examples of applying sentiment analysis to social networks for healthcare purposes, we present the current type of tools existing for sentiment analysis, and summarize the challenges involved in this process focusing on the role of high performance computing.	affective computing;big data;cloud computing;computation;computer scientist;database;privacy;real-time transcription;response time (technology);sql;sentiment analysis;social network;supercomputer	F. Ciullo;Chiara Zucco;Barbara Calabrese;Giuseppe Agapito;Pietro Hiram Guzzi;Mario Cannataro	2016	2016 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCSim.2016.7568365	computer science;data science;data mining;world wide web;sentiment analysis;social network	DB	-36.01558406523791	-5.459092641696293	76814
f2bfcd1d8dd824a7f9f61d93b3f6c144ec4165bc	geometric correlations in real multiplex networks: multidimensional communities, trans-layer link prediction, and efficient navigation		Kaj-Kolja Kleineberg, ∗ Marián Boguñá, † M. Ángeles Serrano, 1, ‡ and Fragkiskos Papadopoulos § Departament de F́ısica Fonamental, Universitat de Barcelona, Mart́ı i Franquès 1, 08028 Barcelona, Spain Institució Catalana de Recerca i Estudis Avançats (ICREA), Passeig Llúıs Companys 23, E-08010 Barcelona, Spain Department of Electrical Engineering, Computer Engineering and Informatics, Cyprus University of Technology, 33 Saripolou Street, 3036 Limassol, Cyprus (Dated: January 19, 2016)	computer engineering;electrical engineering;informatics;multiplexing;source-to-source compiler	Kaj-Kolja Kleineberg;Marián Boguñá;M. Ángeles Serrano;Fragkiskos Papadopoulos	2016	CoRR			DB	-46.790471975360106	-9.194519201780114	76927
7e12ec528ef9d45a01b634dc2c138093021d2184	simple automatic coding systems	simple automatic coding system	Editor's Note: The following paper is an example of a growing trend in programming today, that of translation from one source language to another so that an existing processor for the second source language can be used without loss of time or development costs. There are many such programs in existence and under development. Some of these are tabulated here for background information. If there are others which have been overlooked, a short description will be received gladly. SOURCE LANGUAGE ALGAE (Los Alamos) Lockheed Simultaneous Diff. Equations VIB (General Motors) IT XTRAN FORTRAN FORTRAN IT MATH-MATIC FLOWMATIC OUTPUT SOURCE LANGUAGE FORTRAN FORTRAN FORTRAN FORTRAN FORTRAN (In process, IBM) IT SOAP (In process, IBM) SOAP A-3 C-10, various symbolic assembly languages Dick Hill, Assistant Director of the Western Data Processing Center at UCLA. offers the following puzzles for a minimum 650 using SOAP language. To be consistent, they should be called Code-Nundrums to match those previously printed for the 704. Answers will be given at the end of this section. 1. The instruction at location B is RAL Y. Location Y is a dump, otherwise unused in the program. Write no more than 4 SOAP instructions to change the data address of the instruction B to Y+i and cause the changed instruction to be executed, with these restrictions: a. Use no preloaded constants. b. Do not change the contents of B. c. Use no absolute drum location addresses. d. Use no store or branch instructions. 2. Write no more than 4 SOAP instructions to test the contents of 8000. If = +1 (00 0000 0001), transfer to location A. If = +2, transfer to location B. It is assumed that the contents will always be one or the other, a. Use no instruction containing N in its symbolic operation code. b. Use no preloaded constants or absolute drum addresses. INTRODUCTION In programming any digital computer, there are many routine tasks of a semi-clerical nature. If most of these can be performed by the machine itself, the progr~ in shorter programming time and fewer errors. When a machk as good or almost as good error because most of the t However, there seems to mafic coding or translating	assembly language;automatic programming;computer;drum memory;flow-matic;fortran;like button;math-matic;opcode;printing;soap;second source;semiconductor industry;year 10,000 problem	Eldridge S. Adams;Stewart I. Schlesinger	1958	Commun. ACM	10.1145/368873.368884	coding (social sciences);theoretical computer science;computer science	PL	-46.116057537162526	-3.4870543097568243	77109
c0eafa296c5e51a03db364d49ad5b47cc53cbf5d	evaluation and analysis of dam operating status using one clock-synchronized dual-antenna receiver		School of Civil Engineering, Beijing Jiaotong University, Beijing 100044, China School of Geomatics and Urban Spatial Informatics, Beijing University of Civil Engineering and Architecture, Beijing 100044, China Beijing Key Laboratory of Urban Spatial Information Engineering, Beijing Institute of Surveying and Mapping, Beijing 100038, China Qinghai Research Institute of Transportation, Xining 810001, China		Yunlong Zhang;Songlin Yang;Jiankun Liu;Dongwei Qiu;Xiaoyan Luo;Jianhong Fang	2018	J. Sensors	10.1155/2018/9135630	deformation (mechanics);limiting factor;electronic engineering;geodetic datum;global positioning system;safety factor;finite element method;deformation monitoring;engineering;marine engineering;gnss applications	ML	-45.403585886754044	-8.55953092121635	77199
c919560eb29fa1b6e59720cea879cab36ebd2111	information discovery in loosely integrated data	information discovery;satisfiability;search;xml;object relational;heterogeneous data sources	We model heterogeneous data sources with cross references, such as those crawled on the (enterprise) web, as a labeled graph with data objects as typed nodes and references or links as edges. Given the labeled data graph, we introduce flexible and efficient querying capabilities that go beyond existing capabilities by additionally discovering meaningful relationships between objects that satisfy keyword and/or structured query filters. We introduce the relationship search operator that exploits the link structure between data objects to rank objects related to the result of a filter. We implement the search operator using the ObjectRank [1] algorithm that uses the random surfer model. We study several alternatives for constructing summary graphs for query results that consist of individual and aggregate nodes that are somehow linked to qualifying result nodes. Some of the summary graphs are useful for presenting query results to the user, while others could be used to evaluate subsequent queries efficiently without considering all the nodes and links in the original data graph.	aggregate data;algorithm;cross-reference;graph labeling;information discovery	Heasoo Hwang;Andrey Balmin;Hamid Pirahesh;Berthold Reinwald	2007		10.1145/1247480.1247637	xml;computer science;theoretical computer science;data mining;database;satisfiability	DB	-35.1934472436621	3.8561849041583613	77267
dfd15947f75a95c37e6371affee7498e471a5a6d	integrating web-based documents, shared knowledge bases, and information retrieval for user help	help systems;information retrieval;shared knowledge;natural language;world wide web;knowledge base;knowledge engineering	We describe a prototype system, IKARUS, with which we investigated the potential of integrating web-based documents, shared knowledge bases, and information retrieval for improving knowledge storage and retrieval. As an example, we discuss how to implement both a user manual and an online help system as one system. The following technologies are combined: a web-based design, a frame-based knowledge engine, use of an advanced full-text search engine, and simple techniques to control terminology. We have combined graphical browsing with several unusual forms of text retrieval—for example, to the sentence and paragraph level.	information retrieval	Douglas R. Skuce	2000	Computational Intelligence	10.1111/0824-7935.00107	knowledge base;question answering;computer science;knowledge management;artificial intelligence;knowledge-based systems;knowledge engineering;data mining;natural language;information retrieval;search engine	Web+IR	-39.95603894232392	3.3811352212133006	77288
a99b620f597f39c42a25eed39421404da79e81c8	novel broadband light sources and pulse generation techniques at 1.5 [mu]m	electrical engineering and computer science;thesis	Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2009.		Hanfei M. Shen	2009			engineering;electrical engineering;mechanical engineering	EDA	-46.932872008218354	-6.876059263290151	77766
d58a9bce6c92cf8cc7a0215b3d39c08f9d953de4	ontology engineering, scientific method and the research agenda	ontologie;posicionamiento;red www;ingenierie connaissances;validacion;research agenda;service orientation;scientific method;web semantique;reseau web;orientado servicio;positioning;web intelligence;internet;web semantica;decouverte connaissance;ontology engineering;semantic web;autoorganizacion;world wide web;descubrimiento conocimiento;self organization;ontologia;validation;oriente service;information system;ontology;autoorganisation;design science;service oriented;positionnement;knowledge discovery;knowledge engineering	The call for a “focus on content” in ontology research by Nicola Guarino and Mark Musen in their launching statement of the journal Applied Ontology has quite some implications and ramifications. We reflectively discuss ontology engineering as a scientific discipline, and we put this into the wider perspective of debates in other fields, including the methodology of social and natural sciences, and of Information Systems and design science research. We outline how ontologies provide us with a (new) scientific method for theory formation. This positioning allows for stronger concepts and techniques for theoretical, empirical and practical validation that in our view are now needed in the field. A prerequisite for this is an emphasis on ontology as a (domain) content oriented concept, rather than as primarily a computer representation notion. Taking application domain theories and the associated content reference of ontologies really seriously as first-class citizens will actually increase the contribution of ontology engineering to the development of scientific method in general. Next, ontologies should develop from the current static representations of relatively stable domain content into actionable theories-in-use, and a possible way forward is to build in capabilities for self-organization of ontologies as service-oriented knowledge utilities (SOKUs) that can be delivered over the Web.	application domain;information systems;ontology (information science);ontology engineering;self-organization;service-oriented software engineering;theory;world wide web	Hans Akkermans;Jaap Gordijn	2006		10.1007/11891451_13	upper ontology;idef5;ontology alignment;the internet;self-organization;scientific method;computer science;knowledge management;ontology;artificial intelligence;semantic web;knowledge engineering;ontology;data mining;database;web intelligence;ontology-based data integration;world wide web;process ontology;information system;suggested upper merged ontology	AI	-44.92617482398719	3.058282429164935	77775
e662a02be246a501934ea58cc85d72ffb21f2d47	performance forecasting for performance critical huge databases	performance critical huge databases;performance improvement;high performance system;conceptual performance;performance engineering;performance forecasting;logical tuning method;systematic performance design;active performance improvement;performance monitoring technique;logical performance tuning	Fast databases are no longer nice-to-have --they are a necessity. Many modern applications are becoming performance critical. At the same time, the size of some databases has been increasing to levels that cannot be well supported by current technology. Performance engineering is now becoming a buzzword for database systems. At first physical and partially logical tuning methods have been used for support of high performance systems, but they are mainly based on large and not well understood performance and tuning parameters. Nowadays it becomes obvious that we need methods for systematic performance design.#R##N##R##N#Performance engineering also means, however, support for database's daily operating. Most methods are reactive, i.e. they are using runtime information, e.g. performance monitoring techniques. It is then the operators or administrators business to find appropriate solutions. We target at active methods for performance improvement. One of the potential methods for active performance improvement is performance forecasting based on assumptions on future operating and on extrapolations for the current situation. This paper shows that conceptual performance tuning supersedes physical and logical performance tuning. As a proof of concept we applied our approach within a consolidation project for a databases-intensive infrastructure.	database	Bernhard Thalheim;Marina Tropmann-Frick	2010		10.3233/978-1-60750-690-4-206	simulation;performance engineering;engineering;data mining;management science	ML	-33.817269489374596	0.47403399386122835	77820
70f03e4855d3992ca5db6d73b284f39d0ca05869	in the news		The Web's inherent openness and potential for growth and new applications make it a powerful tool, especially as methods develop for handling data on the semantic Web. In the semantic Web, heterogeneous data sources can be treated as a single source, with an ontology that mediates between them, thereby allowing the creation of knowledge out of information. But formidable problems accompany its potential to publish and store massive amounts of information. As more users access and reason over the information on the Web, particularly in pursuit of creating more valuable knowledge, issues arise of securing and protecting personal information and of this information's accuracy and provenance. Computer scientists are addressing these problems, imagining and developing solutions that range from semantic firewalls to methods for monitoring and auditing the use of information	computer scientist;firewall (computing);genetic heterogeneity;handling (psychology);openness;personally identifiable information;semantic web;solutions;world wide web	Danna Voth	1992	IEEE Intelligent Systems	10.1177/1757913917693720		Web+IR	-46.21297080575046	1.4359553502094757	77849
75ae0e16acc49a673ccfedf5f6ee6927b808b329	knowledge network of scientific claims derived from a semantic publication system	linked data;e science;knowledge networks;terminological knowledge bases;semantic publishing;semantic web;knowledge representation	Currently, the conventional communication channel for reporting scientific results is Web electronic publishing of scientific articles in paper print formats, such as PDFs. The emergence of the Semantic Web and Linked Data environment provides new opportunities for communicating, sharing and integrating scientific knowledge in digital formats that could overcome the limitations of the current print format, which is only suitable for reading by people. The results of scientific research can be published electronically and shared in structured, interlinked formats. This integrated knowledge network could be crawled by software agents, thereby facilitating semantic retrieval, knowledge reuse, validation of scientific results, identification of traces of scientific discoveries, new scientific insights and identification of knowledge contradictions or inconsistencies. This paper explores the possibilities of this new environment for scientific publishing and reports the implementation of a prototype semantic publishing system, which publishes scientific articles in a paper print format and publishes the claims made in the conclusions of each article as structured triples using the Resource Description Framework format.		Carlos Henrique Marcondes	2011	Inf. Services and Use	10.3233/ISU-2012-0646	knowledge representation and reasoning;computer science;data science;semantic web;social semantic web;linked data;data mining;semantic web stack;world wide web;information retrieval	HPC	-43.93750036705646	3.2802776905620465	78387
54b9d771abf4002e5a605bcf8da18b2a6cf1b842	continuously improving data quality in persistent databases	700100 computer software and services;280108 database management;continuous improvement;data quality		data quality;database	Paul L. Bowen;David A. Fuhrer;Frank M. Guess	1998	Data Quality Journal		data quality;data management;computer science;data science;data administration;data mining;database;software quality control	DB	-39.79512432639768	-5.049987495496048	78456
0f462fba3f25fb112391152a22441acca233f42f	a fast large-size production data transformation scheme for supporting smart manufacturing in semiconductor industry		In this paper, we propose a novel data transformation scheme over a big data platform, aiming at injecting production data from the local database in the factory side and transforming them into the workpiece-centric form that many manufacturing analytics systems need. The key idea is to blend big data processing techniques, including table composition with external distributed files, columnar storage, partition, massively parallel processing into the data transformation scheme for minimizing the data processing time. Our proposed scheme brings two main impacts to the smart manufacturing. First, our scheme plays the key component to develop data-driven manufacturing decision systems, since large-volume production data sources can be efficiently transformed into the workpiece-centric form that other smart manufacturing services require. Second, our proposed scheme provides a development exemplar to assist a manufacturing factory toward the Industry-4.0 realm, since big data techniques are ingeniously blended in building data-intensive manufacturing services. We finally implement the prototype of the proposed scheme on the Hadoop platform and apply the prototype to a semiconductor factory for conducting integrated tests. Testing results of a case study physically applying the proposed scheme to a semiconductor factory demonstrate the success of our work.	apache hadoop;big data;data-intensive computing;industry 4.0;parallel computing;prototype;semiconductor industry	Benny Suryajaya;Chao-Chun Chen;Min-Hsiung Hung;Yu-Yang Liu;Jia-Xuan Liu;Yu-Chuan Lin	2017	2017 13th IEEE Conference on Automation Science and Engineering (CASE)	10.1109/COASE.2017.8256114	massively parallel;big data;parallel processing;factory;distributed computing;data processing;analytics;computer science	DB	-33.92990318260099	-1.5788572959805984	79290
900d917ef3980332ef0c29f460c9505bd47cea0f	hybrid integration of molecular-biological annotation data	integrated approach;performance test;expression analysis;gene expression;protein expression;data warehouse;high performance;support function	We present a new approach to integrate annotation data from public sources for the expression analysis of genes and proteins. Expression data is materialized in a data warehouse supporting high performance for data-intensive analysis tasks. On the other hand, annotation data is integrated virtually according to analysis needs. Our virtual integration utilizes the commercial product SRS (Sequence Retrieval System) of LION bioscience. To couple the data warehouse and SRS, we implemented a query mediator exploiting correspondences between molecular-biological objects explicitly captured from public data sources. This hybrid integration approach has been implemented for a large gene expression warehouse and supports functional analysis using annotation data from GeneOntology, Locuslink and Ensembl. The paper motivates the chosen approach, details the integration concept and implementation, and provides results of preliminary performance tests.	data-intensive computing;information privacy	Toralf Kirsten;Hong Hai Do;Christine Kopp;Erhard Rahm	2005		10.1007/11530084_17	support function;gene expression;computer science;bioinformatics;data warehouse;data mining;database;protein expression;genetics	Comp.	-37.028152923269694	0.4100804032365521	79478
4adcc51910be8bcd4569a1c433f63392e049939d	guest editorial: smart transportation based on multimedia data mining		The advent of new technologies such as the Internet of Things (IoT) and cloud computing has brought opportunities for the development of Smart Transportation. In line with the essential of Bsmart cities^, smart transportation highlights the use of data. In particular, data are captured in large quantity, from multi-source, and in real-time. Moreover, Smart Transportation lays greater emphasis on knowledge discovery, information sharing and supported decision making. This new notion involves some tasks in which traditions are replaced by intelligent technology requiring manual discrimination and resolution to reach the optimization. In addition, with the development of Internet of Vehicles, Smart Transportation pays greater attention to the interconnection between transportation system and other information systems to the maximum extent possible. Then it can be seen that further efficient management and deeper analysis of transportation data are the key tasks in developing smart transportation. We have finally selected 15 manuscripts for this special issue after the first, second review processes. Each manuscript selected was blindly reviewed by at least three reviewers consisting of guest editors and external reviewers. With increasing demands for indoor GIS, indoor routing and analysis attracts attention from both GIS and architecture worlds. ‘Implementing two methods in GIS software for indoor routing: An Empirical Study’ aims to provide executable methods in GIS software e.g. ArcGIS for indoor path generation and to explore the possibilities for further analysis. In this paper, two methods are proposed and implemented: Mesh and TIN. The Mesh method used a Multimed Tools Appl DOI 10.1007/s11042-016-3915-z	arcgis;cloud computing;data mining;executable;geographic information system;interconnection;internet of things;mathematical optimization;mesh networking;multi-source;real-time clock;real-time computing;routing	Zhihan Lv;Chen Zhong;Dingde Jiang	2016	Multimedia Tools and Applications	10.1007/s11042-016-3915-z	data science;data mining;multimedia	HPC	-46.854657027522116	-1.7248717834627323	79673
b87f10e601715db5bca8f9444404836277e27d82	study on gml and svg-based open webgis and its application	forestry data;forestry;spatial data;forestry geographic information systems internet spatial data structures geophysical techniques;gmu svg;geometry;data mining;displays graphics forestry geographic information systems xml geometry laboratories data mining content addressable storage internet;internet;gmu svg open webgis gml geographic spatial data web display forestry data;geographic spatial data;geographic information systems;displays;xml;web display;gml;spatial data structures;content addressable storage;open webgis;graphics;geophysical techniques	In this paper, GML and SVG are discussed firstly, especially in the role of open WebGIS. Then an open WebGIS framework is put forward that using GML to store and integrate geographic spatial data and using SVG to implement Web display and operation for geographic spatial data. In the end, taking a forestry data as an example, an open WebGIS application system was set up and discussed.	geography markup language;scalable vector graphics	Qunyong Wu;Qinmin Wang;Chenghu Zhou;Chuanbin Chen;Ruiyin Huang	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1525232	the internet;xml;computer science;graphics;database;spatial analysis;geographic information system;world wide web;information retrieval	Embedded	-37.81092675094329	-2.168464661476494	79790
8492fac708619385bb7ac98893a43c00d928b059	query optimization in a heterogeneous dbms	query optimization	We propose a query optimization strategy for heterogeneous DBMSs that extends the traditional optimizer strategy widely used in commercial DBMSs to allow execution of queries over both known (i.e., proprietary) DBMSs and foreign vendor DBMSs that conform to some standard such as providing the usual relational database statistics. We assume that participating DBMSs are autonomous and may not be able, even if willing, to provide the cost model parameters. The novelty of the strategy is to deduce the necessary information by calibrating a given DBMS. As the calibration has to be done as a user, not as a system administrator, it poses unpredictability problems such as inferring the access methods used by the DBMS, idiosyncrasies of the storage subsystem and coincidental clustering of data. In this paper we propose a calibrating database which is synthetically created so as to make the process of deducing the cost model coefficients reasonably devoid of the unpredictability problems. Using this procedure, we calibrate three commercial DBMSs, namely Allbase, DB2, and Informix, and observe that in 80% of the cases the estimate is quite accurate.	analysis of algorithms;autonomous robot;cluster analysis;coefficient;ibm informix;mathematical optimization;program optimization;query optimization;relational database;system administrator	Weimin Du;Ravi Krishnamurthy;Ming-Chien Shan	1992			sargable;query optimization;query expansion;computer science;query by example;database;rdf query language;web search query;query language;object query language	DB	-36.83179529716052	2.1389792091124016	79945
8a4f56f769d8ec1c50017c26359941c171af0891	i/o-efficiency of shortest path algorithms: an analysis	directed graphs;programming theory database theory directed graphs;shortest path algorithm;longest path;algorithm design and analysis database languages bills of materials computer science graph theory operations research shortest path problem algebra materials reliability reliability theory;input output;programming theory;multisource algorithms input output efficiency single source algorithms all pairs algorithms i o efficiency shortest path algorithms paging environment path problems longest paths most reliable paths bill of materials;bill of material;database theory	Bin J iang Department of Computer Science Information Systems-Databases ETH Zurich CH-8092 Zurich, Switzerland The ability of ezpressing path problems, such as shortest paths or bill of materials, is considered to be a substantial eztension of conventional database languages. To realize this eztension e f i ciently, path algorithms from gmph theory are used. There exist a lot of path algorithms. Most of them, however, were designed and investigated mainly in the contezt of main memo y. In this paper, we unalyre the I/O-eficiency of seveml representative shortest path algorithms. This includes single-source, multi-source, and all pairs algorithms. In addition, some of their properties regarding database applications are discussed.	algorithm;computer science;information system;input/output;multi-source;shortest path problem;switzerland	Bin Jiang	1992		10.1109/ICDE.1992.213212	input/output;database theory;canadian traveller problem;directed graph;dijkstra's algorithm;widest path problem;constrained shortest path first;any-angle path planning;longest path problem;computer science;theoretical computer science;machine learning;yen's algorithm;distributed computing;shortest path problem;distance;k shortest path routing;shortest path faster algorithm	DB	-42.944981932608364	-6.0602692916433245	79977
a45ca5607c0b55250479282c67dffed19450172d	an ontology-guided annotation system for technology monitoring		Currently, in the field of technology monitoring, it is very important to be able to get relevant information from heterogeneous sources, especially on the World Wide Web. The coming of Semantic Web technologies promises intelligent retrieval and access to information through the use of semantic annotations based on ontologies. In a scenario of technology monitoring, agents are useful not only for handling semantic annotations to collect information, but also for automatic generation of these annotations from Web documents. In this article, we describe a new approach based on an ontology for building a multi-agent technology monitoring system.	agent-based model;algorithm;dublin core;freedom of information laws by country;information extraction;information source;information system;multi-agent system;named entity;ontology (information science);peripheral interface adapter;ricochet;semantic web;web page;world wide web	Tuan-Dung Cao;Rose Dieng;Bruno Fiés	2004			ontology;annotation;computer science;bioinformatics	AI	-40.406247225610166	2.7882361990340825	80292
1a678c1dfdefac3b15b7c2e803fe9436a31f1e1e	geological information forecast and 3d reconstruction based on support vector machine	geological engineering geological information forecast 3d reconstruction support vector machine geological drill hole data discretization meshwork model bp neural network;remote education;kernel;geological information forecast;support vector machines;support vector machines computational geometry geology geophysics computing solid modelling;predictive value;computational geometry;remote education web services information integration;discretization meshwork model;web service;geological drill hole data;information integration;3d model;geophysics computing;geology;three dimensional displays;bp neural network;surface model;solid modeling;three dimensional displays support vector machines solid modeling ores data models kernel;ores;web services;geological engineering;support vector machine;3d reconstruction;solid modelling;topological relation;data models;neural network	By analyzing the trend of CORBA and Web Service technology and the bottleneck problem of remote education system, a new design methodology for remote education system is proposed based on CORBA and Web Service to achieve resource share and interoperability. In Local Area Network, the CORBA is used to construct education system in heterogeneous distributed environment. Then, with the service registration and service detection of Web Service, we can achieve the web application extension of CORBA object. The result shows that the method offers a new way of building efficient and flexible remote education system.	3d modeling;3d reconstruction;algorithm;common object request broker architecture;interoperability;simulation;solid modeling;support vector machine;web application;web service	Wu HuiXin;Wang Feng	2010	2010 International Conference on E-Business and E-Government	10.1109/ICEE.2010.886	web service;support vector machine;interoperable object reference;computational geometry;computer science;engineering;artificial intelligence;data science;theoretical computer science;machine learning;data mining	Mobile	-38.24118339633907	-2.3047037290065253	80797
2457ceb29e62d30830bf6afc500207f265991708	www-udk: a web-based environmental meta-information system	front end;tools and techniques;world wide web;information system	The environmental data catalogue Umweltdatenkatalog UDK is a standard meta-information system for environmental data for use by state authorities and the public. Technically, the UDK consists of a database together with a front-end tailored to the needs of environmental specialists. FZI's contribution has been to develop a front-end that makes the UDK database available using the tools and techniques of the World-Wide Web. Among the features of WWW-UDK are several query modes for the UDK objects and addresses, an environmental thesaurus, on-line access to some of the underlying data (e.g., databases and environmental reports), multilingual query and result forms, and an on-line help system. Currently, several installations of WWW-UDK are used in Austria and in Germany on the Internet and on Intranets. WWW-UDK can be easily integrated into a federation architecture which is based on CORBA, WWW, and Java.	common object request broker architecture;database;front and back ends;information system;intranet;java;online and offline;thesaurus;unreal development kit;www;world wide web	Ralf Kramer;Ralf Nikolai;Arne Koschel;Claudia Rolker;Peter C. Lockemann;Andree Keitel;Rudolf Legat;Konrad Zirm	1997	SIGMOD Record	10.1145/248603.248607	computer science;front and back ends;data mining;database;world wide web;information system	DB	-41.89521996095835	-3.3580092457666857	80890
06d453b0d7cddca651e04b3932452fb4839b7a89	ontology-based, tissue microarray oriented, image centered tissue bank	software;data integrity;database management systems;semantics;computational biology bioinformatics;medical records systems computerized;terminology as topic;tissue array analysis;information exchange;tissue microarray;italy;algorithms;databases factual;combinatorial libraries;process analysis;computer appl in life sciences;natural language processing;biological process;microarrays;bioinformatics;gene ontology	Tissue MicroArray technique is becoming increasingly important in pathology for the validation of experimental data from transcriptomic analysis. This approach produces many images which need to be properly managed, if possible with an infrastructure able to support tissue sharing between institutes. Moreover, the available frameworks oriented to Tissue MicroArray provide good storage for clinical patient, sample treatment and block construction information, but their utility is limited by the lack of data integration with biomolecular information. In this work we propose a Tissue MicroArray web oriented system to support researchers in managing bio-samples and, through the use of ontologies, enables tissue sharing aimed at the design of Tissue MicroArray experiments and results evaluation. Indeed, our system provides ontological description both for pre-analysis tissue images and for post-process analysis image results, which is crucial for information exchange. Moreover, working on well-defined terms it is then possible to query web resources for literature articles to integrate both pathology and bioinformatics data. Using this system, users associate an ontology-based description to each image uploaded into the database and also integrate results with the ontological description of biosequences identified in every tissue. Moreover, it is possible to integrate the ontological description provided by the user with a full compliant gene ontology definition, enabling statistical studies about correlation between the analyzed pathology and the most commonly related biological processes.	bioinformatics;british informatics olympiad;compliance behavior;experiment;gene ontology;information exchange;ontology (information science);patients;question (inquiry);tissue bank;tissue microarray;web resource	Federica Viti;Ivan Merelli;Andrea Caprera;Barbara Lazzari;Alessandra Stella;Luciano Milanesi	2008	BMC Bioinformatics	10.1186/1471-2105-9-S4-S4	biology;tissue microarray;dna microarray;information exchange;computer science;bioinformatics;data science;data integrity;data mining;semantics;biological process	Comp.	-41.05411104910645	1.9079065530619705	81116
0e0a900ae2e2320c6dffd4ae793328ca11b31560	parallel and distributed collaborative filtering: a survey	collaborative filtering;recommender systems	Collaborative filtering is among the most preferred techniques when implementing recommender systems. Recently, great interest has turned toward parallel and distributed implementations of collaborative filtering algorithms. This work is a survey of parallel and distributed collaborative filtering implementations, aiming to not only provide a comprehensive presentation of the field's development but also offer future research directions by highlighting the issues that need to be developed further.	algorithm;collaborative filtering;recommender system	Efthalia Karydi;Konstantinos G. Margaritis	2016	ACM Comput. Surv.	10.1145/2951952	computer science;collaborative filtering;data mining;distributed computing;world wide web;recommender system	ECom	-47.48290250939115	0.1392452920369273	81327
97ad5a3a1daf30c6cea0745257c3d9da0f37e687	linking objectives to actions: a decision support approach based on cause-effect linkages	decision support;and strategic decision making;decision support systems;planning strategy;causal models	ABSTRACT#R##N##R##N#The process of translating objectives into actions is a difficult task. This difficulty is due to the wide range of possibilities and the lack of structured information. Managers must take into account relevant information and generate a range of options before a decision is reached. So far, little is available to guide managers in translating a set of objectives into actions. This paper presents a three-stage action-planning process to address this gap. The process, supported by a software tool, takes managers through the stages of model building, action generation, and action evaluation and selection. A case study illustrates the application of the process. The paper concludes by discussing the implication of this work for managers and academics.		Kim Hua Tan;Ken Platts	2003	Decision Sciences	10.1111/j.1540-5414.2003.02257.x	decision support system;decision analysis;computer science;knowledge management;management science;management;causal model	AI	-33.7296111107048	-7.782320614752232	81409
c9934724a1768be28c388dfaa8fa6935ab5a61a9	how big is big? amount of data in intensive care units datamart platform			data mart	M. Li;Ing Tiong;John Dyke;Sergiu Plamadeala;Vitali Fedosov;Brian W. Pickering;Vitaly Herasevich	2016				ML	-38.91711376080566	-6.287850918334047	81570
b20d8b903f28360e956a7a2c54a2602158600b30	an information system to analize cultural heritage information	new kind;important task;conservation work;cultural heritage information;information system;example application;cultural heritage information systems;large amount;cultural heritage site;important relationship	Managing information related to cultural heritage sites is an important task and much work has been devoted to developing special purpose document management systems. These systems are able to store and retrieve large amounts of documents; however, while this is adequate for some purposes, it is not sufficient for research and conservation work. Researchers need to determine relationships between data, and the most important relationships in cultural heritage information are spatial relationships. A new kind of information system is therefore needed, in which the 3D representation of an object is a blackboard on which all data is represented. This paper proposes the concept of Cultural Heritage Information Systems, and presents our implementation of the system. An example application illustrating the use of the system is also presented.	document;granada;information system;linear algebra;personal computer;visual artifact	Juan Carlos Torres;L. López;Celia Romo;Francisco Ortin	2012		10.1007/978-3-642-34234-9_86	engineering;knowledge management;data mining;multimedia	HCI	-44.02284923213822	0.2985821145671816	81671
9071ed6df9d85017b4d3f2eab8b2cfc3a1d5156c	data transformation and semantic log purging for process mining	process mining;data transformation;log purging;process constraints	Existing process mining approaches are able to tolerate a certain degree of noise in process log. However, processes that contain infrequent paths, multiple (nested) parallel branches, or have been changed in an ad-hoc manner, still pose challenges. For such cases, process mining typically returns “spaghetti-models”, that are hardly usable even as a starting point for process (re-)design. In this paper, we address these challenges by introducing data transformation and pre-processing steps that improve and ensure the quality of mined models for existing process mining approaches. We propose the concept of semantic log purging, i.e., the cleaning of logs based on domain specific constraints utilizing knowledge that typically complements processes. Furthermore we demonstrate the feasibility and effectiveness of the approach based on a case study in the higher education domain. We think that semantic log purging will enable process mining to yield better results, thus giving process (re-)designers a valuable tool.	consistency model;embedded system;heterogeneous element processor;heuristic (computer science);hoc (programming language);intelligibility (philosophy);microsoft outlook for mac;mined;plasma cleaning;preprocessor;process modeling	Linh Thao Ly;Conrad Indiono;Juergen Mangler;Stefanie Rinderle-Ma	2012		10.1007/978-3-642-31095-9_16	computer science;engineering;data science;machine learning;data mining;database;process mining;data transformation;world wide web	SE	-36.631163572251076	1.6599167345224215	81806
ee507c14856a8c94e140455b444cbce7a6b65779	rstore: efficient multiversion document management in the cloud		"""Motivation.The iterative and exploratory nature of the data science process, combined with an increasing need to support debugging, historical queries, auditing, provenance, and reproducibility, warrants the need to store and query a large number of versions of a dataset. This realization has led to many efforts at building data management systems that support versioning as a first-class construct, both in academia [1, 3, 5, 6] and in industry (e.g., git, Datomic, noms). These systems typically support rich versioning/branching functionality and complex queries over versioned information but lack the capability to host versions of a collection of keyed records or documents in a distributed environment or a cloud. Alternatively, key-value stores1 (e.g., Apache Cassandra, HBase, MongoDB) are appealing in many collaborative scenarios spanning geographically distributed teams, since they offer centralized hosting of the data, are resilient to failures, can easily scale out, and can handle a large number of queries efficiently. However, those do not offer rich versioning and branching functionality akin to hosted version control systems (VCS) like GitHub. This work addresses the problem of compactly storing a large number of versions (snapshots) of a collection of keyed documents or records in a distributed environment, while efficiently answering a variety of retrieval queries over those.  RStore Overview. Our primary focus here is to provide versioning and branching support for collections of records with unique identifiers. Like popular NoSQL systems, RStore supports a flexible data model; records with varying sizes, ranging from a few bytes to a few MBs; and a variety of retrieval queries to cover a wide range of use cases. Specifically, similar to NoSQL systems, our system supports efficient retrieval of a specific record in a specific version (given a key and a version identifier), or the entire evolution history for a given key. Similar to VCS, it supports retrieving all records belonging to a specific version to support use cases that require updating a large number of records (e.g., by applying a data cleaning step). Finally, since retrieving an entire version might be unnecessary and expensive, our system supports partial version retrieval given a range of keys and a version identifier.  Challenges. Addressing the above desiderata poses many design and computational challenges, and natural baseline approaches (see full paper [2] for more details) that attempt to build this functionality on top of existing key-value stores suffer from critical limitations. First, most of those baseline approaches cannot directly support point queries targetting a specific record in a specific version (and by extension, full or partial version retrieval queries), without constructing and maintaining explicit indexes. Second, all the viable baselines fundamentally require too many back-and-forths between the retrieval module and the backend key-value store; this is because the desired set of records cannot be succinctly described as a query. Third, ingest of new versions is difficult for most of the baseline approaches. Finally, exploiting """"record-level compression"""" is difficult or impossible in those approaches; this is crucial to be able to handle common use cases where large records (e.g., documents) are updated frequently with relatively small changes.  Key Ideas. To address these problems, RStore features a new architecture that partitions the distinct records into approximately equal-sized """"chunks"""", with the goal to minimize the number of chunks that need to be retrieved for a given query workload [2]. We establish that the system can adapt to different data and workload requirements through a few simple tuning knobs. The key computational challenge boils down to deciding how to optimally partition the records into chunks; we draw connections to well-studied problems like compressing bipartitite graphs and hypergraph partitioning to show that the problem is NP-Hard in general. Our system features a novel algorithm, that exploits the structure of the version graph, to find an effective partitioning of the records and is built on top of Apache Cassandra. An extensive experimental evaluation is performed over a large number of synthetically constructed datasets to show the effectiveness of RStore and to validate our design decisions."""	algorithm;apache cassandra;apache hbase;approximation;attribute–value pair;baseline (configuration management);byte;centralized computing;cloud computing;control system;data model;data science;datomic;debugging;file spanning;graph partition;iteration;key-value database;mongodb;np-hardness;nosql;plasma cleaning;requirement;scalability;snapshot (computer storage);unique identifier;veritas cluster server;version control	Souvik Bhattacherjee;Amol Deshpande	2017		10.1145/3127479.3132693	nosql;computer science;scalability;software versioning;data model;cloud computing;database;data management;unique identifier;document management system	DB	-34.75172373026423	1.7274420955780212	82565
a5081939c059add7ed0754cee736e1f272916de3	towards heterogeneous keyword search	keyword search;heterogeneous system;entity relationship	Keyword search is a widely popular mechanism for query processing that alleviates users from understanding complex data structures and learning query languages. Existing keyword search systems are designed and tuned for one specific data model. In big data era, data is usually resident in heterogeneous data sources including unstructured data, semi-structured data and structured data. In order to obtain complete and meaningful results, a system is required to perform keyword search queries upon diverse data sources rather than just one type of data sources. One possible approach is to send the keyword query to all the systems simultaneously, then integrate the returned answers from each data source to obtain final answers. However, answers from different data sources have different formats and may contain duplicates, which yields the heterogeneous search challenge.  In this paper we introduce a heterogeneous keyword search system that facilitates integrating answers from diverse data sources. We present several detailed design challenges and also share our preliminary thoughts to the challenges. In particular, we (i) propose a unified result format entity-relationship pattern (ERP), (ii) define new ranking functions, (iii) build native index structures, (iv) provide an efficient global top-k processing algorithm, and (v) introduce the fuzzy entity mapping problem. We have built a prototype HKSearch based on our current solutions to support heterogeneous keyword search.	big data;data model;data structure;database;erp;emoticon;entity–relationship model;prototype;query language;search algorithm;semi-structured data;semiconductor industry;web search query	Chunbin Lin;Jianguo Wang;Chuitian Rong	2017		10.1145/3063955.3064802	computer science;data mining;database;information retrieval	DB	-34.15368976598354	3.5182989882658577	82577
435b4b89f581d107ffa21501707bc34ed3fe3070	logic-based retrieval: technology for content-oriented and analytical querying of patent data	query processing	Patent searching is a complex retrieval task. An initial document search is only the starting point of a chain of searches and decisions that need to be made by patent searchers. Keyword-based retrieval is adequate for document searching, but it is not suitable for modelling comprehensive retrieval strategies. DB-like and logical approaches are the state-of-the-art techniques to model strategies, reasoning and decision making. In this paper we present the application of logical retrieval to patent searching. The two grand challenges are expressiveness and scalability, where high degree of expressiveness usually means a loss in scalability. In this paper we report how to maintain scalability while offering the expressiveness of logical retrieval required for solving patent search tasks. We present logical retrieval background, and how to model data-source selection and results’ fusion. Moreover, we demonstrate the modelling of a retrieval strategy, a technique by which patent professionals are able to express, store and exchange their strategies and rationales when searching patents or when making decisions. An overview of the architecture and technical details complement the paper, while the evaluation reports preliminary results on how query processing times can be guaranteed, and how quality is affected by trading off responsiveness.		Iraklis A. Klampanos;Hengzhi Wu;Thomas Roelleke;Hany Azzam	2010		10.1007/978-3-642-13084-7_9	natural language processing;computer science;data mining;database;information retrieval	DB	-36.52409304722328	3.759994679947744	82769
3b9f0abe481b2bb328d05ff81bf640121303b56e	from expert knowledge to formal ontologies for semantic interpretation of the urban environment from satellite images	urban environment;owl;swrl;semantic interpretation;satellite imagery;ontology	The widespread introduction of satellite imagery with several spatial, spectral and temporal resolutions is a real opportunity to analyze and characterize the urban environment. These last few years, the Object-Based Image Analysis OBIA approach has been largely developed and applied for urban applications. However, a major issue in this approach is domain knowledge formalization and exploitation. In this paper, after a detailed presentation of related works in the domain of semantic interpretation using ontologies, a taxonomy of urban objects and a framework to describe urban objects and their spatial organization is presented. This taxonomy is then formalized in a domain ontology using the semantic web ontology language and rules OWL and SWRL.	ontology (information science);semantic interpretation	François de Bertrand de Beuvron;Stella Marc-Zwecker;Anne Puissant;Cecilia Zanni-Merk	2013	KES Journal	10.3233/KES-130264	upper ontology;semantic interpretation;computer science;knowledge management;ontology;artificial intelligence;ontology;data mining;owl-s;information retrieval	AI	-35.56275414078175	-3.733781424098585	82784
cac3ae1f9b142a867175bd588c7e25d6bfb5de95	a plan for olap: optimization of financial planning queries in data warehouse systems			online analytical processing	Bernhard Jäcksch	2011			online analytical processing;database;financial plan;data mining;dimensional modeling;data warehouse;computer science	DB	-39.55614329304853	-5.109243453521324	83019
ba28e290f7189b7ecafa5ee34e13a6349758ffd3	"""erratum to """"macro/micro-mobility fast handover in hierarchical mobile ipv6"""" [computer communications, 29 (2006) 611-617]"""	hierarchical mobile ipv6;micro-mobility fast handover;computer communications	Erratum to ‘‘Macro/micro-mobility fast handover in hierarchical mobile IPv6’’ [Computer Communications, 29 (2006) 611–617] M.H. Habaebi *, I. Vivaldi , B.M. Ali , V. Prakash b a Computer Engineering, Faculty of Engineering, Alfateh University, Afurnaj Campus—Tripoli, P.O. Box 83416, Tripoli, Libyan Arab Jamahiriya b Department of Computer and Communications Engineering, Universiti Putra Malaysia, Serdang 43400, Malaysia c Capacity Management Department, PT Aplikanusa Lintasarta, Jakarta, Indonesia	computer engineering;mobile ip	Mohammed Hadi Habaebi;I. Vivaldi;Borhanuddin Mohd Ali;Veeraraghavan Prakash	2006	Computer Communications	10.1016/j.comcom.2006.04.002	real-time computing;telecommunications;computer network	DB	-45.26095185319629	-7.276005880654315	83209
1ef20717f985c23a05057fc1436e2d90443023d0	gazetteer enrichment for addressing urban areas: a case study	geoparsing;openstreetmap;vgi;twitter;gazetteer	The advent of volunteered geographical information VGI has contributed to the growth of the amount of user-contributed spatial data around the world. Spatial data acquired from crowdsourcing environments may contain valuable information which can be useful in other research fields, such as Digital gazetteers, commonly used in Geographic Information Retrieval. Digital gazetteers have a powerful role in the geoparsing process. They need to be kept up-to-date and as comprehensive as possible to enable geoparsers to perform lookup and then resolve toponym recognition precisely over digital documents. The detection of toponyms in digital texts such as social media posts is a bottom line for discovering useful spatially related information such as complaints regarding urban areas. In this context, this article proposes a method for gazetteer enrichment leveraging VGI data sources. Indeed VGI environments are not originally developed to work as gazetteers, however, they often contain more detailed and up-to-date information than gazetteers. Our method is applied within a geoparser environment by adapting its heuristics set besides enriching the corresponding gazetteer. A case study was performed by geoparsing Twitter posts focusing solely on the messages aiming at evaluating the performance of the enriched system. The obtained results were encouraging and have provided a good basis for discussion.	gene ontology term enrichment	Maxwell Guimarães de Oliveira;Cláudio Elízio Calazans Campelo;Cláudio de Souza Baptista;Michela Bertolotto	2016	J. Location Based Services	10.1080/17489725.2016.1196755	data mining;volunteered geographic information;database;multimedia;world wide web;cartography;remote sensing	HCI	-40.702410501464904	-3.2159400295444773	83322
7ebf43b7e61a6d9b83c08c9951433c50dc0d03a5	combining heterogeneous lexical resources		One of the main tasks of the Natural Language Processing Group at the Faculty of Mathematics, University of Belgrade is the development of various lexical resources. Among them the two most important ones are: the system of morphological dictionaries of Serbian (SMD) in Intex format and the Serbian wordnet (SWN) developed in the scope of the Balkanet project. Although these two resources represent dictionaries of a different type, developed using different models, each of them contains information that can either be incorporated in to the other dictionary or that can be used in its development. In this paper we will outline some of the most interesting examples. W e also present an integrated programming tool that enables the integration of these diverse lexical resources, as well as possible applications. We envisage the use of these resources in defining and linking lexical data in a way that will enable their more effective retrieval, integration, and reuse across various Web applications.	lexicon;linker (computing);molecular dynamics;morphological dictionary;natural language processing;programming tool;service mapping description;web application;wordnet	Cvetana Krstev;Dusko Vitas;Ranka Stankovic;Ivan Obradovic;Gordana Pavlovic-Lazetic	2004			artificial intelligence;natural language processing;web application;reuse;computer science;wordnet;serbian	NLP	-42.529852389310676	3.6789931030420293	83410
6b10f5b0ed8dd5f50ae4533e130123edb752ef7a	archiving pushed inferences from sensor data streams		Although pervasively deployed, sensors are currently neither highly interconnected nor very intelligent, since they do not know each other and produce only raw data streams. This lack of interoperability and high-level reasoning capabilities are major obstacles for exploiting the full potential of sensor data streams. Since interoperability and reasoning processes require a common understanding, RDF based linked sensor data is used in the semantic sensor web to articulate the meaning of sensor data. This paper shows how to derive higher levels of streamed sensor data understanding by constructing reasoning knowledge with SPARQL. In addition, it is demonstrated how to push these inferences to interested clients in different application domains like social media streaming, weather observation and intelligent product lifecycle maintenance. Finally, the paper describes how real-time pushing of inferences enables provenance tracking and how archiving of inferred events could support further decision making processes.	archive;file archiver;high- and low-level;interoperability;real-time locating system;sparql;semantic sensor web;social media;streaming media	Jörg Brunsmann	2010			world wide web;data stream mining;internet privacy;computer science	AI	-38.55969347323079	0.42341597174336404	83506
a24668d312f35cf67a9688e6dafcc2462ecc2359	an ontological representation of the digital library evaluation domain	digital libraries;article δημοσίeυση πeριοδικού;ontologies;evaluation;analytic models;domain analysis	Digital library evaluation is a complex field, as complex as the phenomena it studies. The interest of the digital library society still remains vibrant after all these years of solidification, as these systems have entered real-life applications. However the community has still to reach a consensus on what evaluation is and how it can effectively be planned. In the present article, an ontology of the digital library evaluation domain, named DiLEO, is proposed, aiming to reveal explicitly the main concepts of this domain and their correlations, and it tries to combine creatively and integrate several scientific paradigms, approaches, methods, techniques, and tools. This article demonstrates the added value features of the ontology, which are the support of comparative studies between different evaluation initiatives and the assistance in effective digital library evaluation planning.	digital library;real life	Giannis Tsakonas;Christos Papatheodorou	2011	JASIST	10.1002/asi.21559	domain analysis;digital transformation;digital library;social science;computer science;ontology;data science;evaluation;data mining;world wide web;information retrieval	HCI	-44.89229663635789	2.902266890065348	83540
0cf556453a687da2263e82b4901e93dc78a2a5f6	publishing e-rdf linked data for many agents by single third-party server		Linked data is one of the most successful practices in semantic web, which has led to the opening and interlinking of data. Though many agents (mostly academic organizations and government) have published a large amount of linked data, numerous agents such as private companies and industries either do not have the ability or do not want to make an additional effort to publish linked data. Thus, for agents who are willing to open part of their data but do not want to make an effort, the task can be undertaken by a professional third-party server (together with professional experts) that publishes linked data for these agents. Consequently, when a single third-party server is on behalf of multiple agents, it is also responsible to organize these multiple-source URIs (data) in a systematic way to make them referable, satisfying the 4-star data principles, as well as protect the confidential data of these agents. In this paper, we propose a framework to leverage these challenges and design a URI standard based on our proposed E-RDF, which extends and optimizes the existing 5-star linked data principles. Also, we introduce a customized data filtering mechanism to protect the confidential data. For validation, we implement a prototype system as a third-party server that publishes linked data for a number of agents. It demonstrates well-organized 5-star linked data plus E-RDF and shows the additional advantages of data integration and interlinking among agents.	linked data	Dongsheng Wang;Yongyuan Zhang;Zhengjun Wang;Tao Chen	2017		10.1007/978-3-319-70682-5_10	rdf;world wide web;knowledge representation and reasoning;linked data;semantic web;data integration;web service;publication;government;business	ML	-42.48439277751422	0.02244526417697223	83731
fa1d95a4821732364a0d9123c69baaa93c33383b	a human-centered approach for interactive data processing and analytics		In recent years, the amount of data increases continuously. With newly emerging paradigms, such as the Internet of Things, this trend will even intensify in the future. Extracting information and, consequently, knowledge from this large amount of data is challenging. To realize this, approved data analytics approaches and techniques have been applied for many years. However, those approaches are oftentimes very static, i.e., cannot be dynamically controlled. Furthermore, their implementation and modification requires deep technical knowledge only technical experts can provide, such as an IT department of a company. The special needs of the business users are oftentimes not fully considered. To cope with these issues, we introduce in this article a human-centered approach for interactive data processing and analytics. By doing so, we put the user in control of data analytics through dynamic interaction. This approach is based on requirements derived from typical case scenarios.	interactive computing;interactivity	Michael Behringer;Pascal Hirmer;Bernhard Mitschang	2017		10.1007/978-3-319-93375-7_23	data mining;special needs;computer science;visual analytics;data analysis;internet of things;analytics;human-in-the-loop;data processing	DB	-37.3314925040386	-6.502804111285997	83835
03ee9d9dde3fc6d8529ca253d41bfec444ae95ba	hicomb introduction	hicomb introduction	Welcome to the 12th IEEE International Workshop on High Performance Computational Biology (HiCOMB). The interdisciplinary field of computational biology and bioinformatics is at the verge of several exciting possibilities owing to a rapid introduction of many disruptive experimental technologies to procure data. The resulting preponderance of data and the inherent complexity of processing have collectively placed an enormous demand on the computational methods that seek to model and analyze biological data — a demand that can be met only through a comprehensive embrace of high performance computing. The goal of this workshop is to provide a forum for discussion of the latest research in the design and development of high performance computing solutions to dataand compute-intensive problems arising from molecular biology and related life sciences.	bioinformatics;computation;computational biology;procurement;supercomputer;the verge	Mark J. Clement;Quinn Snell;Srinivas Aluru;David A. Bader	2012	2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum	10.1109/IPDPSW.2012.338	computational biology;computational science;biological computation;supercomputer;computer science;bioinformatics	HPC	-47.72745577647425	-0.3560800224645274	83890
438e2116fc742d4472ac31c6bb340b0f0015dac3	exploring the netherlands on a semantic path		This poster gives an overview about the web application “vakantieland.nl”, a Dutch Internet portal in the tourism domain. The core functionality is to provide information about holiday destinations, accommodation and other tourism related points-of-interest as well as a corresponding visualization with a mapping service. Because of the underlying semantic data structures and alternatively generated RFD output, vakantieland.nl can be considered to be a Semantic Web application. Its realization and functionalities, social aspects and furthermore an outlook about future development work constitute the main part of the poster.	data structure;microsoft outlook for mac;semantic web;web application;web mapping	Michael Martin	2007			multimedia;the internet;semantic data model;semantic web;natural language processing;tourism;visualization;artificial intelligence;computer science	Web+IR	-39.84220274925954	1.8378886826131102	83949
38b2dd8df39727b781cd2d19d422a97bdd527e9d	biological knowledge management: the emerging role of the semantic web technologies	gestion des connaissances;representacion conocimientos;ontologie;technology;systems biology;web semantique;knowledge management;semantic web technology;integration de donnees;biologia de sistemas;web semantica;technologie;system biology;representation connaissance;semantic web;biology and life sciences;ontologia;biologie systemique;knowledge representation;ontology;gestion conocimiento;data integration;tecnologia	New knowledge is produced at a continuously increasing speed, and the list of papers, databases and other knowledge sources that a researcher in the life sciences needs to cope with is actually turning into a problem rather than an asset. The adequate management of knowledge is therefore becoming fundamentally important for life scientists, especially if they work with approaches that thoroughly depend on knowledge integration, such as systems biology. Several initiatives to organize biological knowledge sources into a readily exploitable resourceome are presently being carried out. Ontologies and Semantic Web technologies revolutionize these efforts. Here, we review the benefits, trends, current possibilities, and the potential this holds for the biosciences.	101 mouse;achievement;binocular disparity;biological science disciplines;cell nucleus;computer;computers;database;deploy;diabetes mellitus;elfacos ow 100;extraction;genetic heterogeneity;handling (psychology);inference;interaction;interoperability;knowledge integration;knowledge management;knowledge representation and reasoning;least squares;middleware;name;norsk data;ontology (information science);pancreatic carcinoma;paper;portals;preparation;recommender system;repository;scalability;semantic web;software deployment;systems biology;world wide web;benefit;contents - htmllinktype;standards characteristics	Erick Antezana;Martin Kuiper;Vladimir Mironov	2009	Briefings in bioinformatics	10.1093/bib/bbp024	computer science;bioinformatics;knowledge management;artificial intelligence;data integration;mathematical knowledge management;semantic web;data mining;personal knowledge management;systems biology;domain knowledge;technology	Web+IR	-35.731253803444076	-8.564328637092567	83990
615e844d2bdce0d3d80aeefcb3b201fb24b62b1f	utilizing, creating and publishing linked open data with the thesaurus management tool poolparty	semantic web;rdf;personal information management;skos	We introduce the Thesaurus Management Tool (TMT) PoolParty based on Semantic Web standards that reduces the effort to create and maintain thesauri by utilizing Linked Open Data (LOD), text-analysis and easy-to-use GUIs. PoolParty’s aim is to lower the access barriers to managing thesauri, so domain experts can contribute to thesaurus creation without needing knowledge about the Semantic Web. A central feature of PoolParty is the enriching of a thesaurus with relevant information from LOD sources. It is also possible to import and update thesauri from remote LOD sources. Going a step further we present a Personal Information Management tool built on top of PoolParty which relies on Open Data to assist the user in the creation of thesauri by suggesting categories and individuals retrieved from LOD sources. Additionally PoolParty has natural language processing capabilities enabling it to analyse documents in order to glean new concepts for a thesaurus and several GUIs for managing thesauri varying in their complexity. Thesauri created with PoolParty can be published as Open Knowledge according to LOD best practices.	best practice;linked data;natural language processing;open knowledge;personal information management;semantic web;thesaurus (information retrieval);web standards	Thomas Schandl;Andreas Blumauer	2010			linked data;open data;semantic web;best practice;information retrieval;personal information management;publishing;open knowledge;computer science	Web+IR	-41.97667538800296	3.349358977946689	84166
a183cad3b935f95b7792375e110f3eeee6ad2938	imgpedia: enriching the web of data with image content analysis		Linked Data rarely takes into account multimedia content, which forms a central part of the Web. To explore the combination of Linked Data and multimedia, we are developing IMGpedia: we compute content-based descriptors for images used in Wikipedia articles and subsequently propose to link these descriptions with legacy encyclopaedic knowledge-bases such as DBpedia and Wikidata. On top of this extended knowledge-base, our goal is to consider a unified query system that accesses both the encyclopaedic data and the image data. We could also consider enhancing the encyclopaedic knowledge based on rules applied to co-occurring entities in images, or content-based analysis, for example. Abstracting away from IMGpedia, we explore generic methods by which the content of images on the Web can be described in a standard way and can be considered as first-class citizens on the Web of Data, allowing, for example, for combining structured queries with image similarity search. This short paper thus describes ongoing work on IMGpedia, with focus on image descriptors.	dbpedia;entity;linked data;semantic web;similarity search;visual descriptor;wikidata;wikipedia;world wide web	Sebastián Ferrada;Benjamin Bustos;Aidan Hogan	2016			world wide web;content analysis;computer science	Web+IR	-42.249256248937925	2.4536203854048573	84214
1b82b05cbb5a9ed2ee6c859f92d384c0ccd7b7f1	current approaches, challenges, and perspectives on spatial olap for agri-environmental analysis		Spatial OLAP (SOLAP) systems are powerful GeoBusiness Intelligence tools for analysing massive volumes of geo-referenced datasets. Therefore, these technologies are receiving considerable attention in the research community and in the database industry as well. Applications of these technologies are current in several domains such as ad marketing, healthcare, and urban development, to name a few. Contrary to other application domains, in the context of agri-environmental data and analysis, SOLAP systems have been underexploited. Therefore, in this paper, the author makes an exhaustive survey of most of the published studies in the domain of the SOLAP analysis of agri-environmental data with an emphasis on the reasons why only few recent works investigate the use of SOLAP systems in the agri-environmental context. In particular, the author focuses on the complexity of the spatio-multidimensional model and its implementation. Moreover, based on surveying the state of the art in this domain, this paper identifies some general guidelines that must be considered by the scientific community to design and implement efficient SOLAP approaches to the analysis of geo-referenced agri-environmental datasets. Finally, open issues about warehousing and OLAPing agri-environmental data are also shown in the paper.	online analytical processing;spatial analysis	Sandro Bimonte	2016	IJAEIS	10.4018/IJAEIS.2016100103	computer science;data science;data mining;database;management science	ML	-37.63258954624521	-6.939022270005165	84300
c7eee3664716a68d3238f2204fcc32e77735d804	special section international parallel processing projects: a software perspective	parallel processing	Industrial and academic researchers provide a glimpse into efforts to develop a faster, more powerful computer through methods beyond purely technological advances.	computer;parallel processing (dsp implementation)	Joanne L. Martin	1985	IEEE Software	10.1109/MS.1985.231378	computer science;software engineering	Visualization	-48.2537253399421	-1.333349170722805	84555
a983a1ee145f7c7ac0b976330021dd1183208add	design and validation of a light-weight reasoning system to support remote health monitoring applications	decision support systems;inference algorithms;patient monitoring;knowledge engineering	Recently, mobile devices have dramatically improved their communications and processing capabilities, so enabling the possibility of embedding knowledge-based decision support components within Remote Health Monitoring (RHM) applications for the ubiquitous and seamless management of chronic patients. According to these considerations, this paper presents a light-weight, rule-based, reasoning system, purposely designed and optimized to build knowledge-based Decision Support Systems efficiently embeddable in mobile devices. The key issues of such a system are both a domain-independent reasoning algorithm and knowledge representation capabilities, specifically thought for both computation intensive and real-time RHM scenarios. The performance evaluation of the proposed system has been arranged according to the Taguchi's experimental design and performed directly on a mobile device in order to quantitatively assess its effectiveness in terms of memory usage and response time. Moreover, a case study has been arranged in order to evaluate the effectiveness of the proposed system within a real RHM application for monitoring cardiovascular diseases. The evaluation results show that the system offers an innovative and efficient tool to build mobile DSSs for healthcare applications where real-time performance or computation intensive demands have to be met.	reasoning system	Aniello Minutolo;Massimo Esposito;Giuseppe De Pietro	2015	Eng. Appl. of AI	10.1016/j.engappai.2015.01.019	simulation;computer science;knowledge management;artificial intelligence;machine learning;knowledge engineering;remote patient monitoring;data mining	AI	-34.98929195674482	-9.097465995302368	84631
6817aa436ef86f6d40b1be1a473e389d00657458	wireless machine-to-machine networks		1 School of Electronics and Applied Science, Aston University, Birmingham B4 7ET, UK 2 Simula Research Laboratory, Fornebu, 1325 Lysaker, Norway 3 Telecommunications Research Laboratory, Toshiba Research Europe Ltd., Bristol BS1 4ND, UK 4 Department of Engineering Science, National Cheng Kung University, Tainan City 70101, Taiwan 5 School of Electronic and Information Engineering, Beihang University, Beijing 100191, China	information engineering;machine to machine;simula	Jianhua He;Yan Zhang;Zhong Fan;Hsiao-Hwa Chen;Lin Bai	2012	IJDSN	10.1155/2012/535927	wireless ad hoc network;wi-fi;wireless wan;multi-user mimo;heterogeneous network;radio resource management;wireless network;ad hoc wireless distribution service;key distribution in wireless sensor networks;base transceiver station;municipal wireless network;wi-fi array;fixed wireless;wireless intrusion prevention system	ML	-45.10413734898303	-7.722342107505379	85289
7cd8a9c18a1d5a469154a465d9ce7d5d46543c26	bioscout: a life-science query monitoring system	distributed data;context multimedia objects;conditions;query language;scientific data;monitoring system;life sciences;access control	Scientific data are available through an increasing number of heterogeneous, independently evolving, sources. Although the sources themselves are independently evolving, the data stored in them are not. There exist inherent and intricate relationships between the distributed data-sets and scientists are routinely required to write distributed queries in this setting. Being non-experts in computer science, the scientists are faced with two major challenges: (i) How to express such distributed queries. This is a non-trivial task, even if we assume that scientists are familiar with query languages like SQL. Such queries can get arbitrarily complex as more sources are considered; (ii) How to efficiently evaluate such distributed queries. An efficient evaluation must account for batches of hundreds (or even thousands) of submitted queries and must optimize all of them as a whole.  In this demo, we focus on the biological domain for illustration purposes (our solutions are applicable to other scientific domains) and we present a system, called BioScout, that offers solutions in both of the above challenges. In more detail, we demonstrate the following functionality: (i) in BioScout, scientists draw their queries graphically, resulting in a query graph. The scientist is unaware of the query language used or of any optimization issues. Given the query graph, the system is able to generate, as a first step, an optimal query plan for the submitted query; (ii) BioScout uses four different strategies to combine the optimal query plans of individual queries to generate a global query plan for all the submitted queries. In the demo, we illustrate graphically how each of the four strategies works.	computer science;mathematical optimization;query language;query plan;sql	Anastasios Kementsietsidis;Frank Neven;Dieter Van de Craen	2008		10.1145/1353343.1353437	online aggregation;sargable;query optimization;query expansion;web query classification;boolean conjunctive query;computer science;query by example;access control;data mining;database;rdf query language;web search query;range query;world wide web;query language;data;spatial query	DB	-36.735033404335354	2.6916949842358706	85413
cbf2ba02adab7663a75f2a9c992cd4702d8548cc	an architecture and a metamodel for processing analytic and geographic multilevel queries	decision support;decision support tool;geographic information system;geographic information;software platform;application program interface;satisfiability;proof of concept;spatial decision support;metamodel;level of detail;gis;data aggregation;relational database management system;multilevel aggregation;spatial on line analytical processing;spatial analysis;architecture;analytic function	Analytic and geographic decision support queries are characterized by performing spatial analysis on data aggregated at different levels of detail and are typically large and complex to be written from scratch (manually) by a non-specialist user. Considering that decision support tools are in evidence and many databases (DB) have information with some geographic reference, nonspecialist users of these DB need analytic and spatial decision support tools to better analyze DB information. In this context, Spatial On-Line Analytical Processing (SOLAP) tools have received a lot of attention. Nevertheless, as there is no de jure standard language for OLAP yet, like standard SQL is to Relational Database Management Systems (RDBMS), these tools are dependent on specific OLAP languages, Application Programming Interface (API) and servers. In order to propose an alternative to this problem, this paper presents an Analytic and Geographic Information Service (AGIS). Our proposal is based on open and extensible standards (i.e. it is not based on an OLAP server) to offer a service that provides multilevel analytic functions that aim to enrich the set of functionalities of Geographic Information Systems (GIS). In order to do this, AGIS 1) abstracts the complexity of generating analytic and geographic decision support queries and 2) is independent of hardware and software platform. To satisfy these goals, a three-tiered architecture and a metamodel were defined and implemented. As a proof of concept, a study case to analyze the electrical energy situation in Brazil was implemented using our proposal.	application programming interface;decision support system;gnu debugger;geographic information system;metamodeling;online analytical processing;relational database management system;sql;server (computing);spatial analysis	Diego Martins Vieira Barros;Robson do Nascimento Fidalgo	2010		10.1007/978-3-642-12156-2_33	relational database management system;computer science;analytic function;theoretical computer science;architecture;operating system;level of detail;data mining;database;spatial analysis;geographic information system;proof of concept;computer security	DB	-35.91772209007712	-0.35902191548588036	85510
112a61c3b693dd819c8d14f1d40a7d3150beb386	enhanced user search activity by big data tools			big data	Nunzio Cassavia;Elio Masciari;Chiara Pulice;Domenico Saccà	2016			data mining;big data;information retrieval;computer science	ML	-39.178876003263476	-6.550709538444783	85658
7d373d170b089e38be5ba59ecf7ff3f0ddaea077	uncertain context data management in dynamic mobile environments	uncertainty handling;rule based models;mobile systems;context aware systems;big data management	Building systems that acquire, process and reason with context data is a major challenge. Model updates and modifications are required for the mobile context-aware systems. Additionally, the nature of the sensor-based systems implies that the data required for the reasoning is not always available nor it is certain. Finally, the amount of context data can be significant and can grow fast, constantly being processed and interpreted under soft real-time constraints. Such characteristics make it a case for a challenging big data application. In this paper we argue, that mobile context-aware systems require specific methods to process big data related to context, at the same time being able to handle uncertainty and dynamics of this data. We identify and define main requirements and challenges for developing such systems. Then we discuss how these challenges were effectively addressed in the KnowMe project. In our solution, the acquisition of context data is made with the use of the AWARE platform. We extended it with techniques that can minimise the power consumption as well as conserve storage on a mobile device. The data can then be used to build rule models that can express user preferences and habits. We handle the missing or ambiguous data with number of uncertainty management techniques. Reasoning with rule models is provided by a rule engine developed for mobile platforms. Finally, we demonstrate how our tools can be used to visualise the stored data and simulate the operation of the system in a testing environment.		Szymon Bobek;Grzegorz J. Nalepa	2017	Future Generation Comp. Syst.	10.1016/j.future.2016.06.007	real-time computing;computer science;artificial intelligence;operating system;data mining;database;distributed computing;computer security	HCI	-35.49125576596494	-1.0308930525406315	85748
3984c380b2e8a1ad2054e7ece6c8a9f830e61691	an expert system for tractor fault diagnosis based on ontology and web		This paper proposed an Expert System for Tractor Fault Diagnosis (ESTFD) based on ontology and web technologies. The ESTFD consists of several components such as diagnosis interface, OWL reasoner, explanation module, ontology base and database etc. The diagnosis interface was designed as web interface, which could support users to access the ESTFD by internet anytime and anywhere. A domain ontology, tractor fault diagnosis ontology, was constructed to build ontology base. The OWL API was called to manipulate the ontology base. The OWL reasoner, Pellet, was used to make logical reasoning and generate explanations for the process of logical reasoning. The ESTFD could provide tractor fault diagnosis service via internet to tractor maintenance personnel and drivers who located in the wide rural areas in China. Since the ESTFD has explanation module to explain how the diagnostic results was obtained, it also could be used as a training tool .	expert system	Chunyin Wu;Qing Ouyang;Shouhua Yu;Chengjian Deng;Xiaojuan Mao;Tiansheng Hong	2015		10.1007/978-981-10-0356-1_48	embedded system;engineering;data mining;database	AI	-33.95476694878925	-9.228435660146348	85834
4a14458f296b953002625a63de0896cb1eb8631e	amr-aware in situ indexing and scalable querying		Spring Simulation Multi-Conference 2016 April 3-6, Pasadena, CA, USA c ©2016 Society for Modeling & Simulation International (SCS) ABSTRACT Query-driven analytics on scientific datasets is one of fundamental approaches for scientific discoveries. Existing studies have explored query-driven analytics on uniform resolution meshes. However, querying on adaptive mesh refinement (AMR) data has not been explored yet. As many simulations have been lately transitioning to AMR, new methods for efficient query-driven analysis on AMR data are needed.	adaptive multi-rate audio codec;adaptive mesh refinement;intel edison;lexicographical order;overhead (computing);parallel computing;refinement (computing);scalability;simulation;supercomputer;video post-processing	Xiaocheng Zou;David A. Boyuka;Dhara Desai;Daniel F. Martin;Surendra Byna;Kesheng Wu	2016			supercomputer;computer science;data science;parallel;data mining;database	HPC	-42.648375716769536	-6.846022226796188	85849
542448269750d00ca39e7988ff90161cde4d5150	different solution strategies for solving epidemic model in imprecise environment		1Department of Mathematics, Indian Institute of Engineering Science and Technology, Shibpur, Howrah, West Bengal 711103, India 2Department of Mathematics, Netaji Subhash Engineering College, Techno City, Garia, Kolkata, West Bengal 700152, India 3Department of Mathematics, Midnapore College (Autonomous), Midnapore, West Midnapore, West Bengal 721101, India 4Laboratory of Computational Sciences and Mathematical Physics, Institute for Mathematical Research (INSPEM), Universiti Putra Malaysia (UPM), 43400 Serdang, Selangor, Malaysia 5Young Researchers and Elite Club, Mobarakeh Branch, Islamic Azad University, Mobarakeh, Iran		Animesh Mahata;Sankar Prasad Mondal;Ali Ahmadian;Fudziah Ismail;Shariful Alam;Soheil Salahshour	2018	Complexity	10.1155/2018/4902142	machine learning;fuzzy logic;differential inclusion;mathematical optimization;artificial intelligence;differential equation;epidemic model;mathematics	Theory	-44.91181141363063	-9.6897542921159	85917
4b7a6530552634ee2b980797416c607eda91e47e	a web application for extracting key domain information for scientific publications using ontology		We present demos of an ongoing project, domain informational vocabulary extraction (DIVE), which aims to enrich digital publications through entity and key informational words detection and by adding additional annotations. The system implements multiple strategies for biological entity detection, including using regular expression rules, ontologies, and a keyword dictionary. These extracted entities are then stored in a database and made accessible through an interactive web application for curation and evaluation by authors. Through the web interface, the user can make additional annotations and corrections to the current results. The updates can then be used to improve the entity detection in subsequent processed articles. Although the system is being developed in the context of annotating journal articles, it can also be beneficial to domain curators and researchers at large. Keywords—component; Information systems applications; Information integration; Ontology; Text Mining	database;dictionary;digital curation;entity;information systems;ontology (information science);regular expression;text mining;user interface;vocabulary;web application	Weijia Xu;Amit Gupta;Pankaj Jaiswal;Crispin Taylor;Patti Lockhart	2016			open biomedical ontologies;process ontology;web application;data mining;information retrieval;ontology;ontology (information science);bibliographic ontology;upper ontology;computer science	Web+IR	-40.71813601624741	3.4392105023188817	86012
09a5fbbc0a204cc8bdcf7f1ec22d52a7e683be67	semantic multimedia	multimedia content;semantic multimedia;semantic multimedia;sound modeling practice;semantic web querying;semantic web reasoning;high level multimedia analysis;multimedia resource;semantic web modeling;semantic web;semantic description	Multimedia constitutes an interesting field of application for Semantic Web and Semantic Web reasoning, as the access and management of multimedia content and context depends strongly on the semantic descriptions of both. At the same time, multimedia resources constitute complex objects, the descriptions of which are involved and require the foundation on sound modeling practice in order to represent findings of lowand high level multimedia analysis and to make them accessible via Semantic Web querying of resources. This tutorial aims to provide a red thread through these different issues and to give an outline of where Semantic Web modeling and reasoning needs to further contribute to the area of semantic multimedia for the fruitful interaction between these two fields of computer science. 1 Semantics for Multimedia Multimedia objects are ubiquitous, whether found via web search (e.g., Google or Yahoo! images), or via dedicated sites (e.g., Flickr or YouTube) or in the repositories of private users or commercial organizations (film archives, broadcasters, photo agencies, etc.). The media objects are produced and consumed by professionals and amateurs alike. Unlike textual assets, whose content can be searched for using text strings, media search is dependent on, (i), complex analysis processes, (ii), manual descriptions of multimedia resources, (iii), representation of these results and contributions in a widely understandable format for, (iv) later retrieval and/or querying by the consumer of this data. In the past, this process has not been supported by an interoperable and easily extensible machinery of processing tools, applications and data formats, but only by idiosyncratic combinations of system components into sealed off applications such that effective sharing of their semantic metadata remained impossible and the linkage to semantic data and ontologies found on the Semantic Web remained far off. 1 http://images.google.com/ 2 http://images.search.yahoo.com/ 3 http://www.flickr.com/ 4 http://www.youtube.com/ C. Baroglio et al. (Eds.): Reasoning Web 2008, LNCS 5224, pp. 125–170, 2008. c © Springer-Verlag Berlin Heidelberg 2008	archive;flickr;high-level programming language;interoperability;lecture notes in computer science;linkage (software);ontology (information science);semantic web;springer (tank);web modeling;web search engine	Steffen Staab;Ansgar Scherp;Richard Arndt;Raphaël Troncy;Marcin Grzegorzek;Carsten Saathoff;Simon Schenk;Lynda Hardman	2008		10.1007/978-3-540-85658-0_4	semantic computing;data web;semantic search;semantic grid;computer science;semantic web;social semantic web;semantic web stack;multimedia;semantic technology;world wide web;information retrieval;semantic analytics	Web+IR	-44.64728656373607	0.983927113465113	86025
00a6c45b090c759ed3ea6d958d64b6f0e2c48810	integration of multi-scale biosimulation models via light-weight semantics	biological process;fluid dynamics	Currently, biosimulation researchers use a variety of computational environments and languages to model biological processes. Ideally, researchers should be able to semiautomatically merge models to more effectively build larger, multi-scale models. However, current modeling methods do not capture the underlying semantics of these models sufficiently to support this type of model construction. In this paper, we both propose a general approach to solve this problem, and we provide a specific example that demonstrates the benefits of our methodology. In particular, we describe three biosimulation models: (1) a cardio-vascular fluid dynamics model, (2) a model of heart rate regulation via baroreceptor control, and (3) a sub-cellular-level model of the arteriolar smooth muscle. Within a light-weight ontological framework, we leverage reference ontologies to match concepts across models. The light-weight ontology then helps us combine our three models into a merged model that can answer questions beyond the scope of any single model.	biosimulation;hydrodynamics;large;merge sort;numerous;ontology (information science);pressoreceptors;programming languages;smooth muscle (tissue);upper ontology;benefit	John H. Gennari;Maxwell Lewis Neal;Brian E. Carlson;Daniel L. Cook	2008	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing		software;bioinformatics;merge (version control);biosimulation;ontology;ontology (information science);semantics;systems biology;computer science	SE	-40.62168601733379	1.3908387136311606	86073
9a06574bcc641392e7e25ea435eecdd09e58b968	spatial design of transmitter-receiver-constellations for pcl-systems using gis based visibility analysis		Passive Coherent Location (PCL) requires detailed knowledge about the geometric transceiver-receiver-constellations and the technical parameters for simulating the effective field of view. Geographic Information Systems (GIS) provide tools to fuse spatial information with technical metadata to evaluate individual geometric constellations and obtain optimal radar coverage. This paper presents a GIS-based model procedure for planning a passive radar system using base stations of the Global System for Mobile Communications (GSM) as illuminator. The implementation of GIS-technology allows analyzing spatial information about illuminated areas in combination with the underlying technical metadata.	coherent;geographic information system;illuminator (backlight);radar;simulation;transceiver;transmitter;visibility graph analysis	Rick Martin	2010			visibility;computer vision;transmitter;spatial design;computer science;artificial intelligence	Robotics	-37.65557528629308	-4.035495426312897	86176
6829ba65686250a2bb700c240814dd7d3eaa666c	instance-based ontological knowledge acquisition		The Linked Open Data (LOD) cloud contains tremendous amounts of interlinked instances, from where we can retrieve abundant knowledge. However, because of the heterogeneous and big ontologies, it is time consuming to learn all the ontologies manually and it is difficult to observe which properties are important for describing instances of a specific class. In order to construct an ontology that can help users easily access to various data sets, we propose a semi-automatic ontology integration framework that can reduce the heterogeneity of ontologies and retrieve frequently used core properties for each class. The framework consists of three main components: graph-based ontology integration, machine-learning-based ontology schema extraction, and an ontology merger. By analyzing the instances of the linked data sets, this framework acquires ontological knowledge and constructs a high-quality integrated ontology, which is easily understandable and effective in knowledge acquisition from various data sets using simple SPARQL queries.	information extraction;knowledge acquisition;linked data;machine learning;ontology (information science);ontology merging;sparql;semantic web;semiconductor industry;undefined behavior;web application	Lihua Zhao;Ryutaro Ichise	2013		10.1007/978-3-642-38288-8_11	knowledge acquisition;information retrieval;linked data;sparql;cloud computing;ontology (information science);ontology;semantic web;schema (psychology);computer science	AI	-39.06389000156141	3.3473525392681376	86255
7744d77987815c3823740d411627b92ca3c993db	conceptual structures: leveraging semantic technologies : 17th international conference on conceptual structures, iccs 2009, moscow, russia, july 26-31, 2009 : proceedings	algorithm analysis;discrete mathematics;semantic technologies;artificial intelligent;problem complexity;information system;formal language	Invited Papers.- The Maturing Semantic Web: Lessons in Web-Scale Knowledge Representation.- Concept Formation in Linguistic Ontologies.- Conceptual Graph Rules and Equivalent Rules: A Synthesis.- Two Paradigms Are Better Than One, and Multiple Paradigms Are Even Better.- Semantic Search - Using Graph-Structured Semantic Models for Supporting the Search Process.- Human Being and Mathematics Logical and Mathematical Thinking.- Accepted Papers.- Default Conceptual Graph Rules: Preliminary Results for an Agronomy Application.- Towards Extraction of Conceptual Structures from Electronic Health Records.- Algorithm Design Using Traversals of the Covering Relation.- Representing and Reasoning about Different Viewpoints: An Agronomy Application.- Access Policy Design Supported by FCA Methods.- Using VRML Technology for Visualization of Relations between the Main Concepts of an Educational Course.- Efficient Browsing and Update of Complex Data Based on the Decomposition of Contexts.- In Search of Semantic Compositionality in Vector Spaces.- Frequent Itemset Mining for Clustering Near Duplicate Web Documents.- System Consequence.- Fusion of Claude Bernardu0027s Experiments for Scientific Discovery Reasoning.- Distinguishing Answers in Conceptual Graph Knowledge Bases.- A Practical Exploration of Ontology Interoperability.- Relation Algebra Operations on Formal Contexts.- Conceptual Graphs and Datatypes.- Towards the Complexity of Recognizing Pseudo-intents.- Another Reason Why Conceptual Graphs Need Actors.- Relational Scaling in Relational Semantic Systems.		Sebastian Rudolph;Frithjof Dau;Sergei O. Kuznetsov	2009		10.1007/978-3-642-03079-6	semantic data model;natural language processing;semantic computing;computer science;theoretical computer science;machine learning;semantic compression	Robotics	-43.526116103226016	-5.434531199083865	86320
5167850cd9a0059119a0cc9994ac03193a60801a	doctor, my data! data archive support for large research projects			archive;research data archiving	Laurence Horton;Alexia Katsanidou	2013			data science;data mining;data archive;computer science	DB	-40.27505358026194	-5.7399688865627025	86390
192186cbec0110c714008b05b22b317ccae88cb2	sole: linking research papers with science objects	reproducible research;web interface;bibliography-like specification;traditional bibliography tool;science object;research paper;adequate representation;reference element;science object linking;associated science object	We introduce Science Object Linking and Embedding (SOLE), a tool for linking research papers with associated science objects, such as source codes, datasets, annotations, workflows, packages, and virtual machine images. The objective of SOLE is to reduce the cost to an author of linking research papers with such science objects for the purpose of reproducible research. To this end, SOLE allows an author to use simple tags to delimit a science object to be associated with a research paper. It creates an adequate representation of the science object and manages a bibliography-like specification of science objects. Authors and readers can reference elements of this bibliography and associate them with phrases in the text of the research paper through a Web interface, in a similar manner to a traditional bibliography tool.		Quan Pham;Tanu Malik;Ian T. Foster;Roberto Di Lauro;Raffaele Montella	2012		10.1007/978-3-642-34222-6_16	computer science;artificial intelligence;database;distributed computing;multimedia;programming language;world wide web;algorithm	Theory	-42.57005706095344	3.138275657156502	86423
e455a4724f27b0db6c3fc4bf034f6bc26f02b030	there is no ai without ia	computers;maintenance engineering;media;data analysis;big data;artificial intelligence;ontologies;organizations;context modeling;context	Artificial intelligence (AI) is increasingly hyped by everyone from well-funded startups to well-known software brands. Although it's receiving a lot of visibility, the fact that AI technologies all require some element of knowledge engineering, information architecture, and high-quality data sources is not well known. Here, the author describes the need for high-quality, structured data before AI technologies can be of use to organizations and their customers.	artificial intelligence;information architecture;knowledge engineering	Seth Earley	2016	IT Professional	10.1109/MITP.2016.43	applications of artificial intelligence;maintenance engineering;media;big data;progress in artificial intelligence;computer science;organization;ontology;artificial intelligence;data science;data mining;context model;data analysis	AI	-38.35094719535714	-5.723440919178667	86461
d3687d64afa4ec58648820c4fb392e5f9af750e1	data integration and visualization for knowledge mapping in strasbourg university		The work described in this paper is part of the IDEX (excellence initiative) project “Complex Identities” launched by Strasbourg University in 2015. The main goal is to map available knowledge in Strasbourg university in order to provide a comprehensive and structured view of its different components. Our approach consists, first, in building an ontology able to represent available knowledge in the university, making it understandable by users. Then, we are interested in visualizing the ontology to help users explore easily the represented knowledge.	apache axis;correctness (computer science);database;goto;human factors and ergonomics;intelligibility (philosophy);knowledge management;ontology (information science)	Amira Essaid;Quynh Nguyen Thi;Cecilia Zanni-Merk	2016		10.5220/0006069301630170	data science;data mining	Web+IR	-42.79227472936179	2.110802890815634	86478
f04e26c08c3928016cd6cc71d4afea7ec7ba26ce	the zoolib tuplebase: an open-source, scalable database architecture for learning sciences research	zoolib tuplebase;non-relational database architecture;broad spectrum;sciences research;scalable database architecture;fixed schema;interactive application;research data;alternative query mechanism	We introduce a free, open-source, non-relational database architecture that is robust, mature, scalable and extensible. The absence of a fixed schema is particularly important to research in the learning sciences where a broad spectrum of research data may need to be stored. We highlight an alternative query mechanism that is suited to highly interactive applications.	nosql;open-source software;relational database;scalability	Christopher Teplovs;Andrew Green;Marlene Scardamalia	2008			computer science;data mining;database;world wide web;database schema	ML	-34.21130167153218	2.037903766038641	86731
1b2493bba5b690fca185bbdd01236a66f710b82f	towards the development of a mathematician's assistant for the specification and implementation of parallel linear algebra software			linear algebra	T. J. G. Benson	1992				SE	-48.23747752574476	-4.488061892395263	86839
d32ce6e8e2f0f94a9d3ac226e0f7d489505add38	mining efficient training patterns of non-professional cyclists				Paolo Cintia;Luca Pappalardo;Dino Pedreschi	2014			business	ML	-39.48855088250169	-8.363075867076626	86914
2262f3af767e5763451d30ddb9f7a89fa4e39a38	applications of case-based reasoning in molecular biology	case base reasoning;molecular biology	reasoning paradigm that involves the storage and retrieval of past experiences to solve novel problems. It is an approach that is particularly relevant in scientific domains, where there is a wealth of data but often a lack of theories or general principles. This article describes several CBR systems that have been developed to carry out planning, analysis, and prediction in the domain of molecular biology.	case-based reasoning;experience;programming paradigm;theory	Igor Jurisica;Janice I. Glasgow	2004	AI Magazine		case-based reasoning;qualitative reasoning;computer science;bioinformatics;artificial intelligence;model-based reasoning;reasoning system	AI	-35.5995016816542	-8.603586211380359	87097
c934b410459ac2a83bdc63af23b2672ba318002e	eye: big data system supporting preventive and predictive maintenance of robotic production lines		This paper presents the EYE – data storage and analysis system. The EYE is a platform for gathering and processing data coming from production lines. It was developed on the basis of the Big Data technology, allowing not only to process the streaming data but also for performing the batch analyses. The results of data processing are presented in the form of reports and dashboards. The work contains a case study presenting an implementation of the system on a production line which is used for the production of telemetric devices.	big data;data system;robot	Jaroslaw Kurpanik;Joanna Henzel;Marek Sikora;Lukasz Wróbel;Marek Drewniak	2018		10.1007/978-3-319-99987-6_4	predictive maintenance;nosql;production line;database;dashboard (business);big data;streaming data;computer science;computer data storage;data processing	Robotics	-39.6286886225418	-5.227134137968583	87271
75b5b45e419f1b9aad4d51930494f6f482677efc	when will negotiation agents be able to represent us? the challenges and opportunities for autonomous negotiators		textabstractComputers that negotiate on our behalf hold greatrnpromise for the future and will even become indispensable in emergingrnapplication domains such as the smart grid and the Internet of Things.rnMuch research has thus been expended to create agents that are able tornnegotiate in an abundance of circumstances. However, up until now, trulyrnautonomous negotiators have rarely been deployed in real-worldrnapplications. This paper sizes up current negotiating agents andrnexplores a number of technological, societal and ethical challenges thatrnautonomous negotiation systems have brought about. The questions wernaddress are: in what sense are these systems autonomous, what has beenrnholding back their further proliferation, and is their spread somethingrnwe should encourage? We relate the automated negotiation research agendarnto dimensions of autonomy and distill three major themes that we believernwill propel autonomous negotiation forward: accurate representation,rnlong-term perspective, and user trust. We argue these orthogonalrnresearch directions need to be aligned and advanced in unison to sustainrntangible progress in the field.	application domain;autonomous car;autonomous robot;internet of things;propel;unison	Tim Baarslag;Michael Kaisers;Enrico Gerding;Catholijn M. Jonker;Jonathan Gratch	2017		10.24963/ijcai.2017/653	smart grid;management science;autonomy;computer science;knowledge management;internet of things;negotiation	AI	-36.65432664521141	-8.700235164074536	87337
c496eb50655c5a9f0475adfa38ec3db5de26d099	automatic display-camera extrinsic calibration method using an annular mirror		1079 Received September 16, 2014; revised July 8, 2015; accepted January 14, 2016. Communicated by Tyng-Luh Liu. * This work was supported by the National Natural Science Foundation of China under Grant No. 51375476. An earlier version of this paper was present at the International Conference on CYBER, 2015, Shenyang, China, IEEE. Automatic Display-Camera Extrinsic Calibration Method Using an Annular Mirror	hard disk drive;simulation;virtual reality headset	Sheng-Peng Fu;Ji-Bin Zhao;Renbo Xia;Weijun Liu	2016	J. Inf. Sci. Eng.		fast fourier transform;calibration;butterworth filter;quadrature mirror filter;computer science;computer vision;artificial intelligence	Robotics	-47.12782596531264	-9.461698340614277	87460
22d7f43e2552f316a65a17d9d034c4ff89277334	improving transport management with big data analytics		Transport corporations today are rich in data but poor in information. Lack of real business intelligence, may result in lower efficiency, decreased level of travel experience. This paper briefly outlines the course of developments referring to the transport management using big data analytics and their impact on cost effectiveness.	apache hadoop;big data;broadcast driver architecture;commodity computing;computer cluster;erp;enterprise resource planning;hard disk drive;in-memory database;iterative method;mapreduce;mathematical optimization;online transaction processing;query language;spark;sql;scalability;sensor;solid-state drive	Albert Nagy;József Tick	2016	2016 IEEE 14th International Symposium on Intelligent Systems and Informatics (SISY)	10.1109/SISY.2016.7601497	analytics;engineering;data science;operations management;data mining;business analytics;business intelligence	DB	-33.90449122039742	-0.6929967411306556	87485
562afde270ff4f3888d04a11d4b97a0f6099b865	information mining - reflections on recent advancements and the road ahead in data, text, and media mining	media mining;text mining;information mining;data mining;sentiment analysis;opinion and sentiment analysis	In this introduction, we briefly summarize the state of data and text mining today. Taking a very broad view, we use the term information mining to refer to the organization and analysis of structured or unstructured data that can be quantitative, textual, and/or pictorial in nature. The key question, in our view, is, ''How can we transform data (in the very broad sense of this term) into 'actionable knowledge', knowledge that we can use in pursuit of a specified objective(s).'' After detailing a set of key components of information mining, we introduce each of the papers in this volume and detail the focus of their contributions.		Ram D. Gopal;James R. Marsden;Jan Vanthienen	2011	Decision Support Systems	10.1016/j.dss.2011.01.008	concept mining;web mining;text mining;computer science;data science;information integration;data mining;knowledge extraction;world wide web;information retrieval;sentiment analysis	ML	-36.35557259586724	-6.969607500937397	87647
3ae7f527a6858a21072cb093d5632639cd327011	legal ontology for open government data mashups		An important pillar of Linked Open Government Data is to be able to mix datasets by using common ontologies in order to infer new knowledge. The open government datasets to be mashed-up by developers may be subject to distinct licenses, legal notices, terms of use, and applicable law and regulations from multiple jurisdictions. Within this complex ecosystem there is a need to create semi-automatic tools supported by an ontology to help technical reusers of Public Sector Information to utilize datasets according to their intended purpose and in compliance with the legal obligations that govern the rights to reuse the data. Unfortunately, some researchers may avoid considering all the legal frameworks that apply in the domain of Open Government Data and limit their investigation to only the area of licenses. To enable wider, compliant utilisation of mashed-up open data, we have analysed the European Union (EU) legal framework of reuse of Public Sector Information (PSI), the EU Database Directive and copyright framework and other legal sources (e.g., licenses, legal notices, terms of use) that can apply to open government Datasets. From this deep analysis we now model several major concepts in an Ontology of Open Government Data Licenses Framework for a Mash-up Model (OGDL4M). There have been earlier ontologies for creative commons or open licenses, but they did not anticipate the other legal constraints that arise from Open Government regulations. The OGDL4M ontology will be used for qualifying datasets in order to improve the accuracy of their legal annotation. The Ontology also aims to connect each applicable legal rule to official legal texts in order to direct legal experts and reusers to primary sources. This paper aims to present the modules of the OGDL4M ontology in depth and to describe some preliminary evaluation.	crowdsourcing;deontic logic;directive (programming);domain driven data mining;ecosystem;exception handling;human-readable medium;interpro;interaction;knowledge engineer;mechatronics;ontology (information science);open content;predicate (mathematical logic);primary source;sparql;semiconductor industry;terms of service;web ontology language	Martynas Mockus;Monica Palmirani	2017	2017 Conference for E-Democracy and Open Government (CeDEM)	10.1109/CeDEM.2017.25	open data;open government;public sector;ontology;political science;ontology (information science);database directive;knowledge management;european union;government	NLP	-42.03593189925117	4.00347104826614	87803
355e721d92fe1e64a08061c9c42508b7b272b7aa	newsnetexplorer: automatic construction and exploration of news information networks	network olap;information network construction	News data is one of the most abundant and familiar data sources. News data can be systematically utilized and ex- plored by database, data mining, NLP and information re- trieval researchers to demonstrate to the general public the power of advanced information technology. In our view, news data contains rich, inter-related and multi-typed data objects, forming one or a set of gigantic, interconnected, het- erogeneous information networks. Much knowledge can be derived and explored with such an information network if we systematically develop effective and scalable data-intensive information network analysis technologies. By further developing a set of information extraction, in- formation network construction, and information network mining methods, we extract types, topical hierarchies and other semantic structures from news data, construct a semi- structured news information network NewsNet. Further, we develop a set of news information network exploration and mining mechanisms that explore news in multi-dimensional space, which include (i) OLAP-based operations on the hierarchical dimensional and topical structures and rich-text, such as cell summary, single dimension analysis, and promo- tion analysis, (ii) a set of network-based operations, such as similarity search and ranking-based clustering, and (iii) a set of hybrid operations or network-OLAP operations, such as entity ranking at different granularity levels. These form the basis of our proposed NewsNetExplorer system. Although some of these functions have been studied in recent research, effective and scalable realization of such functions in large networks still poses multiple challenging research problems. Moreover, some functions are our on-going research tasks. By integrating these functions, NewsNetExplorer not only provides with us insightful recommendations in NewsNet exploration system but also helps us gain insight on how to perform effective information extraction, integration and mining in large unstructured datasets.	cluster analysis;data mining;data-intensive computing;database;information extraction;natural language processing;online analytical processing;scalability;similarity search	Fangbo Tao;George Brova;Jiawei Han;Heng Ji;Chi Wang;Brandon Norick;Ahmed El-Kishky;Jialu Liu;Xiang Ren;Yizhou Sun	2014		10.1145/2588555.2594537	computer science;data science;information integration;data mining;database;information retrieval	ML	-36.58432552191755	1.7304161695078555	88043
bbab127168615b65e27e87a92cdc5037ed3d01be	single-machine scheduling with past-sequence-dependent setup times and learning effects: a parametric analysis	learning effectiveness;single machine scheduling;setup times;aeronautical engineering;objective function;setup time;aerospace engineering formerly;scheduling;indexation;information processing;parametric analysis;learning effect	Single-machine scheduling with past-sequence-dependent setup times and learning effects: a parametric analysis V. Mani, Pei-Chann Chang* and Shih-Hsin Chen Department of Aerospace Engineering, Indian Institute of Science, Bangalore, India; Department of Information Management, Yuan Ze University, 135, Yuan-Dong Road, Tao-Yuan 320, Taiwan, R.O.C.; Department of Electronic Commerce Management, Nanhua University, Chiayi 62248, Taiwan, R.O.C.	e-commerce;entity–relationship model;information management;scheduling (computing);single-machine scheduling	V. Mani;Pei-Chann Chang;Shih-Hsin Chen	2011	Int. J. Systems Science	10.1080/00207721003718436	mathematical optimization;real-time computing;simulation;information processing;computer science;mathematics;learning effect;scheduling;parametric statistics	AI	-43.81556601247102	-8.933949566343996	88135
9dade2825c199ba522136898a70dbb6828ee5e10	transforming legal rules into online virtual world rules: a case study in the virtuallife platform	virtual worlds	The paper addresses the implementation of legal rules in online virtual world software. The development is performed within a peer-to-peer virtual world platform in the frame of the FP7 VirtualLife project. The goal of the project is to create a serious, secure and legally ruled collaboration environment. The novelty of the platform is an in-world legal framework, which is real world compliant. The approach “From rules in law to rules in artifact” is followed. The development accords with the conception “Code is law” advocated by Lawrence Lessig. The approach implies the transformation of legal rules (that are formulated in a natural language) into machine-readable format. Such a transformation can be viewed as a kind of translation. Automating the translation requires human expert abilities. This is needed in both the interpretation of legal rules and legal knowledge representation.	fitts's law;heuristic (computer science);human-readable medium;informatics;knowledge representation and reasoning;machine code;natural language;peer-to-peer;user (computing);virtual world	Vytautas Cyras	2009		10.1007/978-3-642-12630-7_34	simulation;human–computer interaction;engineering;multimedia	Web+IR	-39.545067446421214	3.55592651982276	88409
0989d67d51c363edc8a3cd7d86eb840c59c531b1	a geospatial approach to managing public housing on superlots	decision support;object oriented;public housing;industry foundation classes;building information model;data structure;spatial decision support system	This paper outlines how an object-oriented geospatial approach to Australian public housing management may help solve a core  problem currently faced by the NSW Department of Housing (DoH) - locating individual tenancies within large, unconsolidated  cadastral units, or Superlots. The concentration of high-rise housing built upon these lots has exposed the limitations of  the standard cadastral data structures and two-dimensional systems currently in use within the DoH. In this paper, we explore  the capacity for Building Information Models (BIMs) based on Industry Foundation Classes (IFC) to create semantically rich,  yet minimal, representations of individual buildings and tenancies located within the Superlot. This schema provides a methodology  to move beyond the ubiquitous land parcel with a visualisation system that spans from a broad, urban scale to that of an individual  building within a Superlot, and progressively into the individual tenancies. This forms the underlying structure for our Spatial  Decision Support System (SDSS) to perform operations ranging from simple queries to more advanced analysis and decision support  for Public Housing areas.  		Jack Barton;Jim Plume	2006		10.1007/978-3-540-36998-1_47	decision support system;systems engineering;knowledge management;data mining;business	HCI	-38.02862215081381	-3.0817039699414144	88460
ff780bfc233660a05d5120864f84678ae133bae5	a contribution to multimedia document modeling and querying	query language;document model	Metadata on multimedia documents may help to describe their content and make their processing easier, for example by identifying events in temporal media, as well as carrying descriptive information for the overall resource. Metadata is essentially static and may be associated with, or embedded in, the multimedia contents. The aim of this paper is to present a proposal for multimedia documents annotation, based on modeling and unifying features elicited from content and structure mining. Our approach relies on the availability of annotated metadata representing segment content and structure as well as segment transcripts. Temporal and spatial operators are also taken into account when annotating documents. Any feature is identified into a descriptor called “meta-document”. These meta-documents are the basis of querying by adapted query languages.	embedded system;query language;structure mining	Ikram Amous;Anis Jedidi;Florence Sèdes	2005	Multimedia Tools and Applications	10.1007/s11042-005-6542-7	computer science;database;world wide web;information retrieval;query language;metadata repository	DB	-39.57785663519174	2.2195931774101454	88536
1354968cc5feb364e0a85b502b74cdf213bd3e5f	mapping metadata for swhi: aligning schemas with library metadata for a historical ontology	semantic web technology;world wide web;structured data	What are the possibilities of Semantic Web technologies for organizations which traditionally have lots of structured data, such as metadata, available? A library is such a particular organization. We mapped a digital library’s descriptive (bibliographic) metadata for a large historical document collection encoded in MARC21 to a historical ontology using an out-of-the-box ontology, existing topic hierarchies on the World Wide Web and other resources. We also created and explored useful relations for such an ontology. We show that mapping the metadata to an ontology adds information and makes the existing information more easily accessible for users. The paper discusses various issues that arose during the mapping process. The result of mapping metadata to RDF/OWL is a populated ontology, ready to be deployed.	archive;digital library;historical document;librarian;library (computing);ontology (information science);out of the box (feature);population;resource description framework;sparql;semantic web;terminology extraction;vocabulary;web application;world wide web	Junte Zhang;Ismail Fahmi;Henk Ellermann;Gosse Bouma	2007		10.1007/978-3-540-77010-7_11	upper ontology;open biomedical ontologies;bibliographic ontology;ontology inference layer;synonym ring;semantic grid;data model;computer science;ontology;database;ontology-based data integration;metadata;world wide web;owl-s;meta data services;information retrieval;process ontology;data mapping;metadata repository;suggested upper merged ontology	Web+IR	-42.10072153229361	2.793391316124654	89036
9eb74e23fc803c0c93c85dbda9b26e268140747f	defining imprecise regions using the web	imprecise regions;great britain;literature review;geographic information retrieval;gir	"""An overview of PhD research into defining imprecise regions(e.g. """"The Midlands"""" of Great Britain) using unstructured data sources found on the web. A literature review is undertaken and future experiments are suggested. Two preliminary experiments, one into geo-tagging, the other into geographic coverage on the web, are described."""	experiment;geotagging;tag (metadata)	Robert Pasley	2008		10.1145/1458550.1458571	data mining;operations research	Web+IR	-41.03213908714374	-7.4580184523016975	89172
0a9d74960bf9accae961f12e7e179173c6f0c33a	the management of a large data base in iris	large data	IRIS (Illinois Resource Information System) is a retrieval and analysis system for land-use planners. The system stores the attributes of land parcels. Currently two major data bases are in use: a 78,000 parcel data base covering eight counties around Chicago and a 16,000 parcel data base covering six counties around Chicago.	base;information system;iris (eye);published database	Peter Alsberg	1975	Journal of chemical information and computer sciences	10.1021/ci60001a007	chemistry;computer science	DB	-38.679639362260026	-3.4242303221952177	89184
4e30ab6484c42bfe622b332f1b6b2ab03acc1bea	big data in hep: a comprehensive use case study	cern lhc coll;dark matter;automatic keywords;cms;physics of elementary particles and fields;programming	Experimental Particle Physics has been at the forefront of analyzing the world’s largest datasets for decades. The HEP community was the first to develop suitable software and computing tools for this task. In recent times, new toolkits and systems collectively called ”Big Data” technologies have emerged to support the analysis of Petabyte and Exabyte datasets in industry. While the principles of data analysis in HEP have not changed (filtering and transforming experiment-specific data formats), these new technologies use different approaches and promise a fresh look at analysis of very large datasets and could potentially reduce the timeto-physics with increased interactivity. In this talk, we present an active LHC Run 2 analysis, searching for dark matter with the CMS detector, as a testbed for ”Big Data” technologies. We directly compare the traditional NTuple-based analysis with an equivalent analysis using Apache Spark on the Hadoop ecosystem and beyond. In both cases, we start the analysis with the official experiment data formats and produce publication physics plots. We will discuss advantages and disadvantages of each approach and give an outlook on further studies needed.	apache hadoop;apache spark;big data;dark matter;ecosystem;exabyte;heterogeneous element processor;interactivity;large hadron collider;list of toolkits;microsoft forefront;microsoft outlook for mac;petabyte;root;testbed;usability	Oliver Gutsche;Matteo Cremonesi;Peter Elmer;Bo Jayatilaka;Jim Kowalkowski;Jim Pivarski;Saba Sehrish;Cristina Mantilla Surez;Alexey Svyatkovskiy;Nhan Tran	2016	CoRR	10.1088/1742-6596/898/7/072012	programming;computer science;data science;dark matter;data mining;world wide web	HPC	-36.148055752705154	-5.41887644019807	89211
c30d3d2e5d575b89f879a684cb09d16ec242bcfa	meta-information as a service: a big social data analysis framework			social data analysis	Kashif Ali;Margaret Hamilton;Charles Thevathayan;Xiuzhen Zhang	2018			sentiment analysis;knowledge management;social data analysis;data science;computer science	ML	-39.09311584752944	-7.003368224834184	89279
25f20599ce6e173e69939c1ebe412dfecacb84d3	design of a distributed planetary image data archive based on an atm network	search and retrieval;remote access;data management;atm networks;network connectivity;remote sensing data;system design;graphic user interface;relational database system;high speed;asynchronous transfer mode;data archive;internal standard	On-going and finished planetary and Earth satellit e missions have spawned large image data archives d stributed all over the world. The searching for specific data suffers from lack of using international query and catalogue standards, slow network connections, no graphical user interfa ce (GUI) and no platform independent client softwar e. The proposed GDSS (Graz Distributed Server System) project outli nes the system design, the setup of a high-speed AT M ( synchronous Transfer Mode) backbone and the implementation of a prototype. The GDSS prototype is using Java and JD BC (Java Data Base Connectivity) in order to access any ANSI SQL compatible relational database system with the same set of SQL statements. The client software provides the user w ith remote access for searching and retrieving imag e data. The 500 GByte NASA Magellan data set from planet Venus was used a a first test dataset. However, the system is not restricted to the given dataset. We expect data management concepts, networ k technology, Java client software and database con nectivity based on international standards illustrated within the GDSS project to also be applicable to Earth based remote sensing data.	atm turbo;archive;client (computing);excite;gigabyte;graphical user interface;internet backbone;jd - java decompiler;jdbc;microsoft sql server;naruto shippuden: clash of ninja revolution 3;planetary scanner;prototype;relational database management system;research data archiving;systems design	Herwig Rehatschek	1996		10.1007/3-540-63636-6_5	computer science;asynchronous transfer mode;data mining;database;world wide web	DB	-38.91679428940311	-1.160941702933545	89420
20879aa586aafd47674f060ca234e2fef9d9ebf4	on the discrete-time geox/g/1 queues under n-policy with single and multiple vacations		1 Department of Management Engineering, Sangmyung University, Cheonan 330-720, Republic of Korea 2Department of Industrial Engineering, Chonnam National University, Gwangju 500-757, Republic of Korea 3 Department of Business Administration, Pai Chai University, Daejeon 302-735, Republic of Korea 4Department of Industrial and Systems Engineering, KAIST, Daejon 305-701, Republic of Korea 5 Division of Business and Commerce, Baekseok University, Cheonan 330-704, Republic of Korea		Sung-Jin Kim;Nam K. Kim;Hyunmin Park;Kyung C. Chae;Dae-Eun Lim	2013	J. Applied Mathematics	10.1155/2013/587163	mathematics	DB	-44.54553474256192	-8.919065809420406	89614
fab7bbbac78876b129cfbad32b264bc7f6b9022b	federated scientific data repositories for the environment towards global scalable management of environmental information: how useful will they be? what is their potential impact? shall we save the environment?	distributed system;gestion informacion;medio ambiente;ecologia;almacenamiento informacion;digital library;information scientifique technique;scientific data;ecologie;ecology;information storage;information management;environment;world wide web;stockage information;environnement;scientific technical information;gestion information;polucion;bibliotheque numerique;informacion cientifica tecnica;geographic distribution;pollution	We are currently witnessing a proliferation of the Internet, the World Wide Web, and new distributed systems technologies. The idea of federated scientific data repositories, built from organizationally and geographically distributed units, is becoming an area of increasing interest to both scientists and authorities. The general public could benefit from them as well with appropriate access. Such a development would pave the way towards global sharing and combination of environmental information, thereby promoting co-operation among scientists and considerably supporting environmental information management and decision making. From a technical perspective, building federated scientific repositories involves confronting problems such as heterogeneity, distribution, integration, knowledge, authentication, appropriate interfaces and performance. From an organizational perspective it involves Inter-disciplinary co-operation, cross-organizational management, as well as important legal and economic issues. Within the computer science arena solutions exist to each of the technical problems, but their integration is still elusive. Notably, the organizational problems that have to be addressed at the management level may prove far more challenging. So, given that the ingredients exist, how are we to proceed? And will such an effort be worthwhile? In the end shall we save the environment?.		Catherine E. Houstis	1998		10.1007/3-540-49653-X_75	digital library;pollution;computer science;knowledge management;data mining;database;information management;natural environment;world wide web;computer security;data	HPC	-44.7498397233515	0.10805765071287257	89698
36567f3e3aa71f8477d9180ab281b20d0482cb85	an analytical framework to analyze dependencies among data quality dimensions	data quality	Dependencies among data quality dimensions, although they are not thoroughly and deeply analyzed in the research area of information quality, represent an issue of primary importance. The knowledge on dependencies can help the data quality analyst in finding errors, it can be useful to assess the quality level of a data set, which can improve knowledge on data quality dimensions. This paper proposes a data-driven approach for the analysis of dependencies among data quality dimensions. The analytical framework proposed here provides the main models of dependencies and analytic formulae, based on the entropy of Shannon. It characterizes each model, and proposes measures of correlation among the dimensions. Automatic identification of models of dependencies for a given data set is performed with the implemented version of the framework. Examples and case study are provided.	automatic identification and data capture;black–scholes model;data quality;information quality;shannon (unit);software quality analyst	Fabrizio De Amicis;Daniele Barone;Carlo Batini	2006			data mining;data quality;business	DB	-35.33656148957772	-7.0493424508916975	89867
0ac7e54964fffd70106dfdaccf6d08471c8c0b26	accessibility of multilingual terminological resources - current problems and prospects for the future		In this paper we analyse the various problems in making multilingual terminological resources available to users. Different levels of diversity and incongruence among such resources are discussed. Previous standardization efforts are reviewed. As a solution to the lack of co-ordination and compatibility among an increasing number of ‘standard’ interchange formats, a higher level of integration is proposed for the purpose of terminology-enabled knowledge sharing. The family of formats currently being developed in the SALT project is presented as a contribution to this solution.	accessibility	Gerhard Budin;Alan K. Melby	2000			artificial intelligence;natural language processing;computer science;knowledge sharing;standardization;knowledge management	AI	-44.66762000389643	4.021572124246091	89978
63e5f5ee2b8980666db77cc0da61e161b7e96ca1	harvard's perspective on the archive ingest and handling test			archive	Stephen Abrams;Stephen Chapman;Dale Flecker;Sue Kreigsman;Julian Marinus;Gary McGath;Robin Wendler	2005	D-Lib Magazine	10.1045/december2005-abrams	data science;operations research	Vision	-40.803473613889075	-5.85439510417911	90074
31edf6e8f88b02aa62c8421ea26d630b857ec760	controlling data warehouses with knowledge networks	knowledge network;data warehouse	DB-Prism is an integrated data warehouse system developed for distributed financial and management controlling (data collection, processing, and reporting) at Deutsche Bank. It combines finegranular availability of historical data with highactuality reporting and planning facilities. Major components of interest include an OLAP system in the Terabyte range, and a meta database responsible for the whole process from heterogeneous data selection to individual OLAP report positions and back via drill-through to the individual business activity. For these purposes, control workbenches have been developed both at the user –level and at the metadata level.	accessibility;client-side;database;online analytical processing;terabyte	Elvira Schaefer;Jan-Dirk Becker;Andreas Boehmer;Matthias Jarke	2000			computer science;data science;data warehouse;data mining;database	DB	-40.50929488412571	-4.108599714105471	90147
6fdb89abab6174afaf1cc8171ad44305440038b1	ontology visualization methods and tools: a survey of the state of the art				Marek Dudás;Steffen Lohmann;Vojtech Svátek;Dmitry Pavlov	2018	Knowledge Eng. Review	10.1017/S0269888918000073	knowledge management;ontology;visualization;computer science	HCI	-40.43935934671717	-7.990374322201881	90255
bad43b5d217e80fdb812bdd3d68e255d7c3f7936	sporadic and continuous clearing policies for a production/inventory system under an m/g demand process	production inventory system;sporadic and continuous clearing policies	To cite this article: David Perryhttp://stat.haifa.ac.il/~dperry, Wolfgang Stadjehttp://www.mathematik.uni-osnabrueck.de/staff/phpages/ stadjew.html, Shelemyahu Zackshttp://www.math.binghamton.edu/shelly, (2005) Sporadic and Continuous Clearing Policies for a Production/Inventory System Under an M/G Demand Process. Mathematics of Operations Research 30(2):354-368. http://dx.doi.org/10.1287/moor.1040.0123	mathematics of operations research	David Perry;Wolfgang Stadje;Shelemyahu Zacks	2005	Math. Oper. Res.	10.1287/moor.1040.0123	simulation;mathematics	Metrics	-43.990598261875995	-8.954174615256902	90437
7a2c00277ae06e25f0874f22ff22cd58489241a2	model-based time-distorted contexts for efficient temporal reasoning	time aware context modeling;temporal data;intelligent systems;reactive systems;knowledge representation	Intelligent systems continuously analyze their context to autonomously take actions. Building a proper knowledge representation of the context is key to take adequate actions. This requires context models, e.g. formalized as ontologies or meta-models. As these systems evolve in dynamic contexts, reasoning processes typically need to analyze and compare the current context with its history. A common approach consists in a temporal discretization, which regularly samples the context at specific timestamps (snapshots) to keep track of history. Fig. 1 shows a context sampled at three different timestamps. Reasoning processes would then need to mine a huge amount of data, extract a relevant view, and finally analyze it. This would require lot of computational power and be time-consuming, conflicting with the near real-time response time requirements of intelligent systems. To address these issues, we define time-distorted contexts as time-aware context models. Fig. 2 shows a context representation, where the context variables belong to different timestamps. Our approach considers temporal information as first-class property crosscutting any context element, and enables building timedistorted views of a context composed by elements from different times rather than a mere stack of snapshots. We claim that these time-distorted views can efficiently empower continuous reasoning processes and outperform traditional full sampling approaches by far.	aggregate data;distortion;graph (discrete mathematics);microsoft windows;traverse;trusted timestamping	Thomas Hartmann;François Fouquet;Grégory Nain;Brice Morin;Jacques Klein;Yves Le Traon	2014			natural language processing;knowledge representation and reasoning;intelligent decision support system;reactive system;computer science;knowledge management;artificial intelligence;machine learning;temporal database	SE	-38.415264778312554	0.3340576733785654	90531
0a7ddf40efe146474d5dd9ac51af7463705490e2	work always in progress: analysing maintenance practices in spatial crowd-sourced datasets	ucl;discovery;theses;conference proceedings;openstreetmap;crowd sourcing;digital web resources;ucl discovery;open access;ucl library;book chapters;open access repository;volunteered geographic information;collaborative practices;ucl research	Crowd-mapping is a form of collaborative work that empowers users to share geographic knowledge. Despite geographic information being intrinsically evolving, little research has so far gone into analysing maintenance practices in these domains. In this paper, we quantitatively capture maintenance dynamics in geographic crowd-sourced datasets, in terms of: the extent to which different maintenance actions are taking place, the type of spatial information that is being maintained, who engages in these practices and where. We apply this method to 117 countries in OpenStreetMap, one of the most successful examples of geographic crowd-sourced datasets. Furthermore, we explore what triggers maintenance, by means of an online survey to which 96 OpenStreetMap contributors took part. Our findings reveal that, although maintenance practices vary substantially from country to country in terms of how widespread they are, strong commonalities exist in terms of what metadata is being maintained, by whom, and what triggers them.	crowdsourcing;database trigger;openstreetmap;point of interest;self-organization	Giovanni Quattrone;Martin Dittus;Licia Capra	2017		10.1145/2998181.2998267	data science;data mining;volunteered geographic information;world wide web	Web+IR	-42.25891916388098	-4.647384676201322	90554
35054ec805d34f6d50b44a6388dd6cff0e3340ef	pds4: a model-driven planetary science data architecture for long-term preservation	object oriented modeling data models xml unified modeling language computer architecture communities standards;service oriented architecture aerospace computing big data data analysis;distributed service oriented architecture pds4 model driven planetary science data architecture approach long term preservation scientific data digital preservation big data distributed nodes data analysis data interpretation disciplined architectural approach scalable data system planetary data assets	The goal of the Planetary Data System (PDS) is the digital preservation of scientific data for long-term use by the scientific research community. After two decades of successful operation, the PDS found itself in a new era of big data, international cooperation, distributed nodes, and multiple ways of analysing and interpreting data. A project was formed to develop a disciplined architectural approach that would drive the design and implementation of a scalable data system that could evolve to meet the demands of this new era. PDS4, the next generation system, uses an explicit model-driven architectural approach coupled with modern information technologies and standards to meet these challenges in order to ensure the planetary data assets can be mined for scientific knowledge for years to come.	big data;data architect;data architecture;mined;model-driven architecture;model-driven integration;next-generation network;planetary data system;planetary scanner;scalability	John S. Hughes;Daniel J. Crichton;Sean Hardman;Emily Law;Ronald Joyner;Paul M. Ramirez	2014	2014 IEEE 30th International Conference on Data Engineering Workshops	10.1109/ICDEW.2014.6818317	data modeling;computer science;data mining;database;programming language;world wide web;data architecture	DB	-40.93108660595503	-1.0795226056494573	90909
14e47f3ba1890b4589794048594c200d82a74991	robust autonomous flight in constrained and visually degraded shipboard environments		Zheng Fang State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, Liaoning 110819, China Shichao Yang, Sezal Jain, and Geetesh Dubey Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania 15213 e-mail: shichaoy@andrew.cmu.edu, sezal@andrew.cmu.edu, gdubey@andrew.cmu.edu Stephan Roth Sensible Machines Inc., Pittsburgh, Pennsylvania 15212 e-mail: sroth@sensiblemachines.com Silvio Maeta and Stephen Nuske Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania 15213 e-mail: smaeta@andrew.cmu.edu, nuske@andrew.cmu.edu Yu Zhang Zhejiang University, Hangzhou, Zhejiang 310027, China e-mail: zhangyu80@zju.edu.cn Sebastian Scherer Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania 15213 e-mail: basti@cmu.edu	automation;degraded mode;email;realms of the haunting;robotics;weitao yang	Zheng Fang;Shichao Yang;Sezal Jain;Geetesh Dubey;Stephan Roth;Silvio Maeta;Stephen Nuske;Yu Zhang;Sebastian Scherer	2017	J. Field Robotics	10.1002/rob.21670	artificial intelligence;simulation;computer vision;odometry;field experiment;trajectory optimization;engineering;obstacle avoidance;lidar	Robotics	-45.580904425937746	-9.322374308638484	91238
2372fb528166c933ec39dc743a449ea387001a8c	smart home communication system		With﻿the﻿development﻿of﻿science﻿and﻿technology,﻿people’s﻿demands﻿for﻿life﻿are﻿getting﻿higher﻿and﻿ higher,﻿smart﻿home﻿has﻿become﻿the﻿new﻿theme﻿of﻿the﻿times.﻿Smart﻿home﻿more﻿and﻿more﻿people﻿to﻿ become﻿a﻿necessary﻿way﻿to﻿pursue﻿a﻿comfortable﻿life.﻿When﻿the﻿smart﻿home﻿gateway﻿to﻿the﻿family﻿ in﻿a﻿variety﻿of﻿home﻿appliances﻿through﻿the﻿home﻿bus﻿technology﻿together,﻿it﻿constitutes﻿a﻿powerful,﻿ highly﻿intelligent﻿modern﻿smart﻿home﻿system.﻿This﻿paper﻿mainly﻿introduces﻿the﻿wisdom﻿of﻿home,﻿ and﻿its﻿related﻿concepts﻿and﻿core﻿technology,﻿as﻿well﻿as﻿from﻿domestic﻿and﻿foreign﻿market﻿situation﻿ and﻿the﻿application﻿of﻿electronic﻿technology﻿in﻿intelligent﻿home. KeywoRdS Core Technology, Green Environmental Protection, Wireless Sensor Network 1. wIReLeSS SeNSoR NeTwoRK, wSN 1.1. what is a wireless Sensor Network? There﻿are﻿a﻿number﻿of﻿definitions﻿of﻿wireless﻿sensor﻿networks,﻿and﻿a﻿few﻿more﻿common﻿ones﻿are﻿listed:﻿ Wireless﻿sensor﻿network﻿is﻿deployed﻿in﻿the﻿monitoring﻿area﻿by﻿a﻿large﻿number﻿of﻿quality﻿and﻿cheap﻿ micro-sensor﻿nodes,﻿It﻿is﻿formed﻿by﻿wireless﻿communication﻿of﻿a﻿multi-hop﻿self-organized﻿dynamic﻿ network﻿system,﻿The﻿purpose﻿is﻿to﻿collaboratively﻿track,﻿perceive,﻿collect﻿and﻿process﻿information﻿ about﻿the﻿perceived﻿objects﻿in﻿the﻿network﻿coverage﻿area,﻿and﻿sent﻿to﻿the﻿data﻿center,﻿in﻿order﻿to﻿carry﻿ out﻿information﻿processing. Wireless﻿sensor﻿network﻿is﻿based﻿on﻿sensor﻿detectors,﻿wireless﻿communications﻿and﻿computer﻿ applications﻿and﻿other﻿science﻿and﻿technology﻿to﻿build﻿the﻿target﻿monitoring﻿system﻿(Han,﻿We,﻿Zhang,﻿ Zhang,2005). 1.2. wireless Sensor Classification Generally﻿used﻿classification: According﻿to﻿the﻿measured﻿parameters﻿classification:﻿temperature,﻿pressure,﻿displacement,﻿speed﻿ and﻿so﻿on. According﻿to﻿the﻿working﻿principle﻿classification:﻿strain﻿type,﻿capacitive,﻿piezoelectric,﻿magnetic﻿ and﻿so﻿on. Functional﻿materials﻿classified﻿as﻿sensitive﻿components:﻿semiconductor﻿sensors,﻿ceramic﻿sensors,﻿ fiber﻿optic﻿sensors,﻿polymer﻿membrane﻿sensors,﻿etc. Combined﻿with﻿ a﻿ technology﻿name:﻿ integrated﻿ sensors,﻿ smart﻿ sensors,﻿ robot﻿ sensors,﻿ bionic﻿ sensors.		Tianze Li	2017	IJAPUC	10.4018/IJAPUC.2017040101	internet of things	Mobile	-45.05803406401065	-7.508956121412246	91778
6a7ec762f3906b035b95d2b9e691f3fabaa3bbf2	extended abstract: formal design of cooperative multi-agent systems.				Rafael Rodrigues da Silva;Bo Wu;Jin Dai;Hai Lin	2016				AI	-45.72250273336792	-4.35207047319768	91946
8598e17b5228ca74c3a9af10fbde488647f4870a	towards a benchmark for lod-enhanced knowledge discovery from structured data		To leverage on KDD architectures designed for business databases, original unbounded RDF data has to be transformed. We report on a use case consisting in adaptation of linked data, around a nucleus of public procurement data, for a data mining challenge event. The generic problems addressed are: linked data sampling; (generalised) concise bounded description extraction; propositionalisation to CSV using SPARQL SELECT; and aggregation behaviour assigned by checking conformance to eligibility criteria formulated as SPARQL queries.	benchmark (computing);conformance testing;data mining;database;linked data;procurement;resource description framework;sparql;sampling (signal processing)	Jindrich Mynarz;Vojtech Svátek	2013			rdf;knowledge extraction;linked data;procurement;data mining;sparql;data model;sampling (statistics);bounded function;computer science	ML	-37.378894243898905	2.303270723143162	92024
695c9556858381feef257d92d3da797edeb6e39c	behavior rule based intrusion detection	information security;rule based;intrusion detection;internet;malware;botnet	Masayoshi Mizutani Keio University, Graduate School of Media and Governance 5322 Endo Fujisawa-shi Kanagawa, Japan mizutani@sfc.wide.ad.jp Keiji Takeda Keio University, Faculty of Environment and Information Studies 5322 Endo Fujisawa-shi Kanagawa, Japan keiji@sfc.keio.ac.jp Jun Murai Keio University, Faculty of Environment and Information Studies 5322 Endo Fujisawa-shi Kanagawa, Japan jun@wide.ad.jp	intrusion detection system	Masayoshi Mizutani;Keiji Takeda;Jun Murai	2009		10.1145/1658997.1659028	asprox botnet;anomaly-based intrusion detection system;intrusion detection system;host-based intrusion detection system;internet privacy;computer security;intrusion prevention system	Web+IR	-44.327850078181356	-7.954370600286004	92169
aa1186b8a816c42e3225d5759fb03cd93bd47d7e	methods and analyses for determining quality	software tool;data mining;general methods;data quality	"""In a possibly ideal world, records in a database would be complete and would contain fields having values that correspond to an underlying reality. An individuals name, address and date-of-birth would be present without typographical error. An income field might be a reasonably close approximation of a """"true income"""" and would not be missing. A list of customers would be complete, unduplicated and current.In this ideal world, a database could be used for several purposes and would be considered to have high quality. A set of databases might be linked using name, address, and other weakly identifying information.In this paper, we describe situations where properly chosen metrics may indicate that data quality is not sufficiently high for monitoring processes, for modeling, and for data mining.Some of the metrics are supplementary to those in the quality literature or have rarely been used. Additionally, we describe generalized methods and software tools that allow a skilled individual to perform massive clean-up of files in some situations.The clean-up, while possibly sub-optimal in recreating """"truth"""", can replace exceptionally large amounts of clerical review and allow many uses of the """"cleaned"""" files."""	approximation;data mining;data quality;database;display resolution	William E. Winkler	2005		10.1145/1077501.1077505	data quality;computer science;data science;data mining;database	DB	-35.04217308129247	-7.652905006726169	92182
0f6c0cae968d16271d390660b4348ec3d35c9466	service oriented computing for ambient intelligence to support management of transport infrastructures	geographic information;sensor network;web services;interoperability	Ensuring effective Ambient Intelligence based upon the wide range of static as well as real-time datasets required to manage a transport infrastructure such as a motorway, is a daunting task. This paper presents how this challenge has been faced through the development of a service-based networked infrastructure capable to make a wide range of Geographical Information (GI) available to the operators of a major Italian motorway. In doing so, the system has used open standards from ISO and Open Geospatial Consortium. Unlike current Spatial Data Infrastructures (SDIs), which address relatively static data in time slices, the work presented allows, similarly to traditional GIS, to manage true dynamic data available through geo-referenced sensors along the motorway. The main objective of the project described within this paper was to link existing systems and networks to deliver a comprehensive, coordinated and sustained intelligent infrastructure which can be used to improve management of the motorway as a system. This approach follows the vision of Ambient Spatial Intelligence, which explores how to better embed intelligent features to respond to spatio-temporal queries and to better monitor geographical events within built and natural environments. The combination of information available from different sources (GI, sensor networks, etc.) with information on traffic and vehicle situation is essential to provide context awareness to operators. The networked platform developed has extended the traditional concept of SDIs with an unprecedented volume of real-time geo-referenced sensor data, deployed along the motorway path, all characterized by high spatial and temporal resolution.	airborne ranger;ambient intelligence;consortium;context awareness;data model;dynamic data;freedom of information laws by country;geographic information system;interoperability;management information system;personalization;precondition;real-time clock;real-time locating system;sensor;server (computing);service-oriented architecture;systems architecture;virtual globe;web service;world wide web	Raffaele de Amicis;Giuseppe Conti;Stefano Piffer;Federico Prandi	2011	J. Ambient Intelligence and Humanized Computing	10.1007/s12652-011-0057-z	web service;embedded system;interoperability;simulation;wireless sensor network;telecommunications;computer science;data mining;computer security;computer network	DB	-38.83530810767023	0.003309738075237329	92198
14f39e37d4930f79f661b0852200890fc67b415a	a survey on distributed visualization techniques over clusters of personal computers	peer reviewed article;computer graphics;distributed computing;info eu repo semantics article;scientific visualization;visualizacao distribuida;computacao grafica;visualizacao cientifica;distributed visualization;info eu repo semantics publishedversion;computacao distribuida	In the last years, Distributed Visualization over Personal Computer (PC) clusters has become important for research and industrial communities. They have made large-scale visualizations practical and more accessible. In this work we survey Distributed Visualization techniques aiming at compiling last decade’s literature on the use of PC clusters as suitable alternatives to high-end workstations. We review the topic by defining basic concepts, enumerating system requirements and implementation challenges, and presenting up-to-date methodologies. Our work fulfills the needs of newcomers and seasoned professionals as an introductory compilation at the same time that it can help experienced personnel by organizing ideas.	compiler;computer cluster;gigapixel image;graphics;infinitereality;information source;organizing (structure);parallel rendering;personal computer;pixel;real-time transcription;requirement;scalability;spatial anti-aliasing;system requirements;visualization software;workstation	José Fernando Rodrigues;André G. R. Balan;Luciana A. M. Zaina;Agma J. M. Traina	2009	CoRR		scientific visualization;human–computer interaction;computer science;artificial intelligence;theoretical computer science;operating system;database;programming language;computer graphics;world wide web;algorithm;computer graphics (images)	Visualization	-47.89834852919532	-1.498687118139626	92300
2675fc8bf2f02f82578a5cf0c929a6c12cd0a36f	evolution of web services in eosdis — search and order metadata registry (echo)	carbon cycle;web services geoscience nasa information systems data systems usability costs earth observing system geology satellites;earth observing system clearinghouse;information systems;metadata;api;wsdl;technology utilization;echo system;data collection;data management;eosdis;web services application program interfaces geology geophysics computing meta data;soa;soap earth science echo eos eosdis web services metadata wsdl soa;application program interface;soap;web service;earth sciences;metadata registry;geoscience;earth science data;eos;geophysics computing;geology;api web service eosdis metadata registry earth observing system data and information system earth observing system clearinghouse echo system interoperability mechanism application program interface;ecosystems;eos data and information system;satellites;application program interfaces;web services;end to end data systems;error handling;earth science;echo;data systems;meta data;scientific communication;earth observation;interoperability;earth observing system;service oriented architecture;earth observing system data and information system;usability;nasa;climate variability;application programming interface;interoperability mechanism	During 2005 through 2008, NASA defined and implemented a major evolutionary change in the Earth Observing system Data and Information System (EOSDIS) to modernize its capabilities. This implementation was based on a vision for 2015 developed during 2005. The “EOSDIS 2015 Vision” emphasizes increased end-to-end data system efficiency and operability; increased data usability; improved support for end users; and decreased operations costs. One key feature of the Evolution plan was achieving higher operational maturity (ingest, reconciliation, search and order, performance, error handling) for the NASA's Earth Observing System Clearinghouse (ECHO). The ECHO system is an operational metadata registry through which the scientific community can easily discover and exchange NASA's Earth science data and services. ECHO contains metadata for 2, 726 data collections comprising over 87 million individual data granules and 34 million browse images, consisting of NASA's EOSDIS Data Centers' and the United States Geological Survey's Landsat Project holdings. ECHO stores metadata from a variety of science disciplines and domains, including Climate Variability and Change, Carbon Cycle and Ecosystems, Earth Surface and Interior, Atmospheric Composition, Weather, and Water and Energy Cycle. ECHO provides a platform for the publication, discovery, understanding and access to NASA's Earth Observation resources (data, service and clients). In their native state, these data, service and client resources are not necessarily targeted for use beyond their original mission. However, with the proper interoperability mechanisms, users of these resources can expand their value, by accessing, combining and applying them in unforeseen ways. ECHO provides access to its capabilities through a set of services. These ECHO Applications Program Interfaces (APIs) are based on industry standards for performing web-based computing, specifically web services profile.	application programming interface;browsing;capability maturity model;carbon cycle;data system;ecosystem;end-to-end encryption;exception handling;heart rate variability;information system;interoperability;operability;usability;web application;web service	Andrew E. Mitchell;Hampapuram K. Ramapriyan;Dawn Lowe	2009	2009 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2009.5417653	application programming interface;data management;computer science;data mining;database;world wide web;remote sensing	HPC	-39.5845067502704	-1.8971739940989714	92478
daed86460abdae5e2032ae0048636735b6d9dff3	spatio-temporal analysis of weibo check-in data based on spatial data warehouse		With the increasing development of the application of location services, massive check-in data is produced by social media applications on the mobile appliances, which includes characteristics of spatio-temporal information, user-emotion information, and etc. Traditional analysis techniques cannot handle check-in data well because of the complexity of spatio-temporal information. Spatial data warehouse provided a good architecture for spatial data’s storage and analysis. In this research, we designed a spatial data warehouse to store and manage the check-in data, used OLAP analysis technology to analyze it, and found many interesting results. It showed spatial data warehouse and OLAP provided a good frame to analyze check-in data.		Liang Zhou;Mingye Bao;Nanhai Yang;Yizhen Lao;Yun Zhang;Yangge Tian	2013		10.1007/978-3-642-41908-9_48	data mining;online analytical processing;check-in;social media;spatial analysis;architecture;spatio-temporal analysis;location-based service;warehouse;computer science	DB	-36.83746526865265	-4.639329943755417	92479
d27dfe6a81206cd980e6a841fd7e9909b49d1d74	facilitating reproducible research by investigating computational metadata	metadata;computational modeling;big data;data visualization;cloud computing;hardware	Computational workflows consist of a series of steps in which data is generated, manipulated, analysed and transformed. Researchers use tools and techniques to capture the provenance associated with the data to aid reproducibility. The metadata collected not only helps in reproducing the computation but also aids in comparing the original and reproduced computations. In this paper, we present an approach, “Why-Diff”, to analyse the difference between two related computations by changing the artifacts and how the existing tools “YesWorkflow” and “NoWorkflow” record the changed artifacts.	computation;high- and low-level;matlab;management system;python;r language;vii	Priyaa Thavasimani;Paolo Missier	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840958	computer science;data science;data mining;world wide web;data element	Visualization	-40.66814376814177	-0.13033435080159259	92495
557f6b7b057786b78b5e101cb1321a77d5887bbe	a look at challenges and opportunities of big data analytics in healthcare	healthcare;medical computing;data analysis;big data;diseases;health care	Big Data analytics can revolutionize the healthcare industry. It can improve operational efficiencies, help predict and plan responses to disease epidemics, improve the quality of monitoring of clinical trials, and optimize healthcare spending at all levels from patients to hospital systems to governments. This paper provides an overview of Big Data, applicability of it in healthcare, some of the work in progress and a future outlook on how Big Data analytics can improve overall quality in healthcare systems.	big data;microsoft outlook for mac	Raghunath Othayoth Nambiar;Ruchie Bhardwaj;Adhiraaj Sethi;Rajesh Vargheese	2013	2013 IEEE International Conference on Big Data	10.1109/BigData.2013.6691753	medicine;data science;data mining;management science	Robotics	-37.97689111697569	-6.731839726562359	92536
075018caec49d734feb4b0226fa40f0808500128	seismological data warehousing and mining: a survey	seismic data management;data mining;data warehousing;survey	Earthquake data composes an ever increasing collection of earth science information for postprocessing analysis. Earth scientists, local or national administration officers and so forth, are working with these data collections for scientific or planning purposes. In this article, we discuss the architecture of a so-called seismic data management and mining system (SDMMS) for quick and easy data collection, processing, and visualization. The SDMMS architecture includes, among others, a seismological database for efficient and effective querying and a seismological data warehouse for OLAP analysis and data mining. We provide template schemes for these two components as well as examples of their functionality towards the support of decision making. We also provide a comparative survey of existing operational or prototype SDMMS.	data mining;online analytical processing;prototype	Gerasimos Marketos;Yannis Theodoridis;Ioannis S. Kalogeras	2008	IJDWM	10.4018/jdwm.2008010101	computer science;data science;data warehouse;data mining;database	DB	-38.870964620005374	-2.910490328444591	93046
917df491138c919616cd82f051511f8a9c926add	embedded sensor networks	selected works;bepress	Embedded sensor networks are distributed systems for sensing and in situ processing of spatially and temporally dense data from resource-limited and harsh environments such as seismic zones, ecological contamination sites are battle fields. From an application point of view, many interesting questions arise from sensor network technology that go far beyond the networking/computing aspects of the embedded system. This talk presents an overview of various open problems that are both of mathematical and engineering interests. These problems include sensor-centric quality of routing/energy optimization among other graph theoretic problems. Bio: Dr. S. S. Iyengar is the Chairman and Roy Paul Daniels Distinguished Professor of Computer Science at Louisiana State University and is also Satish Dhawan Chaired Professor at Indian Institute of Science. He has been involved with research in high-performance algorithms, data structures, sensor fusion, data mining, and intelligent systems since receiving his Ph.D. degree in 1974. He has directed over 40 Ph.D. students, many of whom are faculty at major universities worldwide or scientists or engineers at national labs/industry around the world. His publications include 15 books (authored or coauthored, edited; Prentice-Hall, CRC Press, IEEE Computer Society Press, John Wiley & Sons, etc.) and over 350 research papers in refereed journals and conferences. He has won many best paper awards from various conferences. His research has been funded by NSF, DARPA, DOE-ORNL, ONR among others. He is a Fellow of ACM, Fellow of the IEEE, Fellow of AAAS. Dr. Iyengar is the winner of the many IEEE Computer Society Awards. Dr. Iyengar was awarded the LSU Distinguished Faculty Award for Excellence in Research, the Hub Cotton Award for Faculty Excellence, and many other awards at LSU. He received the Prestigious Distinguished Alumnus Award from Indian Institute of Science, Bangalore in 2003. Also, Elected Member of European Academy of Sciences (2002). He is a member of the New York Academy of Sciences. He has been the Program Chairman for many national/international conferences. He has given over 100 plenary talks and keynote lectures at numerous national and international conferences. S.K. Prasad et al. (Eds.): ICISTM 2009, CCIS 31, p. 1, 2009. c © Springer-Verlag Berlin Heidelberg 2009 Future of Software Engineering	academy;algorithm;artificial intelligence;book;computer science;cyclic redundancy check;data mining;data structure;distributed computing;embedded system;ibm notes;john d. wiley;mathematical optimization;routing;sensor web;software engineering;springer (tank);theory	S. Sitharama Iyengar	2009		10.1007/978-3-642-00405-6_1	sensor web;embedded system;wireless sensor network;sensor node;distributed computing;key distribution in wireless sensor networks;mobile wireless sensor network;electro-optical sensor;computer network;intelligent sensor;visual sensor network	Theory	-47.912295874585766	-8.30839476388189	93163
847301c7700b682c638bbcd8fc91ea02e4375f51	distributed fault estimation with randomly occurring uncertainties over sensor networks	randomly occurring;randomly occurring uncertainties;uncertainties;sensor networks;distributed fault estimation;article	Hongli Donga∗ Zidong Wangb,c Xianye Bua Fuad E. Alsaadic Yuxuan Shena aCollege of Electrical and Information Engineering, Northeast Petroleum University, Daqing 163318, China bDepartment of Computer Science, Brunel University, Uxbridge, Middlesex, UB8 3PH, U.K. cCommunication Systems and Networks (CSN) Research Group, Faculty of Engineering, King Abdulaziz University, Jeddah 21589, Saudi Arabia. (Received 00 Month 201X; final version received 00 Month 201X)	bernoulli polynomials;cell signaling;computer science;information engineering;randomness;simulation;stochastic process	Hongli Dong;Zidong Wang;Xianye Bu;Fuad E. Alsaadi	2016	Int. J. General Systems	10.1080/03081079.2015.1106733	wireless sensor network;computer science;control theory;distributed computing	DB	-45.44199045278975	-8.204668232553304	93197
abf6b8608ac0bd247484ee0e55def81b9f3e88b1	experimenter's portal: the collection, management and analysis of scientific data from remote sites	e science;web based services;remote instrument control;service oriented architecture;cloud computing	This paper describes an e-Science initiative to enable teams of scientists to run experiments with secure links at one or more advanced research facilities. The software provides a widely distributed team with a set of controls and screens via common browsers to operate, observe and record essential parts of an experiment and to access remote cloud-based analysis software to process the large data sets that are often involved in complex experiments. This paper describes the architecture of the software, the underlying web services used for remote access to research facilities and describes the cloud-based approach for data analysis. The core services are general and can be used as the basis for access to a variety of systems, though specific screen interfaces and analysis software must be tailored to a facility. For illustrative purposes, we focus on use of the system to access a single site - a synchrotron beamline at the Canadian Light Source. We conclude with a discussion of the generality and extensibility of the software and services.	cloud computing;e-science;experiment;extensibility;web service	Michael Anthony Bauer;Stewart McIntyre;Nathaniel Sherry;Jinhui Qin;Marina Suominen-Fuller;Y. Xie;Omid Mola;Dylan Maxwell;Dongqing Liu;E. Matias	2012		10.1145/2405136.2405143	simulation;human–computer interaction;computer science;world wide web	SE	-41.42656788406841	-0.9133676863567205	93690
65f8ad45324a7fdd63fd1458c3e3ca015ed493da	phase noise in coherent optical communications	electrical engineering and computer science;thesis	Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1991.	coherence (physics);phase noise	Murat Azizoálu	1991			engineering;electrical engineering;computer engineering;mechanical engineering	Vision	-46.92441812878678	-6.882646397509801	93887
55719a9867d9d1b09b7de05607b118a8e3956748	introducing intelligence in electronic healthcare systems: state of the art and future trends	data interpretation;electronic health record;data mining;healthcare system;context aware systems;data classification;data acquisition	This chapter introduces intelligent technologies applied in electronic healthcare systems and services. It presents an overview of healthcare technologies that enable the advanced patient data acquisition and management of medical information in electronic health records. The chapter presents the most important patient data classification methods, while special focus is placed on new concepts in intelligent healthcare platforms (i.e., advanced data mining, agents and context-aware systems) that provide enhanced means of medical data interpretation and manipulation. The chapter is concluded with the areas in which intelligent electronic healthcare systems are anticipated to make a difference in the near future.	context-aware pervasive systems;data acquisition;data mining	Ilias Maglogiannis	2009		10.1007/978-3-642-03226-4_5	computer science;knowledge management;data science;data mining;data acquisition;data analysis	ML	-35.37849662563153	-9.364440893744447	94010
233340157809f6bb78dd014bd045863a50695b50	link++: a flexible and customizable tool for connecting rdf data sources		Existing interlinking tools focus on finding similarity relationships between entities of distinct RDF datasets by generating owl:sameAs links. These approaches address the detection of equivalence relations between entities. However, in some contexts, more complex relations are required, and the links to be defined follow more sophisticated patterns. This paper introduces Link++, an approach that enables the discovery of complex links in a flexible manner. Link++ enables the users to generate rich links by specifying a link pattern as well as rules and functions to discover them. When visiting the demo, attendees will be introduced to all the aspects of the system explaining the required steps to define custom functions, connection patterns and linking rules until finally obtaining custom connections.	entity;turing completeness	Ali Masri;Karine Zeitouni;Zoubida Kedad;Gabriel Képéklian	2016		10.1007/978-3-319-47602-5_22	data mining;database;world wide web	ML	-37.335221324497304	3.9259817245977136	94046
033ce53703fddbc316ff86400716e8e736453929	interactive query and search in semistructured databases	iterative refinement;semistructured data;keyword search;world wide web	Semistructured graph-based databases have been proposed as well-suited stores for World-Wide Web data. Yet so far, languages for querying such data are too complex for casual Web users. Further, proposed query approaches do not take advantage of the interactivity of typical Web sessions—users are proficient at iteratively refining their Web explorations. In this paper we propose a new model for interactively querying and searching semistructured databases. Users can begin with a simple keyword search, dynamically browse the structure of the result, and then submit further refining queries. Enabling this model exposes new requirements of a semistructured database that are not apparent under traditional database uses. We demonstrate the importance of efficient keyword search, structural summaries of query results, and support for inverse pointers. We also describe some preliminary solutions to these technical issues.	browsing;database;interactivity;requirement;search algorithm;world wide web	Roy Goldman;Jennifer Widom	1998		10.1007/10704656_4	web query classification;computer science;data mining;database;web search query;world wide web;information retrieval	DB	-33.85562311738026	3.9251040424700236	94176
46895da1f21a5828253975b5d6ec14adf68363d9	requirements for a standardized flow storage solution	data sharing;protocols;ip flow information export protocol flow storage solution ip flow data data sharing;xml;relational databases;traffic engineered;xml protocols relational databases security of data;xml protocols relational databases data security information analysis data models secure storage europe data engineering measurement standards;security of data	IP flow data provides input for accounting, traffic engineering and security applications. The storage of flow data is required to allow off-line processing, archiving, and sharing of traffic traces. Currently this is achieved with a variety of proprietary solutions, which results in a variety of formats as input for flow processing tools and hinders data sharing and tool reusability. In this paper we analyze the requirements for a standard file format for storing flow data, and propose using the upcoming standard IP flow information export protocol (IPFIX) as a basis for its design	archive;framing (world wide web);nato architecture framework;online and offline;relevance;requirement;requirements elicitation;self-documenting code;toolchain;tracing (software);xml	Brian Trammell;Elisa Boschi;Lutz Mark;Tanja Zseby	2007	2007 International Symposium on Applications and the Internet Workshops	10.1109/SAINT-W.2007.91	communications protocol;semi-structured data;xml;relational database;computer science;data mining;database;world wide web;computer network	Arch	-37.45490836336706	0.5886739714328344	94567
d56dc74f78fc2ba1e1db42d87cba89cc79abd7f8	new measurement data sources for mvs x/a and its subsystems		New Measurement Data Sources For MVS X/A and Its Subsystems H. W. Barry Merrill, PhD President-Programmer Merrill Consultants Dallas, Texas 75229 Since MVS/XA 2.1.0 became available in 1984, IBM has made significant changes in MVS/XA software and hardware architecture. New data exist to measure and manage these enhancements. SMF/RMF measurement of the vector processor, extended storage and the 3090 I/O processing enhancements will be discussed Additional enhancements and the incompatible changes that have been announced in MVS/XA 2.2.1 will be described. This paper is extracted from Merrill's Expanded Guide Supplement, published by SAS Institute in 1987, and examples use the names of SAS® System variables and data sets as built by the MXG® Software.	eclipse requirements modeling framework;input/output;programmer;sas;vector processor;x/open xa	H. W. Barry Merrill	1987			computer science	Embedded	-43.18478041932678	-3.7621536189332936	94632
bf173ae404772f0f1d8905d6d0614de15eb36a49	facilitating the production of iso-compliant metadata of geospatial datasets	serveur institutionnel;metadata production;archive institutionnelle;data discovery;spatial data infrastructure;open access;harvesting;archive ouverte unige;cybertheses;csw;institutional repository;iso19115	Metadata are recognized as an essential element to enable efficient and effective discovery of geospatial data published in spatial data infrastructures (SDI). However, metadata production is still perceived as a complex, tedious and time-consuming task. This typically results in little metadata production and can seriously hinder the objective of facilitating data discovery. In response to this issue, this paper presents a proof of concept based on an interoperable workflow between a data publication server and a metadata catalog to automatically generate ISO-compliant metadata. The proposed approach facilitates metadata creation by embedding this task in daily data management workflows; ensures that data and metadata are permanently up-to-date; significantly reduces the obstacles of metadata production; and potentially facilitates contributions to initiatives like the Global Earth Observation System of Systems (GEOSS) by making geospatial resources discoverable.		Gregory Giuliani;Yaniss Guigoz;Pierre Lacroix;Nicolas Ray;Anthony Lehmann	2016	Int. J. Applied Earth Observation and Geoinformation	10.1016/j.jag.2015.08.010	spatial data infrastructure;geospatial metadata;geology;computer science;database;metadata;world wide web;data element;meta data services;information retrieval;remote sensing;data mapping;metadata repository;data dictionary	HPC	-40.7355091338212	-0.6895244187509908	94646
8fa0f6ec7738aa9968f48fef03787ed8ceca791c	an architecture for cost optimization in the processing of big geospatial data in public cloud providers		Cloud computing is a suitable platform for running applications to process big data. Currently, with the increase in the volume of geographic and spatial data volume, conceptualized as Big Geospatial Data, a variety of tools have been developed to efficiently process this data. The index applied to the dataset is an important aspect. This paper presents an architecture, supported by a Knownlegde Base and an Inference Engine, to process big geospatial data in public cloud providers with the ultimate goal of optimizing costs. The tests executed demonstrated that the rules created are capable of optimizing the total costs for processing large geospatial data up to 71%.	big data;cloud computing;geographic information system;inference engine	João Bachiega;Marco Antonio Sousa Reis;Maristela Holanda;Aletéia Patrícia Favacho de Araújo	2018	2018 IEEE International Congress on Big Data (BigData Congress)	10.1109/BigDataCongress.2018.00032	data mining;database;geospatial analysis;big data;architecture;computer science;total cost;spatial analysis;cloud computing;search engine indexing;inference engine	DB	-34.984103348174756	-0.8200477730394778	94681
42492ce2d576ea5b73b9b9a3cfdeeb9475830353	resource-constrained reasoning using a reasoner composition approach		The marriage between semantic web and sensor-rich systems can semantically link the data related to the physical world with the existent machined-readable domain knowledge encoded on the web. This has allowed a better understanding of the inherently heterogeneous sensor data by allowing data access and processing based on semantics. Research in different domains has been started seeking a better utilization of data in this manner. Such research often assumes that the semantic data is processed in a centralized server and that reliable network connections are always available, which are not realistic in some critical situations. For such critical situations, a more robust semantic system needs to have some localautonomy and hence on-device semantic data processing is required. As a key enabling part for this strategy, semantic reasoning needs to be developed for resource-constrained environments. This paper shows how reasoner composition (i.e. to automatically adjust a reasoning approach to preserve only the amount of reasoning needed for the ontology to be reasoned over) can achieve resource-efficient semantic reasoning. Two novel reasoner composition algorithms have been designed and implemented. Evaluation indicates that the reasoner composition algorithms greatly reduce the resources required for OWL reasoning, facilitating greater semantic data processing on sensor devices.	algorithm;centralized computing;data access;human-readable medium;semantic web;semantic reasoner;sensor;server (computing);web ontology language	Wei Tai;John Keeney;Declan O'Sullivan	2015	Semantic Web	10.3233/SW-140142	semantic reasoner	AI	-38.619273022499236	1.1410920676165288	94883
a3593c50d3abfcf22c9119622a30935227347a14	a visualization-oriented 3d method for efficient computation of urban solar radiation based on 3d–2d surface mapping	2d raster;surface mapping;solar radiation model;3d triangular mesh	A visualization-oriented 3D method for efficient computation of urban solar radiation based on 3D–2D surface mapping Jianming Liang, Jianhua Gong, Wenhang Li & Abdoul Nasser Ibrahim a State Key Laboratory of Remote Sensing Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing 100101, China b University of Chinese Academy of Sciences, Beijing 100049, China c Zhejiang-CAS Application Center for Geoinformatics, Zhejiang 314100, China Published online: 31 Jan 2014.	academy;computation;geoinformatics	Jianming Liang;Jianhua Gong;Wenhang Li;Ibrahim Abdoul Nasser	2014	International Journal of Geographical Information Science	10.1080/13658816.2014.880168	computer vision;geography;cartography;computer graphics (images)	Robotics	-44.62522471116784	-9.620171298988195	95035
bc7f15a0429835e189308492f082676b8405ece4	review of predictive analytics vendors for transport management systems		Most of transport corporations today already use some business intelligence solutions. However, using of advanced data mining methods may result in higher efficiency, increased level of travel experience. This paper briefly review the potential vendors and technologies to probably select the best possible predictive analytics method for transport management purposes.	data mining;google analytics	Albert Nagy;József Tick	2017	2017 IEEE 15th International Symposium on Intelligent Systems and Informatics (SISY)	10.1109/SISY.2017.8080557	predictive analytics;data science;data visualization;visualization;computer science;data mining;management system;business intelligence;informatics;synthetic aperture sonar;analytics	DB	-38.01517174778512	-6.473656383548134	95105
b24689b7b27360a95de50f16da1a7997ce52cbbb	land cover and land use classification with twopac: towards automated processing for pixel- and object-based image classification	land cover and land use;landoberflache;twopac;decision tree;web processing service;classification;validation;supervised;automated processing	We present a novel and innovative automated processing environment for the derivation of land cover (LC) and land use (LU) information. This processing framework named TWOPAC (TWinned Object and Pixel based Automated classification Chain) enables the standardized, independent, user-friendly, and comparable derivation of LC and LU information, with minimized manual classification labor. TWOPAC allows classification of multi-spectral and multi-temporal remote sensing imagery from different sensor types. TWOPAC enables not only pixel-based classification, but also allows classification based on object-based characteristics. Classification is based on a Decision Tree approach (DT) for which the well-known C5.0 code has been implemented, which builds decision trees based on the concept of information entropy. TWOPAC enables automatic generation of the decision tree classifier based on a C5.0-retrieved ascii-file, as well as fully automatic validation of the classification output via sample based accuracy assessment. Envisaging the automated generation of standardized land cover products, as well as area-wide classification of large amounts of data in preferably a short processing time, standardized interfaces for process control, Web Processing Services (WPS), as introduced by the Open Geospatial Consortium (OGC), are utilized. TWOPAC’s functionality to process geospatial raster or vector data via web resources (server, network) enables OPEN ACCESS Remote Sens. 2012, 4 2531 TWOPAC’s usability independent of any commercial client or desktop software and allows for large scale data processing on servers. Furthermore, the components of TWOPAC were built-up using open source code components and are implemented as a plug-in for Quantum GIS software for easy handling of the classification process from the user’s perspective.	c4.5 algorithm;consortium;decision tree;desktop computer;entropy (information theory);geographic information system;object-based language;open-source software;pixel;plug-in (computing);qgis;quantum;server (computing);usability;web resource;whole earth 'lectronic link	Juliane Huth;Claudia Kuenzer;Thilo Wehrmann;Steffen Gebhardt;Vo Quoc Tuan;Stefan W. Dech	2012	Remote Sensing	10.3390/rs4092530	web processing service;biological classification;computer science;data science;decision tree;data mining;database;remote sensing	ML	-38.82508032174934	-2.23426928989799	95441
a0e7ffd976c10cc5b50237b34fac1fccc0dbeae7	galeon: standards-based web services for interoperability among earth sciences data systems	atmospheric science;geographic information system;geographic information systems galeon web service interoperability earth sciences database;web service;function space;earth science data;geophysics computing;geographic information systems;web services geographic information systems geophysics computing visual databases;web services geoscience data systems network servers encoding markup languages earth databases geographic information systems access protocols;web services;visual databases	One barrier to research at the boundaries of the traditional Earth sciences is a lack of interoperability among data systems employed in the traditional subdisciplines. Solid Earth scientists (including the hydrology community) have tended to view their datasets as descriptions of discrete objects with attributes that can be stored and manipulated conveniently in a database. Geographic Information Systems (GIS) work well in this environment. On the other hand, the oceanographic and atmospheric sciences (the Fluid Earth Sciences or FES) communities think of data as discrete points in a continuous mathematical function space where the behavior of multiple parameters in space and time is governed by a set of equations.	data model;data system;galeon;geographic information system;interoperability;netcdf;shin megami tensei: persona 3;web coverage service;web service;yang	Ben Domenico;John Caron;Ethan Davis;Stefano Nativi;Lorenzo Bigagli	2006	2006 IEEE International Symposium on Geoscience and Remote Sensing	10.1109/IGARSS.2006.85	web service;geoportal;computer science;gis and public health;data mining;database;geographic information system;world wide web	DB	-37.71249412064415	-2.001841316877769	95501
68c54b6e41df1e6fb60b4f8df840dafa55332331	performance monitor for a relational information system	command language;performance monitoring;system designer;data collection;information analyst;performance improvement;performance prediction;information system;general motors;business information systems;systems approach	Although some relational information systems have recently become available for production use, very few, if any of them, contain facilities to collect performance data. This paper describes a method for implementing a performance monitor and some of the data collected by this performance monitor which was recently installed in the REGIS (RElational General Information System). REGIS is currently being used within General Motors. The performance monitor is used to collect data about the usefulness of the command language, performance improvements following major system upgrades and performance predictions based on past runs. While installing the performance monitor several system deficiencies were uncovered. Correction of the deficiencies has already improved performance by almost an order of magnitude. Future improvements are expected to improve performance by at least another order of magnitude.	command language;information system;regis;relational database	N. N. Oliver;John D. Joyce	1976		10.1145/800191.805609	simulation;engineering;operations management;data mining	DB	-42.82632955065929	-4.319755809132543	95528
c88a1174c3944222f6edf00b1d61446d4b015183	an efficient smart card based authentication scheme using image encryption	image encryption;smart card;authentication;torus automorphism;diffusion	CHIN-CHEN CHANG, HSIAO-LING WU, ZHI-HUI WANG AND QIAN MAO Department of Information Engineering and Computer Science Feng Chia University Taichung, 407 Taiwan Department of Computer Science and Information Engineering Asia University Taichung, 413 Taiwan School of Software Dalian University of Technology Dalian City, 116 P.R. China Department of Optical-Electrical and Computer Engineering University of Shanghai for Science and Technology Shanghai, 200 P.R. China	authentication;computer engineering;computer science;encryption;information engineering;smart card	Chin-Chen Chang;Hsiao-Ling Wu;Zhi-Hui Wang;Qian Mao	2013	J. Inf. Sci. Eng.		smart card;telecommunications;computer science;authentication;mathematics;diffusion;internet privacy;openpgp card;computer security	DB	-44.67582273075172	-7.830755102901623	95532
cc5b713d75e9dfefe83ea5985da55558b3bb7be4	listening to the pulse of our cities during city scale events		In recent years, we witnessed: the progressive instrumentation of our cities with diverse sensors; a wide adoption of smart phones and social networks that enable citizen-centric data acquisition; and a growing open release of datasets describing urban environments. As a result, nowadays it is becoming possible to make sense of all those data sources using semantic technologies together with streaming data analysis techniques. In particular, being able to feel the pulse of the city can allow delivering new services for the large number of organised and spontaneous events that take place in our cities. This talk frames the problem space, presents the challenges, proposes a solution centred on RDF stream processing that was applied to several case studies, and discusses the open problems that may be of interest of the Semantic Web community.	data acquisition;problem domain;semantic web;sensor;smartphone;social network;spontaneous order;stream (computing);stream processing;talk box	Siva Sankalp Patel	2013				Web+IR	-35.7304273668406	-6.413055586610442	95623
ae0a056dc0c7fd5f2df12eb2f14149721ec72387	storlet engine for executing biomedical processes within the storage system		The increase in large biomedical data objects stored in long term archives that continuously need to be processed and analyzed requires new storage paradigms. We propose expanding the storage system from only storing biomedical data to directly producing value from the data by executing computational modules storlets close to where the data is stored. This paper describes the Storlet Engine, an engine to support computations in secure sandboxes within the storage system. We describe its architecture and security model as well as the programming model for storlets. We experimented with several data sets and storlets including de-identification storlet to de-identify sensitive medical records, image transformation storlet to transform images to sustainable formats, and various medical imaging analytics storlets to study pathology images. We also provide a performance study of the Storlet Engine prototype for OpenStack Swift object storage.	archive;business process;computation;computer data storage;de-identification;medical imaging;object storage;programming model;prototype;region of interest;swift (programming language)	Simona Rabinovici-Cohen;Ealan A. Henis;John Marberg;Kenneth Nagin	2014		10.1007/978-3-319-15895-2_6	swift;computer science;real-time computing;object storage;systems engineering;computer security model;programming paradigm;architecture;data set;analytics;computer data storage	OS	-40.41039829976604	-4.295286525331949	95887
cb7802a665ffffbc1d4b20f222548f525885fc1c	web services discovery in a pay-as-you-go fashion	dataspace;web service discovery;web service similarity	Extensive effort has been brought forth to assist in web service discovery. In particular, classic Information Retrieval techniques are exploited to assess the similarity between two web services descriptions, while Semantic Web technologies are proposed to enhance semantic service descriptions. These approaches have greatly improved the quality and accuracy of service discovery. However, these works require hard up-front investment before offering powerful functionalities for service discovery, and they do not study how to discover web services in a pay-as-you-go fashion. In this paper, a framework based on dataspace techniques is proposed to discover web services in a pay-as-you-go fashion. In this framework, a loosely structured data model based on dataspace models is presented to describe web services and the relationships among them, and then keyword-based query is supported on top of this model by using the existing dataspace query language. To support similarity-based service discovery, dataspace techniques are extended to declare the similarity among web services, and a discovery algorithm is presented. In addition, a lightweight way adding semantics to the query processing is also shown in the paper. Finally, the differences between our work and previous works are discussed.	algorithm;data model;database;dataspaces;information retrieval;query language;semantic web;service discovery;web services discovery;web service	Ying Pan;Yong Tang;Shu Li	2011	J. UCS	10.3217/jucs-017-14-2029		Web+IR	-35.145092762791386	4.137212011563842	95935
28d2c9e93575b6345aaa8440df164df79388d0da	an advance air-induced air-assisted electrostatic nozzle with enhanced performance	volume median diameter vmd;aerodynamics;electrodynamics;number median diameter nmd;hydrodynamics	http://dx.doi.org/10.1016/j.compag.2017.02.010 0168-1699/ 2017 Elsevier B.V. All rights reserved. ⇑ Corresponding author at: CSIR-Central Scientific Instruments Organisation, Chandigarh 160030, India. E-mail address: manoj_patel@csio.res.in (M.K. Patel). Manoj Kumar Patel a,b,⇑, Bushra Praveen , Hemant Kumar Sahoo , Bharat Patel , Ashwani Kumar , Manjeet Singh , Manoj Kumar Nayak , Pradeep Rajan c	chemical vapor deposition;dc-to-dc converter;multistage interconnection networks;power supply;rechargeable battery	Manoj Kumar Patel;Bushra Praveen;Hemant Kumar Sahoo;Bharat Patel;Ashwani Kumar;Manjeet Singh;Manoj Kumar Nayak;Pradeep Rajan	2017	Computers and Electronics in Agriculture	10.1016/j.compag.2017.02.010	electronic engineering;aerodynamics;engineering;electrical engineering;nanotechnology	AI	-46.81281012163859	-7.088091274434253	96011
a93c37024b22c40874cd684c6106a3a737d3555f	research lattices: towards a scientific hypothesis data model	lattice theory;scientific hypothesis;large scale science;scientific databases;research progress	As the problems of scientific interest raise in scale and complexity, scientists have to tacitly manage too many analytic elements. Hypotheses are worked out to drive research towards successful explanation and prediction, which characterizes science as a dynamic activity that is partially ordered towards progress. This paper motivates and introduces research lattices, carrying out a lattice-theoretic approach for hypothesis representation and management in large-scale science and engineering. The goal of this work is to equip scientists with tools to manipulate and query hypotheses while keeping track of research progress. We refer to SciDB's array data model and discuss how data and theories could be managed in a unified model management framework.	data model;scidb;theory;unified model	Bernardo Gonçalves;Fábio Porto	2013		10.1145/2484838.2484861	hypothesis;computer science;data science;lattice;data mining;management science;statistics	DB	-34.80309015031051	-5.075252872560587	96060
2df89b158dba10271aca2f06d20d46d1c46e812b	g-exec geological data handling system	data handling		cms exec	Stephen Henley	2001	ACM SIGMOD Anthology		computer science;group method of data handling	Graphics	-39.289410221277244	-4.483965755282423	96073
449966e2a308be23b20ff3dad63b2455fb7205cf	a context-aware personalized travel recommendation system based on geotagged social media data mining	geo referenced photographs;context aware;information systems;on line;digital camera;travel;information services;data mining;photographs;tourism;recommender system;photo sharing;preference elicitation;context aware query;social media;spatiotemporal data mining;geographical gazetteer;trip planning	A context-aware personalized travel recommendation system based on geotagged social media data mining Abdul Majid a , Ling Chen a , Gencai Chen a , Hamid Turab Mirza a , Ibrar Hussain a & John Woodward b a College of Computer Science, Zhejiang University, 38 Zheda Road, Hangzhou, 310027, PR China b School of Computer Science, The University of Nottingham Ningbo China, Ningbo, 315100, PR China Published online: 24 Jul 2012.	computer science;data mining;entity–relationship model;geotagging;hsinchun chen;personalization;recommender system;social media	Abdul Majid;Ling Chen;Gencai Chen;Hamid Turab Mirza;Ibrar Hussain;John R. Woodward	2013	International Journal of Geographical Information Science	10.1080/13658816.2012.696649	geography;computer science;database;multimedia;internet privacy;world wide web;information system;recommender system	DB	-43.307924447006506	-8.133334310175334	96138
8d6f93f7310e05a5dd473f94c4c09c567f028a9a	falcon-ao: results for oaei 2007	ontology matching	In this paper, we present a brief overview of Falcon-AO (version 0.7): a practical ontology matching system with acceptable to very good performance, a flexible architecture, and a number of unique features. We also show some preliminary results of Falcon-AO for this year’s OAEI campaign: evaluation on seven different matching tasks. 1 Presentation of the system As an infrastructure for Semantic Web applications, Falcon is a vision of our research group. It desires for providing fantastic technologies for finding, aligning and learning ontologies, and ultimately for capturing knowledge by an ontology-driven approach. It is still under development in our group. 1.1 State, purpose, general statement As a prominent component of Falcon, Falcon-AO starts as an automatic ontology matching system to help enable interoperability between (Semantic) Web applications using different but related ontologies. It has since become a very practical and popular tool for matching Web ontologies expressed in RDFS or OWL. To date, Falcon-AO is continually being improved and elaborated, and the latest version is 0.7. 1.2 Specific techniques used Falcon-AO is implemented in Java, and it is an open source project under the Apache license. Fig. 1 exhibits the architecture of Falcon-AO (version 0.7). It consists of five components: the Repository to temporarily store the data during the matching process; the Model Pool to manipulate ontologies and to construct different models for different matchers; the Alignment Set to generate and to evaluate exported alignments; the Matcher Library to manage a set of elementary matchers; the Central Controller to configure matching strategies and to execute matching operations. Furthermore, Falcon-AO provides a graphical user interface (GUI) to make it easily accessible to users. Due to space limitation, we only provide a brief overview of Falcon-AO’s features in this paper. For more details, we refer the reader to the technical papers [1–6], and our website: http://iws.seu.edu.cn/projects/matching/. G r a p h i c a l U s e r I n t e r f a c e	ambient occlusion;falcon;graphical user interface;interoperability;java;ontology (information science);ontology alignment;open-source software;rdf schema;radio frequency;semantic web;sequence alignment;web ontology language;xfig	Wei Hu;Yuanyuan Zhao;Dan Li;Gong Cheng;Honghan Wu;Yuzhong Qu	2007			simulation;computer science;data mining;management science	AI	-40.94704106697043	4.084537412573455	96275
698740d52939fd24fc5ce81fb388fc18743af69b	supporting complex thematic, spatial and temporal queries over semantic web data	national security;spatial rdf;ontology semantic analytics rdf querying temporal rdf spatial rdf;bepress selected works;temporal data;data model;temporal rdf;efficient implementation;rdf querying;semantic web;ontology;semantic analytics;named entity;semantic association	Spatial and temporal data are critical components in many applications. This is especially true in analytical domains such as national security and criminal investigation. Often, the analytical process requires uncovering and analyzing complex thematic relationships between disparate people, places and events. Fundamentally new query operators based on the graph structure of Semantic Web data models, such as semantic associations, are proving useful for this purpose. However, these analysis mechanisms are primarily intended for thematic relationships. In this paper, we describe a framework built around the RDF metadata model for analysis of thematic, spatial and temporal relationships between named entities. We discuss modeling issues and present a set of semantic query operators. We also describe an efficient implementation in Oracle DBMS and demonstrate the scalability of our approach with a performance study using a large synthetic dataset from the national security domain.	data model;named entity;oracle database;scalability;semantic web;semantic query;synthetic data	Matthew Perry;Amit P. Sheth;Farshad Hakimpour;Prateek Jain	2007		10.1007/978-3-540-76876-0_15	cwm;semantic computing;semantic web rule language;bibliographic ontology;semantic grid;computer science;sparql;simple knowledge organization system;social semantic web;linked data;data mining;semantic web stack;database;rdf query language;information retrieval;semantic analytics;rdf schema	DB	-37.07326777813728	3.9461686798941424	96276
c8581df31ed1bd118b5a9471f9352ccff3bd6aed	using domain knowledge to learn from heterogeneous distributed databases	distributed data;kullback leibler information;distributed database;heterogeneous databases;domain knowledge;association rule;bayesian belief network	  We are concerned with the processing of data held in distributed heterogeneous databases using domain knowledge, in the form  of rules representing high-level knowledge about the data. This process facilitates the handling of missing, conflicting or  unacceptable outlying data. In addition, by integrating the processed distributed data, we are able to extract new knowledge  at a finer level of granularity than was present in the original data. Once integration has taken place the extracted knowledge,  in the form of probabilities, may be used to learn association rules or Bayesian belief networks. Issues of confidentiality  and efficiency of transfer of data across networks, whether the Internet or Intranets, are handled by aggregating the native  data in situ, typically behind a firewall, and carrying out further transportation and processing solely on multidimensional  aggregate tables. Heterogeneity is resolved by utilisation of domain knowledge for harmonisation and integration of the distributed  data sources. Integration is carried out by minimisation of the Kullback-Leibler information divergence between the target  integrated aggregates and the distributed data values.    		Sally I. McClean;Bryan W. Scotney;Mary Shapcott	2004		10.1007/978-3-540-30132-5_28	computer science;pattern recognition;data mining;database	DB	-37.56205700877306	-0.2246919587561473	96317
d66ced56f98601ca3c379ca354988b20c2f43d61	scalable management of trajectories and context model descriptions		The ongoing proliferation of sensing technologies constitutes a huge potential for context-aware computing. It allows selecting relevant information about our physical environment from different sources and providers all over the globe. A fundamental challenge is how to provide efficient access to these immense amounts of distributed dynamic context information – particularly due to the mobility of devices and other entities. To enable such access to current and past position information aboutmoving objects, we propose a family of protocols (CDR, GRTS) for efficiently tracking a moving objects trajectory at some remote database in real-time as well as a distributed indexing scheme (DTI) for optimized access to trajectory data that is partitioned in space to multiple database servers. For discovering context information that is relevant for the situation of an application, we propose a powerful formalism for describing context models in a concise manner and a tailored multidimensional data structure (SDC-Tree) for retrieving relevant context models out of potentially millions of descriptions. Ralph Lange*: E-Mail: ralph.lange@acm.org	authentication;context awareness;data structure;database server;download;entity;real-time clock;real-time locating system;remote database access;semantics (computer science);smart data compression	Ralph Markus Lange	2010		10.1515/pik-2012-0041	computer science;distributed computing;scalability;context model	DB	-35.99036303402856	1.1740696743439507	96615
5376332678fb301b469cde00b03018f02626dd47	hybrid control for automotive engine management: the cut-off case	modelizacion;control optimo;systeme commande;sistema control;industrie automobile;sistema hibrido;hybrid control;motor explosion interna;cut off;hybrid model;optimal control;optimization problem;modelisation;control problem;control system;moteur combustion interne;commande optimale;internal combustion engine;industria automovil;hybrid system;formal analysis;modeling;automobile industry;systeme hybride	OBJECTIVES: • Technical position in the Research & Development division of an embedded systems company, with emphasis on distribution, realtime, and reliability. EMPLOYMENT HISTORY: • October 2006-present: Research Scientistat Cadence Research Labs in Berkeley. Topics: system level design, desynchronization, distribution, reliability, and low-power design. • April 2006-October 2006: Sr. Researcherat General Motors Research, GM Advanced Technology, Silicon Valley Office. Topics: system-level architecture exploration. Static worst-case end to end latency analysis. Architecture analysis and definition for active and passive safety systems. • September 2004-March 2006: Project Engineer at Quantech Global Services, consulting for the GM Berkeley Labs. Topics: architecture exploration tools and methods, for automotive distributed applications. • May 2002-August 2002: Summer Intern at the Cadence Berkeley Labs. Research project: development of a library for the design of fault-tolerant systems in Metropolis. • June 2000-July 2000: Stagist at the INRIA Rĥone-Alpes, Montbonnot, France. Research project: generation of static schedules for distributed fault tolerant control systems. • June 1999-August 1999: Summer Intern at the BMW Technology Office in Palo Alto CA. Research project: study and definition of a design methodology for Automotive Safety-Critical Distributed Applications. • August 1998-September 2004: Graduate Student Researcherat the Electronic Research Laboratory of UC Berkeley. Research project: hybrid systems in engine control problems; embedded systems design methodology for automotive and consumer-oriented applications; fault-tolerant systems. Advisor: Prof. Alberto Sangiovanni-Vincentelli. • July 1997-July 1998:Consultant to PARADES EEIG Rome. Development and control of hybrid models for engine control problems. Study of engine control unit constraint-driven design methodology. EDUCATION: • August 2004: completed the PhD in Electrical Engineering at the University of California, Berkeley. Major: systems. GPA 3.885/4.0. Fault Tolerant Distributed Systems. • May 2001: received thePhD in Computing Systems Engineering from the Scuola Superiore Sant’Anna in Pisa (Italy). Hybrid Systems Control for Engine Control applications. • June 1997: received the Dottore in Ingegneria degreesumma cum laude (equivalent to a MS degree) in Electrical Engineering from the Universit à di Roma “La Sapienza”. 110/110 cum laude.	alberto sangiovanni-vincentelli;bsd;best, worst and average case;bibliothèque des ecoles françaises d'athènes et de rome;control system;distributed computing;electrical engineering;embedded system;engine control unit;fault tolerance;fault-tolerant computer system;hybrid system;level design;low-power broadcasting;metropolis;palo;representation oligonucleotide microarray analysis;systems design;systems engineering;uc browser	Andrea Balluchi;Maria Domenica Di Benedetto;Claudio Pinello;C. Rossi;Alberto L. Sangiovanni-Vincentelli	1998		10.1007/3-540-64358-3_29	optimization problem;internal combustion engine;systems modeling;optimal control;control system;hybrid system	Embedded	-46.52948961396001	-5.999910607530066	97027
1fe152a6013ee34898d1436394911253ea8ce51f	resolving data heterogeneity in scientific and statistical databases	statistical databases;resolving data heterogeneity		database	Abhirup Chatterjee;Arie Segev	1992			computer science;database;data mining	HPC	-35.759802219263726	0.8302853392276058	97134
666db4f97dea1b741e40919855f31eec02b31a91	underwater wireless sensor networks 2015		1School of Computer Science & Engineering, Kyungpook National University, Daegu 702-701, Republic of Korea 2Universidad Politécnica de València, Camino de Vera, S/N, 46022 València, Spain 3Department of Computer Science, San Diego State University, Campanile Drive, San Diego, CA 92115, USA 4Department of Informatics, Electronic and Sistemistic, University of Calabria, 87036 Cosenza, Italy 5Department of Electrical and Computer Engineering, Lawrence Technological University, Southfield, MI 48075, USA		Dongkyun Kim;Juan-Carlos Cano;Wei Wang;Floriano De Rango;Kun Hua	2015	IJDSN	10.1155/2015/623042	wireless sensor network;wireless network;key distribution in wireless sensor networks;wi-fi array;mobile wireless sensor network	Logic	-45.259797971273045	-8.04613896041557	97157
15644b745ea870677ac958cc53040a2a76c3eb1d	software architecture challenges for data intensive computing	resource utilization;data intensive application;general and miscellaneous mathematics computing and information science;social computing;scalable data management software architecture data intensive computing multiterabyte data volumes petabyte data volumes scientific research bio informatics cyber security social computing commerce innovative hardware innovative software data analysis adaptive resource utilization;resource allocation;software architecture resource management application software computer security social network services business hardware data analysis buildings robustness;data management;cyber security;computer architecture;data analysis;software architecture;software architecture data analysis resource allocation;data intensive computing;data intensive computing software architecture;scientific research;data base management;security data intensive computing	Data intensive computing is concerned with creating scalable solutions for capturing, analyzing, managing and understanding multi-terabyte and petabyte data volumes. Such data volumes exist in a diverse range of application domains, including scientific research, bio-informatics, cyber security, social computing and commerce. Innovative hardware and software technologies to address these problems must scale to meet these ballooning data volumes and simultaneously reduce the time needed to provide effective data analysis. This paper describes some of the software architecture challenges that must be addressed when building data intensive applications and supporting infrastructures. These revolve around requirements for adaptive resource utilization and management, flexible integration, robustness and scalable data management.	bioinformatics;british informatics olympiad;computer security;data-intensive computing;petabyte;requirement;scalability;social computing;software architecture;terabyte	Ian Gorton	2008	Seventh Working IEEE/IFIP Conference on Software Architecture (WICSA 2008)	10.1109/WICSA.2008.50	software architecture;computer architecture;in situ resource utilization;scientific method;data management;resource allocation;computer science;software engineering;data-intensive computing;data mining;database;data analysis;social computing;data architecture	HPC	-41.53588034268832	-1.731915748881634	97158
72fdfbaf31be04a24064c4a89135f8c99c176943	the integration methods of 3d gis and 3d cad	computer aided design;information loss;geographic information system;spatial data;software platform;3d gis;information technology;data management;data exchange;formal semantics;directional data;3d representation;coordinate system	GIS (Geographical Information System) and CAD (Computer Aided Design) have evolved largely in the past 30 years. GIS and CAD  describe the same real-world objects but they belong to different domains. The spatial and non-spatial data involved in the  two systems are quite different and the two systems cater for different applications. With the advancement in information  technology and 3D representation of geospatial information, more and more 3D applications demand both of them to be used together.    This paper focuses on how to integrate the 3D GIS and 3D CAD. After reviewing the development of both the 3D GIS and 3D CAD  systems, this paper compares their differences in the domain/purpose, coordinate system, object type, and so on. Then five  approaches for the integration of 3D GIS and 3D CAD system are introduced, i.e. data exchange; direct data import; share the  API of database; CAD-based GIS system/GIS-based CAD system; formal semantic and integrated data management.        Basing on the first approach, this paper designs the scheme of integrating 3D GIS and 3D CAD systems with the considerations  that CAD data maybe in a local coordinate system not compatible with GIS data; the information loss due to data translation;  translating data between the complex geometric description in CAD and the simple geometric description in GIS and so on. According  to this scheme, and based on VGEGIS software platform developed by LIESMARS of Wuhan University and AutoCAD 2006, a prototype  system is developed to demonstrate the integration of 3D GIS and 3D CAD.      	computer-aided design;geographic information system	Juan Li;Tor Yam Khoon;Zhu Qing	2006		10.1007/978-3-540-36998-1_19	idef1x;distributed gis;data modeling;enterprise gis;computer science;theoretical computer science;geospatial analysis;data mining;database;geographic information system;am/fm/gis	Robotics	-36.69085098873911	-2.3320707511494976	97415
e5626f4d48ddab07a955abda5d4d30abff48d64e	assessing educational research - an information service for monitoring a heterogeneous research field		The paper presents a web prototype that visualises different characteristics of research projects in the heterogeneous domain of educational research. The concept of the application derives from the project “Monitoring Educational Research” (MoBi) that aims at identifying and implementing indicators that adequately describe structural properties and dynamics of the research field. The prototype enables users to visualise data regarding different indicators, e.g. “research activity”, “funding”, “qualification project”, “disciplinary area”. Since the application is based on Semantic MediaWiki technology it furthermore provides an easily accessible opportunity to collaboratively work on a database of research projects. Users can jointly and in a semantically controlled way enter metadata on research 1 Corresponding author projects which are the basis for the computation and visualisation of indicators.	computation;mediawiki;prototype	Karima Haddou ou Moussa;Ute Sondergeld;Philipp Mayr;Peter Mutschke;Marc Rittberger	2014	CoRR			SE	-43.68184850421123	1.7622999036181128	97478
5e6ebc7b0aa3bbe2dbecceb89e11439c550a70e7	text-based event temporal resolution and reasoning for information analytics in big data	temporal representation text based event temporal resolution information analytics big data semantic units rule based temporal relation reasoning;resolution;information retrieval;big data event information analytics temporal information resolution reasoning;semantics;natural languages;data mining;temporal information;big data;cognition;cognition data mining big data information retrieval semantics organizations natural languages;knowledge based systems big data inference mechanisms;event;organizations;reasoning;information analytics	Events formulate the world of human being and could be regarded as the semantic units in different granularities for information organization. Extracting events and temporal information from texts plays an important role for information analytics in big data. This paper surveys research work on text-based event temporal resolution and reasoning including the identification of events, temporal information resolutions of events, the rule-based temporal relation reasoning between events and the temporal representations. We point out the shortcomings of existing research work and the future trends for advancing the identification of events and the establishing/reasoning of temporal relations in the future.	big data;information theory;knowledge organization;logic programming;text-based (computing)	Junsheng Zhang;Changqing Yao;Peng Qu;Yunchuan Sun	2015	2015 International Conference on Identification, Information, and Knowledge in the Internet of Things (IIKI)	10.1109/IIKI.2015.24	cognition;big data;resolution;event;computer science;organization;data science;data mining;semantics;natural language;information retrieval;reason	DB	-34.1000372445219	-6.164508929215607	97733
9a8faa3ba37e9296040d1a0c1ddf22a3e37d5423	templated search over relational databases	query recommendations;keyword search	Businesses and large organizations accumulate increasingly large amounts of customer interaction data. Analysis of such data holds great importance for tasks such as strategic planning and orchestration of sales/marketing campaigns. However, discovery and analysis over heterogeneous enterprise data can be challenging. Primary reasons for this are dispersed data repositories, requirements for schema knowledge, and difficulties in using complex user interfaces. As a solution to the above, we propose a TEmplated Search paradigm (TES) for exploring relational data that combines the advantages of keyword search interfaces with the expressive power of question-answering systems. The user starts typing a few keywords and TES proposes data exploration questions in real time. A key aspect of our approach is that the questions displayed are diverse to each other and optimally cover the space of possible questions for a given question-ranking framework. Efficient exact and provably approximate algorithms are presented. We show that the Templated Search paradigm renders the potentially complex underlying data sources intelligible and easily navigable. We support our claims with experimental results on real-world enterprise data.	approximation algorithm;programming paradigm;question answering;real-time computing;relational database;rendering (computer graphics);requirement;search algorithm;user interface	Anastasios Zouzias;Michail Vlachos;Vagelis Hristidis	2014		10.1145/2661829.2661883	computer science;artificial intelligence;machine learning;data mining;database;world wide web;information retrieval	DB	-35.04289516972569	2.2740629356739244	97807
529ae50d73338ce1fd62d10eb068b644f0b55be5	a rule-based citation system for structured and evolving datasets	rule based	We consider the requirements that a citation system must fulfill in order to cite structured and evolving data sets. Such a system must take into account variable granularity, context and the temporal dimension. We look at two examples and discuss the possible forms of citation to these data sets. We also describe a rule-based system that generates citations which fulfill these requirements.	requirement;rule-based system	Peter Buneman;Gianmaria Silvello	2010	IEEE Data Eng. Bull.		computer science	ML	-36.54542072999041	-0.7832443040210882	98078
71b59a6ddb466a95e22fdfe9112140adcbaf5bcb	the spectral database specchio for improved long-term usability and data sharing	busqueda de datos;computadora;tratamiento datos;contraste;computers;software;proyeccion;teledetection;data sharing;metadata;mysql;logiciel;ordinateur;data collection;import;collections;geometry;base dato;data processing;geometrie;traitement donnee;langage java;stockage donnee;relational database;coleccion;data bases;deteccion a distancia;data model;collection;importation;navigation;data storage;modelo;importacion;projection;remote sensing;base de donnees;almacenamiento datos;etalonnage;geometria;modele;analyse automatisee;hyperspectral signatures;restitution donnee;data retrieval;models;calibration;java language;automated analysis	The organised storage of spectral data described by metadata is important for long-term use and data sharing with other scientists. Metadata describing the sampling environment, geometry and measurement process serves to evaluate the suitability of existing data sets for new applications. There is a need for spectral databases that serve as repositories for spectral field campaign and reference signatures, including appropriate metadata parameters. Such systems must be (a) highly automated in order to encourage users entering their spectral data collections and (b) provide flexible data retrieval mechanisms based on subspace projections in metadata spaces. The recently redesigned SPECCHIO system stores spectral and metadata in a relational database based on a non-redundant data model and offers efficient data import, automated metadata generation, editing and retrieval via a Java application. RSL is disseminating the database and software to the remote sensing community in order to foster the use and further development of spectral databases. & 2008 Elsevier Ltd. All rights reserved.	access control;data model;data quality;data retrieval;data structure;emoticon;heterogeneous computing;java;relational database;renderman shading language;sampling (signal processing);server (computing);signature;spectral method;usability	Andreas Hueni;Jens Nieke;Juerg Schopfer;Mathias Kneubühler;Klaus I. Itten	2009	Computers & Geosciences	10.1016/j.cageo.2008.03.015	metadata modeling;navigation;calibration;collection;data processing;data transformation;projection;data model;relational database;computer science;computer data storage;data mining;database;change data capture;database catalog;metadata;world wide web;data retrieval;data element;meta data services;data mapping;metadata repository;data collection;data dictionary	DB	-40.847044297462325	-3.305702263685519	98137
8a4f0fd41ad5c17fad45b99e627554e0b02063cc	what can database do for peer-to-peer?	peer to peer system;peer to peer	The Internet community has recently been focused on peer-to-peer systems like Napster, Gnutella, and Freenet. The grand vision — a decentralized community of machines pooling their resources to benefit everyone — is compelling for many reasons: scalability, robustness, lack of need for administration, and even anonymity and resistance to censorship. Existing peer-to-peer (P2P) systems have focused on specific application domains (e.g. music files) or on providing filesystem-like capabilities; these systems ignore the semantics of data. An important question for the database community is how data management can be applied to P2P, and what we can learn from and contribute to the P2P area. We address these questions, identify a number of potential research ideas in the overlap between data management and P2P systems, present some preliminary fundamental results, and describe our initial work in constructing a P2P data management system.	database;freenet;gnutella;napster;peer-to-peer;scalability	Steven D. Gribble;Alon Y. Halevy;Zachary G. Ives;Maya Rodrig;Dan Suciu	2001			computer science;peer feedback	DB	-46.4918086188472	0.4524474251452397	98175
46a66a93c744f3ba86ea94d6bd1c9caf0a17b653	provenance in web feed mash-up systems	storage optimization method;web feed;provenance tracking;mash up;data provenance	"""The recent emergence of web 2.0 technologies and rich internet applications is driving the development of a new class of applications that combines data from diverse sources which we refer to as """"mash-ups."""" One of the most popular mash-ups comes in the form of web feed mash-ups relying on syndication technologies such as RSS and Atom. This kind of mash-ups aggregates web feeds derived from multiple news websites or blogs and then timely presents them in a single interface. In such systems, it is difficult to know exactly how feed results in data mash-ups are generated. In particular, it is difficult for users to make determinations about whether information is trusted. Therefore, it is necessary that web feed mash-ups have to support a mechanism that is capable of recording and querying provenance information-the information about the process that led to result data. In this paper, the author proposes a provenance tracking solution that enables provenance functionality to be facilitated in web feed mash-ups. He demonstrates how the provenance of feed mash-up results to be determined by means of a provenance query algorithm. To tackle the storage problem resulting from the persistence of intermediate web feeds, a novel storage optimization method is introduced. Finally, the author evaluates his provenance solution in terms of storage consumption for provenance collection, demonstrating significant reductions in storage size and achieving reasonable storage overheads."""	web feed	Watsawee Sansrimahachai	2016	IJITWE	10.4018/IJITWE.2016100103	computer science;operating system;data mining;database;internet privacy;world wide web;computer security;mashup	ECom	-35.62069651435545	3.275586559731752	98218
89560be7bef9933e5ce137e7fb02a37a1aab6541	a geoweb-based tagging system for borderlands data acquisition	spatial publish subscribe;openstreetmap;geoweb;期刊论文;borderlands;mashup;geospatial blog	Borderlands modeling and understanding depend on both spatial and non-spatial data, which were difficult to obtain in the past. This has limited the progress of borderland-related research. In recent years, data collection technologies have developed greatly, especially geospatial Web 2.0 technologies including blogs, publish/subscribe, mashups, and GeoRSS, which provide opportunities for data acquisition in borderland areas. This paper introduces the design and development of a Geoweb-based tagging system that enables users to tag and edit geographical information. We first establish the GeoBlog model, which consists of a set of geospatial components, posts, indicators, and comments, as the foundation of the tagging system. GeoBlog is implemented such that blogs are mashed up with OpenStreetMap. Moreover, we present an improvement to existing publish/subscribe systems with support for spatio-temporal events and subscriptions, called Spatial Publish/Subscribe, as well as the event agency network for routing messages from the publishers to the subscribers. A prototype system based on this approach is implemented in experiments. The results of this study provide an approach for asynchronous interaction and message-ordered transfer in the tagging system.	algorithm;blog;borderlands 2;data acquisition;experiment;georss;geoweb;mashup (web application hybrid);mathematical optimization;openstreetmap;prototype;publish–subscribe pattern;relational operator;requirement;routing;tag (metadata);usability;web 2.0	Hanfa Xing;Jun Chen;Xiaoguang Zhou	2015	ISPRS Int. J. Geo-Information	10.3390/ijgi4031530	geography;database;internet privacy;world wide web	HCI	-36.743539772541865	-4.608642138180247	98233
0bc834c5f646a2e5a3e5c7d74f8953e0704d71b3	the ontological approach for siem data repository implementation	security information and event management;software;secure evaluation tasks ontological approach siem data repository implementation security information and event management computer network security distributed networks internet enabled objects data storage level heterogeneous security events information security ontological repository;computer network security;standards;ontologies artificial intelligence computer network security internet;data representation;ontologies artificial intelligence;data model;repository;computer architecture;security ontologies data models internet standards software computer architecture;internet;ontologies;repository ontology security information and event management data model data representation logical inference;logical inference;security;ontology;data models	The technology of Security Information and Event Management (SIEM) becomes one of the most important research applications in the area of computer network security, including distributed networks of internet enabled objects (as in the Internet of Things). The overall functionality of SIEM systems depends largely on the quality of solutions implemented at the data storage level, which is purposed for the representation of heterogeneous security events, their storage in the data repository and the extraction of relevant data for the analytical modules of SIEM systems. An ontological approach at present becomes more applicable for realizing these tasks in various spheres of information security. The paper discusses the possibilities of applying the ontological approach for implementation of the data repository of SIEM systems for distributed networks of Internet enabled objects. Based on the analysis of existing SIEM systems and standards, the choice of ontological approach is done, an example of the ontological data model of vulnerabilities is presented, a hybrid architecture of the ontological repository is proposed and the issues of developing and testing the repository for attack modelling and secure evaluation tasks are discussed.	computer data storage;data (computing);data model;data visualization;information security;internet of things;network security policy;relational database;security information and event management;xml database	Igor V. Kotenko;Olga Polubelova;Igor Saenko	2012	2012 IEEE International Conference on Green Computing and Communications	10.1109/GreenCom.2012.125	computer science;data mining;database;world wide web	DB	-38.08244723584676	2.200196358050891	98309
1ca1d79734a31eb71e6c8d32adb0548d6f374508	towards a three-level framework for iot redundancy control through an explicit spatio-temporal data model		Abstract: In this paper we present an ongoing work towards the implementation of a framework that tackles service redundancy in IoT/WSNs as an explicit spatio-temporal phenomenon. From this perspective, redundancy is measured and explicitly stored using a spatio-temporal data model. The expected advantages of keeping an explicit history of redundancy evolution in space and time are to compare different redundancy control algorithms, to apply different knowledge extraction techniques in order to identify possible redundancy patterns, and to implement more proactive redundancy control strategies. In this paper we focus on the data model that we propose to control service redundancy at three scales: macro, meso and micro scales, respectively.	data model	Hedi Haddad;Zied Bouyahia;Nafaâ Jabeur	2017		10.1016/j.procs.2017.05.373	machine learning;redundancy (engineering);wireless sensor network;knowledge extraction;theoretical computer science;data mining;artificial intelligence;macro;computer science;data model;temporal database;internet of things;phenomenon	Robotics	-35.605998076450014	-1.5377954644725185	98366
32fbd3305a3030e7408bd7f38db364047f9f729d	the meaningful use of big data: four perspectives - four challenges	semantic technologies;flash memory ssds;article letter to editor;sorted index scan;semantic web;partitioned sort	Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.	big data;semantic web	Christian Bizer;Peter A. Boncz;Michael L. Brodie;Orri Erling	2011	SIGMOD Record	10.1145/2094114.2094129	semantic computing;data web;semantic search;semantic grid;computer science;semantic web;social semantic web;data mining;semantic web stack;database;semantic technology;world wide web;semantic analytics	DB	-44.37645507443443	0.8054556280789094	98399
4a50882f9983585f7d7d0d4072546a2f7063a81d	passive and active acoustics using an autonomous wave glider	modified wave glider;acoustic application;acoustic navigation;wiley periodicals;wave glider;integrated passive acoustic recorder;acoustic positioning performance;active acoustic gateway;passive acoustic monitoring technique;autonomous wave glider;commercial off-the-shelf acoustic positioning	Brian Bingham and Nicholas Kraus Department of Mechanical Engineering, University of Hawaii at Manoa, Honolulu, Hawaii 96822 e-mail: bsb@hawaii.edu, nkraus@hawaii.edu Bruce Howe Department of Ocean and Resources Engineering, University of Hawaii at Manoa, Honolulu, Hawaii 96822 e-mail: bhowe@hawaii.edu Lee Freitag, Keenan Ball, Peter Koski, and Eric Gallimore Department of Applied Ocean, Physics and Engineering, Woods Hole Oceanographic Institution, Woods Hole, Massachusetts 02543 e-mail: lfreitag@whoi.edu, kball@whoi.edu, pkoski@whoi.edu, egallimore@whoi.edu	autonomous robot;brian;email;glider (conway's life);sonar;eric	Brian Bingham;Nicholas Kraus;Bruce Howe;Lee Freitag;Keenan Ball;Peter Koski;Eric Gallimore	2012	J. Field Robotics	10.1002/rob.21424	simulation;acoustics;telecommunications;engineering	Robotics	-46.159893792168326	-8.302570048706523	98503
fc59ff51100dba7504e90dc3f004b74aa8f2057e	scalable and object oriented sdl state(chart)s	object oriented sdl state;object oriented	Personalia Birger Møller-Pedersen, Professor Department of Informatics, University of Oslo P.O.Box 1080 Blindern, 0316 Oslo, Norway Phone: +47 22 85 24 37, Email: birger@ifi.uio.no Home address: Røahagan 33A, 0754 Oslo Born: November 11, 1949, Rønne, Denmark. Nationality: Danish. Qualifications: Cand. scient. (M.Sc.) in Computer Science, Department for Computer Science, University of Aarhus, Denmark(1976).	computer science;email;informatics;international software testing qualifications board	Birger Møller-Pedersen;Dagbjørn Nogva	1999			real-time computing;computer science;programming language;object-oriented programming	Theory	-46.713805188515806	-5.602473237266707	98605
84f52fa7b2899084c6942ec0240904db926f6a3b	use of tencent street view imagery for visual perception of streets		Liang Cheng 1,2,3,4, Sensen Chu 1,2,4,* ID , Wenwen Zong 1,4, Shuyi Li 1,4, Jie Wu 1,4 and Manchun Li 1,4,* 1 Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Nanjing University, Nanjing 210093, China; lcheng@nju.edu.cn (L.C.); MG1627083@smail.nju.edu.cn (W.Z.); DZ1627003@smail.nju.edu.cn (S.L.); wujie_nju@163.com (J.W.) 2 Collaborative Innovation Center for the South Sea Studies, Nanjing University, Nanjing 210093, China 3 Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing University, Nanjing 210093, China 4 Department of Geographic Information Science, Nanjing University, Nanjing 210093, China * Correspondence: chusensen@163.com (S.C.); limanchun_nju@126.com (M.L.); Tel.: +86-25-83597359 (S.C.); +86-25-89681185 (M.L.)	colocation centre;experiment;geographic information science;google street view;grey goo;high-level programming language;image processing;jie wu;naruto shippuden: clash of ninja revolution 3;numerical analysis;openness;real life	Liang Cheng;Sensen Chu;Wenwen Zong;Shuyi Li;Jie Wu;Manchun Li	2017	ISPRS Int. J. Geo-Information	10.3390/ijgi6090265	salient;multimedia;visual perception;cartography;geography;urban planning	HPC	-44.366556262568245	-9.563221105804528	98897
091d48525d5670f93e38d131419fe557822a02d4	distrdf: distributed spatio-temporal rdf queries on spark		The ever-increasing size of data emanating from mobile devices and sensors, dictates the use of distributed systems for storing and querying the data. Typically, these data sources provide some spatio-temporal information, alongside other useful data. The task of interlinking and interchanging this kind of information is challenging in the case of large heterogeneity of data sources. This issue can be addressed by adopting the RDF data model, proposed by W3C. Hence, with respect to an application scenario which analyzes vast amount of spatio-temporal heterogeneous data, the task of efficiently evaluating spatio-temporal queries on RDF is crucial. In this paper, we address the problem of efficiently processing SPARQL spatio-temporal queries in parallel, by proposing the DiStRDF system. We use Spark, a well-known distributed in-memory processing framework, as the underlying processing engine. On top of it, we devise a set of query execution plans which exploit an 1D encoding scheme for improving the performance of our system. Our experimental evaluation demonstrates the efficiency of DiStRDF system.	best practice;data model;distributed computing;experiment;in-memory database;in-memory processing;information retrieval;line code;mobile device;resource description framework;spark;sparql;scalability;sensor	Panagiotis Nikitopoulos;Akrivi Vlachou;Christos Doulkeridis;George A. Vouros	2018			rdf;spark (mathematics);database;computer science	DB	-35.08301171669636	1.7713934849457744	99307
1bd0d84d509b72d641b6e50f9ef6585f4784fd8a	query optimization for ontology-based information integration	query optimization;information integration;indexation;semantic web;synthetic data;ontology;query rewriting	In recent years, there has been an explosion of publicly available RDF and OWL data sources. In order to effectively and quickly answer queries in such an environment, we present an approach to identifying the potentially relevant Semantic Web data sources using query rewritings and a term index. We demonstrate that such an approach must carefully handle query goals that lack constants; otherwise the algorithm may identify many sources that do not contribute to eventual answers. This is because the term index only indicates if URIs are present in a document, and specific answers to a subgoal cannot be calculated until the source is physically accessed - an expensive operation given disk/network latency. We present an algorithm that, given a set of query rewritings that accounts for ontology heterogeneity, incrementally selects and processes sources in order to maintain selectivity. Once sources are selected, we use an OWL reasoner to answer queries over these sources and their corresponding ontologies. We present the results of experiments using both a synthetic data set and a subset of the real-world Billion Triple Challenge data.	algorithm;experiment;mathematical optimization;ontology (information science);query optimization;resource description framework;selectivity (electronic);semantic web;semantic reasoner;synthetic data;web ontology language	Yingjie Li;Jeff Heflin	2010		10.1145/1871437.1871623	sargable;query optimization;query expansion;web query classification;computer science;information integration;semantic web;ontology;data mining;database;ontology-based data integration;web search query;range query;world wide web;information retrieval;query language;synthetic data	Web+IR	-35.7170221986246	4.0725207729837845	99779
6a3b23caa5330dd06ab54c2c051a87060032cfda	semantic access to sensor observations through web apis	databases;web services application program interfaces;mashups;semantics;semantics data models resource description framework ontologies mashups encoding databases;resource description framework;semantic sensor networks semantic mashups web services web apis;semantic mashups;application program interfaces;web services;web apis;ontologies;encoding;web service semantic access sensor observation infrastructure web api sensor network large scale information management gis system rest linked data principle application programming interface;data models;semantic sensor networks	Sensor networks are often deployed with the purpose of providing data to large-scale information management and GIS systems, or to collect measurements for specific scientific experiments. The benefits of such use are clear and widely accepted. The reuse of observations in low-cost, lightweight, web applications and mashups is a further compelling use case for sensor networks, but requires provision of data through simple mechanisms, readily accessible, that are quick to develop with. To enable the latter while maintaining support for larger applications and, to increase information utility, links to and from other datasets, we propose a domain-driven approach that embodies REST and Linked Data principles using a common semantic framework that underpins a separation of concerns between domain models, sensor observation infrastructure, and Application Programming Interfaces (APIs) while maintaining information consistency. We describe a reusable, reconfigurable, web service that realises this design and can be deployed to provide access to multiple sources of sensor information, including databases and streaming data, with flexible semantic configuration of the API and domain mapping.	application programming interface;database;domain model;domain-driven design;environmental resource management;experiment;frequency analysis;geographic information system;graphical user interface;information management;linked data;mashup (web application hybrid);ontology (information science);prototype;rapid application development;reconfigurable computing;resource description framework;sparql;semantic web;semantic data model;semantic integration;sensor;separation of concerns;server (computing);software deployment;state (computer science);streaming media;web application;web service	Kevin R. Page;Alex Frazer;Bart J. Nagel;David De Roure;Kirk Martinez	2011	2011 IEEE Fifth International Conference on Semantic Computing	10.1109/ICSC.2011.86	web service;sensor web;data modeling;computer science;ontology;artificial intelligence;web api;rdf;data mining;database;semantics;world wide web;encoding;mashup	Mobile	-39.22028047280654	1.0919974420207808	99861
8d39ebda24c766feff99bd1acdaf2ff26829a896	why-diff: explaining differences amongst similar workflow runs by exploiting scientific metadata	why-diff;big data;metadata;reproducible research;escience central;workflow;provenance	Majority of workflows executed nowadays need to process a massive amount of data. Re-execution of such dataintensive scientific workflows often results in different outputs. Scientific research progresses when discoveries are reproduced and verified. However, simply re-enacting a scientific computation, such as a workflow, does not guarantee the correctness of results because of unintentional changes that may have interfered with the re-enactment process. We investigate the hypothesis that the metadata of a workflow execution can be used to explain why the experimenter observes different results (cause analysis). Similarly, Scientific metadata can be used to determine the impact of intentional variations that the experimenter may have injected into a new version of the workflow. We explore these two complementary cases using a simple algorithm for traversing two metadata traces in lock-step mode, which we illustrate through two human genomics data analysis workflows.	algorithm;autoregressive integrated moving average;computation;computational science;correctness (computer science);data-intensive computing;entity;experiment;management system;tracing (software)	Priyaa Thavasimani;Jacek Cala;Paolo Missier	2017	2017 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2017.8258275	data mining;metadata;computer science;scientific method;big data;correctness;workflow	HPC	-40.09652550262369	0.09003363147262469	100121
6f813709b17b7ad4028c37bcecfbfa03d48bfe1f	the introduction of the oscar database api (oda)	spatial databases algorithms open systems laboratories high performance computing environmental management innovation management project management information retrieval image databases;project management;open clusters;image databases;learning curve;high performance computing;information retrieval;innovation management;spatial databases;high performance computer;algorithms;it management;open systems;environmental management	"""The OSCAR [14] cluster installation toolkit was created by the Open Cluster Group (OCG) for one particular type of High Performance Computing (HPC) cluster. OSCAR is currently one of the widely used cluster installation toolkits; it boasts hundreds of thousands of downloads and active mailing lists. OSCAR has expanded its area with several sub-projects targeting other types of HPC clusters. Each of these projects share a core set of OSCAR code, including the OSCAR Database and its access API, """"ODA"""" (OSCAR Database API). The ODA abstraction layer, consisting of a database schema and corresponding API, hides a commodity back-end database (e.g., MySQL [15]). Because OSCAR and its sub-projects are targeted at new, innovative environments (including non-HPC environments), there are significant issues with managing various configurations of each project. For example, as we previously showed [8], “previous versions of ODA were unable to represent the complex, ever-growing set of data required to accurately describe the clusters that it manages. Further, its API was extremely complex, requiring a steep learning curve for OSCAR developers”. Therefore, we have designed and implemented a new database schema to deal with these issues. This new version of ODA has not only resolved the above problems but also, as proposed in our previous paper, enabled storage and retrieval of various configuration information, and encouraged data re-use between the main OSCAR project and its derivative projects. In addition, the new version of ODA has sped up the OSCAR installation process. This document presents a simpler, highly flexible design and implementation of ODA slated to be included in OSCAR v5.0. It also suggests a blueprint for maintaining the database modules of ODA in a systematic, organized way."""	acid;abstraction layer;application programming interface;blueprint;care-of address;code;data integrity;database engine;database schema;diagram;entity–relationship model;innodb;list of toolkits;mysql;oscar;package manager;perl;postgresql;table (database);the open group	DongInn Kim;Jeffrey M. Squyres;Andrew Lumsdaine	2006	20th International Symposium on High-Performance Computing in an Advanced Collaborative Environment (HPCS'06)	10.1109/HPCS.2006.44	project management;parallel computing;open cluster;innovation management;computer science;operating system;data mining;database;distributed computing;open system;learning curve;world wide web	DB	-42.396812734603	-2.9399392775264537	100122
e76dee120d5da07e43313a355c95a0b9f4793700	advances and challenges in log analysis	management system;log analysis	Logs contain a wealth of information to help manage systems.	log analysis	Adam J. Oliner;Archana Ganapathi;Wei Xu	2012	Commun. ACM	10.1145/2076450.2076466	computer science;management system	OS	-40.552564020630165	-6.1498130154185775	100483
ae9e568b84eacb906188fc114f9ade654acd0a37	managing large datasets with irods—a performance analysis	database management systems;distributed databases;electronic data interchange;grid computing;data access;data processing;data storage;data transfer;distributed data;grid data management system;integrated rule orientated data system;large dataset management;metadata transfer;performance analysis;rule engine;stress test	The integrated Rule Orientated Data System (iRODS) is a Grid data management system that organizes distributed data and their metadata. A Rule Engine allows a flexible definition of data storage, data access and data processing. This paper presents scenarios implemented in a benchmark tool to measure the performance of an iRODS environment as well as results of measurements with large datasets. The scenarios concentrate on data transfers, metadata transfers and stress tests. The user has the possibility to influence the scenarios to adapt them to his own use case. The results show the possibility to find bottlenecks and potential to optimize the settings of an iRODS environment.	benchmark (computing);business rules engine;computer data storage;data access;data hub;data system;microservices;profiling (computer programming);server (computing);simulation;stress testing (software)	Denis Hünich;Ralph Müller-Pfefferkorn	2010	Proceedings of the International Multiconference on Computer Science and Information Technology		use case;data access;data processing;computer science;operating system;electronic data interchange;computer data storage;data mining;database;stress;writing;world wide web;bandwidth;stress testing;grid computing;server;time	HPC	-34.26306550329215	-0.3312919981975619	100914
9bb46a0c77035e0270e470eaf4e2dff00c016095	suregen-2: a shell system for the generation of clinical documents		Suregen-2 applications are intended for use as add-on modules for clinical information systems. Currently, Suregen-2 permits refinement of the predefined medical ontology, specification of text plans and description knowledge for objects of the ontology. It has built-in constructs for referential expressions, aggregation, enumeration and recurrent semantic constellations. A first application built with Suregen-2, which currently supports German only, is in routine use.	add-ons for firefox;canonical account;information system;refinement (computing)	Dirk Kraus	2003			natural language processing;computer science;theoretical computer science;data mining;database	Web+IR	-39.6495668534971	3.804264660298192	101083
16646a9bb2e75b988df01616b26587fb14c77c02	integrating data from disparate sources: a mass collaboration approach	distributed data;collaborative work;data integrity;query processing;user participation;mobs project;real estate market;mediated schema;collaboration;user interfaces distributed databases query processing;data engineering;collaborative tools;user participation data integration systems disparate sources mass collaboration approach distributed data query interface mediated schema mobs project integration tools;semantic mapping;large scale integration;data integration systems;monitoring;sprites computer;distributed databases;integration tools;world wide web;disparate sources;user interfaces;collaboration sprites computer costs recruitment world wide web monitoring large scale integration collaborative tools collaborative work data engineering;query interface;mass collaboration approach;recruitment	The rapid growth of distributed data at enterprises and on the WWW has fueled significant interest in building data integration systems. Such a system provides users with a uniform query interface (called mediated schema) to a multitude of data sources, thus freeing them from manually querying each individual source. To address some problems in the MOBS (Mass Collaboration to Build Systems) project at the University of Illinois, we develop solutions that learn from the multitude of users in the integration environment to improve the accuracy of integration tools. The improved accuracy in turn can significantly reduce the workload of the system builder. In developing MOBS we address the following key challenges: (i) obtaining user participation, (ii) learning from user participation, and (iii) combining user answers.	mass collaboration;www	Robert McCann;Alexander Kramnik;Warren Shen;Vanitha Varadarajan;Olu Sobulo;AnHai Doan	2005	21st International Conference on Data Engineering (ICDE'05)	10.1109/ICDE.2005.81	information engineering;computer science;data integrity;data mining;database;user interface;world wide web;distributed database;collaboration	DB	-39.19536696080635	-1.1310753893254593	101135
4bcf99e56468730cfa7421dcaa5cc81c1a346027	modeling quantified things using a multi-agent system	databases;internet of things quantified things multi agent system quantified self;multi agent system;neural networks;multi agent systems cloud computing database management systems internet of things modelling;temperature sensors;quantified things;internet of things;internet;monitoring;quantified self;adaptation models;temperature sensors databases adaptation models internet monitoring neural networks;agent based model quantified things modelling qt modelling multiagent system internet of things iot cloud database	"""The Internet of Things (IoT) aims at empowering anything to connect to the Internet and to collect data about itself and the environment in which it is situated. Therefore, the IoT has been producing a wide range of data. Our aim is to deliver a way of incorporating the small data generated by each thing into larger datasets to get more meaning out of these data. A thing, equipped with sensors, can be monitored and can record specific features regarding its behavior. Then it can insert data into a cloud database in order to provide individual-and collective-level analyses. We call this process """"Quantified Things,"""" and we present it as a new area for the application of """"Quantified-Self"""" and """"Quantified-Community"""" that are approaches of the IoT. In this paper, we provide an overview of the Quantified Things (QT) concept and the key requirements for the creation of QT applications. In addition, we present an agent-based model as a solution to meet such requirements. To illustrate the use of this model, we derived an example from one of the Quantified Things applications: """"Quantified Fruit."""" Using the """"Quantified Fruit"""" concept, some fruit storage use sensors to monitor environmental conditions, such as temperature, relative humidity, lighting and some gases that may affect fruit ripening. In turn, they insert these collected data into a cloud database and consequently enable knowledge sharing. Through these collective experiences, fruit storage may provide an advanced informative perspective on fruit shelf life based on local environmental conditions."""	agent-based model;algorithm;artificial neural network;cloud database;experience;experiment;information;internet of things;machine learning;mega man zx;multi-agent system;pattern recognition;population;quantified self;requirement;sensor;situated;supervised learning;true quantified boolean formula	Nathalia Moraes do Nascimento;Carlos José Pereira de Lucena;Hugo Fuks	2015	2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)	10.1109/WI-IAT.2015.139	the internet;web of things;computer science;artificial intelligence;multi-agent system;data mining;quantified self;internet privacy;world wide web;internet of things;artificial neural network	Robotics	-37.73844955676213	-9.004158378894264	101254
9109f3f12f12b2d3fad9f0f9480eb96cb418c641	query reformulation by leveraging crowd wisdom for scenario-based software search	query reformulation;crowd wisdom;software retrieval	The Internet-scale open source software (OSS) production in various communities are generating abundant reusable resources for software developers. However, how to retrieve and reuse the desired and mature software from huge amounts of candidates is a great challenge: there are usually big gaps between the user application contexts (that often used as queries) and the OSS key words (that often used to match the queries). In this paper, we define the scenario-based query problem for OSS retrieval, and then we propose a novel approach to reformulate the raw query by leveraging the crowd wisdom from millions of developers to improve the retrieval results. We build a software-specific domain lexical database based on the knowledge in open source communities, by which we can expand and optimize the input queries. The experiment results show that, our approach can reformulate the initial query effectively and outperforms other existing search engines significantly at finding mature software.	lexical database;open sound system;open-source software;software developer;web search engine;wisdom of the crowd	Zhixing Li;Tao Wang;Yonghui Zhang;Yun Zhan;Gang Yin	2016		10.1145/2993717.2993723	query expansion;computer science;data mining;database;world wide web;query language	SE	-37.34590987988331	2.8845846887567883	101398
75be74a46c4e3b3823e4be841dcc3ce258fbf81e	semantic integration of geospatial concepts - a study on land use land cover classification systems	dissertation;geography		semantic integration	Hua Wei	2011				ML	-38.3879165543593	-3.589423517012196	101630
5ef4d66bbd9e00901a9e2636dde8e514f00c120a	development and application of global databases: considerable progress, but more collaboration needed	satellite data;bottom up;personal computer;top down;public domain;data center;compact disk;global change	This paper reviews the development of international collaboration in the availability of global data and processing abilities in the environmental field, beginning in 1957 with the pioneering activities of the World Data Centres. Considerable progress has been made, particularly through the availability of satellite data, but there are considerable shortcomings in these achievements, arisingfromthe isolatednature ofmanydevelopments, the difficulties ofintegrating different sources and weaknesses in the top-down approach. There are now attempts to developa complementary bottom-up approach, notably through the Global Change Diskette Project. Developments in hardware, such as the widespread availability of compatible personal computers operating on MS-DOS and UNIX systems; and easier storage on compact disks, and in software, through access to low-costand public-domain softwaresuch as GRASS and IDRISI, are helping,but improvedcollaboration is needed, in whichexistinginstitutionssuchas the World Data Centers have a major part to play.	dos;database;floppy disk;global change;ms-dos;personal computer;terrset geospatial monitoring and modeling software;top-down and bottom-up design;unix	David A. Hastings;John J. Kineman;David M. Clark	1991	International Journal of Geographical Information Science	10.1080/02693799108927837	simulation;geography;computer science;top-down and bottom-up design;data mining;database;operations research;cartography	DB	-41.82432944672843	-3.717661351079527	101632
f65c2a9a0763c16e65ad67bc476c47bb168bfe40	multi-scale analysis of the web of data: a challenge to the complex system's community	web of data;linked data;network analysis;paradigm shift;complex system;grouped data;semantic web;pure data;multi scale analysis	The Web of Data (WoD) is an Internet-based network of data resources and their relations. It has recently taken flight and combines over a hundred interlinked data sources with more than 15 billion edges. A consequence of this recent success is that a paradigm shift has taken place: up to now the Web of Data could be studied, searched and maintained like a classical database; nowadays it has turned into a Complex System and needs to be studied as such. In this paper, we introduce the Web of Data as a challenging object of study and provide initial results on two network scales: the pure data-layer, and the global connection between groups data items. In this analysis, we show that the “official” abstract representation of the WoD does not fit the real distribution we derive from the lower scale. As interesting as these results are, bigger challenges for analysis await in the form of the highly dynamic character of the WoD, and the typed, and implicit, character of the edges which is, to the best of our knowledge, hitherto unstudied.	complex system;internet;programming paradigm;pure data;semantic network;social network;www;world wide web	Christophe Guéret;Shenghui Wang;Paul T. Groth;Stefan Schlobach	2011	Advances in Complex Systems	10.1142/S0219525911003153	complex systems;data web;computer science;artificial intelligence;machine learning;semantic web;data mining;database;world wide web;statistics	Web+IR	-45.497807191399325	2.4555627705183936	102231
e8516532341ec6eae57949ffb17e31b08296adec	telematic problems of unmanned vehicles positioning at container terminals and warehouses	automatic identification system;unmanned vehicles;container terminal;position control;transport process	This paper describes the issues of transshipment container terminals operations, in the light of the development of this kind of transport. An increase in handling requires an expansion of stacking yard and automation of handling and transport processes. The development in this area first and foremost depends on modern handling technologies and automatic identification systems. AGV trucks play a key role in in those systems. The role of universities is to promote innovative technologies. Paper [2] contains the status of intermodal terminals development in Poland, which was awarded the prize of the Minister of Infrastructure of Poland in the field of organization and management. The paper contains a detailed description of the principles of positioning, control and propulsion of AGV vehicles. The content was developed to make it understandable to logisticians responsible for the implementation question in Poland.	global positioning system;telematics;unmanned aerial vehicle	Stanislaw Kwasniowski;Mateusz Zajac;Pawel Zajac	2010		10.1007/978-3-642-16472-9_43	embedded system;engineering;automotive engineering;transport engineering	Robotics	-41.52224216650316	-9.539149211457257	102386
45ec14eaa22681126582a9e57a3b7c4d3d1f708a	adaptive query processing: why, how, when, what next	web;information access;integration;search;adaptive query processing;deep web	Adaptive query processing has been the subject of a great deal of recent work, particularly in emerging data management environments such as data integration and data streams. We provide an overview of the work in this area, identifying its common themes, laying out the space of query plans, and discussing open research problems. We discuss why adaptive query processing is needed, how it is being implemented, where it is most appropriately used, and finally, what next, i.e., open research problems.	database;norm (social);open research;query optimization;theme (computing)	Amol Deshpande;Joseph M. Hellerstein;Vijayshankar Raman	2006		10.1145/1142473.1142603	sargable;query optimization;query expansion;web query classification;computer science;query by example;data mining;database;rdf query language;web search query;world wide web;deep web;query language	DB	-34.09853516616118	2.3617639699401494	102539
845fc06f38172c601db0c5c4b338307afb95181f	dealing with metadata quality: the legacy of digital library efforts	digital libraries;data infrastructures;metadata quality frameworks;quality	In this work, we elaborate on the meaning of metadata quality by surveying efforts and experiences matured in the digital library domain. In particular, an overview of the frameworks developed to characterize such a multi-faceted concept is presented. Moreover, the most common quality-related problems affecting metadata both during the creation and the aggregation phase are discussed together with the approaches, technologies and tools developed to mitigate them. This survey on digital library developments is expected to contribute to the ongoing discussion on data and metadata quality occurring in the emerging yet more general framework of data infrastructures.	digital library	Alice Tani;Leonardo Candela;Donatella Castelli	2013	Inf. Process. Manage.	10.1016/j.ipm.2013.05.003	digital library;computer science;data mining;database;world wide web;meta data services	DB	-45.912047196676234	2.791343849355218	102758
375af0400192d30783debde2f2e057d247a7ef4b	big iot data analytics: architecture, opportunities, and open research challenges	distributed computing;internet of things;big data;data analytics;smart city	Voluminous amounts of data have been produced, since the past decade as the miniaturization of Internet of things (IoT) devices increases. However, such data are not useful without analytic power. Numerous big data, IoT, and analytics solutions have enabled people to obtain valuable insight into large data generated by IoT devices. However, these solutions are still in their infancy, and the domain lacks a comprehensive survey. This paper investigates the state-of-the-art research efforts directed toward big IoT data analytics. The relationship between big data analytics and IoT is explained. Moreover, this paper adds value by proposing a new architecture for big IoT data analytics. Furthermore, big IoT data analytic types, methods, and technologies for big data mining are discussed. Numerous notable use cases are also presented. Several opportunities brought by data analytics in IoT paradigm are then discussed. Finally, open research challenges, such as privacy, big data mining, visualization, and integration, are presented as future research directions.	big data;data mining;internet of things;open research;programming paradigm	Fariza Hanum Nasaruddin;Abdullah Gani;Ahmad Karim;Ibrahim Abaker Targio Hashem;Aisha Siddiqa;Ibrar Yaqoob	2017	IEEE Access	10.1109/ACCESS.2017.2689040	analytics;big data;computer science;data science;data mining;data analysis;world wide web;internet of things	DB	-37.18302718309708	-6.463785412092678	102931
5e693bb1f021e94c97085232d177149abe67f2a9	geographic information systems in the u.s.: an overview	geographic information system;spatial relationships	A Geographic Information System (GIS) can be defined as one which is oriented to supplying information pertaining to the geography or spatial relationships of the information in the system.	automated planning and scheduling;geographic information system;online and offline	Robert Amsterdam;Edward Andresen;Harry Lipton	1971		10.1145/1478873.1478942	local information systems;enterprise gis;geoportal;geography;knowledge management;geospatial analysis;gis and public health;data mining;geographic information system	DB	-38.09846609558375	-3.4287124566670446	102941
fcf91846ca9485ecd6e5aa4101a9f867b11a6dee	practice of web data mining methods application	zinātniskās publikācijas;rīgas tehniskā universitāte;izdevums rtu zinātniskie raksti;rtu;riga technical university;web data mining;scientific publications;scientific journal of rtu		data mining;web application	Pavel Osipov;Arkady Borisov	2009	J. Riga Technical University	10.2478/v10143-010-0014-x	remote terminal unit;computer science;engineering;data science;data mining;control theory;world wide web;remote sensing	ML	-39.661020284414946	-6.490462441359509	103114
05a6b35326d239ebeeb4f66b96a212f3e39811b6	cyber-enabled simulations in nanoscale science and engineering	portals;cyber infrastructure;cyber enabled simulations;computational nanotechnology;modeling and simulation;special issues and sections;computer aided instruction;simulation;nanobioscience computational modeling predictive models nanostructured materials biological system modeling electrons biological materials nanotechnology physics computing data engineering;nanotechnology;multiscale modeling;us national science foundation network;web portal;nanoscience;nanoscale science engineering;next generation web portal;nanoscale devices;engineering education;interactive simulation;next generation;next generation web portal cyber enabled simulations nanoscale science engineering atomic level modeling molecular level modeling nanoscale materials simulation us national science foundation network computational nanotechnology instructional materials education community;atomic level modeling;natural sciences computing;multiscale modeling nanoscience nanotechnology modeling simulation cyber infrastructure;simulation tool;modeling;molecular level modeling;education community;digital simulation;nanoscale materials simulation;portals computer aided instruction digital simulation engineering education nanotechnology natural sciences computing;instructional materials	The article describes recent progress in atomic and molecular level modeling and simulation of nanoscale materials and processes, as well as efforts by the US National Science Foundation's Network for Computational Nanotechnology (NCN) to cyber-enable such simulation tools together with instructional materials and research seminars. We believe that making advanced simulation tools widely and easily available to the research and education community will significantly enhance the impact of modeling and simulation on nanoscience and nanotechnology To materialize this vision, NCN established nanoHUB.org, a next-generation Web portal or science gateway that lets users run live, interactive simulations, explore data, and learn-all though a simple Web browser without installing any software or providing compute cycles.	computer simulation	Alejandro Strachan;Gerhard Klimeck;Mark S. Lundstrom	2010	Computing in Science and Engineering	10.1109/MCSE.2010.38	computational science;simulation;systems modeling;engineering education;computer science;modeling and simulation;multiscale modeling	DB	-45.78749766054535	-1.3126493897628728	103520
707291df20ac35b6f7f4969cfef3b1b88388c3bf	dyonipos - redesigned knowledge management	knowledge management	Traditional knowledge management is often combined with extra work to collect the information again which is already electronically available. Another obstacle to be overcome is to make the content of the collected information easy accessible but. At present conventional searching tools provide only documents and not the meaning of the content. They are often based on the search after character strings, deliver many unnecessary hits and no or less context information. DYONIPOS offers a new way. The research project DYONIPOS focuses on detecting the knowledge needs of knowledge workers and automatically providing the required knowledge just in time, while avoiding additional work and violations of the knowledge worker’s privacy. This knowledge is made available through semantic linkage of the relevant information out of existing artifacts.	handling (psychology);just-in-time compilation;knowledge management;knowledge organization;linkage (software);morphologic artifacts;providing (action);sensor;genetic linkage;responsibility	Josef Makolm;Silke Weiß;Doris Ipsmiller	2009			computer science;lubrication;data mining;pinion;sump;clutch;bearing (mechanical);axle;retainer;conical surface;mechanical engineering	HCI	-45.07850124133631	1.469206588895033	103580
57d3f122e81592be7ef8a2dfa9a3e763ac660974	using linked data for prosopographical research of historical persons: case u.s. congress legislators		This paper shows how biographical registries can be represented as Linked Data, enriched by data linking to related data sources, and used in Digital Humanities. As a use case, a database of 11 987 historical U.S. Congress Legislators in 1789–2018 was transformed into a knowledge graph. The data was published as a Linked Data service, including a SPARQL endpoint, on top of which tools for biographical and prosopographical research are implemented. A faceted browser named U.S. Congress Prosopographer with visualization tools for knowledge discovery is presented to provide new insights in political history.	communication endpoint;digital humanities;faceted classification;knowledge graph;linked data page;sparql	Goki Miyakita;Petri Leskinen;Eero Hyvönen	2018		10.1007/978-3-030-01765-1_18	knowledge extraction;prosopography;data science;visualization;sparql;linked data;digital humanities;biography;political history;computer science	ML	-42.78156600391354	0.850327736261507	103700
9c4979e9a3332f94c0f8ea8624f6afac90ab4713	evaluation of data mining techniques for two child-related social problems			data mining	James Little;Hayder Abdulabbas Waheed;Andy Rixon	2017			data mining;social issues;computer science	ML	-39.347225284544095	-7.533693588079065	103857
636afa049d40bc522cb4e2564b8a03381d708180	big graph visual analytics		This special issue of Information Visualization explores the technical challenges and technology development opportunities of graph visual analytics arising from the trend of big data. Big graph visual analytics is about applying visualization and analytics techniques to gather, analyze, and understand big graphs and the knowledge behind them.	visual analytics	David J. Haglin;David Trimm;Pak Chung Wong	2017	Information Visualization	10.1177/1473871616679013	artificial intelligence;data science;computer vision;visual analytics;big data;visualization;computer science;graph;analytics;information visualization	Visualization	-38.000622820276	-7.215808773310379	103919
d691eaf04bf47322b2517c4dcf87926500218eea	key issues and theoretical framework on moving objects data mining	moving object;theoretical framework;semantic representation;information aggregation;topological complexity;theory and method;heterogeneous data;spatio temporal data mining;space time;data mining;data model;spatio temporal data model;spatio temporal data;association rule;concept lattice;natural language;high dimensional data;moving objects;ontology;concept space	"""Considering technical difficulties and bottlenecks in moving objects data mining, such as massive movement data, high dimensional data, topological complexity, and knowledge semantic representation etc., this paper focuses on the study of theory and methods of moving objects data mining. First, it presents two key scientific issues of the research topic, i.e. integration and modeling of heterogeneous data, and information aggregation and interpretation. Second, a theoretical framework of moving object data mining is proposed based on different perspectives of """"space-time data→space-time concept→space-time pattern"""". Two aspects of the framework are then discussed in details, including (1) moving objects data modeling and semantic expression; (2) mining methods and algorithms of association rules based on concept lattice. Finally, future works are discussed."""	algorithm;association rule learning;cluster analysis;data mining;data model;data modeling;formal concept analysis;interpretation (logic);spatiotemporal database;unified framework	Rong Xie;Xin Luo	2010		10.1007/978-3-642-17316-5_55	association rule learning;data model;computer science;data science;theoretical computer science;space time;ontology;data mining;natural language;clustering high-dimensional data	ML	-34.92460073957316	-3.963750044163168	104218
4a4f28090590eed83433d208d38678c04f341599	report of the international workshop on business intelligence and the web: beweb 2011	software;information systems;international workshop on business intelligence and the web;info eu repo semantics article;temporal information;keyword search;beweb;temporal search engine	The 2nd International Workshop on Business intelligencE and the WEB (BEWEB) was co-located with the EDBT/ICDT 2011 Joint Conference in Uppsala (Sweden) on March 25, 2011. BEWEB intends to be an international forum for researchers and practitioners to exchange ideas on how to leverage the huge amount of data that is available on the Web in BI applications and on how to apply Web engineering methods and techniques to the design of BI applications. This report summarizes the 2011 edition of BEWEB.	web engineering;world wide web	Jose-Norberto Mazón;Irene Garrigós;Florian Daniel;Malú Castellanos	2012	SIGMOD Record	10.1145/2380776.2380789	web modeling;web design;computer science;data science;data mining;database;web intelligence;world wide web;information system	DB	-45.482028721235864	0.5009811156173372	104315
d60222bc25443e74ccaecbe6dd7b6fdcb413e1d8	web stream processing workshop chairs' welcome & organization		Applications in different domains require reactive processing of massive, dynamically generated streams of data. This trend is increasingly visible also on the Web, where more and more streaming sources are becoming available. These originate from social networks, sensor networks, the Internet of Things (IoT) and many other technologies that use the Web as a platform for sharing data. This has resulted in new Web-centric efforts such as the Web of Things (WoT), which focuses on exposing and describing the IoT resources on the Web; or the Social Web which provides protocols, vocabularies, and APIs to facilitate access to social communications and interactions on the Web.	stream processing	Payam M. Barnaghi;Jean-Paul Calbimonte;Daniele Dell'Aglio	2018		10.1145/3184558.3192328	streams;world wide web;stream processing;wireless sensor network;web of things;social network;social web;computer science;internet of things	Visualization	-35.928761565474865	-6.2801160035686125	104521
f8b65715d62c3d5d7d291b4d750c93b562c387b7	time-scales, meaning, and availability of information in a global brain	time scale;internet architecture;artificial intelligent;semantic web;computers and society	We note the importance of time-scales, meaning, and availability of information for the emergence of novel information meta-structures at a global scale. We discuss previous work in this area and develop future perspectives. We focus on the transmission of scientific articles and the integration of traditional conferences with their virtual extensions on the Internet, their time-scales, and availability. We mention the Semantic Web as an effort for integrating meaningful information.	computation;computer;emergence;global brain;humans;information processing;internet;open-source software;scientific literature;semantic web	Carlos Gershenson;Gottfried Mayer-Kress;Atin Das;Pritha Das;Matus Marko	2003	CoRR		computer science;knowledge management;artificial intelligence;semantic web;data mining;world wide web	DB	-46.7111421661648	2.266857975362829	104684
197fd4520469cc139cfef5b2f2e826b2b7e64a62	kes: the knowledge explorer system		Generally, to properly perform information extraction from multimedia content, it is necessary to not only understand the media but also how they correlate with each other in time and space to compose the multimedia data. After extracting this information, it is necessary to structure and align it with ontologies and RDF repositories such as dbpedia.org to enhance quality of question answering and information retrieval. The main goal of this demo is to present a system named KES, capable of handling this scenario where a hybrid knowledge representation comes in handy. KES supports exploring and curating such representations stored in graph databases.	align (company);dbpedia;graph database;handy board;information extraction;information retrieval;knowledge representation and reasoning;ontology (information science);question answering	Márcio Ferreira Moreno;Rodrigo C. M. Santos;Wallas H. S. dos Santos;Renato Cerqueira	2018				AI	-39.54158482997057	3.368421122138411	104716
b89252a48125e87b6971e23ea98e6d4e6ff6331e	design/cpn - a computer tool for coloured petri nets	coloured petri net;state space;industrial application;validation and verification	In this paper, we describe the computer tool Design/CPN supporting editing, simulation, and state space analysis of Coloured Petri Nets. So far, approximately 40 man-years have been invested in the development of Design/CPN. It is used world-wide by more than 200 companies and research institutions. For the presentation, we draw from the experiences gained in a recent industrial application using Coloured Petri Nets in the design, validation, and veri cation of communication protocols for audio/video systems.	abstract machine;algorithm;automatic programming;breakpoint;browsing;cp system;cp/m;code generation (compiler);coloured petri net;compiler;debugger;debugging;diagram;feedback;focus group;fractal dimension;functional programming;grammar checker;graphical user interface;graphics;guard (information security);hypertext;interactive storytelling;item unique identification;modeller;message sequence chart;networking hardware;norm (social);operational semantics;parsing expression grammar;pascal;relevance;shading;simulation;snapshot (computer storage);standard ml;state space;syntax error;the circle (file system);thinking outside the box;unique name assumption;window function;xfig	Søren Christensen;Jens Bæk Jørgensen;Lars Michael Kristensen	1997		10.1007/BFb0035390	verification and validation;computer science;state space	SE	-46.673025232899356	-2.9090908516530485	104830
c626b97a4bbab521e4db7fcd6cebcb23db51b385	enriching spatial olap with map generalization: a conceptual multidimensional model	data warehouse spatial olap model map generalization conceptual multidimensional model spatial decision support system;spatial data;pollution measurement;multidimensional systems power system modeling geometry data warehouses displays data mining conferences computer science informatics educational institutions;olap;spatial olap;data mining;argon;multidimensional model olap spatial olap map generalization;decision support systems cartography data mining data warehouses;marine vehicles;multidimensional model;model integration;spatial databases;decision support systems;conceptual multidimensional model;spatial olap model;cities and towns;cartography;data warehouses;data warehouse;map generalization;spatial decision support system;buildings;graphics;generalization capability	Map generalization is used to derive maps for secondary scales and/or specific goals. This operation greatly benefits spatial decision support systems as it can provide a global and simplified representation of a phenomenon discarding irrelevant information. The recent popularity of OLAP systems for various application domains has generated much interest for the development of spatial OLAP (SOLAP) models that integrate spatial data in data warehouse and OLAP systems. Although powerful under some respect, current SOLAP models cannot support map generalization capabilities. In this paper, we present a conceptual multidimensional model integrating map generalization. The model extends SOLAP spatial hierarchies introducing multi-association relationships, and supports imprecise measures.	cartographic generalization;map;online analytical processing;relevance;spatial analysis;spatial decision support system	Sandro Bimonte;Jérôme Gensel;Michela Bertolotto	2008	2008 IEEE International Conference on Data Mining Workshops	10.1109/ICDMW.2008.80	online analytical processing;computer science;graphics;data science;data warehouse;data mining;database;spatial analysis;argon;cartographic generalization	DB	-36.742826334530356	-1.8960468578972167	104890
17a2d8fa24f0eff6c4c1d212c48b8973190d89c2	typhoon insurance pricing with spatial decision support tools	knowledge based system;information systems;spatial data;risk analysis;integrable system;spatial variation;exploratory analysis;database management;spatial decision support;spatial correlation;information science library science;computer science;spatial statistics;spatial decision support system;geospatial data;physical;open source;geography	In disaster insurance and reinsurance, GIS has been used to visualize and manage geospatial data and to help vulnerability and risk analysis for years. However, hazard insurance is a multidisciplinary issue that involves complex factors and uncertainty. GIS, if used alone, has limited functionality due to poor incorporation of intelligence and spatial statistics. The Spatial Decision Support System (SDSS) presented in this paper, addresses some of the deficiencies of traditional GIS, by providing powerful tools to support disaster insurance pricing that involves procedural and declarative knowledge. In the SDSS, the knowledge-based system shell, using the open-source CLIPS and supporting fuzziness and uncertainty, can be applied in at least three phases: hazard simulation, fuzzy comprehensive evaluation of risk, and query for insurance pricing. The libraries of statistics and spatial statistics provide a robust support for analysis of spatial factors, including spatial correlation between zones vulnerable to hazard and spatial variation of exposures. The GIS components provide sophisticated visualization and database management support for geospatial data, helping easily locate the insured points and risk zones as well as exploratory analysis of spatial data. Standard database management interfaces are used to manage other aspatial data. COM, an industry-wide interface protocol, tightly integrates these technologies (the expert shell, GIS, spatial statistics and DBM within an integral system), and can be used to develop mixed complex algorithms in support of other COM objects. An application of typhoon insurance pricing is demonstrated with a case study in Guangdong, China. Developed as a suite of generic tools with abilities to deal with the complex problem of disaster insurance involving spatial factors and field knowledge, this prototype SDSS can also be applied to other disaster insurance and fields that involve similar spatial decision making.	algorithm;clips;database;dbm;geographic information system;hazard analysis;it risk management;knowledge-based systems;library (computing);open-source software;prototype;scientific visualization;simulation;spatial analysis;spatial decision support system;typhoon;vulnerability (computing)	Lianfa Li;Jinfeng Wang;Chengyi Wang	2005	International Journal of Geographical Information Science	10.1080/13658810412331317742	enterprise gis;geography;computer science;knowledge management;data science;geospatial analysis;knowledge-based systems;data mining;database;mathematics;spatial analysis;cartography;statistics	DB	-37.118426593101944	-2.7861264241363637	104992
7a06fa7d3625d79b4639d87b16ec6d6f5039f202	engaging librarians in the process of interlinking rdf resources		By publishing metadata as RDF and interlinking these resources with other RDF datasets on the Semantic Web, libraries have the potential to expose their collections to a larger audience, increase the use of their materials, and allow for more efficient user searches. Despite these benefits, there are many barriers to libraries fully participating in the Semantic Web. Increasing numbers of libraries are devoting valuable time and resources to publishing RDF datasets, yet little meaningful use is being made of them due to lack of interlinking. The goal of this research is to explore the barriers faced by librarians in participating in the Semantic Web with a particular focus on the process of interlinking. We will also explore how interlinking could be made more engaging for this domain.	librarian	Lucy McKenna	2017		10.1007/978-3-319-58451-5_16	linked data;rdf;information retrieval;world wide web;computer science;metadata;semantic web;publishing	HCI	-43.27580895065067	2.1487506947846375	105093
c87c4060c09565e5f1531c14f7a9f99860a0e6fb	message from the general chairs		We would like to welcome you to and thank you for attending The Fifth International Conference on Social Networks Analysis, Management, and Security (SNAMS-2018)) in Valencia, Spain from October 15-18, 2018.		Edson T. Midorikawa;Liria Matsumoto Sato	2004	2018 Fifth International Conference on Social Networks Analysis, Management and Security (SNAMS)	10.1109/LA-WEBMEDIA.2004.10005		Visualization	-46.261623383657835	-8.820643954639696	105689
3873b8bed276e6c8c77aa7f6edabd17300f12d6a	temporal data modeling for rfid data in the retail industry	temporal data modeling for rfid data in the retail industry;temporal data modeling;한국정보통신학회;vol 5 no 2;rfid;journal of information and communication convergence engineering 제5권 제2호;rfid applications;the korea institute of information and communication engineering;hong won yun;temporal query;jung hwa lee;epc	RFID applications are mostly associated with the timestamp when the events occur. For managing RFID data we need to design data modeling for RFID data to support monitoring and temporal queries. In this paper, we propose a temporal RFID data modeling to maintain the history of events and state changes and to monitor the states of RFID objects. This data modeling involves essential basic operations for RFID data to monitor the information of RFID objects and to support temporal queries. Filter operations can achieve better query performance and obtain efficient storage space usage. It is possible to adapt into different temporal business applications.	data modeling;radio-frequency identification	Hong-Won Yun;Jung-Hwa Lee	2007	J. Inform. and Commun. Convergence Engineering		computer science;data mining;database;computer security	DB	-38.792978721632004	-4.627246113941002	105827
69bb9b70bb78a3187b6b1873e4ce43b5acba65a1	big log analysis for e-learning ecosystem	e learning ecosystem;logging architecture;computer aided instruction;characteristics of e learning log data and log analysis;big log analysis logging architecture log service log computation log storage log transport log collection e learning log data life cycle comprehensive logging architecture low latency requirement cascading generation heterogeneous multisource multidimensional correlation e learning log characteristics robust scalable practical logging architecture e learning log analysis e learning ecosystem;characteristics of e learning log data and log analysis e learning ecosystem e learning log data logging architecture;electronic learning ecosystems computer architecture open source software servers real time systems;e learning log data	Recently, e-Learning emerges a rapid development, and e-Learning ecosystem has been proposed to provide sustained and stable services to cope with the growing demands of e-Learning. In e-Learning ecosystem, the amount of e-Learning log grows exponentially, introducing the variety and complexity of e-Learning log analysis. Therefore, a robust, scalable and practical logging architecture is urgently needed. Firstly, the characteristics of e-Learning log and its analysis are studied in this paper. Specifically, e-Learning log implies complicated characteristics, such as multi-dimensional correlation, heterogeneous multi-source, and cascading generation. Furthermore, the log analysis represents abundant diversity of demands, variety of methods and low-latency requirement in computation. Thereupon, this study presents a comprehensive logging architecture covering the whole life cycle of e-Learning log data which includes log collection, transport, storage, computation and service. To verify the proposed logging architecture, a related experimental implementation is developed for a realistic e-Learning ecosystem, and three typical e-Learning analyses are proposed.	computation;ecosystem;log analysis;multi-source;scalability	Qinghua Zheng;Huan He;Tian Ma;Ni Xue;Bing Li;Bo Dong	2014	2014 IEEE 11th International Conference on e-Business Engineering	10.1109/ICEBE.2014.51	computer science;data science;theoretical computer science;database;world wide web	HPC	-44.931419105459405	-0.9222401481048798	105913
8a26362185954f5881c83217152eac54103ab676	formalization of mining association rules based on relational database in eis		In this paper, we study the concrete signification of association rules in relational database, and propose a new formalization and a general process of mining association rules, which is comprehensive, and easy to use and understand. It lays a foundation of designing systems for mining association rules based on relational database, and is a direction for system designer.	association rule learning;relational database;systems design	Hong Zhang;Bo Zhang	2006		10.1007/0-387-34456-X_14	data science;data mining;database	DB	-34.41938570721616	-4.090806411422236	106013
7b15f05b6e4da01397790632af278bc3375c28a7	a multi-component indicator of stream condition for waterway managers: balancing scientific rigour with the need for utility	waterway management;victorian rivers;rivers in victoria;index of stream conditions isc;environmental management;phd thesis;australia	.................................................................................................................................................iii ACKNOWLEDGEMENTS .......................................................................................................................... v TABLE OF CONTENTS ............................................................................................................................vii LIST OF FIGURES.........................................................................................................................................xi LIST OF TABLES........................................................................................................................................xiv LIST OF SYMBOLS.................................................................................................................................xvii ABBREVIATIONS ..................................................................................................................................... xx PREFACE .................................................................................................................................................xxiii	vii	Anthony Richard Ladson	2003			environmental engineering;geography;environmental resource management;operations management	DB	-44.94871497557311	-4.8856285497758805	106015
9e7d2f77296d816f72a97d97e36159520c057beb	information security of phy layer in wireless networks		1Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai University, Shanghai 200444, China 2Key Laboratory of Wireless Sensor Network & Communication, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai 201899, China 3Shanghai Internet of Things Co., Ltd., Shanghai 201899, China 4Shanghai Research Center for Wireless Communications, Shanghai 200335, China 5College of Physics and Electronic Information Engineering, Qinghai University for Nationalities, Xining, Qinghai 810000, China		Weidong Fang;Fengrong Li;Yanzan Sun;Lianhai Shan;Shanji Chen;Chao Chen;Mei-Ju Lin	2016	J. Sensors	10.1155/2016/1230387	telecommunications;computer science;computer security;computer network	Mobile	-44.98823206739755	-7.7262873589841865	106169
d7aab37ed9c048dffc64f94661f25852b19dda32	ontomongo- ontology-based data access for nosql		Ontology-based data access (OBDA) has gained attention in recent years for providing access to large volumes of data by using ontologies as a conceptual layer and exploring their ability to describe domains and deal with data incompleteness. This is done through mappings that connect the data in the database to the vocabulary of the ontology. The first OBDA studies were about data stored in relational databases. Recent studies have begun to extend the use of OBDA to NoSQL databases. In this paper, we present a novel approach for OBDA with document-oriented NoSQL databases. Differently from related works, our approach uses an access interface with an intermediate conceptual layer that is extensible and flexible, capable of providing access to different types of database management systems. To validate the approach, we have implemented a prototype for MongoDB, using a real-world application domain as a case study.	application domain;data access;mongodb;nosql;ontology (information science);prototype;relational database;vocabulary	Thiago Henrique Dias Araujo;Barbara Tieko Agena;Kelly Rosa Braghetto;Renata Wassermann	2017			database;nosql;ontology;data access;computer science	DB	-34.931719458169624	2.709637125183187	106200
f3c8e0c6802ec444c08413abf66d00ce51f75b9b	winbasin: using improved algorithms and the gis technique for automated watershed modelling analysis from digital elevation models	fuzzy c means algorithm;ridge;areas;flow;headwater tracing algorithm hta;drainage networks;outlet tracing algorithm ota;digital elevation model;journal article;geomorphological instantaneous unit hydrograph;basin delineation;taiwan;hydrological response;direction;system;depression watershed method;information;extraction;geomorphological instantaneous unit hydrograph giuh;automated watershed analysis	WinBasin: Using improved algorithms and the GIS technique for automated watershed modelling analysis from digital elevation models W.‐T. Lin a , W.‐C. Chou b , C.‐Y. Lin c , P.‐H. Huang d & J.‐S. Tsai e a Department of Environment and Disaster Management , Ming Dao University , Changhua County 523, Taiwan b Department of Civil Engineering and Engineering Informatics , Chung Hua University , No. 707, Sec. 2, Wufu Road, Hsinchu City 300, Taiwan c Department of Soil and Water Conservation , National Chung Hsing University , Taichung City 402, Taiwan d Graduate Institute of Civil and Hydraulic Engineering , Feng Chia University , Taichung City 407, Taiwan e Department of Landscape Architecture , Chung Hua University , Hsinchu City 300, Taiwan Published online: 05 Mar 2008.	algorithm;digital elevation model;engineering informatics;geographic information system;watershed (image processing)	W.-T. Lin;W.-C. Chou;C.-Y. Lin;P.-H. Huang;J.-S. Tsai	2008	International Journal of Geographical Information Science	10.1080/13658810701300121	extraction;ridge;flow;digital elevation model;information;geography;computer science;system;area;remote sensing	SE	-44.86323658595715	-9.548831590712366	106294
e919db33a815204bd5c409bf8f2edc45a1e2d804	provenance for linked data		Assessing the quality of linked data currently published on the Web is a crucial need of various data-intensive applications. Extensive work on similar applications for relational data and queries has shown that data provenance can be used in order to compute trustworthiness, reputation and reliability of query results, based on the source data and query operators involved in their derivation. In particular, abstract provenance models can be employed to record information about source data and query operators during query evaluation, and later be used e.g., to assess trust for individual query results. In this paper, we investigate the extent to which relational provenance models can be leveraged for capturing the provenance of SPARQL queries over linked data, and identify their limitations. To overcome these limitations, we advocate the need for new provenance models that capture the full expressive power of SPARQL, and can be used to support assessment of various forms of data quality for linked data manipulated declaratively by such queries.		Gregory Karvounarakis;Irini Fundulaki;Vassilis Christophides	2013		10.1007/978-3-642-41660-6_19	query optimization;computer science;data mining;database;information retrieval	DB	-36.234564011987196	4.149601960132772	106323
47a04230b91d5fbdeef466c80f7bd0c2ab08dd84	uncertain data: representations, query processing, and applications		Uncertain data is common in many emerging applications. In this chap- ter, we start by surveying a few applications in sensor networks, ubiquitous com- puting, and scientific databases that require managing uncertain and probabilistic data. We then present two approaches to meeting this requirement. In the first approach, we propose a rich treatment of probability distributions in the system, in particular the SPO framework and the SP-algebra. In the second approach, we stay closer to a traditional DBMS, extended with tuple probabilities or attribute proba- bility distributions, and study the semantics and efficient processing of queries. 1 Probabilistic Databases and Their Applications There is a wide range of emerging applications that produce uncertain data and demand new techniques to manage such data. The mature, industry-standard relational database management systems have a history of about 40 years, but they do not have the capability of managing uncertain or probabilistic data. The appli- cations that are discussed in this chapter are mainly in the areas of sensor net- works, ubiquitous computing, bioinformatics and scientific databases. There are many applications (often related to the Internet) that also fall in this domain, such as information extraction and information integration. In this section, we present several applications where large collections of prob- abilistic data are acquired, stored, and used. We divide those applications into two categories: sensor networks and ubiquitous computing, and scientific databases.	database;uncertain data	Tingjian Ge;Alex Dekhtyar;Judy Goldsmith	2013		10.1007/978-3-642-37509-5_4	computer science;probabilistic database;data science;data mining;database	DB	-35.85378590595638	-1.2229848129666399	106376
ce68043282f3ec3ff20cefe55a9c0ccae73df81d	bankruptcy prediction using a data envelopment analysis	banking;rule induction;journal contribution;risk analysis;decision support system;machine learning;data envelopment analysis;decision support systems;linear program;bankruptcy prediction;data envelope analysis	Univ Limburg, Dept Appl Econ, B-3590 Diepenbeek, Belgium.Vanhoof, K, Univ Limburg, Dept Appl Econ, Univ Campus, B-3590 Diepenbeek, Belgium.	data envelopment analysis	Anja Cielen;Ludo Peeters;Koen Vanhoof	2004	European Journal of Operational Research	10.1016/S0377-2217(03)00186-3	decision support system;computer science;artificial intelligence;operations management;data envelopment analysis;mathematics;operations research	Robotics	-43.647571227570445	-9.303329281564066	106614
61613b279b92960385bf8e6ea3cbfed4ecc5b2fa	people detection based on spatial mapping of friendliness and floor boundary points for a mobile navigation robot		1 Toshiba Corporate Research & Development Center, 1, Komukai-Toshiba-cho, Saiwai-ku, Kawasaki 212-8582, Japan 2 Toshiba Corporation, Power Systems Company, 1-1, Shibaura 1-Chome, Minato-ku, Tokyo 105-8001, Japan 3 Department of Engineering Science and Mechanics, Shibaura Institute of Technology, 3-7-5, Toyosu, Koto-ku, Tokyo 135-8548, Japan 4 Graduate School of Informatics, Kyoto University, Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan	cluster analysis;color;experiment;odometry;omnidirectional camera;regular expression;robot;sensor	Tsuyoshi Tasaki;Fumio Ozaki;Nobuto Matsuhira;Tetsuya Ogata;Hiroshi G. Okuno	2011	J. Robotics	10.1155/2011/683975	computer vision;simulation	Robotics	-46.28886760344345	-9.206729847705999	106620
eebfee4783ef13de66cc65223162b84c6f673ca7	introduction to digital libraries	digital libraries	This tutorial is a thorough and deep introduction to the Digital Libraries (DL) field, providing a firm foundation: covering key concepts and terminology, as well as services, systems, technologies, methods, standards, projects, issues, and practices. It introduces and builds upon a firm theoretical foundation (starting with the `5S' set of intuitive aspects: Streams, Structures, Spaces, Scenarios, Societies), giving careful definitions and explanations of all the key parts of a `minimal digital library', and expanding from that basis to cover key DL issues. Illustrations come from a set of case studies. Attendees will be exposed to four Morgan and Claypool books that elaborate on 5S, published 2012-2014. Complementing the coverage of `5S' will be an overview of key aspects of the DELOS Reference Model and DL.org activities. Further, use of a Hadoop cluster supporting DLs will be described.		Edward A. Fox	2016		10.1145/2910896.2925429	digital library;computer science;world wide web	HCI	-45.60419151833723	1.8768937664698608	106745
3bd63238cb68d9f9c981d9ed1380114415c8d0dd	reminiscences on influential papers		This paper abstracts from the particular features of PRISMA/DB, and evaluates and analyzes the performance trade-offs for a wide range of parallel query processing strategies. Its clear style of presentation, along with careful attention to previous work both in its discussion as well as in the experiments and analysis, make this paper into a concise introductory or “refereshment” text for researchers interested in parallel query execution.	database;experiment	Kenneth A. Ross;Peter A. Boncz;Ihab F. Ilyas;Volker Markl;Vasilis Vassalos	2004	SIGMOD Record	10.1145/1041410.1041431		DB	-47.86733337522304	-2.2942123564577246	106864
a4298fc8263cd13d0430b3d634514983f1df5957	a solution for mining big data based on distributed data streams and its classifying algorithms		With the advent of the era of big data, a require to discover valuable knowledge from big data is being one of focuses. However, big data is a term which has been described with the features of alarming velocity, super volume and various data structures, and so how to express big data to effectively mine is becoming a key problem. Aiming at requires of big data analyses, this paper will construct the concept of DDS (Distributed Data Stream), and build the mining model and some key algorithms. The experiments show integrating these algorithms under our model can get higher mining accuracies to distributed data streams.		Guojun Mao;Jiewei Qiao	2017		10.1007/978-3-319-61845-6_27	data mining;data science;data stream;data stream mining;big data;computer science;data structure;algorithm	ML	-37.50951100347769	-5.423695675629769	106940
aaee76ec84b73882e126a8007ea09454e3871300	pattern mining over star schemas in the onto4ar framework	onto4ar framework;hidden information;itemsets;motion pictures;efficient algorithm;d2apriori;ontologies artificial intelligence data mining data warehouses;usa councils;data mining;ontologies artificial intelligence;pattern mining;data storage;data tables;data warehouses pattern mining star schemas onto4ar framework data tables data storage domain ontology d2apriori;ontologies;data warehouses;data warehouse;domain ontology;data mining ontologies association rules conferences multidimensional systems performance evaluation testing data warehouses data analysis;star schemas;context;knowledge based systems	Storing data according to the multidimensional model, in particular following star schemas, has demonstrated to be one of the most adequate forms to ease the exploration of data. However, this exploration has been limited to be query-based, leaving the discovery of hidden information to a second plan. The main reason for this, relates to the inability of traditional mining techniques to deal with several data tables at the same time. In this paper, we propose a new approach to mine patterns among data stored as a star schema, based in a domain driven framework, where available knowledge is represented in a domain ontology. Pattern mining is performed by an apriori-based algorithm - the D2Apriori, but more efficient algorithms are being implemented and tested, in order to solve performance issues related with the large amount of data stored in data warehouses.	a* search algorithm;apriori algorithm;data mining;ontology (information science);star schema	Cláudia Antunes	2009	2009 IEEE International Conference on Data Mining Workshops	10.1109/ICDMW.2009.68	computer science;ontology;data science;data warehouse;star schema;computer data storage;data mining;database	DB	-35.591675126679014	2.6919128331422675	107023
a707e32031acf4c844b2d80089c42001c8fed790	a standards-based semantic metadata repository to support ehr-driven phenotype authoring and execution	clinical phenotyping;quality data model qdm;metadata repository;hl7 fhir	This study describes our efforts in developing a standards-based semantic metadata repository for supporting electronic health record (EHR)-driven phenotype authoring and execution. Our system comprises three layers: 1) a semantic data element repository layer; 2) a semantic services layer; and 3) a phenotype application layer. In a prototype implementation, we developed the repository and services through integrating the data elements from both Quality Data Model (QDM) and HL7 Fast Healthcare Inteoroperability Resources (FHIR) models. We discuss the modeling challenges and the potential of our system to support EHR phenotype authoring and execution applications.	data element;data model;electronic health records;fast healthcare interoperability resources;health level 7;health level seven;metadata repository;prototype;spectrometry, mass, fast atom bombardment;anatomical layer;standards characteristics	Guoqian Jiang;Harold R. Solbrig;Richard C. Kiefer;Luke V. Rasmussen;Huan Mo;Peter Speltz;William K. Thompson;Joshua C. Denny;Christopher G. Chute;Jyotishman Pathak	2015		10.3233/978-1-61499-564-7-1098	computer science;data mining;database;world wide web;metadata repository	DB	-40.554499220422805	0.7087869696460474	107077
df69426a9b030cd5652f7751099d993c2612e8a1	leveraging semantic web technologies for standardized knowledge modeling and retrieval from the holy qur'an and religious texts	semantic web technology;emerging technology;holy qur an;religious learning;semantic web;ontology;knowledge modeling;knowledge base	In recent years, the desire to automate religious learning using the emerging technologies has led to a number of applications being introduced to ease the retrieval of knowledge from religious literature, especially the holy books. In this paper we investigate the need for developing knowledge-based, efficient and collaborative frameworks for retrieving knowledge from distributed knowledge sources, primarily related to the Holy Qur'an and associated scholarly texts (e.g. Books of Ahadith or Qur'anic Exegeses) to make the process of learning easy, comprehensive and effective. We propose a vision for leveraging the emergent role of Semantic Web technologies for providing efficient and flexible means of knowledge modeling, storage, publishing, reasoning and retrieval from distributed Qur'anic knowledge sources.	book;emergence;knowledge modeling;semantic web	Sumayya Baqai;Amna Basharat;Hira Khalid;Amna Hassan;Shehneela Zafar	2009		10.1145/1838002.1838050	knowledge integration;computer science;knowledge management;data science;social semantic web;data mining;semantic web stack	AI	-44.135039401135884	3.021349259659693	107132
c09de1d20082124668e4779b346dc6bd4e9e43b9	adaptive resource allocation scheme for micromotion feature extraction based on track-before-detect		College of Information Engineering, Engineering University of CAPF, Xi’an 710086, China Institute of Information and Navigation, Air Force Engineering University, Xi’an 710077, China Key Laboratory for Information Science of Electromagnetic Waves (Ministry of Education), Fudan University, Shanghai 200433, China Department of Electrical and Computer Engineering, National University of Singapore, Singapore 117583		Yijun Chen;Qun Zhang;Ying Luo;Tat Soon Yeo	2018	J. Sensors	10.1155/2018/5301253	computer vision;artificial intelligence;radar;feature extraction;track-before-detect;engineering;phased array;resource allocation	AI	-45.433670253094874	-8.73348376362623	107262
99038eb708a26c1075f21952dad009bf299f3116	global agricultural concept scheme: a hub for agricultural vocabularies		"""Thesauri are used to tag semi-structured documents, texts, while more complex semantic structures are used to describe (annotate) scientific data. We are creating a Global Agricultural Concept Scheme (GACS) by mapping AGROVOC, CABT and NALT – three major thesauri in the area of food and agriculture, with a beta release in May 2016. We see GACS as a hub linking user-oriented thesauri with semantically more precise domain ontologies linking, in turn, to datasets about food and agriculture, in order to make that data more interoperable and reusable Keywords—thesauri, ontologies, food, agriculture, GACS, AGROVOC, CABT, NALT, Crop Ontology I. GLOBAL AGRICULTURAL CONCEPT SCHEME The Food and Agricultural Organization of the United Nations (FAO), CAB International (CABI), and the National Agricultural Library of the USDA (NAL) have long maintained separate thesauri about agriculture, food and related topics -the AGROVOC Concept Scheme, CAB Thesaurus, and NAL Thesaurus – for use in indexing their respective bibliographic databases:: AGRIS (8 million records), CAB Abstracts (8.3), and Agricola (5.2). the AGROVOC Concept Scheme, CAB Thesaurus, and NAL Thesaurus. The thesauri provide globally identified concepts for use in automated indexing and retrieval, subject description, natural language processing, and translation. Having previously collaborated on mappings and common classifications, the three organizations resolved in 2013 to explore the feasibility of pooling their most frequently used concepts into a jointly maintained Global Agricultural Concept Scheme (GACS). GACS was seen as the first step towards improving the coherence and interoperability of agricultural data – a vision explored in a July 2015 workshop on “Agrisemantics”, with support from the Gates Foundation, elaborated in the Chania Declaration 5 of May 2016, and pursued by an Agrisemantics Working Group that is forming within the Research Data Alliance initiative. 1 http://aims.fao.org/agrovoc 2 http://www.cabi.org/cabthesaurus/ 3 http://agclass.nal.usda.gov/ 4 http://aims.fao.org/sites/default/files/Report_workshop_Agrisemantics.pdf 5 http://blog.agroknow.com/?p=5067 GACS Core Beta 3.1, soft-launched at the Open Harvest workshop of May 2016, provides 15,000 concepts formed by mapping and merging the most frequently used concepts from the three source thesauri. GACS Core concepts are labeled in multiple languages, with some in more than twenty-five languages. The soft launch opened a period of testing and feedback in preparation for the next phase of its development, which will begin in circa October 2016. GACS Core Beta 3.1 presents a set of concepts that is considered to be fairly stable, with URIs that are not expected to change (see an example of concept in GACS in Fig. 1). Problems resulting from the integration process, such as overlapping labels, have been substantially fixed, though much detailed work remains to be done, notably the specification of a common hierarchical structure. During this test phase, implementers are encouraged to use GACS on an experimental basis and provide feedback. Fig. 1 A concept in GACS In the next phase of development, the scope of GACS will be broadened beyond the core. Concepts from some of the source thesauri that were not included in GACS Core may be given an id.agrisemantics.org URI in a GACS Extension to be maintained by their original owners or, optionally, in collaboration. The notion of GACS Module anticipates a 6 http://agrisemantics.org/gacs longer-term need to devolve maintenance of distinct types of concepts, such as organisms or geographical names, to communities of experts. II. SEMANTIC ASSETS FOR FOOD AND AGRICULTURE Information relevant to food and agriculture encompasses data collected on factors ranging from yield and climate to demographics and markets., Information is presented in forms ranging from narrative texts (policy, technical, and scientific documents) through structured datasets (empirical data). Information may be graphically visualized, e.g., plotted onto timelines or maps, or plugged into models for nowcasting or for forecasting trends. All types of data, from the analytical to the empirical, are required for achieving sustainable food systems. Thesauri provide concepts for indicating the overall topic of information resources, usually semi-structured texts such as bibliographic abstracts, journal articles, but also videos and courseware. Empirical data is composed of data elements with precise definitions at defined levels of granularity. Datasets are typically serialized in formats specific to a particular software application, and their individual data elements are named within the context of that particular application. Interoperability across datasets is hampered by the sheer effort required to determine equivalences among differently named elements, then to extract sets of comparable elements from a diversity of applications and formats. Ontologies, focused set of related concepts specified with precise definitions and global identifiers, are increasingly used to “annotate” data. However, ontologies too may embody ad-hoc semantics in different degrees, and are usually totally disconnected from the world of thesauri, so preventing a seamless access to “hard” and soft data alike. III. LINKING THESAURI TO DATA VIA ONTOLOGIES The more fuzzily defined, globally identified concepts of general-purpose, search-oriented thesauri and concept schemes, such as GACS, may be mapped to the more precisely defined, globally identified, domain-specific, application-oriented ontologies and, from there, to locally defined data elements embedded in software-specific databases. An unbroken chain may be formed linking the most general concepts to the most specific data elements. Semantic authority control for data elements facilitates the re-use of datasets, and links from precise ontologies to search-oriented concepts facilitates the discovery of those datasets. One path to data interoperability is to use appropriately defined ontologies – i.e., ontologies that not only enable the extraction of data from a database (process often called “data annotation”), but that can also situate data within the appropriate """"context"""" -a modeled set of data about the time and place of its collection along with any additional elements required for its correct interpretation. Another path is to place those ontologies in a network with other semantic assets, including the thesauri and concept schemes used to express the “topicality” of information resources. Such an integration of semantic assets may support, for example, an analysis of the yield gap in sub-Saharan African countries by providing wellconnected data elements across a diversity of cropwheatrelated datasets from databases and repositories along with multi-media information, and relevant literature from main bibliographic databases like AGRIS, CABI and NAL with the goal of improving food security. The Agrisemantics vision points in two directions: on the one hand, to turn GACS into a more extensive network of thesauri and concept schemes to ensure the appropriate coverage for our domain of interest. In particular, we are going to test the notion of a GACS Extension on the example of AGROVOC. On the other hand, we aim at establishing tools and methodologies to connect GACS and its constellation of “extensions” to multiple domain-specific ontologies. The first ontology we will be working with is the Crop Ontology [1], which supports data comparison and interpretation at a higher granularity by providing a means for annotating data element with trait measurement method and unit or scale. (See Fig. 2) Fig. 2 Mapping from thesaurus to ontology More specifically, a wheat data element labeled with the code “GW” in a phenotype dataset can be mapped to the general concept """"grain weight"""" as defined, and given global identity (URI), in the CGIAR Crop Ontology. The CO term ‘Grain Weight’ can, in turn, be mapped to ‘Grain’ in AGROVOC and GACS. More information can then be discovered through a query system using this mapping that will return, aside from datasets related to grain weight, references to published papers where grain weight was studied."""	agricola;agris;agrovoc;bibliographic database;cab direct (database);circa;data element;declaration (computer programming);embedded system;feedback;global;gw-basic;general-purpose modeling;hoc (programming language);interoperability;map;natural language processing;network abstraction layer;norm (social);ontology (information science);scheme;seamless3d;semiconductor industry;serialization;software release life cycle;thesaurus (information retrieval);timeline;usb hub;uniform resource identifier;vocabulary;xfig	Caterina Caracciolo;Tom Baker;Elizabeth Arnaud	2016			discrete mathematics;management science;agriculture;mathematics	DB	-41.58323979914537	3.013954765812046	107322
dc47febe1a4be766057ccfb7e6e53d94078e79ef	business logic for geoprocessing of distributed geodata	distributed data;computadora;tratamiento datos;computers;modele numerique elevation;software;spatial information system;europa;allemagne;information systems;systeme information geographique;logiciel;case studies;vulnerability assessment;ordinateur;web processing service;hydrogeology;data processing;web based information system;europa central;traitement donnee;web service;hidrogeologia;web based geoprocessing;software engineering;distributive geodata;inventory;germany;web based information systems;planificacion;gestion recurso agua;internet;estudio caso;geographic information systems;agua subterranea;aquifer vulnerability;gestion ressource eau;europe centrale;etude cas;hydrogeologie;planning;digital elevation models;central europe;vulnerabilite nappe;aquifers;aquifere;information system;water resource management;planification;europe;geo web service;eau souterraine;service oriented architecture;vulnerabilidad acuifero;use case;ground water;alemania;systeme information;inventaire;inventario;environmental planning	This paper describes the development of a business-logic component for the geoprocessing of distributed geodata. The business logic acts as a mediator between the data and the user, therefore playing a central role in any spatial information system. The component is used in service-oriented architectures to foster the reuse of existing geodata inventories. Based on a geoscientific case study of groundwater vulnerability assessment and mapping, the demands for such architectures are identified with special regard to software engineering tasks. Methods are derived from the field of applied Geosciences (Hydrogeology), Geoinformatics, and Software Engineering. In addition to the development of a business logic component, a forthcoming Open Geospatial Consortium (OGC) specification is introduced: the OGC Web Processing Service (WPS) specification. A sample application is introduced to demonstrate the potential of WPS for future information systems. The sample application Geoservice Groundwater Vulnerability is described in detail to provide insight into the business logic component, and demonstrate how information can be generated out of distributed geodata. This has the potential to significantly accelerate the assessment and mapping of groundwater vulnerability. The presented concept is easily transferable to other geoscientific use cases dealing with distributed data inventories. Potential application fields include web-based geoinformation systems operating on distributed data (e.g. environmental planning systems, cadastral information systems, and others).	business logic;geographic information system;geoprocessing	Christian Kiehle	2006	Computers & Geosciences	10.1016/j.cageo.2006.04.002	data processing;geology;computer science;database;computer security;information system	Logic	-40.3404431598905	-2.7809417865324573	107331
292c1435afc24081f83f4ef88454240cb289cfd3	influence of gate oxides with high thermal conductivity on the failure distribution of ingaas-based mos stacks	tecnologia electronica telecomunicaciones;tecnologias;grupo a	a National Scientific and Technical Research Council (CONICET), Av. Rivadavia 1917, Buenos Aires, Argentina b Department of Electronic Engineering, National Technological University (UTN), Medrano 951, Buenos Aires, Argentina c GAIANN, Comision Nacional de Energia Atomica, Gral. Paz 1499 (1650), Buenos Aires, Argentina d Istituto per la Microelettronica e Microsistemi (IMM) Consiglio Nazionale delle Ricerche (CNR), Zona Industriale, Ottava Strada, 5, 95121 Catania, Italy e Department of Materials Science and Engineering, Technion — Israel Institute of Technology, 32000 Haifa, Israel	blu-ray;electronic engineering;f-spot;gate oxide;linear algebra;national research council (italy);percolation;point of view (computer hardware company);programming by demonstration;progressive scan;thermal management (electronics)	Felix Palumbo;Salvatore Lombardo;Moshe Eizenberg	2016	Microelectronics Reliability	10.1016/j.microrel.2015.10.009	electronic engineering;electrical engineering;time-dependent gate oxide breakdown	Theory	-46.92232690768252	-6.963476351207715	107424
11701025e8f98c8c9eef54d0742f1b8b9e60d120	utilization of latest geospatial web service technologies for remote sensing education through geobrain sysem	spatial data;remote sensing education;spatial data transfer geospatial web service technologies remote sensing education geobrain system web based geospatial information knowledge system;information technology;geospatial web service technologies;web based geospatial information knowledge system;web service;web services educational technology remote sensing space technology iso standards data visualization nasa earth observing system geoscience standards development;computer science education;geophysics computing;web services computer science education geophysical techniques geophysics computing remote sensing;geobrain system;knowledge systems;remote sensing;web services;spatial data transfer;geophysical techniques	GeoBrain is a Web based geospatial information and knowledge system that adopts and implements the newly emerged geospatial Web service technologies. It provides innovative methods in remote sensing education to prepare students with world-view training and experiences, transfer spatial data and information technologies to the professional workforce and society at large, and meet the increasing needs for globalization for remote sensing education and research.	experience;geoweb;knowledge-based systems;web service;world wide web	Meixia Deng;Liping Di	2006	2006 IEEE International Symposium on Geoscience and Remote Sensing	10.1109/IGARSS.2006.521	web service;computer science;data science;information technology;world wide web;remote sensing	Arch	-39.79327428215459	-2.3751817177273753	107650
4dddf2082b41e09219e91917409c6ba7b0c9c4bb	a study of suspended-membrane and acoustic techniques for the determination of the mechanical properties of thin polymer films	electrical engineering and computer science;thesis	Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1992.	acoustic cryptanalysis;polymer	Jeffrey Yen Pan	1991			materials science;electrical engineering;mechanical engineering	Robotics	-46.91097221413891	-6.991146323711622	107654
761eb9f364ca546f45b3bafc23b6fcac06fe46bb	an advanced operating environment for mathematics education resources		1School of Data and Computer Science, Sun Yat-Sen University, Guangzhou 510275, China; 2Academy for Intelligent Software, Guangzhou University, Guangzhou 510006, China; 3Institute of Mathematics and Computer Science, Guizhou Normal College, Guiyang 550018, China; 4School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China; 5Key Laboratory of High Confidence Software Technologies, Ministry of Education Beijing, Beijing 100871, China; 6College of Computer Sciences, New Jersey Institute of Technology, Newark 07102, USA	computer science;electronic engineering;operating environment	Yongsheng Rao;Jingzhong Zhang;Yu Zou;Yanchun Sun;Xiangping Chen;Songhua Xu	2017	Science China Information Sciences	10.1007/s11432-017-9235-7	mathematical optimization;applied mathematics;mathematics;operating environment	Theory	-45.37759083335983	-8.759875703215009	107928
437c69fa0e54726a136319eb8d0bc66a73435ef5	data placement for privacy-aware applications over big data in hybrid clouds		1School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China 2Jiangsu Engineering Centre of Network Monitoring, Nanjing University of Information Science and Technology, Nanjing, China 3State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China 4Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA 5School of Information and Control, Nanjing University of Information Science and Technology, Nanjing, China	big data;cloud computing;computer engineering;computer science;data-intensive computing;information and computation;information privacy;information science;platform as a service;requirement	Xiaolong Xu;Xuan Zhao;Feng Ruan;Jie Zhang;Wei Tian;Wan-Chun Dou;Alex X. Liu	2017	Security and Communication Networks	10.1155/2017/2376484	computer science;computer security;renting;access time;big data;energy consumption;cloud computing;cloud computing security	DB	-44.74207598696827	-7.747243071165747	108204
6b533222d158d3458da80408954c26d9746c4c65	medical research data and models for sharing and preserving data: the case of the uk medical research council				Melanie Wright	2003			medical research;data science;data mining;computer science	ML	-40.24919081771201	-5.75461734004755	108294
161813cbdc28efac2fd5b97f7aa7894649621e1d	big data: the power of petabytes	geociencias medio ambiente;ciencias biologicas generalidades;grupo de excelencia;ciencias basicas y experimentales generalidades;ciencias basicas y experimentales;geociencias medio ambiente generalidades;ciencias biologicas;grupo a	Researchers are struggling to analyse the steadily swelling troves of u0027-omicu0027 data in the quest for patient-centred health care.	big data;petabyte	Michael Eisenstein	2015	Nature	10.1038/527S2a	data science;health care;personalized medicine;petabyte;big data;datasets as topic;computer science	EDA	-38.856460757049945	-7.385536611549016	108323
f52fe70bbd49c504ba73dd5840569d071f2c4982	energy-efficient reliability-aware scheduling algorithm on heterogeneous systems		1School of Information Science and Engineering, National Supercomputing Center in Changsha, Hunan University, Changsha 410082, China 2Information Science and Technology College/Southern Regional Collaborative Innovation Center for Grain and Oil Crops in China, Hunan Agricultural University, Changsha 410128, China 3Archive, Hunan University of Humanities, Science and Technology, Loudi 417000, China	acm/ieee supercomputing conference;algorithm;information science;national center for supercomputing applications;scheduling (computing)	XiaoYong Tang;Tan Weizhen	2016	Scientific Programming	10.1155/2016/9823213	mathematical optimization;real-time computing;simulation;computer science	HPC	-45.1494769094966	-7.539703829173828	108326
bb2a0d3e639ecc59c2505a794b6933a219da5e66	representing and coding the knowledge embedded in texts of health science web published articles	electronic journal;hn e journals;eb printing electronic publishing broadcasting;model integration;health science;semantic web;ontologies;scientific communication;electronic publishing;knowledge representation;intelligent software agent;id knowledge representation;semantic retrieval	Despite the fact that electronic publishing is a common activity to scholars electronic journals are still based in the print model and do not take full advantage of the facilities offered by the Semantic Web environment. This is a report of the results of a research project with the aim of investigating the possibilities of electronic publishing journal articles both as text for human reading and in machine readable format recording the new knowledge contained in the article. This knowledge is identified with the scientific methodology elements such as problem, methodology, hypothesis, results, and conclusions. A model integrating all those elements is proposed which makes explicit and records the knowledge embedded in the text of scientific articles as an ontology. Knowledge thus represented enables its processing by intelligent software agents The proposed model aims to take advantage of these facilities enabling semantic retrieval and validation of the knowledge contained in articles. To validate and enhance the model a set of electronic journal articles were analyzed.	embedded system;human-readable medium;intelligent agent;ontology (information science);scientific literature;semantic web;software agent	Carlos Henrique Marcondes;Marília Alvarenga Rocha Mendonça;Luciana Reis Malheiros;Leonardo Cruz da Costa;Tatiana Christina Paredes Santos;Luciana Guimarães Pereira	2007			knowledge integration;computer science;knowledge management;ontology;semantic web;data mining;electronic publishing;world wide web;information retrieval	AI	-43.94703427164873	3.309351886232122	108327
1abedbb9174427fe86fff2412a21fa33cdfd3e58	digital libraries: research and development : first international delos conference, pisa, italy, february 13-14, 2007 : revised selected papers		Similarity Search.- MESSIF: Metric Similarity Search Implementation Framework.- Image Indexing and Retrieval Using Visual Terms and Text-Like Weighting.- Architectures.- A Reference Architecture for Digital Library Systems: Principles and Applications.- DelosDLMS - The Integrated DELOS Digital Library Management System.- ISIS and OSIRIS: A Process-Based Digital Library Application on Top of a Distributed Process Support Middleware.- An Architecture for Sharing Metadata Among Geographically Distributed Archives.- Integration of Reliable Sensor Data Stream Management into Digital Libraries.- Personalization.- Content-Based Recommendation Services for Personalized Digital Libraries.- Integrated Authoring, Annotation, Retrieval, Adaptation, Personalization, and Delivery for Multimedia.- Gathering and Mining Information from Web Log Files.- Interoperability.- Modelling Intellectual Processes: The FRBR - CRM Harmonization.- XS2OWL: A Formal Model and a System for Enabling XML Schema Applications to Interoperate with OWL-DL Domain Knowledge and Semantic Web Tools.- A Framework and an Architecture for Supporting Interoperability Between Digital Libraries and eLearning Applications.- Evaluation.- An Experimental Framework for Interactive Information Retrieval and Digital Libraries Evaluation.- The Importance of Scientific Data Curation for Evaluation Campaigns.- An Approach for the Construction of an Experimental Test Collection to Evaluate Search Systems that Exploit Annotations.- Evaluation and Requirements Elicitation of a DL Annotation System for Collaborative Information Sharing.- INEX 2002 - 2006: Understanding XML Retrieval Evaluation.- Miscellaneous.- Task-Centred Information Management.- Viewing Collections as Abstractions.- Adding Multilingual Information Access to the European Library.- The OntoNL Framework for Natural Language Interface Generation and a Domain-Specific Application.- Preservation.- Evaluating Preservation Strategies for Electronic Theses and Dissertations.- Searching for Ground Truth: A Stepping Stone in Automating Genre Classification.- Video Data Management.- Video Transcoding and Streaming for Mobile Applications.- Prototypes Selection with Context Based Intra-class Clustering for Video Annotation with Mpeg7 Features.- Automatic, Context-of-Capture-Based Categorization, Structure Detection and Segmentation of News Telecasts.- 3D Objects.- Description, Matching and Retrieval by Content of 3D Objects.- 3D-Mesh Models: View-Based Indexing and Structural Analysis.- Similarity-Based Retrieval with MPEG-7 3D Descriptors: Performance Evaluation on the Princeton Shape Benchmark.- Peer to Peer.- Application of the Peer-to-Peer Paradigm in Digital Libraries.- Efficient Search and Approximate Information Filtering in a Distributed Peer-to-Peer Environment of Digital Libraries.- Management of and Access to Virtual Electronic Health Records.		Costantino Thanos;Francesca Borri;Leonardo Candela	2007		10.1007/978-3-540-77088-6	computer science;database;world wide web;information retrieval	EDA	-43.751100103427895	-5.593310782491608	108427
b6e9229f441a44908df16b6b4322e25a798587ff	distributing large historical census samples on the internet		A new project at the University of Minnesota will provide data extraction and distribution over the Internet of the Integrated Public Use Microdata Series – a database which integrates all existing national samples of the US census from 1850 to 1990 into a consistent format.	internet	Steven Ruggles;Matthew Sobek;Todd Gardner	1996	History and Computing	10.3366/hac.1996.8.3.145	geography;data science;data mining;world wide web;american community survey	HPC	-42.43249168117689	-9.324074356674993	108472
f0e3e20e2cbf970ec7b36c9d4b45828e11891056	basic forever	debugging;expert systems;prolog;metrics;tracing;logic programming;wam	"""Basic was designed in 1964 by Kemeny and Kurtz as a simple language to teach the principles of programming on a mainframe with time sharing [5,6]. In the seventies, it was predominantly used on time sharing minicomputers, by 1980 it extended its life cycle because it was the first widely-used high-level language for PC's, and only in the late eighties it seemed to succumb to the success of Turbo-Pascal and C++. Recently, however, Basic reincarnated in the form of Visual Basic, a language specially designed for building Windows applications. Visual Basic is definitely a non-academic language. University libraries usually contain only one or two books on the subject, but an average provincial bookshop may stock some 20 titles, including a 1500-page """"bible"""". Likewise, a search for """"Visual Basic"""" in the scientific literature produced only one article, but there is an overwhelming amount of information on Internet, including long booklists and a newsgroup with hundreds of questions and answers a day. And though Visual Basic is taught in commercial and technical colleges, the only expert in a university CS department is usually the person who cannot get tenure. Anyone whose knowledge of Basic stems from the invectives of structured programming protagonists [1,2] will be surprised by the present-day Basic language as shown by the following example, which lists the main procedure from a towers of Hanoi program in Visual Basic."""	book;c++;google questions and answers;high- and low-level;high-level programming language;internet;library (computing);mainframe computer;minicomputer;reincarnation;scientific literature;structured programming;time-sharing;tower of hanoi;turbo pascal;visual basic	Rommert J. Casimir	1997	SIGPLAN Notices	10.1145/270507.270519	computer architecture;tracing;computer science;database;programming language;debugging;prolog;logic programming;expert system;metrics	HCI	-46.2686439520812	-3.4820860717978053	108517
de92bf246d0eb2b15e4727cda9e112e1f6811924	forecasting in database systems		Time series forecasting is a fundamental prerequisite for decision-making processes and crucial in a number of domains such as production planning and energy load balancing. In the past, forecasting was often performed by statistical experts in dedicated software environments outside of current database systems. However, forecasts are increasingly required by non-expert users or have to be computed fully automatically without any human intervention. Furthermore, we can observe an ever increasing data volume and the need for accurate and timely forecasts over large multi-dimensional data sets. As most data subject to analysis is stored in database management systems, a rising trend addresses the integration of forecasting inside a DBMS. Yet, many existing approaches follow a black-box style and try to keep changes to the database system as minimal as possible. While such approaches are more general and easier to realize, they miss significant opportunities for improved performance and usability. In this thesis, we introduce a novel approach that seamlessly integrates time series forecasting into a traditional database management system. In contrast to flash-back queries that allow a view on the data in the past, we have developed a Flash-Forward Database System (F2DB) that provides a view on the data in the future. It supports a new query type — a forecast query — that enables forecasting of time series data and is automatically and transparently processed by the core engine of an existing DBMS. We discuss necessary extensions to the parser, optimizer, and executor of a traditional DBMS. We furthermore introduce various optimization techniques for three different types of forecast queries: ad-hoc queries, recurring queries, and continuous queries. First, we ease the expensive model creation step of ad-hoc forecast queries by reducing the amount of processed data with traditional sampling techniques. Second, we decrease the runtime of recurring forecast queries by materializing models in a specialized index structure. However, a large number of time series as well as high model creation and maintenance costs require a careful selection of such models. Therefore, we propose a model configuration advisor that determines a set of forecast models for a given query workload and multi-dimensional data set. Finally, we extend forecast queries with continuous aspects allowing an application to register a query once at our system. As new time series values arrive, we send notifications to the application based on predefined time and accuracy constraints. All of our optimization approaches intend to increase the efficiency of forecast queries while ensuring high forecast accuracy.	database;declarative programming;fdb (file format);hoc (programming language);load balancing (computing);mathematical optimization;query optimization;source data;time series	Ulrike Fischer	2014			production planning;time series;database;load balancing (computing);data set;computer science	DB	-34.09363615757214	1.0515572687717953	108704
f05e730cf400bc3ca240c98183bf849b43b4340f	aligning parallel texts with intertext		InterText is a flexible manager and editor for alignment of parallel texts aimed both at individual and collaborative creation of parallel corpora of any size or translational memories. It is available in two versions: as a multi-user server application with a web-based interface and as a native desktop application for personal use. Both versions are able to cooperate with each other. InterText can process plain text or custom XML documents, deploy existing automatic aligners and provide a comfortable interface for manual post-alignment correction of both the alignment and the text contents and segmentation of the documents. One language version may be aligned with several other versions (using stand-off alignment) and the application ensures consistency between them. The server version supports different user levels and privileges and it can also track changes made to the texts for easier supervision. It also allows for batch import, alignment and export and can be connected to other tools and scripts for better integration in a more complex project workflow.	desktop computer;multi-user;parallel text;server (computing);subtext;text corpus;web application;xml	Pavel Vondricka	2014			natural language processing;information retrieval;artificial intelligence;xml;plain text;computer science;workflow;application server;scripting language;segmentation	NLP	-47.13379401755026	3.9723133037259246	108755
05848a8ff2a26e8df2b1fad7a2121835d18d47aa	a survey on semantic web and big data technologies for social network analysis	social network services;owl;semantics;resource description framework;big data;data models	Social Network Analysis (SNA) has become a very important and increasingly popular topic among researchers in recent years especially after emerging Semantic Web and Big Data technologies. Social networking services such as Facebook, Google+, Twitter, etc. provide large amounts of data that can be used for social network analysis by researchers. Semantic Web technology plays an important role for collecting, merging, and aggregating social network data from heterogeneous sources more easily, robustly and in an interoperable manner. Today, data scientists use several different frameworks for querying, integrating and analyzing datasets located at different sources. Meanwhile, most of the big social data is in unstructured or semi-structured format. Big data architectures allow researchers to analyze unstructured data in a time and cost-efficient way. New approaches for SNA are needed to combine Semantic Web and Big Data technologies in order to utilize and add capabilities to existing solutions. To be able to analyze large scale social networks, algorithms should have scalable designs in order to benefit from the emerging Big Data technologies. This survey focuses on recently developed systems for SNA and summarizes the state-of-the-art technologies used by them and points out to future research directions.	algorithm;apache hadoop;apache spark;batch processing;big data;centrality;clustering coefficient;cost efficiency;data science;database schema;distributed control system;google+;interaction;interoperability;ontology (information science);real-time transcription;resource description framework;sparql;scalability;semantic web;semiconductor industry;social network analysis;triplestore	Sercan Kulcu;Erdogan Dogdu;Ahmet Murat Ozbayoglu	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840792	computer science;data science;social semantic web;data mining;world wide web	DB	-36.546374677297045	-5.1562966400842445	108826
e37b3d9afc0538b5d4fa37a89c8bdc52a7d1abfd	novel criteria for deterministic remote state preparation via the entangled six-qubit state	permutation group;deterministic remote state preparation;entangled six qubit state;criteria	Gang Xu 1,2,3, Xiu-Bo Chen 2,3,*, Zhao Dou 2, Jing Li 2, Xin Liu 4 and Zongpeng Li 3 1 School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China; xu.gang@ucalgary.ca 2 Information Security Center, State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; douzhao333@163.com (Z.D.); overfly100@163.com (J.L.) 3 Department of Computer Science, University of Calgary, Calgary, AB T2N 1N4, Canada; zongpeng@ucalgary.ca 4 School of Information Engineering, Inner Mongolia University of Science and Technology, Baotou 014010, China; xinliu_imust@sohu.com * Correspondence: xb_chen@bupt.edu.cn; Tel.: +86-10-6228-2264	call of duty: black ops;chen–ho encoding;chow–liu tree;common criteria;computer science;entity–relationship model;information engineering;information security;jing;qubit;software engineering;telephone exchange	Gang Xu;Xiubo Chen;Zhao Dou;Jing Li;Xin Liu;Zongpeng Li	2016	Entropy	10.3390/e18070267	combinatorics;discrete mathematics;mathematics;permutation group;quantum mechanics;statistics	ML	-45.02883853379508	-8.459563503048576	109004
3e7d5da90cfd7f06e0252f6ae48647822c2662d6	integrating heterogeneous coin datasets in the context of archaeological research		This paper describes the activities carried out under the ARIADNE project to demonstrate the item-level integration process of archaeological archives through the use of semantic technologies. To this end, some ancient coin records, coming from the archives of important European archaeological institutions, were selected. The subset thus created, has been carefully analysed by means of specific tools to identify similar concepts and common metadata elements that could serve as the basis for integration. CIDOC CRM was chosen as the conceptual model for encoding the identified entities, while some important numismatic vocabularies have been employed to improve standardisation. The implementation phase has benefited from the use of advanced tools for mapping and conversion of the original information in a semantic form (RDF), the creation of a triple store to place the newly integrated data and the necessary interfaces for accessing and querying them.	archive;entity;triplestore;vocabulary	Achille Felicetti;Philipp Gerth;Carlo Meghini;Maria Theodoridou	2015			data science;geography	SE	-42.018927496535284	2.550309127310139	109101
0837b6e7a9d238d8ce71eefff621f67d02b2f46c	cross-cluster asymmetric group key agreement for wireless sensor networks		1School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China; 2Research Center of Massive Language Information Processing and Cloud Computing Application, Beijing 100081, China; 3Institute of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zhengzhou 450002, China; 4Department of Computer Science and Technology, Tangshan University, Tangshan 063000, China	cloud computing;computer science;cryptosystem;group key;information processing;key-agreement protocol;public-key cryptography;sensor node	Jiamin Zheng;Yu-an Tan;Qikun Zhang;Xiaosong Zhang;Liehuang Zhu;Quan-Xin Zhang	2017	Science China Information Sciences	10.1007/s11432-017-9212-2	wireless sensor network;group key;mathematical optimization;distributed computing;mathematics	Theory	-44.81858376707826	-7.759305971928034	109125
1d32a15e103901a0a101814848dad2f767cd6994	feedback and optimal control in climate economics		For his work in the economics of climate change, Professor William Nordhaus was a co-recipient of the 2018 Nobel Memorial Prize for Economic Sciences. A core component of the work undertaken by Nordhaus is the Dynamic Integrated model of Climate and Economy, known as the DICE model. The DICE model is a discrete-time model with two control inputs and is primarily used in conjunction with a particular optimal control problem in order to estimate optimal pathways for reducing greenhouse gas emissions. In this paper, we summarize the model for the systems and control community and indicate some current and future work.		Christopher M. Kellett;Steven R. Weller;Timm Faulwasser;Lars Grune;Willi Semmler	2018	CoRR			Robotics	-45.33223060191899	-5.213682953141553	109129
b0a2d2cdf07eb2f1b4a542d790f0b34f6073c496	construction of data resources catalogue system of yellow river based on distributed-ontology	ontologies artificial intelligence cartography cataloguing electronic data interchange geographic information systems geophysics computing internet meta data;ontologies rivers distributed databases catalogs semantics standards;data resource catalogue system construction data searching data positioning data resource sharing data resource integration internet multilevel business process network environment metadata technology catalog technology heterogeneous data resource management distributed data resource management information construction development resource exchange yellow river water conservancy commission data collection methods distributed ontology repositories digital yellow river project;cataloguing;ontologies artificial intelligence;internet;geophysics computing;geographic information systems;resources sharing yellow river data resources catalog service distributed ontology;cartography;meta data;electronic data interchange	With the rapid development of data collection methods and the digital Yellow River Project, the data resources of the Yellow River has been constantly accumulated. The data resources of Yellow River areautonomous, heterogeneous and dynamic. The problems encountered by the Yellow River Water Conservancy Commission is becoming increasingly prominent, such as resource sharing and exchange of the Yellow River data, etc, which hindering the further development of information construction of the Yellow River. To organize and manage the distributed and heterogeneous data resources in the local network environment effectively andby building a data resources catalog service system, the data resources are described in category with uniform standards. By using distributed ontology, catalog technology, metadata technology and network environment to offer the sustainment, the public is provided high-performances services in data searching and positioning, which accelerates the sharing and integration of data resources of the Yellow River Water Conservancy Commission. Oriented towards how to establish distributed ontology repositories efficiently to support semantic interaction among different users through the Internet, This article starts with the domain ontology construction theory and methods, and provides a construction method of data resources of Yellow River domain ontology based on multi-level business process. Then, a distributed ontology integration framework based on multi-mapping is presented. The paper expatiates on the building objectives and main content. We design the whole system technical architecture, and describe in detail the key technologies that are analyzed. Finally, the significance of Yellow River data resources catalog service system is analyzed, and we point out the research direction in the future.	business process;heterogeneous computing;information retrieval;information technology architecture;mathematical optimization;ontology (information science);performance;query optimization;semantic integration;systems architecture	Yan Li;Shi-Xing Jiao	2013	2013 21st International Conference on Geoinformatics	10.1109/Geoinformatics.2013.6626157	geography;data mining;database;world wide web	DB	-39.93459459279264	-2.443359251244071	109221
6b3efb314ed98ae369a2224178234d16522adb1f	arc: an oai service provider for cross-archive searching	service provider;digital library;open archive initiative;relational database;lessons learned	The usefulness of the many on-line journals and scientific digital lib raries that exist today is limited by the lack of a service that can federate them through a unified interface. The Open Archive Initiative (OAI) is one major effort to address technical interoperability among distributed archives. The objective of OAI is to develop a framework to facilitate the discovery of content in distributed archives. In this paper, we describe our experience and lessons learned in building Arc, the first federated searching service based on the OAI protocol. Arc harvests metadata from several OAI compliant archives, normalizes them, and stores them in a search service based on a relational database (MySQL or Oracle). At present we have over 165K metadata records from 16 data providers from various domains.	archive;federated identity;federated search;interoperability;mysql;online and offline;relational database;web search engine	Xiaoming Liu;Kurt Maly;Mohammad Zubair;Michael L. Nelson	2001		10.1145/379437.379451	service provider;digital library;relational database;computer science;data mining;database;world wide web	Web+IR	-42.32883580318796	0.5075111730139119	109309
7936bada8960e65d20b6016a35b23458a5bccedb	solar-powered field server and aerator development for lake palakpakin	lake management;water quality;aquaculture;wireless sensor network;field server	∗1Ateneo Innovation Center, Ateneo de Manila University Loyola Heights, Quezon City 1108, Philippines E-mail: db.solpico@gmail.com ∗2Electronics Computer and Communications Engineering Department, Ateneo de Manila University ∗3Institut Catholique d’Arts et Métiers 6 rue Auber, Lille 59000, France ∗4Alsons Aquaculture Corporation 2286 Chino Roces Ave., Makati City, 1200, Philippines ∗5Environmental Sciences Department, Ateneo de Manila University		Dominic B. Solpico;Nathaniel J. C. Libatique;Gregory L. Tangonan;Paul M. Cabacungan;Guillaume Girardot;Ramon M. Macaraig;Teresita R. Perez;Andrea Teran	2014	JACIII	10.20965/jaciii.2014.p0755	aquaculture;wireless sensor network;computer science	ML	-45.345503545072745	-8.67372361030269	109336
dd80365751efb859ed483534a8864dc76d3fd35e	jsexplain: a double debugger for javascript		We present JSExplain, a reference interpreter for JavaScript that closely follows the specification and that produces execution traces. These traces may be interactively investigated in a browser, with an interface that displays not only the code and the state of the interpreter, but also the code and the state of the interpreted program. Conditional breakpoints may be expressed with respect to both the interpreter and the interpreted program. In that respect, JSExplain is a double-debugger for the specification of JavaScript. ACM Reference Format: Arthur Charguéraud, Alan Schmitt, and Thomas Wood. 2018. JSExplain: a Double Debugger for JavaScript. In WWW ’18 Companion: The 2018 Web Conference Companion, April 23–27, 2018, Lyon, France. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3184558.3185969	breakpoint;debugger;ecmascript;formal language;interactivity;interpreter (computing);javascript;mahdiyar;parsing;proxy server;recursive definition;schmitt trigger;toolchain;tracing (software);www	Arthur Charguéraud;Alan Schmitt;Thomas Wood	2018		10.1145/3184558.3185969	programming language;debugger;breakpoint;javascript;interpreter;computer science	PL	-47.139470158551326	-3.6834105845827727	109549
bb383ba2b093e84dc3f32ca927e714ed17e13121	realtime database support for environmental visualization	environmental visualization;realtime database support	Much research has been done on interfacing databases to visualization applications, and there are many varied approaches. We describe the lessons learned during the design and development of the REINAS software (Real-time Environmental Information Network and Analysis System). We have developed visualization applications that have been in public use for several years and that visualize data from re-lational database engines. Our most popular tools are World Wide Web access tools. Our most sophisticated tools have novel user interfaces{ Spray and CSpray. In this paper we present the interface and API (application programming interface) issues, as well as the development of the required middleware. The REINAS system is a complete data management system whose requirements and construction were driven primarily by the visualization needs, and therefore presents a unique view of how to utilize commercial relational database technologies for environmental visualization. 1 Overview The REINAS{Real{time Environmental InformationNetwork and Analysis system{ is an information management system, including a relational database and mid-dleware, intended to marry data and visualization for environmental science. The project is an OOce of Naval Research university research initiative for bringing environmental science into a more intimate relationship with state of the art visualization and information technology. Figure 1 shows a high-level view of the system. Lines indicate physical or wireless network connections. Lines are bi-directional unless indicated otherwise. Data ows generally from left to right, and feedback and control generally ow from right to left. As the most visible part of our project we have numerous applications that interface to relational database technology. Much of the behind the scenes work has been in developing the system middleware and database schema 9], but the interface has also held numerous challenges. We have satissed several of the challenges, and face in the near term some additional large challenges. To demonstrate our contribution, we rst describe our project, solutions to date, and the trade-oos in diierent interface approaches.	application programming interface;bi-directional text;database schema;high- and low-level;information management system (ims);internet access;middleware;real-time web;relational database;requirement;right-to-left;user interface;world wide web	Craig M. Wittenbrink;Eric C. Rosen;Alex T. Pang;Suresh K. Lodha;Patrick E. Mantey	1995		10.1007/3-540-62221-7_10	database;world wide web;computer graphics (images)	Visualization	-42.99658644303259	-2.6414457399021276	109578
c92b81221b2cd466c3b4fe948f16823e6fd69da4	report of the purdue workshop on grand challenges in computer architecture for the support of high performance computing	engineering;software tool;software systems;computer architecture;operating system;humanities;high performance computer;philosophy;national science foundation;parallel computer;digital video;communication technology;computer science;building design;optimal algorithm;high performance	The “Purdue Workshop on Grand Challenges in Computer Architecture for the Support of High Performance Computing” was sponsored by the National Science Foundation to identify critical research topics in computer architecture as they relate to high performance computing. Following a wide-ranging discussion of the computational characteristics and requirements of the grand challenge applications , the workshop identified four major computer architecture grand challenges as crucial to advancing the state of the art of high performance computation in the coming decade. These are: (1) idealized parallel computer models; (2) usable peta-ops (10 15 ops) performance; (3) computers in an era of HDTV, gigabyte networks, and visualization; and (4) infrastructure for prototyping architectures. This report overviews some of the demands of the grand challenge applications and presents the above four grand challenges for computer architecture.	computer architecture;grand challenges	Howard Jay Siegel;Seth Abraham;William L. Bain;Kenneth E. Batcher;Thomas L. Casavant;Doug DeGroot;Jack B. Dennis;David C. Douglas;Tse-Yun Feng;James R. Goodman;Alan Huang;Harry F. Jordan;J. Robert Jamp;Yale N. Patt;Alan Jay Smith;James E. Smith;Lawrence Snyder;Harold S. Stone;Russ Tuck	1992	J. Parallel Distrib. Comput.	10.1016/0743-7315(92)90033-J	information and communications technology;parallel computing;human–computer interaction;computer science;operating system;building design;software system	HPC	-48.208647259803065	-0.9046651345440203	109607
6cc25661200e9b5fe09584a8b807202c35098f68	a critical review of the integration of geographic information system and building information modelling at the data level		The benefits brought by the integration of Building Information Modelling (BIM) and Geographic Information Systems (GIS) are being proved by more and more research. The integration of the two systems is difficult for many reasons. Among them, data incompatibility is the most significant, as BIM and GIS data are created, managed, analyzed, stored, and visualized in different ways in terms of coordinate systems, scope of interest, and data structures. The objective of this paper is to review the relevant research papers to (1) identify the most relevant data models used in BIM/GIS integration and understand their advantages and disadvantages; (2) consider the possibility of other data models that are available for data level integration; and (3) provide direction on the future of BIM/GIS data integration.	bim;building information modeling;citygml;data model;data structure;fm broadcasting;geographic information system;geomatics;geospatial analysis;gigabyte;high-level programming language;industry foundation classes;information exchange;information model;interoperability;level of detail;ontology (information science);semantic web;shapefile;software incompatibility	Junxiang Zhu;Graeme Wright;Xiangyu Wang	2018	ISPRS Int. J. Geo-Information	10.3390/ijgi7020066	data integration;building information modeling;data science;semantic web;data structure;data modeling;geographic information system;facility management;computer science	DB	-36.77022249930326	-1.7331095912274082	109868
a68bcb61864d0e6f7652f2ec9df9b1ccf8bef4d8	pathways: augmenting interoperability across scholarly repositories	heterogeneous systems;digital library;scholarly communication;service architecture;data model;value chain	In the emerging eScience environment, repositories of papers, datasets, software, etc., should be the foundation of a global and natively-digital scholarly communications system. The current infrastructure falls far short of this goal. Cross-repository interoperability must be augmented to support the many workflows and value-chains involved in scholarly communication. This will not be achieved through the promotion of single repository architecture or content representation, but instead requires an interoperability framework to connect the many heterogeneous systems that will exist. We present a simple data model and service architecture that augments repository interoperability to enable scholarly value-chains to be implemented. We describe an experiment that demonstrates how the proposed infrastructure can be deployed to implement the workflow involved in the creation of an overlay journal over several different repository systems (Fedora, aDORe, DSpace and arXiv).	best practice;core data;dspace;data model;e-science;experiment;ibm notes;interoperability;openurl;surrogates;x image extension	Simeon Warner;Jeroen Bekaert;Carl Lagoze;Xiaoming Liu;Sandra Payette;Herbert Warner	2007	International Journal on Digital Libraries	10.1007/s00799-007-0016-7	semantic interoperability;digital library;data model;value chain;computer science;service-oriented architecture;data mining;database;cross-domain interoperability;world wide web	HPC	-43.19585923354291	3.5355809541859475	109877
65139ee8d23c07e4917802be1e014e9d0040f4cd	conference committees		Marina L. Gavrilova, University of Calgary, Canada Kokichi Sugihara, University of Tokyo, Japan Deok-Soo Kim, International Center for Voronoi Diagram Research, Hanyang University, Korea Nikolai N. Medvedev, Group of Supramolecular Structures, Novosibirsk State University, Russia Christopher Gold, EU Marie Curie Chair, University of Glamorgan, UK Francois Anton, Technical University of Denmark, Denmark	anton (computer);george sugihara;voronoi diagram	Bahman Kalantari;Marina L. Gavrilova;Ivana Kolingerová;Nikolai N. Medvedev;Darka Mioc;Mohammadreza Mostafavi	2005		10.1109/CBMS.2005.45		Arch	-46.15535205209006	-9.795075688478072	110077
6f4cd82439bf45190656727d7d093b154e79a98a	towards scalable model indexing		Model-Driven Engineering (MDE) is a software engineering discipline promoting models as first-class artefacts of the software lifecycle. It offers increased productivity, consistency, maintainability and reuse by using these models to generate other necessary products, such as program code or documentation. As such, persisting, accessing, manipulating, transforming and querying such models needs to be efficient, for maintaining the various benefits MDE can offer. Scalability is often identified to be a bottleneck for potential adapters of MDE, as large-scale models need to be handled seamlessly, without causing disproportionate losses in performance or limiting the ability of multiple stakeholders to work simultaneously on the same collection of large models. This work identifies the primary scalability concerns of MDE and tackles those related to the querying of large collections of models in collaborative modeling environments; it presents a novel approach whereby information contained in such models can be efficiently retrieved, orthogonally to the formats in which models are persisted. This approach, coined model indexing leverages the use of file-based version control systems for storing models, while allowing developers to efficiently query models without needing to retrieve them from remote locations or load them into memory beforehand. Empirical evidence gathered during the course of the research project is then detailed, which provides confidence that such novel tools and technologies can mitigate these specific scalability concerns; the results obtained are promising, offering large improvements in the execution time of certain classes of queries, which can be further optimized by use of caching and database indexing techniques. The architecture of the approach is also empirically validated, by virtue of integration with various state-of-the-art modeling and model management tools, and so is the correctness of the various algorithms used in this approach.	algorithm;control system;correctness (computer science);database index;documentation;model-driven engineering;model-driven integration;run time (program lifecycle phase);scalability;software development process;software engineering;version control	Konstantinos Barmpis	2016			computer science;data mining;database;world wide web	SE	-35.30156684062551	1.2160217787328842	110141
36503e6dd57a105a5b6bd2675e3b3484df7f0469	timebliography: a dynamic and online bibliography on temporal gis				Willington Siabato;Christophe Claramunt;Miguel-Ángel Manso-Callejo;Miguel Ángel Bernabé Poveda	2014	Trans. GIS	10.1111/tgis.12080	data science;bibliography;information retrieval;computer science	HCI	-40.98276990797321	-7.4782062792621895	110298
393576601ef1a46bf1a7bb9b658ea264b5f71f0a	data profiling as a process - bridging the gap between academia and practitioners		Dealing with data necessitates understanding data, which can be facilitated by analyzing its metadata. This metadata is often not readily available, and thus, needs to be extracted and collected. This activity is called data profiling and is well established and researched in the literature. However, what is missing is a structured description of a holistic data profiling process, that puts together all the individual pieces and guides a user from the problem to the solution. This paper describes such a process and the results and insights that have been achieved so far.	bridging (networking);holism	Fabian Schomm	2016				HCI	-43.5265174893637	-0.022181255650207184	110342
07ca437cebb7bb8b11b6c90fadbb908f8bcd45ff	simchemdb-a database management system for facilitating data management in computational chemistry	databases;chemicals;chemical engineering computing;simchemdb;database management systems;data management;database management systems chemical engineering computing data visualisation;database engine;computational chemistry;data visualisation;antimalarial drug design process simchemdb data management computational chemistry database management system apache derby chemical structure database engine tableviewer;engines;drug design;tableviewer;database systems;data visualization;chemistry;apache derby;database systems chemistry chemicals relational databases engines data visualization visual databases high performance computing information retrieval data engineering;chemical structure;database management system;antimalarial drug design process;java	In order to improve the environment for researchers on computational chemistry, a database management system named SimChemDB was developed. SimChemDB provides the facilities for data management, including storing chemical files, displaying and editing chemical structure. It made use of Apache Derby, which served as database engine. The database schema was created for the storage of various data. A TableViewer was designed for the visualization of chemical data. Finally, SimChemDB system was demonstrated through an application on anti-malarial drug design process, where the steps of data preparation were expatiated. The results indicate that SimChemDB is an effective tool for computational chemistry, and it facilitates the management of chemical data.	apache derby;computational chemistry;database engine;database schema;management system	Ruisheng Zhang;Kai Pan;Liangying Luo;Shilin Chen	2008	2008 International Multi-symposiums on Computer and Computational Sciences	10.1109/IMSCCS.2008.9	data modeling;computer science;data science;data administration;data mining;database;physical data model;database testing;database design	DB	-35.74960563636736	-0.36815516665373005	111266
60f83228a7fb2176b1d08dacfa10d35cd1302ad9	sims: an integrated, user-oriented information system	information management system;satisfiability;university of wisconsin;social science;complex data;data base management system;language processing;information processing;information system	SIMS, a Social Science Information Management System, has been developed by the Social Science Data and Computation Center (DACC) at the University of Wisconsin for the purpose of providing the social scientist with a personalized data base management system (DBMS). Over the past four years many DBMS were surveyed and their basic features compared to our requirements. All systems surveyed had their strong and weak points but no one system was found to satisfy all our needs. In addition, new and improved methods and algorithms for language processing, storage and retrieval, and analysis capabilities have been developed by DACC and others since the advent of these earlier systems. These facts, coupled with the fact that conversion of existing systems seemed impractical due to their machine dependence, has resulted in the development of SIMS. It is the intent of SIMS through its features and basic design to serve as an information processing tool for social scientists and others working with large and complex data files.	algorithm;computation;database;information management system (ims);information processing;information system;personalization;requirement	M. E. Ellis;W. Katke;J. Olson;S. C. Yang	1972		10.1145/1480083.1480148	computer science;management information systems;data mining;database	DB	-41.98309152953073	-6.152874266651933	111270
74d637284b99cfbd4981c35bfe64d3818e3c4dfc	archiving with athamas: a framework for optimized handling of domain knowledge	content management;libraries;relational databases data handling knowledge acquisition;protocols;application framework;query domain knowledge optimized handling multitiered project maintenance process athamas business processes dictating data handling relational databases knowledge extraction database storage engines data archival archive usage;oceans;optimized handling;application software;domain knowledge framework archival dataspace database;knowledge extraction;database;publishing;relational database;data mining;domain knowledge;archive usage;engines;data archival;knowledge acquisition;database systems;maintenance process;query domain knowledge;athamas;dataspace;application software relational databases containers data mining oceans engines data handling publishing costs content management;relational databases;data handling;database storage engines;archival;business processes dictating data handling;use case;business process;framework;data archive;containers;multitiered project;data models	Continuous changes in requirements of a multi-tiered project can significantly deteriorate the maintenance process. This is mainly due to incompatibility between application design and newly introduced requirements. This paper presents a potential application framework, Athamas, which provides a scalable way to agilely adapt to changing data requirements. A currently functional initiative of Athamas is to allow domain knowledge generation from application components without users or business processes dictating data handling and integration requirements. This development allows the framework to be used 1) today as a use case alternative to relational databases for archiving domain knowledge into storage containers and 2) in the future for optimally extracting the knowledge from the storage containers. Evaluations of Athamas-based applications are made against applications using MySQL’s MyISAM and ARCHIVE database storage engines for data archival purposes. Athamas using a zlib compression layer significantly reduces the storage size utilization to 3% of MyISAM and 24% of ARCHIVE, along with publishing time improvements by a factor of 4.9 and 2.6 respectively. Using Athamas with bzip2 compression, storage usage is down to 8% of ARCHIVE usage but at a cost of slower performance up to 4.5 times. The paper also discusses future enhancements to Athamas allowing users to efficiently query domain knowledge.	application framework;archive;business process;component-based software engineering;database engine;database storage structures;myisam;mysql;relational database;requirement;scalability;software incompatibility;bzip2;zlib	Eric R. Schendel;Ahmed M. Mahdy	2010	2010 Second International Conference on Advances in Databases, Knowledge, and Data Applications	10.1109/DBKDA.2010.30	relational database;computer science;data mining;database;knowledge extraction;world wide web	DB	-35.50239938966408	1.4921161698568202	111293
6b8abc725c8a06a74cef24a083f601cc0c043155	big-data aggregating, linking, integrating and representing using semantic web technologies		Semantic web provides information for humans as well as computers to semantically maintain a large-scale of data and provide a meaningful content of unstructured data. It offers new benefits for big-data research and applications. Big data is a new term refers to a massive collection of datasets from various sources in structured, semi-structured, and unstructured data collection. Their integration faces many problems such as the structural and the semantic heterogeneity as the processing of these data is difficult using traditional databases and software techniques. In this paper, the data resources are extracted and aggregated from different sources on the web following by using the geospatial ontology to transform this data into RDF format. RDF format is used to integrate the data semantically and construct the big-data semantic model that is used to store data. The major contribution of this research is to aggregate, integrate, and represent geospatial data semantically. A case study of cities data is used to illustrate the proposed workflow functionalities. The main result of this research is to solve the heterogeneous problem in different data sources with improving the data aggregation, integration, and representation.	big data;hyperlink;semantic web	Abeer Saber;Aya M. Al-Zoghby;Samir Elmougy	2018		10.1007/978-3-319-74690-6_33	data mining;geospatial analysis;semantic data model;rdf;big data;semantic web;semantic heterogeneity;workflow;computer science;unstructured data	AI	-37.39943630363639	1.8588901143657124	111486
0e11c41aeab6c72e0db3a8fcf778afb7a8829507	an estimate of land take in municipal planning of the campania region		The issue of land take is characterized by analytical complexity and legislative effectiveness. So, in dealing with this question, several aspects have to be evaluated.		Massimiliano Bencardino	2017		10.1007/978-3-319-62404-4_6	mathematical optimization;management science;computer science	Robotics	-34.748366496461074	-7.26225231319379	111500
21ed93b6364340db2f781de87fdc306c3cc26f30	combining relational and nosql database systems for processing sensor data in disaster management		In disaster and emergency management the integration of different kinds of sensor networks gains in importance and consequently more and more data becomes available. The upcoming NoSQL database systems are flexible and scalable data stores, but up to now lacking in connectivity to traditional data processing systems (data warehouses, business intelligence suites, etc.). Due to that in this work a combined relational and NoSQL data processing approach is proposed to reduce data volume and work load of the relational part and enable the integral solution to process huge amounts of data. In contrast to fully NoSQL-based data warehouse systems, this approach does not face compatibility and integrability issues.	nosql	Reinhard Stumptner;Christian Lettner;Bernhard Freudenthaler	2015		10.1007/978-3-319-27340-2_82	computer science;database model;data mining;database;world wide web;database design	DB	-35.06079032373226	-0.4553338426229227	111587
41d96409d38930776eb155267410d59fc0a3bef0	a quantitative analysis of the use of microdata for semantic annotations on educational resources		A current trend in the semantic web is the use of embedded markup formats aimed to semantically enrich web content by making it more understandable to search engines and other applications. The deployment of Microdata as a markup format has increased thanks to the widespread of a controlled vocabulary provided by Schema.org. Recently, a set of properties from the Learning Resource Metadata Initiative (LRMI) specification, which describes educational resources, was adopted by Schema.org. These properties, in addition to those related to accessibility and the license of resources included in Schema.org, would enable search engines to provide more relevant results in searching for educational resources for all users, including users with disabilities. In order to obtain a reliable evaluation of the use of Microdata properties related to the LRMI specification, accessibility, and the license of resources, this research conducted a quantitative analysis of the deployment of these properties in large-scale web corpora covering two consecutive years. The corpora contain hundreds of millions of web pages. The results further our understanding of this deployment in addition to highlighting the pending issues and challenges concerning the use of such properties.	accessibility;controlled vocabulary;embedded system;markup language;microdata (html);schema.org;semantic web;software deployment;text corpus;web content;web page;web search engine	Rosa Del Carmen Mavarrete Rueda;Sergio Lujan	2018	J. Web Eng.		schema.org;web standards;microdata (html);data mining;semantic web;computer science	Web+IR	-41.45468938602887	2.884835563940785	111663
5afcc3ffa47a9e035451496ab49dded1687b8989	a spelling checker	programming pearl;spelling checker	Spelling mistakes irritate readers. And for most writers, checking spelling is a boring and error-prone job. Fortunately, the problem is ideally suited for computers: dull, repetitive work that requires fast reading and a good memory. In this column we’ll study the design of the Unix’ spelling checker spell. It’s a beautiful and useful program, with a history rich in important lessons about program development.	cognitive dimensions of notations;computer;spell checker;unix	Jon Louis Bentley	1985	Commun. ACM	10.1145/3532.315102		SE	-46.14470002429878	-3.459048581960225	111736
68d3940a4efe37d4088bf9ad2a700c5391bd82f7	a comparative analysis of disciplinary data management workflows	data integration;digital libraries;meta data;oais conceptual model;pid assignment;conceptual workflow models;data curation;data preparation;disciplinary data management workflows;high-energy physics;humanities-and-social sciences;library and information science;metadata;persistent identifier assignment;research data;research datasets;scholarly communication;transdisciplinary service design;transdisciplinary service implementation;conceptual models;oais;author identifiers;persistent identifiers;scientific data curation	Datasets are now an integral part of scholarly communication. The result is that research data has now become a reality in library and information science, and its curation requires dedicated workflows. Here, we compare two disciplinary examples from High-Energy Physics and Humanities and Social Sciences, both referenced to the OAIS conceptual model. Even though we know that the research datasets and their metadata (preparation and curation) are very different in both disciplines, it can be seen that the conceptual workflow models are very similar, including the assignment of persistent identifiers (PIDs). The latter is particularly interesting when discussing the design and implementation of transdisciplinary services in library and information science.	digital curation;floor and ceiling functions;library (computing);library and information science;open archival information system;persistent identifier;qualitative comparative analysis	Suenje Dallmeier-Tiessen;Artemis Lavasa;Patricia Herterich;Laura Rueda;Rachael Kotarski;Elizabeth Newbold	2014	IEEE/ACM Joint Conference on Digital Libraries		data modeling;interoperability;data curation;information science;computer science;conceptual model;data mining;database;publishing;world wide web;information retrieval	HPC	-42.75845191784517	1.2734025341136204	112274
1461684cdcdc3bbbded7abbb6013aea32200cca7	geographic database systems: issues and research needs (abstract).	research needs;geographic database	Geographic information systems (GISS) have been a growth market for the last 15 years. The user communities include domain scientists and professionals in such areas as city planning and land use planning, resource allocation, vehicle navigation, emergency management, archeology marine applications, and geology. The design of GISS is based on Geographic Information Science, a multi-disciplinary field that includes contributions from geographers, cartographers, engineers, mathematicians, computer scientists, and more recently cognitive scientists and psychologists. This tutorial will give an overview of database issues related to geographic information systems. GISS are typically very large software systems dealing with highly structured geographic data and complex spatial relationships. Since data collections are shared for a variety of purposes, multiple conceptual perspectives must be supported. This implies that frequently dramatic conceptual changes must be supported. Three particularly important areas of geographic information science will be covered: ●	cartography;computer scientist;geographic information science;geographic information system;software system	Max J. Egenhofer	1996		10.1145/237661.242785	computer science;gis and public health	DB	-36.5431656619053	-3.5308156820671788	112409
231b7840d8456d499602c02ddc4a77289dc31de1	opportunities for semantic technologies in the nasa heliophysics data environment		Like many scientific disciplines, the NASA heliophysics data environment is implementing distributed search and retrieval systems for its data sets. However, this community is unique in that NASA has determined that they will deploy multiple systems – termed Virtual Observatories instead of one all encompassing system. This decision, along with other needs of our users, provides integration and interoperability challenges that may not be present elsewhere. In this work we present areas in which we are beginning to implement semantic technologies to solve these challenges. In addition, we introduce current and future needs that will require semantic solutions and offer these as discussion points and collaboration areas for artificial intelligence researchers. Background and Introduction In recent years the NASA space physics community was realigned to focus on the new science of heliophysics. The NASA strategic roadmap (http://sec.gsfc.nasa.gov/Roadmap_FINALscr.pdf, accessed 10/19/2007) defines heliophysics as concentrating on “the Sun and its effects on Earth, the other planets of the solar system, and the changing conditions in space”. As a focal point of the new heliophysics data environment NASA chose to implement multiple sub-domain specific “Virtual Observatories” (VOs) that would function as single access points and unified search and retrieval systems. These specialized VOs would then ultimately be united (see NASA Science Plan 2007, http://science.hq.nasa.gov/strategy/Science_Plan_07.pdf, accessed 10/19/2007) for unified access and interoperability of heliophysics data. Some tools and guidelines are available to assist in the long-term integration and interoperability. As an example, the NASA VO community has agreed to use the Space Physics Archive Search and Extract (SPASE) data model (Harvey et al., 2004). SPASE is an international group of space physics researchers, VO developers and data providers who have taken on the task of creating a comprehensive space physics data model. This data model consists of agreed upon terminology and definitions as well as protocols on how to document a data product for use in the community and use in VOs. It serves as a standard to which the diverse discipline vocabularies can be mapped, thus attempting to serve as a lingua franca. The SPASE data model is implementation neutral and the group currently provides an implementation in XML Schema. While such tools may be available, they do not offer a complete solution. In particular, the SPASE data model does not cover all areas of heliophysics and it is not in a form that can be used for computational reasoning. Additionally, other areas of the data environment provide unique data integration issues for which we argue semantic technologies are a natural solution. Thus, we present our initial attempts at using such technologies within the NASA heliophysics environment and discuss future needs where these technologies will also be beneficial. Data Environment Components and the Need for Semantics The need, and natural fit, for semantic technologies within the sciences is beginning to be widely recognized (Shadbolt et al., 2006). Such technologies have begun to take hold within the physical sciences, and in particular heliophysics (McGuinness et al., 2007). Through this solution, as emphasized by Thomas (2007), “the semantic web architecture can provide a common framework for semantics and data to be shared and reused across applications and enterprise boundaries”. Within the NASA heliophysics data environment there are several areas that are either beginning to explore semantic technologies or would benefit from future implementations of these technologies. In the following subsections we outline these areas. Specifically, we address the community need, the current solution and how a semantic solution would augment the current solution.	archive;artificial intelligence;computation;data model;distributed web crawling;focal (programming language);interoperability;semantic web;single-access key;virtual observatory;vocabulary;wireless access point;xml schema	Thomas William Narock;Adam Szabo;Jan Merka	2008			systems engineering;artificial intelligence;machine learning;architecture;implementation;semantic web;semantic technology;data model;interoperability;data integration;data mining;computer science;heliophysics	DB	-47.13189445340471	1.4184744727755563	112532
e5311c163d4f08f8d02a02823113933f0b26bf54	describex: interacting with axpre summaries	visual interactive tool;xml schema;bisimilarity based proposals;standards;feeds;best practice;visual interaction;xml collection;data exchange;web service;visualization;proteins;guidelines;axis path regular expressions;best practices;xml performance analysis proposals proteins feeds blogs digital audio broadcasting web services best practices guidelines;describex;web services;performance analysis;protein protein interaction;axpre;xml;digital audio broadcasting;xml interactive systems;psi mi schema;psi mi schema describex visual interactive tool xml collection axis path regular expressions axpre bisimilarity based proposals;communities;proposals;interactive systems;blogs;regular expression;domain specificity;bioinformatics	DescribeX is a visual, interactive tool for exploring the underlying structure of an XML collection. DescribeX implements a framework for creating XML summaries described using axis path regular expressions (abbreviated AxPRE). AxPRE's capture all the bisimilarity-based proposals in the summary literature and they can be used to define new and more expressive summaries. This demonstration shows how DescribeX helps to analyze diverse XML collections in one particular scenario: the analysis of protein-protein interaction XML data from multiple providers that conform to the PSI-MI schema.	apache axis;bisimulation;level of detail;regular expression;xml	Mir Sadek Ali;Mariano P. Consens;Shahan Khatchadourian;Flavio Rizzolo	2008	2008 IEEE 24th International Conference on Data Engineering	10.1109/ICDE.2008.4497616	web service;xml validation;computer science;data mining;database;programming language;world wide web;xml schema editor;best practice	DB	-37.50277775997422	3.8599860402755923	112845
128d497c3c2226f3c96610e5be9923f78af3fc23	describing and communicating uncertainty within the semantic web	semantic web	The Semantic Web relies on carefully structured, well defined, data to allow machines to communicate and understand one another. In many domains (e.g. geospatial) the data being described contains some uncertainty, often due to incomplete knowledge; meaningful processing of this data requires these uncertainties to be carefully analysed and integrated into the process chain. Currently, within the Semantic Web there is no standard mechanism for interoperable description and exchange of uncertain information, which renders the automated processing of such information implausible, particularly where error must be considered and captured as it propagates through a processing sequence. In particular we adopt a Bayesian perspective and focus on the case where the inputs / outputs are naturally treated as random variables. This paper discusses a solution to the problem in the form of the Uncertainty Markup Language (UncertML). UncertML is a conceptual model, realised as an XML schema, that allows uncertainty to be quantified in a variety of ways i.e. realisations, statistics and probability distributions. UncertML is based upon a soft-typed XML schema design that provides a generic framework from which any statistic or distribution may be created. Making extensive use of Geography Markup Language (GML) dictionaries, UncertML provides a collection of definitions for common uncertainty types. Containing both written descriptions and mathematical functions, encoded as MathML, the definitions within these dictionaries provide a robust mechanism for defining any statistic or distribution and can be easily extended. Universal Resource Identifiers (URIs) are used to introduce semantics to the soft-typed elements by linking to these dictionary definitions. The INTAMAP (INTeroperability and Automated MAPping) project provides a use case for UncertML. This paper demonstrates how observation errors can be quantified using UncertML and wrapped within an Observations & Measurements (O&M) Observation. The interpolation service uses the information within these observations to influence the prediction outcome. The output uncertainties may be encoded in a variety of UncertML types, e.g. a series of marginal Gaussian distributions, a set of statistics, such as the first three marginal moments, or a set of realisations from a Monte Carlo treatment. Quantifying and propagating uncertainty in this way allows such interpolation results to be consumed by other services. This could form part of a risk management chain or a decision support system, and ultimately paves the way for complex data processing chains in the Semantic Web.	decision support system;dictionary;geography markup language;interoperability;interpolation;marginal model;monte carlo method;observations and measurements;rendering (computer graphics);risk management;semantic web;uniform resource identifier;xml schema	Matthew Williams;Lucy Bastin;Dan Cornford;Ben Ingram	2008			computer science;theoretical computer science;data mining;database	Web+IR	-38.007595736210554	2.9318395611984216	113056
c47925236348ee6ec98dc11b6d42d9143ea94218	similarity assessment and efficient retrieval of semantic workflows	retrieval;business workflows;scientific workflows;workflow similarity;case based reasoning	In the recent years, the use of workflows has significantly expanded from its original domain of business processes towards new areas. The increasing demand for individual and more flexible workflows asks for new methods that support domain experts to create, monitor, and adapt workflows. The emergent field of process-oriented case-based reasoning addresses this problem by proposing methods for reasoning with workflows based on experience. New workflows can be constructed by reuse of already available similar workflows from a repository. Hence, methods for the similarity assessment of workflows and for the efficient retrieval of similar workflows from a repository are of core importance. To this end, we describe a new generic model for representing workflows as semantically labeled graphs, together with a related model for knowledge intensive similarity measures. Further, new algorithms for workflow similarity computation, based on A* search are described. A new retrieval algorithm is introduced that goes beyond traditional sequential retrieval for graphs, interweaving similarity computation with case selection. We describe the application of this model and several experimental evaluations of the algorithms in the domain of scientific workflows and in the domain of business workflows, thereby showing its broad applicability.	a* search algorithm;aggregate function;business process;case-based reasoning;central processing unit;computation;emergence;graph isomorphism;image scaling;information sciences institute;internet authentication service;maximal set;multi-core processor;parallel computing;scalability;semantic data model;software repository;subject-matter expert;thread (computing);time complexity	Ralph Bergmann;Yolanda Gil	2014	Inf. Syst.	10.1016/j.is.2012.07.005	case-based reasoning;computer science;artificial intelligence;data mining;database;event-driven process chain;information retrieval	Web+IR	-38.7855798797922	3.321053839260204	113068
1563a6e9f7ed465acb1d9408402413d3e4086792	flued: a novel four-layer model for simulating epidemic dynamics and assessing intervention policies		1 Department of Computer Science and Information Engineering, Chang Gung University, 259 Wen-Hwa 1st Road, Taoyuan 333, Taiwan 2 Research Center for Emerging Viral Infections, Chang Gung University, 259 Wen-Hwa 1st Road, Taoyuan 333, Taiwan 3Department of Geography, National Taiwan University, No. 1 Section 4, Roosevelt Road, Taipei 10617, Taiwan 4 Infectious Disease Research and Education Center (DOH-NTU), No. 17, Hsu-Chu Road, Taipei, Taiwan	computer science;information engineering;network interface device	Chung-Yuan Huang;Tzai-Hung Wen;Yu-Shiuan Tsai	2013	J. Applied Mathematics	10.1155/2013/325816	demography	ML	-45.28335853440039	-8.514443375754885	113117
59a0473bbbb8981a25a892b5f7b85d97160082b1	open and transparent: the review process of the semantic web journal		While open access is established in the world of academic publishing, open reviews are rare. The Semantic Web journal goes further than just open review by implementing an open and transparent review process in which reviews are publicly available, the assigned editors and reviewers are known by name, and are published together with accepted manuscripts. In this article we introduce the steps to realize such a process from the conceptual design, over the implementation, a overview of the results so far, and up to lessons learned.	blog;drupal;linkage (software);linked data;microsoft outlook for mac;openness;scientific literature;semantic web	Krzysztof Janowicz;Pascal Hitzler	2012	Learned Publishing	10.1087/20120107	data web;web standards;semantic web;social semantic web;semantic web stack;database;world wide web;information retrieval;semantic analytics	Web+IR	-43.277352584184484	0.885162140687518	113140
e6e2a692c6907e65980de11b5b7f11ca30598414	mining real estate listings using oracle data warehousing and predictive regression	multiple listing service;visual basic net data mining system predictive regression data warehouse real estate listings multiple listing services database oracle;real estate;three dimensions;data warehouse design;data mining;regression analysis data mining data warehouses real estate data processing;real estate data processing;visual basic;warehousing data mining multilevel systems data warehouses predictive models production systems computer science process design databases visual basic;data warehousing;predictive regression;regression analysis;statistical techniques;data warehouses;data warehouse	The object of this paper is to introduce our development experience of a data mining system for prospective real estate sellers and buyers to determine properties price. The prediction of continuous values of properties selling prices is modeled by a statistical technique called predictive regression. The prerequisite of this data mining process is to design a data warehouse that contains a wide variety of real estate listings in the related areas. The data source is extracted from multiple listing services (MLS) database. It is cleansed and transformed at a staging area. The data warehouse design for this system is a star schema with one large fact table surrounded by three dimension tables. Loading data into the warehouse is the final step in creating data warehouse as preparation for data mining. Oracle data warehousing tool kits were used in the data warehouse construction. Visual Basic .Net was used to implement the data mining system.	a* search algorithm;data mining;disk staging;oracle database;prospective search;star schema;visual basic;visual basic[.net]	Wuri Wedyawati;Meiliu Lu	2004	Proceedings of the 2004 IEEE International Conference on Information Reuse and Integration, 2004. IRI 2004.	10.1109/IRI.2004.1431477	three-dimensional space;data transformation;dimensional modeling;computer science;data science;data warehouse;data mining;database;data stream mining;regression analysis;real estate	DB	-39.05999515906573	-4.720511881664687	113195
1523e79a91eea2b63070a0ab9d845163a38e05fa	supporting scientific collaboration in a network of excellence through a semantically indexed knowledge map		Automatic building of a thesaurus and its application to the management of a distributed research community has been one of the initial achievements of a European network of excellence, the INTEROP NoE, whose main objective is to support scientific advancements and dissemination actions in the field of enterprise and software interoperability. Two INTEROP work packages have contributed to this result. The work-package WP1 “Knowledge Map” (KMap) aims at drawing a picture of the status of research in interoperability, and to keep this picture up-to-date in the future. The main objective of the KMap is to perform a periodic diagnostic of the extent of research collaboration and coordination among INTEROP partners. This diagnostic will support the formulation of recommendations to strengthen this collaboration and the better guidance of future research efforts of the various NoE partners. The work-package WPG “Interoperability Glossary” aims at building a thesaurus of interoperability terms, reflecting the three main INTEROP knowledge domains: ontology, enterprise modeling, software architectures and platforms. There are several benefits in creating a durable INTEROP glossary and associated facility:	enterprise modelling;glossary;interop-vlab;interoperability;knowledge management;software architecture;thesaurus	Paola Velardi;Alessandro Cucchiarelli;Michaël Petit	2006		10.1007/978-1-84628-714-5_22	computer science;knowledge management;data mining;information retrieval	Web+IR	-45.19976462629865	3.7983180264937006	113590
3662f196686ad8da70d26c996e30342f8dac79a0	flexible hybrid stores: constraint-based rewriting to the rescue	data models sparks query processing big data optimization history data integration	Data management goes through interesting times1, as the number of currently available data management systems (DMSs in short) is probably higher than ever before. This leads to unique opportunities for data-intensive applications, as some systems provide excellent performance on certain data processing operations. Yet, it also raises great challenges, as a system efficient on some tasks may perform poorly or not support other tasks, making it impossible to use a single DMS for a given application. It is thus desirable to use different DMSs side by side in order to take advantage of their best performance, as advocated under terms such as hybrid or poly-stores. We present ESTOCADA, a novel system capable of exploiting side-by-side a practically unbound variety of DMSs, all the while guaranteeing the soundness and completeness of the store, and striving to extract the best performance out of the various DMSs. Our system leverages recent advances in the area of query rewriting under constraints, which we use to capture the various data models and describe the fragments each DMS stores.	data model;data-intensive computing;digital multiplex system;rewriting	Francesca Bugiotti;Damian Bursztyn;Alin Deutsch;Ioana Manolescu;Stamatis Zampetakis	2016	2016 IEEE 32nd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2016.7498353	computer science;data mining;database	DB	-34.93277639841946	0.6877888895406756	114207
bddf21bd32b54791735913616eeee98ac1e98a49	new design approach to handle spatial vagueness in spatial olap datacubes: application to agri-environmental data		Spatial-OLAP (SOLAP) technologies are dedicated to multidimensional analysis of large volumes of (spatial) data. Spatial data are subject to different types of uncertainty, in particular spatial vagueness. Although several researches propose new models to cope with spatial vagueness, their integration in SOLAP systems is still in an embryonic state. Also, analyzing multidimensional data with metadata brought by the exploitation of the new models can be too complex and demanding for decision-makers. To help reduce spatial vagueness consequences on the exactness of SOLAP analysis queries, the authors present a new approach for designing SOLAP datacubes based on end-users’ tolerance to the risks of misinterpretation of fact data. An experimentation of the new approach on agri-environmental data is also proposed. New Design Approach to Handle Spatial Vagueness in Spatial OLAP Datacubes: Application to Agri-environmental Data	multidimensional analysis;online analytical processing;vagueness	Élodie Edoh-Alove;Sandro Bimonte;François Pinet;Yvan Bédard	2015	IJAEIS	10.4018/IJAEIS.2015070103	computer science;theoretical computer science;data mining;database	DB	-36.622529486473525	-2.355441093874568	114284
3278e51e12db7e0db2905f583e589c4d772ab75b	traffic analysis as a service via a unified model	conceptual modeling;trajectories;density based clustering;road traffic geographic information systems government data processing;context evolution;spatio temporal;traffic analysis;software as a service;bayesian networks software as a service conceptual modeling traffic analysis context evolution trajectories spatio temporal density based clustering;beijing taxi trajectory data traffic analysis platform implemented as a service government authorities geographic locations numerical analysis unified conceptual model traffic dynamics service implementation unified model consumable format geographic databases service computational model value added traffic analysis spatio temporal er model spatio temporal context variations;trajectory context analytical models unified modeling language erbium context modeling computational modeling;bayesian networks	This paper describes a traffic analysis platform that is implemented as a service, so different government authorities or geographic locations can utilize its functionalities more efficiently and effectively. In particular, most of the research in traffic analysis is solely focused on numerical analysis, while the conceptual relationships among moving objects are rarely captured. In this paper, we investigate into traffic relationships by a unified conceptual model that captures traffic dynamics under changing contexts to facilitate service implementation. By using the unified model, we can capture domain knowledge in a robust and consumable format for our proposed services for different users and on different geographic databases. The knowledge can also be mapped into a service computational model for other value-added traffic analysis. Instead of using the previously proposed spatio-temporal ER model, we define our model by augmenting the traditional ER model with symbolic contexts to better capture spatio-temporal context variations and their relationships with moving objects. We demonstrate the expressive power of our model by using Beijing taxi trajectory data to show that our approach is effective in representing traffic relationships for analysis.	cluster analysis;computation;computational model;consumability;data mining;database;entity–relationship model;erdős–rényi model;neural coding;numerical analysis;refinement (computing);traffic analysis;unified model;voronoi diagram	Victor W. Chu;Raymond K. Wong;Wei Liu;Fang Chen;Chang-Shing Perng	2014	2014 IEEE International Conference on Services Computing	10.1109/SCC.2014.34	traffic generation model;simulation;geography;data mining;database	DB	-35.59893306541123	-3.075441846659832	114423
c5da44ddf57ebc7af9a879d68428b6f0202e07dc	semi-automatic metadata generation workflow for developing a continuing education resource repository	linked data;metadata;library and information science;extensible markup language xml;repository;continuing education;semi automatic metadata;metadata generator;semantic web;web ontology language owl;open source	This paper presents a high-level conceptualized workflow for the development of a repository related to continuing educational resources for the metadata professions. One of the major challenges facing cataloging and metadata communities concerns current developments and emerging trends in standards and technologies for managing digital information. Through utilization and integration of available open-source semiautomatic metadata generation tools, we attempted to design a repository that maximizes the selfsufficiency of the system components while also generating high quality metadata records for metadata resources. We also attempted to design a workflow that will permit the repository to serve its primary function as a continually updated warehouse of material that will constitute a centralized resource of continuing education needs of metadata.	semiconductor industry	Jung-ran Park;Andrew Brenza	2015		10.1007/978-3-319-27974-9_24	geospatial metadata;semantic grid;computer science;semantic web;linked data;database;metadata;world wide web;data element;meta data services;information retrieval;data mapping;metadata repository;data dictionary	HPC	-42.28754603901348	2.755335478510789	114601
19857b21fd8fbabfd55e4f633adaaf7cd61fe68d	poster reception - a meta-provenance service to infer context from provenance data of distributed entities	user needs;contextual information;forecasting model;satisfiability;data mining;distributed computing system;scientific applications;overlapping communication and computation;performance modeling	Provenance management has become an integral part of many large-scale distributed computing systems. Tracking the history of data and its usage has led to better understanding of system requirements as well as user needs. Still, the need for an intelligent service that matches the system requirements with user needs is not satisfied. We propose a meta-provenance service that infers context from the provenance information of distributed entities and uses this contextual information to satisfy user needs. We describe our meta-provenance framework by way of describing its implementation in the Calder system. The Calder streaming system enables dynamic invocation of forecast models in LEAD by using a distributed mesh of data mining agents. The meta-provenance service enables sophisticated mapping of user queries from the LEAD portal down to the set of few data mining agents that execute them. Also our meta-provenance service can work at multiple levels of contextual granularity.	data mining;distributed computing;entity;floor and ceiling functions;requirement;system requirements	Nithya N. Vijayakumar;Beth Plale	2006		10.1145/1188455.1188597	computer science;operating system;data mining;database;programming language;world wide web;satisfiability	HPC	-36.02424835779613	2.723780442088668	114639
1f601ecf59103d49d4bf37bc1b26397923a95ea6	time-bound analytic tasks on large datasets through dynamic configuration of workflows	user needs;semantic workflows;performance;semantics;software engineering;workflows;wings;oodt	Domain experts are often untrained in big data technologies and this limits their ability to exploit the data they have available. Workflow systems hide the complexities of high-end computing and software engineering by offering pre-packaged analytic steps combined into multi-step methods commonly used by experts. A current limitation of workflow systems is that they do not take into account user deadlines: they run workflows selected by the user, but take their time to do so. This is impractical when large datasets are at stake, since users often prefer to see an answer faster even if it has lower precision or quality. In this paper, we present an extension to workflow systems that enables them to take into account user deadlines by automatically generating alternative workflow candidates and ranking them according to performance estimates. The system makes these estimates based on workflow performance models created from workflow executions, and uses semantic technologies to reason about workflow options. Possible workflow candidates are presented to the user in a compact manner, and are ranked according to their runtime estimates. We have implemented this approach in the WOOT system, which combines and extends capabilities from the WINGS semantic workflow system and the Apache OODT Object Oriented Data Technology and workflow execution system.	big data;oodt;software engineering	Yolanda Gil;Varun Ratnakar;Rishi Verma;Andrew F. Hart;Paul M. Ramirez;Chris Mattmann;Arni Sumarlidason;Samuel L. Park	2013		10.1145/2534248.2534257	workflow;performance;computer science;operating system;data mining;database;distributed computing;semantics;windows workflow foundation;world wide web;workflow management system;workflow engine;workflow technology	DB	-36.22074055754382	0.17569513111156856	114656
d76e247f0b1bf487093528650f450c4442207a84	a knowledge discovery framework for spatiotemporal data mining.	spatiotemporal moving pattern;data mining;pattern discovery;real world application;ubiquitous computing;spatiotemporal knowledge discovery;discovery framework;spatiotemporal data mining;knowledge discovery	With the explosive increase in the generation and utilization of spatiotemporal data sets, many research efforts have been focused on the efficient handling of the large volume of spatiotemporal sets. With the remarkable growth of ubiquitous computing technology, mining from the huge volume of spatiotemporal data sets is regarded as a core technology which can provide real world applications with intelligence. In this paper, we propose a 3-tier knowledge discovery framework for spatiotemporal data mining. This framework provides a foundation model not only to define the problem of spatiotemporal knowledge discovery but also to represent new knowledge and its relationships. Using the proposed knowledge discovery framework, we can easily formalize spatiotemporal data mining problems. The representation model is very useful in modeling the basic elements and the relationships between the objects in spatiotemporal data sets, information and knowledge.	context-aware network;data mining;information;multitier architecture;personalization;real-time clock;real-time transcription;smart objects;spatiotemporal database;ubiquitous computing	Jun-Wook Lee;Yong-Joon Lee	2006	JIPS	10.3745/JIPS.2006.2.2.124	computer science;data science;data mining;ubiquitous computing;k-optimal pattern discovery	ML	-35.55561621345089	-3.5457680994340324	115521
2dbcfa8da1dcbd45d77c2945276d54cf06fd2251	provenance tracking and querying in the virolab virtual laboratory	query language;data repositories;complex query languages;database management systems;query formulation;scientific domain;complex query construction;virtual laboratory;universiteitsbibliotheek;ontologies artificial intelligence;data model;virolab;provenance modeling;provenance tracking;query formulation database management systems grid computing ontologies artificial intelligence;data repositories provenance tracking provenance querying virolab virtual laboratory complex query construction provenance records ontologies provenance modeling scientific domain complex query languages;ontologies;provenance querying;virolab virtual laboratory;virolab provenance tracking provenance querying ontologies;provenance records;laboratories ontologies monitoring grid computing database languages data models instruments engines computer science application software;grid computing	We present an approach to provenance tracking and querying which enables end-users to construct complex queries over provenance records. The use of ontologies for modeling provenance, data and applications enables query construction in an end-user oriented manner, i.e. by using terms of the scientific domain familiar to end users, instead of complex query languages. In addition, our ontologies contain mappings to underlying data models and sources. This allows to construct queries over provenance which additionally explore the structure of data items in the provenance records (e.g. experiment input or output data), combining requests to provenance and data repositories.	data model;ontology (information science);query language	Bartosz Balis;Marian Bubak;Michal Pelczar;Jakub Wach	2008	2008 Eighth IEEE International Symposium on Cluster Computing and the Grid (CCGRID)	10.1109/CCGRID.2008.83	data model;computer science;ontology;database;world wide web;information retrieval;query language;grid computing	DB	-37.66475422909484	3.562370224473774	115854
5c790467c257d72f6a652a87490b99e3425b5550	detection of potential updates of authoritative spatial databases by fusion of volunteered geographical information from different sources		A continuous update of authoritative spatial databases is highly demanding task in both aspects, technical and financial. In the same time, alternative modalities to collect content, in particular spatial content, have achieved a certain maturity and must be considered as they may leverage the cost of updating authoritative spatial databases (Al-Bakri, 2010). This alternative data known as Volunteered Geographical Information – VGI (Goodchild, 2007) is easy available and is being collected in almost every moment somewhere in the world.	capability maturity model;spatial database;volunteered geographic information	Stefan S. Ivanovic;Ana-Maria Olteanu Raimond;Sébastien Mustière;Thomas Devogele	2015			data mining;information retrieval;fusion;geography	DB	-38.41251986483713	-4.212125793285697	115986
77b8af108418ffe7508846070ca127fcc9fbe3f3	beyond ngs data sharing and towards open science		Biosciences have been revolutionized by next generation sequencing (NGS) technologies in last years, leading to new perspectives in medical, industrial and environmental applications. And although our motivation comes from biosciences, the following is true for many areas of science: published results are usually hard to reproduce either because data is not available or tools are not readily available, which delays the adoption of new methodologies and hinders innovation. Our focus is on tool readiness and pipelines availability. Even though most tools are freely available, pipelines for data analysis are in general barely described and their configuration is far from trivial, with many parameters to be tuned. In this paper we discuss how to effectively build and use pipelines, relying on state of the art computing technologies to execute them without users need to configure, install and manage tools, servers and complex workflow management systems. We perform an in depth comparative analysis of state of the art frameworks and systems. The NGSPipes framework is proposed showing that we can have public pipelines ready to process and analyse experimental data, produced for instance by high-throughput technologies, but without relying on centralized servers or Web services. The NGSPipes framework and underlying architecture provides a major step towards open science and true collaboration in what concerns tools and pipelines among computational biology researchers and practitioners. We show that it is possible to execute data analysis pipelines in a decentralized and platform independent way. Approaches like the one proposed are crucial for archiving and reusing data analysis pipelines at medium/long-term. NGSPipes framework is freely available at http://ngspipes.github.io/.	archive;centralized computing;communications satellite;computational biology;high-throughput computing;next-generation network;pipeline (computing);qualitative comparative analysis;throughput;web service	Bruno Dantas;Calmenelias Fleitas;Alexandre P. Francisco;José Simão;Cátia Vaz	2016	CoRR		computer science;data science;operating system;data mining;distributed computing;world wide web	Comp.	-40.72867790121879	-0.8196434767984747	116023
721abed80665e5d2d134b50f7572e00eb9d18f80	distributed semantic web data management in hbase and mysql cluster	pattern clustering;distributed database;lehigh university benchmark distributed semantic web data management hbase mysql cluster data resources machine interpretable semantic descriptions metadata interconnection semantic web w3c resource description framework rdf cloud computing relational database clustering technologies;sql cloud computing distributed processing meta data pattern clustering relational databases semantic web;sql;distributed processing;performance;distributed semantic web data management;data management;machine interpretable semantic descriptions;lehigh university benchmark;relational database;resource description framework;data resources;data storage;mysql cluster;w3c resource description framework;query evaluation;pattern matching;hbase;distributed databases;query;semantic web;mysql cluster semantic web cloud computing distributed database sparql sql rdf query performance scalability hbase;clustering algorithms;metadata interconnection;meta data;relational database clustering technologies;sparql;relational databases;scalability;resource description framework pattern matching distributed databases clustering algorithms cloud computing;rdf;cloud computing	Various computing and data resources on the Web are being enhanced with machine-interpretable semantic descriptions to facilitate better search, discovery and integration. This interconnected metadata constitutes the Semantic Web, whose volume can potentially grow the scale of the Web. Efficient management of Semantic Web data, expressed using the W3C's Resource Description Framework (RDF), is crucial for supporting new data-intensive, semantics-enabled applications. In this work, we study and compare two approaches to distributed RDF data management based on emerging cloud computing technologies and traditional relational database clustering technologies. In particular, we design distributed RDF data storage and querying schemes for HBase and MySQL Cluster and conduct an empirical comparison of these approaches on a cluster of commodity machines using datasets and queries from the Third Provenance Challenge and Lehigh University Benchmark. Our study reveals interesting patterns in query evaluation, shows that our algorithms are promising, and suggests that cloud computing has a great potential for scalable Semantic Web data management.	algorithm;apache hbase;benchmark (computing);cloud computing;cluster analysis;computer data storage;data-intensive computing;database schema;mysql cluster;relational database;resource description framework;sparql;sql;scalability;semantic web;triplestore;world wide web	Craig Franke;Samuel Morin;Artem Chebotko;John Abraham;Pearl Brazier	2011	2011 IEEE 4th International Conference on Cloud Computing	10.1109/CLOUD.2011.19	cwm;semantic computing;web modeling;data web;semantic grid;web standards;relational database;computer science;sparql;simple knowledge organization system;semantic web;rdf;social semantic web;linked data;data mining;semantic web stack;database;distributed database;information retrieval;semantic analytics;data mapping	DB	-37.03122852110033	1.629499608914498	116034
0188aceaa8903799c2d81d56901db6f8277e1765	cloud-dew computing support for automatic data analysis in life sciences		In this paper we show how the technologies associated with the evolution of Cloud computing to Dew computing can contribute to the advancing scientific computational productivity through automation. In the current big data paradigm developments, there is growing trend towards automation of data mining and other analytical processes involved in data science to increase productivity of associated applications. There are already several efforts to create automated data science platforms. However, these platforms are prevalently oriented towards business and engineering application domains. This paper addresses the automatic data analysis enabled by Cloud-Dew computing in the context of the life-science sector, in particular, in two application domains: breath gas analysis and brain damage restoration.	application domain;big data;categorization;circuit restoration;cloud computing;data mining;data science;programming paradigm	Peter Brezany;Thomas Ludescher;Thomas Feilhauer	2017	2017 40th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)	10.23919/MIPRO.2017.7973450	automation;semantic computing;data science;data mining;cloud computing;algorithm design;utility computing;big data;dew;computer science	DB	-41.548110666975695	-1.869048625879887	116147
65aa1dfb321d673e6bcd905634841887d1eff5cd	development of a complex geospatial/rf design model in support of service volume engineering design	design model;engineering design;nextgen;rf design;automatic dependent surveillance broadcast ads b system;core software;geospatial;service volume	Today’s National Airspace System (NAS) is managed using an aging surveillance radar system. The radar technology is not adequate to sustain the support of aviation growth and cannot be adapted to use 21st century technologies Therefore, FAA has begun to implement the Next Generation Air Transportation System (NextGen) that would transform today’s aviation and ensure increased safety and capacity. The first building block of the NextGen system is the implementation of the Automatic Dependent SurveillanceBroadcast (ADS-B) system. One of the most important design aspects of the ADS-B program is the design of the terrestrial radio station infrastructure. This design determines the layout of the terrestrial radio stations throughout the United States and is optimized to meet system performance, safety and security. Designing this infrastructure to meet system requirements is at the core of Service Volume (SV) Engineering. In this paper, the authors present a complex Geospatial/RF design ER Development modelbased ADS-B SV Engineering design that captures radio sites layout and configuration parameters. CORE software is selected to implement the model-based SV Engineering environment. DOI: 10.4018/978-1-4666-2154-1.ch005	automatic dependent surveillance – broadcast;engineering design process;radar;radio broadcasting;radio frequency;requirement;system requirements;systemverilog;terrestrial television	Erton S. Boci;Shahram Sarkani;Thomas A. Mazzuchi	2011	IJITN	10.4018/jitn.2011010105	simulation;radio-frequency engineering;telecommunications;geospatial analysis;operating system;computer security;engineering design process;computer network	Mobile	-45.22367258008875	-5.6097466294435225	116174
b6b1b0632eb9d4ab1427278f5e5c46f97753c73d	generalização cartográfica automatizada para um banco de dados cadastral	tese doutorado	Potential users of Geographic Information Systems (GIS), among them, private companies and public institutions, have different necessities in terms of quality, quantity and type of data stored in their spatial database, especially in the development of their land planning and management. Because of the recent computerization of city halls, Geographic Information Systems represent one of the most important tools to manipulate cadastral data. Cadastral Cartography has benefited from this technology, since it needs mapping in different scales which complies with the prerogatives of Multipurpose Technical Cadastre and territorial management. Currently, automated cartographic generalization is one of the most utilized techniques in scale transforming of geo-referenced geographic data. In this context, this thesis aimed at developing methods of cartographic generalization using GIS. Thus, generalization models were generated, evaluated, and presented through different criteria. Among these criteria, the structure of digital data storage, the effectiveness of recovery operations in the generalization process, and the necessity of a spatial perception to apply the operations. This study used Criciúma’s cadastral cartographic base maps (scale 1:5.000, year 2003). The method was applied through the following steps: evaluation of the scientific and technical knowledge development in cartographic generalization, development of automated cartographic generalization models, applying of generalization processes, generation of a multi-scale spatial data base (1:10.000 and 1:25.000), evaluation of the geometric and topological quality of derived data and, finally, validation of the methodology as a support to territorial planning and management.	cartographic generalization;cartography;computer data storage;digital data;geographic information system;map;spatial database;unified model	Mariane Alves Dal Santo	2007			cartography;geography;cadastre	DB	-38.328853502157436	-3.4080284589202168	116217
689be8dd54e9122d6d11991ac4a4b0f80038c742	seamless integration of data mining with dbms and applications	seamless integration;data mining;database management;association rule mining;data analysis;temporal database;machine learning;fouille donnee;data mining algorithm;systeme gestion base donnee;learning artificial intelligence;sistema gestion base datos;database management system;support function;apprentissage intelligence artificielle	Data mining has been widely recognized as a powerful tool for exploring added value from data accumulated in the daily operations of an organization. A large number of data mining algorithms have been developed during the past decade. Those algorithms can be roughly divided into two groups. The fist group of techniques, such as classification, clustering, prediction and deviation analysis, has been studied for a long time in machine learning, statistics, and other fields. The second group of techniques, such as association rule mining, mining in spatial-temporal databases and mining from the Web, addresses problems related to large amounts of data. Most classical algorithms in the first group assume that the data to be mined is somehow available in memory. Although initial effort in data mining has concentrated on making those algorithms scalable with respect to large volume of data, most of those scalable algorithms, even developed by database researchers, are still stand-alone. It is often assumed that data is available in desired forms, without considering the fact that most organizations store their data in databases managed by database management systems (DBMS). As such, most data mining algorithms can only be loosely coupled with data infrastructures in organizations and are difficult to infuse into existing mission-critical applications. Seamlessly integrating data mining techniques with database applications and database management systems remains an open problem. #R##N##R##N#In this paper, we propose to tackle the problem of seamless integration of data mining with DBMS and applications from three directions. First, with the recent development of database technology, most database management systems have extended their functionality in data analysis. Such capability should be fully explored to develop DBMS-awre data mining algorithms. Ideally, data mining algorithms can be fully implemented using DBMS supported functions so that they become database application themselves. Second, major difficulties in integrating data mining with applications are algorithm selection and parameter setting. Reducing or eliminating mining parameters as much as possible and developing automatic or semi-automatic mining algorithm selection techniques will greatly increase the application friendliness of data mining systems. Lastly, standardizing the interface among databases, data mining algorithms and applications can also facilitate the integration to certain extent.	data mining;seamless3d	Hongjun Lu	2001		10.1007/3-540-45357-1_3	concept mining;data modeling;support function;web mining;text mining;association rule learning;intelligent database;computer science;artificial intelligence;data science;data administration;operating system;database model;machine learning;data warehouse;data mining;database;temporal database;data pre-processing;data stream mining;data analysis;computer security;database design	ML	-35.44533618162255	-1.0284700342740027	116305
09d95340bb396eade8132e83a498486eb4d710f5	agent based frequent set meta mining: introducing emads	distributed data;user agent;liverpool;agent based;data mining;scattered data;association rule mining;repository;frequent itemset;university	In this paper we: introduce EMADS, the Extendible Multi-Age nt Data mining System, to support the dynamic creation of communiti es of data mining agents; explore the capabilities of such agents and demonst rate (by experiment) their application to data mining on distributed data. Although, E MADS is not restricted to one data mining task, the study described here, for the sake o f brevity, concentrates on agent based Association Rule Mining (ARM), in particular what we refer to as frequent set meta mining (or Meta ARM). A full description of our proposed Meta ARM model is presented where we describe the concept of Meta A RM and go on to describe and analyse a number of potential solutions in th e context of EMADS. Experimental results are considered in terms of: the number of data sources, the number of records in the data sets and the number of attribute s represented.	arm architecture;agent-based model;algorithm;association rule learning;brute force;centralized computing;data mining;experiment;extensibility;multi-agent system;peer-to-peer;privacy;response time (technology)	Kamal Ali Albashiri;Frans Coenen;Paul H. Leng	2008		10.1007/978-0-387-09695-7_3	computer science;data science;data mining;database	ML	-36.70926741359582	0.21219235441032305	116574
29e1bd6d8fa63e7ef0b76679502c9d87b405242e	managing the knowledge contained in electronic documents: a clustering method for text mining	information resources;information resources data mining pattern clustering intranets;pattern clustering;text mining;intranets;knowledge management;knowledge extraction;information overload;knowledge management clustering methods text mining data mining portals databases prototypes frequency refining instruments;data mining;clustering method;unstructured data textual documents knowledge management web portal knowledge extraction knowledge discovery in database kdd web vertical corporate portal intranets electronic documents clustering method data mining text mining;knowledge discovery in database	The huge amount of unstructured data available on the Web and the intranets creates an information overloading problem. So, managing the knowledge contained in the textual documents is an important problem of Knowledge Management. Knowledge Extraction from collections of data is possible by Knowledge Discovery in Database (KDD), an interactive and iterative process focused on the exploration of data to discover new and interesting patterns within them. The fundamental phase of KDD process is Data Mining if data are in structured form and Text Mining when they are unstructured. This paper describes a prototype of a vertical corporate portal that implements a KDD process for knowledge extraction from unstructured data contained in textual documents. Text mining is realized through a clustering method that produces a partition of a set of documents on the basis of their contents characterized through the frequency of the words.	cluster analysis;text mining	S. Iritano;Massimo Ruffolo	2001		10.1109/DEXA.2001.953103	concept mining;knowledge base;web mining;text mining;software mining;computer science;data science;information overload;data mining;database;knowledge extraction;data stream mining;world wide web	ML	-39.38360456015066	2.7166462050817004	116595
d19685b53e5acc53d7e14c2c447432dc8e7150d5	modeling and processing big data of power transmission grid substation using neo4j		Data sizes in power transmission grid have increased rapidly, which results in challenges. These data are large in volume; they are generated fast and in different format, and come from various sources such as electrical substations. Traditional relational databases are inadequate in terms of response time and have impact on performance when applied to very large data sets, and also make this database difficult to evolve according to business needs. To address this shortcoming, the Big Data implementations are leveraging new technologies such as NoSQL data stores. This research paper aims and tries to improve this process by modeling and processing those data using Neo4j database, and presents modeling and processing the data of power transmission grid substation which has two power transformers, and then adding a new power transformer to simulate the evolving feature of Neo4j database according to the business needs. © 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Conference Program Chairs.	big data;business requirements;data store;neo4j;nosql;relational database;response time (technology);simulation;traction substation;transformer;transformers	Arbër Perçuku;Daniela Minkovska;Lyudmila Stoyanova	2017		10.1016/j.procs.2017.08.276		DB	-34.165329614725515	-0.41908555145426335	116723
c0e403da154272407e850a73a36fe59e2b5bb813	a virtual restoration approach for ancient plank road using mechanical analysis with precision 3d data of heritage site	precision measurement;virtual restoration;three dimensional modeling;mechanical analysis;ancient plank road	1 Architecture Department, Chang’an University, No. 161, Chang’an Road, Xi’an 710061, China 2 Architecture Department, Xi’an University of Architecture & Technology, No. 13, Yanta Road, Xi’an 710055, China 3 School of Remote Sensing and Information Engineering, Wuhan University, No. 129, Luoyu Road, Wuhan 430079, China 4 School of Internation Software, Wuhan University, No. 129, Luoyu Road, Wuhan 430079, China; shwang@whu.edu.cn 5 Shaanxi Bureau of Cultural Heritage, No. 193, Yanta West Road, Xi’an 710061, China; hongjunyang123@126.com * Correspondence: chensiliang@chd.edu.cn (S.C.); huqw@whu.edu.cn (Q.H.); Tel.: +86-29-8553-6075 (S.C.) Academic Editors: Rosa Lasaponara, Gonzalo Pajares Martinsanz and Prasad S. Thenkabail Received: 6 August 2016; Accepted: 27 September 2016; Published: 9 October 2016 Abstract: The ancient plank road is a creative building in the history of Chinese ancient traffic through cliffs. In this paper, a virtual restoration approach for ancient plank road using mechanical analysis with precision 3D data of current heritage site is proposed. Firstly, an aero photogrammetry with multiple view images from Unmanned Aerial Vehicle (UAV) imaging system is presented to obtain the 3D point cloud of ancient plank roads, which adopts a density image matching and aerial triangulation processing. In addition, a terrestrial laser scanner is integrated to obtain detail 3D data of the plank road. Secondly, a mechanical analysis method based on the precision 3D data of the current plank roads is proposed to determine their forms and restore each of their components with detail sizes. Finally, all components and background scene were added to the existing model to obtain a virtual restoration model, which indicates that it is effective and feasible to achieve a three-dimensional digital and virtual restoration of ancient sites. The Chiya Plank Road is taken as a virtual restoration example with the proposed approach. The restored 3D model of the ancient plank can be widely used for digital management, research, and visualization of ancient plank roads.	3d scanner;aerial photography;circuit restoration;image registration;information engineering;photogrammetry;point cloud;polygonal modeling;silk road;structural analysis;terrestrial television;unmanned aerial vehicle;windows aero	Siliang Chen;Qingwu Hu;Shaohua Wang;Hongjun Yang	2016	Remote Sensing	10.3390/rs8100828	geology;computer vision;photogrammetry;point cloud;artificial intelligence;remote sensing;plank;visualization;triangulation (social science)	Visualization	-46.131208347537914	-9.134819447767976	116784
2daea5a69a46ae3f69acad3b32b5aa73a179a602	big data mining using public distributed computing		Public distributed computing is a type of distributed computing in which so-called volunteers provide computing resources to projects. Research show that public distributed computing has the required potential and capabilities to handle big data mining tasks. Considering that one of the biggest advantages of such computational model is low computational resource costs, this raises the question of why this method is not widely used for solving such today’s computational challenges as big data mining. The purpose of this paper is to overview public distributed computing capabilities for big data mining tasks. The outcome of this paper provides the foundation for future research required to bring back attention to this low-cost public distributed computing method and make it a suitable platform for big data analysis. DOI: http://dx.doi.org/10.5755/j01.itc.47.2.19738	big data;data mining;distributed computing	Albertas Jurgelevicius;Leonidas Sakalauskas	2018	ITC	10.5755/j01.itc.47.2.19738	computer science;computational resource;big data;cloud computing;distributed computing	ML	-47.49411259885923	0.20395820044887109	117151
e340a6ddca75309ad74d6f7b8bf661a02eb811cf	energy-aware base stations: the effect of planning, management, and femto layers	eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems	1 Computer Engineering and Telecommunications, University of Thessaly, 38221 Volos, Greece 2 School of Science and Technology, International Hellenic University, Thermi, Greece 3 DIET Department, University of Roma-La Sapienza, Via Eudossiana 18, Rome, Italy 4Mobile Communications Department, Eurecom, Sophia Antipolis, 450 Route des Chappes, 06410 Biot, France 5 Telecommunication Networks Group, Politecnico di Torino, Corso Duca degli Abruzzi 24, 10129 Torino, Italy	algorithm;association rule learning;computer engineering;linear algebra;load management;quality of service;representation oligonucleotide microarray analysis	George Koutitas;Luca Chiaraviglio;Delia Ciullo;Michela Meo;Leandros Tassiulas	2014	J. Electrical and Computer Engineering	10.1155/2014/190586	simulation;telecommunications;computer science;engineering;electrical engineering;computer network	DB	-45.241613478724304	-7.398757256846259	117280
1a060b03ad80fb67291b42ff08a890e3f2ed993b	toward a statistical data integration environment: the role of semantic metadata	spatial dimension;statistical data;semantic metadata;rdf data cube vocabulary;mashup;temporal dimension;data integration	In most government and business organizations alike, statistical data provides the foundation for strategic planning and for the management of operations. In this context, the use of increasingly abundant statistical data available on the web creates new opportunities for interesting applications and facilitates more informed decision-making. For the majority of end users, however, viable means to explore statistical data sets available on the web are still scarce. Gathering and relating statistical data from multiple sources is hence typically a tedious manual process that requires significant technical expertise. Data that is being published with associated semantics, using standards such as the W3C RDF Data Cube Vocabulary, lays the foundation to overcome such limitations. In this paper, we develop a semantic metadata repository that describes each statistical data set and develop mechanisms for the interconnection of data sets based on their metadata. Finally, we support users in exploring data sets through interactive mashups that facilitate data integration, comparisons, and visualization.	data cube;interconnection;mashup (web application hybrid);metadata repository;vocabulary	Ba-Lam Do;Tuan-Dat Trinh;Peb Ruswono Aryan;Peter Wetz;Elmar Kiesling;A Min Tjoa	2015		10.1145/2814864.2814879	spatial data infrastructure;data transformation;semantic grid;computer science;data element definition;data mining;database;metadata;data element;information retrieval;data mapping;metadata repository	DB	-37.93009228089246	1.4807890774723829	117687
58151e1a0c36999beb83c3b07a01843d0b889712	searching workflows with hierarchical views	business process;efficient keyword search engine;experimental evaluation;different granularity;hierarchical view;xml data;diverse application;dimensional viewing plane;concise search result;workflow repository;workflow hierarchy	Workflows are prevalent in diverse applications, which can be scientific experiments, business processes, web services, or recipes. With the dramatically growing number of workflows, there is an increasing need for people to search a workflow repository using keywords and to retrieve the relevant ones. A workflow hierarchy is a three dimensional object containing multiple abstraction views of different granularity on the same workflow. This unique structure poses a new set of challenges compared to keyword search on tree or graph structures typically found in relational or XML data. In this paper, we define an informative, self-contained and concise search result on workflows to be a projection of a workflow hierarchy on a two dimensional viewing plane inferred from user queries. We then design and develop an efficient keyword search engine for workflows. Experimental evaluation demonstrates the effectiveness of our approach.	business process;experiment;information;search algorithm;web search engine;web service;xml	Ziyang Liu;Qihong Shao;Yi Chen	2010	PVLDB	10.14778/1920841.1920958	computer science;data mining;database;workflow management system;information retrieval;workflow engine;workflow technology	DB	-35.76725891437945	3.172546120395255	117753
11da9ae15aa44c443df7688724e68396a3ab1aea	hancock: a language for extracting signatures from data streams	stock market;data stream;data mining	""""""" # %$ '&( ) *+ , ( $ . / 0*1 32 4 , ,5 6, 87!$ 93:; , 5 6! ) ! < = = < > = 6, , 0 @?A B(C, / =D E 2 . F 2 $ 6G C ! EB 4 H B A 2 C JI0:J:JKL 7!$, M ' """"?+ & E 9ANO6, = 6, =/=.PQ $ E6 2, 6, &( = = 2R*S & =/= ,5> J $ . @P ,$ ( C% 6, P ) ?T&( ,53$, 2 >2 +6 H?O $, A A 6, A % E 2 / <UV = = 2QWVX Y Z [EY \ ] W_^ XE[@`J$, 0 6, )$, G2 =.P / 5' % 9 a b E =!P! H C ?A +6G c $ 2 = 5) dG= JUV = =/ 2 [ e/f ZGY WVg%X hE[@` *0 6 R i j=8 H 5 k2 $, 5 6, ,2 ?J . #lm %2 9M:+6, F /5 G H $ n*1 A 6 + $ 6, = / % """"*S $ """" *J6 / , E / , 6 $,5 6o 9 Kp 5 k*1 ! ,5q 5 , $ o >$, k&( L6, 5 6,=.Pr .s 2m&( $, k * 6 Q /s t * 6, k2 u vUV =<5 .5 &!P% n ( <2, P,`+ ,2# 6, %$, >&( + *b 5 G H $ n 4 / ! / UV6%$ G2 2 ) *+ = = / , `E9 lw 5 ) R ,$ > 5 G $ *S o .d, 2o 2, &, = xPk*1 3 ( *S 4 , 9 yA 7!$, ! =.P!C 6 P# """"2 z $,=. n .*8PR G2R 4 % ;9 I) ! EB{ R L2 4 / -_ ( .dG Q=/ 5 $, 5 k 2m L | ,$ , = =.P3 z ! p 5 , $ + 5 p =/ ,=.P!9~}@ """" 6, , ( CJ?A t2 &( Q 6, Q &, = i L ,$ ,5 5 G $ *8  4 4 ' G2u | ,=/ / L6 ?I0 , ! Bu 2,2% 6 &,= 9 a, 3 | ( Pt ,$ ( C~?A ! """"I0 ! EB4$, 5 > $ , , 5> | ,= <*8  6, F = '$, ,2 $, P%6 H?A C% 6 )=/ 5 $G 5 0 . =.*p J5 , =D G2i ,= 7%$, = =.P#?A = =; > 6, E )2 ' $ 9 1. INTRODUCTION  , =,2, F  b ) 7%$ , A *, 2 p 6G b= 5 ! , ~&( @?A """" ! . / 9ba, ~ | =/ C n r *, ! B 4 H B < , 0 *c&,$ P,H = =p 2 E )*1 G H / $  G H ! J *byA ,$ < / , CGyA = =~< , . @P!9 Appears in the  X ^ \ h h  e1Z%f [i^@ WS h4,e  WSt Z WxhEXEZGY WVe^ ZGY <M^ ZG]  hEX hEZ(\ hi^ Z """"Z(^ n/h  f!h'0e1[E\ ^  hEXEtY ZG#3Y W_Y#ke1Z e1Z%f"""	eb/n0;electronic signature;emoticon;hex	Corinna Cortes;Kathleen Fisher;Daryl Pregibon;Anne Rogers	2000		10.1145/347090.347094	computer science;data science;data mining;database;data stream mining	ML	-38.22615053136302	-5.556680528720096	117847
489732c56bf6c3ff6dc8ee74ae7339359ae570a1	agent based approach for searching, mining and managing enormous amounts of spatial image data	search and retrieval;agent based;software agent;data model;agent architecture;system architecture;content based image retrieval;geospatial data	Scientists and intelligence analysts are interested in quickly discovering new results from the vast amount of available geospatial data. The key issues that arise in this pursuit are how to cope with new and changing information and how to manage the steadily increasing amount of available data. This paper describes a novel agent architecture that has been developed and tested to address these issues by combining innovative approaches from three distinct research areas: software agents, georeferenced data modeling, and content-based image retrieval (CBIR). The overall system architecture is based on a multi-agent paradigm where agents autonomously search for images over the Internet, then convert the images to a vector used for use in searching and retrieval. Results show that this system is capable of significantly reducing the time and management effort associated with large amounts of image data.	agent architecture;content-based image retrieval;data modeling;internet;multi-agent system;programming paradigm;software agent;systems architecture	Paul J. Palathingal;Thomas E. Potok;Robert M. Patton	2005			agent architecture;data model;computer science;artificial intelligence;data science;geospatial analysis;software agent;data mining;world wide web;data retrieval;data architecture;systems architecture	AI	-35.54240204601273	-2.091325855248342	117971
9bfc64dae6873ea53e32f58f793c2560bc86e62d	multilingual extraction and mapping of dictionary entry names in business schema integration	warp10;schema integration;data model;term extraction;schema mapping;igreen;schema modeling;natural language processing	Being a research field for many years, natural language processing (NLP) has gained a lot of attention in recent times due to the quality of translation software like Google Translator or Babylon. In the context of the iGreen project, we are developing a schema integration service (working title: Warp 10), which helps to generate business transformations from individual business schemata. To match the different schemas natural language processing plays an important role. Information coming from user input or files is extracted and mapped to a canonical data model (CDM) based on the CCTS standard. In this paper, we illustrate the use of NLP for the extraction of dictionary entry names (DENs) and indicate some of the problems of NLP and term extraction. Furthermore, we describe the NLP-supported mapping of DENs and outline the problems and approaches in a multilingual setting.	babylon;canonical model;conceptual schema;data model;dictionary;google translate;natural language processing;terminology extraction	Michael Dietrich;Dirk Weissmann;Jörg Rech;Gunther Stuhec	2010		10.1145/1967486.1967635	natural language processing;computer science;data mining;database	NLP	-42.531072740170366	4.110856498907894	118249
fe81b96e16632ed105a3f4170bd10e39e4961982	linking historical collections in an event-based ontology		PurposernrnrnrnrnThis study aims to explore a way of representing historical collections by examining the features of an event in historical documents and building an event-based ontology model.rnrnrnrnrnDesign/methodology/approachrnrnrnrnrnTo align with a domain-specific and upper ontology, the Basic Formal Ontology (BFO) model is adopted. Based on BFO, an event-based ontology for historical description (EOHD) is designed. To define events, event-related vocabularies are taken from the Library of Congress’ event types (2012). The three types of history and six kinds of changes are defined.rnrnrnrnrnFindingsrnrnrnrnrnThe EOHD model demonstrates how to apply the event ontology to biographical sketches of a creator history to link event types.rnrnrnrnrnResearch limitations/implicationsrnrnrnrnrnThe EOHD model has great potential to be further expanded to specific events and entities through different types of history in a full set of historical documents.rnrnrnrnrnOriginality/valuernrnrnrnrnThe EOHD provides a framework for modeling and semantically reforming the relationships of historical documents, which can make historical collections more explicitly connected in Web environments.		Qing Zou;Eun G. Park	2018	Digital Library Perspectives	10.1108/DLP-02-2018-0005	data mining;basic formal ontology;ontology;originality;upper ontology;computer science	HCI	-42.45461431637237	2.1767209201748994	118537
41a939ab82664597dd724306a28fead32ec95bb2	mining usage patterns from a repository of scientific workflows	hierarchical clustering;graph clustering;scientific workflow;design practice;graph mining;process schema mining;scientific workflows;data analysis;learning by example;workflow management system;subdue;in silico	In many experimental domains, especially e-Science, workflow management systems are gaining increasing attention to design and execute in-silico experiments involving data analysis tools. As a by-product, a repository of workflows is generated, that formally describes experimental protocols and the way different tools are combined inside experiments. In this paper we describe the use of the SUBDUE graph clustering algorithm to discover sub-workflows from a repository. Since sub-workflows represent significant usage patterns of tools, the discovered knowledge can be exploited by scientists to learn by-example about design practices, or to retrieve and reuse workflows. Such a knowledge, ultimately, leverages the potential of scientific workflow repositories to become a knowledge-asset. A set of experiments is conducted on the my Experiment repository to assess the effectiveness of the approach.	algorithm;cluster analysis;e-science;experiment	Claudia Diamantini;Domenico Potena;Emanuele Storti	2012		10.1145/2245276.2245307	computer science;data science;data mining;clustering coefficient;database;hierarchical clustering;data analysis;world wide web;workflow management system;workflow engine;statistics;workflow technology	DB	-40.34374221962086	1.1844798878867437	118858
e0f558d6742eff0cebef52231ea17de57f30d29f	data mining and its applications for knowledge management: a literature review from 2007 to 2012	knowledge management;data mining	Data mining is one of the most important steps of the knowledge discovery in databases process and is considered as significant subfield in knowledge management. Research in data mining continues growing in business and in learning organization over coming decades. This review paper explores the applications of data mining techniques which have been developed to support knowledge management process. The journal articles indexed in ScienceDirect Database from 2007 to 2012 are analyzed and classified. The discussion on the findings is divided into 4 topics: (i) knowledge resource; (ii) knowledge types and/or knowledge datasets; (iii) data mining tasks; and (iv) data mining techniques and applications used in knowledge management. The article first briefly describes the definition of data mining and data mining functionality. Then the knowledge management rationale and major knowledge management tools integrated in knowledge management cycle are described. Finally, the applications of data mining techniques in the process of knowledge management are summarized and discussed.	association rule learning;cluster analysis;data mining;database;design rationale;information systems research;knowledge management;learning organization;statistical classification;systems theory	Tipawan Silwattananusarn;Kulthida Tuamsuk	2012	CoRR		knowledge base;software mining;data management;computer science;knowledge management;data science;mathematical knowledge management;knowledge engineering;data mining;database;knowledge extraction;process mining	ML	-37.81839092863079	-7.093406272823012	118912
fbc7ca2ae99a9851c364bb0a1fb8b9c9484013a4	annotated domain ontologies for the visualization of heterogeneous manufacturing data		Manufacturing processes such as monitoring and controlling typically confront the user with a variety of heterogeneous data sources and systems. The cognitive efforts to summarize and combine the data from these different sources affect the user’s efficiency. Our goal is to support the user in his work task by integrating the data and presenting them in a more perceivable way. Hence, we introduce an approach in which different data sources are integrated in an annotated semantic knowledge base: our domain ontology. Based on this ontology, contextually relevant data for a specific work task is selected and embedded into a meta-visualization providing an overview of the data based on the user’s mental model. Two systems finally exemplify the usage of our approach.	activity recognition;documentation;embedded system;exemplification;information overload;information visualization;knowledge base;mental model;multi-touch;ontology (information science);proactive parallel suite	Rebekka Alm;Mario Aehnelt;Steffen Hadlak;Bodo Urban	2015		10.1007/978-3-319-20612-7_1	computer science;data mining;database;ontology-based data integration;world wide web	AI	-38.767312665474385	0.8037746599539182	118947
9d580d48d0232a98d9b00db87a074032c530cf0d	metadata on biodiversity: definition and implementation		"""SINP (Information system on nature and landscape) and ECOSCOPE (Observation for research on biodiversity data hub) are two distinct scientific infrastructures on biodiversity relying on different data sources and producers. Their main objective is to document and share information on biodiversity in France. INPN (https://inpn.mnhn.fr) is the reference information system for data related to nature. It manages and disseminates the reference data of the """"geodiversity and biodiversity"""" part of the SINP, and deliver the metadata and data to GBIF (Global Biodiversity Information Facility). For SINP and Ecoscope projects, working groups composed of scientific organisations have defined two compliant metadata profiles, also compliant with INSPIRE Directive, to describe data on this thematic. These profiles are implemented using existing metadata standards: ISO 19115/19139 (for geographic metadata) for SINP and EML (Ecological Metadata Language) and ISO 19115/19139 for ECOSCOPE. A mapping has also been processed between the two profiles, as well as several thesaurus for keywords and a classification system for taxonomic identification are used, so as to ensure interoperability between systems. The profiles are implemented in web applications for editing and managing data (GeoSource/GeoNetwork for SINP and an ad hoc application for ECOSCOPE). These applications allow the harvesting of metadata using OGC/CSW (Catalog Service for the Web) standard. Next steps will permit to increase metadata visibility through the automatization of webservices."""		Etienne Taffoureau	2016			ecoscope;ecological metadata language;metadata;geospatial metadata;metadata repository;interoperability;meta data services;database;information system;data mining;computer science	HPC	-41.83767807419715	0.5693180041791716	119055
51d94b3b25505237a4226135f8fa13759855fe57	evolution in storage and retrieval: the labstat data base and software system	software systems	The use of an integrated data bank for time series data was initially introduced at the Bureau of Labor Statistics two decades ago. The data bank has been available on line since LABSTAT was introduced in 1977. This paper summarizes some of the major developments in the data bank and access software and discusses some enhancements currently being considered.	database;software system	Gwendolyn L. Harllee	1983			computer science;data mining;database;world wide web;software system	DB	-41.6447407195317	-3.7996948495095686	119280
7d9871f011c974b7b403afaaabea1925de233d07	applying temporal databases to geographical data analysis	electrical capacitance tomography;image databases;application software;ski resort;temporal data models;data analysis geography electrical capacitance tomography tin spatial databases proposals application software geographic information systems image databases humans;human behavior;geographical data analysis;leisure industry;data analysis;temporal database;human displacements analysis;geographic information systems;spatial databases;temporal databases;humans;time geography;time geography methodology;tin;proposals;human behaviors;human displacements analysis temporal databases geographical data analysis human behaviors ski resort time geography methodology;data models;geographic information systems temporal databases leisure industry data models;geography	This paper reports an experience in which a temporal database was used to analyze the results of a survey on human behaviors and displacements in a ski resort. This survey was part of a broader study about the use of the resort’s infrastructure, based on the time-geography method ology. As such, the presented experience may be seen as an attempt to implement some concepts of the time-geography using temporal database technology. Throughout the paper, some shortcomings of current temporal data models regarding human displacements analysis are pointed out, and possible solutions are briefly sketched.	data model;dual-energy x-ray absorptiometry;interpolation;requirement;requirements analysis;succession;temporal database;time geography;unified modeling language	Marie-Christine Fauvet;S. Chardonnel;Marlon Dumas;Pierre-Claude Scholl;P. Dumolard	1999		10.1109/DEXA.1999.795229	simulation;computer science;data mining;database;temporal database;human behavior	DB	-34.74447566773063	-3.3233569460322236	119292
33dad099b342ff8a05148fd6e05d42ff9b16abcf	statistical programs at the university of north carolina	data processing;university of north carolina;fortran;contingency table;multiple regression	The Research Computation Center at the University of North Carolina has access to a UNIVAC 1105 general purpose digital computer for use in connection with data processing problems, theoretical studies, and computer research. With respect to data processing problems, three major statistical programs have been written:General Contingency Table Analysis for Questionnaire Data Analysis of Variance (ANOVA) Multiple Regression and Correlation  Some of the concepts and ideas in these programs are new and may be of interest to other computation centers. Hence they are described below. The programs were written in the Remington Rand UNICODE language. Thus it would not be difficult to translate them into any other algebraic language, such as ALGOL, FORTRAN, or IT.	algol;computation;computer;contingency table;fortran;linear algebra;rand index;univac 1105;unicode	Norman Bush	1961	Commun. ACM	10.1145/366105.366186	mathematics education;data processing;contingency table;computer science;linear regression;data science;programming language;statistics	Theory	-45.67593189163659	-3.4674511284841323	119726
3e4feab62820b3a9258a7755e0b0a9778373ae01	a quality study of the openstreetmap dataset for tehran	spatial data quality;vgi;fuzzy;open street map dataset	There has been enormous progress in geospatial data acquisition in the last decade. Centralized data collection, mainly by land surveying offices and local government agencies, has changed dramatically to voluntary data provision by citizens. Among a broad list of initiatives dealing with user generated geospatial information, OpenStreetMap (OSM) is one of the most famous crowd-sourced products. It is believed that the quality of collected information remains a valid concern. Therefore, qualitative assessment of OSM data as the most significant instance of volunteered geospatial information (VGI) is a considerable issue in the geospatial information community. One aspect of VGI quality assessment pertains to its comparison with institutionally referenced geospatial databases. This paper proposes a new quality metric for assessment of VGI accuracy and as well as for quality analysis of OSM dataset by evaluating its consistency with that of a reference map produced by Municipality of Tehran, Iran. A gridded map is employed and heuristic metrics such as Minimum Bounding Geometry area and directional distribution (Standard Deviational Ellipse), evaluated for both VGI and referenced data, are separately compared in each grid. Finally, in order to have a specific output as an integrated quality metric for VGI, its consistency with ground-truth data is evaluated using fuzzy logic. The results of this research verify that the quality of OSM maps in the study area is fairly good, although the spatial distribution of uncertainty in VGI varies throughout the dataset. OPEN ACCESS ISPRS Int. J. Geo-Inf. 2014, 3 751	centralized computing;crowdsourcing;data acquisition;database;fuzzy logic;ground truth;heuristic;openstreetmap;volunteered geographic information	Mohammad Forghani;Mahmoud Reza Delavar	2014	ISPRS Int. J. Geo-Information	10.3390/ijgi3020750	geography;data science;data mining;volunteered geographic information;cartography	AI	-42.0432119124237	-4.631292279366708	119754
d909ae7ae577bb6962e18e4d33366245eb26b3bb	big data quality assessment model for unstructured data		Big Data has gained an enormous momentum the past few years because of the tremendous volume of generated and processed Data from diverse application domains. Nowadays, it is estimated that 80% of all the generated data is unstructured. Evaluating the quality of Big data has been identified to be essential to guarantee data quality dimensions including for example completeness, and accuracy. Current initiatives for unstructured data quality evaluation are still under investigations. In this paper, we propose a quality evaluation model to handle quality of Unstructured Big Data (UBD). The later captures and discover first key properties of unstructured big data and its characteristics, provides some comprehensive mechanisms to sample, profile the UBD dataset and extract features and characteristics from heterogeneous data types in different formats. A Data Quality repository manage relationships between Data quality dimensions, quality Metrics, features extraction methods, mining methodologies, data types and data domains. An analysis of the samples provides a data profile of UBD. This profile is extended to a quality profile that contains the quality mapping with selected features for quality assessment. We developed an UBD quality assessment model that handles all the processes from the UBD profiling exploration to the Quality report. The model provides an initial blueprint for quality estimation of unstructured Big data. It also, states a set of quality characteristics and indicators that can be used to outline an initial data quality schema of UBD.		Ikbal Taleb;Mohamed Adel Serhani;Rachida Dssouli	2018	2018 International Conference on Innovations in Information Technology (IIT)	10.1109/INNOVATIONS.2018.8605945	data type;data mining;data domain;profiling (computer programming);big data;data integrity;unstructured data;data modeling;computer science;data quality	DB	-36.698959339854945	1.3522798270034009	120230
6e7c58179770a2e25f82d1d9aa2b3c0be0cc38bf	semantically enhancing multimedia data warehouses - using ontologies as part of the metadata		Data warehouses are versatile systems capable of storing and processing large quantities of data. They are most suited for aggregating and reporting. The data managed by these systems vary from simple, numeric data, to more complex, multimedia data. One of the domains in which multimedia data is intensively produced is medicine. We present a method for semantically enhancing the metadata stored in a medical multimedia data warehouse. This semantically rich environment will gain in autonomy, reducing the dependence on human intervention to resolve new, unforeseen queries. Furthermore, the use of the semantic relations defined in the ontology allows the system to speed up the execution of a query, by computing the results of new, unforeseen queries, from the fact data already stored in the data warehouse.	autonomy;information retrieval;level of measurement;metadata repository;ontology (information science);semantic web	Andrei Vanea;Rodica Potolea	2011			synonym ring;database;metadata;world wide web;information retrieval;metadata repository	DB	-37.73061811110408	3.1127706992625375	120323
b1101bdf02a8e74a2d3a4b96bbdb533ec6d9b2d5	earth system scientific data sharing mode based on adaptable global spatial grid	data sharing;cloud technology earth system scientific data sharing mode adaptable global spatial grid earth system science researches efficient data sharing system software environments unified spatial reference ess data data sharing;grid computing geographic information systems geophysics computing;spatial reference;adaptable global spatial grid agsg;geophysics computing;geographic information systems;grid computing;spatial reference data sharing adaptable global spatial grid agsg;earth helium abstracts encoding	An efficient data sharing system is vital to Earth System Science (ESS) researches. However, current systems mainly focus on standards, policies and software environments. None of them pay much attention to spatial references. Adaptable Global Spatial Grid (AGSG), was selected as a unified spatial reference for data sharing. Methods of data representation based on AGSG was introduced and the mode of data sharing based on AGSG was presented. Merits of easy reusing of ESS data, uniform requests, and transparent accessing of multi-sources data were shown in the mode. Potential extensions to the volunteer services of data sharing and cloud technology were discussed.	cloud computing;data (computing);earth system science;grid (spatial index);spatial reference system	Jieqing Yu;Lixin Wu;Xiaojing Li;Junyu Wang	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6351533	spatial data infrastructure;computer science;spatial reference system;theoretical computer science;data mining;database;geographic information system;grid computing	HPC	-38.3356549544392	-1.7637718129735058	120346
76a8d72d41660c14dab1e1f1794e4f7fc9601d31	selecting an end user programming language for dss development	user needs assessment;end user computing;decision support systems;architectural features;end user programming;software evaluation	A bstract Today managers and policy makers are confronted with an overwhelming range of choices of computer software to develop decision support systems (DSS). The authors argue that DSS language evaluation and selection should be a multi-step process involving most, if not all, of the following: The objectives of each of these activities are described , as well as specific procedures for accomplishing them. In addition, the authors discuss the usefulness of a multidisciplinary task force to accomplish the DSS language evaluation and selection process. Decision support systems (DSS) represent relatively new way of thinking about managerial use of computers. A decision support system is a computer-based information system that is designed to help managers in private corporations and policymakers in public sector organizations solve problems in relatively unstructured decision-making environments. Long-range and strategic planning, merger and acquisition analysis, policy formulation, policy evaluation, new product development, marketing mix planning , research and development, and portfolio management are a few areas where the DSS concept has been successfully applied [see 6, 9, 14 and 19]. Unstructured decision-making environments are those where the global problem is not well enough understood for a complete analytical description. A DSS is a system which provides computational and analytical support in situations where it is necessary to integrate judgement, experience , and insight of managerial or policy decision makers along with computer-supported modeling and presentation facilities. DSS focuses on achieving productivity improvements from managers and policymakers, rather than from the reduction of clerical and administrative costs. As used in this article, a DSS language is one very important example of the generic set of computer application development tools generally known as end user programming languages (or in some cases, fourth generation languages). Other examples include relational database facilities with powerful report generation and ad hoc inquiry facilities; general purpose statistical data analysis languages; and broad based graphics generation languages. While each of these four types of languages usually address different user needs, they often share some subset of similar capabilities and characteristics such as:-Integrated database management (sometimes relational)-User friendliness to nontechnicians-Both procedural and nonprocedural command structures-Interactive on-line utilization-Support of prototyping and adaptive development-Modest training requirements for end users-Easy debugging and intelligent default assumptions MIS Quarterly~December 1984 267 Selecting an End User Language Quantity of code required only a fraction of Cobol, Fortran, etc. Internal documentation generation support Understandable code for non-developers [5]. The methodology presented here …	cobol;computer;debugging;decision support system;documentation generator;end-user development;fortran;fourth-generation programming language;graphics;hoc (programming language);information system;new product development;online and offline;programming tool;relational database;requirement	C. Lawrence Meador;Richard A. Mezger	1984	MIS Quarterly	10.2307/249096	user interface design;simulation;user modeling;computer user satisfaction;decision support system;computer science;systems engineering;knowledge management;end-user computing;world wide web;user story	HCI	-33.96609736584223	-7.790247152661796	120376
636af105c0124489961530e237fd32f2b821e668	access strategies in packet mobile radio data networks	data transmission;mobile radiocommunication;radiocommunication service mobile;acces;transmission donnee	J. Spruce Riordon is Dean of t'ngineering and Professor of Systems and Computer Engineering at ('arleton University, Ottawa. He received the B.Eng. and M.Eng. degrees in Elecironical Engineering from McGill University in 1957 and 1961, and the Ph.D. degree from Imperial ('ollcgc. London, in 1967. While emplt~yed bv the National Research Council of Canada, he carried out research and development in the fields of circuit design, communications, process control. and optinlization. He jointed Carleton University in 1068. Current research and consulting interests include computer netv, orks. mobile data communications, and distributed databases. Dr. Riordon is a member of IEEI-, the ('anadian Information Processing Society, and the Association of Processional t-ngineers of Ontario.	circuit design;computer engineering;distributed database;information processing;yed	J. Spruce Riordon;Samy A. Mahmoud;Salah E. Aidarous;Morteza Niktash	1983	Computer Networks	10.1016/0376-5075(83)90014-4	telecommunications;computer science;computer network;data transmission	DB	-45.37749743797737	-7.276832418977253	120409
dfc510652a191ac4b87d2dbf6be91d2df6070a24	opyenxes: a complete python library for the extensible event stream standard		There has been a spectacular growth in the availability of event data. To transport, store and exchange these event data, the eXtensible Event Stream (XES) has become the acknowledged standardization due to its simplicity, flexibility, extensibility and expressivity. Currently, the OpenXES library exists as the popular open-source Java implementation of the XES standard. However, despite the gaining popularity of Python as the core programming language for data science, there has yet to exist a complete and open-source implementation of the standard in the language. This paper presents OpyenXES as a complete and opensource implementation of the XES standard in Python. This opens up the rich portfolio of Python packages for data science to researchers and practitioners in the field of business process management.	business process;data science;extensibility;java;open-source software;programming language;python;xerox escape sequence	Hernan Valdivieso;Wai Lam Jonathan Lee;Jorge Munoz-Gama;Marcos Sepúlveda	2018			programming language;python (programming language);extensibility;computer science	PL	-42.41874383131564	-2.293967320523707	120410
0a84f4d913ea318da42fd65e470ac7e21de969d6	letter from the special issue editor		importance of data provenance has been increasingly recognized by both users and publishers of data. For users of data, the scientific basis of their analysis relies largely on the credibility and trusthworthiness of their input data. For publishers of data, the provision of provenance as part of their published data is important for scholarship and reproducibility. In today's Internet era, where complex ecosystems of data are even more prevalent, it is no wonder that data provenance has become a major research topic in many conferences and workshops. Data provenance was already the topic of the December 2007 issue of IEEE Data Engineering Bulletin. This issue attempts to complement the December 2007 issue by focusing on how provenance has been captured and exploited by systems that either have been developed or are in the process of being developed in the industry and the academia, and by reporting on new interesting research directions. The first three articles describe how provenance has been applied in systems developed in the industry and academia. Provenance in ORCHESTRA, by Green et al., describes a collaborative data sharing system ORCHESTRA , whose architecture and provenance model are largely motivated from the needs of the life sciences community. As part of ORCHESTRA, the authors describe an interesting application of provenance to the specification of trust policies and update exchange. In the article titled Refining Information Extraction Rules using Data Provenance, Liu et al. describe how provenance can be used to aid the process of developing information extraction rules. Here, provenance is exploited to understand the changes that need to be made to the information extraction rules so that the result is what the user desires. Gonzalez et al. describe how Google Fusion Tables can be used to faciliate collaboration around data sets in their article Socializing Data with Google Fusion Tables. Attribution and provenance are anticipated to play a significant role in socializing data and this article provides an accessible exposition to the attribution feature of Google Fusion Tables, as well as the authors' experience with the system. It is important for scientists to be able to pinpoint and cite data easily for proper scholarship. In the article A Rule-Based Citation System for Structured and Evolving Datasets, Buneman and Silvello discuss the requirements that must be met by a citation system in order to guarantee consistency and integrity of citations. The issues that arise in two data sets …	computer cluster;ecosystem;google fusion tables;information extraction;internet;requirement;socialization;ws-trust	Wang Chiew Tan	2010	IEEE Data Eng. Bull.		data mining;computer science	DB	-45.18073415903302	-0.036570915057013244	120534
6c485c47b4104fbe9cdcc831f5e2f2352eadf660	knowledge-based information management in intensive care and anesthesia	intensive care;information management;knowledge base	ions with respect to real time constraints; (ii) distributed architectures, agent models and communication protocols; (iii) knowledge acquisition and representation (for a review see [2]). The four articles we selected for this special issue are mainly concerned with the third topic knowledge acquisition and representation. Two articles propose data-driven techniques to improve the exploitation of raw data coming from medical devices present at the patient's bedside. Clearly, a set of	architecture as topic;computer architecture;information management;ions;knowledge acquisition;patients;protocols documentation;intensive care	Michel Dojat;Silvia Miksch;Jim Hunter	2000	Artificial intelligence in medicine	10.1016/S0933-3657(00)00044-0	knowledge base;computer science;knowledge management;artificial intelligence;information management	AI	-35.35362696897396	-9.492369600450276	120579
38a2c654333c07e14a5f268dd634900dbfea3ffa	on knowledge grid and grid intelligence: a survey	distributed data;social intelligence;information infrastructure;course of action;social relationship;virtual community;web interface;data processing;autonomy oriented computing;computer network;grid intelligence;knowledge grid;social network;web intelligence;language processing;information exchange;adaptive learning;information processing;data prefetching;next generation;ubiquitous computing;self organization;management tool;wisdom web;multimedia presentation;markup language;knowledge creation;social evolution;problem solving;security protocol;knowledge base	The next generation Web Intelligence (WI) aims at enabling users to go beyond the existing online information search and knowledge queries functionalities and to gain, from the Web,1 practical wisdom for problem solving. To support such a Wisdom Web, we envision that a grid-like computing infrastructure with intelligent service agencies is needed, where these agencies can interact, self-organize, learn, and evolve their course of actions, identities, and interrelationships for new knowledge creation, as well as scientific and social evolution. In this paper, we first provide an overview of recent development in WI and Semantic/Knowledge Grid. Then, the fundamental capabilities of the Wisdom Web as well as the conceptual architecture of an intelligent Grid for supporting it are described. Technical challenges for realizing Grid Intelligence are highlighted and the recent advancements in related research areas are reviewed.	problem solving;self-organization;web intelligence	William Kwok-Wai Cheung;Jiming Liu	2005	Computational Intelligence	10.1111/j.0824-7935.2005.00267.x	information infrastructure;web service;web application security;knowledge base;web development;web modeling;self-organization;data web;web analytics;web mapping;information exchange;data processing;web design;information processing;web accessibility initiative;web standards;computer science;knowledge management;social evolution;artificial intelligence;machine learning;semantic web;web navigation;social semantic web;data mining;social intelligence;markup language;web intelligence;web engineering;user interface;web 2.0;world wide web;adaptive learning;ubiquitous computing;mashup;social network	AI	-47.147579291033	2.332361280474167	120827
4391fa301b133e2d27ee8fa581049bf1a9d5fba7	introduction to the minitrack on data, text and web mining for business analytics			business analytics;web mining	Dursun Delen;Hamed Majidi Zolbanin	2018			business analytics;computer science;data science;web mining;knowledge management	ML	-39.43199652303698	-6.509040097239134	120984
2547feedc8ed43c72c8cb3572616cfc4cf8367e1	integration of statistics and geographic information systems: the r/terralib case.	geographic information system;statistical software;spatial statistics	This work presents a panorama of GIS integration in Spatial Statistics environments. It highlights the current needs of communities considering such integration. Spatial Statistics is treated in a context focused on the use of computational tools. A review of types of integration is accomplished and a new approach is proposed integrating the statistical software R and the GIS library TerraLib.	algorithm;geographic information system;geoprocessing;list of statistical packages;loose coupling;spatial analysis;spatial database;terralib	Pedro Ribeiro de Andrade Neto;Paulo Justiniano Ribeiro;Karla Donato Fook	2005			local information systems;spatial descriptive statistics;geospatial analysis;spatial analysis;geographic information system;remote sensing	DB	-37.95947572362069	-3.1135153924303003	121104
a4b9f66024de5e1b5ecbc9c83a40502d889ce5e8	metadata transformation and management with oracle intermedia		The management of multimedia data in object-relational dat ab se systems presents a challenging problem for extracting, processing, and managing associated me tadata. This information is usually embedded within the media source using proprietary formats, thus not easily accessible in a uniform fashion. This poses a need for a series of structural transformations t ensure that the metadata gathering process produces a unified representation across a multitude of media sources. The ultimate goal of these transformations is to consolidate management efforts for d iverse sources of media. This paper presents the Oracle MediaAnnotator, an extensib le architecture developed to be used with Oracle8i interMedia. The MediaAnnotator supports automat ic extraction and transformation of metadata into logical annotations. This approach allows for the cr ation of unified metadata repositories, which can then be used for indexing and searching. The extens ibl nature of this architecture makes it applicable to any multimedia domain. The MediaAnnotator le verages Oracle8i interMedia to tie together the multimedia data and its logical annotations; this offer s g eater manageability of media archives and opens the possibility for new applications which integrate multimedia content with other user data.	archive;canonical account;embedded system;extensibility;intermedia (hypertext);new media;object-relational database;oracle database;semiconductor industry	Marco Carrer;Ashok Joshi;Paul Lin;Alok Srivastava	1999	IEEE Data Eng. Bull.			DB	-43.783989770185165	1.973487956721994	121147
1255a7c5d7edaed29bc71d5367e1a2f56ca217de	an approach to enhancing workflows provenance by leveraging web 2.0 to increase information sharing, collaboration and reuse	search engine;scientific workflow;user centered design;collaborative tools;information sharing;user generated content	Web 2.0 promises a more enjoyable experience for creating content by users by providing easy-to-use information sharing and collaboration tools, and focusing on user-centered design. Provenance in Scientific Workflow Management is one kind of user-generated data that can benefit from using Web 2.0. We propose a simple set of Web 2.0 technologies that is easy to implement and can be immediately leveraged by scientific users. Using Atom Syndication Protocol to represent workflow state and its provenance users can easily disseminate their scientific results. Collaboration and authoring can be facilitated by using Atom Publishing Protocol and standard Web 2.0 blogging tools to publish and annotate provenance. Users can search provenance stored by using search engines and if they implement OpenSearch then search results can be in standard Atom Syndication Protocol that allows automatic aggregation of multiple Atom feeds and increases the likelihood of discoveries. By using those Web 2.0 standards, the value of scientific provenance data increases by making it a natural part of growing a variety of user-generated scientific (and non-scientific) content.	atom (standard);blog;opensearch;semantic web;simple set;user-centered design;user-generated content;web 2.0;web search engine;web syndication	Aleksander Slominski	2010		10.1007/978-3-642-17819-1_26	user-centered design;computer science;database;internet privacy;user-generated content;world wide web;search engine	HCI	-39.99688655453965	0.37175283330888936	121330
ebe6a2179c3c60297b06d5b42605381137a41699	interlinking geospatial information in the web of data		There is an increasing presence of geospatial datasets in the Linked Open Data cloud. However, these datasets are published like data silos and the value of the Web of Data depends, among other properties, on the amount and quality of links between data sources. One of the most overlooked problems to date in the linking process is to ensure that two different resources (identified with URIs) are actually referring to the same physical thing, that is, the co-reference problem. In this paper we present a coreference resolution approach that is composed of a set of heuristics for interlinking geospatial Linked Data. We have used these heuristics to connect resources from GeoLinkedData.es and DBpedia.	dbpedia;heuristic (computer science);information silo;linked data;semantic web;tag cloud;world wide web	Luis Manuel Vilches Blázquez;Victor Saquicela;Óscar Corcho	2012		10.1007/978-3-642-29063-3_7	geospatial pdf;web coverage service	AI	-37.55804667857261	4.138345235294244	121364
0b8af926ea39c9163d769ad2de1ef0ab7fdb0059	business analytics for flexible resource allocation under random emergencies	stochastic emergencies;utility;resource allocation;grupo de excelencia;administracion de empresas;scheduling;optimization;economia y empresa;grupo a;gas pipeline maintenance	Citation Angalakudati, Mallik, Siddharth Balwani, Jorge Calzada, Bikram Chatterjee, Georgia Perakis, Nicolas Raad, and Joline Uichanco. “Business Analytics for Flexible Resource Allocation Under Random Emergencies.” Management Science 60, no. 6 (June 2014): 1552–73. As Published http://dx.doi.org/10.1287/mnsc.2014.1919 Publisher Institute for Operations Research and the Management Sciences (INFORMS)	business analytics;institute for operations research and the management sciences	Mallik Angalakudati;Siddharth Balwani;Jorge Calzada;Bikram Chatterjee;Georgia Perakis;Nicolas Raad;Joline Uichanco	2014	Management Science	10.1287/mnsc.2014.1919	mathematical optimization;simulation;economics;resource allocation;operations management;microeconomics;management;operations research;scheduling;utility	Theory	-45.23031898882528	-5.1505100008372215	121443
01b721b4c2b5b5c8f08206232f8133054481d0c8	flexible processing at the amsr-e sips	brightness temperature;integration testing;collaborative work;data stream;information technology;data processing;large scale;hardware brightness temperature information technology collaborative software life testing system testing data processing collaborative work robustness large scale integration;large scale integration;life testing;seasonality;software framework;system testing;robustness;life span;collaborative software;hardware	Processing requirements at NASA’s AMSR-E SIPS have evolved considerably throughout the life span of the mission, as the SIPS has moved from mission testing and early experimental data processing to full operations supporting both forward processing and reprocessing. Working in close collaboration with the University of Alabama in Huntsville’s Information Technology and Systems Center (ITSC), the SIPS has developed a robust and flexible hardware and software framework that supports a variety of data streams and processing requirements. Today the SIPS maintains four operational processing environments in addition to development and integration test areas: • Routine operations environment for forward processing of AMSR-E data upon acquisition, • Late processing environment to handle occasional swaths of data that arrive after gridded products have been created for their time period, • Reprocessing environment for large-scale reprocessing efforts of individual data products or the entire product suite, • A special processing environment to manage special requests by the AMSR-E science team, such as evaluation of new algorithms with seasonal data. In addition, ITSC personnel have integrated science data subsetting and browse imagery generation into the SIPS processing flow, enabling the scientists to quickly and easily target parameters of interest. While similar in structure, each of these environments and their target data streams have unique requirements that would normally necessitate customized software. This paper will describe the evolution of the SIPS, the resulting processing and distribution framework, and plans to meet future requirements.	algorithm;browsing;integration testing;network switch;reduced cost;requirement;software engineering;software framework;systems architecture	Kathryn Regner;Helen Conover;Bruce Beaumont;Sara J. Graves;Lamar Hawkins;Philip Parker	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1526587	life expectancy;simulation;data processing;integration testing;computer science;software framework;data mining;database;brightness temperature;information technology;system testing;collaborative software;seasonality;robustness	Visualization	-41.85461024883866	-2.921804387935205	121511
df4a846732d4223d0d78474f8137df4f9721e0b1	big data and knowledge extraction for cyber-physical systems			big data;cyber-physical system	Xiuzhen Cheng;Yunchuan Sun;Antonio J. Jara;Houbing Song;Yingjie Tian	2015	IJDSN	10.1155/2015/231527	data science;data mining;knowledge extraction	AI	-38.929245006558055	-6.303507800114013	121992
729a5a95be9797be505e5f661a525f1078c7e4f9	a probabilistic relaxation approach for matching road networks	probabilistic relaxation;geographic information science;change detection;data integrity;road network;crowdsourcing data;cost effectiveness;road matching;geospatial data;structural similarity;matching method	A probabilistic relaxation approach for matching road networks Bisheng Yang a b , Yunfei Zhang a b & Xuechen Luan a b a State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing , Wuhan University , Wuhan , PR China b Engineering Research Center for Spatio-temporal Data Smart Acquisition and Application, Ministry of Education of China , Beijing , PR China Published online: 14 Jun 2012.	computation;crowdsourcing;geographic information system;goto;heuristic;information systems;information engineering;information science;linear programming relaxation;nl (complexity);nos/ve;openstreetmap;stochastic matrix;volunteered geographic information;yang;zhi-li zhang	Bisheng Yang;Yunfei Zhang;Xuechen Luan	2013	International Journal of Geographical Information Science	10.1080/13658816.2012.683486	cost-effectiveness analysis;geography;computer science;geospatial analysis;structural similarity;machine learning;pattern recognition;data integrity;data mining;database;change detection;statistics	DB	-43.43669313330533	-8.35141435104701	122203
e4e579394738ed5be6de2b2d7162e615ec982256	an online database and desktop assessment software to simplify systematic reviews in environmental science	software;systematic review;evidence based practice;literature;eco evidence;environmental management	We describe software to facilitate systematic reviews in environmental science. Eco Evidence allows reviewers to draw strong conclusions from a collection of individually-weak studies. It consists of two components. An online database stores and shares the atomized findings of previously-published research. A desktop analysis tool synthesizes this evidence to test causeeeffect hypotheses. The software produces a standardized report, maximizing transparency and repeatability. We illustrate evidence extraction and synthesis. Environmental research is hampered by the complexity of natural environments, and difficulty with performing experiments in such systems. Under these constraints, systematic syntheses of the rapidly-expanding literature can advance ecological understanding, inform environmental management, and identify knowledge gaps and priorities for future research. Eco Evidence, and in particular its online re-usable bank of evidence, reduces the workload involved in systematic reviews. This is the first systematic review software for environmental science, and opens the way for increased uptake of this powerful approach. © 2014 Elsevier Ltd. All rights reserved. Software/data availability Eco Evidence was developed by researchers and programmers at the eWater Cooperative Research Centre, Australia. It was publicly released in December 2012. The Eco Evidence Analyser (v1.1.1) software can be downloaded from www.toolkit.net.au/tools/ecoevidence, and the Eco Evidence Database is also accessible at this address using any web browser. New users must register with the Toolkit website, but there is no charge for registration or subsequent use of Eco Evidence. The Eco Evidence Database has an ASP.NET web interface driven by a Microsoft SQL Server database. Users must go through a self-approval process before they can add new citations and evidence items. The database is highly scalable and was designed to be accessed by multiple concurrent users. All records are associated with the user who enters/edits them, with the latest edit being kept. The relational database schema uses an	asp.net;database schema;desktop computer;ecology;ecosystem model;environmental resource management;expect;experiment;mathematical optimization;microsoft sql server;programmer;relational database;repeatability;scalability;systematic review;user interface	J. Angus Webb;Kimberly A. Miller;Michael J. Stewardson;Siobhan C. de Little;Susan J. Nichols;Stephen R. Wealands	2015	Environmental Modelling and Software	10.1016/j.envsoft.2014.11.011	systematic review;engineering;knowledge management;data mining;management science;software peer review;evidence-based practice	SE	-43.50733231435202	-3.554440210239967	122375
82f81c518daa52a39e4f71041dfce784f85f263a	collaborative ontology development for the geosciences		Ontology-based information publishing, retrieval, reuse, and integration have become popular research topics to address the challenges involved in exchanging data between heterogeneous sources. However, in most cases ontologies are still developed in a centralized top-down manner by a few knowledge engineers. Consequently, the role that developers play in conceptualizing a domain such as the geosciences is disproportional compared with the role of domain experts and especially potential end-users. These and other drawbacks have stimulated the creation of new methodologies focusing around collaboration. Based on a review of existing approaches, this article presents a two-step methodology and implementation to foster collaborative ontology engineering in the geosciences. Our approach consists of the development of a minimalistic core ontology acting as a catalyst and the creation of a virtual collaborative development cycle. Both methodology and prototypical implementation have been tested in the context of the EU-funded ForeStClim project which addresses environmental protection with respect to forests and	centralized computing;core ontology;knowledge engineer;ontology (information science);ontology engineering;subject-matter expert;top-down and bottom-up design	Reza Kalbasi;Krzysztof Janowicz;Femke Reitsma;Luc Boerboom;Ali A. Alesheikh	2014	Trans. GIS	10.1111/tgis.12070	upper ontology;geography;computer science;knowledge management;ontology;artificial intelligence;data science;data mining;database;ontology-based data integration;process ontology	HCI	-45.16528232196882	3.0088165460495198	122430
ff708e184a0cca969a43c85d5879c59959b6fc8f	scientific mashups: runtime-configurable data product ensembles	data sharing;bottom up;long tail;data management;software engineering;programming model;col;organic production	"""Mashups are gaining popularity as a rapid-development, re-use-oriented programming model to replace monolithic, bottom-up application development. This programming style is attractive for the """"long tail"""" of scientific data management applications, characterized by exploding data volumes, increasing requirements for data sharing and collaboration, but limited software engineering budgets.#R##N##R##N#We observe that scientists already routinely construct a primitive, static form of mashup--an ensemble of related visualizations that convey a specific scientific message encoded as, e.g., a Powerpoint slide. Inspired by their ubiquity, we adopt these conventional data-product ensembles as a core model, endow them with interactivity, publish them online, and allow them to be repurposed at runtime by non-programmers.#R##N##R##N#We observe that these scientific mashups must accommodate a wider audience than commerce-oriented and entertainment-oriented mashups. Collaborators, students (K12 through graduate), the public, and policy makers are all potential consumers, but each group has a different level of domain sophistication. We explore techniques for adapting one mashup for different audiences by attaching additional context, assigning defaults, and re-skinning component products.#R##N##R##N#Existing mashup frameworks (and scientific workflow systems) emphasize an expressive """"boxes-and-arrows"""" abstraction suitable for engineering individual products but overlook requirements for organizing products into synchronized ensembles or repurposing them for different audiences.#R##N##R##N#In this paper, we articulate these requirements for scientific mashups, describe an architecture for composing mashups as interactive, reconfigurable, web-based, visualization-oriented data product ensembles, and report on an initial implementation in use at an Ocean Observatory."""		Bill Howe;Harrison Green-Fishback;David Maier	2009		10.1007/978-3-642-02279-1_3	data management;computer science;data science;top-down and bottom-up design;data mining;database;programming paradigm;programming language;world wide web;long tail	NLP	-41.723831207941835	-0.2831510606324974	122436
3e717ed2903f3b99e39bb0f5c870af4759141227	telescope bibliographies: an essential component of archival data management and operations		Assessing the impact of astronomical facilities rests upon an evaluation of the scientific discoveries which their data have enabled. Telescope bibliographies, which link data products with the literature, provide a way to use bibliometrics as an impact measure for the underlying observations. In this paper we argue that the creation and maintenance of telescope bibliographies should be considered an integral part of an observatory’s operations. We review the existing tools, services, and workflows which support these curation activities, giving an estimate of the effort and expertise required to maintain an archive-based telescope bibliography.	archive;bibliographic index;bibliometrics;digital curation;floor and ceiling functions	Alberto Accomazzi;Edwin A. Henneken;Christopher Erdmann;Arnold Rots	2012	CoRR		computer science;data science;data mining;information retrieval;physics	HCI	-42.56663355843868	0.2188999663572686	122474
dd4cffe293a8f1dcfa33bd4e6f9c2cae7b6e31e3	floodwaygis: an arcgis visualization environment to remodel a floodway	flooding;geographical information systems;flood control;mapping;floods;models	Abstract#R##N##R##N#Floodway modeling has been performed extensively using HECRAS in floodplain studies. The model output is typically exported in GIS format and the floodway boundaries are overlaid on other spatial data to further edit or remodel the floodway to meet FEMA and local development requirements. In this article, a tightly coupled system comprised of a commercial GIS (ArcGIS) and HECRAS is presented. FloodwayGIS provides a comprehensive visual environment to edit, remodel, spatially analyze, and map floodway boundaries. The environment uses the HECRAS executable engine for every remodeling iteration. Four different encroachment editing options are provided within FloodwayGIS, which eliminates the need for a modeler to switch between HECRAS and GIS in the floodway modeling process, and results in savings of modeling time. FloodwayGIS also provides a mapping algorithm based on TIN intersection to produce smooth floodway boundaries that can be mapped in Digital Flood Insurance Rate Maps (DFIRMs) with minor editing.		Sivasankkar Selvanathan;Randel L. Dymond	2010	Trans. GIS	10.1111/j.1467-9671.2010.01225.x	simulation;geography;flooding;data mining;database;flood control;cartography	HCI	-38.25138965045912	-3.077038151219394	122563
a029c6d6da99b5434a226dbd6b67fe10c19929db	grid approaches to data-driven scientific and engineering workflows	za4450 databases;ta engineering general civil engineering general;qa76 computer software	"""cannot be reproduced or quoted extensively from without first obtaining permission in writing from the copyright holder/s. The content must not be changed in any way or sold commercially in any format or medium without the formal permission of the copyright holders. When referring to this work, full bibliographic details including the author, title, awarding institution and date of the thesis must be given e.g. AUTHOR (year of submission) """"Full thesis title"""", Enabling the full life cycle of scientific and engineering workflows requires robust middleware and services that support near-realtime data movement, high-performance processing and effective data management. In this context, we consider two related technology areas: Grid computing which is fast emerging as an accepted way forward for the large-scale, distributed and multi-institutional resource sharing and Database systems whose capabilities are undergoing continuous change providing new possibilities for scientific data management in Grid. In this thesis, we look into the challenging requirements while integrating data-driven scientific and engineering experiment workflows onto Grid. We consider wind tunnels that houses multiple experiments with differing characteristics, as an application exemplar. This thesis contributes two approaches while attempting to tackle some of the following questions: How to allow domain-specific workflow activity development by hiding the underlying complexity? Can new experiments be added to the system easily? How can the overall turnaround time be reduced by an end-to-end experimental workflow support? In the first approach, we show how experiment-specific workflows can help accelerate application development using Grid services. This has been realized with the development of MyCoG, the first Commodity Grid toolkit for .NET supporting multi-language programmability. In the second, we present an alternative approach based on federated database services to realize an end-to-end experimental workflow. We show with the help of a real-world example, how database services can be building blocks for scientific and engineering workflows."""	end-to-end encryption;end-to-end principle;experiment;federated database system;grid computing;middleware;requirement	Arumugam Paventhan	2007			computer science;data mining;database;world wide web	DB	-44.47289799948422	-1.1495831291880965	122923
355ae30618f9d17a753fa15a5a9332a62082d453	the first international workshop on automation of software test	software testing;integration testing;model based test;component integration test;test tools and environments;test adequacy and coverage;test case generation;software automation;test cost and effectiveness;model based testing;software test	Hong Zhu Department of Computing Oxford Brookes University Oxford OX33 1HX United Kingdom hzhu@brookes.ac.uk Tel: +44 1865 484580 Fax: +44 1865 484545 Joseph R. Horgan Telcordia Technologies One Telcordia Drive, RRC-1M322, Piscataway, NJ 08854, USA jrh@research.telcordia.com Tel: 732-699-2580 Fax: 732-336-7015 S.C. Cheung Department of Computer Science The Hong Kong University of Science and Technology Clear Water Bay, Hong Kong sccheung@cs.ust.hk Tel: +852 2358-7016 Fax: +852 2358-1477 J. Jenny Li Avaya Labs 233 Mount Airy Road Basking Ridge, NJ 07920,USA jjli@research.avayalabs.com Tel: 908-696-5147 Fax: 908-696-5402	computer science;fax;software testing;uniform resource identifier	Hong Zhu;Joseph Robert Horgan;Shing-Chi Cheung;J. Jenny Li	2006		10.1145/1134285.1134487	test strategy;keyword-driven testing;reliability engineering;model-based testing;manual testing;system integration testing;integration testing;systems engineering;engineering;software reliability testing;software engineering;software construction;test suite;session-based testing;software testing;system under test;test method;test script;test design;test management approach;test harness	Robotics	-46.64764363935429	-5.2086251437107745	122951
4429aa753810201f76fed4878c4c5c67cd2d5249	trustyfeer: a subjective logic trust model for smart city peer-to-peer federated clouds		1Computer Science Department, King Saud University, Riyadh, Saudi Arabia 2Mechanical Engineering Department, Massachusetts Institute of Technology (MIT), Cambridge, MA, USA 3Technical and Vocational Training Corporation, Riyadh, Saudi Arabia 4Information Technology Department, King Saud University, Riyadh, Saudi Arabia 5Information Systems Department, King Saud University, Riyadh, Saudi Arabia 6Media Lab, Massachusetts Institute of Technology (MIT), Cambridge, MA, USA	cryptographic service provider;ecosystem;eigentrust;interaction;peer-to-peer;quality of service;research data archiving;sl (complexity);simulation;smart city;threat model;trust management (information system);virtual machine	Heba Kurdi;Bushra Alshayban;Lina Altoaimy;Shada Alsalamah	2018	Wireless Communications and Mobile Computing	10.1155/2018/1073216	smart city;world wide web;peer-to-peer;subjective logic;computer science	Mobile	-44.58089379803035	-7.720891924191434	122990
28924f1685574507bc3fc1896a93ef8d717d78fc	towards the discovery of person-level data - reuse of vocabularies and related use cases		The Linked Data and the Social Science data communities developed the DDI-RDF Discovery Vocabulary, an ontology of the Data Documentation Initiative, in order to support the discovery of person-level data and its metadata. The Data Documentation Initiative (DDI) is an acknowledged international standard for the documentation and management of data from the social, behavioral, and economic sciences. Within the context of DDI-RDF Discovery Vocabulary, we reuse well elaborated and accepted vocabularies to a large extend. Vocabularies like DCMI, FOAF, ORG, ADMS, PROV-O, SKOS and XKOS, DCAT, and Data Cube. This paper focuses on the description of how other vocabularies are reused reasonably and on the description of use cases which are associated with the usage of the DDI-RDF Discovery Vocabulary.	data cube;documentation;foaf (ontology);linked data;simple knowledge organization system;vocabulary	Thomas Bosch;Benjamin Zapilko;Joachim Wackerow;Arofan Gregory	2013				ML	-41.977048046862045	3.6288416590463317	123232
3a713d6daa0100bb76d5a8eedb281435950b9c37	study on the evolutionary optimisation of the topology of network control systems	multi objective optimisation;network design;topology optimisation;real time;theory and method;performance index;delay distribution;journal;industrial network;computer network;international relations;dominating set;objective function;enterprise computing;network calculus;industrial control;evolutionary algorithm;networked control system;enterprise applications	Study on the evolutionary optimisation of the topology of network control systems Zude Zhou a , Benyuan Chen a , Hong Wang b & Zhun Fan c a School of Information Engineering, Wuhan University of Technology , Wuhan, 430070, China b Department of Management , North Carolina A&T State University , Greensboro, NC, 27411, USA c Department of Management Engineering , Technical University of Denmark , Lyngby, 2800, Denmark Published online: 02 Mar 2010.	chen–ho encoding;control system;crowding;emoticon;enterprise software;entity–relationship model;evolutionary algorithm;information engineering;loss function;mathematical optimization;nl (complexity);network calculus;network topology;one-way function;optimization problem;queueing theory;real-time transcription;star network;topology optimization;unbalanced circuit	Zude Zhou;Benyuan Chen;Hong Wang;Zhun Fan	2010	Enterprise IS	10.1080/17517570903527861	control engineering;network planning and design;process performance index;dominating set;computer science;networked control system;engineering;artificial intelligence;operations management;evolutionary algorithm;international relations	SE	-44.371634570378866	-6.859876220489509	123369
f881e01ea6a825fde2c3c66ddd3ab7e639aee75d	tacs 2006 committees and reviewers		Steering Committee Laurence T. Yang (Co-Chair), St. Francis Xavier University, Canada Jianhua Ma (Co-Chair), Hosei University, Japan Jingde Cheng, Saitama University, Japan Hoang Pham, Rutgers University, USA Sy-Yen Kuo, National Taiwan University, Taiwan Makoto Takizawa, Tokyo Denki University, Japan Rajendra V. Boppana, University of Texas at San Antonio, USA Rajeev Raje, Indiana University-Purdue University, Indianapolis, USA Bharadwaj Veeravalli, National University of Singapore, Singapore H. K. Dai, Oklahoma State University, USA Jogesh Muppala, Hong Kong University of Science and Technology, Hong Kong	francis;yang	Laurence Tianruo Yang	2006		10.1109/AINA.2006.320		Networks	-45.927409565466284	-9.761210393918907	123418
61c70ebc09adce2ad5cd335a403422b962cfcc13	personalized recommendation method of power information operation and maintenance knowledge based on spark		—Power information operation and maintenance knowledge overload has become a pressing issue with the development of smart grid construction. The traditional personalized recommendation method cannot meet the demand of personalized recommendation of power information maintenance knowledge in big data environment. This paper proposes a method based on Spark which gives a personalized recommendation method of power information operation and maintenance knowledge. Firstly, an implicit rating mechanism is introduced, which can transform the learning behavior of users into implicit rating of power information operation and maintenance knowledge. Secondly, a personalized recommendation method combing knowledge features and user interests is designed. Finally, the personalized recommendation method, based on Spark, is applied to recommend power information operation and maintenance knowledge. The experimental results show that the method can effectively improve the accuracy and real-time of recommendation.	big data;personalization;real-time clock	Zhaoyang Qu;Pengfei Xu;Youxue Ren	2016	JCM	10.12720/jcm.11.8.785-791	knowledge management;data mining;information retrieval	AI	-33.7451884912169	-2.345904224976662	123468
6b70e3bfe542e1f4322e0e0bd8752d2e6d898375	sensors and memory: managing context data for situated applications	memory management		sensor;situated	Richard R. Muntz	1999			situated;memory management;data mining;database;computer science	HCI	-40.62871401556764	-5.12038708869646	123572
26d208808ed9de1867bbb683a82e425e6ded8d6d	remote data access via the siesip distributed information system	distributed system;information resources;content based search;meta data distributed databases scientific information systems geophysics computing transport protocols data analysis graphical user interfaces information resources java;user interface;integrable system;transport protocols;data analysis;graphical user interfaces;earth science data;geophysics computing;distributed information system;seasonality;distributed information systems geoscience nasa database systems protocols data analysis user interfaces java graphical user interfaces information analysis;data access;distributed databases;earth science;communication protocol;meta data;database management system;data information system;scientific information systems;java;content based searches remote data access siesip distributed information system online searching online analysis ordering capabilities distributed earth science data seasonal to interannual earth science information partners nasa integrated system database management system communication protocols data analysis tools graphical user interface world wide web based java gui metadata information	Illustrates a distributed system that provides online searching, analysis and ordering capabilities for distributed Earth science data. The system is under development by a consortium led by George Mason University in a project called Seasonal-to-Interannual Earth Science Information Partners (SIESIP) as a part of a federation of information partners funded by NASA. The integrated system is composed of data, a database management system (DBMS), communication protocols, data analysis tools and a user interface. Through a Web-based Java GUI, users can search the DBMS for metadata information, conduct content-based searches, perform some initial analyses and issue an order for the selected data.	data access;information system	Ruixin Yang;Changzhou Wang;Menas Kafatos;Xiaoyang Sean Wang;Tarek A. El-Ghazawi	1999		10.1109/SSDM.1999.787651	data access;communications protocol;integrable system;computer science;data mining;graphical user interface;database;data analysis;user interface;java;metadata;world wide web;distributed database;transport layer;seasonality	DB	-39.0561219575365	-1.4452890333941502	123656
d101b2f7178ebde59a9302c5bb37de6faf34736d	managing scientific metadata using xml	hypermedia markup languages;information resources;xml data systems us department of defense software prototyping space technology communities distributed computing internet geoscience earth observing system;scientific data;interactive data access scientific metadata management xml distributed metadata server dimes search software web based interface multilevel metadata access prototype systems internet scientific data and information super server sdiss open source technologies;internet;distributed databases;meta data;scientific communication;distributed databases scientific information systems meta data hypermedia markup languages data models information resources internet;independent component;scientific information systems;data models;open source	We present our XML-based Distributed Metadata Server (Dimes) - which comprises a flexible metadata model, search software, and a Web-based interface - to support multilevel metadata access, and introduce two prototype systems. Our Scientific Data and Information Super Server (SDISS), which is based on Dimes and GDS, solves accurate data-search and outdated data-link problems by integrating metadata with the data systems. On the implementation front, we combine independent components and open-source technologies into a coherent system to dramatically extend system capabilities. Obviously, our approach can be applied to other scientific communities, such as bioinformatics and space science.	xml	Ruixin Yang;Menas Kafatos;Xiaoyang Sean Wang	2002	IEEE Internet Computing	10.1109/MIC.2002.1020326	metadata modeling;data modeling;the internet;data transformation;computer science;database;metadata;world wide web;distributed database;data element;meta data services;information retrieval;data;metadata repository;data dictionary	HPC	-38.93756390185961	-1.2609371426371825	123686
ec3f4af02074bd48abdd24fa9ab6195124260462	termination for the direct sum of left-linear term rewriting systems -preliminary draft-	part of book or chapter of book	1. I n t r o d u c t i o n An important concern in building algebraic specifications is their hierarchical or modular structure. The same holds for term rewriting systems [1] which can be viewed as implementations of equational algebraic specifications. Specifically, it is of obvious interest to determine which *This paper is an abbreviated version of the IEICE technical report COMP88-30, July 1988. Now we are preparing a final version for submission based on this draft.	algebraic riccati equation;institute of electronics, information and communication engineers;linear algebra;rewriting;term (logic)	Yoshihito Toyama;Jan Willem Klop;Hendrik Pieter Barendregt	1989		10.1007/3-540-51081-8_127	computer science;algorithm	Logic	-48.047897268527386	-5.027829956105057	123702
a31f07822fadfeb577ffc741806c21443829b340	a semantically adaptable integrated visualization and natural exploration of multi-scale biomedical data	multi scale biomedical exploration;medical information systems biology computing data visualisation;knowledge formalization;data visualization ontologies bioinformatics biology pathology degradation organizations;natural exploration;multiscale pathologies semantically adaptable integrated visualization multiscale biomedical data heterogeneous sources spatial scales medical domains multiscale visualization multimodal interaction knowledge formalization exploratory system biologist;ontology;natural exploration multi scale biomedical exploration knowledge formalization ontology	The exploration of biomedical data which involves heterogeneous sources coming from different spatial scales and medical domains is a challenging topic in current research. In this work, we combine efforts regarding multi-scale visualization, multimodal interaction and knowledge formalization for the exploration of multi-scale biomedical data. The knowledge formalization stores and organizes the information sources, the integrated visualization captures all relevant information for the domain expertise of the user and the multimodal interaction provides a natural exploration. We present a concrete example of use of the proposed exploratory system designed for a biologist investigating multi-scale pathologies.	comparison of command shells;experience;knowledge base;mathematical optimization;multimodal interaction;scalability;scientific visualization;spatial scale;usability testing	Ricardo Manuel Millán Vaquero;Asan Agibetov;Jan Rzepecki;Marta Ondresik;Alexander Vais;Joaquim Miguel Oliveira;Giuseppe Patanè;Karl-Ingo Friese;Rui Luís Reis;Michela Spagnuolo;Franz-Erich Wolter	2015	2015 19th International Conference on Information Visualisation	10.1109/iV.2015.96	information visualization;computer science;bioinformatics;data science;data mining	Visualization	-38.012647562582046	3.0437064403657006	124007
beda576be0825b4509ae41b3958b4e9a683900b1	a perspective of adaptation in healthcare		Emerging new technologies in healthcare has proven great promises for managing patient care. In recent years, the evolution of Information and Communication Technologies pushes many research studies to think about treatment plan adaptation in this area. The main goal is to accelerate the decision making by dynamically generating new treatment due to unexpected situations. This paper portrays the treatment adaptation from a new perspective inspired from the human nervous system named autonomic computing. Thus, the selected potential studies are classified according to the maturity levels of this paradigm. To guarantee optimal and accurate treatment adaptation, challenges related to medical knowledge and data are identified and future directions to be explored in healthcare systems are discussed.	acclimatization;autonomic computing;cns disorder;capability maturity model;classification;computation (action);decision making;inspiration function;name;programming paradigm;treatment plan;nervous system disorder	Emna Mezghani;Marcos Da Silveira;Cédric Pruski;Ernesto Exposito;Khalil Drira	2014	Studies in health technology and informatics	10.3233/978-1-61499-432-9-206	knowledge management;autonomic computing;emerging technologies;health care;medicine;information and communications technology	ML	-38.64887321248332	-9.627944191445122	124093
dcd29f49b1969c0c6cd2e6cbe19e40e7b942f657	model railroading and computer fundamentals	experiential learning;computers;curriculum development;programming language;real time embedded system;computer system design;learning activities;embedded system;transportation;fundamental concepts;computer software;computer science;relevance education;models;programming languages	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	computer;embedded system;francis;nl (complexity);primary source;real-time transcription;software project management;spreadsheet;triangulated irregular network;wiring	John W. McCormick	2007	Computer Science Education	10.1080/08993400601165644	transport;computing;simulation;computer science;engineering;experiential learning;software engineering;programming language;management;computer engineering	Robotics	-48.05595834371757	-3.72495970295143	124181
2c938be468e036309cfac2ec3f57e36aeb988b6f	fair secure computation with reputation assumptions in the mobile social networks		1School of Computer Science and Technology, Shandong University, Jinan 250101, China 2School of Information and Electrical Engineering, Ludong University, Yantai 264025, China 3School of Information Science and Engineering, Shandong Normal University, Jinan 250014, China 4School of Computer Science, Shaanxi Normal University, Xi’an 710062, China 5Laboratory of Algorithmics, Cryptology and Security (LACS), 1359 Luxembourg, Luxembourg	algorithmics;computer science;cryptography;electrical engineering;fairness measure;game theory;information science;mobile social network;secure multi-party computation;social network;thin-film-transistor liquid-crystal display	Yilei Wang;Chuan Zhao;Qiuliang Xu;Zhihua Zheng;Zhenhua Chen;Zhe Liu	2015	Mobile Information Systems	10.1155/2015/637458	computer science;secure two-party computation;distributed computing;internet privacy;computer security	Crypto	-44.465063679207574	-7.635316829996943	124407
c5f4984ffee4258a54d91e6bce2ea1535abf517e	a more decentralized vision for linked data		We claim that ten years into Linked Data there are still many unresolved challenges towards arriving at a truly machine-readable and decentralized Web of data. With a focus on the the biomedical domain—currently, one of the most promising “adopters” of Linked Data, we highlight and exemplify key technical and non-technical challenges to the success of Linked Data, and we outline potential solution strategies.	exemplification;human-readable medium;linked data	Axel Polleres;Maulik R. Kamdar;Javier D. Fernández;Tania Tudorache;Mark A. Musen	2018			management science;diagram;position paper;cloud computing;semantic web;linked data;decentralization;business	AI	-44.801341784566986	2.4235774254726095	125070
81d326e082cb578b27b0b403be6e03ae9ed37b51	the probable outcomes of a data processing project	data processing			A. Parkin	1977	Comput. J.	10.1093/comjnl/20.2.98	data processing;computer science;data science;data mining;database	DB	-39.77267491200911	-5.628528034238918	125627
1eba3af287291cd5ee9709fe04ec8580ac7b333a	grid and distributed public computing schemes for structural proteomics: a short overview	research and development;theoretical analysis;protein structure prediction;system biology;interactive environment;computational biology	Grid and distributed public computing schemes has become an essential tool for many scientific fields including bioinformatics, computational biology and systems biology. The adoption of these technologies has given rise to a wide range of projects and contributions that provide various ways of setting up these environments and exploiting their potential resources and services for different domains of applications. This paper aims to provide a distilled overview of some of the major projects, technologies and resources employed in the area of structural proteomics. The major emphasis would be to briefly comment on various approaches related to the gridification and parallelization of some flagship legacy applications, tools and data resources related to key structural proteomics problems such as protein structure prediction, folding and comparison. The comments are based on theoretical analysis of some interesting parameters such as performance gain after gridification, user level interaction environments, workload distribution and the choice of deployment infrastructure and technologies. The study of these parameters would provide a basis for some motivating justification needed for further research and development in this domain.	bioinformatics;computational biology;parallel computing;protein structure prediction;proteomics;software deployment;systems biology	Azhar Ali Shah;Daniel Barthel;Natalio Krasnogor	2007		10.1007/978-3-540-74767-3_44	computer science;bioinformatics;data science;protein structure prediction;distributed computing;management science;systems biology	HPC	-47.65846061092576	-0.1042762371114422	125723
1cd5bd210dd40d2b77254d78733ed92fae38e34f	database of research structures in public and private institutions in the south of italy	base relacional dato;europa;entity relationship mode;science and technology;architecture systeme;analisis sistema;project;formal model;concepcion sistema;proyecto;information source;source information;information scientifique technique;italia;recherche developpement;relational database;compte rendu;data model;adept;prototipo;report;conceptual schema;research and development;italie;investigacion desarrollo;system design;organisme recherche;estructura datos;organismo investigacion;base donnee relationnelle;research institution;system analysis;italy;industrial application;arquitectura sistema;structure donnee;analyse systeme;scientific technical information;systeme gestion base donnee;information system;europe;projet;recherche scientifique;system architecture;sistema gestion base datos;database management system;data structure;scientific research;prototype;database query;conception systeme;systeme information;informe;national research council;fuente informacion;entity relationship;investigacion cientifica;sistema informacion;modele relation entite	"""The development of the research project """"The scientific system in Southern Italy"""" of the National Research Council has required a series of studies aimed at enhancing the knowl edge and analysis of specific problems of southern Italy (Mez zogiorno). The methodology of a specific inquiry about the structure and the research activities of the institutions operat ing in the Mezzogiorno is presented. The project included setting up a database with a conceptual schema which concerns structures, activities and ongoing research projects in public and private institutions in the Mezzogiorno, that can be con sidered as necessary infrastructure, in order to know the geo graphical distribution of the existing deficiencies and the more advanced areas in science and technology. The prototype of the database includes all public and private institutions carry ing out R&D activities but does not yet include academic structures. It also contains information and data on related activities such as training courses, scientif..."""	database	G. Bianchi;M. C. Brandi;A. M. Scarda	1990	J. Information Science	10.1177/016555159001600504	scientific method;data structure;project;entity–relationship model;data model;relational database;computer science;conceptual schema;database;prototype;system analysis;operations research;information system;systems design;science, technology and society	Theory	-45.96225185241863	-0.56191507107251	125896
b59b769295fe84374d5a1f152233c4a7cd259b04	the rmap project: capturing and preserving associations amongst multi-part distributed publications	linked data;digital scholarship;publishing workflows;scholarly communication;digital preservation;semantic web;rest api;data publishing	The goal of the RMap Project is to create a prototype service that can capture and preserve maps of relationships amongst the increasingly distributed components (article, data, software, workflow objects, multimedia, etc.) that comprise the new model for scholarly publication. The demonstration will provide a tour of some of the features of the initial web service prototype. This will include examples of Distributed Scholarly Complex Objects (DiSCOs) and associated provenance data in RMap, as well as some of the options that users might have for interacting with the framework.	interaction;map;prototype;web service	Karen L. Hanson;Tim DiLauro;Mark Donoghue	2015		10.1145/2756406.2756952	computer science;semantic web;linked data;database;world wide web;information retrieval	DB	-41.651507973441355	0.899338385317768	126118
67317005537f230ea0b23d7f9b1fdb7f0bb9466d	feature selection expert - user oriented approach: methodology and concept of the system	user oriented approach;feature selection	The paper describes the methodology for feature selection and the concept of a user-oriented software package (FS Expert) for feature selction with a consulting system integrated into the package. It attempts to provide a guideline which approach to choose with respect to the extent of a priori knowledge of the problem. The methods implemented in FS Expert are based mostly on the methodology developed by the authors, though it is being built as an open system.	feature selection	Pavel Pudil;Jana Novovicová;Petr Somol;Radek Vrnata	1998		10.1007/BFb0033281	pattern recognition	Vision	-33.78355276927633	-8.813887002108581	126133
d9e6e3e8673c149df82938e68bb3781fb9ab4731	ontology metadata to support the building of a library of biomedical ontologies.	health research;uk clinical guidelines;biological patents;vocabulary controlled;medical informatics;europe pubmed central;citation search;uk phd theses thesis;life sciences;databases as topic;uk research reports;medical journals;information storage and retrieval;europe pmc;biomedical research;bioinformatics			Kaustubh Supekar;Mark A. Musen	2005	AMIA ... Annual Symposium proceedings. AMIA Symposium		upper ontology;idef5;open biomedical ontologies;ontology components;bibliographic ontology;computer science;ontology;data science;data mining;ontology-based data integration;metadata;information retrieval;process ontology	Embedded	-41.11990245460914	2.370350928595334	126237
601c851506b08c4c2a6327b390fa98b16841c888	using provenance in sensor network applications for fault-tolerance and troubleshooting - position paper		Provenance is a rapidly progressing new field with many open research problems. Being related to data and processes, provenance research is at the cross-roads of research from several research communities. With the huge amount of information and processes available in sensor networks, provenance becomes crucial for understanding the creation, manipulation and quality of data and processes in this domain too. Sensors collaboratively carry out sensing tasks and forward their data to the closest data processing center, which may further forward it. Provenance provides the means to record the data flow and manipulate snapshots of the network. Consequently given enough data, provenance can be used in sensor network applications to find out causes of faulty behavior, to figure out the circumstances that will affect the performance of the sensor network, to produce trustworthy data after elimination of the causes, etc. In this position paper, we describe provenance work in the sensor network community to sketch a panoramic view of the recent research and give a provenance model of a binary target localization sensor network as a real life example to show how provenance can be used in sensor network applications for fault-tolerance and troubleshooting.	authorization;dataflow;fault tolerance;open research;real life;real-time locating system;sensor;simulation	Gulustan Dogan;Theodore Brown	2012			data mining;world wide web;information retrieval	Mobile	-35.39578159667059	0.4942754734205041	126401
384df599f9ff876c2e85697f6e4fb0c4be670195	isocat: corralling data categories in the wild		To achieve true interoperability for valuable linguistic resources different levels of variation need to be addressed. ISO Technical Committee 37, Terminology and other language and content resources, is developing a Data Category Registry. This registry will provide a reusable set of data categories. A new implementation, dubbed ISOcat, of the registry is currently under construction. This paper shortly describes the new data model for data categories that will be introduced in this implementation. It goes on with a sketch of the standardization process. Completed data categories can be reused by the community. This is done by either making a selection of data categories using the ISOcat web interface, or by other tools which interact with the ISOcat system using one of its various Application Programming Interfaces. Linguistic resources that use data categories from the registry should include persistent references, e.g. in the metadata or schemata of the resource, which point back to their origin. These data category references can then be used to determine if two or more resources share common semantics, thus providing a level of interoperability close to the source data and a promising layer for semantic alignment on higher levels.	data model;iso 12620;interoperability;source data;user interface	Marc Kemps-Snijders;Menzo Windhouwer;Peter Wittenburg;Sue Ellen Wright	2008			natural language processing;data mining;artificial intelligence;source data;application programming interface;terminology;standardization;metadata;data model;semantics;interoperability;computer science	NLP	-41.71813374673529	4.136713841043357	126458
c0c4fdc96d42f1a75f8dfca335c465aa679e5507	development of a cloud-based platform for reproducible science: a case study of an iucn red list of ecosystems assessment		Article history: Received 24 March 2016 Received in revised form 15 August 2016 Accepted 20 August 2016 Available online xxxx One of the challenges of computational-centric research is to make the research undertaken reproducible in a form that others can repeat and re-use with minimal effort. In addition to the data and tools necessary to rerun analyses, execution environments play crucial roles because of the dependencies of the operating system and software version used. However, some of the challenges of reproducible science can be addressed using appropriate computational tools and cloud computing to provide an execution environment. Here, we demonstrate the use of a Kepler scientific workflow for reproducible science that is sharable, reusable, and re-executable. These workflows reduce barriers to sharing andwill save researchers timewhen undertaking similar research in the future. To provide infrastructure that enables reproducible science, we have developed cloud-based Collaborative Environment for Ecosystem Science Research and Analysis (CoESRA) infrastructure to build, execute and share sophisticated computation-centric research. The CoESRA provides users with a storage and computational platform that is accessible from a web-browser in the form of a virtual desktop. Any registered user can access the virtual desktop to build, execute and share the Kepler workflows. This approach will enable computational scientists to share complete workflows in a pre-configured environment so that others can reproduce the computational research with minimal effort. As a case study, we developed and shared a complete IUCN Red List of Ecosystems Assessment workflow that reproduces the assessments undertaken by Burns et al. (2015) onMountain Ash forests in the Central Highlands of Victoria, Australia. This workflow provides an opportunity for other researchers and stakeholders to run this assessment with minimal supervision. The workflow also enables researchers to re-evaluate the assessment when additional data becomes available. The assessment can be run in a CoESRA virtual desktop by opening aworkflow in a Kepler user interface and pressing a “start” button. The workflow is pre-configured with all the open access datasets and writes results to a pre-configured folder. Crown Copyright © 2016 Published by Elsevier B.V. All rights reserved.	cloud computing;computation;crown group;desktop computer;ecosystem;executable;kepler;operating system;red ash: the indelible legend;registered user;software versioning;user interface;victoria (3d figure);virtual desktop	Siddeswara Guru;Ivan C. Hanigan;Hoang Anh Nguyen;Emma Burns;John Stein;Wade Blanchard;David Lindenmayer;Tim Clancy	2016	Ecological Informatics	10.1016/j.ecoinf.2016.08.003	simulation;computer science;data mining;world wide web;workflow management system;workflow engine;workflow technology	HPC	-43.936308079146905	-3.310046764653241	126540
a4031ef05859e91113c0b25dc621bcaeb0a5b15f	about this business of metadata		A brief discussion presents some of the opportunities and challenges involved with creating metadata-centric businesses that bring Music Information Retrieval technologies to the marketplace. In particular, two related difficulties -that of the difficulty of proving incremental value for new metadata systems, and that of the relative influidity of the marketplace for MIR -are highlighted. Potential directions for resolving these issues are also discussed.	information retrieval	Eric Schreier	2002			database;metadata;meta data services;metadata repository;information retrieval;geospatial metadata;computer science	Web+IR	-34.35297994839328	1.9972633144902592	126712
f96837f4a30692ace3c69029ac9bdf4853175c66	publishing bibliographic records on the web of data: opportunities for the bnf (french national library)		Linked open data tools have been implemented through data.bnf.fr, a project which aims at making the BnF data more useful on the Web. data.bnf.fr gathers data automatically from different databases on pages about authors, works and themes. Online since July 2011, it is still under development and has feedbacks from several users, already. First the article will present the issues linked to our data and stress the importance of useful links and of persistency for archival purposes. We will discuss our solution and methodology, showing their strengths and weaknesses, to create new services for the library. An insight on the ontology and vocabularies will be given, with a “business” view of the interaction between rich RDF ontologies and light HTML embedded data such as schema.org. The broader question of Libraries on the Semantic Web will be addressed so as to help specify similar projects.	bibliographic record;data mining;database;diagram;embedded system;feedback;html;linked data;map;ontology (information science);resource description framework;schema.org;semantic web;vocabulary;world wide web	Agnès Simon;Romain Wenz;Vincent Michel;Adrien Di Mascio	2013		10.1007/978-3-642-38288-8_38	computer science;data mining;world wide web;information retrieval	Web+IR	-42.54158196649252	1.9233781252821889	126949
3df2cabcc2bd4ad16ddb4d2c2af13f63829e7d3b	big data challenge: a data management perspective	databases;performance;jinchuan chen yueguo chen xiaoyong du cuiping li jiaheng lu suyun zhao xuan zhou 数据管理 研究人员 物理科学 网络公司 数据整合 数据索引 企业家 科学家 big data challenge a data management perspective;big data;期刊论文;big data performance databases	There is a trend that, virtually everyone, ranging from big Web companies to traditional enterprisers to physical science researchers to social scientists, is either already experiencing or anticipating unprecedented growth in the amount of data available in their world, as well as new opportunities and great untapped value. This paper reviews big data challenges from a data management respective. In particular, we discuss big data diversity, big data reduction, big data integration and cleaning, big data indexing and query, and finally big data analysis and mining. Our survey gives a brief overview about big-data-oriented research and problems.	algorithm;big data;crowdsourcing;data mining;dimensionality reduction;distributed computing;input/output;machine learning;mathematical optimization;plasma cleaning;query optimization;relational database;sampling (signal processing);social security;time complexity	Jinchuan Chen;Yueguo Chen;Xiaoyong Du;Cuiping Li;Jiaheng Lu;Suyun Zhao;Xuan Zhou	2013	Frontiers of Computer Science	10.1007/s11704-013-3903-7	big data;performance;computer science;data science;data mining;operations research	DB	-37.807876822748305	-7.325931263571954	126950
81105df54793546ac4aae4ca66e371eaaa29387c	conference committees		Technical Committee Chairs Prof. Mo Hongwei, Harbin Engineering University, China Prof. Rezia Molfino, University of Genoa, Italy Prof. Bhekisipho Twala, University of Johannesburg, South Africa Prof. Maitree Thamma, Rajamangala University of Technology Isan, Thailand Prof. Geetesh Goga, K.C. Collge of Engineering and I.T., India Prof. Sharmila .B, Sri Ramakrishna Engineering College, Coimbatore, India		Jingxiang Wu;Joy Na Wang;G. W. Xia Hou	2005	2018 3rd International Conference on Control and Robotics Engineering (ICCRE)	10.1109/QSIC.2005.20		DB	-45.76320317650879	-8.979130457734554	127196
019d96c5815a255841096c67606205d2f2d4c7f5	a context sensitive experience feeder for computer aided engineering		Computer aided engineering (CAE) tries to map properties and interactions of real world entities with symbols and values readable to the machine. Modern CAE software packages are powerful in function, but users usually need a lot of knowledge and experience to manipulate them. As a kind of tacit knowledge, experiences require gauged context in order to be fully understood and applied. To better exploit the freely written, hard-to-encode experiences on the web, we propose in this paper a context sensitive experience feeding mechanism which is able to recommend experiences matching the context of a given CAE task. Our method makes use of information extraction and natural language processing techniques to find experience valuable to engineer’s trouble shooting. Empirical evaluation of a prototypical feeder suggests that our method is effective.	encode;entity;experience;human-readable medium;information extraction;interaction;natural language processing;x/open	Bo Song;Zuhua Jiang	2012			data mining;computer science;computer engineering;computer-aided engineering	SE	-39.49126250065747	3.1667562475943862	127248
95e74060de6421f893dfc06a47ba7f7b3c75f4ca	document warehousing based on a multimedia database system	document handling;electronic mail;marketing data processing;personal computer;rule based;business data processing data warehouses multimedia databases document handling business forms marketing data processing office automation electronic mail;decision maker;information sharing;business data processing;business forms;multimedia databases;software framework;warehousing multimedia databases data warehouses image storage marketing and sales html videos microcomputers world wide web text processing;data warehouses;data warehouse;xml data view rules document warehousing multimedia database system structured data sales forms business forms data warehouses decision makers unstructured data email html texts images videos office documents personal computer storage www mailing word processing corporate assets software framework corporate wide information sharing corporate wide information reuse simple documents compound documents keyword based retrieval content based retrieval rule based classification som based clustering xml data query rules;content based retrieval;office automation;word processing;structured data;multimedia database system	Nowadays, structured data such as sales and business forms are stored in data warehouses for decision makers to use. Further, unstructured data such as emails, html texts, images, videos, and oftIce documents are increasingly accumulated in personal computer storage due to spread of mailing, Www, and word processing. Such unstructured data, or what we call multimedia documents, are larger in volume than structured data and precious as corporate assets as well. So we need a document warehouse as a software framework where multimedia documents are analyzed and managed for corporate-wide information sharing and reuse like a data warehouse for structured data. We describe a prototype document warehouse system, which supports management of simple and compound documents, keyword-based and content-based retrieval, rule-based classification, SOM-based clustering, and XML data query and view rules.	cluster analysis;computer data storage;database;email;event condition action;html;logic programming;personal computer;prototype;software framework;www;xml	Hiroshi Ishikawa;Kazumi Kubota;Yasuo Noguchi;Koki Kato;Miyuki Ono;Naomi Yoshizawa;Yasuhiko Kanemasa	1999		10.1109/ICDE.1999.754921	decision-making;data model;computer science;software framework;data warehouse;data mining;database;world wide web	DB	-38.94570007273775	2.889353104919584	127338
2308b9ed0470eb06ff60bea5a7dfb5b643991224	accessing and visualizing scientific spatiotemporal data	clusters;remote sensing image;parallel supercomputers;scientific information systems data visualisation query processing visual databases temporal databases very large databases;concurrent computing;query processing;terrain;large data sets;optical computing;digital elevation model;grid;data visualisation;geographic;remote sensing;data visualization;data access;spatiotemporal phenomena;propulsion;digital elevation models;temporal databases;space technology;scientific spatiotemporal data;astronomy;data visualization spatiotemporal phenomena space technology nasa supercomputers propulsion laboratories concurrent computing grid computing optical computing;very large databases;geospatial;remote data set data access data visualization scientific spatiotemporal data large data sets multiple computing resources parallel supercomputers clusters grid;data sets;multiple computing resources;nasa;grid computing;remote data set;supercomputers;parallel applications;scientific information systems;surveys;rendering;visual databases	This paper discusses work done by JPL's Parallel Applications Technologies Group in helping scientists access and visualize very large data sets through the use of multiple computing resources, such as parallel supercomputers, clusters, and grids. These tools do one or more of the following tasks: visualize local data sets for local users, visualize local data sets for remote users, and access and visualize remote data sets. The tools are used for various types of data, including remotely sensed image data, digital elevation models, astronomical surveys, etc. The paper attempts to pull some common elements out of these tools that may be useful for others who have to work with similarly large data sets.	computer cluster;digital elevation model;grid computing;spatiotemporal database;supercomputer	Daniel S. Katz;Attila Bergou;G. Bruce Berriman;Gary L. Block;Jim Collier;David W. Curkendall;John Good;Laura Husman;Joseph C. Jacob;Anastasia C. Laity;Peggy Li;Craig Miller;Tom Prince;Herb Siegel;Roy Williams	2004	Proceedings. 16th International Conference on Scientific and Statistical Database Management, 2004.	10.1109/SSDBM.2004.11	digital elevation model;computer science;theoretical computer science;data mining;database;data visualization	HPC	-38.41651097650513	-1.7295345337907118	127868
3e8f841bd6e934652b78ccce43f8454fedf9de35	an informatics approach for smart evaluation of water quality related ecosystem services	ecology and environment;data and information;computer science	Understanding the relationship between water quality and ecosystem services valuation requires a broad range of approaches and methods from the domains of environmental science, ecology, physics and mathematics. The fundamental challenge is to decode the association between 'ecosystem services geography' with water quality distribution in time and in space. This demands the acquisition and integration of vast amounts of data from various domains in many formats and types. Here we present our system development concept to support the research in this field. We outline a technological approach that harnesses the power of data with scientific analytics and technology advancement in the evolution of a data ecosystem to evaluate water quality. The framework integrates the mobile applications and web technology into citizen science, environmental simulation and visualization. We describe a schematic design that links water quality monitoring and technical advances via collection by citizen scientists and professionals to support ecosystem services evaluation. These would be synthesized into big data analytics to be used for assessing ecosystem services related to water quality. Finally, the paper identifies technical barriers and opportunities, in respect of big data ecosystem, for valuating water quality in ecosystem services assessment.	ecosystem services;informatics	Weigang Yan;Mike Hutchins;Steven A. Loiselle;Charlotte Hall	2015		10.1007/978-3-319-24474-7_24	computer science;knowledge management;data mining;management science;management;world wide web	SE	-40.27426273733468	-1.5288368083502029	128065
751fd97c38150021000042199d41e9f00af5a6b8	modeling and processing of time interval data for data-driven decision support	aggregated time series time interval data processing data driven decision support time interval data analysis model;time series time interval data driven decision support multi dimensional modeling summarizability bitmap index;time series data analysis decision support systems;decision support systems data models time series analysis context time measurement analytical models business	Over the past decades, several disciplines like artificial intelligence, music, medicine, ergonomics or cognitive science dealt with problems concerning analyses of data associated with time intervals. Topics like pattern recognition, comparison, quality, or visualization are in focus of current research. Using these techniques in the context of data-driven decision support is quite rare even though the importance of data to support better decision making can be enormous. Reasons lie above all in limited insufficient tooling support, expensive data processing, and inapplicable requirements. In this paper, we discuss the use of time interval data and name difficulties arising when processing such data for data-driven decision support. We discuss and present solutions for overcoming the identified problems and enabling the usage of time interval data for data-driven decision support. We introduce a time interval data analysis model that provides fast access to the raw time interval data but especially to aggregated time series, mostly needed when making meaningful decisions.	.mdx;apache axis;artificial intelligence;bitmap index;chart;cognitive science;decision support system;human factors and ergonomics;load balancing (computing);map;online analytical processing;pattern recognition;payment card industry data security standard;persistence (computer science);query language;requirement;time series;tombstone (data store);visual analytics	Philipp Meisen;Marco Recchioni;Tobias Meisen;Daniel Schilberg;Sabina Jeschke	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974378	computer science;data science;machine learning;data mining	DB	-34.679947618305846	-3.2379548669447873	128488
94febf30ebb23f2471a6a8cd5a752765bc05476f	a study of ontology convergence in a semantic wiki	semantic network;emergent structure;semantic web;semantic wiki;emergent ontologies	Semantic Wikis propose a combination of both easy collaboration and semantic expressivity; characteristics of the WikiWikiWeb and the Semantic Web respectively. In this paper we look to define and analyse the Semantic Wiki method, in order to explore the effect of different Semantic Wiki characteristics on the quality of the semantic networks authored within them. We look at a number of different Semantic Wiki implementations, including their semantic expressivity and usability. We focus on support for ontology creation, and perform an evaluation on the effect of type suggestion tools on ontology convergence within a seeded and unseeded Wiki (using Semantic MediaWiki and our own MOCA extension). We find that seeding a Wiki with typed pages and links has a strong effect on the quality of the emerging structure and that convergence tools have the potential to replicate that effect with an unseeded Wiki, but that they have limited impact on the reuse of elements from the evolving ontology.	mediawiki;self-replicating machine;semantic web;semantic network;usability;wiki;wikiwikiweb	Chrysovalanto Kousetti;David E. Millard;Yvonne Margaret Howard	2008		10.1145/1822258.1822281	semantic data model;upper ontology;semantic similarity;semantic computing;semantic search;semantic grid;computer science;social semantic web;semantic web stack;semantic compression;database;semantic technology;world wide web;information retrieval;semantic analytics	Web+IR	-46.032621597922414	3.9051232207027993	128638
ce5fd2cf0657cf03b2ffdb0cd3b3aa25d3038c58	cendari: establishing a digital ecosystem for historical research	history;ontologies ecosystems standards communities xml libraries europe;ontologies artificial intelligence;research and development;intermediary metadata schemas metadata digital ecosystems virtual research environments;information retrieval systems;ontology development cendari digital ecosystem historical research collaborative european digital archive infrastructure world war i research infrastructure diverse information requirements metadata data integration;research and development data integration history information retrieval systems ontologies artificial intelligence;data integration	The CENDARI (Collaborative European Digital Archive Infrastructure) project, which aims to create a research infrastructure for World War I and medieval history, is examined as a example digital ecosystem. The diverse information requirements of these two communities are met by a strategy which combines newly devised approaches to metadata and tools for data integration and ontology development. The ways in which this strategy produces an environment which conforms to the notion of a digital ecosystem is also examined.	archive;battle of midway;collective intelligence;design rationale;digital ecosystem;information architecture;requirement;virtual community	Richard Gartner;Mark Hedges	2013	2013 7th IEEE International Conference on Digital Ecosystems and Technologies (DEST)	10.1109/DEST.2013.6611330	computer science;data mining;metadata;world wide web;information retrieval	EDA	-42.203311910990514	0.5756168313049482	128969
9128786410502530e1e03bbd7f84c4c1e02f8f60	compimage'14 - computational modeling of objects presented in images: fundamentals, methods and applications	ciencias tecnologicas ciencias da engenharia e tecnologias;outra publicacao em revista cientifica internacional	CompIMAGE’14 – computational modeling of objects presented in images: fundamentals, methods and applications João Manuel R. S. Tavares To cite this article: João Manuel R. S. Tavares (2016) CompIMAGE’14 – computational modeling of objects presented in images: fundamentals, methods and applications, Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization, 4:2, 45-45, DOI: 10.1080/21681163.2015.1081081 To link to this article: http://dx.doi.org/10.1080/21681163.2015.1081081		João Manuel R. S. Tavares	2016	CMBBE: Imaging & Visualization	10.1080/21681163.2015.1081081		Visualization	-47.510188483707715	-9.242254257961646	128984
92fec694e539b5e0ef8de99e0dbad2aa875169a2	in-memory databases in business information systems		a funder's repository at a funder's request, provided it is not made publicly available until 12 months after publication.	management information system	Peter Loos;Jens Lechtenbörger;Gottfried Vossen;Alexander Zeier;Jens Krüger;Jürgen Müller;Wolfgang Lehner;Donald Kossmann;Benjamin Fabian;Oliver Günther;Robert Winter	2011	Business & Information Systems Engineering	10.1007/s12599-011-0188-y	information engineering;management information systems;business analytics;business system planning;information management;business software;business process discovery;information system;business activity monitoring	DB	-40.10167732658443	-4.968700587710686	129026
e4a4b29ab1749b8760076a60c67c949b9f39ca79	assessing the quality of geospatial linked data - experiences from ordnance survey ireland (osi)		Ordnance Survey Ireland (OSi) is Ireland’s national mapping agency that is responsible for the digitisation of the island’s infrastructure in terms of mapping. Generating data from various sensors (e.g. spatial sensors), OSi build its knowledge in the Prime2 framework, a subset of which is transformed into geo-Linked Data. In this paper we discuss how the quality of the generated sematic data fares against datasets in the LOD cloud. We set up Luzzu, a scalable Linked Data quality assessment framework, in the OSi pipeline to continuously assess produced data in order to tackle any quality problems prior to publishing.	data quality;linked data;osi model;scalability;sensor	Jeremy Debattista;Eamonn Clinton;Rob Brennan	2018			geospatial analysis;data science;linked data;scalability;cloud computing;computer science;data quality	Web+IR	-40.42956217756849	-1.5406411440850059	129046
56e2dacc8f55180276b29ed021a33624fbf283ae	accessibility through preferences: context-aware recommender of settings		A proposal for merging context-awareness and user preferences in the same software system is provided. Several modules from the on-going CLOUD4All project (European Commission Seventh Framework Programme) are enhanced with Context Awareness, including the Semantic Matching Framework, the RuleBased Matchmaker (with new rules) and the Statistical Matchmaker (with new features to be used as predictors). Some other components are created exclusively to deal with context features, as the Context Aware Server (to add context from motes) and the Minimatchmaker (to save computation and network resources for well-known situations) 1 Corresponding Author. Technosite C/Albasanz 16, 3-B 28037 Madrid. Spain. aiglesias@technosite.es 2 Results presented in this paper have been researched within the Cloud4all project. Cloud4all is an R&D project that receives funding from the European Commission under the Seventh Framework Programme (FP7/2007-2013) under grant agreement n° 289016.	accessibility;computation;context awareness;recommender system;semantic matching;software system;user (computing)	Andrés Iglesias-Pérez;Claudia Loitsch;Nikolaos Kaklanis;Konstantinos Votis;Andreas Stiegler;Konstantinos Kalogirou;Guillem Serra-Autonell;Dimitrios Tzovaras;Gerhard Weber	2014		10.1007/978-3-319-07437-5_22	data mining;merge (version control);software system;personalization;semantic matching;computation;resource (disambiguation);commission;computer science;context awareness	Web+IR	-42.28906129015532	-1.9992748190958396	129155
4eb0bd7c63874f81405b7dc73e7da64fa091176b	estimating the selectivity of tf-idf based cosine similarity predicates	approximate string matching;data integrity;personalization;visualization;data cleaning;mashup;data integration	An increasing number of database applications today require sophisticated approximate string matching capabilities. Examples of such application areas include data integration and data cleaning. Cosine similarity has proven to be a robust metric for scoring the similarity between two strings, and it is increasingly being used in complex queries. An immediate challenge faced by current database optimizers is to find accurate and efficient methods for estimating the selectivity of cosine similarity predicates. To the best of our knowledge, there are no known methods for this problem. In this paper, we present the first approach for estimating the selectivity of tf.idf based cosine similarity predicates. We evaluate our approach on three different real datasets and show that our method often produces estimates that are within 40% of the actual selectivity.	approximate string matching;cosine similarity;plasma cleaning;selectivity (electronic);string searching algorithm;tf–idf	Sandeep Tata;Jignesh M. Patel	2007	SIGMOD Record	10.1145/1328854.1328855	visualization;approximate string matching;computer science;data integration;data integrity;data mining;personalization;database;information retrieval;mashup	DB	-34.45907260185417	3.7014232727744107	129834
35cf21d293dc7c2f0e0f7215822f39e0af0c0fd5	the role of terminology in creating knowledge data bases			database	Alexander K. Dzhincharadze;Irina N. Volkova	1990			knowledge management;data science;computer science;terminology	AI	-40.587774295888245	-6.235196775977656	130015
f32f5eab3f0130db91d6c88d008a237f117b0a3f	"""acquiring configuration rules of form elements from """"historic"""" architectural facade employing inductive logic programming"""	inductive logic programming	"""A summary of application of an ILP technique to the study of architectural facades design is presented. We focused on discovering the configuration rules of form elements in the architectural facade elevations evaluated as """"historic"""". We obtained supervisory data based on the record of Sanneizaka Preservation District for Groups of Historic Buildings archived in the Kyoto City Planning Bureau. Architectural objects were converted into simple geometric form elements based on their positional relationships. By employing Progol, an ILP application, we acquired 27 resulting sets of configuration rules."""	inductive logic programming	Atsushi Saito;Junzo Munemoto;Daisuke Matsushita	2005		10.1007/11780496_22	structural engineering;engineering;engineering drawing	PL	-38.500725590604794	-3.465258557441661	130259
92ad6c74a78635407da3aae2f53cbfe00bf7148c	electronic laboratory notebooks for collaborative research	remote researcher access;groupware;collaborative work;history;laboratories collaboration world wide web collaborative work information analysis computer networks educational institutions history data analysis us department of energy;research facilities;common understanding;collaboration;research and development management;computer networks;data analysis;world wide web;information repository;collaborative research;electronic laboratory notebooks;us department of energy;remote researcher access electronic laboratory notebooks collaborative research scientific research information repository common understanding research facilities;scientific research;information analysis;scientific information systems	The laboratory notebook is a vital tool in scientific research. I t is the central repository of information about the reasoning and preparation behind experiments, about the analyses done to obtain results, and about plans for future research. The notebook captures the scientific process that gives meaning to a scientist's observations. Sharing a notebook can help collaborating researchers to build a common understanding of their work. Unfortunately, at modern research facilities, where collaborations may span the globe, providing remote researchers with access to a laboratory notebook becomes a difficult task.	experiment	James D. Myers;Cedrus Fox-Dobbs;Justin Laird;Dai Le;Dan Reich;Thad Curtz	1996		10.1109/ENABL.1996.555050	human–computer interaction;computer science;knowledge management;software engineering;database;data analysis;management;world wide web	HPC	-41.53876806768561	-2.1157524467180417	130546
de8600be493957ffab962c558f11c79d9fcd5265	an agent-oriented architecture for researcher profiling and association using semantic web technologies	knowledge representation.;ontology;researcher profiling;agent;semantic web	  Collaboration within the international scientific community has steadily increased over the years especially in the presence  of complex interdisciplinary problems being investigated. At the same time the amount of research artifacts produced by the  research community has grown exponentially making it difficult for individual researchers to filter and search through such  information. In the presence of a vast amount of research information the problem of identifying potential project partners  or collaborators with specific profiles can become extremely difficult. This paper presents a semantic multi-agent architecture  (called SemoRA) aimed at tackling such a problem. The architecture combines agent and Semantic Web technologies in order to  develop a framework capable of efficiently acquiring researcher information, making sense of it and giving meaning to it.  The architecture ultimately enables the retrieval and matching of scored profiles aimed at enhancing collaborations among  researchers – collaborations that can transcend both institutional and national boundaries.    	semantic web	Sadaf Adnan;Amal Tahir;Amna Basharat;Sergio de Cesare	2011		10.1007/978-3-642-16089-9_5	data web;computer science;semantic web;social semantic web;semantic web stack;database;world wide web;information retrieval;semantic analytics	AI	-44.82670022351634	2.8252489558939673	130782
2e4fe928f703bb800e283d6ae071275bc5dbe80c	an information system for distillation data farming	analytical models;information systems;visualization tool;query processing;software prototyping;data visualisation military computing digital simulation data analysis query processing client server systems;sample queries;semantics;client server systems;visualization tool information system distillation data farming project albert us marine corp new sciences computing power data analysis non linearity military decision maker operational questions analysis method semantics database schema sample queries prototype client server software servlets query generation software;operational questions;data visualisation;computer architecture;data analysis;virtual prototyping;computational modeling;new sciences;client server;non linearity;analysis method;computing power;project albert;database schema;software tools;information system;information analysis;us marine corp;query generation software;digital simulation;servlets;military decision maker;military computing;information systems software prototyping military computing data analysis information analysis software tools computer architecture virtual prototyping computational modeling analytical models;distillation data farming;prototype client server software	Project Albert is the US Marine Corp's program for researching the new sciences and leveraging advances in computing power and data analysis to better understand the effects of non-linearity, intangibles, and co-evolving landscapes on answers to military decision-maker's operational questions. The paper describes the design, architecture, and prototyping of an information system to support Data Farming, an analysis method to process and analyze large numbers of simulations. Data Farming and the prototype system are described including semantics, the database schema, sample queries, prototype client/server software and servlets, query generation software, and a visualization tool.	data farming;information system	Ambrose Lewis;Jacob Berlin;Ted Meyer;Stepan Kruglikov;Steve Miller;IV W. Lyver JohnW.Lyver;Ray Gharavi	2001		10.1109/SSDM.2001.938563	computer science;theoretical computer science;data mining;database;semantics;data analysis;programming language;information system;statistics	Robotics	-37.40058463694294	-2.414577033563544	131087
0a56d71e48ff05da3716015e99ad0ad95f511368	"""corrigendum to """"discovering business intelligence from online product reviews: a rule-induction framework"""" [expert systems with applications 39 (15) (2012) 11870-11879]"""	online product review;discovering business intelligence;rule-induction framework;expert systems		expert system;rule induction	Wingyan Chung;Tzu-Liang Tseng	2013	Expert Syst. Appl.	10.1016/j.eswa.2012.07.036	computer science;knowledge management;data science;data mining	Web+IR	-39.41218523929269	-6.707426018507725	131154
e35789132ae68cd74a16e799d22aca789770bebb	metadata mapping and application profiles. approaches to providing the cross-searching of heterogeneous resources in the eu project renardus	data model;application profile;dublin core	This paper present the approach and results of a mapping process to define a common metadata format for cross-searching distributed and heterogeneous subject gateways in the EU project Renardus. The outcome in Renardus is a well defined data model with semantic and syntactical definitions of each metadata element. It results in richer and semantically controlled cross-searching. The metadata elements are mainly based on Dublin Core, some further elements and qualifiers are defined in Renardus namespaces. A collection level description schema has also been developed to allow a well structured description of each participating gateway. A Renardus application profile is under development. The Renardus experience and some of its solutions may well be a good basis for similar interoperability efforts.	data model;dublin core;interoperability	Heike Neuroth;Traugott Koch	2001			computer science;data mining;database;world wide web	HPC	-42.11294979710904	4.14657055792684	131249
61f33ffe5fabdb37c332b60556b8fc22bbc841bf	infrastructure modelling 2.0	infrastructure modelling;modelizacion;web 20;renewable energy;representacion conocimientos;bioelectricity infrastructures;agent based systems;reseau electrique;recoleccion dato;electrical network;ingenierie connaissances;data gathering;system dynamics;inversion;critical infrastructures;information technology;red electrica;biomasse;semantics;mas;technologie information;complexity;sistema complejo;semantica;semantique;investment;modelisation;assembly;evolutionary modelling;multi agent systems;internet;systeme complexe;energia renovable;visualisation;complex system;energie renouvelable;power generation options;investissement;coste;representation connaissance;biomass;web 2 0;semantic web;algorithme evolutionniste;montage;algoritmo evolucionista;agent based modelling;infrastructures;information system;bio electricity;evolutionary algorithm;montaje;biomasa;knowledge representation;tecnologia informacion;collecte donnee;modeling;systeme information;abm;infraestructura;infrastructure;cout;sistema informacion;knowledge engineering	To support stakeholders involved in infrastructure development, we develop evolutionary models of these complex systems, which is a formidable task with respect to data requirements, information representation and knowledge management. Re-addressing a case on bio-electricity infrastructure evolution, we demonstrate first a series of visualisations of economic and ecologic system parameters as they change during infrastructure development over simulated decades. This setup allows us to demonstrate to stakeholders a means to anticipate the consequences of decisions on (dis)investment of power generation options available. In developing these tools, our approach needed to be expanded to better handle the complexity of infrastructure systems, due to the multiple relevant social and technical contexts from which these systems need to be considered. The second part of this paper describes our work on enabling collaborative mapping of our knowledge of infrastructure systems to help integrate diverse types of knowledge. Current internet-enabled developments such as Web 2.0 and the Semantic Web offer tremendous scope to lower the transaction cost of gathering and assembling data. Already, these are changing the ways scientific collaboration is conducted. Finally, we suggest to connect this to evolutionary models to elucidate the dynamics of these systems.	agent-based model;british informatics olympiad;collaborative mapping;complex systems;decision support system;emergence;evolutionary algorithm;knowledge management;requirement;semantic web;web 2.0	Chris Davis;Igor Nikolic;Gerard P. J. Dijkema	2010	IJCIS	10.1504/IJCIS.2010.031073	inversion;renewable energy;information infrastructure;electrical network;spatial data infrastructure;complexity;simulation;biomass;investment;computer science;engineering;artificial intelligence;evolutionary algorithm;semantic web;knowledge engineering;assembly;semantics;system dynamics;operations research;information technology;information system	HCI	-44.508326044274625	0.22057005223283865	131266
04d034081b670b74db2a15aacd09aa6973cc8af3	a data model for object-based seamless digital map using digital map 2.0		Recently, a demand for a spatial information service as well as spatial data model that can manage spatial objects effectively has increased rapidly with the emergence and proliferation of the smart societies due to the introduction and diffusion of Ubiquitous, Intelligent Transport Systems (ITSs), Telematics and Location-based Systems (LBSs). Various spatial data models have been proposed to meet the demand. However, most of these models have some problems such as the construction of exceptional functions, inefficient data management and limited availability. Digital map 2.0, a tile-based data model, which is very popular in South Korea, suffers from the aforementioned problems. This paper tackles the limitations of tile-based digital maps by suggesting a data model for Object-based seamless maps. In essence, the proposed data model can be easily implemented (constructed) and applied to a variety of application domains by constructing object-based data model build on the Digital map 2.0. The proposed model was applied to a real world application scenario as a verification of the model’s applicability to various systems and application domains.	data model;object-based language;seamless3d	Hyeongsoo Kim;Hyun Woo Park;Wonwoo Cho;Incheon Paik;Keun Ho Ryu	2014		10.1007/978-3-319-11538-2_33	data mining;computer science;digital mapping;data management;data model;spatial analysis;telematics;limited availability;intelligent transportation system	HCI	-37.09058584745734	-2.036006670019876	131458
51ca92889f1e79c457828e903e264b7c90274014	proficient node scheduling protocol for homogeneous and heterogeneous wireless sensor networks		1 Research Scholar, Faculty of Information and Communication Engg., Anna University, Chennai, Tamil Nadu 600 025, India 2Department of Electronics and Communication Engineering, Pavendar Bharathidasan College Engineering and Technology, Tiruchirappalli, Tamil Nadu 620 024, India 3 Department of Electronics and Communication Engineering, SKP Engineering College, Tiruvannamali, Tamil Nadu 606 611, India 4Department of Electronics and Communication Engineering, Sri Sairam Engineering College, Chennai, Tamil Nadu 600 044, India	electronic engineering;energy level;scheduling (computing);sketchup;unified model	Radhakrishnan Saravanakumar;N. Mohankumar;J. Raja	2013	IJDSN	10.1155/2013/826482	real-time computing;distributed computing;key distribution in wireless sensor networks;computer network	DB	-45.122554303683394	-7.573286981331374	131606
38f0c67c2abc2f985a0d954e16ce19de64691d55	recommending environmental big data using semantically guided machine learning	expanding knowledge in the environmental sciences;environmental science and management;environmental sciences;expanding knowledge;environmental management	In information technology, Big Data is a collection of data sets so large and complex#R##N#that it becomes difficult to process using on-hand database management tools#R##N#or traditional data-processing applications. The trend to larger data sets is#R##N#due to the additional information derivable from analysis of a single large set of#R##N#related data, as compared with separate smaller sets with the same total amount of#R##N#data. Scientists regularly encounter limitations due to large data sets#R##N#in many areas, including meteorology, genetics, complex physics simulations, and#R##N#environmental research. Wireless technology-based automated data gathering#R##N#from the large environmental sensor networks have increased the quantity of sensor#R##N#data available for analysis and sensor informatics. Next-generation environmental#R##N#monitoring, natural resource management, and agricultural decision support systems#R##N#are becoming heavily dependent on very large scale multiple sensor network deployments,#R##N#massive-scale accumulation, harmonization, web-based Big Data integration#R##N#and interpretation of Big Data. With large amount of the data availability, the complexity#R##N#of data has also increased hence regular maintenance of large-scale sensor#R##N#are becoming a difficult challenge. Uncertainty factors in the environmental monitoring#R##N#processes are more evident than before due to current technological transparency#R##N#achieved by most recent advanced communication technologies.#R##N#The other challenges include capture, storage, search, sharing, analysis, and visualization.#R##N#Data availability from a particular environmental sensor web is often very#R##N#limited and data quality is subsequently very poor. This practical limitation could be#R##N#due to difficult geographical location of the sensor node or sensor station, extreme#R##N#environmental conditions, communication network failure, and lastly technical failure#R##N#of the sensor node. Data uncertainty from a sensor network makes the network#R##N#unreliable and inefficient. This inefficiency leads to failure of natural resource management#R##N#systems such as agricultural water resource management, weather forecast,#R##N#crop management including irrigation scheduling and natural resource-based#R##N#crop business model systems. The ultimate challenge in environmental forecasting#R##N#and decision support systems, is to overcome the data uncertainty and make the#R##N#derived output more accurate. It is evident that there is a need to capture and integrate#R##N#environmental knowledge from various independent sources including sensor#R##N#networks, individual sensory system, large-scale environmental simulation models,#R##N#and historical environmental data for each of the independent#R##N#sources). It is not good enough to produce efficient decision support system using a#R##N#single data source. So there is an urgent requirement for on demand complementary#R##N#knowledge integration where different sources of environmental sensor data could#R##N#be used to complement each other automatically.	big data;machine learning	Ritaban Dutta;Ahsan Morshed;Jagannath Aryal	2014		10.1201/b17112-16	engineering;data science;data mining;management science	ML	-39.60773322668341	-3.3352628101099357	131722
ae1d7764ce79388f7d47ddb784bc7dfc95979f83	energy efficiency in big data complex systems: a comprehensive survey of modern energy saving techniques	simulation and modeling;operation research decision theory;operations research management science;complex systems	Introduction Due to the advancement in computer technology, the computer systems have become widespread and complex. This complex arrangement of the system results into a complex adaptive system (CAS). A huge increase in the scale and complexity of the systems have been observed (Niazi and Laghari 2012) since from last few years. CAS exists in different forms. The management of such systems is carried out with efficient algorithms and is only controlled by different computational methods (Batool and Niazit 2015). Big data is one of the examples of such complex systems. CAS is comprised of large entities and components which require interaction and adoption in performing certain operations. The nature of the big data is more or less the same as complex systems. In other words, big data is one of the forms of the complex systems. Most of the CASs are tied to Abstract	algorithm;big data;complex adaptive system;complex systems;computation;computer;entity	Abdul Majeed;Munam Ali Shah	2015	CASM	10.1186/s40294-015-0012-5	simulation;engineering;data science;data mining	DB	-47.96886601021347	-0.40360464421083553	131823
f4c4a6c3f5b8c8e0486bdd637acfa6a60f24252e	user-defined view automation of genomic databases	database views;database system;search method;keyword search;genome database;ontology	This paper presents a solution to the problem of creating a subset database from the public genome databases, also known as a database view. While the techniques to generate views are well established already in the database system there are still some problems found where applying this technique in the genome database environment. The main problems that exist in the current methods of view creation are missing relevant results, returning irrelevant results and view creation processes are generally very time consuming for the user. The solution presented within provides an automated approach aimed at reducing the time needed to create a view, which is usually done by hand. The solution improves the searching method needed for view creation by the addition of two extra phases; the first, expanding the keyword search so that it captures all relevant results and second, a filtering phase to remove all the extra irrelevant results. The whole process is done in the background so that the user isn't required to spend much time fixing the results of inadequate search tools.	database;relevance;search algorithm;view (sql)	Andrew J. Robinson;J. Wenny Rahayu;Susanna Herd;Tharam S. Dillon	2005		10.1145/1066677.1066829	computer science;ontology;data mining;database;view;world wide web;database schema;information retrieval;database testing;database design	DB	-33.95732626289697	3.7395893704706538	131909
30935afd0b90bc9dce9794e9cfa444228eadcb22	a pragmatic approach to semantic repositories benchmarking	query language;institutional repository research archive oaister;image retrieval	The aim of this paper is to benchmark various semantic repositories in order to evaluate their deployment in a commercial image retrieval and browsing application. We adopt a two-phase approach for evaluating the target semantic repositories: analytical parameters such as query language and reasoning support are used to select the pool of the target repositories, and practical parameters such as load and query response times are used to select the best match to application requirements. In addition to utilising a widely accepted benchmark for OWL repositories (UOBM), we also use a real-life dataset from the target application, which provides us with the opportunity of consolidating our findings. A distinctive advantage of this benchmarking study is that the essential requirements for the target system such as the semantic expressivity and data scalability are clearly defined, which allows us to claim contribution to the benchmarking methodology for this class of applications.	benchmark (computing);computer simulation;image retrieval;query language;real life;requirement;scalability;software deployment;two-phase locking	Dhavalkumar Thakker;Taha Osman;Shakti Gohil;Phil Lakin	2010		10.1007/978-3-642-13486-9_26	image retrieval;computer science;data mining;database;world wide web;information retrieval;query language	SE	-36.28153415832719	3.8845147947990273	132004
9e9e9b4c6ceea05f84b673fb6801a0f83b93a8fb	the art of scheduling for big data science.		Many applications generate big data, like social networking and social influence programs, Cloud applications, public websites, scientific experiments and simulations, data warehouses, monitoring platforms, and e-government services. Data grow rapidly, since applications produce continuously increasing volumes of unstructured and structured data. The impact on data processing, transfer, and storage is the need to reevaluate the approaches and solutions to better answer user needs. In this context, scheduling models and algorithms have an important role. A large variety of solutions for specific applications and platforms exist, so a thorough and systematic analysis of existing solutions for scheduling models, methods, and algorithms used in big data processing and storage environments has high importance. This chapter presents the best of existing solutions and creates an overview of current and near-future trends. It will highlight, from a research perspective, the performance and limitations of existing solutions and will offer the scientists from academia and designers from industry an overview of the current situation in the area of scheduling and resource management related to big data processing.	algorithm;big data;data science;e-government;experiment;scheduling (computing);simulation	Florin Pop;Valentin Cristea	2015		10.1201/b18050-9	computer science;data science;data mining;operations research	DB	-37.33276613951709	-6.34905405286629	132405
ec6e171cb582b07c6d928dd97aae7dcbe2271d6f	cpd 2018: the first workshop on combining physical and data-driven knowledge in ubiquitous computing		Real-world ubiquitous computing systems face the challenge of requiring a significant amount of data to obtain accurate information through pure data-driven approaches. The performance of these data-driven systems greatly depends on the quantity and 'quality' of data. In ideal conditions, pure data-driven methods perform well due to the abundance of data. However, in real-world systems, collecting data can be costly or impossible due to practical limitations. Physical knowledge, on the other hand, can be used to alleviate these issues of data limitation. This physical knowledge can include domain knowledge from experts, heuristics from experiences, as well as analytic models of the physical phenomena.  This workshop aims to explore the intersection between (and the combination of) data and physical knowledge. The workshop will bring together domain experts that explore the physical understanding of the data, practitioners that develop systems and the researchers in traditional data-driven domains. The workshop welcomes addressing these issues in different applications/domains as well as algorithmic and systematic approaches to applying physical knowledge. Therefore, we further seek to develop a community that systematically analyzes the data quality regarding inference and evaluates the improvements from the physical knowledge. Preliminary and on-going work is welcomed.	algorithm;collaborative product development;data quality;experience;heuristic (computer science);pure data;ubiquitous computing;world-system	Shijia Pan;Xinlei Chen	2018		10.1145/3267305.3274144	human–computer interaction;data collection;domain knowledge;ubiquitous computing;inference;data-driven;mobile device;heuristics;computer science;data quality	AI	-37.12568461135025	-8.56025403831968	132407
ee9048b8d413d2e2da186349b08f9928daa9a3f4	dynamic role assignment for large-scale multi-agent robotic systems		In thi sp aper ,w e introduce an approach fo rd esignin ga n dd eploy- ing organizations on large scale Multi-Agen tR obotic Systems .L yin g on the well-known organizational meta-mode lA GR ,w ep r opose a rol ea ss ignment protocol fo ra utomatically distributin gt he roles over the robots .W eh av er un as er ies o fs imulation st ov alidate the approach' sf eas ibility. The simulations show that the protocol is well scalabl ea swell.	robot	Van Tuan Le;Serge Stinckwich;Noury Bouraqadi;Arnaud Doniec	2009		10.1007/978-3-642-16098-1_19	telecommunications;engineering;communication;cartography	AI	-45.9112560549315	-4.5015277326789205	132521
fce57b4f25f63dc95bc44a3e3568085dfbad40e9	rdfreduce: customized aggregations with provenance for rdf data based on an industrial use case	semantic aggregations;linked data;provenance;industrial systems	Industrial manufacturing environments are typically supported by a backbone network of technical control- and business information systems. To carry out daily work routines like maintaining technical processes or managing business information like sales and orders, different user groups as, e.g., IT management or engineers, need different forms of integrated and aggregated views on the information stored in and flowing through these systems. In this paper, we are building on recent advances in the fields of Information Integration, Semantic Web and Linked Data and report on an approach called RDFreduce to meet aforementioned needs. RDFreduce allows for the creation of custom aggregated views on heterogeneous RDF datasets. The aggregates are equipped with provenance information, in order to be tracked back easily to the datasets they origin from. An industrial use case of RDFreduce stemming from steel production especially aiming at integrating and aggregating information flows is presented.	internet backbone;linked data;management information system;resource description framework;semantic web;stemming	Christian Lettner;Mario Pichler;Wilhelm Kirchmayr;Friedrich Kokert;Martin Habringer	2013		10.1145/2539150.2539207	engineering;data mining;database;world wide web	AI	-38.86849233162213	1.830465248216986	132730
a49c631c4bb9adad5ff507d6b1d1864aeadeee6b	ontology based expert-system for suspicious transactions detection		The development of an effective mechanism to detect suspicious transactions is a critical problem for financial institutions in their endeavor to prevent anti-money laundering activities. This research addresses this problem by proposing an ontology based expert-system for suspicious transaction detection. The ontology consists of domain knowledge and a set of (SWRL) rules that together constitute an expert system. The native reasoning support in ontology is used to deduce new knowledge from the predefined rules about suspicious transactions. The presented expert-system has been tested on a real data set of more than 8 million transactions of a commercial bank. The novelty of the approach lies in the use of ontology driven technique that not only minimizes the data modeling cost but also makes the expert-system extendable and reusable for different applications.	computation;data modeling;expert system;extensibility;heuristic (computer science);knowledge base;semantic web rule language;semantic reasoner;web ontology language	Quratulain Nizamuddin Rajput;Nida Sadaf Khan;Asma Sanam Larik;Sajjad Haider	2014	Computer and Information Science	10.5539/cis.v7n1p103	computer science;data mining;database;ontology-based data integration;world wide web;computer security;process ontology	Web+IR	-38.657117907580734	3.434402341306179	132896
3b2b88f2726bdaccc9b49ac863d04e9bccc1fde0	an integrated information system for snowmelt flood early-warning based on internet of things	integrated information system;implementation;enterprise systems;e science;data;snowmelt flood early warning;machine;internet of;internet of things;decision support system;water resources management;remote sensing;期刊论文;model;design;cloud services;service workflow;geoinformatics;things	Floods and water resource management are major challenges for human in present and the near future, and snowmelt floods which usually break out in arid or semi-arid regions often cause tremendous social and economic losses, and integrated information system (IIS) is valuable to scientific and public decision-making. This paper presents an integrated approach to snowmelt floods early-warning based on geoinformatics (i.e. remote sensing (RS), geographical information systems (GIS) and global positioning systems (GPS)), Internet of Things (IoT) and cloud services. It consists of main components such as infrastructure and devices in IoT, cloud information warehouse, management tools, applications and services, the results from a case study shows that the effectiveness of flood prediction and decision-making can be improved by using the IIS. The prototype system implemented in this paper is valuable to the acquisition, management and sharing of multi-source information in snowmelt flood early-warning even in other tasks of water resource management. The contribution of this work includes developing a prototype IIS for snowmelt flood early-warning in water resource management with the combination of IoT, Geoinformatics and Cloud Service, with the IIS, everyone could be a sensor of IoT and a contributor of the information warehouse, professional users and public are both servers and clients for information management and services. Furthermore, the IIS provides a preliminary framework of e-Science in resources management and environment science. This study highlights the crucial significance of a systematic approach toward IISs for effective resource and environment management.	information system;internet of things;syn flood	Shifeng Fang;Lida Xu;Yunqiang Zhu;Yongqiang Liu;Zhihui Liu;Huan Pei;Jianwu Yan;Huifang Zhang	2015	Information Systems Frontiers	10.1007/s10796-013-9466-1	design;machine;simulation;decision support system;computer science;data mining;database;implementation;world wide web;computer security;internet of things;geoinformatics;data	DB	-39.780374966768626	-3.361867724806962	132908
549602e8d6ae9096fadcfa06d2ba9a8cc8aaa696	a navigation model for exploring scientific workflow provenance graphs	integrated approach;scientific workflow;dependence graph;workflow interoperability;publish subscribe;web services;notification message;ws eventing	"""Many scientific workflow systems record provenance information in the form of data and process dependencies as part of workflow execution. Users often wish to explore these dependencies to reproduce, validate, and explain workflow results, e.g., by examining the data and processes that were used to produce particular workflow outputs. A natural interface for determining relevant provenance information, which is adopted by many systems, is to display the complete provenance dependency graph. However, for many workflows, provenance graphs can be large, with thousands or more nodes and edges. Displaying an entire provenance graph for such workflows can result in """"provenance overload,"""" where the large amount of provenance information available makes it difficult for users to find relevant information and explore data and process dependencies. In this paper, we address the challenges of """"provenance overload"""" through a novel navigation model that provides operations for creating different views of provenance graphs along with approaches for easily navigating between different views. Further, our proposed navigation model provides an integrated approach for exploring, summarizing, and querying portions of provenance graphs. We also discuss different architectures for efficiently navigating large provenance graphs against an underlying provenance database."""		Manish Kumar Anand;Shawn Bowers;Bertram Ludäscher	2009		10.1145/1645164.1645166	web service;computer science;data mining;database;publish–subscribe pattern;law;world wide web;workflow engine;workflow technology	DB	-36.651075106851565	3.0544994804634995	132954
5109c00fafea26645509fe5c3ed9e4cba7a306e2	privacy-preserving data mashup	data mining;etl;data warehousing;data exploration;business intelligence;web technology;data integration	Mashup is a web technology that combines information from more than one source into a single web application. This technique provides a new platform for different data providers to flexibly integrate their expertise and deliver highly customizable services to their customers. Nonetheless, combining data from different sources could potentially reveal person-specific sensitive information. In this paper, we study and resolve a real-life privacy problem in a data mashup application for the financial industry in Sweden, and propose a privacy-preserving data mashup (PPMashup) algorithm to securely integrate private data from different data providers, whereas the integrated data still retains the essential information for supporting general data exploration or a specific data mining task, such as classification analysis. Experiments on real-life data suggest that our proposed method is effective for simultaneously preserving both privacy and information usefulness, and is scalable for handling large volume of data.	algorithm;data mining;experiment;information privacy;information sensitivity;mashup (web application hybrid);real life;scalability;web application	Noman Mohammed;Benjamin C. M. Fung;Ke Wang;Patrick C. K. Hung	2009		10.1145/1516360.1516388	data modeling;data quality;computer science;data virtualization;data integration;data warehouse;data mining;database;business intelligence;world wide web;mashup;data mapping	ML	-42.35286240322174	-0.6012355846670414	132981
bec1ce7e27827fdfa6608a15157ed37a004000e2	biggis: a continuous refinement approach to master heterogeneity and uncertainty in spatio-temporal big data (vision paper)	big data analytics;data architecture;knowledge generation;inproceedings;knowledge generation big data analytics data architecture	Geographic information systems (GIS) are important for decision support based on spatial data. Due to technical and economical progress an ever increasing number of data sources are available leading to a rapidly growing fast and unreliable amount of data that can be beneficial (1) in the approximation of multivariate and causal predictions of future values as well as (2) in robust and proactive decision-making processes. However, today's GIS are not designed for such big data demands and require new methodologies to effectively model uncertainty and generate meaningful knowledge. As a consequence, we introduce BigGIS, a predictive and prescriptive spatio-temporal analytics platform, that symbiotically combines big data analytics, semantic web technologies and visual analytics methodologies. We present a novel continuous refinement model and show future challenges as an intermediate result of a collaborative research project into big data methodologies for spatio-temporal analysis and design for a big data enabled GIS.	approximation;big data;causal filter;decision support system;geographic information system;proactive parallel suite;refinement (computing);semantic web;visual analytics	Patrick Wiener;Manuel Stein;Daniel Seebacher;Julian Bruns;Matthias Frank;Viliam Simko;Stefan Zander;Jens Nimis	2016		10.1145/2996913.2996931	analytics;big data;computer science;knowledge management;data science;data mining;database;business intelligence;semantic analytics;data architecture	DB	-36.137822740236416	-2.2431101349816847	133000
cf936d2b0c8bae5b60214601fcceaddc688a9370	better knowledge management through knowledge engineering	knowledge management knowledge engineering knowledge representation relational databases companies technology management engineering management knowledge acquisition petroleum terminology;business data processing knowledge engineering knowledge representation knowledge acquisition oil technology natural gas technology;knowledge deployment knowledge management knowledge engineering technology knowledge acquisition processes structured knowledge capture knowledge representation technology case study drilling optimization group oil and gas service company knowledge engineering practices knowledge management task knowledge storage;knowledge management;relational database;natural gas technology;business data processing;knowledge acquisition;oil technology;knowledge representation;knowledge engineering	captured knowledge must then be made available throughout the organization in a timely manner. In terms of technology, most current knowledge management activities rely on database and Internet systems. If knowledge is stored explicitly at all, it is typically in databases either as simple tables (for example, relational databases) or semistructured text (as in Lotus Notes). The use of sophisticated knowledge representation systems such as Classic, Loom, or G2 is rare. Also, few organizations have a systematic process for capturing knowledge, as distinct from capturing information. (See the “Current Practice” sidebar for a description of techniques.) We believe that current knowledge management practice significantly under-utilizes knowledge-engineering technology, despite recent efforts to promote its use.4 In this article, we focus on two knowledgeengineering processes:	gnutella2;ibm notes;knowledge engineering;knowledge management;knowledge representation and reasoning;loom;lotus 1-2-3;relational database	Alun D. Preece;Alan Flett;Derek H. Sleeman;David Curry;Nigel Meany;Phil Perry	2001	IEEE Intelligent Systems	10.1109/5254.912383	knowledge base;organizational learning;knowledge integration;idef3;relational database;computer science;knowledge management;artificial intelligence;data science;mathematical knowledge management;knowledge-based systems;knowledge engineering;open knowledge base connectivity;data mining;procedural knowledge;knowledge extraction;personal knowledge management;commonsense knowledge;knowledge value chain;domain knowledge	DB	-37.591265679048135	-0.9643483562931349	133056
42c814359feb275032c1ee95ba8ddb25c55b1a8e	database systems for advanced applications		As data expands into big data, enhanced or entirely novel data mining algorithms often become necessary. The real value of big data is often only exposed when we can adequately mine and learn from it. We provide an overview of new scalable techniques for knowledge discovery. Our focus is on the areas of cloud data mining and machine learning, semi-supervised processing, and deep learning. We also give practical advice for choosing among different methods and discuss open research problems and concerns.	algorithm;big data;data mining;deep learning;machine learning;open research;scalability;semiconductor industry	Josef Kittler;Friedemann Mattern;John C. Mitchell;Moni Naor	2015		10.1007/978-3-319-18123-3		ML	-36.72003072811358	-6.5141156383700585	133415
b956b6230d29c34d20b3d4ff8a78d4e2784a8b1d	pcs2owl: a generic approach for deriving web ontologies from product classification systems		The classification of products and services enables reliable and efficient electronic exchanges of product data across organizations. Many companies classify products (a) according to generic or industryspecific product classification standards, or (b) by using proprietary category systems. Such classification systems often contain thousands of product classes that are updated over time. This implies a large quantity of useful product category information for e-commerce applications on the Web of Data. Thus, instead of building up product ontologies from scratch, which is costly, tedious, error-prone, and high-maintenance, it is generally easier to derive them from existing classifications. In this paper, we (1) describe a generic, semi-automated method for deriving OWL ontologies from product classification standards and proprietary category systems. Moreover, we (2) show that our approach generates logically and semantically correct vocabularies, and (3) present the practical benefit of our approach. The resulting product ontologies are compatible with the GoodRelations vocabulary for e-commerce and with schema.org and can be used to enrich product and offer descriptions on the Semantic Web with granular product type information from existing data sources.	cognitive dimensions of notations;e-commerce;emoticon;ontology (information science);product type;schema.org;semantic web;semiconductor industry;vocabulary;web ontology language;world wide web	Alex Stolz;Bene Rodriguez-Castro;Andreas Radinger;Martin Hepp	2014		10.1007/978-3-319-07443-6_43	product type;data mining;information retrieval;semantic web;product category;computer science;ontology (information science);scratch;product classification;vocabulary	Web+IR	-39.95814867477252	4.041691933027896	133768
542bcb79920ddd304d8adcb9db8790e97ec4b420	big data generation		Big data challenges are end-to-end problems. When handling big data it usually has to be preprocessed, moved, loaded, processed, and stored many times. Current benchmarks related to big data, however, only focus on isolated aspects. In this abstract, we briefly discuss the necessity of ETL like tasks in big data benchmarking and make a case for the parallel data generation framework (PDGF) for big data generation. PDGF is an open-source generic data generator that was implemented at the University of Passau and is currently adopted in TPC benchmarks.	big data;end-to-end encryption;ibm tivoli storage productivity center;open-source software	Tilmann Rabl;Hans-Arno Jacobsen	2012		10.1007/978-3-642-53974-9_3	engineering;data science;data mining;database	ML	-34.12555158142741	-0.3711020147804021	134004
4ba44b4c66d3551efc82c961386a73f7891929af	a hierarchical approach to improving data quality	data quality		data quality	Marcey L. Abate;Kathleen V. Diegert;Heather W. Allen	1998	Data Quality Journal		data science;data mining;computer science;data quality	DB	-39.716446000400566	-5.5628150290483696	134347
cfc2a6057726582cf05b65e6e2b1b4123d0a1d60	evaluating hybrid queries through service coordination in hypatia	anonymity;mobility data;data mining;trajectories;dynamic environment;access to information;query evaluation;privacy	The emergence of mobile and ambient computing technologies brings democratization in the access to information and data; services play a crucial role, thereby opening new research challenges for data querying. A promising method to access data within these novel dynamic environments in a convenient and efficient way is declarative querying. Such queries, which we characterize as hybrid queries, involve streaming and on-demand data originated from services, possibly with temporal and mobile properties. To evaluate these queries we propose an approach implemented in the HYPATIA system, which addresses two main aspects (i) using service coordination for query evaluation and (ii) an efficient and flexible mechanism that facilitates incorporating new capabilities.	emergence;freedom of information laws by country	Víctor Cuevas-Vicenttín;Genoveva Vargas-Solar;Christine Collet	2012		10.1145/2247596.2247676	anonymity;computer science;trajectory;data mining;database;privacy;world wide web	DB	-35.47457257572329	1.6369623537249025	134423
3e1b2d7f940c5a469fe94ff6b32a9d581fc0bcca	incmap: a journey towards ontology-based data integration			ontology-based data integration	Christoph Pinkel;Carsten Binnig;Ernesto Jiménez-Ruiz;Evgeny Kharlamov;Andriy Nikolov;Andreas Schwarte;Christian Heupel;Tim Kraska	2017			data science;data mining;ontology-based data integration;computer science	AI	-39.70257422351995	-6.173909537071226	134472
56851cdf2d90d107f296e0234ab80e55fef67f36	searching shared content in communities with the data ring	database system	Information ubiquity has created a large crowd of users (most notably scientists), who could employ DBMS technology to share and search their data more effectively. Still, this user base prefers to keep its data in files that can be easily managed by applications such as spreadsheets, rather than deal with the complexity and rigidity of modern database systems. In this article, we describe a vision for enabling non-experts, such as scientists, to build content sharing communities in a true database fashion: declaratively. The proposed infrastructure, called the data ring, enables users to share and search their data with minimal effort; the user points to the data that should be shared, and the data ring becomes responsible for automatically indexing the data (to make it accessible), replicating it (for availability), and reorganizing its physical storage (for better query performance). We outline the salient features of our proposal, and outline recent technical advancements in realizing data rings.	database;spreadsheet	Serge Abiteboul;Neoklis Polyzotis	2009	IEEE Data Eng. Bull.		data-intensive computing;database;data administration;data mining;automatic indexing;search engine indexing;computer science;rigidity (psychology);data center	DB	-46.40979119000511	0.4432490267457666	134476
feb05b9f30586eada452703dd1050ea88884fc4b	an object-relational data warehouse modelig for complex data	star schema;complex data;object reltiaonl;data warehouse;object relational	A data warehouse is an essential component to the decision support system. The traditional data warehouse provides only numeric and character data analysis. But as information technologies progress, complex data such as semi-structured and unstructured data become vastly used. Developing new data warehouse technologies to accommodate the demand of integrating complex data is important. In this paper, we propose a data model for the complex data warehouse. This model incorporates the well-known star schema with object-relational concepts, inheriting the easy-understanding and multidimensional characteristic, and providing mechanisms to integrate complex data in a data warehouse model.	data model;decision support system;object-relational database;semi-structured data;semiconductor industry;star schema;whole earth 'lectronic link	Wen-Yang Lin;Chin-Ang Wu;Chuan-Chun Wu	2006		10.2991/jcis.2006.253	data transformation;dimensional modeling;data science;data warehouse;star schema;data mining;database;database design	DB	-36.198589534954365	-1.3127189276520637	134757
e0a04129c183ab386ff6dcdb2b579baaca4732fc	accessibility commons: a metadata infrastructure for web accessibility	new technology;metadata;web accessibility;visually impaired users;database;annotation;assistive technology	Research projects, assistive technology, and individuals all create metadata in order to improve Web accessibility for visually impaired users. However, since these projects are disconnected from one another, this metadata is isolated in separate tools, stored in disparate repositories, and represented in incompatible formats. Web accessibility could be greatly improved if these individual contributions were merged. An integration method will serve as the bridge between future academic research projects and end users, enabling new technologies to reach end users more quickly. Therefore we introduce Accessibility Commons, a common infrastructure to integrate, store, and share metadata designed to improve Web accessibility. We explore existing tools to show how the metadata that they produce could be integrated into this common infrastructure, we present the design decisions made in order to help ensure that our common repository will remain relevant in the future as new metadata is developed, and we discuss how the common infrastructure component facilitates our broader social approach to improving accessibility.	assistive technology;web accessibility	Shinya Kawanaka;Yevgen Borodin;Jeffrey P. Bigham;Darren Lunn;Hironobu Takagi;Chieko Asakawa	2008		10.1145/1414471.1414500	human–computer interaction;web accessibility initiative;web standards;computer science;web accessibility;data mining;metadata;world wide web	HCI	-44.365027247768104	1.510349902508794	134763
b29361161a3f48b16481d0aabb9b75aeb04e39e6	delay-bounded skyline computing for large-scale real-time online data analytics			real-time locating system	Qian Wang;Chao Yu;Yiming Zhang;Huiba Li;Ping Zhong	2017	Concurrency and Computation: Practice and Experience	10.1002/cpe.4085	analytics;data science;data mining;database;business intelligence	DB	-38.55608699045877	-6.081519204497842	135647
eaac267f7690da87f525ffc91f68a60139fbf445	good: a geographical data manager using spatial indices	data management			Byunng-Wao Oh;Ki-Joon Han	1998			database;data mining;data management;business	DB	-38.4367566813084	-3.9575743019685685	135728
6f3713af98d7eb2824ba50f23f8c0308d057e06e	medical image segmentation for mobile electronic patient charts using numerical modeling of iot		1 The Research Institute of IT, Chosun University, 309 Pilmun-daero, Dong-gu, Gwangju 501-759, Republic of Korea 2 Electronics and Telecommunications Research Institute, 161 Gajeong-dong, Yuseong-gu, Daejeon 305-350, Republic of Korea 3 Department of Information Security, Seowon University, 377-3 Musimseo-ro, Heungdeok-gu, Cheongju-si, Choong-Chung Buk-do 361-742, Republic of Korea 4Department of Electronics Engineering, Chosun University, 309 Pilmun-daero, Dong-gu, Gwangju 501-759, Republic of Korea	electronic engineering;image segmentation;information security	Seung-Hoon Chae;Daesung Moon;Deok-Gyu Lee;Sung Bum Pan	2014	J. Applied Mathematics	10.1155/2014/815039	segmentation-based object categorization;data mining;image segmentation;scale-space segmentation	Robotics	-45.30484533680995	-8.937576704346347	136076
a9ec8531dec713060ce548536f8f6931d218f4a6	intelligent systems in context-based distributed information fusion	articulo;sistemas inteligentes;info eu repo semantics article;fusion de la informacion;distributed sensor networks;intelligent systems;information fusion;redes de sensores distribuidos	1 University Carlos III de Madrid, Avenida Gregorio Peces-Barba Mart́ınez 22, Colmenarejo, 28270 Madrid, Spain 2University of Salamanca, Plaza de la Merced S/N, 37008 Salamanca, Spain 3 Technical University of Madrid, Campus Montegancedo, Boadilla del Monte, 28660 Madrid, Spain 4Vestec, Inc., 125 Northfield Drive West, Waterloo, ON, Canada N2L 6N8 5University College Dublin, Belfield, Dublin 4, Ireland	campus party;linear algebra	Miguel A. Patricio;Jesús Caja García;Juan Manuel Corchado;Javier Bajo;Alaa M. Khamis;Eleni E. Mangina	2013	IJDSN	10.1155/2013/836463	intelligent decision support system;computer science;artificial intelligence;world wide web	Crypto	-45.5675798813592	-9.546127114046154	136092
cceaa8f79fb56e64064d90d7047b720404ac15c3	optimal course scheduling for united states air force academy cadets		Gerardo Gonzalez,a Christopher Richards,b Alexandra Newmanc aDepartment of Management, United States Air Force Academy, Colorado Springs, Colorado 80920; bDeputy Under Secretary of the Air Force, Management, 1790 Air Force Pentagon, Washington, DC 20330; cDepartment of Mechanical Engineering, Colorado School of Mines, Golden, Colorado 80401 Contact: gerry.gonzalez@usafa.edu (GG); christopher.richards@us.af.mil (CR); anewman@mines.edu, http://orcid.org/0000-0001-9886-9721 (AN)	academy;gadu-gadu;pentagon;schedule (project management)	Gerardo Gonzalez;Christopher Richards;Alexandra Newman	2018	Interfaces	10.1287/inte.2017.0935	operations management;discontinuation;project commissioning;personalization;information system;scheduling (computing);cadet;software;oracle;engineering	AI	-44.50984093304374	-8.898696915896853	136244
5ed81f6f20ee296f17be1b1aa184c7bfaaf9d07f	skait: a parameterized key assignment scheme for confidential communication in resource constrained ad hoc wireless networks	collusion resistance;symmetric key assignment;confidential communication;key predistribution	http://dx.doi.org/10.1016/j.adhoc.2014.04.006 1570-8705/ 2014 Elsevier B.V. All rights reserved. q Parts of this work have appeared earlier in the Proceedings of the 9th International Symposium on Parallel and Distributed Computing (ISPDC), 2010. ⇑ Corresponding author. Address: Department of Computer Science, The University of Texas at Dallas, 800 W Campbell Rd, MS EC 31, Richardson, TX 75080, USA. Tel.: +1 972 883 2347; fax: +1 972 883 2349. E-mail address: neerajm@utdallas.edu (N. Mittal). 1 The author is currently working at Symantec in San Francisco, California, USA. 2 This work was supported, in part, by the National Science Foundation (NSF) under Grant Number CNS-1115733. Ramon L. Novales , Neeraj Mittal ⇑,2, Kamil Sarac	computer science;confidentiality;distributed computing;fax;hoc (programming language);ibm notes;like button;richardson number	Ramon Novales;Neeraj Mittal;Kamil Saraç	2014	Ad Hoc Networks	10.1016/j.adhoc.2014.04.006	computer science;distributed computing;computer security;computer network	Arch	-44.415702452596236	-7.575178721834939	136497
6f0682adeb3c57417bb68465fe22dd8346b56433	an investigation of how data quality is affected by dataset size in the context of big data analytics			data quality	Philip Woodall;Alexander Borek;Jing Gao;Martin Oberhofer;Andy Koronios	2014			data science;big data;data mining;computer science;analytics;data quality	ML	-38.24290352145113	-6.394613586781071	136526
56ad09adf4575bb768d47f535e540db6d18b0d16	entropy parameter m in modeling a flow duration curve		Yu Zhang 1, Vijay P. Singh 1,2,* and Aaron R. Byrd 3 1 Department of Biological and Agricultural Engineering, Texas A&M University, College Station, TX 77840, USA; zhangyu199002@tamu.edu 2 Zachry Department of Civil Engineering, Texas A&M University, College Station, TX 77843-2117, USA 3 Hydrologic Systems Branch, Coastal and Hydraulics Laboratory, Engineer Research Development Center, U.S. Army Corps of Engineers, Vicksburg, MS 39181, USA; Aaron.R.Byrd@erdc.dren.mil * Correspondence: vsingh@tamu.edu; Tel.: +1-979-845-7028	like button	Yu Zhang;Vijay P. Singh;Aaron R. Byrd	2017	Entropy	10.3390/e19120654	entropy (information theory);mathematics;statistics;hydrology;streamflow;hydropower;sediment;drainage basin;climate change	ML	-45.292944279670394	-9.542428958074504	136550
69eab6f9cc8cb33ebfcdeba97f64f822b0b9e892	toward core subject vocabularies for community-oriented subject gateways	community oriented gateways;metadata vocabularies;subject vocabularies;vocabulary maintenance system;subject gateway;core vocabularies;community oriented subject vocabulary;interoperability;subject gateways;dublin core;vocabulary maintenance systems;core vocabulary	Subject classification schemes and vocabularies for subject indexing, i.e. subject vocabularies, have a crucial role in the development of subject gateways. From our experiences on a few subject gateway projects which are designed for regional and domainspecific communities, we learned that the subject gateways require a subject vocabulary which is reasonably small and tailored in accordance with the resources in the domain and the audience. The goal of this paper is to discuss underlying issues for the small “core” subject vocabularies defined as a subject description/classification scheme for community oriented resources. First, this paper describes some basic aspects for metadata schema and requirements for the region/domain-specific subject gateways. Second, we show three subject gateway projects which are resources designed primarily for libraries and public library users. And then, we show a few examples of small vocabularies designed for regional resources. This paper compares some vocabularies designed for regional resources and discusses issues for the core subject vocabularies. Costs to develop and maintain the vocabularies can not be neglected even if they are small. We consider the Semantic Web technologies will help develop software tools to support development and maintenance of the subject vocabularies and to enhance their interoperability and reusability.	application domain;bespoke;browsing;comparison and contrast of classification schemes in linguistics and metadata;domain-specific language;guide to information sources;internet;interoperability;library (computing);public library;requirement;resource description framework;semantic web;statistical classification;subject indexing;vocabulary;world wide web	Wonsook Lee;Shigeo Sugimoto	2005	IJMSO	10.1504/IJMSO.2006.012340	natural language processing;interoperability;controlled vocabulary;computer science;database;world wide web	Web+IR	-42.6574089147736	3.714026357815576	136565
507e1585f9e7bb06c16def292f869364014fa107	the integration of research information sources at university level	information sources	When building our new CRIS system, we have tried to embed in the system a series of success factors. Accordingly, we believe that some of the most important aspects of the system are mechanisms for minimising as much as possible the registration efforts of the research community, e.g. by providing a series of data entry support tools. Moreover, the ultimate success of the system is dependent on the possibilities of creating flexible and diverse output products in the system’s different fields of usage and in various user-defined formats.	ccir system a	Cecile Ohm;Johanne Revheim;Jostein H. Hauge	2001			data science;data mining;information system;computer science	HPC	-43.41026311896534	0.11354440411181793	136817
814ea8ed14d2303d8e16a0873607e081b5d2c43b	describing capabilities of internet data sources for information discovery and sharing	process capability;information discovery;information system;data structure	Effective Compression for the Web: Exploiting Document Linkages p. 68 Derivation of Incremental Equations for Nested Relations p. 76 Self-Maintaining Web Pages--An Overview p. 83 Efficient Declustering Techniques for Temporal Access Structures p. 91 An Algorithm for Answering Queries Efficiently Using Views p. 99 Measuring Similarity of Interests for Clustering Web-Users p. 107 Similarity-Based Algebra for Multimedia Database Systems p. 115 An Agent-Based Approach for Supporting Cross-Enterprise Workflows p. 123 Assessment by Belief p. 131 Are Two Pictures Better Than One? p. 138 Heuristic Algorithms for I/O Scheduling for Efficient Retrieval of Large Objects from Tertiary Storage A Comparison of Multi-Level Concurrency Control Protocols p. 153 Evolution Support in Large-Scale Interoperable Systems: A Metadata Driven Approach p. 161	information discovery	Jian Yang;Lai Xu	2001		10.1145/545538.545540	computer science;data mining;database;information retrieval	DB	-43.569800628179344	-5.51426530712295	136958
f1a82d8eea944eaa32beae1eaf7f11a4096c2ec2	large scale data mining to improve usability of data - an intelligent archive testbed	large scale systems data mining usability geoscience nasa earth observing system system testing information systems intelligent structures bridges;information systems;bridges;data mining;earth sciences;large scale;data storage;geoscience;remote sensing;system testing;artificial intelligence;knowledge representation;earth observing system;usability;intelligent structures;nasa;large scale systems	* This work was performed by the first author as part of his duties as a U.S. Government employee. It was supported by NASA’s Intelligent Systems Project and the Earth Science Data and Information System Project. The remaining authors worked as subcontractors under Cooperative Agreement NCC5-645 between NASA and George Mason University. The opinions expressed are those of the authors and do not necessarily reflect the official position of NASA. Abstract—Research in certain scientific disciplines—including Earth science, particle physics, and astrophysics—continually faces the challenge that the volume of data needed to perform valid scientific research can at times overwhelm even a sizable research community. The desire to improve utilization of this data gave rise to the Intelligent Archives project, which seeks to make data archives active participants in a knowledge building system capable of discovering events or patterns that represent new information or knowledge.	archive;data mining;information system;mason;research data archiving;testbed;usability	Hampapuram K. Ramapriyan;David Isaac;Wenli Yang;Steve Morse	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1526050	knowledge representation and reasoning;usability;computer science;data science;computer data storage;data mining;system testing;information system;remote sensing	Robotics	-41.50065761272066	-2.5315443715861505	137165
7a6a1a0fa28000f3fe2e146389ba790c8dba1681	data mining with r: learning with case studies, second edition		 Covers the main data mining techniques through carefully selected case studies  Describes code and approaches that can be easily reproduced or adapted to your own problems  Requires no prior experience with R  Includes introductions to R and MySQL basics  Provides a fundamental understanding of the merits, drawbacks, and analysis objectives of the data mining techniques  Offers data and R code on www.liaad.up.pt/~ltorgo/DataMiningWithR/	data mining;mysql;r language	Luís Torgo	2010		10.1201/9781315399102	business analytics;associate professor;data mining;data science;computer science	ML	-43.73799226489682	-0.9936436727627137	137244
4eef230c4d44984ca78bc77f5e74093b188539e6	addressing the shimming problem in big data scientific workflows	web services big data xml data models ports computers registers databases;shimming problem dataview system nosql document oriented database system mongodb heterogeneous task components web services big data analysis data type incompatibility big data scientific workflows;shimming;scientific workflow;big data shimming scientific workflow;big data;web services big data data analysis relational databases sql	Substantial amount of research has been done recently to address the shimming problem in scientific workflows, in which a special kind of adaptors, called shims, are inserted between workflow tasks to resolve the data type incompatibility issue. Recently, scientific workflows are increasingly used for big data analysis and processing, which poses additional challenges, such as volume, velocity and variety of data to the shimming problem. One issue is to scale the registration and configuration procedure to a large number of workflow tasks. Another issue is the ease of integrating a large number of remote Web services and other heterogeneous task components that can consume and produce data in various formats and models into a uniform and interoperable workflow. Existing approaches fall short in usability and scalability in addressing these issues. In this paper we 1) propose a new simplified single-component based task model based on extensive experiences and lessons learned from our original multiple-component based task model. The new model separates registration from configuration and eases the process of registering external functional components (such as Web services) into p-workflows, 2) propose a shim generation algorithm that elegantly solves the shimming problem raised by Web service based scientific workflows, and 3) we integrate MongoDB, a NoSQL document-oriented database system for storing and managing large-scale unstructured documents. A new version of the DATAVIEW system has been developed to support the proposed techniques and a case study has been conducted to show the feasibility and usability of our proposed techniques.	algorithm;big data;document-oriented database;interoperability;mongodb;nosql;scalability;shim (computing);software incompatibility;usability;velocity (software development);web service	Aravind Mohan;Shiyong Lu;Alexander Kotov	2014	2014 IEEE International Conference on Services Computing	10.1109/SCC.2014.53	computer science;data mining;database;world wide web	DB	-35.140298777285324	1.9390460397326534	137253
8489dcb4c413efe0108649dc2697000b447cb7fc	design and implementation of small watershed management information system	reservoirs;small watershed;client server mode;decision support;com technology;applications development environment;geographic information system;management information systems water conservation soil measurements remote monitoring visual basic geographic information systems reservoirs information management information analysis visual databases;data management;database;miyun reservoir;watershed management;client server systems;soil conservation;arcgis;environmental science computing;miyun and;soil erosion;monitoring system;development environment;client server;erosion;gis;environmental science computing reservoirs hydrological techniques geographic information systems client server systems erosion relational databases management information systems water conservation;visual basic;design and implementation;data query;geographic information systems;information abstract;remote sensing;ms visual basic;information management;guanting reservoir;arcobject;soil erosion spatial temporal change;oracle technology;management information systems;special functions;soil and water conservation;relational databases;remote monitoring;information system;esri arcsde technology;c s;china;management information system;information analysis;visual basic for application;small watershed management information system;soil measurements;water conservation;hydrological techniques;arcmap;temporal change;visual databases;china small watershed management information system soil conservation water conservation geographic information system remote sensing miyun reservoir guanting reservoir soil erosion spatial temporal change data management data query information analysis decision support information abstract client server mode database oracle technology esri arcsde technology com technology arcobject arcgis arcmap ms visual basic applications development environment	A soil and water conservation-oriented small watershed management information system had been developed based on geographic information system and remote sensing technology. It is a sub-system of soil and water conservation monitoring system in the upper basin of Miyun and Guanting reservoir, and its main objective lays a strong emphasis on the analysis of soil erosion spatial-temporal change and the benefit evaluation of soil and water conservation measures for each small watershed in the region. The number of small watersheds, especially for those which had been taken soil and water conservation measures by projects, is more than 100. The main functions of the system include data management, data query, information analysis and decision support with functionalities of information abstract and measures effect evaluation et al. The procedures developed will contribute to the decision support for the soil and water conservation planning. The system adopted the client/server mode and the database was built on the support of Oracle and ESRI ArcSDE technology. Based on the COM technology, Arcobject and ArcGIS desktop product, ArcMap, the system was developed with the MS Visual Basic for Applications Development Environment. Simultaneously, DLL was built to implement special functions with the language of MS Visual Basic. Based on the above, a small watershed-oriented management information system really was realized.	arcgis;arcmap;arcsde;client–server model;decision support system;desktop computer;geographic information system;management information system;oracle database;server (computing);visual basic for applications;visual basic[.net];watershed (image processing)	Yuemin Zhou;Bingfang Wu;Jihua Meng	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1526243	erosion;geology;hydrology;computer science;management information systems;data mining;database;geographic information system;soil conservation;remote sensing	Embedded	-38.291527259060814	-2.1667702374779347	137356
a7ff568d04e5649eec48fc47125321a708f3857d	how charles bachman invented the dbms, a foundation of our digital world		His 1963 Integrated Data Store set the template for all subsequent database management systems.	database;integrated data store	Thomas Haigh	2016	Commun. ACM	10.1145/2935880	computer science;database	DB	-42.76198242755128	-5.028319854831898	137539
79ff04db715b3e604e283e0628c203975c7d99e5	ten simple rules for biomedical ontology development		Biomedical ontology development is often a time and resource consuming endeavor. To maximize efficiency of the process, we present a set of 10 simple rules covering basic technical requirements such as scoping and versioning, while considering additional elements such as licensing and community engagement. When applied, the rules will help avoid common pitfalls and jump-start ontology building. Keywords—ontology development, tutorial, rules	interoperability;ontology engineering;requirement;scope (computer science);simple set;version control	Mélanie Courtot;James Malone;Christopher J Mungall	2016			open biomedical ontologies;process ontology;information retrieval;ontology-based data integration;ontology (information science);ontology;data mining;upper ontology;suggested upper merged ontology;computer science	HCI	-42.613101079229274	3.044501085485095	137595
79a89cb5c0bd42ee7da0af3b5128c0819088333f	building notification services with microsoft sqlserver	image database;classification;cluster merging;content based image retrieval;relevance feedback	"""s of Invited Industrial Track Presentations 1. The Future of Web Services – I, Adam Bosworth, BEA Systems This talk will discuss what changes must take place in web services in the next three years in order for web services to live up to their potential. It will discuss some hard problems that have to be solved and others that industry is in the process of solving. It will discuss how web services will lead to a pervasive change from pull to push computing and why. Finally it will discuss the challenges web services are already putting on traditional databases. 2. The Future of Web Services – II, Felipe Cabrera, Microsoft This talk will introduce the distributed application architecture for Web Services that Microsoft is pursuing. The talk will cover selected topics of the specifications published to date. In particular it will spend time in WS-Security and its related specifications, WS-Coordination and WS-Transaction, WS-Policy and WS-ReliableMessaging. The presentation highlights architectural impacts that arise in the Web Service environment. 3. Oracle RAC: Architecture and Performance, Angelo Pruscino, Oracle This talk will describe the Oracle Real Application Cluster (RAC) Architecture and will give performance data to prove that it is possible to scale both OLTP and DSS application using RAC. It will contrast and compare the RAC architecture with other popular Database cluster architecure. It will show that for real word applications and to satisfy High Availability business requirement the shared cache approach of RAC is the preferred method to built cluster database. It will discuss the both the serilaization and cache coherency techniques used to achieve scalability and the techniques used to achieve high availability and recovery time as low as 1 second. It will present industry standard and Lab benchmark to show the scalability ranges of RAC. 4. Oracle XML DB Repository, Viswanathan Krishnamurthy, Oracle Commercial database systems have built up the very powerful notion of a table for use as a container for structured data. The strong management capabilities that such systems provide have fueled the desire to unify the storage and retrieval of other kinds of data under the same umbrella. However, when it comes to unstructured or semistructured documents, a folder hierarchy is a more natural container than tables. Oracle implements a featurerich XML Repository within the database, which creates a way to view database content as versionable folders in a hierarchy secured by standard WebDAV ACLs and accessed from a variety of fileoriented protocols like FTP, HTTP, NFS and WebDAV. In addition, the Repository is accessible from SQL, and documents stored in this repository can be queried for organization, metadata (including user-supplied metadata), as well as content. In this paper we discuss the main features and architecture of the Oracle XML DB Repository. 5. Google in a Box – Building the Google Search Appliance, Narayan Shivakumar, Google We discuss some of the challenges in (a) building a scalable crawling/search system for large corpuses of documents within corporate intranets, (b) some of the integration touchpoints with third party content management/security systems, (c) putting it all together in an appliance solution that your average IT person can install in a few minutes, and (d) mixing paint colors. 6. Extracting and exploiting structure in text search, Prabhakar Raghavan, Verity While text has long been the bulk of the volume of enterprise information, it is the (smaller) volume of transactional data that has always held the bulk of the value. We argue that there are two principal technical challenges to bridging this gap: (1) extracting structure from unstructured text and (2) exploiting this structure to deliver value to the enterprise. This talk will focus on these two challenges and how their solution could lead to a new class of content-driven applications, much as the advent of the relational database gave rise to a new class of enterprise transaction-driven applications. 7. Visionary: A Next Generation Visualization System for Data bases, Michael Stonebraker, M.I.T. and Rocket Software Visionary was originally written as the Tioga/DataSplash system at Berkeley in the mid 1990’s. After commercialization by Illustra, two rewrites by Informix, and a brief ownership by IBM, it is now being enhanced and marketed by Rocket Software. This talk will present the key features of Visionary, a very sophisticated presentation system for ODBC and JDBC data sources. Visionary shows the result of a query on an arbitrarysized canvas. The end user controls a viewer, which is a certain elevation above the canvas, and which can be freely panned in all directions. Semantic zooming is also supported; hence, at high Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGMOD 2003, June 9-12, 2003, San Diego, CA. Copyright 2003 ACM 1-58113-634-X/03/06...$5.00. elevation, an employee could be viewed as a dot on a geographic map; at lower elevation, he could morph into a picture and then into a historical graph of his salary versus time. Even more powerful is the Visionary notion of wormholes. Canvases can have holes in them, through which other canvases (behind the current one) are visible. The viewer is merely a greater distance above the partially hidden canvas than the one in front of it. This talk will show several Visionary demo applications, as well present Visionary Studio, the drag-and-drop, wizard-oriented development environment. More importantly, the talk will present several unsolved research problems in this style of interface that merit investigation. 8. Building Notification Services with Microsoft SQL Server, Praveen Seshadri, Microsoft Many applications (especially web applications) have added subscription and notification capabilities in the last few years. The reasons are many: (a) end-users are swamped with information, and so would rather personalize the information they receive, (b) there are many applications competing for the attention of the end-user, and so a customized information push model is emerging as valuable, (c) with an increase in the use of mobile devices, the ability to send information of value as messages to the devices becomes attractive. Microsoft SQLServer Notification Services is a platform for such notification applications. It provides a highlevel programming abstraction, and an execution runtime. The runtime uses a database system, and specifically the database query processor, for highly efficient and scalable processing. 9. NonStop SQL/MX Publish/Subscribe: Continuous Data Streams in Transaction Processing, Hansjorg Zeller, Hewlett-Packard The talk will describe the first commercially available implementation of a system that integrates relational database queries with the concept of continuous streams of data. We will discuss multiple extensions to the RDBMS NonStop SQL: Continuous streams of data are produced by insert and update operations on relational tables. Tables can also be used as transactional queues through extensions that allow a """"destructive select"""". Data in streams and queues never leaves the relational context and is protected by transactions. By using partitioned tables the system scales to very large applications. 10. Data Management Challenges in CRM, George Colliat, Siebel Systems Customer Relationship Management presents some unusual data management challenges. Some of them, such as customization of a data model and automatic upgrade of a customized data model are also found in back office applications. Other challenges are more typical of the requirements for multi-channel customer access and the networking of CRM with a multitude of back office applications. This presentation will cover these challenges, some of the approaches to solve them, and suggest some areas of improvement to existing technologies."""	applications architecture;benchmark (computing);bridging (networking);business requirements;cache coherence;canvas element;color;customer relationship management;data model;distributed computing;drag and drop;dynamic data;emoticon;google search;high availability;hypertext transfer protocol;ibm informix;illustra;intranet;jdbc;microsoft sql server;mobile device;narayanan shivakumar;next-generation network;nonstop sql;online transaction processing;open database connectivity;oracle database;oracle rac;personalization;pervasive informatics;push technology;relational database management system;requirement;scalability;table (database);technical standard;transactional memory;ws-coordination;ws-policy;ws-reliablemessaging;ws-security;ws-transaction;web application;web service;webdav;world wide web;xml;zeller's congruence;zooming user interface	Praveen Seshadri	2003		10.1145/872757.872833	biological classification;computer science;data mining;multimedia;automatic image annotation;information retrieval	DB	-43.228744485270354	-3.0735250499741027	137598
5cd919635442ee0c613dadb7e4b2407c886cf11b	information file on compilers, tools, books, courses, tutorials, and the standard for the fortran language and its derivatives: version of 20 may 1999 (the penultimate year of the millennium)	information file;penultimate year;fortran language		book;compiler;fortran	Mike Metcalf	1999	Scientific Programming		computational science;computer science;programming language	HPC	-47.188391803330894	-3.5285645838067636	137712
1f86264f760c7fbc1123cc7cab0b255c9f5321bd	the open search.org in open science era: a communication platform for everyone building their repositories and using others		Open Science represents a new approach to scientific process and have attracted attention worldwide. The Open Search.org is built up on the National Science Library, Chinese Academy of Sciences (NSL, CAS) for implementing open science in practice. In this paper, we describe the background, three components, development situation, and the future development vision of the Open Search.org (http://www.open-search.org).	academy;national science library (georgia)	Li-Ping Ku;Qingwen Bao	2017	2017 6th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI)	10.1109/IIAI-AAI.2017.221	world wide web;scientific method;open content;open science;engineering	Robotics	-44.523925263843175	-1.9724302367763387	137784
d437355e989d57e82d84a1c10073057b042076cc	characterizing the landscape of musical data on the web: state of the art and challenges		Musical data can be analysed, combined, transformed and exploited for diverse purposes. However, despite the proliferation of digital libraries and repositories for music, infrastructures and tools, such uses of musical data remain scarce. As an initial step to help fill this gap, we present a survey of the landscape of musical data on the Web, available as a Linked Open Dataset: the musoW dataset of catalogued musical resources. We present the dataset and the methodology and criteria for its creation and assessment. We map the identified dimensions and parameters to existing Linked Data vocabularies, present insights gained from SPARQL queries, and identify significant relations between resource features. We present a thematic analysis of the original research questions associated with surveyed resources and identify the extent to which the collected resources are Linked Data-ready.	data model;digital library;lambda lifting;library (computing);linked data;sparql;sampling (signal processing);semantic web;software repository;spec#;vocabulary;world wide web	Marilena Daquino;Enrico Daga;Mathieu d'Aquin;Aldo Gangemi;Simon D Holland;Robin C. Laney;Albert Meroño-Peñuela;Paul Mulholland	2017			data science;thematic analysis;musical;world wide web;linked data;data mining;digital library;sparql;engineering	HCI	-42.623754748843254	1.311878065537571	137833
0bbaaab66ee42eb8ad86a668ba72c96c5ce8bc66	exposing provenance metadata using different rdf models		A standard model for exposing structured provenance metadata of scientific assertions on the Semantic Web would increase interoperability, discoverability, reliability, as well as reproducibility for scientific discourse and evidence-based knowledge discovery. Several Resource Description Framework (RDF) models have been proposed to track provenance. However, provenance metadata may not only be verbose, but also significantly redundant. Therefore, an appropriate RDF provenance model should be efficient for publishing, querying, and reasoning over Linked Data. In the present work, we have collected millions of pairwise relations between chemicals, genes, and diseases from multiple data sources, and demonstrated the extent of redundancy of provenance information in the life science domain. We also evaluated the suitability of several RDF provenance models for this crowdsourced data set, including the N-ary model, the Singleton Property model, and the Nanopublication model. We examined query performance against three commonly used large RDF stores, including Virtuoso, Stardog, and Blazegraph. Our experiments demonstrate that query performance depends on both RDF store as well as the RDF provenance model.	crowdsourcing;discoverability;experiment;interoperability;linked data;redundancy (engineering);resource description framework;semantic web;triplestore	Gang Fu;Evan Bolton;Núria Queralt-Rosinach;Laura Inés Furlong;Vinh Nguyen;Amit P. Sheth;Olivier Bodenreider;Michel Dumontier	2015			rdf/xml;cwm;computer science;sparql;data mining;database;rdf query language;world wide web;information retrieval;rdf schema	Web+IR	-40.02486408316265	2.048069306512458	137932
03a9c1cbaa9082f291178d02c670e346889db600	mace: a dynamic caching framework for mashups	dynamic caching framework;cache storage;mace;data manipulation;mashups;b tree data structure mace dynamic caching framework mashups web 2 0 application web service data aggregation data manipulation world wide web indexing scheme system scalability;caching;b tree data structure;costs and benefits;indexing scheme;tree data structures;web service;personalization;data mining;surges;radio frequency;caching mashup web 2 0 web services personalization;indexing;web services cache storage tree data structures;aggregates;data aggregation;indexation;web services;web sites;performance analysis;web 2 0;web 2 0 application;world wide web;mashup;optimization;vehicles;scalability;system scalability;mashups scalability vehicle dynamics web services surges aggregates web sites vehicles performance analysis indexing;vehicle dynamics	The recent surge of popularity has established Mashups as an important category of Web 2.0 applications. Mashups are essentially Web services that are often created by end-users. They aggregate and manipulate data from sources around the World Wide Web. Surprisingly, there are very few studies on the scalability and performance of mashups. In this paper, we study caching as a vehicle for enhancing the scalability and the efficiency of mashups. Although caching has long been used to improve the performance of Web services, mashups pose some unique challenges that necessitate a more dynamic approach to caching. Towards this end, we present MACE - a cache specifically designed for mashups. In designing the MACE framework this paper makes three technical contributions. First, we present a model for representing mashups and analyzing their performance. Second, we propose an indexing scheme that enables efficient reuse of cached data for newly created mashups. Finally, this paper also describes a novel caching policy that analyzes the costs and benefits of caching data at various stages of different mashups and selectively stores data that is most effective in improving system scalability. We report experiments studying the performance of the MACE system.	aggregate data;cache (computing);experiment;kronos;mashup (web application hybrid);scalability;web 2.0;web service;world wide web	Osama Al-Haj Hassan;Lakshmish Ramaswamy;John A. Miller	2009	2009 IEEE International Conference on Web Services	10.1109/ICWS.2009.119	web service;computer science;operating system;database;internet privacy;world wide web;mashup	DB	-33.72935230297729	2.454102191886269	138082
c5b2c0c8c9a836a2e02424164293232b80088e39	an event-based spatiotemporal data model (estdm) for temporal analysis of geographical data	spatial database;data model;temporal information;spatio temporal data;compact representation;temporal analysis;human activity	Abstract Representations historically used within GIS assume a world that exists only in the present. Information contained within a spatial database may be added-to or modified over time, but a sense of change or dynamics through time is not maintained. This limitation of current GIS capabilities has recently received substantial attention, given the increasingly urgent need to better understand geographical processes and the cause-and-effect interrelationships between human activities and the environment. Models proposed so-far for the representation of spatiotemporal data are extensions of traditional raster and vector representations that can be seen as location- or feature-based, respectively, and are therefore best organized for performing either location-based or feature-based queries. Neither form is as well-suited for analysing overall temporal relationships of events and patterns of events throughout a geographical area as a temporally-based representation. In the current paper, a new spatio-tem...	data model;spatiotemporal database	Donna Peuquet;Niu Duan	1995	International Journal of Geographical Information Systems	10.1080/02693799508902022	computer vision;geography;data model;computer science;data science;data mining;database;spatial database	DB	-35.96472061936772	-3.180061857669353	138244
46803a960a6aef648871133281eb513c525230f4	feedback-based annotation, selection and refinement of schema mappings for dataspaces	data integrity;data management;user feedback;mapping annotation;optimization problem;large scale;feedback;pay as you go;query evaluation;mapping refinement;user requirements;schema mapping;evolutionary algorithm;mapping selection;dataspaces;data integration	The specification of schema mappings has proved to be time and resource consuming, and has been recognized as a critical bottleneck to the large scale deployment of data integration systems. In an attempt to address this issue, dataspaces have been proposed as a data management abstraction that aims to reduce the up-front cost required to setup a data integration system by gradually specifying schema mappings through interaction with end users in a pay-as-you-go fashion. As a step in this direction, we explore an approach for incrementally annotating schema mappings using feedback obtained from end users. In doing so, we do not expect users to examine mapping specifications; rather, they comment on results to queries evaluated using the mappings. Using annotations computed on the basis of user feedback, we present a method for selecting from the set of candidate mappings, those to be used for query evaluation considering user requirements in terms of precision and recall. In doing so, we cast mapping selection as an optimization problem. Mapping annotations may reveal that the quality of schema mappings is poor. We also show how feedback can be used to support the derivation of better quality mappings from existing mappings through refinement. An evolutionary algorithm is used to efficiently and effectively explore the large space of mappings that can be obtained through refinement. The results of evaluation exercises show the effectiveness of our solution for annotating, selecting and refining schema mappings.	dataspaces;evolutionary algorithm;expect;feedback;iteration;mathematical optimization;optimization problem;population;precision and recall;refinement (computing);requirement;schema (genetic algorithms);schema evolution;search algorithm;software deployment;type conversion;user experience;user requirements document;xml schema	Khalid Belhajjame;Norman W. Paton;Suzanne M. Embury;Alvaro A. A. Fernandes;Cornelia Hedeler	2010		10.1145/1739041.1739110	optimization problem;data management;computer science;theoretical computer science;user requirements document;data integration;evolutionary algorithm;data integrity;data mining;feedback;database	DB	-35.03257086063952	3.7915893571016994	138252
9af232494314622906d099792278afcb7e4e0905	seweb: a knowledge flowing service network mode of super knowledge nodes transmission based on p2p		Knowledge service has not been a strange information technology since web users can actively query and serve useful knowledge. In this paper, we present SeWeb, which is an effective and effi cient knowledge recommending system using classical P2P technology based on super nodes. Super nodes are users with fast, mobile connections. This paper has solved two problems: (1) how to construct knowledge services model pointing to Web application; (2) how to eliminate misunderstanding applied knowledge services on Web Platforms. SeWeb Platforms make sure of effi cient information communication because of HTTP protocol. SeWeb can construct wider P2P networks (VPNs) whose goal is to serve knowledge of various kinds. Besides, we show one application interface: SeCollege, which is a sort of real time document resources provider. SeCollege users can upload or download needed documents wherever physical location is. From evaluation results, the emergency of Server Platform has brought vast, great convenience to users.	download;effi;peer-to-peer;regular expression;upload;virtual private network;web application	Chao Liu;Zuhua Jiang	2009	IJWA		web application;world wide web;effi;sort;hypertext transfer protocol;transmission (mechanics);information technology;computer science;upload;download	Web+IR	-46.318937079860774	1.0166409328912913	138269
5571b230588aa27bfbc881cf313d0911629b1ec6	experiences with a schematic logic preprocessor	jackson method;cobol;preprocessor;program design language;structured programming;schematic logic	In the Computation Department at UMIST all B.Sc. students are required in their second year to write a suite of three programs in COBOL’ and, in their third year, to undertake a significant computing project.2 In his third year project J. F. S. Yow wrote a Schematic Logic Preprocessor under the supervision of J. M. Triance. This preprocessor is currently being used by second year students for two of the three COBOL programs they write and is also used for third year projects involving data processing applications.	cobol;computation;preprocessor;schematic	J. M. Triance;J. F. S. Yow	1980	Softw., Pract. Exper.	10.1002/spe.4380101004	computer science;program design language;cobol;programming language;structured programming;preprocessor;algorithm	SE	-45.787178338052456	-3.388512934763978	138314
f4d6c0260b55dcabc85d60f697f7280cb4f34f4b	towards standardisation of online dispute resolution tools	online dispute resolution;standardisation	This contribution summarizes the main goals, objectives, and tasks done so far in the framework of the the CEN/ISSS Workshop on Standardization of Online Dispute Resolution Tools (WS/Stand-ODR), started on 17 December at the CEN Management Centre in Brussels (Belgium).	one definition rule	Arno R. Lodder;Andrea Borri;Jacques Gouimenou;Brian Hutchinson;Vincent Tilman	2008			sodium carbonate;sodium sesquicarbonate;calcination;boiling;sodium bicarbonate;online dispute resolution;suspension (vehicle);nuclear chemistry;chemistry	ML	-47.31525766939173	-7.220995968795317	138614
1fdda6037451141dc345d9be970c9a83a0f74365	multimedia analysis for ecological data	pollution monitoring;ecological multimedia data retrieval;animal and plant identification;multimedia content analysis	The ACM International Workshop on Multimedia Analysis for Ecological Data (MAED'12) is held as part of ACM Multimedia 2012. MAED'12 is concerned with the processing, interpretation, and visualization of ecology-related multimedia content with the aim to support biologists in their investigations for analyzing and monitoring natural environments, with particular attention to living organisms and pollution effects.	ecology	Concetto Spampinato;Vasileios Mezaris;Jacco van Ossenbruggen	2012		10.1145/2393347.2396540	computer science;data mining;multimedia;world wide web	DB	-41.95030709509284	-7.977934439243195	138707
3dd3e41d5d7c6d09bf499a2eb6cc1b7c80a890fc	foundations of an ontology of philosophy	philosophy;ontology	We describe an ontology of philosophy that is designed to aid navigation through philosophical literature, including literature in the form of encyclopedia articles and textbooks and in both printed and digital forms. The ontology is designed also to serve integration and structuring of data pertaining to the philosophical literature, and in the long term also to support reasoning about the provenance and contents of such literature, by providing a representation of the philosophical domain that is oriented around what philosophical literature is about.	approximation;documentation;geopolitical ontology;ontology (information science);printing	Pierre Grenon;Barry Smith	2009	Synthese	10.1007/s11229-009-9658-x	upper ontology;philosophical theory;bibliographic ontology;philosophy;epistemology;ontology;ontology;fundamental ontology;process ontology;philosophy of computer science;suggested upper merged ontology	HCI	-43.858932145369685	3.203158978624111	138847
b9feec76061817733c84f08407ff152fcb997cd8	olap formulations for supporting complex spatial objects in data warehouses	spatial data cube;user centric olap;region hierarchy	In recent years, there has been a large increase in the amount of spatial data obtained from remote sensing, GPS receivers, communication terminals and other domains. Data warehouses help in modeling and mining large amounts of data from heterogeneous sources over an extended period of time. However incorporating spatial data into data warehouses leads to several challenges in data modeling, management and the mining of spatial information. New multidimensional data types for spatial application objects require new OLAP formulations to support query and analysis operations on them. In this paper, we introduce a set of constructs called C for defining data cubes. These include categorization, containment and cubing operations, which present a fundamentally new, user-centric strategy for the conceptual modeling of data cubes. We also present a novel region-hierarchy concept that builds spatially ordered sets of polygon objects and employs them as first class citizens in the data cube. Further, new OLAP constructs to help define, manipulate, query and analyze spatial data have also been presented. Overall, the aim of this paper is to leverage support for spatial data in OLAP cubes and pave the way for the development of a user-centric	categorization;data cube;data modeling;database schema;first-class function;fractal dimension;global positioning system;mathematical model;olap cube;online analytical processing;spatial application;usability;via c3	Ganesh Viswanathan;Markus Schneider	2011		10.1007/978-3-642-23544-3_4	online analytical processing;computer science;data science;data mining;database	DB	-36.21975369366832	-2.6349635679192076	138890
174d27f7dfaf32342c1d407dd810eade4495f424	building knowledge in open source software research in six years of conferences		Since its origins, the diffusion of the OSS phenomenon and the information about it has been entrusted to the Internet and its virtual communities of developers. This public mass of data has attracted the interest of researchers and practitioners aiming at formalizing it into a body of knowledge. To this aim, in 2005, a new series of conferences on OSS started to collect and convey OSS knowledge to the research and industrial community. Our work mines articles of the OSS conference series to understand the process of knowledge grounding and the community surrounding it. As such, we propose a semi-automated approach for a systematic mapping study on these articles. We automatically build a map of cross-citations among all the papers of the conferences and then we manually inspect the resulting clusters to identify knowledge building blocks and their mutual relationships. We found that industry-related, quality assurance, and empirical studies often originate or maintain new streams of research.	internet;jan bergstra;open sound system;open-source software;semiconductor industry;social network;virtual community	Fabio Mulazzani;Bruno Rossi;Barbara Russo;Maximilian Steff	2011			computer science;engineering;knowledge management;artificial intelligence;software engineering;data mining;management;world wide web	HCI	-41.77425083597184	-5.731689008145709	138941
6bcb2f32560650d7a033ca6518e276a4692f335e	beyond university rankings? generating new indicators on universities by linking data in open platforms	rankings;indicators design;university;data integration	The need for new indicators on universities is growing enormously. Governments and decision makers at all levels are faced with the huge opportunities generated by the availability of new knowledge and information and, simultaneously, are pressed by tight budget constraints. University rankings, in particular, are attracting policy and media attention, but at the same time receive harsh methodological criticism. After summarizing the main criticisms of rankings, we describe 2 trends in the user requirements for indicators; namely, granularity and cross-referencing. We then suggest that a change in the paradigm of the design and production of indicators is needed. The traditional approach is one that not only leverages on the existing data but also suggests heavy investment to integrate existing databases and to build up tailored indicators. We show, based on the European universities case, how the intelligent integration of existing data may lead to an open-linked data platform which permits the construction of new indicators. The power of the approach derives from the ability to combine heterogeneous sources of data to generate indicators that address a variety of user requirements without the need to design indicators on a custom basis.	cross-reference;database;linked data platform;programming paradigm;requirement;user requirements document	Cinzia Daraio;Andrea Bonaccorsi	2017	JASIST	10.1002/asi.23679	computer science;data integration;data mining;world wide web	DB	-45.34975053041802	0.5687496807543125	139054
4606869a3df503c9363810bab25047fab43d3212	a novel complex event processing engine for intelligent data analysis in integrated information systems		Novel and effective engines for data analysis in integrated information systems are urgently required by diverse applications, in which massive business data can be analyzed to enable the capturing of various situations in real time. The performance of existing engines has limited capacity of data processing in distributed computing. Although Complex Event Processing CEP has enhanced the capacity of data analysis in information systems, it is still a big challenging task since events are rapidly increasing in diverse applications. In this paper, a lightweight intelligent data analysis system with a novel CEP engine named LIDA-E is introduced, which employs the knowledge base with rules and an event processing algorithm for analysis. Event models as well as operators support the rules for event selection and aggregation. These operators and rules have been utilized for constructing new CEP system architecture which combines expressiveness and efficiency in analysis. It adopts the agents and filter conception explicitly to provide the event transmission mechanism efficiently. Finally, the comparison between the proposed engine and the existing engine shows that LIDA-E has 48.65% averagely reduced time cost in different tests. The experimental results demonstrate that the developed architecture has better performance in both transmitting and analyzing a large number of events.	complex event processing;information system	Mingquan Zhou;Sajid Ali;Pengbo Zhou;Yusong Liu;Xuesong Wang	2016	IJDSN	10.1155/2016/6741401	embedded system;real-time computing;simulation;complex event processing;data mining	Robotics	-35.216005299759225	-1.290286236311844	139490
032f1e0ab79a0de005090b4e20ad5a69395225cf	brainy: a machine learning library		Brainy is a newly created cross-platform machine learning library written in Java. It defines interfaces for common types of machine learning tasks and implementations of the most popular algorithms. Brainy utilizes a complex mathematical infrastructure which is also part of the library. The main difference compared to other ML libraries is the sophisticated system for feature definition and management. The design of the library is focused on efficiency, reliability, extensibility and simple usage. Brainy has been extensively used for research as well as commercial projects for major companies in Czech Republic and USA. Brainy is released under the GPL license and freely available from the project web page.	algorithm;artificial neural network;data structure;extensibility;graphical model;information system;java;library (computing);machine learning;pipeline (computing);production system (computer science);reliability engineering;web page	Michal Konkol	2014		10.1007/978-3-319-07176-3_43	computer science;machine learning;implementation;license;artificial intelligence;web page;extensibility;java	PL	-43.53594468177384	-3.1206275172806683	139815
3712f56e3331be955ca3176782b68a8de857d92a	peer-to-peer computing - building supercomputers with web technologies	selected works;bepress	Overview of Peer-to-Peer System.- File-Sharing Peer-to-Peer System.- The Need for More Powerful Computers.- Problems and Solutions.- Web Server and Related Technologies.- to Servlets.- Java Network Programming.- Testing and Enhancements of Servlets.- Power Server: Model 1.- Power Server: Model 2.- Power Server: Model 3.- Power Server: Model 4.- Power Server: Model 5.- Wireless P2P System.- Implementation of Wireless P2P Systems.- Computer Architecture.- Distributed and Parallel Algorithms.- Infrastructure and Future Development.	supercomputer	Alfred Wai-Sing Loo	2007		10.1007/978-1-84628-747-3	computer science;theoretical computer science;operating system;appleshare;distributed computing;application server;server farm	HPC	-45.02181266870774	-6.783350347590345	140014
555792f6abaf7343dc487390f845e33de0674731	representing aggregate works in the digital library	aggregate documents;digital library;digital libraries;organisational structure;architecture	This paper studies the challenge of representing aggregate works such as encyclopedias, collected poems and journals in heterogenous digital library collections. Reflecting on the materials used by humanities academics, we demonstrate the varied range of aggregate types and the problems of faithfully representing this in the DL interface. Aggregates are complex and pervasive, challenge common assumptions and confuse boundaries within organisational structures. Existing DL systems can only provide imperfect representation of aggregates, and alterations to document encoding are insufficient to create a faithful reproduction of the physical library. The challenge is amplified through concrete examples, and solutions are demonstrated in a well-known DL system and related to standard DL architecture.	aggregate data;digital library;display resolution;information seeking;interaction;mg (editor);pervasive informatics;user interface	George Buchanan;Jeremy Gow;Ann Blandford;Jon Rimmer;Claire Warwick	2007		10.1145/1255175.1255224	digital library;computer science;architecture;multimedia;world wide web	HCI	-43.137523545374776	2.3189138561084253	140062
3d12ab81030d8cf4c9e497f9567b8edae8f846b1	successfully integrating traditional and object-oriented approaches with ada 95	parallel computing;visualization;object oriented approach;laboratory	INTRODUCTION A strategy is mapped out to successfully integrate both traditional and object-oriented approaches to software development in CS 1, CS2 and Software Engineering. This paper describes the authors’ experiences during the transition from Pascal to Ada as the introductory programming language in CSE200-220, Fundamentals of Computer Science I-II, and the restructuring of CSE422, Introduction to Software Engineering, to focus on object-oriented design with Ada 95. Similar CS 1 and CS2 experiences at other institutions are described in [3]. CSE200-220 serve as the introductory CS 1-CS2 sequence and CSE422 is the required software engineering course for the Computer Science (CS) and Computer Engineering (CPE) curricula in the Computer Science and Engineering (CSE) Department. These are also popular electives for non-majors, primarily mathematics, business, and other engineering disciplines. CSE200 provides a general introduction to structured programming, and CSE220 is a continuation which includes traditional data structure topics. CSE422 introduces the broad field of software engineering and then focuses on object-oriented design. CSE200, 220, and 422 are each a single quarter in duration and CSE200 and 220 each carry four credit hours 3 hours of lecture (28 meetings per quarter) and a 3-hour weekly supervised lab (9 weeks). CSE422 is three credit hours -2 hours of lecture (19 meetings per quarter) and a 3-hour weekly supervised lab (9 weeks). Enrollment is approximately 45 students per quarter in CSE200, 35 per quarter in CSE220, and 35 per quarter in CSE422. CSE200-220 are offered every quarter of the academic year, and frequently also in the summer. CSE422 is currently	abstraction (software engineering);ada;computer engineering;computer science;constant phase element;continuation;data structure;iterator;pascal;programming language;software development;software engineering;structured programming;subroutine	James H. Cross;Thomas M. Phillips	1996		10.1145/236452.236481	computational science;computer architecture;visualization;computer science;programming language;laboratory	SE	-45.63624581280519	-3.2093889746512354	140185
740b4439e050fc1d895c6dd5255ddb96bfde05fa	automatic tracking of pupillary dynamics from in vivo functional optical coherence tomography images	functional optical coherence tomography;pupillary dynamics;tracking	Automatic tracking of pupillary dynamics from in vivo functional optical coherence tomography images Chenyi Liu, Alexander Wong, Alireza Akhlagh Moayed, Paul Fieguth, Hongxia Bie & Kostadinka Bizheva a Department of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, P.R. China b Department of Systems Design Engineering, University of Waterloo, Waterloo, Ontario, Canada c Department of Physics and Astronomy, University of Waterloo, Waterloo, Ontario, Canada Published online: 24 Jul 2014.	algorithm;chicken;systems design;systems engineering;time series;tomography;video-in video-out	Chenyi Liu;Alexander Wong;Alireza A. Moayed;Paul W. Fieguth;Hongxia Bie;Kostadinka K. Bizheva	2016	CMBBE: Imaging & Visualization	10.1080/21681163.2014.929536	psychology;computer vision;optics;computer graphics (images)	HPC	-46.77224466547228	-9.427940737033188	140368
54356ff0960100e27cf17ff682825bba2662e90c	the research object suite of ontologies: sharing and exchanging research data and methods on the open web		Research in life sciences is increasingly being conducted in a digital and online environment. In particular, life scientists have been pioneers in embracing new computational tools to conduct their investigations. To support the sharing of digital objects produced during such research investigations, we have witnessed in the last few years the emergence of specialized repositories, e.g., DataVerse and FigShare. Such repositories provide users with the means to share and publish datasets that were used or generated in research investigations. While these repositories have proven their usefulness, interpreting and reusing evidence for most research results is a challenging task. Additional contextual descriptions are needed to understand how those results were generated and/or the circumstances under which they were concluded. Because of this, scientists are calling for models that go beyond the publication of datasets to systematically capture the life cycle of scientific investigations and provide a single entry point to access the information about the hypothesis investigated, the datasets used, the experiments carried out, the results of the experiments, the conclusions that were derived, the people involved in the research, etc. In this paper we present the Research Object (RO) suite of ontologies, which provide a structured container to encapsulate research data and methods along with essential metadata descriptions. Research Objects are portable units that enable the sharing, preservation, interpretation and reuse of research investigation results. The ontologies we present have been designed in the light of requirements that we gathered from life scientists. They have been built upon existing popular vocabularies to facilitate interoperability. Furthermore, we have developed tools to support the creation and sharing of Research Objects, thereby promoting and facilitating their adoption.	digital data;emergence;entry point;experiment;interaction;interoperability;ontology (information science);open web;requirement;runescape;vocabulary;myexperiment	Khalid Belhajjame;Jun Zhao;Daniel Garijo;Kristina M. Hettne;Raúl Palma;Óscar Corcho;José Manuél Gómez-Pérez;Sean Bechhofer;Graham Klyne;Carole A. Goble	2014	CoRR		computer science;data science;data mining;database;world wide web	HCI	-43.53464831448356	-0.16437875487320783	140916
75759754553264d94908432a3830bccefdbfc290	semantic concept discovery over event data		Preparing a comprehensive, accurate, and unbiased report on a given topic or question is a challenging task. The first step is often a daunting discovery task that requires searching through an overwhelming number of information sources without introducing bias from the analyst’s current knowledge or limitations of the information sources. A common requirement for many analysis reports is a deep understanding of various kinds of historical and ongoing events that are reported in the media. To enable better analysis based on events, there exist several event databases containing structured representations of events extracted from news articles. Examples include GDELT [4], ICEWS [1], and EventRegistry [3]. These event databases have been successfully used to perform various kinds of analysis tasks, e.g., forecasting societal events [6]. However, there has been little work on the discovery aspect of the analysis, that results in a gap between the information requirements and the available data, and potentially a biased view of the available information. In this presentation, we describe a framework for concept discovery over event databases using semantic technologies. Unlike existing concept discovery solutions that perform discovery over text documents and in isolation from the remaining data analysis tasks [5, 8], our goal is providing a unified solution that allows deep understanding of the same data that will be used to perform other analysis tasks (e.g., hypothesis generation [7] or building models for forecasting [2]). Figure 1 shows the architecture of our system. The system takes in as input a set of event databases and RDF knowledge bases and provides as output a set of APIs that provide a unified retrieval mechanism over input data and knowledge bases, and an interface to a number of concept discovery algorithms. Figures 2 shows different portions of our system’s UI that is built using our concept discovery framework APIs. The analyst can enter a natural language question or a set of concepts, and retrieve collections of relevant concepts identified and ranked using different concept discovery algorithms. A key aspect of our framework is the use of semantic technologies. In particular:	algorithm;database;existential quantification;natural language;requirement;resource description framework;user interface	Oktie Hassanzadeh;Shari Trewin;Alfio Massimiliano Gliozzo	2017			semantic computing;data mining;computer science	ML	-36.19687565444937	-7.007973150319681	141040
7f53c741c131524997d5f2acb90ba51ee52067cd	fault-tolerant optical-penetration-based silkworm gender identification	agri photonics;image processing;silkworm pupae;sericulture;spectral imaging;optical sensors	National Electronics and Computer Technology Center (NECTEC), National Science and Technology Development Agency, Ministry of Science and Technology, 112 Thailand Science Park, Phahonyothin Rd, Khlong Nueng, Khlong Luang, Pathumthani 12120, Thailand b Photonics Technology Laboratory, Intelligent Devices and Systems Research Unit, 112 Thailand Science Park, Phahonyothin Rd, Khlong Nueng, Khlong Luang, Pathumthani 12120, Thailand	banking transformation toolkit;embedded system;fault tolerance;illumination (image);image processing;prototype;region of interest;silkworm	Sarun Sumriddetchkajorn;Chakkrit Kamtongdee;Sataporn Chanhorm	2015	Computers and Electronics in Agriculture	10.1016/j.compag.2015.10.004	biology;image processing;computer science;sericulture;optics;spectral imaging;anatomy;quantum mechanics	Robotics	-46.585845124304555	-9.022822634490685	141430
3474b50290928f16d81e7a9324f7db452cbb8680	the cclrc dataportal.	scientific data;web service;data mining;service model;grid service;data access;java 2 enterprise edition	The project aims to provide easy, transparent access to experimental, observational, simulation and visualisation data kept on a multitude of systems and sites. Further more it will provide links to other web/grid services, which will allow the scientists to further use the selected data, e.g. via data mining, simulations or visualisation. The Data Portal will aim to work as a broker between the scientists, the facilities, the data and other services. The problem addressed is that currently the scientific data is stored distributed across a multitude of sites and systems. Scientists have only very limited support in accessing, managing and transferring their data or indeed in identifying new data resources. In a true Grid environment it is essential to ease many of these processes and the aim of the Data Portal is to help with automating many of these tasks. The Data Portal originally used Suns Java 2 Enterprise Edition (J2EE) but was replaced using a component based web service model.	data mining;java platform, enterprise edition;java version history;simulation;web service	Glen Drinkwater	2003			web service;data access;computer science;service-oriented modeling;service design;data mining;database;enterprise data management;data as a service;law;world wide web;data;web coverage service	HPC	-41.34023501806832	-0.6835475012788051	141613
2e92a93cd619a39d7ed7c8d39f7a008227a98e1d	flexible querying techniques based on cbr		DTD Graph from an XML Document: A Reverse Engineering Approach Joseph Fong and Herbert Shiu (2010). Principle Advancements in Database Management Technologies: New Applications and Frameworks (pp. 204-224). www.igi-global.com/chapter/abstract-dtd-graph-xml-document/39357?camid=4v1a Performance Studies of Locking Protocols for Real-time Databases With Earliest Deadline First Kam-Yiu Lam, Sheung-Lun Hung and Ken Chee-Keung Law (1995). Journal of Database Management (pp. 22-32). www.igi-global.com/article/performance-studies-locking-protocols-real/51148?camid=4v1a Fuzzy Spatial Data Types for Spatial Uncertainty Management in Databases Markus Schneider (2008). Handbook of Research on Fuzzy Information Processing in Databases (pp. 490515). www.igi-global.com/chapter/fuzzy-spatial-data-types-spatial/20365?camid=4v1a	case-based reasoning;database;earliest deadline first scheduling;information processing;lam/mpi;logical unit number;real-time transcription;reverse engineering	Guy De Tré;Marysa Demoor;Bert Callens;Lise Gosseye	2008			relational database;information retrieval;data mining;case-based reasoning;computer science	DB	-43.5510453068389	-5.6066084501109374	141781
ac02f7cd60f952a67303906afdefbdf5bfbe3516	interlinking of resources with semantics.		Semantic Web applications that are after utilising linkeddata datasets are often limited by the interlinking methodology available. Based on our experiences with building a linked-data dataset for humans and machines, we propose a new way of creating semantic links, labelled as User Contributed Interlinking. 1 Linking Open Data 101 Links are one of the key factors that made the Web of Documents successful. The same holds for the Web of Data; rather than having untyped hyperlinks between documents, RDF properties (such as rdfs:seeAlso) are interpreted as “semantic hyperlinks”. The linked data principles basically capture this idea. The W3C Linking Open Data (LOD) project is a collaborative effort to bootstrap the building of linked-data datasets. In [1] the publication of linked data is described; further, the two basic approaches for creating links to other datasets are outlined: RDF links can either be set manually or generated by automated linking algorithms for large datasets. 2 User Contributed Interlinking With the User Contributed Interlinking (UCI) [2] we have proposed a novel approach for creating high-quality interlinks by relying on end-users. For large datasets, such as riese 2 where the entire European statistics are brought to the Semantic Web, it might appear impractical at first sight to manually generate interlinks to other datasets. However, we strongly believe that by applying the Wiki-principle end-users can be encouraged to contribute to linked datasets with similar enthusiasm as for example the case with Wikipedia. We have implemented a generalised UCI in a demonstrator called “interlinking of resources with semantics” (irs), available at http://143.224.254.32/irs/. It allows to list, add, and remove user-contributed semantic links between any resources identified through URIs. With an additional format parameter the output format can be controlled (RDF/XML, Turtle, XHTML+RDFa). 1 http://www.w3.org/DesignIssues/LinkedData.html 2 http://riese.joanneum.at 3 http://www.w3.org/TR/xhtml-rdfa-primer/ A screen shot of irs is shown in Fig. 1. The system enables end-users to create semantic links (currently owl:sameAs, rds:seeAlso, and foaf:topic are supported), as well as to ask about existing links. Further, a content preview is available: Due the fact that irs builds upon ARC, a range of structured metadata (such as microformats, RDFa, eRDF) is supported. A simple version of provenance tracking is offered, as well. The user-contributed links are placed into a named graph, which defaults to http://example.org/#unknown. When retrieving link information, one is therefore able to understand who stated what. Currently, all users concurrently work in a single space. Finally, a simple off-theshelf SPARQL-endpoint is also available in irs. Fig. 1. A demonstrator for a generalised UCI: irs. Acknowledgements The research reported in this paper was carried out in two projects: the K-Space project, partially funded under the 6th Framework Programme of the European Commission, and the “Understanding Advertising” (UAd) project, funded by the Austrian FIT-IT Programme.	algorithm;communication endpoint;hyperlink;linked data;microformat;named graph;rdf/xml;rdfa;resource description framework;sparql;screenshot;semantic web;turtle (robot);uniform resource identifier;web ontology language;wiki;wikipedia;world wide web;xhtml+rdfa;xml	Michael Hausenblas;Wolfgang Halb	2008			hyperlink;foaf;information retrieval;rdf;named graph;linked data;xhtml;rdf schema;semantic web;computer science	Web+IR	-40.58023177510561	3.932652941830423	141926
16ff5527e6b0a775c123989d80d3a1312ee41456	m.h. trauth, matlab recipes for earth sciences (2006) springer, berlin 3-540-27983-0 (237p., plus cd-rom, us$70 hardbound)	documentation;miscellaneous	MATLAB is a general-purpose software package developed by The Mathworks, Inc., for doing mathematical calculations, primarily based on vectors, matrices, and associated manipulations. The system is very powerful, and is widely used in several specialties, notably the earth sciences and engineering. Because of its generality and many capabilities, complicated operations with MATLAB can be difficult to set up for execution. Martin Trauth, a researcher at the University of Potsdam, wrote this book to help smooth the way for MATLAB users with earth-science applications. He states that he was inspired by the Numerical Recipes books by W. H. Press and others, and used a similar philosophy for this book. Most chapters in the book first provide a brief introduction to the subject and application at hand, and then go through the MATLAB steps that would be required to process the data appropriately. The MATLAB commands are interspersed throughout the text, along with descriptions of what they are doing. The enclosed CD contains the complete set of these instructions. The author clearly expects that the reader is executing the MATLAB steps while reading the book, as he describes calculations and displays to the screen that are not shown in the book. The book consists of nine chapters. The first chapter (9p) introduces types of data (e.g., nominal, ratio), sampling methods (e.g., regular, random, along traverse), and methods of analysis. Chapter 2 (18p) gives a brief introduction to MATLAB. The presentation starts with fundamentals such as starting the program and setting paths to data directories, and then moves on to define vectors and matrices. It also gets into data-storage issues. Scripts and functions are mentioned, but not in such a way that I feel is useful to the novice. The	9p (protocol);book;cd-rom;general-purpose markup language;matlab;norm (social);numerical recipes;numerical method;sampling (signal processing);springer (tank);traverse;vector graphics	Thomas A. Jones	2007	Computers & Geosciences	10.1016/j.cageo.2006.06.001	engineering physics	HCI	-47.01053269721523	-3.4951151213910503	142090
1f0485b37c9716b3be735ae7b31c04f3e5fb9194	dpm mapper: a concept to bridge the gap between xml-based digital product memories and their binary representation		In this chapter we introduce a concept that improves the use of Digital Product Memory (DPM) on industrial embedded control systems to control decentralized production processes. The core of this concept is an XML schema that supports the specification of machine-readable mappings between an XML-based and binary DPM representation. This supports the separate description of the DPM information used for production control and its binary memory representation, e.g., on RFID tags. A server that stores those XML mapping documents and processes the address queries from the embedded production control systems has been developed. To demonstrate the feasibility of the approach this mapping technology was implemented in a demonstration module which was presented at Hannover Messe 2010.	mapper;xml	Marc Seissler;Peter Stephan;Jochen Schlick;Ines Dahmann	2013		10.1007/978-3-642-37377-0_9	electronic engineering;computer hardware;computer science;theoretical computer science	AI	-47.333328235713786	3.6512359684578293	142161
f1797171acde204d4fc939940da220489cfe3b8f	deepdive: declarative knowledge base construction		The dark data extraction or knowledge base construction (KBC) problem is to populate a SQL database with information from unstructured data sources including emails, webpages, and pdf reports. KBC is a long-standing problem in industry and research that encompasses problems of data extraction, cleaning, and integration. We describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems. The key idea in DeepDive is that statistical inference and machine learning are key tools to attack classical data problems in extraction, cleaning, and integration in a unified and more effective manner. DeepDive programs are declarative in that one cannot write probabilistic inference algorithms; instead, one interacts by defining features or rules about the domain. A key reason for this design choice is to enable domain experts to build their own KBC systems. We present the applications, abstractions, and techniques of DeepDive employed to accelerate construction of KBC systems.	knowledge base	Christopher De Sa;Alexander Ratner;Christopher Ré;Jaeho Shin;Feiran Wang;Sen Wu;Ce Zhang	2016	SIGMOD Record	10.1145/2949741.2949756	computer science;theoretical computer science;data mining;database;programming language	DB	-38.37146418548912	2.7448542102182842	142298
0288f510e7d513d35ec071e63794b39330160eef	social network analysis on educational data set in rdf format		The increased usage of information technologies in educational tasks resulted in high volume of data, exploited to build analytical systems that can provide practical insight in the learning process. In this paper, we propose a method of running social network analysis on multiple data sources (academic years, communication tools). To achieve this, the collected data that describe social interactions were converted into a common format by employing a prior developed semantic web educational ontology. Using a mapping language the relational data set was linked to the appropriate concepts defined in the ontology and then it was exported in RDF format. The means for SPARQL access was also provided. Subsequently, query patterns were defined for different social interactions in the educational platform. To prove the feasibility of this approach, Gephi tool set was used to run SNA (Social Network Analysis) on data obtained with the SPARQL queries. The added value of this research lies in the potential of this method to simplify running social network analysis on multiple data sets, on a specific course or the entire academic year, by simply modifying the query pattern.	archive;data model;data structure;gephi;interaction;resource description framework;sparql;semantic web;social network analysis;triplestore	Bogdan Dragulescu;Marian Bucos;Radu Vasiu	2015	CIT		social network analysis;computer science;sparql;artificial intelligence;machine learning;semantic web;data mining;database;world wide web	Web+IR	-42.42087696547192	-0.712230418612818	142507
09e6dde9d094af954d3f34e49a95ff9133781602	modeling system behavior for risk and reliability analysis using kbarm	reliability parameters;model system;grey theory;fuzzy;reliability analysis;system failure;petri nets	Case Study Modeling System Behavior for Risk and Reliability Analysis using KBARM Rajiv Kumar Sharma1,∗,†, Dinesh Kumar2 and Pradeep Kumar2 1Department of Mechanical Engineering, National Institute of Technology (formerly Regional Engineering College), Hamirpur 177005, India 2Department of Mechanical and Industrial Engineering, Indian Institute of Technology, IIT Roorkee, Roorkee (Uttranchal) 247667, India	industrial engineering;integrated information theory;pradeep tapadiya	Rajiv Kumar Sharma;Dinesh Kumar;Pradeep Kumar	2007	Quality and Reliability Eng. Int.	10.1002/qre.849	fuzzy logic;reliability engineering;computer science;engineering;artificial intelligence;operations management;data mining;petri net;statistics	SE	-44.32958304884072	-9.064082157107007	142511
95c6a8541ea874d6edc1ba9ef3d2de19694aa942	an optical backplane connection system with pluggable active board interfaces	data transfer;integrated circuit;polymers;integrated circuits;active optics;circuit boards;design methodology	England & Wales. Company no: 03134912. Registered Office: Langstone Road, Havant, Hampshire PO9 1SA, England. The information given in this brochure is for marketing purposes and is not intended to be a specification nor to provide the basis for a warranty. The products and their details are subject to change. For a detailed specification or if you need to meet a specific requirement please contact Xyratex. UK HQ Langstone Road Havant Hampshire PO9 1SA United Kingdom	backplane	Richard Pitwon;Ken Hopkins;Dave Milward	2006			embedded system;electronic engineering;computer science;engineering;integrated circuit;engineering drawing	DB	-47.03134959730047	-6.377968718000932	142577
e4d7b2313767501b7895366064fe3e1da249daa9	verbalising owl ontologies in isizulu with python		Ontologies as a component of Semantic Web technologies are used in Sub-Saharan Africa mainly as part of ontology-driven information systems that may include an interface in a local language. IsiZulu is one such local language, which is spoken by about 23 million people in South Africa, and for which verbalisation patterns to verbalise an ontology exist. We have implemented the algorithms corresponding to these patterns in Python so as to link it most easily to the various technologies that use ontologies and for other NLP tasks. This was linked to Owlready, a new Python-based OWL API, so as to verbalise an ontology in isiZulu. The verbaliser can run in ‘ontology inside’ mode, outputting the sentences in the terminal for further processing in an ontology-driven information system, and in GUI mode that displays colour-coded natural language sentences for users such as domain experts and linguists. The demo will showcase its features.	algorithm;graphical user interface;information system;natural language processing;ontology (information science);python;semantic web;web ontology language	C. Maria Keet;Musa Xakaza;Langa Khumalo	2017		10.1007/978-3-319-70407-4_12	information retrieval;natural language processing;semantic web;ontology;computer science;ontology (information science);information system;natural language;artificial intelligence;python (programming language);local language	AI	-41.07171031883906	4.183098944898973	142695
af6d08ec5c929751d73029d03ef5ce0ad5b0cc00	first workshop on knowledge base construction, mining and reasoning		1. Motivation and Goals. e success of data mining and search technologies is largely aributed to the ecient and eective analysis of structured data. e construction of a well-structured, machine-actionable database from raw data sources is oen the premise of consequent applications. Meanwhile, the ability of mining and reasoning over such constructed databases is at the core of powering various downstream applications on web and mobile devices. Recently, we have witnessed a signicant amount of interests in building large-scale knowledge bases (KBs) from massive, unstructured data sources (e.g., Wikipedia-based methods such as DBpedia [9], YAGO [19], Wikidata [22], automated systems like Snowball [1], KnowItAll [5], NELL [4] and DeepDive [15], and opendomain approaches like Open IE [2] and Universal Schema [14]); as well as mining and reasoning over such knowledge bases to empower a wide variety of intelligent services, including question answering [6], recommender systems [3] and semantic search [8]. Automated construction, mining and reasoning of the knowledge bases have become possible as research advances in many related areas such as information extraction, natural language processing, data mining, search, machine learning, databases and data integration. However, there are still substantial scientic and engineering challenges in advancing and integrating such relevant methodologies. e goal of this proposed workshop is to gather together leading experts from industry and academia to share their visions about the eld, discuss latest research results, and exchange exciting ideas. With a focus on invited talks and position papers, the workshop aims to provide a vivid forum of discussion about knowledge base-related research. 2. Relevance to WSDM. Knowledge base construction, mining and reasoning is closely related to a wide variety of applications in WSDM, including web search, question answering, and recommender systems. Building a high-quality knowledge base from	dbpedia;data mining;database;downstream (software development);information extraction;knowledge base;machine learning;mobile device;natural language processing;never-ending language learning;question answering;recommender system;relevance;semantic search;web services distributed management;web search engine;wikidata;wikipedia;yago	Xiang Ren;Craig A. Knoblock;William Wang;Yu Su	2018		10.1145/3159652.3160596	data mining;information extraction;knowledge base;computer science	AI	-43.8053836707542	0.9661682931663619	143143
12272aa2d46b38788ae96358f511d3aa4e28ec6b	adding semantics to the discovery of web services in life sciences		Research in the Life Sciences largely depends on the integration of large, distributed and heterogeneous data sources and web services. Due to the large number of available web services, the sheer complexity of the data and the frequent lack of documentation, discovering the most appropriate web service for a given task is a challenge for the user. In this paper we propose a semi-automatic approach, based on semantic techniques, to assist researchers in the discovery and selection of the most appropriate web services to fulfill a set of given requirements. We describe the overall framework of our approach, and the usefulness of our techniques is demonstrated through a Bioinformatics case study.		María Pérez Catalán;Ismael Sanz;Rafael Berlanga Llavori;María José Aramburu Cabo	2011		10.1007/978-3-642-19931-8_18	web service;web application security;web development;web modeling;data web;web analytics;web mapping;web standards;computer science;ws-policy;semantic web;web navigation;social semantic web;data mining;ws-addressing;database;services computing;web intelligence;ws-i basic profile;world wide web;universal description discovery and integration	DB	-41.16217802623472	3.1382304619911108	143198
a8208415eed75db1208ca461e97ab054b1a1fa75	semantic mediation to improve reproducibility for biomolecular nmr analysis	computational reproducibility;ontology;provenance	Two barriers to computational reproducibility are the ability to record the critical metadata required for rerunning a computation, as well as translating the semantics of the metadata so that alternate approaches can easily be configured for verifying computational reproducibility. We are addressing this problem in the context of biomolecular NMR computational analysis by developing a series of linked ontologies which define the semantics of the various software tools used by researchers for data transformation and analysis. Building from a core ontology representing the primary observational data of NMR, the linked data approach allows for the translation of metadata in order to configure alternate software approaches for given computational tasks. In this paper we illustrate the utility of this with a small sample of the core ontology as well as tool-specific semantics for two third-party software tools. This approach to semantic mediation will help support an automated approach to validating the reliability of computation in which the same processing workflow is implemented with different software tools. In addition, the detailed semantics of both the data and the processing functionalities will provide a method for software tool classification.		Michael R. Gryk;Bertram Ludäscher	2018	Transforming digital worlds : 13th International Conference, iConference 2018, Sheffield, UK, March 25-28, 2018, Proceedings. International Conference on Transforming Digital Worlds	10.1007/978-3-319-78105-1_70	core ontology;data mining;linked data;computation;semantics;ontology (information science);metadata;software;workflow;computer science	SE	-40.785446219308255	1.5691711875239842	143375
673e2e5c3685034f8565cfee26c7b79632782a35	tactical route planning: new algorithms for decomposing the map	a algorithm;route planning;robotic vehicles;heirarchical planning	This paper defines a n e w approach and investigates a fundamen ta l problem in route planners. This capability is impor tan t f o r robotic vehicles( M a r t i a n Rovers , etc.) and f o r planning off-road mi l i tary maneuvers. T h e emphas is throughout this paper will be on the design and analysis and hierarchical implementa t ion of our route p lanner . This work was motivated by anticipation of t he need t o search a grid of a trill ion poin ts f o r o p t i m u m routes. This cannot be done s imply by scaling upward f r o m t h e algorithms used t o search a grid of 10,000 points. A lgor i thms SUDc ien t f o r t he sma l l grid are totally inadequate f o r the large grid. Soon, t he challenge will be t o compzGte ofroad routes more t h a n 100 k m long and w i t h a one or two-me ter grid. Prev ious efforts are reviewed and the data structures, decomposition methods and search algorithms are analyzed and l imi ta t ions are discussed. A detailed discussion of a hierarchical imp lemen ta t ion i s provided and the experimental results are analyzed. T h e principal contributions of t he paper are (1) n e w algorithms f o r decomposing the m a p nnd new search methods , (2) analysis of n e w approaches, and (3) the use of expert sys tems , deductive databases and mediators. Exper imenta l results are included of a detailed implementa t ion . *This work was supported by the Army Research Office under Grant Nr. DAAL-03-92-G-0225 and by the National Science Foundation under Grant Nr. IRI-9109755. V.S. Subrahmanian Department of Computer Science & Institute for Advanced Computer Studies University of Maryland College Park, Maryland 207.42.	clips;co-ment;computer science;data structure;deductive database;expert system;geographic information system;image resolution;image scaling;os-tan;robot;sma*;search algorithm;software engineer	John R. Benton;S. Sitharama Iyengar;Weian Deng;Nathan E. Brener;V. S. Subrahmanian	1996	International Journal on Artificial Intelligence Tools	10.1142/S0218213096000146	mathematical optimization;simulation;a* search algorithm;computer science;artificial intelligence;operations research	Robotics	-42.960251569341786	-6.3655586264717385	143419
a3bcbda820f3e8fe8a29b7df00fcef4879555f6f	development of object oriented frameworks for spatio-temporal information systems	information systems;environmental science computing information systems object oriented programming;frameworks;design and development;earth;spatio temporal information systems;information systems object oriented modeling jacobian matrices monitoring geographic information systems data processing remote sensing information analysis earth spatial resolution;data processing;object oriented framework;object oriented programming;object oriented frameworks;environmental science computing;temporal information;environmental information system;monitoring;geographic information systems;remote sensing;design pattern;environment information system;design patterns;object oriented technology;information system;jacobian matrices;information analysis;object oriented modeling;domain specificity;structure analysis;spatial resolution;design patterns object oriented frameworks spatio temporal information systems environmental information system	Domain specific Information Systems (IS) have traditionally been developed using conventional structured analysis and design. This investigation looks into the various aspects of designing and developing an Environmental Information System (EIS) using Object Technology and in the process identifying new, generic and specific design patterns which can be used in developing object oriented frameworks for the environment domain. EIS’s deal with data associated with a phenomena or process at some location on the earth and represented in spatial coordinates (latitude, longitude, height or depth), and at some point in time represented in temporal coordinates (day, month, year, hour, minutes). Large volumes of heterogeneous data are currently available for monitoring and forecasting the environment. These data come from different sensors that have different resolutions in space and time, come in different formats and measure different parameters. Users need to use these data singly or in combination (synergetitally), as well as share and exchange them on a regional, national and/or international level. Data collected over land differ from data collected from the marine environment. Problems specific to data from the marine environment such as manipulation of 4D data, irregular sampling on one or more dimensions and specialized processing requirements make a general purpose Geegraphic Information System (GIS) inappropriate for the task. For land based EIS on the other hand, an off-theshelf GIS may be well suited for both processing and presentation of data, but for marine applications there will be some facilities which are not part of a COTS tool. Thus a framework for EIS, allowing components to be added as needed will be useful in the development of IS’s for terrestrial or marine applications. A system is required where different types of spatial and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists. requires prior specific permission and/or a fee. ICSE ‘99 Los Angeles CA Copyright ACM 1999 l-581 13-074-0/99/05...$5.00 temporal data can be integrated, and the processing tools and models used can be configured depending on the user requirement. A user should be able to include the data, processing tools and models that suit his/her requirements best, while excluding others. Further, it should be possible to use legacy systems with as little modification as possible. By representing the system components viz. data sets, tools and models as objects, it is possible to develop a flexible Environmental Information System which can be easily extended and with a high degree of portability. Additional benefits of using the object oriented paradigm include design and code reuse, which in turn lead to reduced maintenance costs.. An initial study of three different scenarios within the environment domain has been undertaken to identify the processes and parameters relevant for monitoring the environment and for identifying functionalities within the domain. The three scenarios included are the Marine Information System, the Ground Water Information System and an Acoustic Information System. Even though the data requirements, data processing and modeling requirements were vastly different for the three scenarios, some of the basic functionalities were found to be the same. All the ISs required the data to undergo some pre-processing; these data had to be visualized for subjective verification and some of the data were used in models for monitoring and forecasting purposes. By abstracting the common functionalities on a high level, it was found that the Model-View-Controller (MVC) framework architecture was applicable to the IS design as it divided the interactive application into three communicating components. Frameworks for data mangement, data processing and data visualization were common to all the scenarios. By further refining the high level architecture, smaller and simpler frameworks were identified. While frameworks for some components, such as for Graphical User Interface already exist in syome form, others are being developed externally, such as those for data storage and data management. Some of the frameworks, such as those for format conversion, resolution conversion, data preparation and	code reuse;computer data storage;data visualization;design pattern;display resolution;domain-specific language;geographic information system;graphical user interface;high-level architecture;high-level programming language;icse;legacy system;model–view–controller;preprocessor;programming paradigm;requirement;sampling (signal processing);sensor;software portability;structured analysis;terrestrial television;user requirements document;viz: the computer game	Anita Jacob	1999		10.1145/302405.302993	computer vision;data processing;computer science;knowledge management;data mining;programming language;information system	Visualization	-38.880722772063855	-1.9076851567360464	143803
e2014e9c021b2173f21f2e1ceeded8957b560383	producing linked data for smart cities: the case of catania	linked open data for smart cities;semantics and egovernment;rdf data processing;data modeling;interoperability	Semantic Web technologies and in particular Linked Open Data provide a means for sharing knowledge about cities as physical, social, and technical systems, so enabling the development of smart city applications. This paper presents a prototype based on the case of Catania with the aim of sharing the lessons learnt, which can be reused as reference practices in other cases with similar requirements. The importance of achieving syntactic as well as semantic interoperability as a result of transforming heterogeneous sources into Linked Data is discussed: semantic interoperability is solved at data level in order to ease further development on top. We present a comprehensive data model for smart cities that integrates several data sources, including, georeferenced data, public transportation, urban fault reporting, road maintenance and municipal waste collection. We show some novel ontology design patterns for modeling public transportation, urban fault reporting and road maintenance. Domain practitioners and general members of the public have been asked to play with the prototype, and fill out a survey with questions and feedbacks. A computational experiment has been also conducted to evaluate the performance of our data model in terms of practical scalability over increasing data and efficiency under complex queries. All produced data, models, prototype and questionnaire results are publicly accessible online.	align (company);communication endpoint;computation;content negotiation;data cube;data model;design pattern;fault reporting;feedback;general transit feed specification;information system;linked data;password;prototype;requirement;sparql;scalability;semantic web;semantic interoperability;smart city;table (information);usability;user (computing);user experience;vocabulary;web service	Sergio Consoli;Valentina Presutti;Diego Reforgiato Recupero;Andrea Giovanni Nuzzolese;Silvio Peroni;Misael Mongiovì;Aldo Gangemi	2017	Big Data Research	10.1016/j.bdr.2016.10.001	semantic interoperability;data modeling;interoperability;computer science;linked data;data mining;database;world wide web	HCI	-39.701200349397	1.4217620245973654	144022
2ffdeed82238ddccd145ff8eb978cb3b22ef529d	exploiting observations and measurement data standard for distributed lter-italy freshwater sites. water quality issues		Water quality is a multi-source, multi-purpose problem that needs exploiting observations, often taken by a number of heterogeneous bodies. This problem has been tackled within the Italian Long Term Ecological research network (LTER-Italy) in an experiment aimed at testing how ecological observations of mountain lakes water can be shared by OGC (Open Geospatial Consortium) standard services of the Sensor Web Enablement (SWE) initiative. A friendly and easy implementation of these services is fostered by the usage of the open source software Geoinformation Enabling Toolkit StarterKit ® (GET-IT 1 ). It has been used in the experiment to create SOS services, upload observations and create SensorML metadata of the involved sensors. This contribution describes the experiment and presents its results.	apple sos;consortium;geographic information system;multi-purpose viewer;multi-source;open-source software;sensor web;upload	Simone Lanucara;Paola Carrara;Alessandro Oggioni;Michela Rogora;Lyudmila Kamburska;Giampaolo Rossetti	2016	PeerJ PrePrints	10.7287/peerj.preprints.2233v1	data standard;geospatial analysis;metadata;sensorml;database;spatial data infrastructure;biology;geographic information system;observations and measurements;upload	Robotics	-40.491309110859916	-1.4320025326720633	144145
8abc5786ccf87b63885d6a32759792e2b77a4317	case-based data masking for software test management		Data masking is a means to protect data from unauthorized access by third parties. In this paper, we propose a case-based assistance system for data masking that reuses experience on substituting (pseudonymising) the values of database fields. The data masking experts use rules that maintain task-oriented properties of the data values, such as the environmental hazards risk class of residential areas when masking address data of insurance customers. The rules transform operational data into hardly traceable, masked data sets, which are to be applied, for instance, during software test management in the insurance sector. We will introduce a case representation for masking a database column, including problem descriptors about structural properties and value properties of the column as well as the data masking rule as the solution part of the case. We will describe the similarity functions and the implementation of the approach by means of myCBR. Finally, we report about an experimental evaluation with a case base of more than 600 cases and 31 queries that compares the results of a case-based retrieval with the solutions recommended by a data masking expert.	data masking;test management	Mirjam Minor;Shohini Sen-Britain;BS TimothyBergquist	2018		10.1007/978-3-030-01081-2_19	computer science;data mining;test management;column (database);data masking;software;data protection act 1998;masking (art);data set	SE	-37.99291076542135	-3.5542041130670565	144284
d822c014d3a32ca0f77968335b915bc3b5318b25	sanzu: a data science benchmark	science benchmark	The volume of data that is generated each day is rising rapidly. There is a need to analyze this data efficiently and produce results quickly. Data science offers a formal methodology for processing and analyzing data. It involves a work-flow with multiple stages, such as, data collection, data wrangling, statistical analysis and machine learning. In this paper, we look at data analytics systems that support the data science work-flow. The variety of current commercial and open-source data analytics systems differ significantly in terms of available features, functionality, and scalability. A benchmark can be used to evaluate the functionality and performance of a system. However, there is no standard benchmark for evaluating or comparing these data systems for doing data science. In this paper, we introduce a data science benchmark, Sanzu, to evaluate systems with data processing and analytics tasks. Our benchmark includes a micro and macro benchmark. The micro benchmark tests basic operations in isolation. It consists of task suites for reading and writing, data wrangling, statistical analysis, machine learning and time series analysis. Each macro workload evaluates an analytics application where a series of analysis or functions are based on a real world application. The macro benchmark focuses on sports and smart grid analytics. We evaluate these tasks on five different popular data science frameworks and systems: R, Anaconda Python, Dask, PostgreSQL (MADlib) and PySpark. For micro benchmark we generate synthetic datasets with 3 scale factors: 1, 10 and 100 (scale factor 1=1 million). The macro benchmark uses data generated from real-world data sources.	anaconda;benchmark (computing);dask;data model;data science;data system;machine learning;open-source software;performance evaluation;postgresql;python;r language;scalability;source data;synthetic intelligence;time series	Alex Watson;Deepigha Shree Vittal Babu;Suprio Ray	2017	2017 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2017.8257934	data mining;data science;data collection;data modeling;computer science;data system;macro;big data;benchmark (computing);data analysis;analytics	DB	-35.85611947528944	-4.741799337323897	144361
8ae36c7efede45b7ab951f06b4cfebf059b5e85d	the esa approach to long-term data preservation using caspar.			esa	Sergio Albani	2010	ERCIM News		data mining;computer science	NLP	-41.8913663092111	-7.743767594247561	144397
76989dd0aa70929e556eb7c5f807bf653eac6650	semantic annotation and publication of linked open data		Nowadays, there has been an increment of open data government initiatives promoting the idea that particular data produced by public administrations (such as public spending, health care, education etc.) should be freely published. However, the great majority of these resources is published in an unstructured format (such as spreadsheets or CSV) and is typically accessed only by closed communities. Starting from these considerations, we propose a semi-automatic experimental methodology for facilitating resource providers in publishing public data into the Linked Open Data (LOD) cloud, and for helping consumers (companies and citizens) in efficiently accessing and querying them. We present a preliminary method for publishing, linking and semantically enriching open data by performing automatic semantic annotation of schema elements. The methodology has been applied on a set of data provided by the Research Project on Youth Precariousness, of the Modena municipality, Italy.	linked data	Serena Sorrentino;Sonia Bergamaschi;Elisa Fusari;Domenico Beneventano	2013		10.1007/978-3-642-39640-3_34	linked data;data mining;world wide web;information retrieval	NLP	-41.92759567458515	2.779395863086165	144413
62212e34aad4cdf35a24ebf530b2c6015e5bfb9f	towards semantic reasoning in knowledge management systems		Modern applications of AI systems rely on their ability to acquire, represent and process expert knowledge for problem-solving and reasoning. Consequently, there has been significant interest in both industry and academia to establish advanced knowledge management (KM) systems, promoting the effective use of knowledge. In this paper, we examine the requirements and limitations of current commercial KM systems and propose a new approach to semantic reasoning supporting Big Data access, analytics, reporting and automation related tasks. We also provide comparative analysis of how state-of-the-art KM systems can benefit from semantics by illustrating examples from the life-sciences and industry. Lastly, we present results of our semantic-based analytics workflow implemented for Siemens power generation plants.	knowledge management;management system	Gulnar Mehdi;Sebastian Brandt;Mikhail Roshchin;Thomas A. Runkler	2016		10.1007/978-3-319-92928-6_9	knowledge economy;automation;semantic technology;semantics;big data;workflow;data access;knowledge management;computer science;analytics	AI	-37.41819429297156	-5.947422210698627	144568
ab224cb62d971a4fad22ff0f9b0920fed824d112	chilean virtual observatory	chilean virtual observatory software technologies alma data ivoa compliant services chivo prototype alma observatory big data problems complex data models ivoa international virtual observatory alliance astronomical data processing software infrastructure chivo;ict infrastructure virtual observatory ivoa;ict infrastructure;virtual observatory;ivoa;observatories software data models big data xml telescopes;virtual reality astronomy computing big data software architecture	This paper presents the challenges, architecture and current status of the Chilean Virtual Observatory (ChiVO), which is a software infrastructure for accessing and processing astronomical data generated in Chile. As ChiVO is part of the International Virtual Observatory Alliance (IVOA), we strictly follow the protocols and standards that this organization produce. However, there are always open challenges due to the new observational technologies and local requirements that motivates research on every new virtual observatory, such as the complex data models and Big Data problems that the ALMA Observatory is confronting. The current ChiVO prototype includes IVOA compliant services as well as new solutions designed for ALMA data, all of them using modern software technologies.	big data;data model;prototype;requirement;virtual observatory	Mauricio Solar;Mauricio Araya-López;Luis Arevalo;Víctor Parada;Ricardo Contreras;Diego Mardones	2015	2015 Latin American Computing Conference (CLEI)	10.1109/CLEI.2015.7359465	simulation;engineering;data science;world wide web	Visualization	-40.921957473441125	-1.1415260265494092	144623
92ef19db71e189ed3bc7a6cef7d9bef449cbaf13	special issue on entropy generation in thermal systems and processes	n a;entropy generation	Thermodynamics is defined as the science of energy and entropy which are applicable to all fields of science and engineering. This special issue is essentially devoted to the entropy generation in thermal engineering systems and applications. Entropy generation is one of the most significant problems to overcome to optimize a system/process and its performance, and this will unfortunately remain as a crucial problem to the next generation. In this regard, better understanding of the concept of entropy and its role amo ng various classes of thermal systems and processes with a diverse coverage is crucial. Research into transport phenomena in energy systems and applications has substantially increased during the past a few decades due to its diversity in applications. This makes the special issue a most timely addition to existing literature. It includes recent major developments in both the fundamental and applications, and provides a valuable source to researchers dealing with analysis of entropy generation in thermal systems and processes. This special issue contains 14 technical papers on entropy generation in thermal systems and processes ranging from heat engines to natural/forced convection systems. The collection of topic included is suitable for a wide range of interests from practitioners to researchers working on thermal systems and processes. Each paper was peer-reviewed under the guidelines of the Journal before publication.	entropy (information theory);mit engineering systems division;next-generation network	Ibrahim Dincer	2003	Entropy	10.3390/e5050357	entropy production;thermodynamics;physics;entropy	SE	-47.82096279304471	-0.3672548485862511	144672
6d0b057d0b9837f657eab01205111f35c473e9f8	query operators for comparing uncertain graphs		Extending graph models to incorporate uncertainty is important for many applications, including citation networks, disease transmission networks, social networks, and observational networks. These networks may have existence probabilities associated with nodes or edges, as well as probabilities associated with attribute values of nodes or edges. Comparison of graphs and subgraphs is challenging without probabilities. When considering uncertainty of different graph elements and attributes, traditional graph operators and semantics are insufficient. In this paper, we present a prototype SQL-like graph query language that focuses on operators for querying and comparing uncertain graphs and subgraphs. Two interesting operators include ego neighborhood similarity and semantic path similarity. Similarity operators are particularly useful for comparison queries, the focus of this paper. After motivating and describing our operators, we present an implementation of a query engine that uses this query language. This implementation combines a layered and service-oriented architecture and is designed to be extensible, so that simple operators can be used as building blocks for more complex ones. We demonstrate the utility of our query language and operators for analyzing uncertain graphs based on two real world networks, a dolphin observation network and a citation network. Finally, we conduct a performance evaluation of some of the more complex operators, illustrating the viability of these operators for analysis of larger graphs.		Denis Dimitrov;Lisa Singh;Janet Mann	2015	Trans. Large-Scale Data- and Knowledge-Centered Systems	10.1007/978-3-662-46485-4_5	architecture;machine learning;operator (computer programming);artificial intelligence;citation;semantics;query language;social network;extensibility;graph;computer science	HCI	-35.10831014791953	3.485473433392282	144766
5f02f7270aa74d1e20a64ce51ae14d7f70c04143	an etl framework for online analytical processing of linked open data	olap;etl;multidimensional model;linked open data	Growing amount of data are being published online in machinereadable formats, and LOD (Linked Open Data) has emerged as a way to share such data across Web resources. Since LOD data often contain numerical data, such as statistics, there is a growing demand to make OLAP (Online Analytical Processing) analysis over such data. To make it possible to apply off-the-shelf OLAP systems for analyzing LOD data, we propose a framework to streamline the Extract, Transform, and Load (ETL) process from LOD to multidimensional data models for OLAP. Unlike other related approaches, our framework does not require RDF vocabularies dedicated for specifying multidimensional model for OLAP. Instead, given an LOD dataset, we exploit the relationships among entities and external information in the referenced LOD to generate an OLAP data model. In a case study, we demonstrate that our framework can extract OLAP data models from different kinds of real LOD datasets.		Hiroyuki Inoue;Toshiyuki Amagasa;Hiroyuki Kitagawa	2013		10.1007/978-3-642-38562-9_12	online analytical processing;computer science;linked data;data mining;database;world wide web	ML	-36.15909840172704	1.7693641007758039	144899
83f04c18e33d9bad5e519a5c4816f6de37637de5	linkitup: link discovery for research data	universiteitsbibliotheek	Linkitup is a Web-based dashboard for enrichment of research output published via industry grade data repository services. It takes metadata entered through Figshare.com and tries to find equivalent terms, categories, persons or entities on the Linked Data cloud and several Web 2.0 services. It extracts references from publications, and tries to find the corresponding Digital Object Identifier (DOI). Linkitup feeds the enriched metadata back as links to the original article in the repository, but also builds a RDF representation of the metadata that can be downloaded separately, or published as research output in its own right. In this paper, we compare Linkitup to the standard workflow of publishing linked data, and show that it significantly lowers the threshold for publishing linked research data.		Rinke Hoekstra;Paul T. Groth	2013			computer science;data mining;metadata;world wide web;data element;meta data services;information retrieval;metadata repository	ML	-42.95633173168624	1.9543069652314045	145155
48bf0bcd799409ebe3b832ff4356d91effcef847	graph mining and community evaluation with degeneracy. (fouille de graphe et communautaire evaluation avec degenerescence)			degeneracy (graph theory);structure mining	Christos Giatsidis	2013				NLP	-39.74300386220679	-9.401190576300538	145226
c28e540d201e9418dd59c38d2582683ba55001eb	enhancing the discoverability and interoperability of multi-disciplinary semantic repositories		The aggregation of multi-disciplinary information is a challenge faced by large-scale data infrastructures serving scientific domains such as biodiversity, agronomy or ecology. This requires the integration of ontologies or thesauri from different domains. These semantic resources are often hosted within domain specific repositories which can be harvested for that purpose. The lack of discoverability, the technical and metadata heterogeneity of the semantic repositories pose a challenge for their effective integration. In this context, we argue that there is a need for a semantic lookup-service to access and use this heterogeneous landscape. We then present a proof-of-concept design and implementation for harvesting different ontology repositories (BioPortal, AgroPortal and EBI-OLS). We show some preliminary analytics and discuss technical issues regarding aggregation. Finally, we conclude with an open call for collaboration to address the issues hampering such initiatives.	discoverability;ecology;external bus interface;interoperability;lookup table;ontology (information science);ordinary least squares;thesaurus	Doron Goldfarb;Yann Le Franc	2017			world wide web;semantic interoperability;information retrieval;interoperability;discipline;discoverability;computer science	AI	-41.44461237053793	1.404468979465781	145324
7de2721ff0c141e737ad40159521e92ed61b5c7f	spatial social networks identified from urban group travel		1. State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University Beijing, 100044, China 2. Key Laboratory of Transport Industry of Big Data Application Technologies for Comprehensive Transport, Ministry of Transport, Beijing Jiaotong University, 100044, China 3. School of Reliability and Systems Engineering, Beihang University, Beijing 100191, China 4. Beijing Transportation Information Center, Beijing, 100161, China	big data;social network;systems engineering	Huijun Sun;Kangli Zhu;Jianjun Wu;Daqing Li;Ziyou Gao;Haodong Yin;Yunchao Qu;Xin Yang;Hao Liu	2018	CoRR		friendship;machine learning;trips architecture;artificial intelligence;management science;individual mobility;predictability;beijing;social network;mathematics;redistribution (cultural anthropology)	ML	-45.10664712939982	-8.582907057402053	145554
22e1974d3d8a4796e792226bcbc6357f0865d7ae	a methodology for integrating new scientific domains and applications in a virtual laboratory environment	virtual laboratory;universiteitsbibliotheek	Emergence of advanced complex experiments in experimental sciences resulted in a change in the way of experimentation. Several solutions have been proposed to support scientists with their complex experimentations, ranging from simple data portals to virtual laboratories. These solutions offer a variety of facilities to scientists, such as management of experiments and experiment-related information, and management of resources. However, issues related to adding new types of experiments to proposed support environments still remain untouched, causing inefficient utilization of efforts and inadequate transfer of expertise. The main topic of this paper is to present a methodology for integrating new scientific domains and applications in a multidisciplinary virtual laboratory environment. In order to complement the methodology with the right context, the paper also presents an experiment model that uniformly represents scientific experiments, data models for modelling experiment-related information, and mechanisms for the management of this information.	data model;emergence;experiment;portals	Ersin Cem Kaletas;Hamideh Afsarmanesh;Louis O. Hertzberger	2004			computational science;human–computer interaction;computer science	DB	-45.037134920078	-0.5088097295652342	145594
5d14b1f966b216313fbbda21a5732e985f0b48ba	spree - trial-based improvements: transferring an enterprise 2.0 into a web 2.0 expert community application		The Innovation Development of Deutsche Telekom Laboratories has developed a web portal for enterprise expert communities. It combines expert matching and communication functions to initiate and support domain specific online discussions. This paper describes the move from basic features of this corporate portal to a Web 2.0 instance using the results of a field trial and a usability evaluation. Following a new short term scope and integrating an innovative location mashup, the web portal was overhauled in a completely new design. The achievements also lead into comprising an architectural review and operational experiences. The conclusion contains an outlook regarding both, new innovative features and system architecture.	enterprise 2.0;mashup (web application hybrid);microsoft outlook for mac;systems architecture;usability;web 2.0	Andreas Rederer;Gerald Eichler;Thomas Kury;Roland Schwaiger	2011			enterprise 2.0;mashup;systems architecture;usability;knowledge management;web 2.0;engineering	Web+IR	-48.124863655336725	3.253843056646913	145693
1de2637bd0aeec43204e418d66e8495849d9b6b1	genetic agent approach for improving on-the-fly web map generalization		The utilization of web mapping becomes increasingly important in the domain of cartography. Users want access to spatial data on the web specific to their needs. For this reason, different approaches were appeared for generating on-the-fly the maps demanded by users, but those not suffice for guide a flexible and efficient process. Thus, new approach must be developed for improving this process according to the user needs. This work focuses on defining a new strategy which improves on-the-fly map generalization process and resolves the spatial conflicts. This approach uses the multiple representation and cartographic generalization. The map generalization process is based on the implementation of multiagent system where each agent was equipped with a genetic patrimony.	agent-based model;cartographic generalization;cartography;map;multi-agent system;web mapping	Brahim Lejdel;Okba Kazar	2012	CoRR		computer science;artificial intelligence;machine learning;data mining;cartographic generalization	ML	-36.830118143977806	-0.7851478042266918	145907
209dcce85c02d6398863f1163a70fc8d79cabce2	discovery informatics in biological and biomedical sciences: research challenges and opportunities		"""New discoveries in biological, biomedical and health sciences are increasingly being driven by our ability to acquire, share, integrate and analyze, and construct and simulate predictive models of biological systems. While much attention has focused on automating routine aspects of management and analysis of """"big data"""", realizing the full potential of """"big data"""" to accelerate discovery calls for automating many other aspects of the scientific process that have so far largely resisted automation: identifying gaps in the current state of knowledge; generating and prioritizing questions; designing studies; designing, prioritizing, planning, and executing experiments; interpreting results; forming hypotheses; drawing conclusions; replicating studies; validating claims; documenting studies; communicating results; reviewing results; and integrating results into the larger body of knowledge in a discipline. Against this background, the PSB workshop on Discovery Informatics in Biological and Biomedical Sciences explores the opportunities and challenges of automating discovery or assisting humans in discovery through advances (i) Understanding, formalization, and information processing accounts of, the entire scientific process; (ii) Design, development, and evaluation of the computational artifacts (representations, processes) that embody such understanding; and (iii) Application of the resulting artifacts and systems to advance science (by augmenting individual or collective human efforts, or by fully automating science)."""	automated planning and scheduling;big data;biological system;computation;documented;experiment;informatics (discipline);information processing;large;morphologic artifacts;pacific symposium on biocomputing;predictive modelling;review [publication type];science;simulation;software documentation	Vasant Honavar	2015	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing		knowledge management;body of knowledge;bioinformatics;big data;information processing;materials informatics;biomedical sciences;engineering informatics;informatics;translational research informatics;biology	HPC	-35.49924922024651	-9.651757826086293	145927
4a6131e395150fe496185597044fb192c0929ac0	modelling, representation and implementation of imperfect information for an enhanced exploration of large databases	fuzzy attributes;fuzziness;uncertainty;imprecision;large data sets;imperfect information;computing;incomplete data;mapping rule;business information systems;fuzzy database	The rapid development of social networks, intelligent sensors, mobile solutions and Internet of things has led to the emergence of large data sets. Efficiently and effectively exploring these data sets is a challenging question, especially when the imperfectness of real-world needs to be taken into account. The objective of this paper is thus to propose several solutions for modelling, representing and implementing imperfect information within large fuzzy databases. More specifically, in this paper imperfect information is modelled through a series of generic fuzzy data types, uniformly represented by means of possibility distributions and implemented using the basic constructs of object databases. These solutions are particularly useful for exploring large fuzzy databases since they permit to minimize the space required to store imperfect information, and access efficiently and effectively these databases.		Sabrine Jandoubi;Afef Bahri;Nadia Yaacoubi Ayadi;Salem Chakhar;Ashraf Labib	2016	Journal of Decision Systems	10.1080/12460125.2017.1252232	computing;uncertainty;computer science;theoretical computer science;management information systems;data mining;database	DB	-36.02687906311772	-1.4737170873159708	145962
f71be21f9d553252939ce4dcba3522eeafa9a590	analysis of water remote sensed data: requirements for data bases and data bases interactions	coastal waters;remote sensing data	Geo-information system and analysis Raster data base design GIS Vector based data structure/design Data based creation for urban area analysis Urban information system for resources and integrated developing planning Urban modeling GIS application case studies Grid cell Data Processing Principle of grid cell date processing, Rasterizing point, line and polygons Selection of grid cell size and effect on data quality Raster versus Vector Advantages and disadvantages	data quality;data structure;database;geographic information system;interaction;raster data;rasterisation	Piero Mussio;R. Rabagliati	1979		10.1007/3-540-09763-5_20	data mining;remote sensing	HPC	-38.47047636636904	-3.3733332486109804	146074
10bdfa25144a75c3d5cbebefad5b7ebddaa1c0a7	making biomedical data usable: nih community-based data and metadata standards efforts		• Collect, organize, and make available trusted, systematically organized, and curated information about data-related standards Community-based standards development • Activities that could advance community-based standards landscape (e.g., creating a collaborative workspace or an advising structure toward standards development, extension, or adoption). • Gaps in community-based data standards of relevance to NIH research, including real use-cases (e.g., emerging fields, research domains with multiple existing data standards that could benefit from additional work, integration and/or reconciliation). • Lessons learned from existing field-tested processes and infrastructure. • Common challenges/pain points in development (e.g., methods for community engagement or building interoperability with other related standards). The mission of the NIH Big Data to Knowledge (BD2K) initiative is to enable biomedical scientists to capitalize more fully on the Big Data being generated by research communities. With advances in technologies, these investigators are increasingly generating and using large, complex, and diverse datasets. However, the ability of researchers to locate, analyze, and use Big Data (and more generally all biomedical and behavioral data) is often limited for reasons related to access to relevant software and tools, expertise, and other factors. BD2K aims to develop the new approaches, standards, methods, tools, software, and competencies that will enhance the use of biomedical Big Data by supporting research, implementation, and training in data science and other relevant fields. One initiative within BD2K is to establish community-driven frameworks for developing and using standards for data and metadata. Such standards enable broad data sharing and reuse of data generated across the full spectrum of NIH-relevant research, from single investigators conducting R01-driven research to large collaborative networks and consortia. Standards for the metadata that describe the samples and experiments associated with the data, in addition to standards for each of the data types themselves, would greatly facilitate (and are probably even required for) large-scale data sharing and data integration. NIH should help establish flexible frameworks for developing data and metadata standards for newly emerging data types that are expected to be used widely, thereby encouraging various biomedical research communities to develop such standards in coordinated ways. Priorities for standardization should be community-driven. Standards should be applicable to both research and clinical data, where appropriate. It will be necessary to address a range of issues, including developing common data formats and data elements for particular types of studies and linking established care standards to meaningful use standards for electronic health records (EHRs), to the …	big data;collaborative network;data science;experiment;global variable;interoperability;relevance;workspace	Allen Dearry;Cindy Lawler;Rebecca Boyles;Astrid Haugen;Huerta Michael	2014			metadata;data mining;usable;information retrieval;computer science	HCI	-42.947153323773755	-0.4555252879501668	146183
8bbee585ea8b74c528c14a7f6356ab41bd1f04d7	small programming exercises 2	programmation;exercice;programming	The four exercises we present this time are all variations on a common theme. See if you can discover their underlying solution strategy. All exercises allow linear solutions, i.e. solutions whose computation times are proportional to the sizes of the arrays given. None of them require the introduction of auxiliary arrays. Exercise 5 is due to Jan Tijmen Udding. The other three exercises have been around for some time already. (For an exposition on the notation used the reader is referred to the previous issue of Small Programming Exercises.)	computation;jan bergstra	Martin Rem	1983	Sci. Comput. Program.	10.1016/0167-6423(83)90023-0	programming;artificial intelligence;algorithm	PL	-47.676285692571106	-2.5707195486903105	146215
593e9f3085833d96cd3cb11cd61e4fd38b9abdd0	towards a linked-data based visualization wizard	consuming linked data;visualization;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;lod;visualization human machine interaction lod consuming linked data;human machine interaction	Datasets published in the LOD cloud are recommended to follow some best practice in order to be 4-5 stars Linked Data compliant. They can often be consumed and accessed by different means such as API access, bulk download or as linked data fragments, but most of the time, a SPARQL endpoint is also provided. While the LOD cloud keeps growing, having a quick glimpse of those datasets is getting harder and there is a need to develop new methods enabling to detect automatically what an arbitrary dataset is about and to recommend visualizations for data samples. We consider that “a visualization is worth a million triples”, and in this paper, we propose a novel approach that mines the content of datasets and automatically generates visualizations. Our approach is directly based on the usage of SPARQL queries that will detect the important categories of a dataset and that will specifically consider the properties used by the objects which have been interlinked via owl:sameAs links. We then propose to associate type of visualization for those categories. We have implemented this approach into a so-called Linked Data Vizualization Wizard (LDVizWiz).	application programming interface;best practice;communication endpoint;data visualization;download;javascript;linked data;music visualization;online and offline;prototype;sparql;sensor;star catalogue;vocabulary	Ghislain Auguste Atemezing;Raphaël Troncy	2014			computer science;data science;data mining;world wide web	Visualization	-40.01544412467232	0.2593646365906976	146290
8a720d020c8ed34706a999d5dbf50ba6808c2c36	semantic wiki for visualization of social media analysis		A semantic wiki provides visualization of social media analysis applicable to military Information Operations and law enforcement counterterrorism efforts. Using inputs from disparate data sets, semantic software exports data to link analysis, geospatial displays, and temporal representation. Challenges encountered in software development include the balance between automated and human assisted entity extraction, interoperability with existing visualization systems and ontology management.	interoperability;link analysis;named-entity recognition;ontology (information science);social media;software development;wiki	Daniel Reininger;David Ihrie;Bob Bullard	2010			semantic interoperability;semantic computing;information visualization;computer science;data mining;database;world wide web	SE	-40.4356555907253	0.17013476641425998	146637
15a57dadddf5af0127d750377903487d0c94422e	a multi-criteria content-based filtering system	content based filtering;settore inf 01 informatica;frameworks;multiple criteria;information overload;requirements gathering;models	In this paper we present a novel filtering system, based on a new model which reshapes the aims of content-based filtering. The filtering system has been developed within the EC project PENG, aimed at providing news professionals, such as journalists, with a system supporting both filtering and retrieval capabilities. In particular, we suggest that in tackling the problem of information overload, it is necessary for filtering systems to take into account multiple aspects of incoming documents in order to estimate their relevance to a user's profile, and in order to help users better understand documents, as distinct from solely attempting to either select relevant material from a stream, or block inappropriate material. Aiming to so this, a filtering model based on multiple criteria has been defined, based on the ideas gleamed in the project requirements stage. The filtering model is briefly described in this paper.	information overload;recommender system;relevance;requirement	Gabriella Pasi;Gloria Bordogna;Robert Villa	2007		10.1145/1277741.1277903	requirements analysis;computer science;software framework;collaborative filtering;information filtering system;information overload;data mining;multimedia;world wide web;information retrieval	Web+IR	-45.00890612102303	3.6246398638107378	146860
44e8a3dc6bbcccaaee864e309b1d72c571874687	the design of a community science cloud: the open science data cloud perspective	open data data intensive computing cloud computing science clouds;social science community science cloud design open science data cloud perspective osdc petabyte scale data cloud infrastructure data center biology earth science;natural sciences computing cloud computing;science clouds;open data;data intensive computing;natural sciences computing;cloud computing	In this paper we describe the design, and implementation of the Open Science Data Cloud, or OSDC. The goal of the OSDC is to provide petabyte-scale data cloud infrastructure and related services for scientists working with large quantities of data. Currently, the OSDC consists of more than 2000 cores and 2 PB of storage distributed across four data centers connected by 10G networks. We discuss some of the lessons learned during the past three years of operation and describe the software stacks used in the OSDC. We also describe some of the research projects in biology, the earth sciences, and social sciences enabled by the OSDC.	cloud computing;data center;petabyte;tag cloud	Robert L. Grossman;Matthew Greenway;Allison P. Heath;Ray Powell;Rafael D. Suarez;Walt Wells;Kevin P. White;Malcolm P. Atkinson;Iraklis A. Klampanos;Heidi L. Alvarez;Christine Harvey;Joe Mambretti	2012	2012 SC Companion: High Performance Computing, Networking Storage and Analysis	10.1109/SC.Companion.2012.127	cloud computing;computer science;data science;operating system;data-intensive computing;data mining;world wide web	HPC	-41.34119181765567	-1.197284239245092	146871
b050dc42d1ed1710b8442f406856b1ebfc4b7967	semantic computing in social media	semantic computing;social networks;social media	The ever-increasing amount of information flowing through Social Media presents numerous opportunities for the generation of Business Intelligence. Challenges exist in the leveraging of these data sources due to their heterogeneity and unstructured content. This paper presents the application of Semantic Computing to Social Media for industrial application, focusing on topic identification and behavior prediction. The methodologies described can benefit many areas of an organization including support of marketing, customer service, engineering and public relations. Results demonstrate that business operations can be substantially enhanced through application of Semantic Computing to Social Media.	semantic computing;social media	David A. Ostrowski	2013	Int. J. Semantic Computing	10.1142/S1793351X13500062	semantic computing;social media;semantic grid;computer science;knowledge management;data science;social semantic web;data mining;database;world wide web;social computing;social network	HPC	-40.061844770538094	-7.2301279492587724	147011
d58a33cc3e0a70f18a624a2e37293635e1373870	an evaluation approach for dynamics-aware applications using linked data	protocols;change detection;linked data;dynamics aware application;vocabulary;resource description framework;vocabulary load modeling benchmark testing data models resource description framework ontologies protocols;data dependence;data quality dynamics aware application linked dataset dynamics;ontologies;evaluation;data quality;data handling;evaluation dataset dynamics linked data;load modeling;dataset dynamics;linked dataset dynamics;benchmark testing;data models	One possible threat to linked data quality is the lack of knowledge about the dynamics in dependent remote datasets. Linked data consuming applications often need to be aware of changes in these datasets in order to update local data dependencies. Dataset dynamics recently emerged as an active research topic and methods for the detection, propagation, and description of the dynamics in linked datasets are being developed. It is, however, difficult to compare these methods on a quantitative and qualitative level due to the lack of appropriate benchmarks and evaluation infrastructures. Therefore we have implemented a reusable infrastructure for the evaluation of applications that are concerned with linked dataset dynamics. Our contributions comprise a vocabulary for the description of timely-ordered changes in linked datasets and an extensible tool-set for extracting and simulating these changes. We also provide two example datasets and explain their usage in the evaluation of our own tool dealing with dynamic datasets. We consider these contributions as a first step towards a reference benchmark for measuring the performance of solutions dealing with issues resulting from dataset dynamics, such as low and high-level change detection or efficient and scalable notification mechanisms.	algorithm;benchmark (computing);data dependency;data quality;extensibility;high- and low-level;linked data;linked list;scalability;simulation;software propagation;testbed;vocabulary	Niko Popitsch;Bernhard Haslhofer;Elaheh Momeni	2010	2010 Workshops on Database and Expert Systems Applications	10.1109/DEXA.2010.52	data modeling;communications protocol;benchmark;data quality;computer science;ontology;artificial intelligence;data science;evaluation;group method of data handling;rdf;linked data;data mining;database;world wide web;change detection	AI	-35.63234585229635	2.2643121393220857	147385
7f1e0c95a90229cdcf00a62231d3fdc8f0e1b9f9	data alignment and integration	carb;homeland security;alignement;data sharing;databases air pollution control systems us government merging monitoring terrorism electronic mail protection automatic control;administracion electronica;information theory government data processing distributed databases internet;information model;data integrity;integration information;data collection;auxiliary information;guspin;facilities registry system;national emission inventory;similitude;information integration;identity theft data alignment data integration entity matching problem heterogeneous data sources us government metadata information theory principle data driven paradigm guspin system sift system government problem terrorist detection;internet;sift;sift carb ceidars digital government data sharing national emission inventory facilities registry system information modeling guspin;administration electronique;electronic government;similarity;integracion informacion;alineamiento;distributed databases;information modeling;pure data;similitud;theorie information;government data processing;alignment;air quality;california air resources board;domain specificity;information theory;digital government;identity theft;ceidars;heterogeneous data sources;teoria informacion	T o handle the wide range of geographic scales and complex tasks that it must administer, the government splits its data in many different ways, collecting it at different times and through different agencies. The resulting massive data heterogeneity makes it impossible to effectively locate, share, or compare data across sources, let alone achieve computational data interoperability. Many settings urgently need some form of data alignment or merging. For example, an air-quality scientist at a state environmental agency such as the California Air Resources Board (CARB) reconciles air emissions data from local regions to monitor overall patterns and support air-policy regulation. In a homeland security scenario, analysts identify and track threat groups using separately collected and stored individual behaviors such as phone calls, e-mail messages, financial transactions, and travel itineraries. Addressing these issues requires finding similarities between entities within or across heterogeneous data sources. To date, most approaches for integrating data collections, or even for creating mappings across comparable data sets, require manual effort. Despite some promising recent work, the automated creation of such mappings is still in its infancy because equivalences and differences manifest themselves at all levels, from individual data values through metadata to the explanatory text surrounding the data collection as a whole. Some data sources contain auxiliary information such as relational structure or metadata, which have proven useful in interrelating entities. However, such auxiliary data can be outdated, irrelevant, overly domain-specific, or simply nonexistent. Therefore, a general-purpose solution can’t rely on such auxiliary data. All we can count on is the data itself: a set of observations describing the entities. Applying this purely data-driven paradigm, we’ve built two systems: Guspin for automatically identifying equivalence classes or aliases, and Sift for automatically aligning data across databases. The key to our underlying technology is identifying the most informative observations and then matching entities that share them. We’ve used our systems to align US Environmental Protection Agency (EPA) data between the Santa Barbara County Air Pollution Control District (SBCAPCD) and Ventura County Air Pollution Control District (VCAPCD) emissions inventory databases and the CARB statewide inventory database, as well as to identify equivalence classes in the EPA’s Facilities Registry System (FRS). This work can significantly reduce the amount of human effort involved in creating single-point access to multiple heterogeneous databases.		Patrick Pantel;Andrew Philpot;Eduard H. Hovy	2005	IEEE Computer	10.1109/MC.2005.406	information theory;information model;computer science;artificial intelligence;operating system;data mining;management;world wide web;computer security	ML	-37.86007416525548	-0.6237239647847989	147393
7c8d8a6ba38f702a3faad428242f8591bf61cb93	torpedo: improving the state-of-the-art rdf dataset slicing	rdf dataset slicing;slicing distributed open dataset;relevant fragment extraction	Over the last years, the amount of data published as Linked Data on the Web has grown enormously. In spite of the high availability of Linked Data, organizations still encounter an accessibility challenge while consuming it. This is mostly due to the large size of some of the datasets published as Linked Data. The core observation behind this work is that a subset of these datasets suffices to address the needs of most organizations. In this paper, we introduce Torpedo, an approach for efficiently selecting and extracting relevant subsets from RDF datasets. In particular, Torpedo adds optimization techniques to reduce seek operations costs as well as the support of multi-join graph patterns and SPARQL FILTERs that enable to perform a more granular data selection. We compare the performance of our approach with existing solutions on nine different queries against four datasets. Our results show that our approach is highly scalable and is up to 26% faster than the current state-of-the-art RDF dataset slicing approach.	accessibility;constraint satisfaction dual problem;dbpedia;drugbank;high availability;linked data;mathematical optimization;maximal set;performance tuning;relevance;resource description framework;sparql;scalability;triplestore;world wide web	Edgard Marx;Saeedeh Shekarpour;Tommaso Soru;Adrian M. P. Brasoveanu;Muhammad Saleem;Ciro Baron;Albert Weichselbraun;Jens Lehmann;Axel-Cyrille Ngonga Ngomo;Sören Auer	2017	2017 IEEE 11th International Conference on Semantic Computing (ICSC)	10.1109/ICSC.2017.79	computer science;data mining;database;world wide web	Visualization	-35.7315142459911	3.888492968337558	147394
99588254258fa9db65dd8088d2219204657380cf	requirements on linked data consumption platform		The publication of data as Linked Open Data (LOD) gains traction. There are lots of different datasets published, more vocabularies are becoming W3C Recommendations and with the introduction of DCAT-AP v1.1 and the emergence of the European data portal and a multitude of national open data portals, lots of datasets are discoverable and accessible using their DCAT-AP metadata in RDF. Yet, the consumption of LOD is lacking in comfort and availability of tools that would exploit the benefits of LOD and allow users to discover, access, integrate and reuse LOD easily, as promised by the promoters of LOD and supposedly paid by the additional effort put into the 5-star data publication by the publishers. Compared to the consumption of 3-star CSV and XML files, the consumption of LOD is still quite complicated and the LOD benefits are not exploited enough nor visible enough to justify the effort for many publishers. In this paper we identify 40 requirements which a Linked Data Consumption Platform (LDCP) should satisfy in order to be able to exploit the LOD benefits in a way that would ease the LOD consumption and justify the additional effort put into LOD publication. We survey 8 relevant and currently available tools based on their coverage of the identified requirements.	emergence;linked data;portals;recommender system;requirement;traction teampage;vocabulary;xml	Jakub Klímek;Petr Skoda;Martin Necaský	2016			database;linked data;computer science	Web+IR	-41.588209539944174	2.415277718768001	147476
27e6bc0e3068dc33805ec58141461a2084020085	special issue on advances in sensor array processing in memory of alex b. gershman		Sensor Array Processing has been a vibrant and influential research area that has spawned important new theory and methods over the last three decades. Practical applications are all around us: in wireless communications, radar, sonar, acoustics, speech, and biomedicine, just to mention a few. The dual aim of this special issue is to showcase recent research in the area of sensor array processing in memory of our distinguished colleague and friend Alex B. Gershman, and to celebrate his many research accomplishments. Alex solved longstanding problems and made fundamental contributions to sensor array processing, paving the way for many additional contributions. Because of this dual purpose, the selection of topics is naturally aligned with some of Alex’s favorite topics — the kinds of problems that he enjoyed, and his scientific legacy. Alex was born in Nizhny Novgorod, Russia, in 1962. Both his parents were scientists, and he was their only child. He studied Radiophysics at the Nizhny Novgorod State University, where he received his Diploma. He started his research career in 1984 at the Gorky Radiotechnical Institute, where he completed his Ph.D. in 1990. He then joined the Institute of Applied Physics of the Russian Academy of Sciences as a Research Scientist. In the summer of ’94, Alex left Russia and began his journey abroad. After a short period at the École Polytechnique Fédérale de Lausanne, Switzerland, he was awarded a prestigious Alexander von Humboldt Fellowship at the Signal Theory Group of Ruhr University, Bochum, Germany. He stayed in Bochum until 1999, when he moved to Canada as Associate Professor at McMaster University. He was appointed Full Professor at McMaster 3 years later. In the meantime, he received the very prestigious 2001 Wolfgang Paul Award from the Alexander von Humboldt Foundation. In 2003, Alex returned to Germany as Visiting Professor and founder of the highly recognized Smart Antennas Research Team at the University of Duisburg-Essen. In 2005 he joined Darmstadt University of Technology, as Professor and Head of the Communication Systems Group. Alex was a brilliant, ambitious, and highly accomplished researcher – one of the very best in his area, worldwide. Besides the Wolfgang Paul Award “in recognition	academy;array processing;diploma;google summer of code;radar;radiophysics;sonar (symantec);sensor;signal processing;smart antenna;switzerland	Marius Pesavento;Yuri I. Abramovich;Fulvio Gini;Nikos D. Sidiropoulos;Abdelhak M. Zoubir	2013	Signal Processing	10.1016/j.sigpro.2013.07.003	control theory;theoretical computer science;sensor array;mathematics	ML	-48.16360856401944	-8.073202717189428	147592
acc48f09b8b2867378e28aa8481ae8c623f01b16	inist experience in hyper-document building from bibliographic databases		After a presentation of INIST -documentation center dealing with every aspect of documentation- we present the main applications which use or can use techniques of hyperdocument browsing or building. In order to process that wide variety of applications, the Ru0026D Department of INIST is building a toolbox for manipulating scientific u0026 technical information. Then, we present SDOC, the first prototype of this toolbox, as part of the ECC ESPRIT project KWICK, and the first results of this experiment	bibliographic database	Jacques Ducloy;Luc Grivel;Jean-Charles Lamirel;Xavier Polanco;Laurent Schmitt	1991			knowledge representation and reasoning;documentation;toolbox;bibliographic database;database;hypertext;geography	OS	-43.18849251291523	-2.245584287254508	147984
7686a4f5bd619edb2540b0034f7633b3b5dfab52	a flexible framework for defining, representing and detecting changes on the data web		The dynamic nature of Web data gives rise to a multitude of problems related to the identification, computation and management of the evolving versions and the related changes. In this paper, we consider the problem of change recognition in RDF datasets, i.e., the problem of identifying, and when possible give semantics to, the changes that led from one version of an RDF dataset to another. Despite our RDF focus, our approach is sufficiently general to engulf different data models that can be encoded in RDF, such as relational or multi-dimensional. In fact, we propose a flexible, extendible and data-model-independent methodology of defining changes that can capture the peculiarities and needs of different data models and applications, while being formally robust due to the satisfaction of the properties of completeness and unambiguity. Further, we propose an ontology of changes for storing the detected changes that allows automated processing and analysis of changes, cross-snapshot queries (spanning across different versions), as well as queries involving both changes and data. To detect changes and populate said ontology, we propose a customizable detection algorithm, which is applicable to different data models and applications requiring the detection of custom, user-defined changes. Finally, we provide a proof-of-concept application and evaluation of our framework for different data models.	algorithm;computation;data web;data model;extensibility;file spanning;population;sensor;snapshot (computer storage)	Yannis Roussakis;Ioannis Chrysakis;Kostas Stefanidis;Giorgos Flouris;Yannis Stavrakas	2015	CoRR		computer science;data mining;database;world wide web	Web+IR	-37.26115128758677	3.6353788554034008	148410
7a81b7b4d59b1b12f26d285e17c78c03f8b992b3	multimedia semantic annotation propagation	content management;multimedia metadata;vegetation mapping;resource description format;semantic annotation;ontological relations multimedia semantic annotation propagation content management data transformation processes annotated multimedia data sets contextual information;rule based;contextual information;multimedia systems;ontologies artificial intelligence;multimedia annotations;data structures;satellites;multimedia data;multimedia communication;semantic annotations;data transformation;production;ontologies;ontologies production multimedia computing content management acoustic sensors satellites video sharing video recording environmental management proposals;ontologies artificial intelligence content management data structures multimedia systems;scientific research;annotation propagation multimedia metadata multimedia annotations semantic annotations;temperature distribution;annotation propagation	Scientific research is producing and consuming large volumes of multimedia data at an ever growing rate. Annotations to the data helps associating context and enhances content management, making it easier to interpret and share data. However, raw data often needs to go through complex processing steps before it can be consumed. During these transformation processes, original annotations from the production phase are often discarded or ignored, since their usefulness is usually limited to the first transformation step. New annotations must be associated with the final product, a time consuming task often carried out manually. Systematically associating new annotations to the result of each data transformation step is known as annotation propagation. This paper introduces techniques for structuring annotations by applying references to ontologies and automatically transforming these annotations along with data transformation processes. This helps the construction of new annotated multimedia data sets, preserving contextual information. The solution is based on: (i) the notion of semantic annotations; and (ii) a set of transformations rules, based on ontological relations.	ontology (information science);software propagation	Gilberto Zonta Pastorello;Jaudete Daltio;Claudia Bauzer Medeiros	2008	2008 Tenth IEEE International Symposium on Multimedia	10.1109/ISM.2008.77	rule-based system;scientific method;data structure;content management;computer science;ontology;artificial intelligence;database;data transformation;world wide web;information retrieval;satellite	Visualization	-39.045005596204064	2.4631639949338524	148442
343e166d00a16bdbabeada313ee01e19c59bee9a	a proposal for publishing data streams as linked data - a position paper		Streams are appearing more and more often on the Web in sites that distribute and present information in real-time streams. We anticipate a rapidly growing need of mashing up this streaming information with more static one. While best practices for linking static data on the Web were published and facilitate the mash up of static information published on the Web, streams were neglected. In this short position paper, we propose an approach to publish Data Streams as Linked Data.	best practice;linked data;mash-1;real-time web;world wide web	Davide Francesco Barbieri;Emanuele Della Valle	2010				Web+IR	-38.131846834970915	0.8235392110476458	148466
197188bffe1420c74204013504e4a8aa5f8049c9	search by strategy	probabilistic databases;db and ir	This position statement advocates that the integration of information retrieval and databases, a topic that has been studied for many years (see e.g. [3]), is now in a state where the technology is ready to be brought out of the laboratory, and that this technology is especially a good match for the meaningful, semantic annotations that are the topic of this workshop.	database;information retrieval	Arjen P. de Vries;Wouter Alink;Roberto Cornacchia	2010		10.1145/1871962.1871979	computer science;data science;data mining;information retrieval	DB	-44.39908426065793	0.78069813576459	148701
28310c4d7835d1d156b1913db8e0dfbd89704585	the canonicalproducer: an instrument monitoring component of the relational grid monitoring architecture (r-gma)	distributed databases;entity-relationship modelling;grid computing;information management;relational databases;software architecture;virtual enterprises;canonicalproducer;consumer-producer model;european crossgrid project;european datagrid project;global grid forum;grid information and monitoring system;grid information and monitoring systems;instrumentmonitoring;r-gma;relational grid monitoring architecture;santa-g;grid computing;grid environment;information streaming;instrument monitoring component;network monitoring tool;relational database;relational model;virtual organisation;canonicalproducer;grid information and monitoring systems;grid monitoring architecture;grids;instrumentmonitoring;rgma;santa-g	We describe how the R-GMA (Relational Grid Monitoring Architecture) can be used to allow for instrument monitoring in a Grid environment. The R-GMA has been developed within the European DataGrid Project (EDG) as a Grid Information and Monitoring System. It is based on the Grid Monitoring Architecture (GMA) from the Global Grid Forum (GGF), which is a simple Consumer-Producer model. The special strength of this implementation comes from the power of the relational model. It offers a global view of the information as if each Virtual Organisation had one large relational database. It provides a number of different Producer types with different characteristics; for example some support streaming of information. We describe the R-GMA component that allows for instrument monitoring, the CanonicalProducer. We also describe an example use of this approach in the European CrossGrid project, SANTA-G, a network monitoring tool.	egi;intel gma;java servlet;relational database;relational model;sql;select (sql);storage model;table (information);virtual organization	Stuart Kenny;Brian A. Coghlan;David O'Callaghan;John Ryan;Rob Byrom;Laurence Field;Steve Hicks;Manish Soni;Antony J. Wilson;Xiaomei Zhu;Roney Cordenonsi;Ari Datta;Linda Cornwall;Abdeslem Djaoui;Norbert Podhorszki	2004	Third International Symposium on Parallel and Distributed Computing/Third International Workshop on Algorithms, Models and Tools for Parallel Computing on Heterogeneous Networks	10.1109/ISPDC.2004.50	relational database;computer science;data science;data mining;database;information management	HPC	-39.903275740668946	-1.566849979574453	148874
6008ea0ff9406834436c2ab39dca17390f978a23	model-driven design of graph databases		Graph Database Management Systems (GDBMS) are rapidly emerging as an effective and efficient solution to the management of very large data sets in scenarios where data are naturally represented as a graph and data accesses mainly rely on traversing this graph. Currently, the design of graph databases is based on best practices, usually suited only for a specific GDBMS. In this paper, we propose a model-driven, system-independent methodology for the design of graph databases. Starting from a conceptual representation of the domain of interest expressed in the Entity-Relationship model, we propose a strategy for devising a graph database in which the data accesses for answering queries are minimized. Intuitively, this is achieved by aggregating in the same node data that are likely to occur together in query results. Our methodology relies a logical model for graph databases, which makes the approach suitable for different GDBMSs. We also show, with a number of experimental results over different GDBMSs, the effectiveness of the proposed methodology.	attribute–value pair;best practice;data store;entity–relationship model;graph (discrete mathematics);graph database;intermediate representation;model-driven architecture;model-driven engineering;nosql;requirement	Roberto De Virgilio;Antonio Maccioni;Riccardo Torlone	2014		10.1007/978-3-319-12206-9_14	data mining;management system;computer science;logical data model;graph database;data set;graph	DB	-34.88243576455155	2.2065072788462654	148994
2a2602450a1b0b8cb2531311064fe0ff8ded6975	semantic web meets computational intelligence: state of the art and perspectives [review article]	evolutionary computation;query processing;neural nets;computational intelligence;learning capability decentralized complex system computational intelligence methods fuzzy logic web semantics uncertainty evolutionary computations tractability issues semantic data reasoning semantic data mapping semantic data querying artificial neural network;fuzzy logic;semantic web;semantic web evolutionary computation fuzzy logic learning artificial intelligence neural nets query processing;learning artificial intelligence;semantic web computational intelligence evolutionary computation	The Semantic Web, as a decentralized complex system, is akin to be fuzzy and evolutionary. In this article, a comprehensive survey on the applications of variant Computational Intelligence methods to enhance a variety of Semantic Web applications is provided. The survey consists of three aspects: fuzzy logic to deal with vagueness and uncertainty in Web semantics; evolutionary computations to deal with the vastness and tractability issues in storing, querying, reasoning and mapping semantic data; artificial neural network to improve the learning capability of the Semantic Web. Based on the survey of the existing approaches in the literature, some potential future research directions in this area have also been discussed and proposed.	artificial neural network;complex system;computation;computational intelligence;fuzzy logic;semantic web;vagueness;web application	Huajun Chen;Zhaohui Wu;Philippe Cudré-Mauroux	2012	IEEE Computational Intelligence Magazine	10.1109/MCI.2012.2188592	semantic data model;fuzzy logic;natural language processing;semantic computing;web modeling;semantic web rule language;semantic search;semantic grid;computer science;artificial intelligence;neuro-fuzzy;machine learning;computational intelligence;semantic web;social semantic web;data mining;semantic compression;web intelligence;semantic analytics;intelligent control;evolutionary computation	AI	-39.038757667025756	2.0035264149290386	149056
cc4b29fc8084da3d05e21f72d713c89000201ea1	a survey on data collection for machine learning: a big data - ai integration perspective		Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning where feature engineering is the bottleneck, deep learning techniques automatically generate features, but instead require large amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.	artificial intelligence;big data;computer vision;data acquisition;deep learning;feature engineering;machine learning;natural language	Yuji Roh;Geon Heo;Steven Euijong Whang	2018	CoRR			ML	-36.72814057196487	-7.8962731138990145	149198
ca4ab5b1302521a35c9efa63f7432eacba4044a6	cape: extending clips for the internet	knowledge based system;programming environment;rule based system;rule based programming;regular expression	Abstract   This paper describes  Cape , a programming environment combining  Clips  And Perl with Extensions.  Clips  is an efficient and expressive forward-chaining rule-based system with a flexible object system. Perl is a popular procedural language with extremely powerful regular expression matching facilities, and a huge library of freely available software.  Cape  closely integrates these languages, and provides extensions to facilitate building systems with an intimate mixture of the two. The paper describes the facilities  Cape  offers programmers and the demonstration systems and “component applications” distributed with it. The use of the system is then discussed with reference to  dime  (Distributed Information Manipulation Environment), a toolkit being developed to support identifying and coordinating the use of external knowledge sources. Finally, planned developments of the system are indicated.		Robert Inder	2000	Knowl.-Based Syst.	10.1016/S0950-7051(00)00056-3	rule-based system;simulation;computer science;artificial intelligence;knowledge-based systems;machine learning;regular expression;computer graphics (images)	Metrics	-43.4090582343601	-2.334208766026355	149200
5aeec9e12c5f057b93a094da00536d420cf61400	discovering knowledge in data using formal concept analysis	formal concept analysis fca;formal context;formal concepts;visualization;concept lattice;guided automation;business intelligence;formal concept analysis;knowledge discovery	Formal Concept Analysis FCA has been successfully applied to data in a number of problem domains. However, its use has tended to be on an ad hoc, bespoke basis, relying on FCA experts working closely with domain experts and requiring the production of specialised FCA software for the data analysis. The availability of generalised tools and techniques, that might allow FCA to be applied to data more widely, is limited. Two important issues provide barriers: raw data is not normally in a form suitable for FCA and requires undergoing a process of transformation to make it suitable, and even when converted into a suitable form for FCA, real data sets tend to produce a large number of results that can be difficult to manage and interpret. This article describes how some open-source tools and techniques have been developed and used to address these issues and make FCA more widely available and applicable. Three examples of real data sets, and real problems related to them, are used to illustrate the application of the tools and techniques and demonstrate how FCA can be used as a semantic technology to discover knowledge. Furthermore, it is shown how these tools and techniques enable FCA to deliver a visual and intuitive means of mining large data sets for association and implication rules that complements the semantic analysis. In fact, it transpires that FCA reveals hidden meaning in data that can then be examined in more detail using an FCA approach to traditional data mining methods.	formal concept analysis	Simon C Andrews;Constantinos Orphanides	2013	IJDST	10.4018/jdst.2013040103	visualization;computer science;formal concept analysis;knowledge management;data mining;database;business intelligence	AI	-36.64100676996301	-0.02264043499847113	149393
4d87c6f257f63a526b08e3d400f8904766a79522	document-oriented views of guideline knowledge bases	information content;knowledge base	A computer-interpretable guideline knowledge base can be a very large network whose information content is difficult for developers and clinicians to comprehend and review. We created a method to annotate a guideline model and use the annotations to export the guideline knowledge base in an XML format that can be transformed into a readable document. We applied this method to knowledge bases developed in three different guideline modeling projects to analyze uses and limitations of this approach. We demonstrate the promise of creating such document-oriented views, but conclude that guideline models and knowledge bases should be constructed with the goal of creating such human-comprehensible views from the beginning	document-oriented database;human-readable medium;knowledge base;self-information;xml	Samson W. Tu;Shantha Condamoor;Tim Mather;Richard W. Hall;Neill Jones;Mark A. Musen	2007		10.1007/978-3-540-73599-1_57	knowledge base;self-information;computer science;knowledge management;artificial intelligence;data mining;database	Web+IR	-40.09971796024188	3.4735886637664843	149412
2c14566517dcd5e1abe37024c0c0c64507646175	an intelligent design framework proposal leveraging axiomatic design and the semantic web	axiomatic design;optical backplane engineering;semantic web;artificial intelligence;kbe	A new Intelligent Design Framework (IDF) approach that leverages Knowledge-Based Engineering (KBE) techniques for Optical Backplane engineering is proposed leveraging axiomatic design principles and Semantic Web technology. The framework enables the development of KBE systems for improving engineering design and processes utilizing intelligent systems coupled to Web Services. As a proof-of-concept, a prototype KBE system using HP Labs Jena 2 will be built to demonstrate the IDF approach for optical backplane engineering.	axiomatic design;semantic web	Urcun John Tanik;Gary J. Grimes;Varadraj P. Gurupur;Charles J. Sherman	2005	Transactions of the SDPS		computer science;systems engineering;knowledge management;database	Mobile	-47.85561832034012	3.8324648457541954	149644
d3a083b831d62cd86615b781c113e488c377c7b3	can data integration quality be enhanced on multi-cloud using sla?	multi cloud environment;systematic mapping;service level agreement;data integration	This paper identifies trends and open issues regarding the use#R##N#of SLA in data integration solutions on multi-cloud environments. Therefore it presents results of a Systematic Mapping [3] that analyzes the way SLA, data integration and multi-cloud environments are correlated in existing works. The main result is a classification scheme consisting of facets and dimensions namely (i) data integration environment (cloud; data warehouse; federated database; multi-cloud); (ii) data integration description (knowledge; metadata; schema); and (iii) data quality (confidentiality; privacy; security; SLA; data protection; data provenance). The proposed classification scheme is used to organize a collection of representative papers and discuss the numerical analysis about research trends in the domain.	cloud computing;service-level agreement	Daniel A. S. Carvalho;Plácido A. Souza Neto;Genoveva Vargas-Solar;Nadia Bennani;Chirine Ghedira	2015		10.1007/978-3-319-22852-5_13	computer science;data integration;data warehouse;data mining;database;ontology-based data integration;world wide web;data mapping	DB	-42.35840487642633	-0.19817506786489347	149867
bfc207eba2442d3c9fb8b79e3de9c110d5ed7a5b	contribution to the classification of web of data based on formal concept analysis	triadic con cept analysis;linked open data;pattern structures;formal concept analysis	During the last decade, the web has taken a huge importance in everyday life, and has become what is commonly called a web of data. The available resources can be used by human agents but also by software agents, as it is the case for very large ontologies such as YAGO or resources such as DBpedia. These particular datasets can be linked together for constituting the Linked Open Data (LOD) cloud, where basic data are expressed as (subject, predicate, object) triples. One issue of main interest is knowledge discovery within LOD, which can help information retrieval and knowledge engineering. Formal concept analysis (FCA), which is a mathematical theory allowing classification and data analysis, was already used to classify LOD elements. In this research work, we are interested in analyzing the different approaches (extensions) based on FCA for knowledge discovery in the web of data. One objective is to study the efficiency and the applicability of the existing approaches and to propose some improvements.	dbpedia;experiment;formal concept analysis;information retrieval;knowledge engineering;linked data;ontology (information science);resource description framework;semantic web;simulation;software agent;world wide web;yago	Justine Reynaud;Yannick Toussaint;Amedeo Napoli	2016			computer science;data science;data mining;database	Web+IR	-40.09118519314773	2.759784770210493	150170
3149327e44319826f80f62412ffa586a90ba263f	geospatial cyberinfrastructure and geoprocessing web - a review of commonalities and differences of e-science approaches	e science;geoprocessing web;geospatial cyberinfrastructure	Online geoprocessing gains momentum through increased online data repositories, web service infrastructures, online modeling capabilities and the required online computational resources. Advantages of online geoprocessing include reuse of data and services, extended collaboration possibilities among scientists, and efficiency thanks to distributed computing facilities. In the field of Geographic Information Science (GIScience), two recent approaches exist that have the goal of supporting science in online environments: the geospatial cyberinfrastructure and the geoprocessing web. Due to its historical development, the geospatial cyberinfrastructure has strengths related to the technologies required for data storage and processing. The geoprocessing web focuses on providing components for model development and sharing. These components shall allow expert users to develop, execute and document geoprocessing workflows in online environments. Despite this difference in the emphasis of the two approaches, the objectives, concepts and technologies they use overlap. This paper provides a review of the definitions and representative implementations of the two approaches. The provided overview clarifies which aspects of e-Science are highlighted in approaches differentiated in the geographic information domain. The discussion of the two approaches leads to the conclusion that synergies in research on e-Science environments shall be extended. Full-fledged e-Science environments will require the integration of approaches with different strengths.	computational resource;computer data storage;cyberinfrastructure;distributed computing;e-science;geographic information science;geoprocessing;google code-in;real-time clock;requirement;synergy;web service;wrapping (graphics)	Barbara Hofer	2013	ISPRS Int. J. Geo-Information	10.3390/ijgi2030749	computer science;data science;data mining;world wide web	Web+IR	-44.545239437712	-0.004945315677798107	150408
1f44285bdafe0fe1624fdfc23fed878487b5a620	development of a sliding-leg tripod as an add-on device for manufacturing	sliding leg tripod;system analysis;manufacturing industry;programmable device	Robotica / Volume 19 / Issue 03 / May 2001, pp 285 294 DOI: 10.1017/S0263574700002964, Published online: 25 April 2001 Link to this article: http://journals.cambridge.org/abstract_S0263574700002964 How to cite this article: Fengfeng Xi, Wanzhi Han, Marcel Verner and Andrew Ross (2001). Development of a sliding-leg tripod as an add-on device for manufacturing. Robotica, 19, pp 285-294 doi:10.1017/S0263574700002964 Request Permissions : Click here	add-ons for firefox;han unification;here document;tripod	Fengfeng Xi;Wanzhi Han;Marcel Verner;Andrew Ross	2001	Robotica	10.1017/S0263574700002964	control engineering;simulation;engineering;system analysis;manufacturing	Robotics	-46.54134354529922	-6.0956795425251	150502
32e90b320bb699ec961483f815009929cdfc3585	the geoviz toolkit: using component-oriented coordination methods for geographic visualization and analysis	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;software engineering;research articles;abstracts;open access;software component;life sciences;clinical guidelines;on the fly;spatial analysis;software design pattern;full text;software design;geographic visualization;rest apis;orcids;europe pmc;biomedical research;coordination;open source;bioinformatics;literature search	In this paper we present the GeoViz Toolkit, an open-source, internet-delivered program for geographic visualization and analysis that features a diverse set of software components which can be flexibly combined by users who do not have programming expertise. The design and architecture of the GeoViz Toolkit allows us to address three key research challenges in geovisualization: allowing end users to create their own geovisualization and analysis component set on-the-fly, integrating geovisualization methods with spatial analysis methods, and making geovisualization applications sharable between users. Each of these tasks necessitates a robust yet flexible approach to inter-tool coordination. The coordination strategy we developed for the GeoViz Toolkit, called Introspective Observer Coordination, leverages and combines key advances in software engineering from the last decade: automatic introspection of objects, software design patterns, and reflective invocation of methods.	component-based software engineering;geovisualization;imagery;introspection;open-source software;physical object;software design pattern;spatial analysis	Frank Hardisty;Anthony C. Robinson	2011	International journal of geographical information science : IJGIS	10.1080/13658810903214203	software design pattern;text mining;computer science;data science;software design;component-based software engineering;machine learning;data mining;database;spatial analysis;world wide web	SE	-40.46278627416297	0.049612351029690076	150574
798a30580a63d102e2863eeda07d48518a253c03	subgraph isomorphism search in massive graph data. (isomorphisme de sous-graphes dans les graphes de données massifs)			subgraph isomorphism problem	Chemseddine Nabti	2017				DB	-39.73403949738432	-9.45073665971138	150935
30318d97303072f17a2ccb1b7559942e69104518	writing, designing, and processing information in the darwin information typing architecture (dita)	microsoft word;darwin information typing architecture;robohelp;online help;adobe acrobat;pdf;single source	The Darwin Information Typing Architecture is an XML architecture for producing and reusing technical information. This tutorial covers the basics of topic-oriented writing, and then proceeds to the specifics of topic structures in DITA, and the DITA information types.After covering how to create information in DITA, the tutorial will cover how to use the architecture to create specialized topic types and transforms.The tutorial assumes you have some familiarity with topic-oriented writing and information types. Familiarity with XML DTD syntax, and with XSLT, will be helpful but not essential.Bring laptops with an XML editor, parser, XSLT interpreter, and the DITA package installed. At minimum, have a simple text editor, plus the DITA package, plus Apache's Xalan and Xerces installed.You can get the DITA package from its developerWorks home: http://www.ibm.com/developerworks/xml/library/x-dita1	apache xerces;darwin information typing architecture;laptop;simpletext;text editor;xml editor;xslt	Michael Priestley	2001		10.1145/501516.501563	single source publishing;human–computer interaction;computer science;operating system;software engineering;database;programming language;management;world wide web	NLP	-44.42111933208128	-2.9721460468120644	150937
8658a09e3ce12bf3b765a28cf2bc3639cd8b430e	an intelligent system for the acquisition and management of information from bill of quantities in building projects	bill of quantities;project management;intelligent system;construction	An intelligent system for bill of quantities information management is proposed.The system is implemented using a web-based application.The system imports and classifies information with a high percentage of success.The integration of bill of quantities data permits to make more reliable decisions. Construction projects success largely depends on a good access to and management of information, which is stored in different databases and applications (even on paper). This heterogeneity makes the management and decision making process difficult. One of the most relevant documents in the project management context is the Bill of Quantities (BoQ), which is a collection of work descriptions specifying, in a textual way, the nature of the different tasks needed to be done in order to achieve the project goal. Since there are no standards for developing the BoQ, its structure and linguistics vary among practitioners, making the acquisition and management of the information contained in this document very laborious. In this paper, i-BoQ, an intelligent system for retrieving and structuring data coming from BoQ, is presented. As we will see, intelligent capabilities together with a friendly web-based application allow to easily acquire and manage integrated BoQ information for supporting decision making in the construction project management.	artificial intelligence	María Martínez-Rojas;Nicolás Marín;M. Amparo Vila	2016	Expert Syst. Appl.	10.1016/j.eswa.2016.07.011	project management;construction;knowledge management;artificial intelligence	HCI	-33.987571264748226	-7.737407358579171	151035
50a0dda2e6d1785145a83ebf982a6a9fbdf60eac	improvement on the flexible tree-based key management framework	key management;multiple content distribution systems;low power;content distribution;tree structure;access control;user hierarchy	Department of Management Information System, National Chung Hsing University, 250 Kuo Kuang Road, Taichung 402, Taiwan, ROC Department of Information Management, National Taichung Institute of Technology, 129 Sec. 3, San-min Rd., Taichung 404, Taiwan, ROC Department of Computer Science, National Chung Hsing University, 250 Kuo Kuang Road, Taichung 402, Taiwan, ROC Graduate Institute of Networking and Communication Engineering, Chao Yang University of Technology, 168 Gifeng E. Rd., Wufeng, Taichung County 413, Taiwan, ROC	adversary (cryptography);chao (sonic);computation;computer science;information management;key management;management information system;multistage interconnection networks;tree (data structure);yang	Min-Shiang Hwang;Jung-Wen Lo;Chia-Hsin Liu	2005	Computers & Security	10.1016/j.cose.2004.09.009	computer science;access control;key management;tree structure;world wide web;computer security;computer network	DB	-44.703900207682125	-7.759215439453145	151081
5fc4a62e34f04ab752b1b3755136d4115ff88563	an efficient query mechanism of data provenance based on dependency view	query processing big data data analysis;collaboration;data mining;runtime;forward provenance tracking query mechanism data provenance dependency view data analysis auditing debugging trust evaluation quality evaluation access control opm global data provenance description correlation information backward provenance tracking;big data;distributed databases;dependency view provenance query big data;collaboration big data context handheld computers runtime distributed databases data mining;context;handheld computers	Data provenance is useful for data analysis, including auditing, debugging, evaluating the trust and quality, access control and so on. For the scale of data keep increasing, data provenance also become large and constantly growing, and it brings challenges to the efficiency of provenance tracking which is the important base of data analysis. This paper proposes a kind of dependency view based on OPM to extract a global data provenance description for data process instance with more correlation information among the elements of data provenance, and then provides an efficient query mechanism based on dependency view of data provenance to support provenance tracking by constructing a set of query operations for both forward and backward provenance tracking.	access control;big data;data-intensive computing;debugging;query language;requirement;uncertain data	Xin Gao;Xuan Sun	2014	2014 14th International Symposium on Communications and Information Technologies (ISCIT)	10.1109/ISCIT.2014.7011914	computer science;data mining;database;world wide web	DB	-35.490337541574	2.4602416977175494	151297
7a990fa38cb5d7ed3d6decb90db9fd016cff4f82	agentcities: lessons and future perspectives for large-scale agent deployment		"""Agent technology has long been proposed as an important technology for development of applications for open environments such as the public Internet. New technologies such as Web Services, Semantic Web, GRID computing, e-Business systems and Agent standards might now also provide the means to begin applying agents widely in such environments. Beyond individual technologies however there lie huge challenges in developing, deploying and managing such systems. This talk will summarise some of the experiences from the Agentcities projects one of the largest """"agent"""" testbed deployments to date and including a wide range of demonstration applications. Specifically the talk will cover the lessons learned in establishing the network, views on the technologies used (and how they might evolve) and a view on Research and development challenges for the future. Steven Willmott is currently a Research Associate at the Artificial Intelligence section of the Departament de Llenguatges i Sistemes Informatics Universitat Politecnica de Catalunya. His main area of research is Distributed Artificial Intelligence with a particular focus on coordination and communication problems in large-scale open systems. Steven is also the technical coordinator of the EU 5th Framework IST funded projects Agentcities.RTD and Agentcities.NET. http://lsi.upc.es/~webia/KEMLG/ http://www.agentcities.org/"""	distributed artificial intelligence;electronic business;grid computing;informatics;internet;semantic web;software deployment;steven anson coons;testbed;web service	Steven Willmott	2003			software deployment;environmental science;environmental resource management;systems engineering	AI	-47.39418151046603	1.3527717047817212	151347
9020778d758ce4dd2b9873debbf16d468f8451e2	macsyma - the fifth year	multiple representation;power series;computational algorithms;polynomial greatest common divisor gcd;univariate polynomials;modular arithmetic;hensel constructions;rational function	MACSYMA is a large symbolic and algebraic manipulation system which has been under development at Project MAC, M.I.T. since 1969. We first discuss some of the design decisions, such as multiple representations, that led to the current system. We then give a brief description of the facilities of MACSYMA by dividing the system into seven packages: language and interactive facilities, general representation, rational function representation and related algorithms, the integration subsystem, the power series subsystem, the MACLISP system, and miscellaneous facilities.	algorithm;function representation;mit computer science and artificial intelligence laboratory;maclisp;macsyma	Joel Moses	1974	ACM SIGSAM Bulletin	10.1145/1086837.1086857	rational function;modular arithmetic;discrete mathematics;computer science;pure mathematics;mathematics;power series;algebra	Graphics	-47.28871957855062	-4.303216471075846	151477
76b74e5945df7ed53c9e3131e02bdc677e75b05e	role of thesauri in the information management in the web-based services and systems	semanticweb;thesauri;geospatial information;web services	Information sharing, exchanging and archiving is the backbone of any organized activity, regardless if it is performed in the sphere of business, home or administration. Semantic Web technologies allow controlling the growth and structure of information, and provide search and inference methods. The article touches one of the main problems in the information-based societies. It refers to the problem of information management and sharing in the heterogonous, complex systems, exposing their functions through the web services.#R##N##R##N#The main axis of the paper is the implementation of remotely accessible knowledge bases in form of thesauri - dynamic, centrally coordinated dictionaries. Thesauri may contain terms and concepts with an indication of their semantic relationships. They can serve as sources of concept definitions used by various registries, may provide additional information to the search engines, and may support multilingual representation. The article provides some examples of such use cases, targeting special concerns on applications of thesauri in the geospatial domain. The aim of this article was also to show practical aspects of thesauri implementation. The article shows the way of applying officially published standards as guidelines in building interoperable thesauri in form of web service. The implementation of such service, involving the use of SKOS specification as a core information model of the thesauri, and SOAP and REST technologies as a base for communication implementation, is presented. The backend of the service is built on SESAME repository supporting SeRQL and RQL query languages. The examples of implemented clients of the service are: Internet enabled desktop application and web control that can be inserted into any web page.		Tomasz Kubik	2011	Trans. Computational Collective Intelligence	10.1007/978-3-642-19968-4_2	computer science;data mining;world wide web;information retrieval	AI	-42.04853710085458	3.4382718084349198	151609
bd71cb3f8f4df81d73f99a2d962e516664d59654	noncommutative lightweight signcryption for wireless sensor networks		1 Information Security Center, State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China 2 School of Computer Science, Communication University of China, Beijing 100024, China 3 School of Computer Science and Engineering, The University of Aizu, Aizu Wakamatsu 965-8580, Japan 4Department of Information and Electronic Engineering, Muroran Institute of Technology, Muroran 050-8585, Japan	computer science;electronic engineering;information security;mos technology sid;signcryption;telephone exchange	Lize Gu;Yun Pan;Mianxiong Dong;Kaoru Ota	2013	IJDSN	10.1155/2013/818917	signcryption;internet privacy;computer security;computer network	Theory	-44.788235056472026	-7.7901161596690045	151877
3fad69e4e6a68f59b769042340c582e3d59d1f0b	ontology-based data integration in epnet: production and distribution of food during the roman empire	ontology based data integration;epnet;knowledge representation and reasoning;ontop;e culture;ontology based data access	Semantic technologies are rapidly changing the historical research. Over the last decades, an immense amount of new quantifiable data have been accumulated, and made available in interchangeable formats, in social sciences and humanities, opening up new possibilities for solving old questions and posing new ones. This paper introduces a framework that eases the access of scholars to historical and cultural data about food production and commercial trade system during the Roman Empire, distributed across different data sources. The proposed approach relies on the Ontology-Based Data Access (OBDA) paradigm, where the different datasets are virtually integrated by a conceptual layer (an ontology) that provides to the user a clear point of access and a unified and unambiguous conceptual view. & 2016 Published by Elsevier Ltd. 2 http://www.europeana.eu. 3 http://www.kulttuurisampo.fi. 4 http://www.cs.vu.nl/STITCH. 5 http://e-culture.multimedian.nl. 6 http://chip.win.tue.nl. 7 http://http://www.eagle-network.eu. 8 http://www.cidoc-crm.org.	big data;computer science;conceptual schema;data access;data point;data visualization;database;epidoc;knowledge representation and reasoning;linked data;ontology-based data integration;programming paradigm;solid modeling;table (information);user interface	Diego Calvanese;Pietro Liuzzo;Alessandro Mosca;José Remesal;Martín Rezk;Guillem Rull	2016	Eng. Appl. of AI	10.1016/j.engappai.2016.01.005	knowledge representation and reasoning;computer science;knowledge management;artificial intelligence;data mining;database;ontology-based data integration	DB	-39.11381046152526	4.054662518673482	152042
ea69e2e1f26ee881b63bdf1a4d613e746e61244f	study on record linkage of anonymizied data			linkage (software)	Hiroaki Kikuchi;Takayasu Yamaguchi;Koki Hamada;Yuji Yamaoka;Hidenobu Oguri;Jun Sakuma	2018	IEICE Transactions		theoretical computer science;record linkage;mathematics;data science;big data;information privacy	DB	-39.35280874742119	-5.811502690113583	152100
debb38d92d2aa1307c8b624470e2f7f0e747bc03	making sense of scientific information on worldwideweb: www-ted	thesauri;internet;indexing;scientific information systems internet indexing thesauri;internet html thesauri databases libraries world wide web quality management guidelines search engines information retrieval;scientific research;scientific information systems;www links scientific information world wide web html document management tool quality search capability www ted thesaurus tool medium sized collections html pages document databases;document management	This article considers the demand for a HTML document management tool which provides high quality search capability for scientific research. It proposes guidelines for the implementation of WWW-TED, an evolving thesaurus tool for medium-sized collections of HTML pages.	www	Marcia J. Bossy	1998		10.1109/ADL.1998.670424	computer science;database;world wide web;information retrieval	HPC	-42.01097734179097	2.550689866419217	152101
dffc2be709a602630cb22e141e2087b8c503a260	towards interactive analytics and visualization on one billion tweets	spatial temporal text data;visualization;parallel database;social networks;data analytics;apache asterixdb	"""We present a system called """"Cloudberry"""" that allows users to interactively query, analyze, and visualize large amounts of data with temporal, spatial, and textual dimensions. As a general-purpose full-stack solution, it has a friendly UI, intelligent middleware, and a powerful big data management backend running Apache AsterixDB to enable big data analytics and visualization. We will demonstrate the system using Twitter data on a computer cluster."""	big data;computer cluster;general-purpose markup language;interactivity;middleware;user interface	Jianfeng Jia;Chen Li;Xi Zhang;Chen Li;Michael J. Carey;Simon Su	2016		10.1145/2996913.2996923	analytics;visual analytics;visualization;computer science;data science;database;data analysis;world wide web;social network	Visualization	-35.79216772831753	-5.202295120169484	152136
c88369e28f460ee0261801822ba1c34ec696e5bc	atmospheric environmental information - an overview with canadian examples	capmon;natchem;data management;air quality data;research data management and quality control rdmq system;naps;quality control;meteorology;air quality	The study of meteorology has been revolutionized by modern computing. In this paper, an overview of the handling of meteorological, climatological and air quality data is provided. Some suggestions are given as to a framework for designing systems to handle atmospheric environmental data. Several examples of Canadian systems that are in use now are presented, some of which illustrate application of the framework.		A. C. McMillan;D. MacIver;W. B. Sukloff	2000	Environmental Modelling and Software	10.1016/S1364-8152(00)00010-4	meteorology;quality control;air quality index;nap;data management;data mining;operations research	AI	-39.15247117868352	-3.030764926043191	152147
b07b0c63912e0f9d7d02a52e369c0491c150e592	survey of biodata analysis from a data mining perspective		Recent progress in biology, medical science, bioinformatics, and biotechnology has led to the accumulation of tremendous amounts of biodata that demands in-depth analysis. On the other hand, recent progress in data mining research has led to the development of numerous efficient and scalable methods for mining interesting patterns in large databases. The question becomes how to bridge the two fields, data mining and bioinformatics, for successful mining of biological data. In this chapter, we present an overview of the data mining methods that help biodata analysis. Moreover, we outline some research problems that may motivate the further development of data mining tools for the analysis of various kinds of biological data.	bioinformatics;data mining;database;scalability;tree accumulation	Peter Bajcsy;Jiawei Han;Lei Liu;Jiong Yang	2005			biological data;computer science;bioinformatics;data mining	ML	-37.061101261915084	-7.501942310963623	152179
f071ea0be5b308cbaf071284c03542e512726e5e	operational support in fish farming through case-based reasoning	decision support system;case based reasoning;intelligent fish farming	Farmed fish is the third biggest export in Norway (around NOK 30 billion/e3.82 billion/US$ 5.44 billion in 2010), and large fish farms have biomass worth around NOK 150 million/e19.38 million/US$ 26.72 million. Several processes are automated (e.g. the feeding system), and sensory logging systems are becoming ubiquitous. Still, the key to successful management of a site is the operational knowledge possessed by the fish farmers. In most cases, this information is not stored formally. To capture, store and reuse this knowledge in a more systematic way is called for. We present a system that employs case-based reasoning (CBR) for such knowledge management, combined with sensor data and numerical models. The CBR system will ultimately be the core part of a decision support for regional managers surveying fish farming sites. Data is acquired from multiple fish farms, spanning several years. We present recent results in testing how well the CBR system finds similar cases. An important part of this test is the evaluation of three different methods for case retrieval (kNN, linear programming for setting feature weights, Echo State Network).	case-based reasoning;computer simulation;decision support system;echo state network;file spanning;knowledge management;linear programming	Axel Tidemann;Finn Olav Bjørnson;Agnar Aamodt	2012		10.1007/978-3-642-31087-4_12	case-based reasoning;simulation;decision support system;computer science;artificial intelligence;data mining	AI	-39.72025240316786	-3.4199766552977193	152291
9f2f30df6ebb51558de0245ac026463f630ae534	data processing aspects of the integrated circuit and magnetic stripe cards	tarjeta magnetica;tratamiento datos;carte magnetique;integrated circuit;electronic fund transfer;data processing;traitement donnee;circuito integrado;transferencia computarizada de fondos;criptografia;cryptography;security key;cryptographie;procesador;magnetic card;processeur;cle securite;processor;monetique;circuit integre;llave seguridad	M.A. Reid gained his BSc. degree in physics from the University of Glasgow, UK, and his Ph.D. in electrical engineering from the University of Edinburgh, UK He became involved in computing and programming through his work in geophysics, subsequently moving towards electronic engineering specifically the device and fabrication of microelectronics. He is currently with the Department of Electrical Engineering at the Chisholm Institute of Technology, Australia. and is doing postgraduate : research work at Monash University, Australia. His interests are in the areas of data comnmnications and microelectronics.	authorization;electrical connection;electrical engineering;electronic engineering;encryption;forward compatibility;integrated circuit;magnetic storage;magnetic stripe card;requirement;smart card;subscriber identity module	M. S. Madan;M. A. Reid	1992	Information & Management	10.1016/0378-7206(92)90005-Z	data processing;telecommunications;computer science;engineering;cryptography;electrical engineering;integrated circuit;computer security	DB	-46.80902344930971	-6.3467469228337	152519
5a3b569620afbeb86b81c40dae6b70d5935291c7	task achieving agents on the world wide web in spinning the semantic web	semantic web;world wide web	"""An important class of problems is related to performing activities, and the planning of future activity, The """"doing of things"""" is at the heart of human endeavour. The WWW has primarily concentrated to date on information storage and retrieval, and the data models and standards mostly relate to such things. More emphasis should now be placed on modelling activity and the collaboration between human and system agents that can be conducted through the WWW. The planning and process modelling communities have started to develop shared models and ontologies to represent activities, tasks, agent capabilities, constraints, etc. These might form the generic core of a shared ontology to support the movement of information about activities over the WWW. constraint model of activity and the more general <I-N-CA> constraint model of synthesised artefacts are described. These could provide a robust conceptual model to underlie future web standards for describing task achieving agents on the web and their behaviours. The World Wide Web currently acts as a vast electronic library, serving information and providing search facilities for accessing that information. However, given that the Web actually consists of a vast network of task achieving agents (humans and computers), this view of the Web as a static pool of information is only using a small fraction of its real capabilities. The idea of the Web being a place where you can ask agents to do things and to plan activities seems intuitively attractive. However, the data models and standards developed to date for the Web mostly relate to information retrieval, rather than activity and the planning of future activity. In order to make the Web a place for """"doing things"""" as well as """"finding things"""", we need shared models and ontologies to represent the entities involved in planning and doing: activities, tasks, plans, agent capabilities, and so on. The AI planning and the process modelling communities have recently started to develop standards in these areas, for the purpose of working on common models and sharing information about activities and processes (Tate, 1998). These common models and ontologies might form the generic core of a shared ontology to support the movement of information and data relating to activities over the World Wide Web. This paper is in two parts. In the first part, we describe work towards the creation of a common ontology and representation for plans, processes and other information related to activity. We …"""	authorization;automated planning and scheduling;computer data storage;data model;digital library;endeavour (supercomputer);entity;information retrieval;ontology (information science);process modeling;semantic web;www;web application;web standards;world wide web	Austin Tate;Jeff Dalton;John Levine;Alex Nixon	2003			web service;web mining;web development;web modeling;data web;web mapping;web-based simulation;web accessibility initiative;web standards;semantic web;web navigation;social semantic web;web page;semantic web stack;database;web intelligence;web 2.0;world wide web;information retrieval;web server	AI	-38.22088624488503	4.179043178938964	152634
8186790621847bac7be3a4098f5de321a62afef1	sharing data on the aquileia heritage: proposals for a research project		Basic ideas are presented of a multi-national research project to share data about the Roman city of Aquileia employing the Information and Communication Technologies (ICT). A Consortium is proposed to manage the project, adopting the Open-source approach to the software design. The Consortium proposes a common vision of the data which we detail in the paper. There results a shared vocabulary of terms and meanings, as well as standard metadata formats to encode, classify and exchange data from whatever source. A federated system of computer resources realizes and supports the project. We also discuss the results to be realistically expected in short time of a low-cost, joint research effort.	consortium;encode;software design;vocabulary	Vito Roberto;Paolo Omero	2011			encode;metadata;software design;knowledge management;vocabulary;political science;federated architecture;information and communications technology	SE	-43.91018643932742	2.525297316442019	152804
795517d41e2e0ff146925cd0c5ecc84703aa41d7	geospatial tools address emerging issues in spatial ecology: a review and commentary on the special issue	species distribution model;temporal processes;spatial ecology;processes;naturgeografi;fysisk geografi;gis;remote sensing;species distribution models;temporal;temporal processing;environmental change;biodiversity	Spatial ecology focuses on the role of space and time in ecological processes and events from a local to a global scale and is particularly relevant in developing environmental policy and (mandated) monitoring goals. In other words, spatial ecology is where geography and ecology intersect, and high-quality geospatial data and analysis tools are required to address emerging issues in spatial ecology. In this commentary and review for the International Journal of GIS Special Issue on Spatial Ecology, we highlight selected current research priorities in spatial ecology and describe geospatial data and methods for addressing these tasks. Geoinformation research themes are identified in population ecology, community and landscape ecology, and ecosystem ecology, and these themes are further linked to the assessment of ecosystem services. Methods in spatial ecology benefit from explicit consideration of spatial autocorrelation, and applications discussed in this review include species distribution modeling, remote sensing of community and ecosystem properties, and models of climate change. The linkages of the Special Issue papers to these emerging issues are described.	ecology	Andrew K. Skidmore;Janet Franklin;Terry P. Dawson;Petter Pilesjö	2011	International Journal of Geographical Information Science	10.1080/13658816.2011.554296	biodiversity;applied ecology;environmental change;geography;spatial ecology;systems ecology;geospatial analysis;ecology;information ecology	HPC	-41.95751322232489	-7.968927024589978	152982
964bd9fc75be18096f98f56c9aca98e34beded98	improving data workflow systems with cloud services and use of open data for bioinformatics research		Data workflow systems (DWFSs) enable bioinformatics researchers to combine components for data access and data analytics, and to share the final data analytics approach with their collaborators. Increasingly, such systems have to cope with large-scale data, such as full genomes (about 200 GB each), public fact repositories (about 100 TB of data) and 3D imaging data at even larger scales. As moving the data becomes cumbersome, the DWFS needs to embed its processes into a cloud infrastructure, where the data are already hosted. As the standardized public data play an increasingly important role, the DWFS needs to comply with Semantic Web technologies. This advancement to DWFS would reduce overhead costs and accelerate the progress in bioinformatics research based on large-scale data and public resources, as researchers would require less specialized IT knowledge for the implementation. Furthermore, the high data growth rates in bioinformatics research drive the demand for parallel and distributed computing, which then imposes a need for scalability and high-throughput capabilities onto the DWFS. As a result, requirements for data sharing and access to public knowledge bases suggest that compliance of the DWFS with Semantic Web standards is necessary. In this article, we will analyze the existing DWFS with regard to their capabilities toward public open data use as well as large-scale computational and human interface requirements. We untangle the parameters for selecting a preferable solution for bioinformatics research with particular consideration to using cloud services and Semantic Web technologies. Our analysis leads to research guidelines and recommendations toward the development of future DWFS for the bioinformatics research community.	3d reconstruction;base;bioinformatics;cloud computing;computation (action);data access;distributed computing;embedding;genome;high-throughput computing;information privacy;knowledge bases;large;overhead (computing);repository;requirement;scalability;semantic web;terabyte;throughput;user interface;web standards;standards characteristics	Md. Rezaul Karim;Audrey M. Michel;Achille Zappa;Pavel V. Baranov;Ratnesh Sahay;Dietrich Rebholz-Schuhmann	2018		10.1093/bib/bbx039	open data;data mining;cloud computing;linked data;semantic web;bioinformatics;dna sequencing;workflow;biology	HPC	-34.92887574445002	-2.273593275437384	153240
6a69b22ce12fae8a0583ef12eb8066bec1eb8636	visualizing research digital libraries with open standards	acm/ieee cc2001;mis classification;arxiv;digital library visualization;xml;inspec;oai- pmh;xml topic maps;acm ccs;metadata;classification system;management information system;digital library;open standard;visual system	Large scale research Digital Libraries (DLs) have a large array of potentially useful metadata. Yet, many popular DLs do not provide a convenient way to navigate the metadata or to visualize classification schema in the user session. For example, in the broad world of Management Information Systems (MIS) research, a high-level overview of MIS topics and their interrelationships would be useful to navigate a MIS DL before zooming in on a specific article. To address this obstacle, this paper describes a prototype, the Technical Report Visualizer System (TRV), which uses a wide variety of open standards to expose DL classification metadata in the navigation interface. The system captures MIS article metadata from the Open Archives Initiative (OAI) compliant arXiv e-Print archive at Cornell University. The OAI Protocol for Metadata Harvesting (OAI-PMH) is used to collect the topic metadata; the articles’ Association for Computing Machinery’s (ACM) Computing Classification System codes. We display the topic metadata in a Java hyperbolic tree and make use of XML conceptual product and implementation product standards and specifications, such as the Dublin Core and BiblioML bibliographic metadata sets, XML Topic Maps, Xalan and Xerces, to link user navigation activity to the abstracts and full text contents of the articles. We discuss the flexibility and convenience of XML standards and link this effort to related digital library visualization approaches.	acm computing classification system;apache xalan;archive;code;digital library;dublin core;high- and low-level;hyperbolic tree;java;management information system;prototype;topic maps;xml	Mark Ginsburg	2004	CAIS		digital library;open standard;visual system;computer science;data mining;database;metadata;world wide web;meta data services;information retrieval;metadata repository	Web+IR	-43.11562071551092	-2.2814802891873422	153583
dfbeb34c8d9d524d77f73fd36bfeb746ed88e5bb	multimedia: a technology for efficient information representation (summary)	efficient information representation			Peter Stucki	1991			computer science;theoretical computer science;data mining;information retrieval	NLP	-40.221978510243716	-8.002545451623632	153676
7c47339fd10354a2681c810919e9c61a871489ce	achieving adaptation for adaptive systems via runtime verification: a model-driven approach		Institute of Mathematics, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Haidian Dstr., Beijing 100190, P. R. China Key Laboratory of High Confidence Software Technologies (MoE), Peking University, Haidian Dstr., Beijing 100871, P. R. China Software Engineering Dept., College of Computer Science and Information Technology, Guangxi Normal University, Guilin, Guangxi 541004, P. R. China	academy;adaptive system;cr rao advanced institute of mathematics, statistics and computer science;model-driven integration;runtime verification;software engineering;systems science	Zhuoqun Yang;Zhi Jin;Zhi Li	2017	CoRR		real-time computing;simulation;computer science;artificial intelligence	Logic	-46.47438289327768	-5.504265474874682	153707
e3f72892832bdb1d91ba57a31e38613afd477b95	ciao-wps - automatic and intelligent orchestration of geospatial web services using semantic web (web 3.0) technologies		Current geospatial datasets and web services are disparate, obscure and difficult to expose to the world. With the advent of geospatial processes utilizing temporal data and big data, along with datasets continually increasing in size, the problem of under-exposed datasets and web services is amplified. Current text search capabilities do not sufficiently expose web services and datasets for use in on-the-fly geospatial use cases. End users are required to know the exact location of these online resources, their format and what they do. For example, to locate an OGC (Open Geospatial Consortium http://www.opengeospatial.org)-compliant WPS (Web Processing Service) that performs flood modelling, a Google Search for “Flood Modelling WPS” is insufficient to find relevant results. This paper proposes the integration of semantic web concepts and technologies into geospatial datasets and web services, making it possible to link these datasets and services via functionality, the inputs required and the outputs produced. To do so requires the extensive use of metadata to allow for a standardised form of description of their function. There are already ISO (International Organization for Standardization www.iso.org) standards in place (ISO 19115-1:2014) that specify the schema required for describing geographic information and services. The use of ontologies and AI (Artificial Intelligence) then allows for the intelligent determination of which web services and datasets to use, and in what order they are to be used to achieve the desired final output. This research aims to provide a method to automatically and intelligently chain together web services and datasets to assist in a geospatial analyst’s productivity. A simple prototype termed CIAO-WPS (Chet’s Intelligent, Automatically-Orchestrated Web Processing Services) is created as a proof of concept, using the Python programming language. The prototype seeks to reinforce ideas in regards to pathing and cost constraints, as well as explore overlooked designs.	artificial intelligence;big data;ciao;consortium;file server;geoweb;google search;ontology (information science);pathfinding;programming language;prototype;python;search algorithm;semantic web;server (computing);string searching algorithm;wps office;web processing service;web service	Chet Bing Tan;Geoff A. W. West;David A. McMeekin;Simon Moncrieff	2016		10.5220/0005821100710079	web service;web application security;web development;web modeling;data web;web mapping;web design;web standards;web api;semantic web;web navigation;social semantic web;web page;semantic web stack;multimedia;web intelligence;web 2.0;world wide web;web coverage service	Web+IR	-38.45535781211415	2.112815514511313	153810
bae2a73dcefb7dab5565dbdcf2507e224e275809	sensor data management in the cloud: data storage, data ingestion, and data retrieval		SummaryrnSensors are widely used in the field of manufacturing, railways, aerospace, cars, medicines, robotics, and many other aspects of our everyday life. There is an increasing need to capture, store, and analyse the dynamic semi-structured data from those sensors. A similar growth of semi-structured data in the modern web has led to the creation of NoSQL data stores for scalability, availability, and performance, whereas large-scale data processing frameworks for parallel analysis. NoSQL data store such as MongoDB and data processing framework such as Apache Hadoop has been studied for scientific data analysis. However, there has been no study on MongoDB with Apache Spark, and there is a limited understanding of how sensor data management can benefit from these technologies, specifically for ingesting high-velocity sensor data and parallel retrieval of high volume data. In this paper, we evaluate the performance of MongoDB sharding and no-sharding databases with Apache Spark, to identify the right software environment for sensor data management.	cloud computing;data retrieval;data storage tag	Prajwol Sangat;Maria Indrawan;David Taniar	2018	Concurrency and Computation: Practice and Experience	10.1002/cpe.4354	computer science;nosql;scalability;cloud computing;software;database;data mining;data management;data retrieval;computer data storage;data processing	DB	-34.46081325135345	-1.1783880534021343	154034
a2443a967d9e4d295466b6f7e7024760baf792d6	approaches to building metadata for data curation	metadata;academic library;data curation	In National Taiwan University (NTU), the Library aims to provide data curation services for university researchers from different research fields, particularly focusing on those from small sciences. In this paper, we will first investigate existing metadata schemas used for data curation services in North America and Europe. Next, we will attempt to develop an application profile, proposing metadata fields to be applied to data curation services in NTU. Finally, we will discuss our findings in this study, and take further action to develop a repository platform.	data curation;digital curation;network interface device	Hsueh-hua Chen;Yu Lin;Cynthia Chen	2013			data curation;computer science;data science;data mining;metadata;world wide web;meta data services	HPC	-43.775128990004085	-0.9122916492400939	154045
8f9211fcdde0dc6964671abb261c289b464c4c79	provenance, trust, explanations - and all that other meta knowledge		One of the key benefits of the Semantic Web technology is the better support of decentralized, self-organizing knowledge exchange between users. When integrating knowledge from different sources or even from the full range of the Semantic Web, we are faced with highly varying quality of information. Hence, one of the major challenges is to investigate the value of information based on the trustworthiness of its sources, the time of validity, the certainty, or the vagueness asserted to explicitly specified or derived facts. In fact, this list of different types of ’knowledge about knowledge’ (i.e. meta knowledge) is not even complete, one may be interested in the creator(s) of a fact, the document(s) in which it is stated — maybe implicitly, the web sites that certified it, and maybe many more dimensions. It becomes evident from such an enumeration that different types of meta knowledge should become accessible by a generic framework and not only by custom approaches that consider one particular dimension of meta knowledge at a time, possibly with incompatible assumptions. It is the contribution of this paper to develop such a coherent framework by illustration with and reference to more specialized papers in the area of management of meta knowledge (in particular [1, 4, 10, 11]). To illustrate our framework let us first discuss a short example. Our sample application aims to suggest the local chair(s) for a multimedia congress in Koblenz, Rhineland-Palatinate. We may assume that the search for suitable candidates with relevant research profiles and appropriate location exploits collected knowledge from Semantic Web pages of multiple Computer Science departments. Table 1 shows the instantiation for our sample scenario using a simplified abstract syntax for representing relevant facts and associated meta knowledge. We assume that all facts and axioms have been obtained from academic sites (University of Koblenz, FU Berlin, and Fraunhofer institutes) and some of them are also associated with last-modified timestamps (in the range between 2002 and 2008) which reflect the recency of knowledge. The collection presented in our example contains information about affiliation and research interests of academic scientists (e.g. the fact #1 represents the statement ’Research topic of Stefan Mueller is Computer Graphics’), as well as some definitions for the domain terminology (e.g. statement #6 defines a scientist to be a researcher working at the university) and rules for suggesting candidate chairs (e.g. statements #8 defines that ’Stefan Mueller is affiliated with only the University of Koblenz’ and #9 postulates that the successful candidate should work in Rhineland-Palatinate). The reader may note that the facts shown in Table 1 may require in practice the use of representation formalisms with different expressivity and complexity. While the facts #1..#5 can be expressed in RDF, the remainder requires more expressive frameworks like Logic Programming (for facts #9, #10) or OWL (for facts #6..#8). An evident problem with the presented collection is that particular facts about affiliations and research interests (facts #1..#5) refer to different scientists, named likewise professor Stefan Mueller. The first one is a professor for Computer Graphics from the University of Koblenz, the second one is a professor for German Grammar of the University of Berlin. For this reason, some conclusions of our mix-up knowledge base may appear strange and curious. From the user’s perspective, several sceptical questions may arise: 1. What are the research topic(s) of Stefan Mueller? Who said this (and when)? 2. May I trust the assessment that Stefan Mueller is a scientist? 3. What is the explanation that Stefan Mueller is the recommended candidate? This paper points out the commonalities and differences between approaches that can be used to compute the solutions to these questions using meta knowledge. We consider three of the most widespread paradigms for knowledge representation, i.e. an algebraic approach, logic programming, and description logics. The differences result, first, from the different algebraic and logical mechanisms used for querying and inferencing upon a knowledge base. Second, the differences result from the algebraic or logic approaches used to formalize a theory for meta knowledge. The commonalities are found in the way in which we treat meta knowledge results computation. We represent meta knowledge by annotations of statements/axioms and combine such annotations in a versatile manner. The design space of our treatment for meta knowledge is depicted in Figure 1. In the following, we instantiate in Section 2 the use of meta knowledge for algebraic querying methods (using our system Meta-K), in Section 3 for Logic Programming approaches (using the logic programming system Ontobroker [3]), and in Section 4 for OWL knowledge bases (using our system OWLMeta-K). The figure also indicates which combinations of knowledge and meta knowledge theories constitute straightforward extensions (using a ’+’) due to the decrease of the expressiveness power of the	abstract syntax;admissible numbering;coherence (physics);complexity;computation;computer graphics;computer science;definition;description logic;knowledge base;knowledge representation and reasoning;linear algebra;logic programming;organizing (structure);self-organization;semantic web;theory;trust (emotion);trusted timestamping;universal instantiation;vagueness;web page	Renata Queiroz Dividino;Simon Schenk;Sergej Sizov;Steffen Staab	2009	KI		semantic web;rdf;information retrieval;knowledge representation and reasoning;axiom;description logic;knowledge base;abstract syntax;logic programming;computer science;knowledge management	AI	-43.95830021614673	3.0873827640400706	154112
636776f2019258ea526be93311dd8c4972d4860b	ermrest: a web service for collaborative data management		The foundation of data oriented scientific collaboration is the ability for participants to find, access and reuse data created during the course of an investigation, what has been referred to as the FAIR principles. In this paper, we describe ERMrest, a collaborative data management service that promotes data oriented collaboration by enabling FAIR data management throughout the data life cycle. ERMrest is a RESTful web service that promotes discovery and reuse by organizing diverse data assets into a dynamic entity relationship model. We present details on the design and implementation of ERMrest, data on its performance and its use by a range of collaborations to accelerate and enhance their scientific output.	access control;benchmark (computing);computer hardware;data access;data element;dhrystone;entity–relationship model;hypertext transfer protocol;immutable object;mathematical optimization;metamodeling;open-source software;organizing (structure);performance evaluation;performance tuning;refinement (computing);representational state transfer;sql;snapshot (computer storage);usability;web application;web service	Karl Czajkowski;Carl Kesselman;Robert Schuler;Hongsuda Tangmunarunkit	2018		10.1145/3221269.3222333	computer science;web service;data mining;asset management;metadata;data management;reuse;entity–relationship model	HCI	-41.09568450372516	-0.2531064901742816	154269
d2b0310711d9ad2c8a65c0eb41387607b750d16e	sharing summarised semantic data rather than just data	information retrieval;data mining;social networking online data mining decision making information retrieval semantic web;semantics decision making conferences computers collaborative work chaos educational institutions;social networking online;semantic web;summarised semantic data sharing decision making information retrieval on line analytical processing data cubes business intelligence semantic olap framework web data representation internet data retrieval data management knowledge sharing information sharing eu funded projects social media simulation tools hardware cost reduction multiple disciplinary computer software systems	Summary form only given. To enable a group of multiple disciplinary computer software systems to effectively and efficiently collaborate is an important task for applications such as cooperative designs, energy efficiency monitoring system, real-time collaborative mechatronic control systems, e-learning, and enterprise computing etc. With the cost reduction of the hardware for storing data and the need of maintaining the vast amount of data generated from simulation tools, social media and sensors for decision making, to retrieve the right information and to share appropriate knowledge at right time among the participants is a challenging task. On-line Analytical Processing, OLAP, is an effective tool to summarize vast structured data into concise data cubes with multiple dimensions for business intelligence and decision making. Semantic OLAP was developed in the research communities is to process the linked data which is a form of web data representation with semantics increasingly gaining its popularity over the Internet. The introduction of OLAP has increased the efficiency of data retrieval and data management, but it does not address the issues of information and knowledge sharing. In this talk, these issues will be reviewed with examples and a new OLAP framework which could bridge the gaps and increase the sharing among different OLAPs will be presented with a case study. This talk will also share delegates with recent research findings from some ongoing EU-funded projects.	control system;data (computing);data retrieval;enterprise software;internet;linked data;mechatronics;olap cube;online analytical processing;real-time clock;sensor;simulation;social media;software system	Kuo-Ming Chao	2013	Proceedings of the 2013 IEEE 17th International Conference on Computer Supported Cooperative Work in Design (CSCWD)	10.1109/CSCWD.2013.6580930	online analytical processing;computer science;knowledge management;semantic web;social semantic web;data mining;database;world wide web;data retrieval	DB	-36.96425246918923	-4.771466450137561	154280
62dd3699198353f6b4010aa2f966d1c8ae58667c	information requirements for systems understanding	stress;logic;stability;hardware stress large scale systems logic decision making character recognition stability;character recognition;large scale systems;hardware	Some significant ways are discussed for describing or identifying systems and systems concepts so that less information is needed to understand them and this information can be communicated more quickly. The three major categories, structure, names of distinguishing qualities, and magnitude, probability, and time, identify the similarities and differences between various systems. Emphasis is put on the possibility that parts of these various systems may be sufficiently similar for detailed and authentic work on one system to be of value for another system. With the advent of large multiprocessing computers, such characteristics may result in more efficient use of hardware and software.	requirement	Harold Chestnut	1970	IEEE Trans. Systems Science and Cybernetics	10.1109/TSSC.1970.300322	stability;system of systems;computer science;artificial intelligence;theoretical computer science;stress;logic;statistics	Embedded	-46.567549267370865	-1.000579491492197	154351
4ac046240915d35bafc2f06c082982f2b5c5e3b5	interactive data exploration via machine learning models		This article provides an overview of our research on data exploration. Our work aims to facilitate interactive exploration tasks in many big data applications in the scientific, biomedical and healthcare domains. We argue for a shift towards learning-based exploration techniques that automatically steer the user towards interesting data areas based on relevance feedback on database samples, aiming to achieve the goal of identifying all database objects that match the user interest with high efficiency. Our research realizes machine learning theory in the new setting of interactive data exploration and develops new optimizations to support “automated” data exploration with high performance over large databases. In this paper, we discuss a suite of techniques that draw insights from machine learning algorithms to guide the exploration of a big data space and leverage the knowledge of exploration patterns to optimize query processing inside the database.	algorithm;big data;classification tree method;database;dataspaces;hoc (programming language);machine learning;relevance feedback	Olga Papaemmanouil;Yanlei Diao;Kyriaki Dimitriadou;Liping Peng	2016	IEEE Data Eng. Bull.		data exploration;robot learning;active learning (machine learning);machine learning;computer science;artificial intelligence	DB	-36.24398054214406	-7.918447362268494	154522
261920fa50d4cd7a8cacf4a8691fd7d0ffd9701c	compact hierarchical bipolar transistor modeling with hicum		978-981-4273-22-0(ebook) US$124 COMPACT HIERARCHICAL BIPOLAR TRANSISTOR MODELING WITH HICUM Dr. Michael Schroter received his electrical engineering Ph.D. in 1988 from the Ruhr-University Bochum, Germany. He was with Nortel, Ottawa, Canada, as Team Leader and Advisor until 1996 when he joined Rockwell (now Conexant), Newport Beach, California, where he managed the RF Device Modeling Group. Dr. Schroter has been a Full Professor at the University of Technology at Dresden (UTD), Germany since 1999, and a Research Professor at UC San Diego, USA. For several years, he was on the Technical Advisory Board of RFMAGIC (now Entropic Inc.), a communication system design company in San Diego, California. Dr. Schroter is a co-founder of XMOD Technologies in Bordeaux, France. He is the author of the industrial standard bipolar transistor compact model HICUM, the subject of this monograph. Since 2008, he has been the Technical Program Manager of DOTFIVE, a large European Research project on Half-THz SiGe HBT technology. He is presently on Leave of Absence from TUD to assume the position of Vice President of RF Engineering at RFNano, Newport Beach, California, where he is responsible for the device design of the first production carbon nanotube FET process technology.	electrical engineering;program manager;radio frequency;radio-frequency engineering;silicon-germanium;systems design;transistor;uc browser;uniform theory of diffraction	Michael Schröter;Anjan Chakravorty	2010		10.1142/7257	electronic engineering;engineering;electrical engineering	EDA	-47.003528088461984	-6.80688564656961	154886
74767e8b38c74cd116e4605cd1db987d11e72312	special issue on cognitive-inspired network systems	communication networks;special issues and sections cognitive radio cognition collaboration communication networks;special issues and sections;collaboration;cognitive radio;cognition	The eight papers in this special issue focus on cognitive inspired networked applications. Cognition is emerging as a new and promising methodology with the development of cognitive-inspired computing, ubiquitous networks, and systems, which has the potential to enable a large class of applications. However, recent advances, such as interference alignment, collaborative communications, and cognitive computational theory, make us lack a clear understanding of analytical cognitive-inspired computing, networks, and systems, as well as best practices, to design resource-efficient, utility, pervasive, scalable, and autonomy oriented cognitive systems and applications. The objective of this special Issue is to bring together state-of-the-art research contributions that address these key aspects of cognitive inspired network systems and applications.	artificial intelligence;best practice;cognition;interference (communication);pervasive informatics;scalability;theory of computation	Yuhang Yang;Maode Ma;Bahram Honary	2016	IEEE Systems Journal	10.1109/JSYST.2015.2411413	cognitive radio;cognition;cognitive network;computer science;knowledge management;artificial intelligence;management science;management;computer network;collaboration	HCI	-47.73049518656134	2.284588756173736	155105
e3f329b03100553eb30fc690b88910388a054535	research objects for audio processing: capturing semantics for reproducibility		Earlier work has identified the potential for reuse and reproducibility when applying workflow systems to audio analysis and Music Information Retrieval. In this paper we extend this approach with the introduction of Research Objects to capture semantic information about the use of workflows within the audio research and development process. Once aggregated, the metadata encapsulated in a Research Object can be used to manage and disseminate research output, providing a well structured foundation for meeting the needs of reproducibility. We report on the development and deployment of a software suite that practically applies this notion of Research Objects to capture the semantics surrounding the use of an audio processing workflow, and reflect upon how this might be further integrated with lower level semantics from the audio processing	information retrieval;software deployment;software suite	Kevin R. Page;Raúl Palma;Piotr Holubowicz;Graham Klyne;Stian Soiland-Reyes;Daniel Garijo;Khalid Belhajjame;Rudolf Mayer	2014			aes11;computer science;database;world wide web;information retrieval	AI	-46.642690424082176	3.272012052441169	155185
5e8ab0bb4ba4846a192c0bae45fc05b0b11db6f9	truth discovery and crowdsourcing aggregation: a unified perspective		In the era of Big Data, data entries, even describing the same objects or events, can come from a variety of sources, where a data source can be a web page, a database or a person. Consequently, conflicts among sources become inevitable. To resolve the conflicts and achieve high quality data, truth discovery and crowdsourcing aggregation have been studied intensively. However, although these two topics have a lot in common, they are studied separately and are applied to different domains. To answer the need of a systematic introduction and comparison of the two topics, we present an organized picture on truth discovery and crowdsourcing aggregation in this tutorial. They are compared on both theory and application levels, and their related areas as well as open questions are discussed.	big data;comefrom;crowdsourcing;database;display resolution;web page	Jing Gao;Qi Li;Bo Zhao;Wei Fan;Jiawei Han	2015	PVLDB	10.14778/2824032.2824136	computer science;data science;data mining;database;world wide web	DB	-36.415756594377	-6.95692953227128	155392
54bc2ca2495a98026fb5df8bc149e98a07ad4eec	the design of measurement and control system of the radon detection instrument based on mcu	microcontrollers;environmental monitoring geophysics;database;control system;delphi;computerised instrumentation;radon;gas sensors;object oriented languages	The design of measurement and control system of the radon detection instrument, which used for geological prospecting and environmental monitoring, is introduced in this paper. This paper includes the present research situation of the radon detection instrument, the software and hardware design, the focus is the data processing software design which based on the fourth language Delphi. The data processing software includes the data serial communication, the data acceptance, the data display, the data storage, the curve drawing, the database management and so on.	computer data storage;control system;database;microcontroller;serial communication;software design	Ping Liang;YaMei Li	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023936	microcontroller;embedded system;simulation;computer science;engineering;control system;data mining;radon;object-oriented programming	DB	-38.68117511453842	-3.081387729782965	155546
d075f25341d239ac0ec1f34a0176318bc386fc52	the finite element method and earthquake engineering: the 2006 benjamin franklin medal in civil engineering presented to ray w. clough	engineering design;earthquake engineering;finite element method;element shape functions;nodal displacements;scientific computing;dynamic analysis;finite element method earthquake engineering	The Franklin Institute, Philadelphia, PA awarded the 2006 Benjamin Franklin Medal in civil engineering to Ray W. Clough for revolutionizing engineering and scientific computation, and engineering design methods through his formulation and development of the finite element method, and for his innovative leadership in applying the method to the field of earthquake engineering with special emphasis on the seismic performance of dams.	finite element method;franklin electronic publishers	Brian J. Sullivan	2010	J. Franklin Institute	10.1016/j.jfranklin.2008.04.014	earthquake engineering;engineering;electrical engineering;civil engineering;finite element method;dynamic program analysis;mechanical engineering	DB	-47.34982414861779	-7.545176170125643	155677
2d03c0b247b6980f5c1c5e709cf59c6550e1c907	"""integrating the personal computer into the university computing center environment: """"a marriage of necessity!"""""""	personal computer	In this paper an approach to the integration of personal computers with a university computing center is outlined. The approach involves the development of application-oriented software local to the personal computer effectively transforming it into a software workstation. This permits users to take advantage of the features and resources of both the personal computer and the main-frame. To facilitate the development of such software the paper outlines an approach to the design of workstations. An experiment involving the development of a simple editing workstation based upon this method is described and discussed.	mainframe computer;personal computer;workstation	Michael Anthony Bauer;J. Michael Bennett;Albert Lee	1983		10.1145/800041.801430	personal software process;computing;human–computer interaction;computer science;software engineering;personal information manager;computer engineering	SE	-47.90014500250592	-1.4822416558187606	155986
1b0cba653efbc64fbeaa931fdfcdcf739c837f06	integrating similarity retrieval and skyline exploration via relevance feedback	search space;similarity retrieval;relevance feedback	Similarity retrieval have been widely used in many practical search applications. A similarity query model can be viewed as a logical combination of a set of similarity predicates. A user can initialize a query model, but model parameters or the model itself may be inadequately specified. As a result, a retrieval system cannot guarantee that it has presented all the relevant tuples to the user. In this paper, we propose a framework that integrates the similarity retrieval and skyline exploration. Using the relevance feedback as a way to constrain the search space, our framework can intelligently explore only a necessary portion of data that contains all the relevant tuples. Our framework is also flexible enough to incorporate model refinement techniques to retrieving relevant results as early as possible.	information retrieval;refinement (computing);relevance feedback	Yiming Ma;Sharad Mehrotra	2007		10.1007/978-3-540-71703-4_101	searching the conformational space for docking;data mining;database;information retrieval	Web+IR	-34.14223349058825	4.122880276855011	156184
b3041a5367388851a415440f4449dd1ccead9c2d	power of one non-clean qubit		Tomoyuki Morimae, ∗ Keisuke Fujii, 3, † and Harumichi Nishimura ‡ ASRLD Unit, Gunma University, 1-5-1 Tenjincho, Kiryu, Gunma, 376-0052, Japan Photon Science Center, Graduate School of Engineering, The University of Tokyo, 2-11-16 Yayoi, Bunkyoku, Tokyo 113-8656, Japan JST, PRESTO, 4-1-8 Honcho, Kawaguchi, Saitama, 332-0012, Japan Graduate School of Information Science, Nagoya University, Furocho, Chikusaku, Nagoya, Aichi, 464-8601, Japan (Dated: October 25, 2016)	iso 8601;information science;list of information schools;presto;qubit;tomoyuki nishita	Tomoyuki Morimae;Keisuke Fujii;Harumichi Nishimura	2016	CoRR		qubit;quantum mechanics	NLP	-46.03709501461613	-9.785201803367798	156388
fa49b4d286a86e78c800837e18de1297d2190058	financial reporting and auditing agent with net knowledge (fraank) and extensible business reporting language (xbrl)	financial reporting	"""This paper describes the development and applications of FRAANK – Financial Reporting and Auditing Agent with Net Knowledge. The prototype of FRAANK presented here provides automated access to, and understanding and integration of rapidly changing financial information available from various sources on the Internet. In particular, FRAANK implements intelligent parsing to extract accounting numbers from natural-text financial statements available from the SEC EDGAR repository. FRAANK develops an """" understanding """" of the accounting numbers by means of matching the line-item labels to synonyms of tags in an XBRL taxonomy. As a result, FRAANK converts the consolidated balance sheet, income statement, and statement of cash flows into XBRL-tagged format. Based on FRAANK, we propose an empirical approach towards the evaluation and improvement of XBRL taxonomies and for identifying and justifying needs for specialized taxonomies by assessing a taxonomy fit to the historical data, i.e., the qua r-terly and annual EDGAR filings. Using a test set of 10-K SEC filings, we evaluate FRAANK's performance by estimating its success rate in extracting and tagging the line items using the year 2000 C&I XBRL Taxonomy, Version 1. The evaluation results show that FRAANK is an advanced research prototype that can be useful in various practical applications. FRAANK also integrates the accounting numbers with other financial information publicly available on the Inter-net, such as timely stock quotes and analysts' forecasts of earnings, and calculates important financial ratios and other financial-analysis indicators."""	computer security;internet;parsing;prototype;taxonomy (general);test set	Matthew Bovee;Alexander Kogan;Kay M. Nelson;Rajendra P. Srivastava;Miklos A. Vasarhelyi	2005	J. Information Systems	10.2308/jis.2005.19.1.19	accounting;public relations;accounting management;computer science;marketing;data mining;management;world wide web;computer security	AI	-34.85133485217145	-8.156187861884192	156423
13dbfa88bc762efb3ddf4122b728407d96daf81e	internet delivery of meteorological and oceanographic data in wide area naval usage environments	oceanographic data;distributed system;user needs;training;data format;resumable object streams;meteorological data;spatio temporal data;internet;meteorological and oceanographic data;interoperability;effective bandwidth;data displays	Access and retrieval of meteorological and oceanographic data from heterogeneous sources in a distributed system presents many issues. Effective bandwidth utilization is important for any distributed system. In addition, specific issues need to be addressed in order to assimilate spatio-temporal data from multiple sources. These issues include resolution of differences in datum, map-projection and time coordinate. Reduction in the complexity of data formats is a significant factor for fostering interoperability. Simplification of training is important to promote usage of the distributed system. We describe techniques that revolutionize Web-based delivery of meteorological and oceanographic data to address needs of the Naval/Marine user.	bandwidth (signal processing);distributed computing;geodetic datum;interoperability;map projection;programming paradigm;publish and subscribe (mac os);relevance;rule-based system;siprnet;text simplification;world wide web	Udaykiran Katikaneni;Roy Ladner;Frederick E. Petry	2004		10.1145/1013367.1013382	interoperability;the internet;computer science;data mining;database;world wide web	ML	-39.99977670441931	-1.7896890239342054	156495
195ea61031aaa0a06ed2abaf48e5306198a895fe	nasa technology transfer system	xml;aerospace computing;cloud computing;database management systems;document handling;grid computing;information resources;information systems;gxd framework;http;ipp;internet;internet engineering task force;nasa software acquisition;nasa technology transfer system;ntts;saas;webdav;world wide web consortium architecture domain;xdb;agreements & partnerships management;awards management and payment;cost reduction;distributed information resources;document-centered object-relational xml database mapping;extensible database technology;grid xml datastore framework;heterogeneous information resources;information management systems;innovative partnership program;intellectual property management;invention disclosure;leads development;software as a service;software release management;success stories management;third-party application plug;grid;search;xml	"""This paper discusses the modern approach of the implementation of Software as a Service (SaaS) for NASA as a way to reduce cost and increase efficiency. The Grid XML Datastore Framework is an extension of SaaS framework based on eXtensible Database technology (XDB). This is implemented to support the Innovative Partnership Program (IPP) for its Technology Transfer System (NTTS) project. NTTS supports NASA's entire technology transfer process and is the agency's one system with all of its technological assets. NTTS is the primary IT backbone that supports the entire technology transfer process for NASA, which includes Invention Disclosure, Intellectual Property Management, Awards Management and Payment, Software Release Management, Agreements & Partnerships Management, Success Stories Management and Leads Development. The system is intended to have three interfaces serving three distinct communities, mainly agency-wide, center specific and the public. The SaaS implementation deploys the software as an application hosted as a NASA service, which is provided across the Internet. By eliminating the need to install and run the application on the customer's own computer, SaaS alleviates the customer's burden of software maintenance, ongoing operation, and support. Using SaaS can also reduce the up-front expense of software purchases, through less costly, on-demand pricing. From a NASA software acquisition perspective, NASA pays one time for the database and storage and thus third-party application plug in on demand. This paper describes how NTTS utilizes and benefits Grid XML Datastore Framework (GXD Framework), an open and extensible database architecture that supports efficient and flexible integration of heterogeneous and distributed information resources. GXD Framework provides a novel """"schema-less"""" database approach using a document-centered object-relational XML database mapping. This enables structured, unstructured, and semi-structured information to be integrated without requiring document schemas or translation tables. GXD Framework utilizes existing international protocol standards of the World Wide Web Consortium Architecture Domain and the Internet Engineering Task Force, primarily HTTP, XML and WebDAV. Through a combination of these international protocols, universal database record identifiers, and physical address data types, GXD enables an unlimited number of desktops and distributed information sources to be linked seamlessly and efficiently into an information grid. GXD Framework has been used to create a powerful set of novel information management systems for a variety of scientific and engineering applications."""	.net framework;architecture domain;consortium;data store;hypertext transfer protocol;identifier;information management;internet backbone;object-relational database;physical address;purchasing;release management;resource description framework;row (database);semiconductor industry;software as a service;software maintenance;software release life cycle;webdav;world wide web;xdb enterprise server;xml database	David A. Maluf;Takeshi Okimura;Mohana Gurram	2011	2011 IEEE Fourth International Conference on Space Mission Challenges for Information Technology	10.1007/978-3-642-22732-5_7	computer science;data mining;database;world wide web	DB	-42.36900773993311	-2.9465223862555976	157574
d552207f09ceeb9f81d1b625376c8a38cf0c5cc5	ccf: a framework for collaborative computing	integrated support;scientific endeavor;cooperative computer-based work;computational transformation;collaborative computing frameworks toolkit;different site;data management	The Collaborative Computing Frameworks toolkit enables cooperative computer-based work among collaborators at different sites. The collaborators are typically engaged in scientific endeavors, but CCF offers integrated support for any collaboration that depends heavily on distributed computational transformations and data management	microsoft customer care framework		2000	IEEE Internet Computing	10.1109/4236.815845		Visualization	-46.025776338097465	2.884793243326638	157631
34b4541397a09a93292465113d14b1bd17221966	architecture and applications of a geovisual analytics framework	other electrical engineering electronic engineering information engineering;annan elektroteknik och elektronik	The large and ever-increasing amounts of multi-dimensional, multivariate, multi-source, spatio-temporal data represent a major challenge for the future. The need to analyse and make decisions based on these data streams, often in time-critical situations, demands integrated, automatic and sophisticated interactive tools that aid the user to manage, process, visualize and interact with large data spaces. The rise of ‘Web 2.0’, which is undisputedly linked with developments such as blogs, wikis and social networking, and the internet usage explosion in the last decade represent another challenge for adapting these tools to the Internet to reach a broader user community. In this context, the research presented in this thesis introduces an effective web-enabled geovisual analytics framework implemented, applied and verified in Adobe Flash ActionScript and HTML5/JavaScript. It has been developed based on the principles behind Visual Analytics and designed to significantly reduce the time and effort needed to develop customized web-enabled applications for geovisual analytics tasks and to bring the benefits of visual analytics to the public. The framework has been developed based on a component architecture and includes a wide range of visualization techniques enhanced with various interaction techniques and interactive features to support better data exploration and analysis. The importance of multiple coordinated and linked views is emphasized and a number of effective techniques for linking views are introduced. Research has so far focused more on tools that explore and present data while tools that support capturing and sharing gained insight have not received the same attention. Therefore, this is one of the focuses of the research presented in this thesis. A snapshot technique is introduced, which supports capturing discoveries made during the exploratory data analysis process and can be used for sharing gained knowledge. The thesis also presents a number of applications developed to verify the usability and the overall performance of the framework for the visualization, exploration and analysis of data in different domains. Four application scenarios are presented introducing (1) the synergies among information visualization methods, geovisualization methods and volume data visualization methods for the exploration and correlation of spatio-temporal ocean data, (2) effective techniques for the visualization, exploration and analysis of selforganizing network data, (3) effective flow visualization techniques applied to the analysis of time-varying spatial interaction data such as migration data, commuting data and trade flow data, and (4) effective techniques for the visualization, exploration and analysis of flood data.	actionscript;adobe flash;blog;component-based software engineering;data visualization;dataspaces;geovisualization;html5;information visualization;interaction technique;interactivity;internet access;linked list;multi-source;snapshot (computer storage);synergy;usability;virtual community;visual analytics;web 2.0;wiki;window of opportunity	Q. Tri Ho	2013			systems engineering;engineering;data science;data mining	Visualization	-35.85307017408692	-5.423984481829083	158045
dd1b55ed74508fb9b3593a2566243f1657753e50	consolidating drug data on a global scale using linked data	data mining and knowledge discovery;computational biology bioinformatics;algorithms;combinatorial libraries;computer appl in life sciences;bioinformatics	BACKGROUND Drug product data is available on the Web in a distributed fashion. The reasons lie within the regulatory domains, which exist on a national level. As a consequence, the drug data available on the Web are independently curated by national institutions from each country, leaving the data in varying languages, with a varying structure, granularity level and format, on different locations on the Web. Therefore, one of the main challenges in the realm of drug data is the consolidation and integration of large amounts of heterogeneous data into a comprehensive dataspace, for the purpose of developing data-driven applications. In recent years, the adoption of the Linked Data principles has enabled data publishers to provide structured data on the Web and contextually interlink them with other public datasets, effectively de-siloing them. Defining methodological guidelines and specialized tools for generating Linked Data in the drug domain, applicable on a global scale, is a crucial step to achieving the necessary levels of data consolidation and alignment needed for the development of a global dataset of drug product data. This dataset would then enable a myriad of new usage scenarios, which can, for instance, provide insight into the global availability of different drug categories in different parts of the world.   RESULTS We developed a methodology and a set of tools which support the process of generating Linked Data in the drug domain. Using them, we generated the LinkedDrugs dataset by seamlessly transforming, consolidating and publishing high-quality, 5-star Linked Drug Data from twenty-three countries, containing over 248,000 drug products, over 99,000,000 RDF triples and over 278,000 links to generic drugs from the LOD Cloud. Using the linked nature of the dataset, we demonstrate its ability to support advanced usage scenarios in the drug domain.   CONCLUSIONS The process of generating the LinkedDrugs dataset demonstrates the applicability of the methodological guidelines and the supporting tools in transforming drug product data from various, independent and distributed sources, into a comprehensive Linked Drug Data dataset. The presented user-centric and analytical usage scenarios over the dataset show the advantages of having a de-siloed, consolidated and comprehensive dataspace of drug data available via the existing infrastructure of the Web.	categories;dataspaces;departure - action;gucy2c protein, human;gene regulatory network;generic drugs;genetic heterogeneity;linked data;lung consolidation;methodology aspects;pharmaceutical preparations;programming languages;resource description framework;semiconductor consolidation;silo (dataset);world wide web	Milos Jovanovik;Dimitar Trajanov	2017		10.1186/s13326-016-0111-z	drug;open data;linked data;data mining;data science;granularity;computer science;data model	Comp.	-40.44018538121415	1.6810199857308232	158291
c58b3035b4140555b5aaa42c82f008901e31b182	delivery of agricultural drought information via web services		Data, information, knowledge, and wisdom are four basic steps of human perception process of objects. In order to better understand agricultural drought and make proper decisions, it is necessary to extract drought information out of related data (e.g., remotely sensed data) and discover knowledge from the extracted information. This paper explores advantages of Web services in providing on-demand agricultural drought analysis and facilitating the perception process in agricultural drought management. Four Web services, drawROI, getVCIStats, getDroughtPercentageByStates, and getDroughtTimeSeries, are presented in details in this paper. These Web services demonstrate improved support to drought analysis and decision-making for the general public and illustrate the potential of Web services in automating geospatial knowledge discovery and dissemination in the Big Data era.		Chunming Peng;Meixia Deng;Liping Di;Weiguo Han	2015	Earth Science Informatics	10.1007/s12145-014-0198-7	knowledge management;data science;data mining;services computing	Theory	-39.93255687445258	-3.117401558634199	158340
fc25b8f51764f847ee766e0bbeef1cf9102d018c	implementing a data management infrastructure for big healthcare data		The advancements in healthcare have brought to the fore the need for flexible access to health-related information and created an ever-growing demand for efficient data management infrastructures. To this direction, in this paper, we present an effective and efficient data management infrastructure implemented for the iManageCancer EU project. The architecture focuses on enabling data access to multiple, heterogeneous and diverse data source that are initially available in a data lake. Parts of these data are integrated and semantically uplifted using a modular ontology. This integration can be either at run-time or through an ETL process ensuring efficient access to the integrated information. A unique feature of out platform is that it allows the uninterrupted, continuous evolution of ontologies/terminologies. Finally, summarization tools enable the quick understanding of the available information, whereas APIs and anonymization services ensure the secure access to the requested information.	data access;ontology (information science)	Haridimos Kondylakis;Lefteris Koumakis;Manolis Tsiknakis;Kostas Marias	2018	2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)	10.1109/BHI.2018.8333443	data mining;ontology (information science);semantics;data management;architecture;modular design;automatic summarization;data access;data modeling;computer science	DB	-37.07984247402387	0.7303686741936976	158502
117e5fd7de23afe73fea5269a66353b9b7559e9a	business intelligence and geographic information system for hydrogeology		We have developed the Hydrogeological Information System (HgIS). Its purpose is to load data from available data sources of any kind, to visualize and analyze data and to implement simple models. HgIS is mostly built upon the Pentaho business intelligence (BI) platform. HgIS uses only some components of BI in comparison to enterprise BI solutions. Adequacy and limitation of data warehousing and BI application for groundwater data is discussed. Data extraction, transformation and loading is focused on integration of wide variety of structured and semi-structured data. Data warehouse uses a hybrid snowflake/star schema. Inmon’s paradigm is used because data semantics is known and the volume of data is limited. HgIS is data agnostic, database agnostic, scalable and interoperable. The architecture of the system corresponds to a spatial business intelligence solution (GeoBI) – a combination of BI and geographic information systems (GIS). Groundwater practitioners have worked with GIS software for decades but BI technologies and tools have not previously been applied to groundwater data.	geographic information system	Kamil Nesetril;Jan Sembera	2017		10.1007/978-3-319-89935-0_14	star schema;data mining;systems engineering;architecture;environmental science;data model;information system;geographic information system;data extraction;data warehouse;business intelligence	AI	-36.341384951292	-1.3042937798697998	158632
a03108f68bb3859f4c396c735f932cec641fb5e5	where has this hard disk been?: extracting geospatial intelligence from digital storage systems		Digital storage systems (DSS) contain an abundance of geospatial data which can be extracted and analysed to provide useful and complex intelligence insights. This data takes a number of forms such as data within text files, configuration databases and in operating system generated files—each of which require particular forms of processing. This paper investigates the breadth of geospatial data available on DSS, the issues and problems involved in extracting and analysing them and the intelligence insights that the visualisation of the data can provide. We describe a framework to extract a wide range of geospatial data from a DSS and resolve this data into geographic coordinates.	hard disk drive	Harjinder Singh Lallie;Nathan Griffiths	2014		10.1007/978-3-319-04447-7_20	computer science;data science;data mining;database;geographic information systems in geospatial intelligence	OS	-37.654685861612755	-1.3183168338751756	158795
334095312fa3183ad227097acebdc9a82bdb3777	synthetic biology driven by harnessing forces of disruptive innovation	dna;synthetic biology;complex adaptive systems;lifesciences organizations harnessing forces synthetic biology research energy food industrial processing diseases disruptive innovation cloud computing big data mobile computing social it cost reduction r d collaboration bioinformatics;life science;big data;keywords it;data analytics;mobile computing;mobile computing big data bioinformatics cloud computing;synthetic biology bioinformatics cloud computing dna genomics computational modeling;cloud computing;bioinformatics	The resultant outcome of synthetic biology research offers the promise of changing the way that we create energy, produce food, optimize industrial processing, and detect, prevent, and cure diseases. Researchers can now harness forces of disruptive innovation (Cloud, Big Data, Mobile, Social) to realize significant benefits and reduce IT cost and complexity, enhance R&D collaboration, and improve bioinformatics to allow lifesciences organizations to focus on answering some of lifes most challenging questions.	big data;bioinformatics;bioinformatics;biological system;mathematical optimization;resultant;synthetic biology;synthetic intelligence;vii	Melvin Greer;Manuel Rodríguez-Martínez;Jaime Seguel	2013	2013 International Conference on Social Computing	10.1109/SocialCom.2013.126	complex adaptive system;big data;cloud computing;computer science;bioinformatics;data science;operating system;data mining;data analysis;mobile computing;world wide web;synthetic biology;dna	DB	-38.720510081551076	-9.463343281091072	158865
d9381f4161d8341969a82853a553fdb2f395011d	web data stores (aka nosql databases): a data model and data management perspective	nosql database;scalable architecture;web services	During the past decade, several Web companies have developed what I would call 'Web data stores' to support the performance, scalability and availability requirements of their interactive Web services. Google's Bigtable and Amazon's Dynamo in particular have energised a community of developers to develop similar systems as open source. These systems have come to be known as 'NoSQL database' systems. In this paper, I will examine these systems from the data model and data management perspective which has not been given enough attention. This is to shed further light on understanding the nature of these systems and their future directions.	data model;data store;database;nosql	Won Young Kim	2014	IJWGS	10.1504/IJWGS.2014.058774	web service;web development;web modeling;data web;computer science;data mining;database;law;world wide web	DB	-34.011994164718416	1.3251049120479357	159260
2ede92422d8d96994b4e2b6fcd7421473ec30903	omit: domain ontology and knowledge acquisition in microrna target prediction - (short paper)	domain ontology;manual knowledge acquisition;effective knowledge acquisition;mirna domain;cancer researcher;knowledge discovery;human cancer;mirna databases;microrna target prediction;cancer patient;limited prior knowledge;formal knowledge representation	The identification and characterization of important roles microRNAs (miRNAs) played in human cancer is an increasingly active area in medical informatics. In particular, the prediction of miRNA target genes remains a challenging task to cancer researchers. Current efforts have focused on manual knowledge acquisition from existing miRNA databases, which is time-consuming, error-prone, and subject to biologists’ limited prior knowledge. Therefore, an effective knowledge acquisition has been inhibited. We propose a computing framework based on the Ontology for MicroRNA Target Prediction (OMIT), the very first ontology in miRNA domain. With such formal knowledge representation, it is thus possible to facilitate knowledge discovery and sharing from existing sources. Consequently, the framework aims to assist biologists in unraveling important roles of miRNAs in human cancer, and thus to help clinicians in making sound decisions when treating cancer patients. Corresponding Author. Corresponding Author. R. Meersman et al. (Eds.): OTM 2010, Part II, LNCS 6427, pp. 1162–1169, 2010. c © Springer-Verlag Berlin Heidelberg 2010 OMIT Knowledge Acquisition Framework 1163	bmc bioinformatics;bottom-up proteomics;cluster analysis;cognitive dimensions of notations;consortium;database;gene ontology;informatics;knowledge acquisition;knowledge representation and reasoning;lecture notes in computer science;national center for biomedical ontology;ontology (information science);operations and technology management;springer (tank);top-down and bottom-up design;web ontology language;world wide web	Christopher Townsend;Jingshan Huang;Dejing Dou;Shivraj Dalvi;Patrick J. Hayes;Lei He;Wen-chang Lin;Haishan Liu;Robert Rudnick;Hardik Shah	2010		10.1007/978-3-642-16949-6_36	computer science;bioinformatics;data science;data mining	ML	-35.23024527351997	-9.510331931851212	159410
a1cb4845c6cb1fa0ee2661f87ccf1a9ece93cc69	an internet of things platform for real-world and digital objects		The vision of the Internet of Things (IoT) relies on the provisioning of real-world services, which are provided by smart objects that are directly related to the physical world. A structured, machine-processible approach to provision such real-world services is needed to make heterogeneous physical objects accessible on a large scale and to integrate them with the digital world. The incorporation of observation and measurement data obtained from the physical objects with the Web data, using information processing and knowledge engineering methods, enables the construction of ”intelligent and interconnected things”. The current research mostly focuses on the communication and networking aspects between the devices that are used for sensing amd measurement of the real world objects. There is, however, relatively less effort concentrated on creating dynamic infrastructures to support integration of the data into the Web and provide unified access to such data on service and application levels. This paper presents a semantic modelling and linked data approach to create an information framework for IoT. The paper describes a platform to publish instances of the IoT related resources and entities and to link them to existing resources on the Web. The developed platform supports publication of extensible and interoperable descriptions in the form of linked data.	dmz (computing);dynamic infrastructure;entity;information framework;information processing;internet of things;interoperability;knowledge engineering;linked data;provisioning;sparql;smart objects;world wide web	Suparna De;Tarek Elsaleh;Payam M. Barnaghi;Stefan Meissner	2012	Scalable Computing: Practice and Experience		web of things;computer science;data mining;internet privacy;world wide web;internet of things	Web+IR	-39.16376416552587	1.4802121750236783	159677
c45d07b599f719d7883a865a4902706e8a733854	analyzing web services networks: theory and practice	datorsystem;computer systems;datalogi;computer science	This paper addresses the problem of applying the general network theory for analyzing qualitatively Web services networks. The paper reviews current approaches to analyzing Web services networks, generalizes the published approaches into a formal framework for analyzing Web services networks and demonstrates its applicability in practice. More specifically, two case studies are described where the presented framework has been applied. The first one considers identification of redundant data in large-scale service-oriented information systems, while the second one measures information diffusion between individual information systems.	web service	Peep Küngas;Marlon Dumas;Shahab Mokarizadeh;Mihhail Matskin	2014		10.1007/978-1-4614-7535-4_16	web modeling;computer science;data science;data mining;world wide web	ML	-43.5506893668178	-1.4275899058369044	159751
3ba6a2adb1d610d0ff242d65d23c2ecdd6d8ddf1	integrating data mining into an e-commerce system	e commerce;data mining		data mining;e-commerce	Frank Wang;Na Helian	2003			web mining;e-commerce;data mining;computer science;text mining	ML	-39.56319087478146	-6.841217660537621	160124
30addfac7428c88b0dee0a2af81a7069ecea5793	assisted browsing for semistructured data	user interface;information retrieval;semistructured data;semi structured data	The development of the RDF[2] standard highlights the fact that a great deal of useful information is in the form of semistructured data—objects connected by relations fitting no rigorous schema. To make use of this information it is important to be able to search and browse semistructured data repositories. In this paper, we present a framework, system, and user interface supporting navigation in such repositories. The system is general purpose, involving no hard-coded assumptions about the structure of the repository. The focus is on providing users with helpful “next steps” leading them to the information they are seeking.	browsing;hard coding;user interface	Vineet Sinha;David R. Karger;David Huynh	2003			semi-structured data;computer science;database;user interface;world wide web;information retrieval	DB	-34.16150417601468	4.050559564208091	160155
c1692e4e3eafcd03eb4146c1cadc2d8a557c8620	theory and computer simulation of the moiré patterns in single-layer cylindrical particles		patterns in single-layer cylindrical particles VLADIMIR SAVELJEV AND IRINA PALCHIKOVA 1 Department of Physics, Myongji University, 116 Myongji-ro, Cheoin-gu, Yongin, Gyeonggi-do, Korea, 17058 2 Technological Design Institute of Scientific Instrument Engineering (TDISI), Siberian Branch RAS, 41, Russkaya St., 630058 Novosibirsk, Russia 3 Novosibirsk State University, 2, Pirogova St., 630090 Novosibirsk, Russia *Corresponding author: saveljev.vv@gmail.com	computer simulation	Vladimir Saveljev;Irina Palchikova	2016	CoRR		moiré pattern;graphene;cylinder;physics;quantum mechanics;paraxial approximation;optics	ML	-46.558732990396294	-7.855219989932511	160483
2973ea26a8d4134cda88ff576edd2954535a11e5	technologien der vermittlungstechnik in schnellen kommunikationsnetzen		Vol. SAC-6, No. 6, July 1988, pp. 1025-1032 [26] J. O. Limb, C. Flores: Description of Fasnet An Unidirectional LocalArea Communications Network, The Bell System Technical Journal, Vol. 61, No. 7, Sept. 1982, pp. 1413-1440 [27] I. P. Kaminow: Non-Coherent Photonic Frequency-Multiplexed Access Networks, IEEE Network, Vol. 3, No. 2, March 1989, pp. 4-12 [28] M. J. Karol, B. Glance: Performance of the PAC Optical Packet Network, GLOBECOM '91, Phoenix, AZ, Dec. 1991, pp. 1258-1263 [29] N. F. Maxemchuk: The Manhattan Street Network, GLOBECOM '85, New Orleans, Dec. 1985, paper 9.1, pp. 255-261 [30] S. Morris, T. Suda, T. Nguyen: A Tree LAN with Collision Avoidance: Photonic Switch Design and Simulated Performance, Computer Networks and ISDN Systems, Vol. 17, No. 2,1989, pp. 89-100 [31] B. Mukherjee: WDM-Based Local Lightwave Networks, Part I: SingleHop Systems, IEEE Network, Vol. 6, No. 3, May 1992, pp. 12-27 [32] B. Mukherjee: WDM-Based Local Lightwave Networks. Part II: Multihop Systems, IEEE Network, Vol. 6, No. 4, July 1992, pp. 20-32 [33] B. Mukherjee, C. Bisdikian: A Journey through the DQDB Network Literature, Performance Evaluation, Vol. 16, No. 1-3, November 1992, pp. 129-158 [34] M. M. Nassehi: CRMA: An Access Scheme for High-Speed LANs and MANs, ICC '90, Atlanta, GA, April 1990, pp. 1697-1702 [35] Y. Ofek, M. Yung: Asynchronous Lossless Broadcast-with-Feedback on the MetaNet Architecture, INFOCOM '91, Bai Harbour, FL, April 1991, pp. 1050-1063 [36] R. Ramaswami: Multiwavelength Lightwave Networks for Computer Communication, IEEE Communications Magazine, Vol. 31, No. 2, Feb. 1993, pp. 78-88 [37] F E. Ross: An Overview of FDDI: The Fiber Distributed Data Interface, IEEE J. Select. Areas Commun., Vol. SAC-7, No. 7, Sept. 1989, pp. 1043-1051 [38] J. A. Salehi: Emerging Optical CodeDivision Multiple Access Communications Systems, IEEE Network, Vol. 3, No. 2, March 1989, pp. 31-39 [39] T. Suda, S. Morris: Tree LANs with Collision Avoidance: Station and Switch Protocols, Computer Networks and ISDN Systems, Vol. 17, No. 2, 1989, pp. 101-110 [40] F. A. Tobagi: Multiaccess Protocols in Packet Communication Systems, IEEE Trans, on Commun., Vol. COM28, No. 4, April 1980, pp. 468-488 [41] F A. Tobagi, F. Borgonovo, L. Fratta: Expressnet: A High Performance Integrated-Services Local Area Network, IEEE J. Select. Areas Commun., Vol. SAC-1, No. 5, Nov. 1983, pp. 898-913 Technologien der Vermittlungstechnik in schnellen Kommunikationsnetzen	access network;apollonian network;assignment zero;coherent;global communications conference;integrated services digital network;lightwave 3d;optical switch;performance evaluation;software release life cycle;wake-on-lan;wavelength-division multiplexing	Gert J. Eilenberger;Dietrich Böttle;Thomas Dripke	1993	it+ti - Informationstechnik und Technische Informatik	10.1524/itit.1993.35.4.19	embedded system;computer science	Visualization	-45.23105556040522	-6.812660056139615	160566
a69aadc0ffab5343650ddaaaa766df473f842809	applying an innovative semantic sensor network model in internet of things	standards;semantics;resource description framework;semantics resource description framework ontologies standards interoperability data models;semantic sensor web semantic sensor network model internet of things world wide web open geospatial consortium ogc semantic web technology;web sites internet internet of things semantic networks;ontologies;interoperability;data models;semantic sensor networks semantic sensor web internet of things semantic web machine learning sensor web	It will not take a longtime that the human will apply the networks of sensors in controlling the environment and predict natural disasters. This vast function would create bulks of sensor data to be applied World Wide Web. These bulks of data should be handled and processed a primary manner, which is a difficult task; therefore only the expert in this field can process them. One solution to overcome this drawback is the application of Open Geospatial Consortium (OGC) standard which to the sensor networks would develop the concept of sensor web; hence access to the sensor observations through the web. Data semantic is prerequisite of this process. To overcome this drawback the combination of sensor web and semantic web technology that constitutes the new concept named semantic sensor web should be adopted. The objective of this article is to develop a semantic sensor web model and the challenges involved. Within it is found here that adopting this model is applicable in all sensors in a more rapid manner.	consortium;internet of things;machine learning;network model;semantic sensor web;semantic web;world wide web	Mohammadreza Rezvan;Mohammadamin Barekatain;Ahmad Zaeri;Kazem Taghandiki	2015	2015 International Conference on Information and Communication Technology Convergence (ICTC)	10.1109/ICTC.2015.7354556	web service;sensor web;semantic interoperability;semantic computing;web development;web modeling;data web;web of things;semantic search;semantic grid;web standards;computer science;semantic web;web navigation;social semantic web;linked data;data mining;semantic web stack;database;web intelligence;world wide web;semantic analytics;web coverage service	Mobile	-39.13512718219604	1.411926012956583	160691
e972a3dd608af6a60aa60a914680b65922b08217	a community-oriented workflow reuse and recommendation technique	hidden information;social networking;information extraction;social sciences;software modules;collaboration;data processing;scientific workflows;sna;earth science data;innovation;knowledge sharing;community oriented workflow reuse;social network analysis;distributed workflow development;software reuse;recommender systems;recommendation systems	NASA Earth Exchange (NEX) is a collaborative compute platform aiming to improve the availability of Earth science data, models, analysis tools and scientific results through a centralised environment that fosters knowledge sharing, collaboration, innovation and direct access to compute resources. One of the main objectives of NEX is to help Earth scientists leverage and reuse various data processing software modules developed by their peers, in order to quickly run value-added executable experiments (workflows). Toward this goal, this paper reports our efforts of leveraging social network analysis to intelligently extract hidden information from data processing workflows. By modelling Earth science workflow modules as social entities and their dependencies as social relationships, this research opens up new vistas for applying social science to facilitate software reuse and distributed workflow development. As a proof of concept, a prototyping system has been developed as a plug-in to the NEX workflow design and management system (VisTrails) to aid Earth scientists in discovering and reusing workflow modules and extending them to solve more complex science problems.	centralisation;code reuse;entity;executable;experiment;plug-in (computing);random access;real-time transcription;social network analysis;vistrails	Jia Zhang;Chris Lee;Petr Votava;Tsengdar J. Lee;Ramakrishna R. Nemani;Ian T. Foster	2015	IJBPIM	10.1504/IJBPIM.2015.071265	innovation;social network analysis;data processing;computer science;systems engineering;knowledge management;software engineering;data mining;database;management;information extraction;workflow engine;social network;collaboration;workflow technology	DB	-41.99759373780872	-0.792721165319108	160699
1ba303e45d4a9fb4357dfafcc198e07d7e9f3980	querying sensor data for semantic product memories				Christian Seitz;Christoph Legat;Jörg Neidig	2009		10.3233/978-1-60750-056-8-51	semantic computing;data mining;information retrieval;computer science	Robotics	-38.58797362930206	1.0907962965747446	161000
08b39018534ff581976b30a2b9121a4e107c309f	big data in wisdom manufacturing for industry 4.0		with the maturity of Internet and an aggressive push towards Internet of Things (IoT) as well as Industry 4.0, data has become more accessible and ubiquitous; the era of Big Data has come. Big Data analytics in manufacturing aims to upgrade manufacturing sector into a smarter level. Managing, correlating and extracting useful information from the Big Data will greatly promote manufacturing intelligence. However, research work exploiting Big Data has mainly focused on the social networking domain rather than manufacturing. In this paper, the authors survey the Big Data background of modern manufacturing and introduce the most recently developed paradigms. The Big Data reference architecture for wisdom manufacturing (WM) is proposed, which is based on social-cyber-physical system (SCPS). WM is a humans-computers-things collaborative manufacturing system with human as top decision-makers, smart things as the basis, computers' data processing as the core, and services as the purpose. And the possible implementation of Big Data analytics in future manufacturing are investigated and summarized	big data;capability maturity model;computer;cyber-physical system;industry 4.0;internet of things;reference architecture	Jiajun Zhou;Xifan Yao;Jianming Zhang	2017	2017 5th International Conference on Enterprise Systems (ES)	10.1109/ES.2017.24	data science;reference architecture;cloud computing;the internet;big data;industry 4.0;upgrade;social network;engineering;data processing	Robotics	-37.45572122464524	-6.211782508509239	161005
c08876bc0115c15d558a9b0c0b7c612f33edccf9	information-driven robotic sampling in the coastal ocean		1Department ofMarine Technology, NorwegianUniversity of Science andTechnology (NTNU), Trondheim,Norway 2Centre of AutonomousMarineOperations and Systems (AMOS), Trondheim,Norway 3Department ofMathematical Sciences, NorwegianUniversity of Science and Technology (NTNU), Trondheim,Norway 4SINTEFOceanAS, Trondheim,Norway 5Department of Biology, NorwegianUniversity of Science and Technology (NTNU), Trondheim,Norway 6University Centre in Svalbard (UNIS), Longyearbyen, Norway 7Underwater Systems and Technology Laboratory, Faculty of Engineering, University of Porto (UP), Portugal 8InterdisciplinaryCenter forMarine and Environmental Research (CIIMAR), UP, Portugal 9PhysicsDepartment, CESAM,University of Aveiro, Portugal 10Department of EngineeringCybernetics, NorwegianUniversity of Science and Technology (NTNU), Trondheim,Norway		Trygve Olav Fossum;Jo Eidsvik;Ingrid H. Ellingsen;Morten Omholt Alver;Glaucia M. Fragoso;Geir Johnsen;Renato Mendes;Martin Ludvigsen;Kanna Rajan	2018	J. Field Robotics	10.1002/rob.21805	library science;simulation;engineering;sampling (statistics);european union	Logic	-45.63318730781496	-9.233875025389073	161197
294e3b51e36973ac853a03bc124373bfdb2864a4	advanced design, analysis, and implementation of pervasive and smart collaborative systems enabled with knowledge modelling and big data analytics		NOTICE: this is the author’s version of a work that was accepted for publication in Advanced Engineering Informatics. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Advanced Engineering Informatics, [33, (2017)] DOI: 10.1016/j.aei.2017.01.001		Amy J. C. Trappey;Fredrik Elgh;Timo Hartmann;Anne James;Josip Stjepandic;Charles V. Trappey;P. M. Wognum	2017	Advanced Engineering Informatics	10.1016/j.aei.2017.01.001	data mining;data science;collaboration;systems engineering;big data;analytics;computer science	SE	-43.94265472659284	-0.8644434618448744	161256
4a25784fcf1e2691dd60d1236190e43be461778d	an xml-based language to connect netcdf and geographic communities	atmospheric science;geographic information;geometry;geography markup language;data model;multi dimensional;geology;geographic information systems;markup languages;data access;councils;atmospheric modeling;geographic information systems data models geology markup languages encoding geometry geography atmospheric modeling atmosphere councils;atmosphere;encoding;markup language;geospatial data;data models;geography	Geosciences and Geographic -information realms present several common aspects and share common data (i.e. they both generate, manage and use geospatial data). Nevertheless, they have been developing their own semantics, schemas and tools, which very often are not completely interoperable. That is the case of ncML (netCDF Markup Language) and GML (Geography Markup Language) content/semantics. Such issue is one of the main motivations for developing the presented ncML extension called ncML-GML. NcML-GML leverages the ncML ability to encode multidimensional arrays, and the wide acceptance of GML for encoding geospatial coverage. Based on GML 3.1.1 grammar, ncML-GML implements one of the possible encodings of a general interoperability model (i.e. both abstract and content interoperability models) which reconciles typical atmospheric sciences and geospatial data models. In this paper we describe the recently released version: ncML-GML ver. 0.7.3. The motivation, objectives and the newly-developed schemas are described and discussed. We also present the API (called N2G) developed to support ncML-GML documents as being used in the framework of GALEON IE – an OGC interoperability experiment which aims at extending the OGC WCS specification in order to support netCDF datasets. The role played by the ncMLGML in this experiment is presented and discussed. Keywords-geosciences data access; data model interoperability; science markup languages; netCDF; GML.	addresses (publication format);community;coverage data;data access;data model;data structure;earth sciences;encode (action);gis applications;galeon;geographic information systems;geographic information system;geography markup language;global variable;inactivator chemotaxis (function);interoperability;loose coupling;motivation;netcdf;programming languages;real-time computing;realms;slc25a11 gene;science;specification;test of visual evoked responses in children;ver (command);web coverage service;xml	Stefano Nativi;Lorenzo Bigagli;Ben Domenico;John Caron;Ethan Davis	2006	2006 IEEE International Symposium on Geoscience and Remote Sensing	10.1109/IGARSS.2006.632	geography markup language;computer science;geospatial analysis;data mining;database;markup language;information retrieval	SE	-37.677896087369156	-1.9339077737566481	161383
226a7b073eae151dae1d6cb586d9b8120810025d	an agent system for managing uncertainty in the integration of spatio-environmental data	geographic information system;information retrieval;large scale system;large scale;focal point;agent systems;fuzzy databases;mobile agent;fuzzy database;large scale systems;geography	Recent applications in environmental systems have necessitated the integration of data from multiple, heterogeneous sources. The integration process involves challenges related to issues of uncertainty and imprecision associated with both the data and the process itself. While the handling of uncertainty in geographical information systems (GIS) has been a focal point of research in recent years, the additional challenges of dealing with multiple data sources and types, as well as specific fields of analysis, lead to much more complex situations. In this paper, we present a framework for the use of fuzzy mobile agents to address these additional challenges from the standpoint of large-scale environmental systems.		Frederick E. Petry;Maria Cobb;Marcin Paprzycki;Dia Ali	2003	Soft Comput.	10.1007/s00500-002-0229-z	computer science;information integration;data mining;mobile agent;database;geographic information system;management science	Robotics	-36.66215066878937	-1.9974403672217136	161692
986ef8937f25bcb8f538f66c2031daabcbbc1589	middleware support to the specification and execution of active rules for biological database constraint management	biology computing;xml biology computing database management systems formal specification internet middleware user interfaces;formal specification;database management systems;web based interfaces middleware biological database constraint management xml based active rule biorule system biological semantics web based user friendly tool;internet;middleware databases;xml;middleware;biological data;biological database;user interfaces	Constraints are important to ensure consistency and validity of biological data to computerize biological findings. Incorrect data could lead to serious scientific problems when conducting further biology research and experiments. We propose an XML-based active rule system named BioRule to enforce constraints on top of existing data sources. The BioRule system serves as a middleware to filter inconsistent data before populating or updating data sources. Biological semantics are specified in active rules, while the validation can be enforced by the BioRule system automatically according to the defined rules. To facilitate and simplify the specification of active rules, we developed a web-based user-friendly tool to guide end-users. Rule examples, over an existing biology application, are also provided in this paper to illustrate the specification of active rules though web-based interfaces for constraint validation.	biological database;event condition action;experiment;middleware;population;usability;web application;xml	Ying Jin;Matthew L. Tescher;Huaqin Xu	2007	2007 IEEE International Conference on Information Reuse and Integration	10.1109/IRI.2007.4296654	middleware;the internet;xml;biological database;biological data;computer science;middleware;data mining;formal specification;database;user interface;world wide web	DB	-37.79352676977213	3.694089129784977	161867
b731435d12ed89ae413930779382503ee76e561a	studio e confronto delle strutture di apache spark		English. This document is designed to study the data structures that can be used in the Apache Spark framework and to evaluate the best performing ones to implement solutions, in particular we will evaluate advantages / disadvantages deriving from the use of Dataset for job creation. The observation of the results provides further support in evaluating the use of Dataset as an alternative to RDD, in order to understand its strengths and weaknesses. The examination of the results is possible thanks to specifically designed and implemented in Java 1.8 language. The execution of the jobs, entrusted to a suitable distributed environment, will end with the comparison between execution times and results obtained. Italiano. Il presente documento nasce allo scopo di studiare le strutture dati utilizzabili nel framework Apache Spark e valutare quelle più performanti per implementare soluzioni; valuteremo in particolare i vantaggi/svantaggi derivanti dall’utilizzo dei Dataset nella progettazione dei job. L’osservazione dei risultati fornisce ulteriore supporto nel valutare l’utilizzo dei Dataset in alternativa a RDD, al fine di comprederne i punti di forza e di debolezza. L’esame dei risultati è possibile in virtù di due casi appositamente pensati e implementati in linguaggio Java 1.8. L’esecuzione dei job, affidata a un adeguato ambiente distribuito, si concluderà con il confronto tra tempi di esecuzione e risultati ottenuti.	apache spark;data structure;java;job stream;naruto shippuden: clash of ninja revolution 3	Massimiliano Morrelli	2018	CoRR		database;spark (mathematics);pi;data structure;computer science;studio;java	Web+IR	-46.05807687060385	-2.254215287185401	162220
3bcaddd09c13be219c6b60b5ca79e5b85deda3e0	quantum unfolding: a program for unfolding electronic energy bands of materials	energy bands unfolding;first principle calculation	Fawei Zheng, Ping Zhang, and Wenhui Duan Institute of Applied Physics and Computational Mathematics, Beijing, People’s Republic of China, zheng fawei@iapcm.ac.cn Institute of Applied Physics and Computational Mathematics, Beijing, People’s Republic of China, zhang ping@iapcm.ac.cn Department of Physics, Tsinghua University, Beijing, People’s Republic of China, dwh@phys.tsinghua.edu.cn (Dated: March 5, 2014)	computation;computational mathematics;electronic band structure;quantum;unfolding (dsp implementation)	Fawei Zheng;Ping Zhang;Wenhui Duan	2015	Computer Physics Communications	10.1016/j.cpc.2014.12.009	simulation;computer science;theoretical computer science;mathematics;physics;algorithm;quantum mechanics	ML	-46.64162200243718	-8.214500499643075	162493
c746bf71729cc3597e9c16947157f84bccf62bd0	a systematic analysis of levels of integration between high-level task planning and low-level feasibility checks		Esra Erdem a,∗∗, Volkan Patoglu b and Peter Schüller c,∗∗∗ a Computer Sciences and Engineering, Faculty of Engineering and Natural Sciences, Sabancı University, İstanbul, Turkey E-mail: esraerdem@sabanciuniv.edu b Mechatronics Engineering, Faculty of Engineering and Natural Sciences, Sabancı University, İstanbul, Turkey E-mail: vpatoglu@sabanciuniv.edu c Department of Computer Engineering, Marmara University, İstanbul, Turkey E-mail: peter.schuller@marmara.edu.tr	computer engineering;computer science;high- and low-level;mechatronics	Esra Erdem;Volkan Patoglu;Peter Schüller	2016	AI Commun.	10.3233/AIC-150697	computer science;data mining	DB	-45.9840842450132	-8.613974130374277	162531
7abd28b2e892436cf514b06a6935de1e1a99da0e	jazzcats: navigating an rdf triplestore of integrated performance metadata		Applying Linked Data techniques to musical metadata can facilitate new paths of musicological inquiry. JazzCats: Jazz Collection of Aggregated Triples is a prototype project interlinking four discrete jazz performance datasets and external sources as references. Tabular, relational, and graph legacy datasets have necessitated different RDF production and ingestion workflows to support scholarly study of performance traditions. This paper highlights critical processes of data curation for digital libraries, including quality assessment of the ingested datasets. In addition, we describe research questions enabled by JazzCats, raise musicological implications, and offer suggestions to overcome current limitations.		Daniel Bangert;Terhi Nurmikko-Fuller;J. Stephen Downie;Yun Hao	2018		10.1145/3273024.3273031	rdf;world wide web;digital library;semantic web;linked data;metadata;sparql;triplestore;computer science;data curation	OS	-42.67973178366139	1.8114465918589706	162840
1981734f8ee73e9e0f2f8a7d1df7aa5851e564d7	tube (text-cube) for discovering documentary evidence of associations among entities	multi dimensional;numerical analysis;scientific computing;interactive ir;document retrieval;information system;association discovery	User-driven discovery of associations among entities, and documents that provide evidence for these associations, is an important search task conducted by researchers and do-main information specialists. Entities here refer to real or abstract objects such as people, organizations, ideologies, etc. Associations are the inter-relationships among entities. Most current works in query-driven document retrieval and finding representative subgraphs are ill-suited for the task as they lack an awareness of entity types as well as an intuitive representation of associations. We propose the TUBE model, a text cube approach for discovering associations and documentary evidence of these associations. The model consists of a multi-dimensional view of document data, a flexible representation of multi-document summaries, and a set of operations for data manipulation. We conduct a case study on real-life data to illustrate its applicability to the above task and compare it with the non-TUBE approach.	document retrieval;entity;real life	Hady Wirawan Lauw;Ee-Peng Lim;HweeHwa Pang	2007		10.1145/1244002.1244185	document retrieval;numerical analysis;computer science;data mining;database;world wide web;information retrieval;information system	Web+IR	-36.29354203145732	2.340693126468514	163062
6ad68224566b1fa8d370a00918fa50e6c083bc61	cross-layer design for the physical, mac, and link layer in wireless systems	signal image and speech processing;quantum information technology spintronics;cross layer design;link layer;wireless systems	1 Department of Electronic Systems, Aalborg University, Niels Jernes Vej 12, 9220 Aalborg Øst, Denmark 2 Oticon A/S, Kongebakken 9, 2765 Smørum, Denmark 3 School of Electrical and Computer Engineering, Georgia Institute of Technology, 777 Atlantic Drive NW, Atlanta, GA 30332-0250, USA 4 ArrayComm LLC, San Jose, CA 95131-1014, USA 5 Graduate School of Engineering, Osaka City University, Osaka-shi 558-8585, Japan 6 Department of Electrical and Computer Engineering, University of Cyprus, P.O. Box 20537, 1678 Nicosia, Cyprus	computer engineering;netware;software release life cycle	Petar Popovski;Mary Ann Ingram;Christian B. Peel;Shinsuke Hara;Stavros Toumpis	2009	EURASIP J. Adv. Sig. Proc.	10.1155/2009/528675	data link layer;link layer;telecommunications;computer science;wi-fi array;physical layer;network layer;computer network	Visualization	-45.16650629731089	-7.448921402873844	163128
3596f4d41d7e6a9ad337ce45105571a93b046f29	a live multimedia stream querying system	multimedia streaming;data stream;data embedding;manifold learning;data model;multimedia data;dimensionlaity reduction;media streaming;neighborhood graph	Querying live media streams captured by various sensors is becoming a challenging problem, due to the data heterogeneity and the lack of a unifying data model capable of accessing various multimedia data and providing reasonable abstractions for the query purpose. In this paper we propose a system that enables directly capturing media streams from sensors and automatically generating more meaningful feature streams that can be queried by a data stream processor. The system provides an effective combination between extendible digital processing techniques and general data stream management research.	data model;digital data;extensibility;sensor;stream processing	Bin Liu;Amarnath Gupta;Ramesh C. Jain	2005		10.1145/1160939.1160950	data stream clustering;computer science;data mining;database;data stream mining;world wide web	DB	-34.520013560429916	1.9840183076406739	163410
ebf2b6961d07642933622672358a9212a808fe5a	'favorite' sql-statements - an empirical analysis of sql-usage in commercial applications	empirical study;empirical analysis;software quality assurance;data collection;cost efficiency	An empirical study investigates usage of SQL in commercial applications of three large Austrian companies. Based on 38,000 statements we analyse the practical meaning of the DML-Part of SQL language constructs. A cost-efficient method of data collection for IBM-DB2 environments is described. We also propose a simple complexity scheme for classifying SQL statements and apply it to our data. Some of our findings are compared with SQL-features used in standard database benchmarks. Since empirical but non-laboratory results in this area are rare this study may be of general interest to the database community.	sql	Richard Pönighaus	1995		10.1007/3-540-60584-3_25	computer science;data science;data mining;operations research	SE	-38.649746573374415	-8.565958518424758	163556
41a8c56702511cbde151f7bcd6a8f3e2ad0e3d07	storing and indexing iot context for smart city applications		IoT system interoperability, data fusion, data discovery and access control for providing Context-as-a-Service as well as tools for building context-aware smart city applications are all significant research challenges for IoT-enabled smart cities. These middleware platforms have to cope with potentially big data generated from millions of devices in large cities. The amount of context, metadata, annotations in IoT ecosystems equals and may even exceed the amount of raw data. This paper discusses the challenges of context storage, retrieval and indexing for smart city applications. We analyse, compare and categorise existing approaches, tools and technologies relevant to the identified challenges. The paper proposes a conceptual architecture of a hybrid context storage and indexing mechanism that enables and supports the Context Spaces theory based representation of context for large-scale smart city applications. We illustrate the proposed approach using solid waste management system with adaptive on-demand garbage collection from IoT-enabled garbage bins.	smart city	Alexey Medvedev;Arkady B. Zaslavsky;Maria Indrawan;Pari Delir Haghighi;Alireza Hassani	2016		10.1007/978-3-319-46301-8_10	database;internet privacy;world wide web	HCI	-38.635253998376186	0.32613897369752703	163622
3fecb4dff60c76284c4975c183b806d1bf7ee1d9	the open annotation collaboration (oac) model	web of data;groupware;open systems formal specification groupware internet multimedia systems;formal specification;linked data;digital library;best practice;closed system;multimedia systems;resource use;internet;lessons learned;multimedia communication media collaboration transform coding data models semantics servers;open systems;use case;linked data effort open annotation collaboration model proprietary closed systems multimedia resources annotation linked data principles web standards annotation interoperability specifications	Annotations allow users to associate additional information with existing resources. Using proprietary and closed systems on the Web, users are already able to annotate multimedia resources such as images, audio and video. So far, however, this information is almost always kept locked up and inaccessible to the Web of Data. We believe that an important step to take is the integration of multimedia annotations and the Linked Data principles. This should allow clients to easily publish and consume, thus exchange annotations about resources via commonWeb standards.We first present the current status of the Open Annotation Collaboration, an international initiative that is currently working on annotation interoperability specifications based on best practices from the Linked Data effort. Then we present two use cases and early prototypes that make use of the proposed annotation model and present lessons learned and discuss yet open technical issues.	best practice;closed system;interoperability;java annotation;linked data;object action complex;world wide web	Bernhard Haslhofer;Rainer Simon;Robert Sanderson;Herbert Van de Sompel	2011	2011 Workshop on Multimedia on the Web	10.1109/MMWeb.2011.21	use case;digital library;the internet;computer science;linked data;data mining;formal specification;database;closed system;open system;world wide web;best practice	Web+IR	-43.461005209745345	2.601933651339653	163801
772d141eab7f4897476d8e984d62cf0bd41afe09	advance in safe and useful social network services with context-sensitive data in cyber-physical system		1 Department of Computer Engineering, Mokpo National University, Muan, Jeonnam 534729, Republic of Korea 2Department of Informatics, J. E. Purkinje University, Ceske mladeze 8, 400 96 Usti nad Labem, Czech Republic 3 AGH University of Science and Technology, 30 Mickiewicza Avenue, 30-059 Krakow, Poland 4 Institute of Engineering-Polytechnic of Porto, 4200-072 Porto, Portugal 5 GECAD-Knowledge Engineering and Decision Support Group, 4200-072 Porto, Portugal	computer engineering;cyber-physical system;informatics;knowledge engineering;social network	Jong-Myoung Choi;Hoon Ko;Marek R. Ogiela;Goreti Marreiros	2014	IJDSN	10.1155/2014/589276	world wide web;computer security;computer network	DB	-45.02554470569507	-8.136530796614595	164264
3c5158477ddad8fcbaf219eeb7387eb399d88aa9	data structures for databases		University of Florida 60.1 Overview of the Functionality of a Database Management System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60-1 60.2 Data Structures for Query Processing . . . . . . . . . . . . . . 60-3 Index Structures • Sorting Large Files • The Parse Tree • Expression Trees • Histograms 60.3 Data Structures for Buffer Management . . . . . . . . . . . 60-12 60.4 Data Structures for Disk Space Management . . . . . 60-14 Record Organizations • Page Organizations • File Organization 60.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60-21	algorithm;b-tree;british informatics olympiad;computer data storage;data mining;data structure;disk space;geographic information system;management system;oracle database;parse tree;r* tree;software system;sorting;xml	Joachim Hammer;Markus Schneider	2004		10.1201/9781420035179.ch60	parse tree;binary expression tree;database;histogram;sorting;data structure;protein structure database;computer science	DB	-43.32646243097454	-5.523017738833709	164321
3422b467ad59e5d6048a8d9e93b82c487709769a	linked data in drug discovery	drugs;linked data;drug discovery;medical computing data handling drugs;semantic web life sciences drugs knowledge discovery;biology;medical computing;innovation;biology linked data drug discovery chemistry;life sciences medical computing data handling drugs biology linked data drug discovery chemistry semantic web life sciences drugs knowledge discovery innovation drug discovery linked data semantic web;chemistry;life sciences;semantic web;data handling;knowledge discovery	Drug discovery presents many challenges, but several linked data initiatives are under way to address the huge increase in the amount of data available from chemistry, biology, and drug discovery in the past two decades. This Web extra provides a list of relevant resources for linked data in drug discovery.	linked data	Michel Dumontier;David J. Wild	2012	IEEE Internet Computing	10.1109/MIC.2012.122	innovation;computer science;bioinformatics;data science;semantic web;group method of data handling;linked data;data mining;knowledge extraction;world wide web;drug discovery	Comp.	-41.0222693453394	1.7065378994165787	164595
5e39e53f6fcb46884110c1489b2f55ad424ec292	identification and visualization of emerging trends from blogosphere		"""* # # + , + , + % , ## + , , , # + , + , , , , , , (# . / # % % %( ( , + # # ++( ##% 0 -10 , 2( %% (% ( % 3 , # , ##% + , %% """" # % 4 + # # , + , ( (%%"""	blogosphere	Makoto Uchida;Naoki Shibata;Susumu Shirayama	2007			data science;visualization;data mining;blogosphere;computer science	Visualization	-39.48251849983415	-7.763067903027103	165003
8aca3f807569183aa5fcf401dd75d17c306d0709	"""""""what is a good digital library?"""" - a quality model for digital libraries"""	evaluation performance;indicator;performance evaluation;life cycle;indicateur;information life cycle;digital library;digital libraries;evaluacion prestacion;qualite service;biblioteca electronica;quality criterion;quality evaluation;critere qualite;quality;theory;indicador;evaluation;electronic library;quality model;service quality;bibliotheque electronique;criterio calidad;calidad servicio	In this article, we elaborate on the meaning of quality in digital libraries (DLs) by proposing a model that is deeply grounded in a formal framework for digital libraries: 5S (Streams, Structures, Spaces, Scenarios, and Societies). For each major DL concept in the framework we formally define a number of dimensions of quality and propose a set of numerical indicators for those quality dimensions. In particular, we consider key concepts of a minimal DL: catalog, collection, digital object, metadata specification, repository, and services. Regarding quality dimensions, we consider: accessibility, accuracy, completeness, composability, conformance, consistency, effectiveness, efficiency, extensibility, pertinence, preservability, relevance, reliability, reusability, significance, similarity, and timeliness. Regarding measurement, we consider characteristics like: response time (with regard to efficiency), cost of migration (with respect to preservability), and number of service failures (to assess reliability). For some key DL concepts, the (quality dimension, numerical indicator) pairs are illustrated through their application to a number of ‘‘real-world’’ digital libraries. We also discuss connections between the proposed dimensions of DL quality and an expanded version of a workshop’s consensus view of the life cycle of information in digital libraries. Such connections can be used to determine when and where quality issues can be measured, assessed, and improved – as well as how possible quality problems can be prevented, detected, and eliminated. 2006 Elsevier Ltd. All rights reserved.	accessibility;composability;conformance testing;digital library;extensibility;library (computing);numerical analysis;relevance;response time (technology);spaces;virtual artifact	Marcos André Gonçalves;Bárbara Lagoeiro Moreira;Edward A. Fox;Layne T. Watson	2007	Inf. Process. Manage.	10.1016/j.ipm.2006.11.010	biological life cycle;digital library;computer science;evaluation;world wide web;service quality;theory	DB	-45.84589247783846	2.2479867264650233	165013
3a35c0b78b7842ff60da4386f0097f42ee9fff19	qos multimedia multicast routing		Ion Mandoiu, Alex Olshevsky, Alex Zelikovsky 1 Department of Computer Science, University of Connecticut Storrs, CT 06269-2155, E-mail: ion@engr.uconn.edu 2 Laboratory for Information and Decision Systems, Massachusetts Institute of Technology Cambridge, MS 02139, E-mail: alex o@mit.edu 3 Department of Computer Science, Georgia State University Atlanta, GA 30303, E-mail: alexz@cs.gsu.edu September 12, 2005	computer science;multicast;quality of service;routing;software release life cycle	Ion I. Mandoiu;Alexander Olshevsky;Alex Zelikovsky	2007		10.1201/9781420010749.ch71	multicast;xcast;mobile qos;protocol independent multicast;computer network;source-specific multicast;inter-domain;distance vector multicast routing protocol;mathematics;pragmatic general multicast	Theory	-45.06662186994748	-7.060420112376528	165124
511b9ecbb475ef053ee0f2562db68a0123eea219	a model base for identifying mathematical programming structures	mathematical programming	Jae Sik Lee has completed his dissertation in the Department of Decision Sciences at the Wharton School of the University of Pennsylvania. He holds an M.S. degree in Industrial Sciences and a B.B.A degree. He has published papers on heuristic algorithm and model management and presented talks on model management systems at national conferences. He is now an assistant professor in the Ajou University in Korea. Mathematical programming models have been playing important roles in complex decision making situations for many years. While management scientists and operations researchers have built mathematical programming models, for a little more than a decade researchers in the information systems field have studied the way to manage those models efficiently: the Model Management Systems (MMS). Generally, the functions of MMS include Model Building, Model Representation, Interface with Database, Model Storage, Model Retrieval, Model Maintenance, Link between Models and Algorithms, and Model Solving. While practitioners have developed and used their matrix generators, e.g., GAMMA (developed by Bonner and Moore), MAGEN, PDS, OMNI (developed by Haverly Systems), to solve mathematical programming models for real world problems, theoreticians have tried to develop a conceptual framework of MMS applying characteristics similar to those of database management systems. For example, Elam et al. [5,6,7,8] employed the entity-relationship approach utilizing knowledge engineering concepts, Blanning [2,3,4] used the relational approach, and Konsynski and Dolk [11,12,13] adopted CODASYL's network approach. Geoffrion [9] suggested a unified framework for systematizing the model formulation process. In this note, we will focus on how models should be stored to fully utilize the functions of MMS. In section 2, the important role of Model Storage, especially when mathematical programming models are involved, will be discussed. A language we developed for storing mathematical programming models will be introduced in section 3. In sections 4, 5 and 6, we will propose an efficient way to store mathematical programming models utilizing a data base and a knowledge base.	algorithm;codasyl;database model;durchmusterung;entity–relationship model;gamma correction;heuristic (computer science);information system;knowledge base;knowledge engineering;mathematical optimization;unified framework	Jae Sik Lee	1991	Decision Support Systems	10.1016/0167-9236(91)90049-H	programming domain;reactive programming;functional reactive programming;computer science;machine learning;procedural programming;symbolic programming;inductive programming;algorithm	AI	-43.06518666998345	-5.890834840469645	165252
d90d28e78e6da26ca6ee5e6cb240e8df54ce052f	research on the extended metadata model of methodological digital resources				Svetlana Kubilinskiene;Valentina Dagiene	2012		10.3233/978-1-61499-161-8-371	information retrieval;metadata modeling;metadata;meta data services;data element;computer science	Theory	-42.48013009150151	1.22864920059929	165343
02a0f19a4a39e86013e00437284564de8e2e47f2	genie: an inference engine with diverse applications.			genie;inference engine	F. Brundick;John Dumer;Tim Hanratty;P. Tanenbaum	1985			bioinformatics;data mining;world wide web	Arch	-40.017421955742385	-6.942723971634097	165448
e3f98b6748dc5ca23e54e92b92f06e64734bec19	a survey of the semantic specification of sensors	selected works;bepress	Semantic sensor networks use declarative descriptions of sensors promote reuse and integration, and to help solve the difficulties of installing, querying and maintaining complex, heterogeneous sensor networks. This paper reviews the state of the art for the semantic specification of sensors, one of the fundamental technologies in the semantic sensor network vision. Twelve sensor ontologies are reviewed and analysed for the range and expressive power of their concepts. The reasoning and search technology developed in conjunction with these ontologies is also reviewed, as is technology for annotating OGC standards with links to ontologies. Sensor concepts that cannot be expressed accurately by current sensor ontologies are also discussed.	ontology (information science);semantic sensor web	Michael Compton;Cory A. Henson;Holger Neuhaus;Laurent Lefort;Amit P. Sheth	2009			sensor web;computer science;data mining;world wide web;information retrieval	AI	-39.11920101280296	0.963007565699172	165458
1054e25fc3839870da0a3eb3b13bbe45a8656e2a	semantic-aware blocking for entity resolution	semantics erbium faa;entity resolution semantic aware blocking large scale data environment similarity spaces record semantic similarity similarity metrics er blocking process semantic features textual features lsh techniques locality sensitive hashing techniques;records management data handling	In this paper, we propose a semantic-aware blocking framework for entity resolution (ER). The proposed framework is built using locality-sensitive hashing (LSH) techniques, which efficiently unifies both textual and semantic features into an ER blocking process. In order to understand how similarity metrics may affect the effectiveness of ER blocking, we study the robustness of similarity metrics and their properties in terms of LSH families. Then, we present how the semantic similarity of records can be captured, measured, and integrated with LSH techniques over multiple similarity spaces. In doing so, the proposed framework can support efficient similarity searches on records in both textual and semantic similarity spaces, yielding ER blocking with improved quality. We have evaluated the proposed framework over two real-world data sets, and compared it with the state-of-the-art blocking techniques. Our experimental study shows that the combination of semantic similarity and textual similarity can considerably improve the quality of blocking. Furthermore, due to the probabilistic nature of LSH, this semantic-aware blocking framework enables us to build fast and reliable blocking for performing entity resolution tasks in a large-scale data environment.	blocking (computing);erdős–rényi model;experiment;knowledge base;locality of reference;locality-sensitive hashing;scalability;semantic similarity;lsh	Qing Wang;Mingyuan Cui;Huizhi Liang	2016	IEEE Transactions on Knowledge and Data Engineering	10.1109/ICDE.2016.7498378	semantic similarity;computer science;data mining;database;world wide web	DB	-35.45887115901671	2.362008689486564	165970
6759ac1bb4f5b751d0f79af082500d5888b26001	increasing the financial transparency of european commission project funding	european commission;ontowiki;limes;linked open data;financial transparency system	The Financial Transparency System (FTS) of the European Commission contains information about grants for European Union projects starting from 2007. It allows users to get an overview on EU funding, including information on beneficiaries as well as the amount and type of expenditure and information on the responsible EU department. The original dataset is freely available on the European Commission website, where users can query the data using an HTML form and download it in CSV and most recently XML format. In this article, we describe the transformation of this data to RDF and its interlinking with other datasets. We show that this allows interesting queries over the data, which were very difficult without this conversion. The main benefit of the dataset is an increased financial transparency of EU project funding. The RDF version of the FTS dataset will become part of the EU Open Data Portal and eventually be hosted and maintained by the European Union itself.	algorithm;display resolution;download;eu open data portal;fleet telematics system;form (html);html;php;resource description framework;software release life cycle;vocabulary;xml	Michael Martin;Claus Stadler;Philipp Frischmuth;Jens Lehmann	2014	Semantic Web	10.3233/SW-130116	public relations;data mining;business;world wide web	Web+IR	-42.497844060326116	-4.001717321242813	166065
c855cd284490fc5de8bb9ae141c53893cb516a07	a framework for maintaining provenance information of cultural heritage 3d-models	3d models production workflow;virtual heritage;data provenance	The advances in 3D digitizing technology have found significant application in the Cultural Heritage domain. The systematic large-scale production of digital cultural objects, the diversity of the processes involved and the complexity of describing historical relationships among them imposes the need for innovative knowledge management to handle all the semantic information in order to monitor, manage and document the origins and derivation of digital products. Flexibility is also very important for the enrichment and sharing of the acquired knowledge. The latter can be exploited by all members of the Cultural Heritage scientific community who can take advantage of the recorded provenance information for evaluating the 3D representation, tracking reliability of production, detecting tolerance and accuracy in acquisition and processing phases, documenting digitization techniques, studying evidence of historic and cultural events. In this paper we present a framework that supports the documentation, archiving and dissemination of data and metadata of 3D digital cultural heritage objects, enhancing the OAIS approach by permitting the ingest of all the involved data, not only the final result, during a workflow progress.	3d modeling;3d scanner;archive;gene ontology term enrichment;knowledge management;open archival information system;sensor;software documentation	Martin Doerr;Ioannis Chrysakis;Anastasia Axaridou;Maria Theodoridou;Christos Georgis;Emmanuel Maravelakis	2014		10.14236/ewic/eva2014.32	engineering;knowledge management;data mining;world wide web	HCI	-43.469055048436886	0.1718953572689765	166283
f6231e824fff6c1c6981044fbf91249cfd1f0f73	sound and mechanised compositional verification of input-output conformance	input output conformance;conformance verification;csp;compositional conformance	1Centro de Informática, Universidade Federal de Pernambuco, P.O. Box 7851, 50740-540 Recife, Pernambuco, Brazil 2Departamento de Informática e Estatística, Universidade Federal Rural de Pernambuco, Dois Irmãos 52171-900 Recife, Pernambuco, Brazil 3Research Institute for Secure Systems, National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba Central 2, 1-1-1, Umezono, Tsukuba, Ibaraki, 305-8568 Japan	conformance testing	Augusto Sampaio;Sidney Nogueira;Alexandre Mota;Yoshinao Isobe	2014	Softw. Test., Verif. Reliab.	10.1002/stvr.1498	reliability engineering;real-time computing;computer science;communicating sequential processes;conformance testing;programming language;algorithm	Logic	-46.69653801642608	-5.340562514755512	166534
2685f9c6d98377d6cfb0949e921331059ff3c880	the parallel asynchronous data routing environment padre	lanl	""" Kei Davis and Daniel QuinlanComputing, Information, and Communications, Scientic Computing Group CIC-19,Los Alamos National Laboratory, Los Alamos, NM 87545, USA,WWW home page: <a href=""""http://citeseer.ist.psu.edu/rd/0/http%3AqSqqSqwww.c3.lanl.govqSqcic19qSqteamsqSqnapcqSq"""" onmouseover=""""self.status=""""http://www.c3.lanl.gov/cic19/teams/napc/""""; return true"""" onmouseout=""""self.status=""""""""; return true"""">http://www.c3.lanl.gov/cic19/teams/napc/</a>1  """	routing	Kei Davis;Daniel J. Quinlan	1998		10.1007/3-540-49255-0_146	parallel computing;real-time computing;computer science;distributed computing	EDA	-45.49868525537878	-6.710445259179962	166553
015f1c8a05724853834cdf970a5c8422bc4d688f	discharge and referral data exchange using global standards - the sciphox project in germany	hospital information system;compvterized;information model;electronic messages;computerized;medical information science;shared care;informing science;data exchange;clinical document architecture;hl7;information exchange;xml;information system;clinical documentation;markup language;standardization;internal standard	"""The goal of the German project """"Standardization of Communication between Information Systems in Physician Offices and Hospitals using XML"""" (aka SCIPHOX) is to provide an XML based information exchange between Hospital Information Systems (HIS) and Physician Office Systems (POS). HL7's Clinical Document Architecture (CDA) was chosen to serve as the """"backbone"""" specification. The CDA is an ANSI approved document architecture for exchange of clinical information using XML. The SCIPHOX proposal specifies the use of the CDA in the context of discharge and referral letters in Germany, taking local needs (insurance information etc.) into account."""		Kai U. Heitmann;Ralf Schweiger;Joachim Dudeck	2002	Studies in health technology and informatics	10.1016/S1386-5056(03)00036-4	data exchange;xml;information exchange;information model;computer science;data mining;internal standard;database;markup language;world wide web;information system;standardization	DB	-46.645618462702764	3.5070569509248575	166604
945f4ba95dd6a25fb35262edb15316eebef84f0a	a cloud framework for big data analytics workflows on azure.		Since digital data repositories are more and more massive and distributed, we need smart data analysis techniques and scalable architectures to extract useful information from them in reduced time. Cloud computing infrastructures offer an effective support for addressing both the computational and data storage needs of big data mining applications. In fact, complex data mining tasks involve dataand compute-intensive algorithms that require large and efficient storage facilities together with high performance processors to get results in acceptable times. In this chapter we present a Data Mining Cloud Framework designed for developing and executing distributed data analytics applications as workflows of services. In this environment we use data sets, analysis tools, data mining algorithms and knowledge models that are implemented as single services that can be combined through a visual programming interface in distributed workflows to be executed on Clouds. The first implementation of the Data Mining Cloud Framework on Azure is presented and the main features of the graphical programming interface are described.	algorithm;application programming interface;big data;central processing unit;cloud computing;computer data storage;data mining;digital data;graphical user interface;knowledge representation and reasoning;microsoft azure;scalability;visual programming language	Fabrizio Marozzo;Domenico Talia;Paolo Trunfio	2012		10.3233/978-1-61499-322-3-182	database;big data;computer science;cloud computing;distributed computing;workflow	HPC	-35.3057252745547	-0.4659688156011327	166857
d2a34eb5af2d2a0d3a8cf32f1b814d406fe78492	spatio-temporal aggregation of european air quality observations in the sensor web	geoprocessing web;spatio temporal aggregation;air quality monitoring;sensor web	An increasing amount of observations from different applications such as long-term environmental monitoring or disaster management is published in the Web using Sensor Web technologies. The standardization of these technologies eases the integration of heterogeneous observations into several applications. However, as observations differ in spatio-temporal coverage and resolution, aggregation of observations in space and time is needed. We present an approach for spatio-temporal aggregation in the Sensor Web using the Geoprocessing Web. In particular, we define a tailored observation model for different aggregation levels, a process model for aggregation processes and a Spatio-Temporal Aggregation Service. The presented approach ∗Corresponding author. Current affiliation: Institute for Geoinformatics, University of Muenster, Weseler Strasse 253, 48151 Muenster, Germany. Tel.: +49 251 8331965. Fax.: +49 251 8339763 Email addresses: staschc@uni-muenster.de (Christoph Stasch), foerster@uni-muenster.de (Theodor Foerster), autermann@uni-muenster.de (Christian Autermann), e.pebesma@uni-muenster.de (Edzer Pebesma), pebesma@52north.org (Edzer Pebesma) Preprint submitted to Computers & Geosciences August 16, 2011 is demonstrated by a case study of delivering aggregated air quality observations on-demand in the Sensor Web.	email;fax;geoinformatics;geoprocessing;process modeling;rss;sensor web;world wide web	Christoph Stasch;Theodor Foerster;Christian Autermann;Edzer J. Pebesma	2012	Computers & Geosciences	10.1016/j.cageo.2011.11.008	sensor web;computer science;data mining;world wide web;computer security	ML	-42.130285170832856	-1.9489295424910749	167051
66eede27b03db21ea3d3922256212572ec321856	considerations on spatial data and the development of gis	metric space;spatial data;multi dimensional;spatial relation;dynamic analysis	"""We discuss the following theoretical questions mgurding 01s spatinl dnita in this pupei': the integration of spulinl data concepts, the quantity of spatitil data. the structure or spatial datu, oriented multi-diiilcnsional und dynamic analysis, and the initinlizutioii and mensureinent of GIS spatial dsta. In this paper, we discuss the problems that occur when using GIS to resolve these questions. Bused on the analysih of the necersary spatial data for GIs, Four types of sputial mlation detn wwe defined: location, adjacency. nearness, and influence. These types am ordered by increasing data quantity. The tvgnnizalion of all these data roniprehcnsively and appriiwtly is very difficult work, and is the soui'cc of dntil problems. such as """"not prepatacl when needed '' und """"ptepmd but nerw used """". As a mathad uf itsolving these problem% the feasibility and features oFMnp Algebra UIP discussed."""	geographic information system;spatial analysis;uip	Peng Hu;Shengquan Li;Jiangmei Kang	2005	Annals of GIS	10.1080/10824000509480602	spatial relation;metric space;geospatial analysis;mathematics;spatial analysis;dynamic program analysis;remote sensing	DB	-38.15770869088436	-3.50714410326428	167084
68339bd735eab3f97307d86898d73d29d68153f1	enhancing multilabel classification for food truck recommendation				Adriano Rivolli;Carlos Soares;André Carlos Ponce de Leon Ferreira de Carvalho	2018	Expert Systems	10.1111/exsy.12304	computer science;data mining;truck	AI	-39.48950170222773	-8.540642341541902	167516
56fff2884bf861b09b678f1f8ba4cd81e8ffffa5	05271 report on the dagstuhl seminar on semantic grid - convergence of technologies	004;semantic grid semantic web grid web services agents peer to peer;semantic grid	The scientific paradigms of the Semantic Web, Web Services,#R##N#Agents, Peer-to-Peer Networks and Grid Computing are currently#R##N#receiving a lot of attention in the research community, and are#R##N#producing solutions to important problems ranging from e-science#R##N#to e-business. The United States DAML program, the European#R##N#Commission and other organisations have also been investing#R##N#heavily in these technologies. This Dagstuhl Seminar brought#R##N#together world-leading experts from the diverse organizations and#R##N#research areas. It strengthened the international collaboration#R##N#with the aim to realize the vision of the Semantic Grid.		York Sure-Vetter;Carole A. Goble;Carl Kesselman	2005			semantic computing;semantic grid;computer science;knowledge management;semantic web;social semantic web;semantic web stack;database;world wide web;semantic analytics;grid computing	HPC	-47.38961580847675	1.353384428083422	167566
efa631b23a35d2f655fbbf31bd2e0d5cb32424f0	querying web data		As data expands into big data, enhanced or entirely novel data mining algorithms often become necessary. The real value of big data is often only exposed when we can adequately mine and learn from it. We provide an overview of new scalable techniques for knowledge discovery. Our focus is on the areas of cloud data mining and machine learning, semi-supervised processing, and deep learning. We also give practical advice for choosing among different methods and discuss open research problems and concerns.	algorithm;big data;data mining;deep learning;machine learning;open research;scalability;semiconductor industry	Andrea Calì	2015			data web;static web page;web development;web api;database;world wide web;computer science;web mining;web page;web mapping;semantic web stack	ML	-36.65021564820588	-6.439129215739701	167614
2947c6732139c7cadf471f6335f4e31fb5ba0e0d	design of an integrated dbms to support advanced applications.		New applications of DBMS’s in areas of sciences, engineering and offices have produced new requirements that are not satisfied in current DBMS’s. Included among these requirements are support for both the normalized and non-normalized models directly at the system interface level, support for text processing, and support of the temporal domain. To provide these supports, one can try to build additional functions on top of or into an existing DBMS. This approach has been deemed to be inefficient. It is believed that much can be gained by designing a new system to satisfy the new requirements more directly.		Vincent Y. Lum;Peter Dadam;R. Erbe;Jürgen Günauer;Peter Pistor;Georg Walch;H. Werner;John Woodfill	1985		10.1007/978-3-642-70284-6_26	database;computer science	DB	-36.43011454069533	0.32049661757698616	167760
f84e7ccf2d8a4144f886f197360f82e652b65fd9	automatic translation of programs from one computer to another				Ascher Opler;D. Farbman;M. Heit;W. King;E. O'Connor;Roy Goldfinger;H. Landow;J. Ogle;D. Slesinger	1962			programming language;computer-assisted translation;computer science	Arch	-46.65745210638851	-2.427521925500231	167892
e66173e6aa1db476cd847e3142dd1e92c87af735	data harmonisation put into practice by the humboldt project		Data harmonisation is a key prerequisite for an efficient and meaningful combination of heterogeneous information in cross-border applications and spatial data infrastructures. This is also the main objective of the INSPIRE Directive which has entered its implementation phase. Data Specifications for INSPIRE Annex I data themes have been published containing harmonised, panEuropean data models and a number of other requirements. Data providers across Europe face the challenge of transforming their legacy data to comply with these Data Specifications. This paper presents results of the European project HUMBOLDT. Data harmonisation requirements identified in nine scenarios covering a wide range of application domains and using heterogeneous data from a number of European countries are illustrated. Processes required to achieve data harmonisation are described from an application point of view. The open-source software framework for data harmonisation and services integration developed in the project is introduced and its use in two application scenarios is demonstrated.	data model;directive (programming);infrastructure for spatial information in the european community;open-source software;requirement;software framework	Astrid Fichtinger;Joachim Rix;Ulrich Schäffler;Ines Michi;Moses Gone;Thorsten Reitz	2011	IJSDIR		computer science;software engineering;data mining;database	DB	-41.26594918654635	-0.8807563651206946	168236
fad6777a813fe8ee7f75a9cc03d3feed0ba41ba1	providing structural computing services on the world wide web	software systems;of research and development;world wide web	The World Wide Web is one of the most successful software systems. The web provides a simple, extensible, and standardized hypermedia platform with millions of users that have access to millions of servers holding billions of documents. Hence, an increasing number of researchers and developers are making their systems and services available on the web. In conformance with this trend, this paper describes the first important results in the ongoing effort to provide the Construct structural computing services on the web. The paper is organized into five parts: an introduction to the research area, a brief overview of the Construct structural computing environment, a detailed description of the completed development effort to provide the Construct metadata services on the web, a quick overview of ongoing and future work in this area, and finally, our conclusions.	world wide web	Uffe Kock Wiil;David L. Hicks	2001		10.1007/3-540-45844-1_16	web service;web application security;web development;web modeling;web mapping;web-based simulation;web design;web accessibility initiative;web standards;computer science;ws-policy;semantic web;web navigation;social semantic web;data mining;database;services computing;web intelligence;web engineering;web 2.0;world wide web;web server	DB	-41.550342421752745	2.915352502568976	168487
9d824c04ccea65f7cc622513a77273e87e04b46a	new challenges of astroinformatics - stark-b database and serbian virtual observatory - servo, and relations to european virtual atomic data center - vamdc	astroinformatics;data collection;data mining;virtual observatory;digital archive;data communication;data center;atomic and molecular data;stark broadening;molecular data;atomic data;stellar evolution;virtual observatories	The development of space born astronomy, providing a huge amount of high quality astronomical data created an information avalanche and leaded to the formation of huge data collections. In order to address the problem how to analyse such amount of data, the idea of Virtual Observatory was formulated at the end of 2000, and from 2001 the FP5 project Astrophysical Virtual Observatory -- AVO was the basis for creation of European Virtual Observatory - EURO-VO (http://www.euro-vo.org).  SerVO - Serbian virtual observatory (http://www.servo.aob.rs/~darko) is a project created in 2008, with the objectives: a) Establishing SerVO and join the EuroVO and IVOA; b) Establishing SerVO data Center for digitizing, archiving and publishing in VO format photo-plates and other astronomical data produced at Belgrade Astronomical Observatory; c) Development of tools for visualization of data; d) Publishing, together with Observatoire de Paris, STARK-B - Stark broadening data base containing as the first step Stark broadening parameters obtained within the semiclassical perturbation approach by two of us (MSD-SSB) in VO compatible format; e) Make a mirror site for DSED (Darthmouth Stellar Evolution Database in the context of VO.  In order to enable an efficacious and convenient search for available atomic and molecular data, to build a secure, flexible and interoperable e-science environment based interface to the existing Atomic and Molecular databases and solve the existing problems in A&M data community, preventing productive search and data mining, the FP7 founded project Virtual Atomic and Molecular Data Center (VAMDC) started on July 1 2009. The core of the VAMDC e-infrastructure is the databases upon which it is based, and our contribution to the VAMDC e-infrastructure are the STARK-B database (http://stark-b.obspm.fr), a collaborative project between Laboratoire d'Etude du Rayonnement et de la matière en Astrophysique of the Observatoire de Paris-Meudon and the Astronomical Observatory of Belgrade. This is a database of the theoretical widths and shifts of isolated lines of atoms and ions due to collisions with charged perturbers, obtained within the impact approximation.  We review here SerVO, STARK-B and VAMDC projects within the context of e-science in Astronomy --- Astroinformatics.	approximation;archive;astroinformatics;astrophysical virtual observatory;cyberinfrastructure;data center;data mining;database;display resolution;e-science;emoticon;euro-vo;interoperability;linear algebra;semiclassical physics;stellar (payment network);super smash bros.	Milan S. Dimitrijevic;Sylvie Sahal-Bréchot;Andjelka Kovacevic;Darko Jevremovic;Luka C. Popovic	2011		10.1145/2023607.2023612	stellar evolution;data center;simulation;astroinformatics;computer science;artificial intelligence;operating system;database;stark effect;law;world wide web;statistics;data collection	DB	-44.52501429661085	-5.509292153761435	168967
624a71eb0525bf63fb813278d0d689545db5edb9	program committee		Rong-Guey Chang, National Chung Cheng University, Taiwan Houcine Hassan, Universidad Politécnica de Valencia, Spain Julio Sahuquillo, Universidad Politécnica de Valencia Ruo Ando, National Institute of Information and Communication Technology, Japan Zhiyang Li, Dalian Maritime University, China Dong Xiang, Tsinghua University Kuei-Ping Shih, Tamkang University Carmen Fernández-Gago, University of Malaga Julio Hernandez-Castro, School of Computing, University of Kent, United Kingdom Juan E. Tapiador, Department of Computer Science, Universidad Carlos III de Madrid Zheng Yan, Aalto University Jinhua Guo, University of Michigan-Dearborn Khaled Salah, Khalifa University of Science, Technology and Research Miroslaw Kutylowski, Wroclaw University of Technology Martin Middendorf, University of Leipzig Peter Gutmann, University of Auckland Dhiraj Pradhan, UOB Theo Ungerer, University of Augsburg Ruben Rios, University of Malaga Huanyu Zhao, Yahoo! Inc Marco Casassa Mont, Hewlett-Packard Labs Patricia Arias Cabarcos, University Carlos III of Madrid Fatih Turkmen , Technical University of Eindhoven Muhammad Khurram Khan, King Saud University Sokratis Katsikas, University of Piraeus, Dept. of Digital Systems Nora Cuppens-Boulahia, Telecom Bretagne Stefano Paraboschi, Universita di Bergamo Jay Ligatti, University of South Florida Wenbing Zhao, Cleveland State University Gerardo Pelosi, Politecnico di Milano Roberto Di Pietro, Bell Labs Susan Donohue, The College of New Jersey Xinyi Huang, Fujian Normal University, China Shawkat Ali, Senior Lecturer Willy Susilo, University of Wollongong Jigang Liu, Metropolitan State University Xiaoguang Mao, Department of Computer Science, National University of Defense Technology Claudio Agostino Ardagna, Universita' degli Studi di Milano Dipartimento di Tecnologie dell'Informazione Jinhu Lu, Chinese Academy of Sciences, China Chih Hung Wang, Department of Computer Science and Information Engineering, National Chiayi University Luis Gomes, Universidade Nova de Lisboa	tadashi watanabe	François Abel;Vikas Aggarwal;Jude Angelo Ambrose;J. Anderson;Suhaib A. Fahmy;Haohuan Fu;Frank Hannig;Dirk Koch;Krzysztof Kuchcinski;Philip Leong;Wayne Luk;Terrence Mak;H. -W. Kong;Andrey Mokhov;J Muller;Soojung Ryu;Chih Hung Wang;Qiang Wang;Andreas Agne;Giovanni Agosta;Tanvir Ahmed	2005		10.1109/ICHIS.2005.90		Crypto	-45.91187404955971	-9.698610042234883	169065
2a93f7449a08a412664a6cd7360c3dbff8486fc4	an empirical analysis of semantic techniques applied to a network management classification problem	owl;axioms;swrl;social networking online data integration knowledge representation languages pattern classification query processing semantic networks;network performance management;arq network management classification problem heterogeneous data empirical analysis specific data integration data classification problems semantic techniques telecommunication domain network node health status performance management instance data pm instance data owl2 axioms sparql queries swrl rules data set sizes pellet;sparql;network performance management owl swrl sparql axioms rules queries;queries;rules	"""Semantic technologies are increasingly being employed to integrate, relate and classify heterogeneous data from various problem domains. To date, however, little empirical analysis has been carried out to help identify the benefits and limitations of different semantic approaches on specific data integration and classification problems. This paper evaluates three alternative semantic techniques for performing classification over data derived from the telecommunications domain. The problem of interest involves inferring the """"health"""" status of network nodes (femtocells) from synthesized performance management (PM) instance data based on the operational PM schema. The semantic approaches used in the comparison include OWL2 axioms, SPARQL queries and SWRL rules. Empirical tests were performed across a range of data set sizes, using Pellet for axioms and rules and ARQ for queries. The experimental results provide (mostly) quantitative and (some) qualitative indication of the relative merits of each approach. Key among these findings is confirmation of the clear superiority of queries over rules and axioms in terms of raw performance and scalability."""	automatic repeat request;field (computer science);problem domain;sparql;scalability;semantic web rule language;statistical classification;web ontology language	Aidan Boran;Ivan Bedini;Christopher J. Matheus;Peter F. Patel-Schneider;Stefan Bischof	2012	2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2012.177	computer science;sparql;artificial intelligence;data mining;database;axiom;world wide web;information retrieval	AI	-36.641077321928606	1.4985920027530057	169188
1da5f03e38df3c6a6a3d9beb1901a4a4bf957b7e	interactive querying and data visualization for abuse detection in social network sites	social network services;computer architecture;ibcn;internet;engines;big data;data visualization;distributed databases	Big Data technologies have traditionally operated in an offline setting, collecting large batches of information on clusters of commodity machines and performing complex and time-consuming computations over it. While frameworks following this approach served well for most applications involving big data analysis during the last decade, other use cases have recently emerged posing challenging requirements on latency and demanding real-time data processing, querying and visualization. That is the case for applications aiming at detecting threatening behaviors in social network platforms, where timely action is required to avoid adverse consequences. In this sense, more and more attention has been drawn towards online data processing systems claiming to address the limitations of batch-oriented frameworks. This paper reports a work in progress on distributed data processing for enabling low-latency querying over big data sets. Two software architectures are discussed for addressing the problem and an experimental evaluation is performed on a proof of concept implementation showing how an approach based on query pre-processing and stateful distributed stream computation can meet the requirements for supporting interactive querying on large and continuously generated data.	architectural pattern;batch processing;big data;bridging (networking);computation;cyberbullying;data visualization;data-intensive computing;database;distributed computing;hoc (programming language);information retrieval;online and offline;parallel computing;preprocessor;real-time clock;real-time computing;real-time data;real-time locating system;requirement;sql;scalability;sensor;separation of concerns;social media;social network;software architecture;state (computer science);stream processing;thermal copper pillar bump	Leandro Ordoñez-Ante;Thomas Vanhove;Gregory van Seghbroeck;Tim Wauters;Filip De Turck	2016	2016 11th International Conference for Internet Technology and Secured Transactions (ICITST)	10.1109/ICITST.2016.7856676	computer science;data mining;database;world wide web	DB	-35.956821040479085	-5.879017170814715	169215
d92e9dad00772e8582721cc50ab2a4aa53c5b5b0	rule-based models of legal expertise	rule based;language design;expert system	This paper describes a rule-based legal de-cisonmaking system (LDS) that embodies the skills and knowledge of an expert in product liability law. The system is being used to study the effect of changes in legal doctrine on settlement strategies and practices. LDS is implemented in ROSIE, a rule-oriented language designed to facilitate the development of large expert systems. The ROSIE language is briefly described and our approach to modeling legal expertise using a prototype version of LDS is presented. L. INTRODUCTION We are currently engaged in designing and building rule-based models of legal expertise. A rule-based model of expertise is a computer program organized as a collection of antecedent-consequent rules [l] that embodies the skills and knowledge of an expert in some domain. The primary goal of our work is to develop rule-based models of the de-cisionmaking processes of attorneys and claims ad-justers involved in product liability litigation. We will use these models to study the effect of changes in legal doctrine on settlement strategies and practices. Some progress has already been made in developing computer systems to perform legal analysis. The LEGOL Project [2] has been working for a number of years on the construction of a language for expressing legislation. In addition, systems have been developed for analyzing cases on the basis of legal doctrine 13,41, investigating the tax consequences of corporate transactions [S], automating the assembly of form legal documents if313 and performing knowledge-based legal information retrieval [7]. Our legal decisionmaking system (LDS) is being implemented in ROSIE, a rule-oriented language designed to facilitate the development of large expert systems. In section II the ROSIE language is briefly described. Section III discusses our approach to modeling legal expertise and describes the operation of our prototype version of IDS. The conclusions are presented in section IV. II. METHODOLOGY-A rule-oriented system for implementing expertise (ROSIE) is currently under development-to provide a tool for building expert systems in complex domains 181. ROSIE is a direct descendant of RITA [9] and more distantly MYCIN [lo] in that the models created are rule-based with data-directed control [ll], and are expressed in an English-like syntax. In addition, the models use special language primitives and pattern matching routines that facilitate interaction with external computer systems. not found The ROSIE design also includes features in these successor systems, such as a hierarchical data structure capable of supporting abstraction and inheritance in a general way, partitioned …	computer program;data structure;expert system;http 404;hierarchical database model;legal information retrieval;logic programming;mycin;pattern matching;prototype	Donald A. Waterman;Mark Peterson	1980			legal expert system;computer science;knowledge management;artificial intelligence;data mining;expert system	AI	-33.98554828910696	-8.191659141793448	169358
ca55c518efb98327a6254b8a147b1e57f5c1810b	meet db2: automated database migration evaluation		Commercial databases compete for market share, which is composed of not only net-new sales to those purchasing a database for the first time, but also competitive “win-backs” and migrations. Database migration, or the act of moving both application code and its underlying database platform from one database to another, presents a serious administrative and application development challenge fraught with large manual costs. Migration is typically a high cost effort due to incompatibilities between database platforms. Incompatibilities are caused most often by product specific extensions to language support, procedural logic, DDL, and administrative interfaces. The migration evaluation is the first step in any competitive database migration process. Historically this has been a manual process, with the high costs and subjective results. This has led us to reexamine traditional practices and explore an automatic, innovative solution. We have designed and implemented the Migration Evaluation and Enablement Tool for DB2 for Linux Unix and Windows, or MEET DB2, a tool for automatically evaluating database migration projects. Encapsulated in a simple one-click interface, MEET DB2 is able to provide detailed evaluation of migration complexity based on its deep analysis on the source database. In this paper, we present MEET DB2, and discuss many aspects of our design, and report measurements from real-world use cases. In particular, we show a novel way to use XML and XQuery in this domain for better extensibility and interoperability. We have evaluated MEET DB2 on 18 source code samples, covering nearly 1 million lines of code. The utility has provided benefits in several dimensions including: dramatically reduced time for evaluation, consistency, improved accuracy over human analysis, improved reporting, reduced skill requirements for migration analysis, and clear analytics for product planning. ∗Work done while at IBM Canada. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Articles from this volume were presented at The 36th International Conference on Very Large Data Bases, September 13-17, 2010, Singapore. Proceedings of the VLDB Endowment, Vol. 3, No. 2 Copyright 2010 VLDB Endowment 2150-8097/10/09... $ 10.00.	1-click;c++;data definition language;database;expert system;extensibility;interoperability;java;linux;logic programming;microsoft windows;prototype;purchasing;requirement;rough set;sql;software incompatibility;source lines of code;unix;vldb;xml;xquery	Reynold Xin;Patrick Dantressangle;Sam Lightstone;William McLaren;Steve Schormann;Maria Schwenger	2010	PVLDB	10.14778/1920841.1921016	data migration;simulation;computer science;data mining;database;programming language;database testing	DB	-43.279869819957106	-3.4135612075339297	169550
0173fe42b63be540c98b8b9cf51de90efc12e99e	report on the second workshop on sustainable software for science: practice and experiences (wssspe2)		Daniel S. Katz1, Sou-Cheng T. Choi2, Nancy Wilkins-Diehr3, Neil Chue Hong4, Colin C. Venters5, James Howison6, Frank Seinstra7, Matthew Jones8, Karen A. Cranston9, Thomas L. Clune10, Miguel de Val-Borro11 and Richard Littauer12 1 Computation Institute, University of Chicago & Argonne National Laboratory, Chicago, IL, USA dsk@uchicago.edu 2 NORC at the University of Chicago and Illinois Institute of Technology, Chicago, IL, USA 3 University of California-San Diego, San Diego, CA, USA 4 Software Sustainability Institute, University of Edinburgh, Edinburgh, UK 5 University of Huddersfield, School of Computing and Engineering, Huddersfield, UK 6 University of Texas at Austin, Austin, TX, USA 7 Netherlands eScience Center, Amsterdam, Netherlands 8 National Center for Ecological Analysis and Synthesis, Santa Barbara, CA, USA 9 Department of Biology, Duke University, Durham, NC, USA 10 NASA Goddard Space Flight Center, Greenbelt, MD, USA 11 Department of Astrophysical Sciences, Princeton University, Princeton, NJ, USA 12 University of Saarland, Germany Corresponding author: Daniel S. Katz	bookmark (world wide web);breakout box;certificate authority;compiler;computation;easychair;emoticon;experience;hashtag;hoc (programming language);ibm naval ordnance research calculator;job stream;pervasive informatics;requirement;school of computing (robert gordon university);social network;software development;software engineering	Daniel S. Katz;Sou-Cheng T. Choi;Nancy Wilkins-Diehr;Neil P. Chue Hong;Colin C. Venters;James Howison;Frank J. Seinstra;Matthew Jones;Karen A Cranston;Thomas L. Clune;Miguel de Val-Borro;Richard Littauer	2015	CoRR			ML	-46.626089040279126	-8.544921298586402	169731
61c43ba4ffc7904787c427a1900c0cb51c7936ec	banking comprehensive risk management system based on big data architecture of hybrid processing engines and databases		Banks are shifting from a simple credit risk management model to the comprehensive risk management model. Banking risks come from many channels and systems. Big data technology provides an innovative and effective solution for data management, and thus is suitable to be applied in the risk management scenarios that require high-quality data and complex data analysis. This paper firstly proposes big data architecture of hybrid processing engines and databases. This architecture uses Hadoop ecosystem with ETL and Spark processing engines, and using massive parallel processing databases (MPP), transactional databases, and HDFS. Then a banking comprehensive risk management system prototype based on the proposed big data architecture is implemented. Comparisons and evaluations clearly demonstrate that the proposed system has better performance.		Shenglan Ma;F. Y. Xie;Hao Wang;Hao Dai;Ran Tao;Ruihua Yi;Tongsen Wang	2018	2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)	10.1109/SmartWorld.2018.00310	massively parallel;computer science;risk management;credit risk;big data;architecture;data management;transactional leadership;database;complex data type	DB	-33.94426465516198	-1.4812516798870778	169739
b149ba7e18ad70c86eeb3813683095e46792def5	interlinking educational resources and the web of data: a survey of challenges and approaches	educational resources;linked data;metadata;technology enhanced learning;resource units;data;information technology;computer and information science;computer uses in education;classification;web data;natural sciences;learning methods;information dissemination;semantic web;open educational resources;data och informationsvetenskap	Research in the area of technology-enhanced learning (TEL) throughout the last decade has largely focused on sharing and reusing educational resources and data. This effort has led to a fragmented landscape of competing metadata schemas, or interface mechanisms. More recently, semantic technologies were taken into account to improve interoperability. However, so far Web-scale integration of resources is not facilitated, mainly due to the lack of take-up of shared principles, datasets and schemas. On the other hand, the Linked Data approach has emerged as the de facto standard for sharing data on the Web and is fundamentally based on established W3C standards (e.g. RDF, SPARQL). To this end, it is obvious that the application of Linked Data principles offers a large potential to solve interoperability issues in the field of TEL. In this paper, we survey approaches aimed towards our vision of Linked Education, i.e. education which exploits educational Web data. This particularly considers the exploitation of the wealth of already existing TEL data on the Web by allowing its exposure as Linked Data and by taking into account automated enrichment and interlinking techniques to provide rich and well-interlinked data for the educational domain.	categorization;dbpedia;europeana;gene ontology term enrichment;interoperability;linked data;pubmed;resource description framework;sparql;semantic web;the european library;vocabulary;web resource;world wide web	Stefan Dietze;Salvador Sánchez Alonso;Hannes Ebner;HongQing Yu;Daniela Giordano;Ivana Marenzi;Bernardo Pereira Nunes	2013	Program	10.1108/00330331211296312	biological classification;computer science;data science;semantic web;linked data;data mining;database;metadata;information technology;world wide web;information retrieval;data	Web+IR	-43.468791610319094	2.125807776720568	169779
42be3a93c6bad327b4228813bb2f77cf15cb4681	tripartite community structure in social bookmarking data	community detection;data sharing;social bookmarking;network analysis;hypergraphs;navigation;visualization;clustering;community structure;structure preservation;algorithms;clustered data	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	agile software development;algorithm;archive;data structure;document;embedded system;experiment;francis;ground truth;nl (complexity);organizing (structure);primary source;relevance;selection (genetic algorithm);simulation;tag cloud	Nicolas Neubauer;Klaus Obermayer	2011	The New Review of Hypermedia and Multimedia	10.1080/13614568.2011.598952	navigation;visualization;network analysis;computer science;data science;data mining;cluster analysis;world wide web;community structure	Robotics	-39.98203201943723	-9.223879294042879	169842
158f9e4e385645d2db3949483789cc84ceb41c3c	model-based feedback in the language modeling approach to information retrieval	sql aware data mining;query language;information retrieval;probabilistic model;query operators;language model;data mining primitives	The language modeling approach to retrieval has been shown to perform well empirically. One advantage of this new approach is its statistical foundations. However, feedback, as one important component in a retrieval system, has only been dealt with heuristically in this new retrieval approach: the original query is usually literally expanded by adding additional terms to it. Such expansion-based feedback creates an inconsistent interpretation of the original and the expanded query. In this paper, we present a more principled approach to feedback in the language modeling approach. Specifically, we treat feedback as updating the query language model based on the extra evidence carried by the feedback documents. Such a model-based feedback strategy easily fits into an extension of the language modeling approach. We propose and evaluate two different approaches to updating a query language model based on feedback documents, one based on a generative probabilistic model of feedback documents and one based on minimization of the KL-divergence over feedback documents. Experiment results show that both approaches are effective and outperform the Rocchio feedback approach.	fits;feedback;heuristic;information retrieval;kullback–leibler divergence;language model;query language;statistical model	ChengXiang Zhai;John D. Lafferty	2001		10.1145/502585.502654	natural language processing;statistical model;query expansion;ranking;data control language;computer science;machine learning;data mining;database;rdf query language;information retrieval;query language;language model	Web+IR	-33.96753780316787	4.109133002475712	169996
1db0ba32169c06f958b0bde2bdb1c7c173df9c2e	abstract feature space representation for volumetric transfer function exploration	004;volumetric transfer function abstract feature space	Feature Space Representation for Volumetric Transfer Function Exploration Ross Maciejewski, Yun Jang, David S. Ebert, and Kelly P. Gaither 1,3 Purdue University Visual Analytics Center West Lafayette, IN, USA {rmacieje|ebertd}@purdue.edu 2 ETH Zurich Zurich, Switzerland	feature vector;kelly criterion;switzerland;transfer function;visual analytics	Ross Maciejewski;Yun Jang;David S. Ebert;Kelly P. Gaither	2011		10.4230/DFU.Vol2.SciViz.2011.212	computer vision;function space;machine learning;mathematics;geometry	ML	-47.193168714184445	-9.874076021746076	170009
bfa1c889c9bea00c938558c7ca99c48c94415fa9	build intelligence from the physical world	social intelligence;long period;contextual information;internet services	Sensor: a device that measures a physical quantity and converts it into a signal which can be read by an observer or by an instrument (from Wiki) Device time Device location Make the Cloud Intelligent The coming era of cloud computing brings new opportunities to this long studied research area By accumulating and aggregating context from multiple users, multiple devices, and over a long period, we can obtain collective social intelligence from them	artificial intelligence;cloud computing;multi-user;sensor;wiki	Xing Xie	2011		10.1007/978-3-642-19173-2_1	intelligence cycle;knowledge management;social intelligence;multimedia;world wide web	HCI	-37.78763062005268	-9.178865778921814	170240
73cd88ecbcad14cdbd635a65eb93c2558d7928ca	integrating ddi metadata into the nara transcontinental persistent archive prototype via the oai-pmh	persistent archives;metadata;sdsc;social science;university of north carolina;digital preservation;oai pmh;ddi;interoperability;grid computing;data grid;data archive	The H.W. Odum Institute for Research in Social Science (Odum), the Renaissance Computing Institute (RENCI), and the School of Information and Library Science (SILS), all part of the University of North Carolina at Chapel Hill (UNC-CH), are collaborating with the San Diego Supercomputer Center (SDSC) on an extension of the National Archives and Records Administration's (NARA) transcontinental persistent archive prototype (TPAP) data grid with the new integrated Rule Oriented Data System (iRODS). The goal of the project is to enable collection interoperability between UNC-CH and SDSC using an iRODS environment. This poster presents the results of one part of that project, which is the development of a crosswalk between the Odum Institute Data Archive (OIDA) Data Document Initiative (DDI) metadata and the NARA TPAP iRODS metadata catalogue (iCAT) via the OAI-PMH.	archive;chapel;data system;interoperability;library science;prototype;renaissance;san diego supercomputer center	Jewel Ward;Jonathan Crabtree	2008		10.1145/1378889.1379018	interoperability;computer science;data grid;database;metadata;world wide web;grid computing	HPC	-44.0637890466998	-5.92259319596978	170334
8d1c211cba5a4dc7c95ae370d788561e65b190d1	special issue on “theory and practice of high-performance computing, communications, and security”	qa75 electronic computers computer science;high performance computer	With the rapid growth in computing and communication technology, the past decade has witnessed a proliferation of powerful parallel and distributed systems and an ever increasing demand for practical applications of high-performance computing, communications, and security (HPCCS). HPCC has moved into the mainstream of computing and has become a key technology in determining future research and development activities in many academic and industrial branches, especially when the solution of large and complex problems must cope with very tight timing schedules. This special issue aims to foster the dissemination of high-quality research in any new HPCCS idea, method, theory, technique, and application. The objective of this special issue is to showcase the most recent developments and research in the HPCCS field, as well as to enhance its state-of-the-art. Original research articles have been solicited in all aspects of HPCCS including theoretical studies, practical applications, new communication technology, and experimental prototypes for HPCCS.	distributed computing;hpcc;supercomputer	Tai-Hoon Kim;Omer F. Rana;Juan Touriño;Isaac Woungang	2010	The Journal of Supercomputing	10.1007/s11227-010-0514-z	computer security model;computer literacy;computing;computer science;theoretical computer science;information and computer science	HPC	-48.08994560348621	0.30283683007823786	170350
914925f95e45af3c96db6bc61b67d72442943478	on benchmarking data translation systems for semantic-web ontologies	data exchange;semantic web technology;semantic web;point of view;semantic web and ontologies	Data translation, also known as data exchange, is an integration task that aims at populating a target model using data from a source model. This task is gaining importance in the context of semantic-web ontologies due to the increasing interest in graph databases and semantic-web agents. Currently, there are a variety of semantic-web technologies that can be used to implement data translation systems. This makes it difficult to assess them from an empirical point of view. In this paper, we present a benchmark that provides a catalogue of seven data translation patterns that can be instantiated by means of seven parameters. This allows us to create a variety of synthetic, domain-independent scenarios one can use to test existing data translation systems. We also illustrate how to analyse three such systems using our benchmark. The main benefit of our benchmark is that it allows to compare data translation systems side by side within a homogeneous framework.	benchmark (computing);cad data exchange;graph database;ontology (information science);open-source software;population;semantic web;synthetic intelligence	Carlos R. Rivero;Inma Hernández;David Ruiz;Rafael Corchuelo	2011		10.1145/2063576.2063810	data exchange;computer science;semantic web;data mining;database;world wide web;information retrieval	AI	-38.268558648827565	3.6687852013130016	170354
077aa450df7d83231bb23dfcd568c56717f88691	personalizing xml search in pimento	score function;paints;query processing;pimento project;xml search;usa councils;xml repositories;navigation;user profile;manufacturing;xml;query answers;web search;xml query processing;query answering;xml marketing and sales usa councils query processing design methodology algorithm design and analysis paints web search navigation manufacturing;pimento project xml search xml repositories query answers query processing;algorithm design and analysis;marketing and sales;design methodology	XML search is increasing in popularity as more and larger XML repositories are becoming available. The accuracy of XML search varies across different systems and a lot of effort is put into designing scoring functions tailored to specific users and datasets. We argue that there is no one scoring function that fits all and advocate incorporating user profiles into XML search to personalize query answers by accounting for user profiles. First, we propose a framework for defining user profiles and for enforcing them during query processing. Second, we adapt the well-known top-k pruning to account for user profiles. Finally, we present effectiveness and efficiency experiments which show that query personalization in XML search dramatically improves the accuracy of query results while incurring negligible processing overhead. This work is in the context of the Pimento project which aims at improving the relevance of searching structured and unstructured content.	database;experiment;fits;overhead (computing);personalization;precision and recall;relevance;rewrite (programming);scope (computer science);scoring functions for docking;user profile;whole earth 'lectronic link;xml;xmark93	Sihem Amer-Yahia;Irini Fundulaki;Laks V. S. Lakshmanan	2007	2007 IEEE 23rd International Conference on Data Engineering	10.1109/ICDE.2007.367936	xml validation;algorithm design;navigation;xml;design methods;computer science;xml framework;data mining;xml database;xml schema;database;score;manufacturing;web search query;world wide web;information retrieval;statistics	DB	-34.40314167849021	3.8895519097265137	170451
043d0c68b3fd14da05f456bfdde11fe051185e86	future directions in computer science research		Over the last 40 years, computer science research was focused on making computers useful. Areas included programming languages, compilers, operating systems, data structures and algorithms. These are still important topics but with the merging of computing and communication, the emergence of social networks, and the large amount of information in digital form, focus is shifting to applications such as the structure of networks and extracting information from large data sets. This talk will give a brief vision of the future and then an introduction to the science base that needs to be formed to support these new directions.	algorithm;compiler;computer science;data structure;emergence;operating system;programming language;social network	John E. Hopcroft	2012		10.1007/978-3-642-35261-4_1	discrete mathematics;compiler;computer science;merge (version control);theoretical computer science;data set;social network;data structure	Theory	-48.00587230301825	-1.7534176397011578	170582
88fea05dcd93a84d7131d8bdd6bb3f7619e77ffe	extracting relevant information for a domain-specific search service using knowledge-based mining techniques	databases;portals;search engine;data mining search engines ontologies databases computer science world wide web portals data analysis web search java;search engines;data mining;resource use;data analysis;ranking algorithm;world wide web;web search;ontologies;computer science;domain specificity;java;knowledge base	Opposite to general search engines, specialized search engines have the advantage to exploit specified properties of a special domain in order to allow a better search. For that, special methods are developed for the analysis of web sites and the retrieval is adapted. The example of a weather search service in Germany illustrates a knowledge-based, specialized search engine in this paper. There are a couple of WWW portals about weather information, climate, country sayings and health weather in Germany. The weather information providers present their data in a special way. These specialties are used for a knowledge-based data analysis of the web sites. Therefore an ontology and special analysis tools were developed. The goal is to present the most relevant information to the user in a web search service. To that, a three-stage process will be presented. In the first stage a traditional search process is started. In the second stage an expansion, an arrangement, and a classification of the document set is done using link tracing and ranking algorithms. And in the final stage the ontology concepts and relations are searched. The documents can be stored as URL-objects, HTML-sequences or java access classes. The data for the relevant information of an inquiry are aggregated from different resources using knowledge-based techniques.	algorithm;deep linking;fansite;html;heuristic (computer science);information systems;java;knowledge-based systems;link analysis;list of academic databases and search engines;pattern recognition;portals;systems engineering;www;web search engine	Ilvio Bruder;Christian Dethloff	2002		10.1109/WISEW.2002.1177871	knowledge base;computer science;data mining;database;world wide web;information retrieval;search engine	Web+IR	-39.576219409640544	2.5783387698988727	170641
7156072fe03dd50c0ad8fcd5124af441e0b6386f	technology trends in knowledge management tools	knowledge management tools;knowledge management;collaboration;knowledge extraction;knowledge mapping;ontologies;knowledge discovery	A large number of tools are available in the software industry to support different aspects of knowledge management (KM). Some comprehensive applications and vendors try to offer global solutions to KM needs; other tools are highly specialized. In this paper, state-of-theart KM tools grouped by specific classification areas and functionalities are described. Trends and integration efforts are detailed with a focus on identifying current and future software and market evolution. backGrounD anD DefInITIons: a focus on PeoPle anD conTeXT	knowledge management	Gilles Balmisse;Denis Meingan;Katia Passerini	2007	IJKM	10.4018/jkm.2007040106	knowledge base;knowledge integration;software mining;data management;computer science;knowledge management;ontology;data science;mathematical knowledge management;knowledge engineering;open knowledge base connectivity;data mining;knowledge extraction;personal knowledge management;domain knowledge;collaboration	DB	-43.21910549473712	-0.37816485562714014	170738
f2aaadf85f57fd895f45314bb3066025c28459e8	student modelling by adaptive testing : a knowledge-based approach				Sophiana Chua Abdullah	2003			data science;computerized adaptive testing;data mining;text mining;computer science	SE	-39.83280612803863	-6.924105012929712	170837
ac1c10d306082c16aa3f1fae8eac951304964027	research on the sharing and interoperation of geospatial information based on the combination of grdi and ogc	data sharing;interoperation;rail transportation;ogc web service specification;expert systems;spatial data;grid technology;web map service;transparent services;geocomputing;grid middleware;data migration;data format;gloubs interoperation gis web gis grid ogc;grid;ogc;computer architecture;web services geographic information systems grid computing rail transportation computer architecture middleware java remote sensing satellite navigation systems global positioning system;web services expert systems geographic information systems geography geophysical techniques grid computing java middleware;gis;global positioning system;gloubs;geographic information systems;remote sensing;spatial databases;web services;data format transform;distributed databases;grdi;middleware;web gis;gloubs toolkit 4;information service;ogc web services;ogc web map service;service oriented architecture;distributed massive spatial data;grid computing;java technology;geospatial information services;data direct access;grid map services;geophysical techniques;containers;data interoperation;satellite navigation systems;java;ogc web map service data sharing data interoperation grdi data format transform data direct access web gis distributed massive spatial data integrated services transparent services geocomputing data migration geospatial information services grid technology ogc web service specification grid middleware gloubs toolkit 4 java technology grid map services;integrated services;geography	The research on the sharing and interoperation of geospatial information is always a hotspot in GIS field. Many methods have been proposed such as data format transform, data direct access, Web GIS and data interoperation. These technologies have made great progresses in the sharing and interoperability of geospatial information. But there are many unconquerable problems such as the process of the distributed massive spatial data, providing integrated and transparent services, capability of geocomputing and migration of geospatial information services for users. In this paper, we combine grid and OGC web service to research how to realize the sharing and interoperation of geospatial information. We introduce the grid technology and OGC web service specifications. Then we design the architecture of integrated Grid and OGC. At last based on grid middleware Gloubs Toolkit 4 and Java technology, we develop and deploy Grid map services which encapsulate OGC Web Map Service.	geographic information system;interoperability;interoperation;java hotspot virtual machine;middleware;random access;web map service;web service;world wide web	Yumei Sun;Yu Fang;Bin Chen;Jiayuan Lin;Lihong Bi	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4779248	distributed gis;geomatics;geospatial pdf;computer science;data mining;database;world wide web;distributed database;expert system;grid computing;remote sensing;web feature service;web coverage service	HPC	-38.443843293286385	-1.7430102589097678	170967
b86283289ec2b6bf9e636e3b2584b27f323bdc45	stochastic measures of resilience and their application to container terminals	recoverability;infrastructure systems;vulnerability;resilience	a Environmental Change Institutue, School of Geography and Environment, University of Oxford, United Kingdom b School of Industrial and Systems Engineering, University of Oklahoma, United States c Engineering Management Program, System Development and Maturity Lab, School of Systems and Enterprises, Stevens Institute of Technology, United States d Facultad de Ingenieria, Universidad Central de Venezuela, Venezuela	capability maturity model;systems engineering	Raghav Pant;Kash Barker;Jose Emmanuel Ramirez-Marquez;Claudio M. Rocco Sanseverino	2014	Computers & Industrial Engineering	10.1016/j.cie.2014.01.017	vulnerability;engineering;environmental resource management;operations management;computer security;psychological resilience	SE	-44.82783613970585	-8.873173390767695	171002
98d1d50e08bf7937f6e500684b2e069f27d64c07	a cloud computing approach to on-demand and scalable cybergis analytics	geographic information systems gis;cybergis;auto scaling;cloud computing	Spatial data analysis has become ubiquitous as geographic information systems (GIS) are widely used to support scientific investigations and decision making in many fields of science, engineering, and humanities (e.g., ecology, emergency management, environmental engineering and sciences, geosciences, and social sciences). Tremendous data and computational capabilities are needed to handle and analyze massive quantities of spatial data that are collected across multiple spatiotemporal scales and used for diverse purposes. CyberGIS has emerged as a new-generation GIS based on advanced cyberinfrastructure to seamlessly integrate such capabilities into scalable geospatial analytics and modeling tools. One of the key challenges and opportunities of CyberGIS research is to build an on-demand service framework that can manage underlying cyberinfrastructure resources dynamically, in order to provide responsive support for interactive online CyberGIS analytics for which users can generate massive service requests in a short amount of time. This paper presents a cloud computing approach to implementing CyberGIS analytics using cloud computing services in the CyberGIS Gateway, a multiuser and collaborative online problem-solving environment. The primary purpose of this research is to address the question of how to achieve on-demand and scalable CyberGIS analytics that provide a stable response time to the user. We do that through integration with the Nimbus Phantom cloud platform. We then investigate how the cloud platform is able to adaptively handle fluctuating requests for analytics while providing a stable response time.	cloud computing;computation;cybergis;cyberinfrastructure;ecology;geographic information system;multi-user;online algorithm;problem solving environment;response time (technology);scalability;spatial analysis	Pierre Riteau;Myunghwa Hwang;Anand Padmanabhan;Yizhao Gao;Yan Liu;Katarzyna Keahey;Shaowen Wang	2014		10.1145/2608029.2608032	analytics;cloud computing;computer science;data science;operating system;data mining;world wide web	HPC	-35.33902184976488	-2.148015509172149	171025
e17ed40197702b648a5766e73fa27ceb12285c99	minimum mean square error estimators for the exponential ssalt model		1School of Business Administration, Southwestern University of Finance and Economics No. 555, Liutai Ave, Wenjiang Zone, Chengdu 611130, China 2Yangtze Normal University, No. 98, Julong Ave, Fuling Zone, Chongqing, 408100, China 3School of Management and Economics University of Electronic Science and Technology of China No. 2006, Xiyuan Ave, West Hi-Tech Zone, Chengdu 611731, China e-mail: kougang@swufe.edu.cn, lin-changsheng@qq.com, pengyicd@gmail.com, lgx9889@126.com, francisnju@gmail.com	censoring (statistics);email;failure rate;linear function;log-linear model;mathematical optimization	Gang Kou;Changsheng Lin;Yi Peng;Guangxu Li;Yang Chen	2016	Informatica, Lith. Acad. Sci.		statistics	ECom	-44.508657956918725	-9.288338500332543	171054
5ec40dd6e94186356e05af1025320245db5aabd1	ingesting the auslan corpus into the dada annotation store	elan annotation;rdf data;auslan corpus;ingesting data;collaborative access;abstract model;australian sign language;language resource;dada system;wide range;dada annotation store;data model;web accessibility;linked data;web pages;design pattern;sign language;semantic web	The DADA system is being developed to support collaborative access to and annotation of language resources over the web. DADA implements an abstract model of annotation suitable for storing many kinds of data from a wide range of language resources. This paper describes the process of ingesting data from a corpus of Australian Sign Language (Auslan) into the DADA system. We describe the format of the RDF data used by DADA and the issues raised in converting the ELAN annotations from the corpus.	resource description framework;semantic web;server (computing);text corpus	Steve Cassidy;Trevor Johnston	2009			natural language processing;digital library;sign language;data model;computer science;data science;semantic web;web accessibility;web page;linked data;database;design pattern;world wide web	NLP	-41.057473468862014	3.857450999772068	171060
5a69f6a7dc5a38c381f3a26c335c8d9dc19469e1	a review of data mining techniques and applications	data mining techniques;data mining;data mining application;big data	Data mining is the analytics and knowledge discovery process of analyzing large volumes of data from various sources and transforming the data into useful information. Various disciplines have contributed to its development and is becoming increasingly important in the scientific and industrial world. This article presents a review of data mining techniques and applications from 1996 to 2016. Techniques are divided into two main categories: predictive methods and descriptive methods. Due to the huge number of publications available on this topic, only a selected number are used in this review to highlight the developments of the past 20 years. Applications are included to provide some insights into how each data mining technique has evolved over the last two decades. Recent research trends focus more on large data sets and big data. Recently there have also been more applications in area of health informatics with the advent of newer algorithms.	data mining	Ratchakoon Pruengkarn;Kevin Kok Wai Wong;Lance Chun Che Fung	2017	JACIII	10.20965/jaciii.2017.p0031	big data;computer science;data science;data mining;business intelligence;information retrieval	ML	-37.58589320618337	-7.224214055638218	171165
53beb591a97ed4b03dfbb0be7f96b5611fb9a023	from raw data to management reports - historical performance data made useful				Bradford Camp;Christopher Lynn	2003			raw data;data mining;computer science	HPC	-40.047998689342286	-5.8774127612579905	171204
c3f553cf59260d9e9f7cd5e85633bab3676b48d9	cobweb multidimensional model: visualizing olap query results using tag-cloud operators		OLAP (On-Line Analytical Processing) systems provide decision makers with multidimensional analyses of large databases by generating an aggregated vision of data. Nowadays, these systems face growing non-numeric data. In this context, we propose in this paper a new generic multidimensional model called CobWeb dedicated to the OLAP of XML documents; it is based on the concept of facet. The CobWeb model aims to ease the expression of queries; also it offers an appropriate vision of the document warehouse. In this context and for manipulation purposes, we propose new visualization operators for OLAP query results by using the concept of Tag clouds as a means to help decision-makers to see the content in an efficient manner and then to focus on the knowledge in results.	database;experiment;level of measurement;online analytical processing;recursion;tag cloud;text corpus;xml	Omar Khrouf;Kaïs Khrouf;Jamel Feki	2016		10.1007/978-3-319-53480-0_98	artificial intelligence;data science	DB	-36.172579787954454	2.040954623054994	171588
b6da9e64e3b48fbfa78f4ea987c49097a528fecf	a web of musical information	relational data;web service;music analysis;semantic web	We describe our recent achievements in interlinking several music-related data sources on the Semantic Web. In particular, we describe interlinked datasets dealing with Creative Commons content, editorial, encyclopedic, geographic and statistical data, along with queries they can answer and tools using their data. We describe our web services, providing an on-demand access to content-based features linked with such data sources and information pertaining to their creation (including processing steps, applied algorithms, inputs, parameters or associated developers). We also provide a tool allowing such music analysis services to be set up and scripted in a simple way.	algorithm;semantic web;web service	Yves Raimond;Mark B. Sandler	2008			web service;web development;web modeling;data web;web mapping;web standards;relational database;computer science;ws-policy;semantic web;social semantic web;linked data;data mining;semantic web stack;music theory;web intelligence;web 2.0;world wide web;information retrieval	Web+IR	-42.2825532554771	1.8691715918942502	171715
6be52a0c3a49ee58d6b0f03863c0e20dbf67d042	data base administration in accounting information systems	prolog;query languages;data base management system;decision making process;accounting information system;deductive database systems;information need;management information system;historical data;relational database management systems	This research will explain the innovations in gathering and communicating data, outside the traditional role of the accounting function, which are required to adequately fulfill the information needs of all levels of management. Data Base Management Systems (DBMS) and their respective administration offer a good basis for collecting and utilizing available data, to aid the decision-making process at all levels of management.  At the functional level, managers are concerned with the use of historic data. Using a DBMS, these managers will be able to obtain specific, updated summary information from the database as needed.  Data Base Management, as part of an overall management information system, offers vast opportunities for more effective decision-making, installing the system that's best for your type of information, means whether or not you'll he getting appropriate data.	database;information systems;information needs;management information system;management system	Avi Rushinek;Sara F. Rushinek	1985	DATA BASE	10.1145/2147769.2147771	data modeling;information needs;decision-making;relational database management system;information technology management;system of record;data model;data management;computer science;knowledge management;data administration;personal information management;management information systems;data mining;database;risk management information systems;structure of management information;information management;accounting information system;prolog;world wide web;information system;database design;query language	DB	-33.712478692465965	-7.8354015355363344	171751
f20116e46b41d175325cb963e5b234084185d2cc	towards principles for structuring and managing very large semantic multidimensional data models	key performance indicator;data cube;multidimensional data modeling;adapt;tool support;data management;multidimensional data;application design for analytical processing technologies;repository;semantic data model;lessons learned;data warehouse;large data	The management of semantic multidimensional data models plays an important role during the phases of development and maintenance of data warehouse systems. Unfortunately, this is not done with the necessary stress by now. Reasons might be seen in the plethora of semantic notations or the insufficient tool support for multidimensional modeling. The paper on hand provides experiences gained within a project with an industry partner of the telecommunications industry. Their problem is a very huge data warehouse with more than 400 data cubes and several hundred key performance indicators. We developed a repository-based solution for managing the semantic data models. Our lessons learned show that especially for very large data models there has to be a repository based solution as well as a clear concept on how to break them up into their component pars. The aim of our principles is to increase the understandability as well as the maintainability of semantic multidimensional data models.	data cube;semantic data model	Christian Kurze;Peter Gluchowski	2009			idef1x;data modeling;semantic computing;dimensional modeling;computer science;data science;data warehouse;data mining;database;data mapping	DB	-34.014830749086784	0.32331391956233846	172253
3e5525aeaac0cc9aaaf3a1b066b1b2ea8ab3ea67	on the quality of vocabularies for linked dataset papers published in the semantic web journal		Linked Data and knowledge graphs more generally have been a main driver of research and technology transfer in our field. Linked data serves as a testbed for Semantic Web technologies, as an outreach to application communities interested in data sharing, and contributes more generally to the big data effort by making a large variety of structured data available for all kinds of purposes in various communities [24]. As of December 2017, the LOD Laundromat aggregator [38] alone shows over 38,000,000,000 Linked Data triples which is only a fraction of the data that has been published as Linked Data to date. Much of Linked Data is generated by researchers as a contribution to the community effort. Due to the importance of this type of data for the advance of research and applications in our field, the Semantic Web journal began in 2012 to solicit papers containing linked dataset descriptions. The idea behind this was to provide the broader re-	big data;blog;knowledge graph;linked data;semantic web;testbed;vocabulary	Stella Sam;Pascal Hitzler;Krzysztof Janowicz	2018	Semantic Web	10.3233/SW-180290	data science;linguistics;philosophy;semantic web	Web+IR	-43.346994784791775	0.8913783097008262	172623
e232e5001a9d75176b21386b4f7f2a0df590d4cf	arguminsci: a tool for analyzing argumentation and rhetorical aspects in scientific writing		Argumentation is arguably one of the central features of scientific language. We present ArguminSci, an easy-to-use tool that analyzes argumentation and other rhetorical aspects of scientific writing, which we collectively dub scitorics. The main aspect we focus on is the fine-grained argumentative analysis of scientific text through identification of argument components. The functionality of ArguminSci is accessible via three interfaces: as a command line tool, via a RESTful application programming interface, and as a web application.	application programming interface;command-line interface;jamie wilkinson;representational state transfer;web application	Anne Lauscher;Goran Glavas;Kai Eckert	2018			world wide web;argumentation theory;natural language processing;application programming interface;artificial intelligence;argumentative;computer science;scientific language;web application;rhetorical question;scientific writing	NLP	-41.71451613454432	-0.3300966279363569	172809
bfc0a0ba5261524f07d31d538667225ffc115ef0	evolution of supercomputers	supercomputer;novel devices;scientific computing;architecture;scientific research;supercomputer scientific computing architecture challenging applications novel devices;challenging applications	Supercomputers are prevalent and vital to scientific research and industrial fields, and may be used to represent the level of national scientific development. A summary of the evolution of supercomputers will help direct the future development of supercomputers and supercomputing applications. In this paper, we summarize the accomplishments in supercomputing, predict the trend of future supercomputers, and present several breakthroughs in supercomputer architecture research.	supercomputer architecture	Xiang-Hui Xie;Xing Fang;Sutai Hu;Dong Wu	2010	Frontiers of Computer Science in China	10.1007/s11704-010-0118-z	computational science;supercomputer;computer science;theoretical computer science;architecture	HPC	-47.94444093366475	-0.49077621753806133	172921
19c4ea5b7533327a9653f31e2c38de21b47fe5c2	an efficient phishing webpage detector	web pages;information security;e commerce;heuristic method;false negative;black list based;phishing attack;anti phishing tools;support vector machine;false positive	a School of Mathematics and Computer Engineering, Xihua University, Chengdu 610039, PR China b Department of Computer Science and Information Engineering, National Taiwan University of Science and Technology, Taipei 106, Taiwan c Institute of Mobile Communications, Southwest Jiaotong University, Chengdu, Sichuan 610031, PR China d Center of Excellence in Information Assurance, King Saud University, Saudi Arabia e Department of Electronic Engineering, National United University, Miaoli, Taiwan	computer engineering;computer science;electronic engineering;information assurance;information engineering;phishing;web page	Mingxing He;Shi-Jinn Horng;Pingzhi Fan;Muhammad Khurram Khan;Ray-Shine Run;Jui-Lin Lai;Rong-Jian Chen;Adi Sutanto	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.01.046	support vector machine;phishing;type i and type ii errors;computer science;information security;machine learning;web page;internet privacy;world wide web;computer security;tabnabbing	DB	-44.287227526730526	-7.922145081469524	173287
bbee3c9aa439c3d883301cf232a56e5c681e1a12	new prospects for electrostatic data storage systems	data storage	New Prospects for Electrostatic Data Storage Systems Alexander N. Korotkov and Konstantin K. Likharev State University of New York at Stony Brook Stony Brook, NY 11794-3800 klikharev@notes.cc.sunysb.edu tel +1-631-632-8159 fax +1-631-632-8774	brookgpu;fax;like button	Alexander N. Korotkov;Konstantin Likharev	2000			electronic engineering;materials science;computer data storage	OS	-46.977507354166	-7.1052499992700255	173560
2cd9b9267076316e0752584f354ca5de1534bc6e	e-science meets computational science and information technology	distributed system;information resources;information technology computational modeling high performance computing data mining world wide web moore s law physics computing instruments force sensors sensor phenomena and characterization;information technology;information resources natural sciences computing internet information technology technological forecasting;internet;high performance computing and communications;government policies;web sites computational science e science information technology natural science computing national science foundation virtual lab;national science foundation;research initiatives;computer science;natural sciences computing;technological forecasting	"""Over the last decade or so, there has been much discussion of and progress in computational science. There was the HPCC (High Performance Computing and Communications) Initiative and the NSF Grand Challenge programs which largely focused on this. However recently there has been a subtle change – for example NSF initiated the ITR (Information Technology Research) program based on the recommendations of PITAC – the President's Information Technology Advisory Committee. We did not have the CSR (Computational Science Research), CSITR or PCSAC. One will find classic computational goals as one part of the Information Technology agenda but they are just one part and perhaps a part that is getting smaller. One sees the same trend in research with work on grids and distributed computing overshadowing that on classic parallel computing. Now NSF has a new cyberinfrastructure report ((http://www.cise.nsf.gov/b_ribbon/) continuing the same emphasis on distributed systems. What does this mean for computational science and its associated technology and research? Once we spoke of the three approaches to science: experiment, theory and the emerging computational view. Does Information Technology and cyberinfrastructure enable a fourth paradigm? Such a proliferation of scientific methods does not seem too reasonable. However the National Virtual Observatory (http://www.us-vo.org/) eloquently describes the new approach to astronomy. Just a few years ago, astronomy involved individual groups designing new instruments, developing particular observing strategies and managing the data taking on some instrument either in the space or on the ground – this data was lovingly analyzed to discover and publish new scientific insights. This """" stove-pipe """" approach to observational (experimental) science is typical in many fields. It ensures that those who build the equipment can ensure their data is properly interpreted with the usually difficult instrumental corrections properly applied. Note that it is usual to compare """" reasonably corrected experimental data """" with hypotheses (models) to which other instrumental effects are applied. Often it is not possible to remove all instrumental effects from a dataset so that it can be compared with a pristine theory (developed by a pristine theorist who needs no significant understanding of the instrument). Nevertheless we imagine a new astronomy where our investigator has the best analysis and visualization capabilities connected to the global internet. This analysis will integrate the data from multiple instruments spanning multiple wavelengths and multiple regions of the sky. This vision stresses the high performance networks, data access, management and processing enabled …"""	computation;computational science;cyberinfrastructure;data access;distributed computing;e-science;file spanning;grand challenges;hpcc;ibm notes;national virtual observatory;parallel computing;programming paradigm;theory	Geoffrey C. Fox	2002	Computing in Science and Engineering	10.1109/MCISE.2002.1014985	computational science;the internet;simulation;human–computer interaction;information science;computer science;data science;human-centered computing;data mining;information technology;information and computer science	HPC	-45.0095136178961	-1.5907251229916886	174173
5039b11c2074a61dd1f7a0b502503eae7d7ac14c	a field-based database management method for city air pollutants information system	databases;spatio temporal analysis;query language;geographic data;object relational database management system;city air pollutants information system;tsp;database management systems;information technology;data management;geometry;spatio temporal analysis functions;information access;spatial query language;database management;technology management;spatial database;pm 10;environmental information system;dynamic information;gis;geographic information systems;field based application;air pollution;cities and towns air pollution management information systems geographic information systems spatial databases relational databases object oriented modeling technology management database systems environmental management;spatial databases;database systems;spatial query language gis environmental information system spatial database ordb based extension field based application;management information systems;cities and towns;relational databases;information system;spatio temporal analysis functions field based database management method city air pollutants information system geographic data tsp pm 10 object relational database management system;environmental management;relational databases air pollution database management systems;ordb based extension;air pollutants;object oriented modeling;field based database management method	Along with the wide and deep use of information technology, much research has focused on how to impose the technology of databases to handle geographic data also in terms of temporal features. Recently, field databases emerge as an important research issue in order to deal with complex phenomena in real world, and to develop better approaches to store and manage field-based data such as intensity and dynamical information of city air pollutants including CO, SO2, NO2, TSP, PM10 etc. The paper proposed a new method to store and manage city air pollutants data through an extension mechanism provided by ORDBMS. The extensions mainly referred to extend spatio-temporal analysis functions between different air pollutants data sources. An application of city air pollutants information system was developed using the field-based extended database management method. By using the system, effective data management and useful information accession were achieved.	accession number (bioinformatics);information system;object-relational database	Fengru Huang;Xuesong Wang;Bin Chen;Yu Fang	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4779596	particulates;data management;relational database;computer science;technology management;data mining;database;information technology;spatial database;information retrieval;information system;query language;air pollution	Robotics	-37.12268552442753	-1.9600668018523033	174335
29ee1e66016ad99959175205bc99c8f99444cb18	ontology acquisition from on-line knowledge sources	computer program;web pages;semantic web	Electronic knowledge representation is becoming more and more pervasive both in the form of formal ontologies and less formal reference vocabularies, such as UMLS. The developers of clinical knowledge bases need to reuse these resources. Such reuse requires a new generation of tools for ontology development and management. Medical experts with little or no computer science experience need tools that will enable them to develop knowledge bases and provide capabilities for directly importing knowledge not only from formal knowledge bases but also from reference terminologies. The portions of knowledge bases that are imported from disparate resources then need to be merged or aligned to one another in order to link corresponding terms, to remove redundancies, to resolve logical conflicts. We discuss the requirements for ontology-management tools that will enable interoperability of disparate knowledge sources. Our group is developing a suite of tools for knowledge-base management based on the Protégé-2000 environment for ontology development and knowledge acquisition. We describe one such tool in detail here: an application for incorporating information from remote knowledge sources such as UMLS into a Protégé knowledge base.	alignment;computer science;conflict (psychology);human-readable medium;interoperability;knowledge bases;knowledge acquisition;knowledge base;knowledge modeling;knowledge representation and reasoning;merge;online and offline;ontology (information science);ontology learning;pervasive informatics;protégé;requirement;reuse (action);smart;unified medical language system;vocabulary;web ontology language	Qi Li;Philip Shilane;Natalya Fridman Noy;Mark A. Musen	2000	Proceedings. AMIA Symposium		knowledge base;knowledge integration;computer science;knowledge management;data science;body of knowledge;knowledge-based systems;knowledge engineering;open knowledge base connectivity;data mining;knowledge extraction;personal knowledge management;domain knowledge	AI	-41.83782097159304	3.70046791689578	174514
e7927b61ea6bb7fd0ea8d3edfa80348791528bdd	integration of heterogeneous data for real world domain	ontologies artificial intelligence data analysis data integration information retrieval;ontology heterogeneous data data integration;semantics;data mining;companies;data mining data integration semantics companies ontologies internet visualization;visualization;internet;data semantic property analysis heterogeneous data integration real world domain data processing ontological tool data extraction heterogeneous data sources;ontologies;data integration	There are a large number of companies that have abundant amount of data presented in unstructured forms. For example, accounting firms and banking sectors have clients who may provide financial data for them. However, these clients have data which is of diverse nature. To make this data presentable in a meaningful way, companies usually use their employees to key in data manually to their own systems. This approach is tedious and labor intensive. To facilitate the data processing, these companies normally prefer to have automated softwares that can easily recognize the different formats of data, process and present them in a usable form suitable for further processing. However, there are currently no automated software tools which can perform this task. In this paper, we propose a novel ontological tool in this study to extract data from heterogeneous sources. Our tool will be able to analyze the semantic properties of the various data.		Jer Lang Hong	2013	2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2013.6816316	idef1x;data modeling;data migration;the internet;semantic integration;visualization;data quality;data model;computer science;ontology;data virtualization;data science;data integration;data warehouse;data mining;database;semantics;ontology-based data integration;data retrieval;data element;data architecture;enterprise information integration;data mapping	SE	-36.77476601343696	0.0639104595821971	174873
71a3c040a3a9cb3343096a2381620e20b0337a47	a case study of building spatial database of town based on quick bird image	maps;land development;computer aided design;town and country planning;high resolution;urban planning;geographic information system;cost saving;cad;statistical methods;spatial database;gps;gis;technological plan building spatial database data quick bird image china rapid town growth economical booming land development abuse construction market chaos geographical information system supermap gis software high resolution remote sensing data rs global positioning system gps town planning construction computer aided design cad data map predefined layer building cost saving;global positioning system;remote sensing data;geographic information systems;remote sensing;database systems;image databases spatial databases cities and towns birds geographic information systems buildings global positioning system costs chaos information systems;town;quick bird;computer software;digital image;technological forecasting geographic information systems remote sensing visual databases global positioning system town and country planning cad;visual databases;technological forecasting	As economy is booming in China, the number of town grows rapidly. The rapid urbanization has brought many problems, such as the abuse of land development, the chaos of construction market, etc. Informationization based on GIS, RS and GPS is dedicated to this aid, that is, to build up geographical information system of planning and construction of town. However the biggest obstacle of informationization is the cost of building the spatial database, to which most of towns cannot afford. For this problem, by linking high-resolution remote sensing data, SuperMap GIS, and GPS, this paper proposes a workable solution. Quick Bird Image-a kind of high-resolution serves as main data source of spatial database, based on it CAD data or maps of planning and design of town are translated into predefined layers stored in spatial database by SuperMap GIS software, if there exists blanks, just digitize image into vector before screen. At last, a case study proves the above technological plan is a cost saving and fast one.	computer-aided design;distortion;geographic information system;global positioning system;image registration;image resolution;map;spatial database;towns	Jihua Hu;Yulin Cai	2004	IGARSS 2004. 2004 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2004.1368598	simulation;global positioning system;computer science;geographic information system;physics;remote sensing	Robotics	-39.306148463220055	-3.528958345661285	174902
712aa8e0d2d3d30671d5c8884a001a1742b526e0	measurements and modeling of moisture diffusion processes in transformer insulation using interdigital dielectrometry sensors	electrical engineering and computer science;thesis	Thesis (Ph.D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1999.	sensor;transformer	Yanqing Du	1999			electronic engineering;engineering;electrical engineering;mechanical engineering	Robotics	-46.903826558086806	-6.964607176199603	175014
8e1280d641747ae12a1dd4b3eb0c51ebb6df0586	geofem: high performance parallel fem for solid earth	science and technology;geophysics;degree of freedom;finite element method;finite element;large scale;large scale computing;linear elasticity;parallel;fiscal year;earth simulator;high performance	"""Abs t r ac t . The science and technology agency has begun an """"Earth Simulator"""" project from the fiscal year of 1997, which enables the forecast of various earth phenomena through the simulation of virtual earth placed in a supercomputer. The """"GeoFEM"""" is a parallel finite element software to be run on the """"Earth Simulator"""" to solve problems involving the solid earth and is being developed at RIST. This project is expected to be a breakthrough in bridging the geoscience and information science fields. In this paper, we briefly describe the """"GeoFEM"""" project, then capability for large-scale analysis is discussed and simple example analyses are shown. At this stage, the largest linear elastic problem solved by """"GeoFEM"""" is more than 100M (100,000,000) degree of freedoms on 1,000 PEs Hitachi SR2201 at University of Tokyo."""	bing maps platform;bridging (networking);earth simulator;finite element method;hitachi sr2201;information science;simulation;supercomputer	Kazuteru Garatani;Hisashi Nakamura;Hiroshi Okuda;Genki Yagawa	2001	Future Generation Comp. Syst.	10.1016/S0167-739X(00)00080-7	simulation;finite element method	HPC	-47.3130095853484	-7.923138500883292	175033
a808235a091ed285927d4e229a0a6506a7fa6b36	toward efficient and reliable genome analysis using main-memory database systems	spatial indexing;proximity join;multi dimensional database	Improvements in DNA sequencing technologies allow to sequence complete human genomes in a short time and at acceptable cost. Hence, the vision of genome analysis as standard procedure to support and improve medical treatment becomes reachable. In this vision paper, we describe important data-management challenges that have to be met to make this vision come true. Besides genome-analysis performance, data-management capabilities such as data provenance and data integrity become increasingly important to enable comprehensible and reliable genome analysis. We argue to meet these challenges by using main-memory database technologies, which combine fast processing capabilities with extensive data-management capabilities. Finally, we discuss possibilities of integrating genome-analysis tasks into DBMSs and derive new research questions.	computer data storage;data integrity;in-memory database	Sebastian Dorok;Sebastian Breß;Horstfried Läpple;Gunter Saake	2014		10.1145/2618243.2618276	computer science;bioinformatics;data mining;database	DB	-34.839279337677105	-2.2153481492788694	175161
fc6130b0306133b7c9054d7edf3a7435f9cf2c4a	toward a realization of knowledge creation grid for big data era	correlation knowledge creation grid big data diversity consensus building;connection merit knowledge creation grid big data era scale merit scope merit consensus building;grid computing big data;big data correlation semantics knowledge based systems mathematical model context measurement	"""We represent a new framework - Knowledge creation grid for Big data era. Currently, there are various types of data in various fields. The essences of ICT are """"scale merit,"""" """"scope merit,"""" and """"connection merit."""" The Big data itself represents """"scale merit"""" and """"scope merit,"""" because there are massive of data and these data are utilized in various fields. However, the aspect of the """"connection merit"""" in big data has not represented. To lead to value creation from such data, it is important to interconnect data set among heterogeneous fields. We propose Knowledge creation grid as the one method of interconnection for data set. Almost correlation between data sets are discovered by synchronization or co-occurring on spatiotemporal. We, persons discover correlation from the meaning of data. In this framework, a meaningful data set is abstracted and represented as knowledge. This abstraction as knowledge is possible to discover various effective correlations by diversity and consensus building. In this paper, we represent a whole design of Knowledge creation grid and its primitive functions to realize diversity and consensus building."""	big data;essence;interconnection	Takafumi Nakanishi	2014	2014 IIAI 3rd International Conference on Advanced Applied Informatics	10.1109/IIAI-AAI.2014.43	computer science;knowledge management;data science;data mining	ML	-35.022961335094756	-4.110464502830493	175229
42ababe83499757140f961db68a4a1ac2c703f9d	special issue on soft computing for information mining	soft computing	The rapid development of computers has not only led to an enormous increase in computing power but also to cheap mass storage media. As a result, collecting large amounts of data has nowadays become a routine in science, production, business, and commerce. However, to access the information contained in the raw data, appropriate data analysis techniques are needed. The traditional approach in statistics is to generate hypotheses first and to test them afterwards: One starts by defining the question to be answered, then designs an appropriate experiment and collects the data, and finally analyzes the data against the background of the original goals. This process is more or less turned upside down when permanently collecting huge amounts of data, mostly without having a concrete hypothesis in mind. As a consequence, there is a high demand for exploratory	soft computing	Ulrich Bodenhofer;Eyke Hüllermeier;Frank Klawonn;Rudolf Kruse	2007	Soft Comput.	10.1007/s00500-006-0105-3	computer science;artificial intelligence;machine learning;soft computing	NLP	-37.70795781100375	-8.215143625109048	175362
118f03887f6861c5703f894d6c23e9636c273a86	user-oriented querying over repositories of data and provenance	data repositories;query processing;ontology models;query processing data models natural sciences computing ontologies artificial intelligence;e science environment;ontologies data models grid computing computer science laboratories drugs medical treatment diseases database languages human immunodeficiency virus;user oriented querying;virtual laboratory;ontologies artificial intelligence;query translation tools;data model;unified model;ontology integration;query translation tools user oriented querying data repositories end user oriented approach e science environment ontology models data models;end user oriented approach;query translation;infectious disease;natural sciences computing;semantic grid;data models;in silico	We propose an end-user oriented approach to querying repositories of data and provenance in e-Science environments. The approach is based on ontology models describing multiple domains - in silico experiments, provenance, data, and applications. Those ontologies, integrated in a unified model and containing mappings to underlying data models, allow to query repositories of data and provenance in a unified way, or even combine provenance and data aspects in one query. We demonstrate QUery TRanslation tools (QUaTRo), built on top of the ontology models, which allow to construct complex queries over both data and provenance repositories, expressed in the terms of the domain familiar to end users. We present, in the context of the ViroLab virtual laboratory for infectious diseases, examples of construction of complex queries, combining provenance and data model aspects, which can be of practical value to scientists or medical users.	data model;e-science;experiment;natural language;ontology (information science);text mining;unified model;web ontology language;wizard (software)	Bartosz Balis;Marian Bubak;Jakub Wach	2007	Third IEEE International Conference on e-Science and Grid Computing (e-Science 2007)	10.1109/E-SCIENCE.2007.81	data modeling;infectious disease;semantic grid;data model;computer science;unified model;data mining;database;information retrieval	DB	-37.82225299139209	3.5591482352240753	175694
82485dc6d5a591349f9831749378747dbbaf9829	xml-based model libraries application in geographic information system	software libraries;water quality;xml geographic information systems software libraries water quality;high abstract level areas water quality model geography system extensible markup language xml based model library application geographic information system gis data processing graphic display;gir xml model libraries water quality model;geographic information systems;xml;libraries analytical models xml object oriented modeling computational modeling data models geographic information systems	In recent years, with the development of technology, the potential and application value of GIS (Geography Information System) has been shown in various fields. GIS performs well in data processing and graphic display but still has some shortcomings in some high-abstract-level areas like model library. In this paper we presented a model library by XML (Extensible Markup Language), and described its detailed description and realization. This model library has favorable encapsulation, expansibility and openness, perfectly meeting the demand of Geography System.	accession number (bioinformatics);automaton;encapsulation (networking);geographic information system;library (computing);library classification;markup language;openness;xml	Shuyi Zhang;Yi Li;Yong Gao	2013	2013 21st International Conference on Geoinformatics	10.1109/Geoinformatics.2013.6626105	xhtml;xml;xml schema;geography markup language;streaming xml;computer science;xml framework;soap;xml database;database;world wide web;xml schema editor;information retrieval;efficient xml interchange	DB	-38.80239715113267	-2.2955394990535245	175789
bd86e3653cdec0ff5a331ac13651a4bb6432d42d	mixed driven refinement design of multidimensional models based on agglomerative hierarchical clustering	olap;data mining;multidimensional design;data warehouse	Data warehouses (DW) and OLAP systems are business intelligence technologies allowing the on-line analysis of huge volume of data according to users’ needs. The success of DW projects essentially depends on the design phase where functional requirements meet data sources (mixed design methodology) (Phipps and Davis, 2002). However, when dealing with complex applications existing design methodologies seem inefficient since decision-makers define functional requirements that cannot be deduced from data sources (data driven approach) and/or they have not sufficient application domain knowledge (user driven approach) (Sautot et al., 2014b). Therefore, in this paper we propose a new mixed refinement design methodology where the classical data-driven approach is enhanced with data mining to create new dimensions hierarchies. A tool implementing our approach is also presented to validate our theoretical proposal.	algorithm;application domain;cluster analysis;data mining;database schema;dreamwidth;functional requirement;gqm;hierarchical clustering;online analytical processing;online and offline;rapid prototyping;refinement (computing);support vector machine	Lucile Sautot;Sandro Bimonte;Ludovic Journaux;Bruno Faivre	2015		10.5220/0005404605470555	online analytical processing;computer science;data science;data warehouse;data mining;database	EDA	-36.195048763286955	-1.1689615880285382	175837
82492eddb57b749ea7f66d7099010c5b8d1f985b	ontology based approach for precision agriculture		In this paper, we propose a framework of knowledge for an agriculture ontology which can be used for the purpose of smart agriculture systems. This ontology not only includes basic concepts in the agricultural domain but also contains geographical, IoT, business subdomains, and other knowledge extracted from various datasets. With this ontology, any users can easily understand agricultural data links between them collected from many different data resources. In our experiment, we also import country, sub-country and disease entities into this ontology as basic entities for building agricultural linked datasets later.	artificial intelligence;big data;cognitive map;entity;linked data;ontology (information science);preprocessor;velocity (software development);vocabulary	Quoc Hung Ngo;Nhien-An Le-Khac;M. Tahar Kechadi	2018		10.1007/978-3-030-03014-8_15	data mining;computer science;data link;knowledge base;ontology;precision agriculture;internet of things	AI	-39.677386775835004	-0.3458323076593966	176086
66faec70a8f780102dc2694ee3aa62e55ae56d9b	textgrid - virtual research environment for the humanities		The TextGrid research group, a consortium of 10 research institutions in Germany, is developing a virtual research environment for researchers in the arts and humanities that provides services and tools for the analysis of text data and supports the curation of research data by means of grid technology. The TextGrid virtual research environment consists of two main components: the TextGrid Laboratory (TextGridLab), which serves as the entry point to the virtual research environment, and the TextGrid Repository (TextGridRep), which is a long-term humanities data archive ensuring sustainability, interoperability and long-term access to research data. To support all stages of the research lifecycle, preserve and maintain research data, and ensure its long-term usefulness, existing research practices must be supported. Therefore the TextGridLab provides common functionalities in a sustainable environment to intensify re-use of data, tools, and services, and the TextGridRep enables researchers to publish and share their data in a way that supports long-term availability and re-usability. The International Journal of Digital Curation is an international journal committed to scholarly excellence and dedicated to the advancement of digital curation across a wide range of sectors. ISSN: 1746-8256 The IJDC is published by UKOLN at the University of Bath and is a publication of the Digital Curation Centre. Heike Neuroth et al. 223	archive;digital curation;entry point;international standard serial number;interoperability;research data archiving;text corpus;usability	Heike Neuroth;Felix Lohmeier;Kathleen Marie Smith	2011	IJDC	10.2218/ijdc.v6i2.198	human–computer interaction;knowledge management;database;world wide web	Visualization	-43.88745817173882	-0.9377892177320343	176271
cd2693de22d886c3e2f004f7861d078459d5f967	metadata management platform for the canadian research data centre network				Pascal Heus	2011			data mining;metadata;data center;meta data services;database;metadata management;data management plan;computer science	DB	-40.31638704032474	-1.8965881949429908	176311
a31790d4f33fb6dda8cf3d330c055a3bcce36128	the big mechanism program: changing how science is done.		The talk will describe details of actively evolving research conducted by the UChicago consortium of the Big Mechanism program, funded by the US DARPA agency. The consortium’s work focuses on: (1) probabilistic reasoning across cancer claims culled from literature which uses custom-designed ontologies; (2) the computational modelling of cancer mechanisms and pathways to automatically predict therapeutic clues; (3) automated hypothesis generation to strategically extend this knowledge, and; (4) developing a ‘Robot Scientist’ that performs experiments to test hypotheses probabilistically, then feeding those results back to the system.	big mechanism;experiment;ontology (information science)	Andrey Rzhetsky	2016			computer science	AI	-35.51795303022256	-9.756166803495434	176619
687e90c3fbe0067337b5191f181241da2833a9b1	advanced delivery mechanisms in the grelc project	relational data;data type;data distribution;xml and query;data access;grelc project;grid computing;protein data bank;data grid;relational data source	Today many Data Grid applications need to manage and process a very large amount of data distributed across multiple grid nodes. Several applications often access large databases (i.e. protein data banks, in the bioinformatics field) without any data access services taking into account characteristics of either applications or data types. Such applications could improve their performance and quality of results by using efficient, cross-DBMS, specialized and ad hoc implemented data access services. The Grid Relational Catalog Project (GRelC) developed at the CACT/ISUFI Laboratory of the University of Lecce provides a grid-enabled access service for relational and not relational repositories. In this paper we propose some advanced delivery mechanisms developed within the GRelC project, showing up experimental results related to an European testbed.	bioinformatics;data access;database;hoc (programming language);quality of results;testbed	Giovanni Aloisio;Massimo Cafaro;Sandro Fiore	2004		10.1145/1028493.1028505	data access;protein data bank;data type;relational database;computer science;data grid;data mining;database;world wide web;grid computing	HPC	-36.66777944482661	0.5814330282045648	176654
1fcc9f235e98d9cbf9e9a18f59b3e9708ad9e4cf	leveraging flexible data management with graph databases	graph database;property graph;linked open data	"""Integrating up-to-date information into databases from different heterogeneous data sources is still a time-consuming and mostly manual job that can only be accomplished by skilled experts. For this reason, enterprises often lack information regarding the current market situation, preventing a holistic view that is needed to conduct sound data analysis and market predictions. Ironically, the Web consists of a huge and growing number of valuable information from diverse organizations and data providers, such as the Linked Open Data cloud, common knowledge sources like Freebase, and social networks. One desirable usage scenario for this kind of data is its integration into a single database in order to apply data analytics. However, in today's business intelligence tools there is an evident lack of support for so-called situational or ad-hoc data integration. What we need is a system which 1) provides a flexible storage of heterogeneous information of different degrees of structure in an ad-hoc manner, and 2) supports mass data operations suited for data analytics. In this paper, we will provide our vision of such a system and describe an extension of the well-studied property graph model that allows to """"integrate and analyze as you go"""" external data exposed in the RDF format in a seamless manner. The proposed integration approach extends the internal graph model with external data from the Linked Open Data cloud, which stores over 31 billion RDF triples (September 2011) from a variety of domains."""	freebase;graph database;hoc (programming language);holism;linked data;seamless3d;social network;tag cloud;whole earth 'lectronic link;world wide web	Elena Vasilyeva;Maik Thiele;Christof Bornhövd;Wolfgang Lehner	2013		10.1145/2484425.2484437	data modeling;data quality;computer science;data virtualization;data science;data warehouse;linked data;data mining;database;enterprise information integration	DB	-38.62556192151581	1.9080275029901983	176819
018782b8fffed60e0ad262bfbd86f31687ca1676	scientific data mining in astronomy	moving object;information retrieval;real time;research paradigm;scientific data;data mining;data analysis;large synoptic survey telescope;data mining algorithm;time domain;heterogeneous data integration;distributed data mining	We describe the application of data mining algorithms to research problems in astronomy. We posit that data mining has always been fundamental to astronomical research, since data mining is the basis of evidencebased discovery, including classification, clustering, and novelty discovery. These algorithms represent a major set of computational tools for discovery in large databases, which will be increasingly essential in the era of data-intensive astronomy. Historical examples of data mining in astronomy are reviewed, followed by a discussion of one of the largest data-producing projects anticipated for the coming decade: the Large Synoptic Survey Telescope (LSST). To facilitate data-driven discoveries in astronomy, we envision a new data-oriented research paradigm for astronomy and astrophysics – astroinformatics. Astroinformatics is described as both a research approach and an educational imperative for modern data-intensive astronomy. An important application area for large timedomain sky surveys (such as LSST) is the rapid identification, characterization, and classification of real-time sky events (including moving objects, photometrically variable objects, and the appearance of transients). We describe one possible implementation of a classification broker for such events, which incorporates several astroinformatics techniques: user annotation, semantic tagging, metadata markup, heterogeneous data integration, and distributed data mining. Examples of these types of collaborative classification and discovery approaches within other science disciplines are presented.	algorithm;astroinformatics;cluster analysis;data-intensive computing;database;examples of data mining;imperative programming;markup language;programming paradigm;real-time locating system	Kirk D. Borne	2009	CoRR		astroinformatics;time domain;data mining;data stream mining;data analysis;data	ML	-35.88801453835511	-6.507469498921453	176958
7d915b88a26bfb939635cdd184874e8d3cdb25a9	ontology-based natural language query interfaces for data exploration		Enterprises are creating domain-specific knowledge bases by curating and integrating all their business data, structured, unstructured and semi-structured, and using them in enterprise applications to derive better business decisions. One distinct characteristic of these enterprise knowledge bases, compared to the open-domain general purpose knowledge bases like DBpedia [16] and Freebase [6], is their deep domain specialization. This deep domain understanding empowers many applications in various domains, such as health care and finance. Exploring such knowledge bases, and operational data stores requires different querying capabilities. In addition to search, these databases also require very precise structured queries, including aggregations, as well as complex graph queries to understand the various relationships between various entities of the domain. For example, in a financial knowledge base, users may want to find out “which startups raised the most VC funding in the first quarter of 2017”; a very precise query that is best expressed in SQL. The users may also want to find all possible relationships between two specific board members of these startups, a query which is naturally expressed as an all-paths graph query. It is important to note that general purpose knowledge bases could also benefit from different query capabilities, but in this paper we focus on domain-specific knowledge graphs and their query needs. Instead of learning and using many complex query languages, one natural way to query the data in these cases is using natural language interfaces to explore the data. In fact, human interaction with technology through conversational services is making big strides in many application domains in recent years [13]. Such interfaces are very desirable because they do not require the users to learn a complex query language, such as SQL, and the users do not need to know the exact schema of the data, or how it is stored. There are several challenges in building a natural language interface to query data sets. The most difficult task is understanding the semantics of the query, hence the user intent. Early systems [3, 30] allowed only a set of keywords, which had very limited expressive power. There have been works to interpret the semantics of a full-blown English language query. These works in general try to disambiguate among the potentially multiple meanings of the words and their relationships. Some of these are machine-learning based [5, 24, 29] that require good training sets, which are hard to obtain. Others require user feedback [14, 17, 18]. However, excessive user interaction to resolve ambiguities can be detrimental to user experience. In this paper, we describe a unique end-to-end ontology-based system for natural language querying over complex data sets. The system uses domain ontologies, which describe the semantic entities and their relationships, to reason about and capture user intent. To support multiple query types, the system provides a poly store	aggregate function;dbpedia;data store;database;end-to-end principle;enterprise software;entity;expressive power (computer science);freebase;instance (computer science);knowledge graph;knowledge base;machine learning;natural language user interface;need to know;ontology (information science);partial template specialization;query language;sql;semiconductor industry;user experience	Chuan Lei;Fatma Özcan;Abdul Quamar;Ashish R. Mittal;Jaydeep Sen;Diptikalyan Saha;Karthik Sankaranarayanan	2018	IEEE Data Eng. Bull.		data exploration;database;data mining;ontology;computer science;natural language user interface	DB	-34.54953195074489	3.210184107875309	176983
c636e165bd76e4b5551e78d61847732faf2220b7	heimdall - internet of things (iot) platform for data retrieval			data retrieval;heimdall;internet of things	Gabriel M. Costa;M. Araujo;Tiago Cruz de França;Claudio Miceli de Farias	2018				Vision	-39.828158071260326	-6.47307470071039	177103
4d117513771ff45d5d7f3c5c0366e1d566ee604a	towards wikis as semantic hypermedia	semantic wikis;hypermedia;semantic web;of research and development;world wide web;wikis;semantic wiki	Similarly to the Web Wikis have advanced from initially simple ad-hoc solutions to highly popular systems of widespread use. This evolution is reflected by the impressive number of Wiki engines available and by the numerous settings and disciplines they have found applicability to in the last decade. In conjunction to these rapid advances the question on the fundamental principles underlying the design and the architecture of Wiki technologies becomes inevitable for their systematic further development and their long-lasting success at public, private and corporate level. This paper aims at be part of this endeavor; building upon the natural relationship between Wikis and hypermedia, we examine to which extent the current state of the art in the field (complemented by results achieved in adjacent communities such as the World Wide Web and the Semantic Web) fulfills the requirements of modern hypermedia systems. As a conclusion of the study we outline further directions of research and development which are expected to contribute to the realization of this vision.	hoc (programming language);hypermedia;outline (list);requirement;semantic web;wiki;world wide web	Robert Tolksdorf;Elena Paslaru Bontas Simperl	2006		10.1145/1149453.1149470	personal wiki;computer science;knowledge management;web navigation;social semantic web;semantic web stack;multimedia;world wide web	Web+IR	-45.211727963508416	3.1567617885064063	177168
07696b347942f9dddd4fca9ee44cae883ffd288b	a bottom-up approach for automatically grouping sensor data layers by their observed property	sos;information retrieval;data mining;ogc;data interoperability;gis	The Sensor Web is a growing phenomenon where an increasing number of sensors are collecting data in the physical world, to be made available over the Internet. To help realize the Sensor Web, the Open Geospatial Consortium (OGC) has developed open standards to standardize the communication protocols for sharing sensor data. Spatial Data Infrastructures (SDIs) are systems that have been developed to access, process, and visualize geospatial data from heterogeneous sources, and SDIs can be designed specifically for the Sensor Web. However, there are problems with interoperability associated with a lack of standardized naming, even with data collected using the same open standard. The objective of this research is to automatically group similar sensor data layers. We propose a methodology to automatically group similar sensor data layers based on the phenomenon they measure. Our methodology is based on a unique bottom-up approach that uses text processing, approximate string matching, and semantic string matching of data layers. We use WordNet as a lexical database to compute word pair similarities and derive a set-based dissimilarity function using those scores. Two approaches are taken to group data layers: mapping is defined between all the data layers, and clustering is performed to group similar data layers. We evaluate the results of our methodology.	apple sos;approximate string matching;bottom-up parsing;bottom-up proteomics;categorization;cluster analysis;consortium;database normalization;geographic information system;hereditary property;internet;interoperability;lexical database;ontology (information science);prototype;recommender system;satellite digital imaging system;semantic similarity;sensor web;sorting;string searching algorithm;tokenization (data security);top-down and bottom-up design;usability;www;wordnet;world wide web	Ben Knoechel;Chih-Yuan Huang;Steve H. L. Liang	2013	ISPRS Int. J. Geo-Information	10.3390/ijgi2010001	sensor web;computer science;data mining;world wide web;information retrieval	DB	-40.8225918020438	1.006388838981051	177234
49a7e0915d83b275ad9ce0fb88a4136dc184b2cc	mynda: an intelligent data mining application generator	mynda;data mining tools;application generator	Development of a Decision Support System (DSS) based on data mining is expensive. It consists of three main phases: produce quality input data, develop quality knowledge models and developed an application based on the model, which needs experts in the domain, data mining and software development respectively. Current commercial data mining tools, such as Insightful miner, aims for the development of quality knowledge models which are conducted by data mining expert. The knowledge model is not meaningful to the end user without the development of a DSS application based on the knowledge model. Mynda is a web-based data mining tool for domain expert users to generate knowledge models from client's data (model generator) and also generate a data mining application from the knowledge model (application generator). The user only provides input data sets (for example in Excel format) and set the mining technique profile. Mynda will automatically develop the knowledge model and generate an executable data mining application based on the profile. The data mining application can be run independently as a stand alone application. Mynda has reduced the complexity of the development of data mining based DSS applications.		Zulaiha Ali Othman;Abdul Razak Hamdan;Azuraliza Abu Bakar;Suhaila Zainudin;Hafiz Mohd Sarim;Mohd Zakree Ahmad Nazri;Zalinda Othman;Salwani Abdullah;Masri Ayob;Ahmad Tarmizi Abdul Ghani	2011		10.1007/978-3-642-25200-6_21	text mining;software mining;computer science;data science;data mining;database;knowledge extraction;data stream mining	ML	-36.49493449737386	-0.19243033659626688	177521
4241eb709f5d32f96c131f0dc3e9e309f82f2149	the lifecycle of provenance metadata and its associated challenges and opportunities		This chapter outlines some of the challenges and opportunities associated with adopting provenance principles [CFLV12] and standards [MGC15] in a variety of disciplines, including data publication and reuse, and information sciences.	information science	Paolo Missier	2016	CoRR		computer science;data mining;database;world wide web	DB	-42.72247268463446	0.30166990257319354	177849
4d0f4c6a70393014874acf375c4b17d0e788f608	building virtual earth observatories using scientific database and semantic web technologies		TELEIOS is a recent European project that addresses the need for scalable access to petabytes of Earth Observation (EO) data and the identification of hidden knowledge that can be used in applications. To achieve this, TELEIOS builds on scientific databases, linked geospatial data and ontologies. TELEIOS was the first project internationally that introduced the Linked Data paradigm to the EO domain, and developed prototype services such as the real-time fire monitoring service that has been used for the last two years by decision makers and emergency response managers in Greece.	bing maps platform;database;linked data;ontology (information science);petabyte;programming paradigm;prototype;real-time clock;scalability;semantic web	Kostis Kyzirakos;Stefan Manegold;Charalampos Nikolaou;Manolis Koubarakis	2014	ERCIM News		data web;semantic grid;computer science;data science;world wide web;information retrieval	HPC	-40.15489381403105	-1.6466553136564916	177956
06724adb82cf5693154dfafc0a716c558b1dc635	1st workshop on big data management in clouds - bdmc2012	exascale system;exabyte era gain;computational science data management;data volumes increase;big data;big data management;revolutionary new approach;new architecture;new trend;overall performance;traditional file-based data management	As data volumes increase at exponential speed in more and more application fields of science, the challenges posed by handling Big Data in the Exabyte era gain an increasing importance. High-energy physics, statistics, climate modeling, cosmology, genetics or bio-informatics are just a few examples of fields where it becomes crucial to efficiently manipulate Big Data, which are typically shared at large scale. Rapidly storing this data, protecting it from loss and analyzing it to understand the results are significant challenges, made more difficult by decades of improvements in computation capabilities that have been unmatched in storage. For many applications, the overall performance and scalability becomes clearly driven by the performance of the data handling subsystem. As we anticipate Exascale systems in 2020, there is a growing consensus in the scientific community that revolutionary new approaches are needed in computational science data management. These new trends lead us to rethink the traditional file-based data management abstraction for large-scale applications. Moreover, for obvious cost-related reasons, new architectures are clearly needed as well as alternate infrastructures to supercomputers., like hybrid or HPC clouds.	big data	Alexandru Costan;Ciprian Dobre	2012		10.1007/978-3-642-36949-0_1	parallel computing;simulation;computer science;data science;operating system;data mining;database;distributed computing	DB	-37.46361462772025	-7.394125024938063	178000
c9665ab15b1616d25d47c31e34e0b09881cb60ac	data warehouses are dead, long live data warehousing!				Mehul Shah	2019			computer science;database;data warehouse	DB	-40.01939796934631	-5.291816195500199	178043
e8a499834a66dfc3c1410f76b76e53f7e845e256	making databases easier to use.		Database design, development and documentation. Data Warehousing and Data Mart ETL projects. Database conversion and migration projects. Available TODAY as an integrated package offering the best value database toolkit. Improve productivity, better manage your database environment and above all save time and money. A 2003 survey of over 500 IT Managers, Application Developers and Database Administrators shows that more than 90% are seeking productivity gains. Of those surveyed, 50% wished to improve central administration and 50% sought help with database conversions. 29% sought productivity gains with data synchronization projects and 28% were looking for Data Warehousing tools. Finally 30% wanted help when developing applications in a multi-DBMS environment.	data mart;data synchronization;database design;documentation;system migration	Jim Zeigler	1993			computer science;database	DB	-43.16159779983484	-3.5637456918998924	178151
48f7dfaafe60d40a115fda6bc143e3ca181e4657	tackling the provenance challenge one layer at a time	visualization;provenance;workflow evolution	VisTrails is a new workflow and provenance management system that provides support for scientific data exploration and visualization. Whereas workflows have been traditionally used to automate repetitive tasks, for applications that are exploratory in nature, change is the norm. VisTrails uses a new change-based provenance mechanism which was designed to handle rapidly-evolving workflows. It uniformly and automatically captures provenance information for data products and for the evolution of the workflows used to generate these products. In this paper, we describe how the VisTrails provenance data is organized in layers and present a first approach for querying this data that we developed to tackle the Provenance Challenge queries.	vistrails	Carlos Eduardo Scheidegger;David Koop;Emanuele Santos;Huy T. Vo;Steven P. Callahan;Juliana Freire;Cláudio T. Silva	2008	Concurrency and Computation: Practice and Experience	10.1002/cpe.1237	visualization;computer science;data science;management system;database;world wide web;data	DB	-41.25137130515656	0.3104222545284692	178794
b3555e835e9f859b17817d37ae60a0197b97baac	demonstration of the fdb query engine for factorised databases	relational databases;nell repository;fdb query engine;factorised result;select-project-join query;factorised databases;query performance;in-memory query engine;compact factorised representation;data redundancy	FDB is an in-memory query engine for factorised databases, which are relational databases that use compact factorised representations at the physical layer to reduce data redundancy and boost query performance. We demonstrate FDB using real data sets from IMDB, DBLP, and the NELL repository of facts learned from Web pages. The users can inspect factorisations as well as plans used by FDB to compute factorised results of select-projectjoin queries on factorised databases. 1. FACTORISED DATABASES The thesis underlying factorised databases is that relational databases can admit compact representations by algebraic factorisation using distributivity of product over union. This is similar in spirit to the relationship between logic functions in disjunctive normal form and their equivalent nested forms obtained by algebraic factorisation. In earlier work [7] we give a complete characterisation of the compactness of factorised results for select-project-join queries on relational databases and show that the gap between the sizes of query results and of their factorised representations can be exponential. In particular, there are arbitrarily large queries for which the query results have sizes exponential in the query size yet their factorised representations only have sizes bounded by the input database size. A similar exponential gap holds between the times needed to compute from the input relational database the query results and their factorised representations. Furthermore, the succinctness and performance gaps widen when we consider factorised databases as input. Experiments with our in-memory engine FDB for select-project-join queries on factorised databases show that FDB can be up to six orders of magnitude faster than relational engines such as PostgreSQL, SQLite, and a home-bred in-memory relational engine, for a wide range of queries on data sets with many-to-many relationships [3]. Factorised databases have applications beyond relational query evaluation. Factorised provenance polynomials are used as compact encoding of provenance [6] and for efficient query evaluation in probabilistic databases [8]. Factorised representations are a natural fit whenever we deal with a large space of possibilities and can be used to represent, e.g., AND/OR trees used in design specification [5] and world-set decompositions used for incomplete information [1]. They can also be used to compactly represent the space of feasible solutions to configuration problems in constraint satisfaction, where we need to connect a set of components so as to meet an objective while respecting given constraints [2]. The focus of this demonstration is our query engine FDB. The audience will experiment with FDB on several data sets including the NELL knowledge base learned from a large corpus of Web pages [4], will explore visually FDB evaluation plans as well as factorised intermediate and final query results, and will compare the time and space requirements of FDB to those of PostgreSQL and SQLite. FDB will be introduced to the audience by examples such as those in Section 2. The emphasis of the demonstration will be on FDB’s evaluation and optimisation techniques, in particular on (1) the evaluation plans with novel operators to restructure factorisations, and on (2) the interplay of the standard optimisation objective of minimising the overall computation time and of the new objective of computing small factorised representations of query results. 2. FDB BY EXAMPLES We introduce factorised databases and FDB by examples using the NELL data set. The demonstration will feature the examples from this section and further scenarios using NELL as well as IMDB and DBLP data sets. We will also show how FDB can manage solutions to crossword puzzles that are gradually refined as clues are supplied. Sports Scenario. The NELL data set contains a myriad of small-size unary relations about players, teams, sports leagues and stadiums, and binary relations such as PlaysFor (players play for teams), CompetesIn (teams play in leagues), LeagueStadium (leagues played on stadiums), BasedIn and IsIn (teams and stadiums located in cities). We exemplify with queries that join these relations to compute (Q1) players, teams and leagues that one can see playing on stadiums, (Q2) teams and stadiums co-located in the same city, and (Q3) players playing on stadiums in their home cities. The NELL binary relations are in general many-to-many. This makes the factorised results of our queries very compact when compared to flat relational representations. In the sequel, we discuss challenges in computing factorised results of our queries on input relational or factorised databases. PlaysFor player team Messi Barcelona Villa Barcelona Cech Chelsea Torres Chelsea van Persie Arsenal CompetesIn team league Barcelona Primera Barcelona Champions Chelsea Premier Chelsea Champions Arsenal Premier LeagueStadium league stadium Primera CampNou Champions CampNou Champions Wembley Premier Stamford Premier Wembley BasedIn team city Barcelona Barcelona Chelsea London Arsenal London IsIn stadium city CampNou Barcelona Wembley London Stamford London Figure 1: A sample from the NELL database. A constantly evolving database is available at rtw.ml.cmu.edu. T1: T2: T3: T4: team	computation;constraint satisfaction;crossword;dbl-browser;data redundancy;disjunctive normal form;exemplification;fdb (file format);grammar-based code;in-memory database;internet movie database (imdb);knowledge base;linear algebra;many-to-many (data model);mathematical optimization;monoid factorisation;never-ending language learning;polynomial;postgresql;probabilistic database;relational database;requirement;sqlite;the times;time complexity;unary operation;web page	Nurzhan Bakibayev;Dan Olteanu;Jakub Závodný	2012	PVLDB	10.14778/2367502.2367545	computer science;data mining;database;world wide web;information retrieval	DB	-34.80106299575275	2.806258922074383	179010
55dfece9cb440d120843a024360afb2c49b4279a	seki@home, or crowdsourcing an open knowledge graph api		In May 2012, the Web search engine Google has introduced the so-called Knowledge Graph, a graph that understands real-world entities and their relationships to one another. It currently contains more than 500 million objects, as well as more than 3.5 billion facts about and relationships between these different objects. Soon after its announcement, people started to ask for a programmatic method to access the data in the Knowledge Graph, however, as of today, Google does not provide one. With SEKI@home, which stands for Search for Embedded Knowledge Items, we propose a browser extension-based approach to crowdsource the task of populating a data store to build an Open Knowledge Graph. As people with the extension installed search on Google.com, the extension sends extracted anonymous Knowledge Graph facts from Search Engine Results Pages (SERPs) to a centralized, publicly accessible triple store, and thus over time creates a SPARQL-queryable Open Knowledge Graph. We have implemented and made available a prototype browser extension tailored to the Google Knowledge Graph, however, note that the concept of SEKI@home is generalizable for other knowledge bases.	application programming interface;browser extension;centralized computing;crowdsourcing;data store;entity;google search;knowledge graph;knowledge base;linked data;norm (social);open knowledge;population;prototype;sparql;search engine results page;triplestore;web search engine;world wide web	Thomas Steiner;Stefan Mirea	2012				Web+IR	-36.98084466713494	3.733208620751363	179129
bf9d54f5c19aa7ca4abf2d783b0e1fe3b7416c9a	research on real time processing and intelligent analysis technology of power big data		This paper focuses on the research of all kinds of data in the power grid business system, and carries on the research of intelligent analysis technology. It focuses on breaking the technical difficulties of intelligent and efficient analysis and mining, distributed multi-stream real-time processing, developing large-scale stock data for power and high-frequency incremental data efficient analysis system prototype, for wide-area distributed multi-stream real-time computing, from the data quick access to valuable information to solve the problems in the grid business system to improve the overall business performance, to achieve support power Data real-time processing and other technology applications.	big data;prototype;real-time clock;real-time computing;real-time locating system;real-time transcription	Jiarui Xue;Xiangzhou Chen;Huixia Ding;Xiao He	2017		10.1145/3175684.3175717	data mining;grid;computer science;big data;data flow diagram	HPC	-34.131049057758005	-1.8855423624594716	179212
59d88a3eae10b67cb216357d8f0f93ea918bfd78	peer-to-peer management of xml data: issues and research challenges	p2p system;data sharing;data management;p2p;indexation;peer to peer;query routing	Peer-to-peer (p2p) systems are attracting increasing attention as an efficient means of sharing data among large, diverse and dynamic sets of users. The widespread use of XML as a standard for representing and exchanging data in the Internet suggests using XML for describing data shared in a p2p system. However, sharing XML data imposes new challenges in p2p systems related to supporting advanced querying beyond simple keyword-based retrieval. In this paper, we focus on data management issues for processing XML data in a p2p setting, namely indexing, replication, clustering and query routing and processing. For each of these topics, we present the issues that arise, survey related research and highlight open research problems.	cluster analysis;open research;peer-to-peer;routing;xml	Georgia Koloniari;Evaggelia Pitoura	2005	SIGMOD Record	10.1145/1083784.1083788	xml schema;data management;computer science;soap;peer-to-peer;data mining;database;world wide web;xml schema editor;cxml;efficient xml interchange	DB	-34.03224786744649	2.154786708742429	179643
80a6025d711cad643a2ec0be06a1019b6f302001	from sensor to observation web with environmental enablers in the future internet	micro electrical mechanical systems;climate;environmental usage area;water quality;future internet;internet of things;internet;internet of content;air pollution;internet of services;environmental enablers;internet of people;humans;europe;sensor web;open standards;biodiversity;observation web;environmental monitoring;requirements analysis	"""This paper outlines the grand challenges in global sustainability research and the objectives of the FP7 Future Internet PPP program within the Digital Agenda for Europe. Large user communities are generating significant amounts of valuable environmental observations at local and regional scales using the devices and services of the Future Internet. These communities' environmental observations represent a wealth of information which is currently hardly used or used only in isolation and therefore in need of integration with other information sources. Indeed, this very integration will lead to a paradigm shift from a mere Sensor Web to an Observation Web with semantically enriched content emanating from sensors, environmental simulations and citizens. The paper also describes the research challenges to realize the Observation Web and the associated environmental enablers for the Future Internet. Such an environmental enabler could for instance be an electronic sensing device, a web-service application, or even a social networking group affording or facilitating the capability of the Future Internet applications to consume, produce, and use environmental observations in cross-domain applications. The term """"envirofied"""" Future Internet is coined to describe this overall target that forms a cornerstone of work in the Environmental Usage Area within the Future Internet PPP program. Relevant trends described in the paper are the usage of ubiquitous sensors (anywhere), the provision and generation of information by citizens, and the convergence of real and virtual realities to convey understanding of environmental observations. The paper addresses the technical challenges in the Environmental Usage Area and the need for designing multi-style service oriented architecture. Key topics are the mapping of requirements to capabilities, providing scalability and robustness with implementing context aware information retrieval. Another essential research topic is handling data fusion and model based computation, and the related propagation of information uncertainty. Approaches to security, standardization and harmonization, all essential for sustainable solutions, are summarized from the perspective of the Environmental Usage Area. The paper concludes with an overview of emerging, high impact applications in the environmental areas concerning land ecosystems (biodiversity), air quality (atmospheric conditions) and water ecosystems (marine asset management)."""	acclimatization;addresses (publication format);best practice;bridging (networking);common platform;community;computation;convergence (action);data quality;deploy;ecosystem services;environmental informatics;future internet;generic drugs;grand challenges;handling (psychology);image resolution;information systems;information management;information retrieval;information system;interoperability;knowledge management;microsoft outlook for mac;numerous;outlines (document);pointer <dog>;programming paradigm;propagation of uncertainty;requirement;scalability;sensor web;service-oriented architecture;simulation;software deployment;software propagation;solutions;spatial scale;usability;virtual reality;web service;research grants;sensor (device);standards characteristics	Denis Havlik;Sven Schade;Zoheir A. Sabeur;Paolo Mazzetti;Kym Watson;Arne-Jørgen Berre;José Lorenzo	2011		10.3390/s110403874	climate;sensor web;requirements analysis;biodiversity;environmental management system;the internet;simulation;open standard;computer science;engineering;knowledge management;environmental monitoring;internet of things;air pollution	HCI	-40.33188927831509	-1.3042506277310986	179834
c10a3058be43e5aae0322b1ab556822e880c83cf	toward intelligent data warehouse mining: an ontology-integrated approach for multi-dimensional association mining	association mining;intelligent assistance;pattern generation;issn 0957 4174;user preferences;data mining;multi dimensional;decision support system;ontology integration;association rule;knowledge discovery and data mining;expert systems with applications;data warehousing;multidimensional association rule;data warehouse;domain ontology;ontology	A data warehouse is an important decision support system with cleaned and integrated data for knowledge discovery and data mining systems. In reality, the data warehouse mining system has provided many applicable solutions in industries, yet there are still many problems causing users extra problems in discovering knowledge or even failing to obtain the real and useful knowledge they need. To improve the overall data warehouse mining process, we present an intelligent data warehouse mining approach incorporated with schema ontology, schema constraint ontology, domain ontology and user preference ontology. The structures of these ontologies are illustrated and how they benefit the mining process is also demonstrated by examples utilizing rule mining. Finally, we present a prototype multidimensional association mining system, which with intelligent assistance through the support of the ontologies, can help users build useful data mining models, prevent ineffective 1 Chin-Ang Wu is also a lecturer in Cheng Shiu University, Niaosong Township, Kaohsiung County 833.hsiung County * Corresponding author. Tel: +886-7-5919517, Fax: +886-7-5919514	association rule learning;data mining;decision support system;failure;fax;national supercomputer centre in sweden;ontology (information science);prototype;uniform resource identifier;web ontology language	Chin-Ang Wu;Wen-Yang Lin;Chang-Long Jiang;Chuan-Chun Wu	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.02.144	concept mining;web mining;association rule learning;software mining;dimensional modeling;computer science;data science;data warehouse;data mining;database;knowledge extraction;data stream mining	ML	-38.63707583728185	2.6217647322129887	179860
535a2e5abfb79cbbf06d301229ede6674f3e50cb	enriching the description of learning resources on disaster risk reduction in the agricultural domain: an ontological approach	search and retrieval;learning resource;associative learning;semantic metadata;disaster risk reduction;natural disaster;domain ontology;climate sensitivity	The collection, compilation and dissemination of relevant information and knowledge about the risk of natural disasters is a critical element in the Hyogo Framework for Action 2005-2015 for disaster risk reduction. Knowledge, innovation and education are needed not only to build a culture of safety and resilience at all levels, but also to mainstream disaster risk reduction especially in weather and climate-sensitive sectors such as agriculture. Describing learning resources about these topics with semantic metadata enhances their availability and further facilitates the search and retrieval process by using richer annotations based on ontologies. This paper reports about ongoing work concerning the creation of a domain ontology based on AGROVOC as well as the UN/ISDR and related terminology on disaster risk reduction for the description of associated learning resources in the agricultural domain.		Thomas Zschocke;Juan Carlos Villagrán de León;Jan Beniest	2010		10.1007/978-3-642-16552-8_29	natural disaster;knowledge management;data science;data mining;climate sensitivity	AI	-40.55109747163992	-2.2948857208399898	179983
958efd48d1ba21fd1aee717f8c2425b0cda82596	spatial database to store years of earth observation information obtained from field expeditions in the amazon				Gabriel Crivellaro Goncalves;Lucas Augusto de Souza;Maria Isabel Sobral Escada;Silvana Amaral	2018				DB	-41.52386216737493	-7.255009763346683	180099
5ea3ff5390ec5c36db5ecc8d7a04fa2d88559005	persisting big-data: the nosql landscape	database distribution;big data;nosql databases;database persistence;relational databases;distributed systems	The growing popularity of massively accessed Web applications that store and analyze large amounts of data, being Facebook, Twitter and Google Search some prominent stores, known as NoSQL databases, has arisen. This paper reviews implementations of NoSQL databases in order to provide an understanding of current tools and their uses. First, NoSQL databases are compared with traditional RDBMS and important concepts are explained. Only databases allowing to persist data and distribute them along different computing nodes are within the scope of this review. Moreover, NoSQL databases are divided into different types: Key-Value, Wide-Column, Document-oriented and Graphoriented. In each case, a comparison of available databases is carried out based on their most important features. & 2016 Published by Elsevier Ltd.	amazon simple storage service;berkeley db;big data;cloud database;cloud storage;coexist (image);computer data storage;confidentiality;data as a service;deployment environment;distributed database;encryption;google search;mongodb;mysql;neo4j;niche blogging;nosql;open-source software;openshift;persistence (computer science);platform as a service;postgresql;redis;relational database management system;riak;software design;terabyte;titan;web application;web page	Alejandro Corbellini;Cristian Mateos;Alejandro Zunino;Daniela Godoy;Silvia N. Schiaffino	2017	Inf. Syst.	10.1016/j.is.2016.07.009	big data;relational database;computer science;data mining;xml database;database;world wide web	DB	-35.63902893766149	0.3034015003882897	180306
c19f24eb504b0970b5945f0dcd5724b30cb1aa1a	guest editorial for the special issue collaborative p2p systems	p2p system	The last few years have witnessed a rapid expansion of collaborative applications, going from popular applications such as Twitter to international research projects like the Large Hadron Collider, which have made collaboration a part of the daily life of many people. Unfortunately, collaborative applications commonly use centralized infrastructures which, in practice, do not scale. One notorious example is Twitter. Twitter has a history of substantial downtime, particularly during times of high load. For instance, as many as 3% of page requests in June 2008 yielded ‘‘over capacity” errors. To accommodate its rapid growth, Twitter has been spending thousands of dollars to improve its performance, though its growing pool of users greedily consumes the extra capacity. Such situations have generated a great interest in the design and development of scalable infrastructures for collaborative applications. It has been argued that peer-to-peer (P2P) technology could provide the substrate to facilitate large-scale collaboration on the Internet. However, providing the same functionality of a centralized service on a fully distributed system is not trivial: the difficulty in managing collaborative knowledge, the inability to both keep track of concurrent changes and to encourage peers to contribute their resources, the need to develop new infrastructures for searching and delivering ephemeral content such as news and microblog discussions are questions that require sophisticated algorithms. Recently, P2P systems have also found their way into social networks, basically because social networks can be driven, for example, to improve collaborative P2P applications by leveraging the inherent trust associated with social links. We see social connectivity as yet another variable to tackle, which can be particularly useful to search information, to aggregate community data or to facilitate communication between users sharing the same interests. Again, we may witness a promising future if collaborative applications can harness social connectivity. This special issue has its origins in the 4th International Workshop on Collaborative Peer-to-Peer Systems (COPS), which was held within IEEE WETICE in Rome (Italy) on June 23–25, 2008. The focus of the workshop was to discuss new research ideas and prepare position papers on various hot	aggregate data;centralized computing;distributed computing;downtime;greedy algorithm;large hadron collider;peer-to-peer;scalability;social engineering (security);social network;yet another	Pedro García López;Michael W. Sobolewski;Marc Sánchez Artigas	2010	Computer Networks	10.1016/j.comnet.2010.06.011	computer science	OS	-46.89889301649967	0.44161881212052395	180326
47b3a6c12ddd9af32f3f118bb59cd24944cf3c49	fermi estimate on the web: placing sensor networks on the web with noise		This paper proposes a framework for web intelligence based on virtual sensor networks on the web. “Strong AI” requires a framework that allows the AI to make inferences using incomplete, dynamic and growing data on the web. An example of such inferences may be found in the human reasoning known as the “Fermi estimate”. However, the huge amounts of data available through the internet necessitate an appropriate reasoning engine. The Fermi estimate is reformulated to be used in a virtual sensor network and several examples are discussed. © 2015 The Authors. Published by Elsevier B.V. Peer-review under responsibility of KES International.	artificial general intelligence;internet;semantic reasoner;sensor node;web intelligence;web search query;world wide web	Yoshiteru Ishida;Ryunosuke Chiba	2015		10.1016/j.procs.2015.08.285	web modeling;artificial intelligence;machine learning;data mining;database;world wide web	AI	-38.68876714579599	1.4242016464574334	180376
b48e972668becef65206a2809cc7dbfb89029476	an information system for judicial and public administration using artificial intelligence and geospatial data		The adoption of information technology in judicial and public administration has become a major need nowadays with the rapid growth of information regarding managerial issues. This paper presents an advanced methodology developed by using Information and Communication Technologies (ICT) and artificial intelligence to support decision making in public and judicial administration. A prototype Management Information System for public administration (MISPA) was developed to provide a computerized way of managing geospatial urban, environmental and crime data of an urban area. This system was developed by using several programming languages, a Database Management System (DBMS) and other technologies and programming tools. The proposed system was developed aiming at the systemization and modernization of public, judicial and police authorities that are associated with issues that have to be dealt by studying urban data regarding crime and environmental data and supports decision making based on crime forecasting.	artificial intelligence;management information system;programming language;programming tool;prototype;relational database management system	Georgios N. Kouziokas	2017		10.1145/3139367.3139402	geospatial analysis;information system;management information systems;environmental data;public administration;computer science;modernization theory;information and communications technology;information technology;artificial intelligence	AI	-38.2893419983699	-3.9481246493250857	180429
dc05a3e11b7852f86e27e84c0e6a14650e8e586e	a practical solution to the geospatial data barrier problem	geospatial data	The inconvenience and expense of converting geospatial data from one format to another is a significant barrier to the proliferation of geographic information system applications: up to 85% of the cost of a new GIS may be attributable to data conversion costs. Although standardization efforts are under way, they are unliiely to eliminate the problem This paper proposes a solution, the Open Geospatial Datastore Interface (OGDI), an application programming interface (API) that provides a standardized method through which a GIS software package can access, “on the fly”, a variety of geospatial data products. OGDI uses a client/server architecture to facilitate the dissemination of gcospatial data products both locally and over any TCWIP network, and a driver-oriented approach to provide access to many geospatial data products and formats. The use of OGDI in the GRASSLAND GIS software is demonstrated, highlighting direct access to several geospatial data formats, including GRASS, VRP, ADRG and DTBD. OGDI and its source code are freeware.	application programming interface;client–server model;data store;gis file formats;geographic information system;on the fly;random access;server (computing);vehicle routing problem	Christian Larouche;Gilles Clement;Denis Gouin;Paul Morin	1997		10.1145/267825.267842	computer science;geospatial analysis;cartography;remote sensing;geographic information systems in geospatial intelligence	OS	-40.93149160898782	-3.3446605014795128	180551
a2e125c2ba38e21c99d15fb0d535928288dddba0	building the mass storage system at jefferson lab ian bird, bryan hess, andy kowalski	databases;file servers;birds drives java throughput software libraries virtual machining technology management databases file servers hardware;software libraries;department of energy;data collection;virtual machining;drives;technology management;data analysis;birds;mass storage system;throughput;hardware;java	Thomas Jefferson National Accelerator Facility (Jefferson Lab) is a U.S. Department of Energy Facility [1] conducting Nuclear Physics experiments that currently have data collection rates of up to 20 MB/second. Future experiments, however, are expected to greatly exceed these rates. Post processing and data analysis produce similar amounts of data. Both the raw and processed data are stored on tape and need to be easily accessible. Between data collection and processing, the Mass Storage System at Jefferson Lab currently moves over 2 TB of data per day.	experiment;markus hess;mass storage;terabyte	Ian Bird;Bryan Hess;Andy Kowalski	2001	2001 Eighteenth IEEE Symposium on Mass Storage Systems and Technologies	10.1109/MSS.2001.10005	computer science;operating system;database;world wide web	Embedded	-43.76487566691723	-4.033377852513893	180625
2e01b4dbefe150b7fd2814f1cbb67d442247a892	e-learning platform for educational resources repurposing in earth observation	elearning platform;electronic learning;egle application;grid computing astronomy computing computer aided instruction;egle e learning platform earth observation educational resource grid computing cloud computing;learning resource;earth;computer aided instruction;materials;grid;teaching material;educational resource;computer architecture;astronomy computing;monitoring;environmental modelling;distributed databases;satellite image;earth observation;user interaction;materials electronic learning earth distributed databases computer architecture monitoring;grid computing;egle application elearning platform grid earth observation;distributed architecture;cloud computing	Recent development of eLearning Platforms, presentation technologies and user interaction methods has enabled the development of new, more complex and more efficient teaching materials for Earth Observation domain. However, the process of creating new learning resources is not a trivial task and most of the time technical knowledge is required for such an action. Furthermore, the data that is usually involved in Earth Observation related computations is represented by satellite images, terrain measured values or complex environmental modellings, all of these representing large amounts of data or requiring complex processing procedures. As a consequence, the computational power needed for dealing with such resources requires distributed architectures (like Grid or Cloud computing) that cannot be easily used by specialists without advanced technical knowledge (like meteorologists, geography specialists, hydrologists, etc.). Addressing these problems, eGLE eLearning Platform described in this paper provides to the teachers from different Earth Observation related disciplines a number of specialized tools that allow operations like description, execution and monitoring of Grid computations to be performed through a simple, yet powerful interface. Furthermore, eGLE application facilitates the re-usability of previously developed teaching resources, shortening the time required to create new materials and lowering their price.	cloud computing;complexity;computation;grid computing;high-level programming language;interaction;usability	Teodor Stefanut;George Popescu;Dorian Gorgan	2010	2010 12th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2010.60	earth observation;simulation;cloud computing;computer science;theoretical computer science;operating system;data mining;database;earth;grid;distributed database;grid computing	HPC	-39.33479311154591	-1.3913117279427476	180668
032231b37f209c6677ca8b3d01a8ef7f5e0ec13e	evolution of the coma match system		The schema and ontology matching systems COMA and COMA++ are widely used in the community as a basis for comparison of new match approaches. We give an overview of the evolution of COMA during the last decade. In particular we discuss lessons learned on strong points and remaining weaknesses. Furthermore, we outline the design and functionality of the upcoming COMA 3.0.	evolution;ontology alignment;ontology merging;scalability	Sabine Maßmann;Salvatore Raunich;David Aumüller;Patrick Arnold;Erhard Rahm	2011			computer vision;simulation;computer science;artificial intelligence	DB	-45.216860727145004	3.427420233060601	180694
823b4761f2a74d866a7896debf65f4af41f38456	architectures for independent test data review on npoess viirs	databases;decision support;performance test;time change;government;data mining;rapid inspection;independent test data review;data distribution;data analysis;npoess viirs;servers;independent government assessment;viirs data analysis;low cost data distribution operation;decision support cluster independent test data review npoess viirs thermal vacuum testing viirs flight unit independent government assessment test data analysis low cost data distribution operation rapid inspection viirs data analysis;decision support cluster;thermal vacuum testing;book reviews;near real time;test data analysis;aerospace test facilities;databases book reviews government servers data analysis user interfaces data mining;user interfaces;viirs flight unit;data analysis aerospace test facilities	The presence of an on-site data clerk team proved to be a lowcost, effective method for VIIRS test data distribution. As opposed to a strictly electronic distribution path, the human data clerk component was always on-hand to obtain and organize relevant test procedure as-run documents, and various other shift logs, test logs, and test reports. It was the data clerk's responsibility to locate data, find documents, manage the data flow process, and serve any requests from the sensor scientist teams. The data clerks served a program librarian role, allowing government sensor scientists to concentrate on data analysis rather than how or where to find the necessary data and documentation.	dataflow;documentation;effective method;high availability;librarian;test data	Carl H. Fischer;Michael Denning;Kristin E. Clark;Bruce Guenther	2010	2010 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2010.5651547	simulation;decision support system;computer science;data mining;data analysis;government;remote sensing	SE	-41.826287190943724	-2.9126289198829274	180761
a4403970a43dd72e9883e4d4b37f9e6271bf16f3	metadata in the context of the european library project	european digital library;dublin core metadata;collection level description;sru;application profiles;interoperability;search and retrieve via urls	The European Library Project (TEL), sponsored by the European Commission, brings together 10 major European national libraries and library organisations to investigate the technical and policy issues involved in sharing digital resources. The objective of TEL is to set up a cooperative framework which will lead to a system for access to the major national and deposit collections in European national libraries. The scope of the project encompasses publisher relations and business models but this paper focuses on aspects of the more technical work in metadata development and the interoperability testbeds. The use of distributed Z39.50 searching in conjunction with HTTP/XML search func-tionality based on OAI protocol harvesting is outlined. The metadata development activity, which will result in a TEL application profile based on the Dublin Core Library Application Profile together with collection level description, is discussed. The concept of a meta-data registry to allow the controlled evolution of the application profile to be inclusive of other cultural heritage institutions is also introduced.	dublin core;hypertext transfer protocol;interoperability;library (computing);openness;testbed;the european library;xml	Theo van Veen;Robina Clayphan	2002			computer science;database;metadata;world wide web;information retrieval	HPC	-42.36149104480319	0.6873691304416414	180789
8ab2d7cb2389dafd63cecb119cf4ac25780d03eb	towards real-time optimal vp management for atm nets	management system;real time;dynamic routing;real time optimization;optimization problem;network topology;large scale;computer experiment;mathematical model;article;virtual path;heuristic algorithm	Sung-Jin Chung a, Sung-Pil Hong b and Hae-Goo Song c a Department of Industrial Engineering, Seoul National University, Seoul 151-742, Korea E-mail: chung@optima.snu.ac.kr b Division of Commerce and Business Administration, Chung-Ang University, An-Sung-Gun, Kyung-gi-Do 456-756, Korea E-mail: sphong@cau.ac.kr c i2 Technologies, 7th Floor, Hansol Building, Yeoksam-Dong, Kangham-Gu, Seoul 135-080, Korea E-mail: hae-goo-song@i2.com	atm turbo;grey goo;industrial engineering;real-time clock	Sung-Jin Chung;Sung-Pil Hong;Hae-Goo Song	2000	Telecommunication Systems	10.1023/A:1019197518573	heuristic;optimization problem;mathematical optimization;real-time computing;computer experiment;adaptive routing;telecommunications;computer science;mathematical model;management system;distributed computing;network topology;computer network	DB	-44.4072971320332	-6.844877044982449	180968
00a7290fdfeb3be65bc18ffdbe293ae08f696bb8	scanning the literature		In the next decade, millions of sensors and small-scale mobile devices will integrate processors, memory and communication capabilities. Networks of devices will be widely deployed for measurement, detection and surveillance applications. In this context, users need to query large collections of devices in an ad-hoc manner. Some existing systems rely on a database server to provide a declarative access to data extracted from devices. However, these systems lack flexibility because data is extracted in a predefined way; also, they do not scale to a large number of devices because raw data is transferred from each device to a database server. In our new concept of a device database system, query processing is tightly integrated with the network of devices. In this paper, we detail our approach and show how device database systems overcome the limitations of existing systems.	ct scan;central processing unit;database server;hoc (programming language);mobile device;sensor;server (computing)	Brian D. Noble	2000	IEEE Personal Commun.	10.1145/372346.606215		DB	-36.0171964516597	1.066930210216045	181271
004f162c12637153f75ab1a8faf09aed6f6d353f	hybrid multi-biometric template protection using watermarking		1Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Kingdom of Saudi Arabia 2Department of Computer Science, College of Computer Science and Engineering, Taibah University, Madinah, Kingdom of Saudi Arabia 3Department of Information Technology, College of Computer and Information Sciences, King Saud University, Riyadh, Kingdom of Saudi Arabia 4Department of Computer Science, College of Computer and Information Sciences, King Saud University, Riyadh, Kingdom of Saudi Arabia ∗Corresponding author: OMOHAMMADI@taibahu.edu.sa	algorithm;baseline (configuration management);biometrics;chaos theory;computer engineering;cryptography;database;digital watermarking;discrete wavelet transform;embedded system;encryption;fingerprint;hadamard code;high-level programming language;image noise;information and computer science;information science;multimodal interaction;requirement;return loss;singular value decomposition	Ohoud Nafea;Sanaa Ghouzali;Wadood Abdul;Qazi Emad-ul-Haq	2016	Comput. J.	10.1093/comjnl/bxv107	theoretical computer science;computer science;biometrics;digital watermarking	Theory	-44.38965501594204	-7.892104159355394	181468
2fcce98e8df8443276f2f399302f5527a934c92e	using gis and outranking multicriteria analysis for land-use suitability assessment	multicriteria analysis;geographic information system;hydrologie;land use;close relationships;indexation;point of view;land use planning	Land-use planners often make complex decisions within a short period of time when they must take into account sustainable development and economic competitiveness. A set of land-use suitability maps would be very useful in this respect. Ideally, these maps should incorporatecomplexcriteria integrating several stakeholders’ points of view. To illustrate the feasibility of this approach, a land suitability map for housing was realised for a small region of Switzerland. Geographical Information System technology was used to assess the criteria requested to de ne the suitability of land for housing. An example dealing with the evaluation of noise levels illustrates the initial steps of this procedure. Because the required criteria are heterogeneous and measured on various scales, an outranking multicriteria analysis method called ELECTRE-TRI was used. However, using it to assess the suitability of any point in a territory was impractical due to computational limitations. Therefore, a mathematical function to evaluate closeness relationships and classify the study area into homogeneous zones was used. This function is compatible with the outranking function of ELECTRETRI used to assess the suitability index. The resulting maps lend e cient support to negotiation and are very useful in dealing with inherent con icts in land-use planning.	automated planning and scheduling;cma-es;centrality;coherence (physics);computation;data access;decision support system;geographic information system;holism;mapinfo professional;mind map;switzerland	Florent Joerin;Marius Thériault;André Musy	2001	International Journal of Geographical Information Science	10.1080/13658810051030487	land-use planning;land use;geography;suitability analysis;geographic information system;management science	Vision	-38.36647053647349	-3.5981185762296186	181738
67672d0c18bb6f52e04ec76a1dd108b91be22ff7	lexflow: a system for cross-fertilization of computational lexicons	extensive use;work-flow management system;web-based application;lexical entry;lexicon management;computational lexicon;xml technology;workflow type;document workflows;open-source tool;management system	This demo presents LeXFlow, a workflow management system for crossfertilization of computational lexicons. Borrowing from techniques used in the domain of document workflows, we model the activity of lexicon management as a set of workflow types, where lexical entries move across agents in the process of being dynamically updated. A prototype of LeXFlow has been implemented with extensive use of XML technologies (XSLT, XPath, XForms, SVG) and open-source tools (Cocoon, Tomcat, MySQL). LeXFlow is a web-based application that enables the cooperative and distributed management of computational lexicons.	apache cocoon;apache tomcat;lexicon;mysql;open-source software;prototype;scalable vector graphics;web application;xml;xpath;xslt	Maurizio Tesconi;Andrea Marchetti;Francesca Bertagna;Monica Monachini;Claudia Soria;Nicoletta Calzolari	2006			natural language processing;computer science;management system;database;programming language;world wide web	Web+IR	-40.84949784258686	4.072774237094253	181798
9b4b9ffad1d8901be68e709b055ba202243a7024	interim report: ansi/x3/sparc study group on data base management systems 75-02-08			management system		1975	FDT - Bulletin of ACM SIGMOD			DB	-41.00886665925116	-4.82316067513924	181840
2d40813fe8f4556799f8422276d41543c6b8f4e6	advanced system for organization of work and scientific data exchange	kryptograficzne funkcje hashujące;trust management;system zarządzania treścią;uwierzytelnianie;hashing;content management system;fonetyczne porownanie slow;phonetic string matching			Adam Piotr Zochowski;Wojciech Lason	2006	Bio-Algorithms and Med-Systems		computer science;theoretical computer science;database;world wide web	DB	-40.63278576056817	-5.577274405653353	182121
0f71d4cf58946af66678c476ca316cdd568b8635	a viable system for tracing illegal users of video	distributed system;securite informatique;tracing;intelligence artificielle;copyright protection;computer security;senal video;signal video;seguridad informatica;tracage;video signal;artificial intelligence;inteligencia artificial;trazado	1 Graduate School of Information Systems, University of Electro-Communications, 1-5-1, Chofugaoka, Chofu-shi, Tokyo 182-8585, Japan kang@ice.uec.ac.jp 2 Dept. of Inf. and Communications Eng., University of Electro-Communications, 1-5-1, Chofugaoka, Chofu-shi, Tokyo 182-8585, Japan {kang, kurkoski, yama, kingo}@ice.uec.ac.jp 3 Department of Information Security, Pukyong National University, 599-1 Daeyeon-3Dong, Nam-Gu, Busan 608-737, Republic of Korea Podosongei@hanmail.net, shinsu@pknu.ac.kr	information systems;information security;nam	Hyun-Ho Kang;Brian M. Kurkoski;Young-Ran Park;Sanguk Shin;Kazuhiko Yamaguchi;Kingo Kobayashi	2006		10.1007/11734628_20	tracing;telecommunications;computer science;artificial intelligence;operating system;database;world wide web;computer security	OS	-44.27150387440732	-7.627971797966649	182245
58be972c23ee19eeb7c3ae747e09b5ec27ee43ec	spatial data infrastructures and digital libraries: paths to convergence			digital library	James S. Reid;Chris Higgins;David Medyckyj-Scott;Andrew Robson	2004	D-Lib Magazine	10.1045/may2004-reid	theoretical computer science;computer science;digital library;spatial analysis;convergence (routing);distributed computing	ML	-37.911813510685256	-4.094500276850868	182385
7146ce7d15f084c1558acbc8270d14760417436c	description logic programs: a practical choice for the modelling of ontologies	004;semantic technologies;industrial application;description logic;knowledge representation;description logic programs	Knowledge representation using ontologies constitutes the heart of semantic technologies. Despite successful standardization efforts by the W3C, however, there are still numerous different ontology representation languages being used, and interoperability between them is in general not given. The problem is aggrevated by the fact that current standards lay foundations only and are well-known to be insufficient for the modelling of finer details. Thus, a plethora of extensions of the basic languages is being proposed, rendering the picture of ontology representation languages to be chaotic, to say the least. While semantic technologies start to become applicable and are being applied in adjacent areas of research and in research projects with industrial participation, and can soon be expected to become an integral part of industrial applications, the practitioner is faced with the difficult task of choosing his basic ontology representation paradigm. We will argue that the OWL subset known as Description Logic Programs constitutes a very reasonable choice. 1 Semantic Technologies are everywhere Accelerated by the vision of the semantic web, semantic technologies have recently made significant advances towards applications. The underlying methods and paradigms are already being transferred to adjacent areas of research in artificial intelligence, knowledge management, and elsewhere. Textbooks explaining the foundations, e.g. [12], have appeared. Large national and international projects on the topic are under way, like the EU funded SEKT project, the KnowledgeWeb and REWERSE Networks of Excellence, or the SmartWeb project financed by the German Federal Ministry of Education and Research (BMBF), to mention only a few. ∗The authors acknowledge support by the European Union under the SEKT project and the KnowledgeWeb Network of Excellence, and by the German Federal Ministry for Education and Research (BMBF) under the SmartWeb project. 1http://www.sekt-project.com 2http://knowledgeweb.semanticweb.org 3http://rewerse.net 4http://www.smartweb-project.org	artificial intelligence;description logic;floor and ceiling functions;interoperability;knowledge management;knowledge representation and reasoning;ontology (information science);programming paradigm;semantic web	Pascal Hitzler;York Sure-Vetter;Rudi Studer	2005			knowledge representation and reasoning;description logic;computer science;knowledge management;theoretical computer science;data mining;ontology language;web ontology language	AI	-44.5924688322584	3.655383348842025	182502
26b7d7720d9653a379d133762439eeea2abcc109	historical archive ontologies requirements, modeling and visualization		— Most ontology development methodologies and tools for ontology management deal with ontology snapshots, i.e. they model and manage only the most recent version of ontologies, which is inadequate for contexts where the history of the ontology is of interest, such as historical archives. This work presents a set of requirements for the modeling and visualization of a temporal ontology used as a tool for the representation of historical information. In accordance to these requirements, a visualization plug-in was designed and implemented, featuring a set of tools that enable users to efficiently examine ontology temporal characteristics such as class and instance evolution along the timeline.	archive;heuristic (computer science);ontology (information science);ontology versioning;plug-in (computing);protégé;requirement;requirements analysis;timeline	Akrivi Katifori;Costas Vassilakis;Elena Torou;Giorgos Lepouras;Constantin Halatsis;Ilias Daradimos	2007			data mining;visualization;ontology (information science);idef5;computer science	Web+IR	-41.45965073619818	0.42037893999212006	182615
a6fe8c60138f4544b26407c1f60be353e47d7c59	a virtual primary key for reversible watermarking textual relational databases			relational database;unique key	Chin-Chen Chang;Thai-Son Nguyen;Chia-Chen Lin	2014		10.3233/978-1-61499-484-8-756	parallel computing;information retrieval;relational database;primary key;computer science;digital watermarking	DB	-40.272837403179366	-8.277330310119584	182681
70bfb47f9813a636f4085f757297bd7d4bc1c6ab	skeletonization with particle filters	skeleton;particle filter;skeletonization;skeletal path	YUCHUN TANG Research Center of Sectional and Imaging Anatomy Shandong University School of Medicine 44#, Wenhua Xi Road, Jinan, Shandong 250012, China yuchuntang@ucla.edu XIANG BAI* Department of Electronics and Information Engineering Huazhong University of Science and Technology NO. 1037, Luoyu Road, Wuhan, Hubei 430074, China xbai@hust.edu.cn XINGWEI YANG Department of Computer and Information Sciences Temple University 1805 North Broad Street, Philadelphia, PA 19122 xingwei.yang@temple.edu	computation;computational anatomy;contour line;grayscale;information and computer science;information engineering;particle filter;pixel;raid;self-similarity;silk road;yang	Yuchun Tang;Xiang Bai;Xingwei Yang;Liang Lin;Shuwei Liu;Longin Jan Latecki	2010	IJPRAI	10.1142/S021800141000807X	skeletonization;computer vision;particle filter;computer science;geometry;skeleton	Vision	-46.16819529077486	-9.4015533163465	182689
9bb3fa019a526a560b29c5a7fbd8cbfa6d815267	discovering information from an integrated graph database		The information explosion in science has become a different problem, not the sheer amount per se, but the multiplicity and heterogeneity of massive sets of data sources. Relations mined from these heterogeneous sources, namely texts, database records, and ontologies have been mapped to Resource Description Framework (RDF) triples in an integrated database. The subject and object resources are expressed as references to concepts in a biomedical ontology consisting of the Unified Medical Language System (UMLS), UniProt and EntrezGene and for the predicate resource to a predicate thesaurus. All RDF triples have been stored in a graph database, including provenance. For evaluation we used an actual formal PRISMA literature study identifying 61 cerebral spinal fluid biomarkers and 200 blood biomarkers for migraine. These biomarkers sets could be retrieved with weighted mean average precision values of 0.32 and 0.59, respectively, and can be used as a first reference for further refinements.	biological database;graph database;information discovery;information explosion;information retrieval;mined;ontology (information science);predicate (mathematical logic);resource description framework;thesaurus;uniprot	Erik M. van Mulligen;Wytze Vlietstra;Rein Vos;Jan A. Kors	2016			uniprot;rdf;information explosion;information retrieval;graph database;ontology (information science);ontology;predicate (grammar);unified medical language system;computer science	DB	-40.64576516334483	2.4737045003385196	183162
6b79d811621b7acd40e9db9e3e6b7f5748cc0c77	on interoperability in distributed geoinformational systems		"""Means for providing in a centralized manner a technical interoperability of distributed geographic information systems are suggested. A """"format conversion module"""" which is optimal from the point of view of security and scalability of geoinformational systems and a low cost of software upgrade is developed. The solution is based on the file-server-embedded software that performs the format conversion hidden from the user. The authors have developed algorithms and implemented computer programs for converting data formats. The format description *.ion was analyzed and restored in the course of modernization of Russian vertical ionosphere sounding stations. The problems of embedding the format conversion module into the geoinformational system structure and also technical problems of integration of Russian geoinformational systems into the structure of World Data Centers are considered. A model for building a common information space of geographic informational systems is suggested."""	algorithm;automatic sounding;centralized computing;computer program;distributed gis;distributed computing;embedded software;file server;geographic information system;information model;interoperability;point of view (computer hardware company);scalability;server (computing);visual instruction set	Elena N. Velichko;Aleksey Grishentsev;Constantine Korikov;Anatoliy Korobeynikov	2015		10.1007/978-3-319-23126-6_43	scalability;information space;interoperability;software;upgrade;geographic information system;distributed computing;computer science	OS	-40.465743457412565	-2.142650483874481	183186
34eee0368afd214113dd32864919d7175c25645c	a methodology for the evaluation of source database quality in a decision support system or data warehouse: development and testing	decision support system;data warehouse		decision support system	M. Pamela Neely	2000			decision support system;dimensional modeling;systems engineering;data warehouse;data mining;database	DB	-39.52724529082926	-4.965100554371165	183298
9d59ab29ff60625c7da4b3b9617762508ce0eab2	a framework for integrating natural language tools	information loss;client server architecture;conceptual model;input output;natural language;communication protocol;natural language processing	Natural Language processing (NLP) systems are typically characterized by a pipeline architecture in which several independently developed NLP tools, connected as a chain of filters, apply successive transformations to the data that flows through the system. Hence when integrating such tools, one may face problems that lead to information losses, such as: (i) tools discard information from their input which will be required by other tools further along the pipeline; (ii) each tool has its own input/output format. This work proposes a solution that solves these problems. We offer a framework for NLP systems. The systems built using this framework use a client server architecture, in which the server acts as a blackboard where all tools add/consult data. Data is kept in the server under a conceptual model independent of the client tools, thus allowing the representation of a broad range of linguistic information. The tools interact with the server through a generic API which allows the creation of new data and the navigation through all the existing data. Moreover, we provide libraries implemented in several programming language that abstract the connection and communication protocol details between the tools and the server, and provide several levels of functionality that simplify server use.	client–server model;communications protocol;input/output;library (computing);natural language processing;programming language;server (computing)	João Graça;Nuno J. Mamede;João D. Pereira	2006		10.1007/11751984_12	computer science;data mining;database;world wide web	HPC	-37.27582140853519	2.506881945988023	183431
e2958c238a019263838751141efcfb4255cdc1bb	the micro gallery - observations from three projects: london; san diego; washington d.c		Dr. Piotr Moncarz is a Corporate Vice President at Exponent. Dr. Moncarz’s efforts are directed particularly in the energy sector, including assistance in power plant development projects, and in the oil and gas industry in programs implementing risk management in system operations. With a background in civil engineering, Dr. Moncarz has worked in the areas of reinforced and prestressed concrete, the study of concrete distress due to material problems and adverse conditions, cracking of concrete, wood mechanics, steel structures, earthquake engineering and seismic assessments, field and analytical structural failure investigations, structural analyses of transmission towers, and investigations of offshore platform capsizing. Dr. Moncarz is a Stanford Certified Project Manager skilled at providing means and methods to project and program organization and management. For over 15 years Dr. Moncarz has worked on projects associated with energy. He leads Exponent’s Energy Initiative Program with an emphasis on Liquefied Natural Gas (LNG), oil, natural gas, and renewable resources. Dr. Moncarz has conducted energy policy studies focusing on gas for Central Asian Republics and Bangladesh.	dr. sbaitso;inventory;password cracking;risk management;structural integrity and failure	Alex Morrison	1995			cartography;history	SE	-47.59898412297689	-7.506362518653191	183508
4d106fa3fe1e0555971c68576d4add714616580e	metadata based management and sharing of distributed biomedical data	distributed data;health research;uk clinical guidelines;data sharing;biological patents;semantic tagging;metadata;europe pubmed central;citation search;scientific data management;uk phd theses thesis;computer supported collaborative work;web based collaboration;life sciences;cscw;biomedical data;uk research reports;medical journals;europe pmc;data integration;biomedical research;bioinformatics	Biomedical research data sharing is becoming increasingly important for researchers to reuse experiments, pool expertise and validate approaches. However, there are many hurdles for data sharing, including the unwillingness to share, lack of flexible data model for providing context information, difficulty to share syntactically and semantically consistent data across distributed institutions, and high cost to provide tools to share the data. SciPort is a web-based collaborative biomedical data sharing platform to support data sharing across distributed organisations. SciPort provides a generic metadata model to flexibly customise and organise the data. To enable convenient data sharing, SciPort provides a central server based data sharing architecture with a one-click data sharing from a local server. To enable consistency, SciPort provides collaborative distributed schema management across distributed sites. To enable semantic consistency, SciPort provides semantic tagging through controlled vocabularies. SciPort is lightweight and can be easily deployed for building data sharing communities.	1-click;clickstream;community;computer cluster;controlled vocabulary;data model;experiment;generic drugs;reuse (action);server (computer);server (computing);web application	Fusheng Wang;Cristobal Vergara-Niedermayr;Peiya Liu	2014	International journal of metadata, semantics and ontologies	10.1504/IJMSO.2014.059126	computer science;data integration;computer-supported cooperative work;data mining;database;metadata;world wide web	DB	-40.61205789071515	1.188381043935938	183516
524adb8b3d0815db37e49d8af117d7b3d9e413cd	purple sox extraction management system	management system	We describe the Purple SOX (PSOX) EMS, a prototype Extraction Management System currently being built at Yahoo!. The goal of the PSOX EMS is to manage a large number of sophisticated extraction pipelines across different application domains, at the web scale and with minimum human involvement. Three key value propositions are described: extensibility, the ability to swap in and out extraction operators; explainability, the ability to track the provenance of extraction results; and social feedback support, the facility for gathering and reconciling multiple, potentially conflicting sources.	asc purple;attribute–value pair;extensibility;management system;pipeline (computing);prototype;quantum gate;scalability	Philip Bohannon;Srujana Merugu;Cong Yu;Vipul Agarwal;Pedro DeRose;Arun Shankar Iyer;Ankur Jain;Vinay Kakade;Mridul Muralidharan;Raghu Ramakrishnan;Warren Shen	2008	SIGMOD Record	10.1145/1519103.1519107	computer science;data mining;management system;database;computer security	DB	-40.57935874856382	3.1098617182720267	183531
e3edfc60099cf92b32c1927c3306219b73e9d3d6	quantitative graph theory: a new branch of graph theory and network science		Quantitative Graph Theory: A new branch of graph theory and network science Matthias Dehmer1,2∗, Frank Emmert-Streib3,4∗, and Yongtang Shi 5,6 Department of Computer Science, Universität der Bundeswehr München, Germany Department of Mechatronics and Biomedical Computer Science, UMIT, Hall in Tyrol, Austria Computational Medicine and Statistical Learning Laboratory, Department of Signal Processing, Tampere University of Technology, Finland Institute of Biosciences and Medical Technology, 33520 Tampere, Finland Center for Combinatorics and LPMC-TJKLC, Nankai University, Tianjin 300071, China 6 College of Computer and Control Engineering, Nankai University, Tianjin 300071, China	computation;computer science;control engineering;graph theory;mechatronics;network science;nmap security scanner;signal processing	Matthias Dehmer;Frank Emmert-Streib;Yongtang Shi	2017	Inf. Sci.	10.1016/j.ins.2017.08.009	graph (abstract data type);mathematics;graph property;graph theory;machine learning;theoretical computer science;artificial intelligence;moral graph;extremal graph theory;null graph;topological graph theory;voltage graph	Theory	-46.60285010230102	-9.69642237151518	183817
0f8cd7d96367f77f15deefd9dec31bbe1b9bf0a3	correction: yin, j., et al. exploring multi-scale spatiotemporal twitter user mobility patterns with a visual-analytics approach. isprs international journal of geo-information 2016, 5, 187	n a	Junjun Yin 1,2, Yizhao Gao 1,2, Zhenhong Du 2,3 and Shaowen Wang 1,2,4,* 1 Department of Geography and Geographic Information Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA; jyn@illinois.edu (J.Y.); ygao29@illinois.edu (Y.G.) 2 CyberGIS Center for Advanced Digital and Spatial Studies, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 3 Institute of Geographic Information Science, School of Earth Sciences, Zhejiang University, Hangzhou 310028, China; duzhenhong@zju.edu.cn 4 National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA * Correspondence: shaowen@illinois.edu; Tel.: +1-217-333-7608; Fax: +1-217-244-1785	acm/ieee supercomputing conference;cybergis;fax;geographic information science;les trophées du libre;like button;national center for supercomputing applications;visual analytics	Junjun Yin;Yizhao Gao;Zhenhong Du;Shaowen Wang	2016	ISPRS Int. J. Geo-Information	10.3390/ijgi5120226	geography;data mining;internet privacy;world wide web	Robotics	-44.36305554294404	-9.54075434806113	184603
44a22193ac360af12e5a1facd3d94cd859cff9ac	how the minotaur turned into ariadne: ontologies in web data extraction	web data extraction;open data;data extraction;semantically annotated data;automated data extraction;data provider;diadem project;small data provider;automated extraction;structured data;open data initiative	web data extraction;open data;data extraction;semantically annotated data;automated data extraction;data provider;diadem project;small data provider;automated extraction;structured data;open data initiative	ontology (information science)	Tim Furche;Georg Gottlob;Xiaonan Guo;Christian Schallhart;Andrew Jon Sellers;Cheng Wang	2011		10.1007/978-3-642-22233-7_2	computer science;data mining;database;world wide web	NLP	-41.09786498186723	2.7329200446116726	184631
96e4b391bb47520f614bf9ea4e308da862f59577	frappe: a vocabulary to represent heterogeneous spatio-temporal data to support visual analytics		Nowadays, we are witnessing a rapid increase of spatiotemporal data that permeates different aspects of our everyday life such as mobile geolocation services and geo-located weather sensors. This big amount of data needs innovative analytics techniques to ease correlation and comparison operations. Visual Analytics is often advocated as a doable solution thanks to its ability to enable users to directly obtain insights that support the understanding of the data. However, the grand challenge is to offer to visual analytics software an integrated view on top of multi-source, geo-located, time-varying data. The abstractions described in the FraPPE ontology address this challenge by exploiting classical image processing concepts (i.e. Pixel and Frame), a consolidated geographical data model (i.e. GeoSparql) and a time/event vocabulary (i.e. Time and Event ontologies ). FraPPE was originally developed to represent telecommunication and social media data in an unified way and it is evaluated modeling the dataset made available by ACM DEBS 2015 Grand Challenge.	debs;data model;geosparql;geolocation;grand challenges;image processing;multi-source;ontology (information science);pixel;sensor;social media;visual analytics;vocabulary	Marco Balduini;Emanuele Della Valle	2015		10.1007/978-3-319-25010-6_21	natural language processing;analytics;visual analytics;computer science;data science;data mining	Visualization	-36.1487356897246	-3.589525574804981	184753
ec9a7cad03ec1785af57c3ddc8588b871d2874e7	vitruvius: vehicle sensor based model-driven engineering application generation		On July 7, 2017 at 6 pm, Guillermo Cueva Fernández defended his Ph.D. thesis entitled Vitruvius: Vehicle Sensor Based Model-Driven Engineering Application Generation at the University of Oviedo. Guillermo Cueva Fernández presented his dissertation in a publicly open event held in the School of Computer Engineering, and was able to defend all his work on every question raised by his thesis committee and the audience. The thesis was supervised by his advisors, Jordán Pascual Espada and Vicente García-Díaz, together with the thesis committee, Juan Luis Pavón Mestras, Yago Sáez Achaerandio and José Emilio Labra Gayo. It has been approved, receiving the highest rating. All the cited people were present at the event.	computer engineering;model-driven engineering;yago	Vicente García-Díaz;Jordán Pascual Espada;Guillermo Cueva-Fernandez	2018	JAISE	10.3233/AIS-180474	human–computer interaction;computer science;model-driven architecture	HCI	-46.657762958994425	-8.474152781990078	184944
06bd60bc18415be95ecc62ee5bbe72a264ba140b	pubchemrdf: towards the semantic annotation of pubchem compound and substance databases	biological patents;biomedical journals;text mining;europe pubmed central;citation search;computer applications in chemistry;citation networks;theoretical and computational chemistry;computational biology bioinformatics;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;documentation and information in chemistry;biomedical research;bioinformatics;literature search	BACKGROUND PubChem is an open repository for chemical structures, biological activities and biomedical annotations. Semantic Web technologies are emerging as an increasingly important approach to distribute and integrate scientific data. Exposing PubChem data to Semantic Web services may help enable automated data integration and management, as well as facilitate interoperable web applications.   DESCRIPTION This work, one of a series covering the PubChemRDF project, describes an approach to translate PubChem Substance and Compound information into Resource Description Framework (RDF) format. Basic examples are provided to demonstrate its use. The aim of this effort is to provide two new primary benefits to researchers in a cost-effective manner. Firstly, we aim to remove the inherent limitations of using the web-based resource PubChem by allowing a researcher to use readily available semantic technologies (namely, RDF triple stores and their corresponding SPARQL query engines) to query and analyze PubChem data on local computing resources. Secondly, this work intends to help improve data sharing, analysis, and integration of PubChem data to resources external to NCBI and across scientific domains, by means of the association of PubChem data to existing ontological frameworks, including CHEMical INFormation ontology, Semanticscience Integrated Ontology, and others.   CONCLUSIONS With the goal of semantically describing information available in the PubChem archive, pre-existing ontological frameworks were used, rather than creating new ones. Semantic relationships between compounds and substances, chemical descriptors associated with compounds and substances, interrelationships between chemicals, as well as provenance and attribute metadata of substances are described. Graphical abstract:Schematic representation of the semantic links for PubChem compounds and substances.	annotation;archive;database;databases;interoperability;national center for biotechnology information;ontology (information science);pubchem;question (inquiry);resource description framework;sparql;schematic;semantic web service;triplestore;web application;benefit	Gang Fu;Colin R. Batchelor;Michel Dumontier;Janna Hastings;Egon L. Willighagen;Evan Bolton	2015		10.1186/s13321-015-0084-4	text mining;medical research;data web;computer science;bioinformatics;data science;social semantic web;data mining;information retrieval	Web+IR	-41.21423535511608	2.14100265087608	185166
c5c1b6dbf28c18f7feb85783c1d28ea5f261416e	expert finding by the dempster-shafer theory for evidence combination				Nafiseh Torkzadeh Mahani;Mostafa Dehghani;Maryam S. Mirian;Azadeh Shakery;Khalil Taheri	2018	Expert Systems	10.1111/exsy.12231	data science;dempster–shafer theory;data mining;body of knowledge;computer science;artificial intelligence;machine learning;bibliography;ranking	NLP	-34.301017358846444	-9.644591786673955	185205
9529aed2ef34fc930817151e87fbe00f366fdfb3	is inductive machine learning just another wild goose (or might it lay the golden egg)?	induction machine;geography physical;backpropagation neural network;technology;earth;social sciences;teaching and learning;statistical method;physical sciences;journal article;regression;learning methods;science technology;remote sensing data;environment;information science library science;statistical inference;spatial nonstationarity;optimization;computer science;computer science information systems;classification accuracy;models;multisource;physical geography;geography	The research reported here contrasts the roles, methodologies and capabilities of statistical methods with those of inductive machine learning methods, as they are used inferentially in geographical analysis. To this end, various established problems with statistical inference applied in geographical settings are reviewed, based on Gould's (1970) critique. Possible solutions to the problems outlined by Gould are suggested via reviews of: (i) improved statistical methods, and (ii) recent inductive machine learning techniques. Following this, some newer problems with inference are described, emerging from the increased complexity of geographical datasets and from the analysis tasks to which we put them. Again, some solutions are suggested by pointing to newer methods. By way of results, questions are posed, and answered, relating to the changes brought about by adopting inductive machine learning methods for geographical analysis. Specifically, these questions relate to analysis capabilities, methodologies,...	machine learning	Mark Gahegan	2003	International Journal of Geographical Information Science	10.1080/713811742	statistical inference;regression;geography;computer science;artificial intelligence;data science;machine learning;mathematics;earth;operations research;cartography;statistics;technology	Theory	-36.63420476198738	-9.240426827011202	185212
2ab0d1768904d87dd6a0991470b3af8d2e72e0b7	carpé data: supporting serendipitous data integration in personal information management	personal information management;end user data integration;mash ups;sensemaking with data	The information processing capabilities of humans enable them to opportunistically draw and integrate knowledge from nearly any information source. However, the integration of digital, structured data from diverse sources remains difficult, due to problems of heterogeneity that arise when data modelled separately are brought together. In this paper, we present an investigation of the feasibility of extending Personal Information Management (PIM) tools to support lightweight, user-driven mixing of previously un-integrated data, with the objective of allowing users to take advantage of the emerging ecosystems of structured data currently becoming available. In this study, we conducted an exploratory, sequential, mixed-method investigation, starting with two pre-studies of the data integration needs and challenges, respectively, of Web-based data sources. Observations from these pre-studies led to DataPalette, an interface that introduced simple co-reference and group multi-path-selection mechanisms for working with terminologically and structurally heterogeneous data. Our lab study showed that participants readily understood the new interaction mechanisms which were introduced. Participants made more carefully justified decisions, even while weighing a greater number of factors, moreover expending less effort, during subjective-choice tasks when using DataPalette, than with a control set-up.	bespoke;digital data;ecosystem;executable compression;expectation propagation;hoc (programming language);hyperbolic absolute risk aversion;information processing;information source;inner loop;input method;linear algebra;mixing (mathematics);personal information management;recommender system;sensemaking;tom;undo;user-subjective approach;yang	Max Van Kleek;Daniel A. Smith;Heather S. Packer;Jim Skinner;Nigel Shadbolt	2013		10.1145/2470654.2481324	human–computer interaction;computer science;information integration;personal information management;data mining;group information management;database;world wide web;enterprise information integration	HCI	-39.5646998142556	1.1231568026440397	185251
7db04e0dafed5ad67056459d220e270485128b03	technology corner: a regular expression training app		Regular expressions enable digital forensic analysts to find information in files. The best way for an analyst to become proficient in writing regular expressions is to practice. This paper presents the code for an app that allows an analyst to practice writing regular expressions.	regular expression	Nick V. Flor	2012	JDFSL		computer science;theoretical computer science;multimedia	HCI	-46.64271653329106	-2.8883255140409125	185468
f99f0408262b493c50e16080ccef34940b171946	low level big data processing			big data;high- and low-level	Jaime Salvador-Meneses;Zoila Ruiz;José García Rodríguez	2018		10.5220/0007227103450350	data mining;big data;computer science	DB	-38.900751619451896	-6.395904934272846	185540
010e282e9a979f5c56428e7e9868823e9dabff53	graph constraints in urban computing: dealing with conditions in processing urban data		"""Smart Cities is a world-wide initiative leading to better exploit the resources in a city in order to offer higher level services to people. In this context, urban computing is a process of acquisition, integration, and analysis of big and heterogeneous data generated by a diversity of sources in urban spaces, such as sensors, traffic devices, vehicles, buildings, and humans, to tackle the major issues that cities face, e.g. air pollution, increased energy consumption and traffic congestion. The majority of these information can be represented as graphs, such as the transportation network, in which places (nodes) are connected by some form of public transportation (edges). A vision of the """"city of the future"""", or even the city of the present, rests on the integration of science and technology through information systems. This vision requires a re-thinking of the relationships between technology, government, city managers, business, academia and the research community. This position paper presents our views towards developing techniques for querying and evolving graph-modeled datasets based on user-defined constraints. Our focus is to show how these techniques can be applied to effectively retrieve urban data and have automated mechanisms that guarantee data consistency."""	graph (discrete mathematics);information system;network congestion;sensor;smart city;urban computing	Laurent DOrazio;Mirian Halfeld Ferrari Alves;Carmem S. Hara;Nádia P. Kozievitch;Martin A. Musicante	2017	2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)	10.1109/iThings-GreenCom-CPSCom-SmartData.2017.171	business;management science;position paper;urban computing;information system;data consistency;graph database;exploit;traffic congestion;government	Visualization	-35.83426308726843	-2.7398228119640557	185646
454133b1577b799c04ab1dbb95cd660b2f574873	applications of linked data in the rail domain	railway data;linked data;semantic data;transport domain linked data data integration british rail domain ontology;rails ontologies maintenance engineering vehicles rail transportation industries vocabulary;ontology;traffic engineering computing data integration ontologies artificial intelligence rail traffic;railway data linked data semantic data ontology	This paper presents early findings from a larger study, into the use of linked data in the rail domain. The study and other literature has shown there to be benefits from improved integration of data in this domain and proposes that linked data in general and ontology in particular will address this. The paper will set out the current state of data integration in the British rail domain, highlighting issues found there. The manner in which linked data is employed in the broader transport domain will then be examined along with previous work pertaining to the rail domain.	bespoke;linked data;location (geography);multimodal interaction	Christopher Morris;John M. Easton;Clive J Roberts	2014	2014 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2014.7004429	engineering;data mining;database;ontology-based data integration;world wide web	Robotics	-40.43061743824213	1.5021268736772395	185806
1718d2f6f20c449531e9e936dadf11932ab29d4f	efficient top-k query processing algorithms in highly distributed environments	highly distributed;4rut;bulkdbpa;top k query;communication cost	SPECIAL ISSUE PAPERS Efficient Top-k Query Processing Algorithms in Highly Distributed Environments Qiming Fang and Guangwen Yang MCBS: Matrix Computation Based Simulator of NDN Xiaoke Jiang, Jun Bi, and You Wang A Fast Searching Approach for Top-k Partner Selection in Dynamic Alliances of Virtual Enterprises Bishan Ying, Pingping Zhu, and Ye Gu A Public Opinion Analysis System for Urban Management Information Guanlin Chen, Shengquan Li, Xiaoyang Shen, Yujia Zhang, and Gang Chen A Scale Adaptive Method Based On Quaternion Correlation in Object Tracking Jie Hu, Huaxiong Zhang, Yu Zhang, Jie Feng, and Hanjie Ma 2000	algorithm;computation;entity–relationship model;nonogram;numerical linear algebra;pingping (payment);yang	Qiming Fang;Guangwen Yang	2014	JCP	10.4304/jcp.9.9.2000-2006	computer science;theoretical computer science;distributed computing;world wide web	DB	-43.299808787459604	-8.025559065884554	186169
59ab2adbea1fd9f7972e3cd146acec383eb68e75	environmental service discovery based on semantically annotated ogc service descriptions	environmental service discovery;ogc;semantic annotations	Environmental information is a valuable resource for a wide range of applications and decision support systems in different domains. This is reflected in different initiatives around the world, such as the Open Geospatial Consortium (OGC), which focuses on defining strategies and standards for collecting, defining and sharing environmental data. One of the key challenges in order to boost the collaboration in the community is to find interoperable ways to share this information and powerful mechanisms to discover it. Semantic technologies can provide the necessary capabilities to achieve this. In this paper, we present a solution for environmental service discovery based on semantic annotations. As we target the environmental community, we build our solution on existing OGC standards for service discovery. Our approach is integrated inside the ENVISION platform, the aim of which is to provide non-ICT users with the means to create decision support Web portals based on environmental services.	consortium;decision support system;ecosystem services;interoperability;portals;service discovery	Iker Larizgoitia;Ioan Toma;Arturo Beltrán;Alejandro Llaves;Patrick Maué	2013		10.1145/2480362.2480427	computer science;data mining;world wide web;information retrieval;web coverage service	Web+IR	-40.97902724147054	1.4690579337779717	186256
7d77d2885fc8fd1aa25ccea373c393aa314ec8d1	towards exploratory olap over linked open data - a case study		Business Intelligence (BI) tools provide fundamental support for analyzing large volumes of information. Data Warehouses (DW) and Online Analytical Processing (OLAP) tools are used to store and analyze data. Nowadays more and more information is available on the Web in the form of Resource Description Framework (RDF), and BI tools have a huge potential of achieving better results by integrating real-time data from web sources into the analysis process. In this paper, we describe a framework for so-called exploratory OLAP over RDF sources. We propose a system that uses a multidimensional schema of the OLAP cube expressed in RDF vocabularies. Based on this information the system is able to query data sources, extract and aggregate data, and build a cube. We also propose a computer-aided process for discovering previously unknown data sources and building a multidimensional schema of the cube. We present a use case to demonstrate the applicability of the approach.	aggregate data;data cube;dreamwidth;exploratory testing;linked data;olap cube;online analytical processing;prototype;real-time clock;real-time data;resource description framework;void;vocabulary;world wide web	Dilshod Ibragimov;Katja Hose;Torben Bach Pedersen;Esteban Zimányi	2014		10.1007/978-3-662-46839-5_8	online analytical processing;computer science;data science;data mining;database	DB	-36.32471377231997	1.5933385060990528	186292
ed93393d50c59e4287761733d5e6da9f91fffa56	using open information extraction and linked open data towards ontology enrichment and alignment		The interlinking, maintenance and updating of different Linked Data repositories is steadily becoming a critical issue as the amount of published data increases. The wealth of information across the World Wide Web can be exploited in order to provide additional information about the way that an object is described in the real world. This paper proposes a method for discovering new concepts and examining the equivalence of properties in different LOD description schemas by using Open Information Extraction techniques on web resources. The method relies on constructing association graphs from the extracted information, proceeding to a transfer on the conceptual level using information previously known from the LOD repositories and examining the similarities and discrepancies between the produced graphs and the LOD descriptions, as well as between the graphs derived from different repositories.	gene ontology term enrichment;information extraction;linked data;turing completeness;web resource;world wide web	Antonis Koukourikos;Pythagoras Karampiperis;George A. Vouros;Vangelis Karkaletsis	2012		10.1007/978-3-642-31069-0_10	database;world wide web;information retrieval	AI	-39.22394977599796	3.928281463108107	186372
17603e91544f01a89ca05675e43f606670834bd8	validating observation data in waterml 2.0	xml;hydrology;validation;distributed systems;water;controlled vocabularies	Water observation data is a key element of a water resources information system as it is commonly used in national reports, environmental impact assessments and other analysis or modeling applications. A data standard is vital to support replication, synchronization and delivery for these applications. An international standard encoding for transfer of water information (WaterML 2.0) was previously developed to support data transfer requirements at all phases of the ingestion, data fusion, and delivery process, and suitable for adoption internationally by the water data community. Here we report on the design and implementation of a WaterML 2.0 validation service for executing specific schematic and semantic rules of the WaterML 2.0 standard. A WaterML 2.0 validation service allows for WaterML 2.0 encoded data to be tested in a context-sensitive manner. The OGC standard for Modular Specifications has been used for the WaterML 2.0 specification and includes a set of requirements and conformance classes. We report on how the set of requirements and conformance classes relate directly to the validation of WaterML 2.0 XML encodings. The result of a validation run allows users to determine the level of conformance of the respective WaterML 2.0 encoded data against the WaterML 2.0 specification. We also highlight the use of vocabulary services with the WaterML 2.0 validation service to perform code-list content and vocabulary checking. Design and implementation of a WaterML 2.0 validation service is presented.WaterML 2.0 validation service provides online testing of WaterML 2.0 data.WaterML 2.0 validation service features OGC vocabulary and code-list checking.Conformance levels can be tested against the WaterML 2.0 specification.		Jonathan Yu;Peter Taylor;Simon J. D. Cox;Gavin Walker	2015	Computers & Geosciences	10.1016/j.cageo.2015.06.001	water;controlled vocabulary;xml;computer science;data mining;database;world wide web	HCI	-40.46643227231958	0.37339325316362043	186554
f324f5709135cb272b97307f7ca07c36b76c29fa	evolution of teams for the asynchronous pursuit domain				Stefan Mandl;Herbert Stoyan	2005	Comput. Syst. Sci. Eng.		asynchronous communication;distributed computing;computer science	DB	-45.787285935555445	-4.321525394126521	186659
410d41d2f30319f25962f1cd1bc16b7801abd69f	an open source, integrated data management system for medical registries: a case study using rexdb®			management system	Charles Tirrell;Frank J. Farach;Meredith Yourd;Owen McGettrick;Oleksiy Golovko;Leon Rozenblit	2014			database;data mining;data management;computer science	SE	-40.49832593817039	-3.8454018317828527	186666
8547921ee6ae1ac51a34145c634fd82f321f451b	energy-efficient data fusion technique and applications in wireless sensor networks		1Department of Electronic and Information Engineering, Key Laboratory of Communication and Information Systems, Beijing Municipal Commission of Education, Beijing Jiaotong University, Beijing 100044, China 2Department of Computer Systems Technology, North Carolina Au0026T State University, Greensboro, NC 27411, USA 3Department of Computer Science u0026 Information Engineering, Tamkang University, New Taipei City 25137, Taiwan		Yun Liu;Qing-An Zeng;Ying-Hong Wang	2015	J. Sensors	10.1155/2015/903981	wireless sensor network;key distribution in wireless sensor networks;wi-fi array;mobile wireless sensor network	Embedded	-45.13302043839224	-7.988514422569947	187070
2ad307f99d9a567b9cb3df6769663f001a85fbed	automationml, isa-95 and others: rendezvous in the opc ua universe		OPC Unified Architecture (UA) is a powerful and versatile platform for hosting information from a large variety of domains. In some cases, the domain-specific information models provide overlapping information, such as (i) different views on a specific entity or (ii) different levels of detail of a single entity. Emerging from a multi-disciplinary engineering process, these different views can stem from various tools that have been used to deal with that entity, or from different stages in an engineering process, e.g., from requirements engineering over system design and implementation to operations. In this work, we provide a concise but expressive set of OPC UA reference types that unobtrusively allow the persistent instantiation of additional knowledge with respect to relations between OPC UA nodes. We will show the application of these reference types on the basis of a rendezvous of Automation ML and ISA-95 in an OPC UA server.		Bernhard Wally;Christian Huerner;Alexandra Mazak;Manuel Wimmer	2018	2018 IEEE 14th International Conference on Automation Science and Engineering (CASE)	10.1109/COASE.2018.8560600	automation;information model;opc unified architecture;engineering design process;rendezvous;systems design;requirements engineering;distributed computing;reference type;computer science	DB	-46.81712352576735	3.452311301084417	187324
487649e7f9e664342693a0e5386b0a8f01676f7a	optique 1.0: semantic access to big data: the case of norwegian petroleum directorate's factpages		The Optique project aims at developing an end-to-end system for semantic data access to Big Data in industries such as Statoil ASA and Siemens AG. In our demonstration we present the first version of the Optique system customised for the Norwegian Petroleum Directorate’s FactPages, a publicly available dataset relevant for engineers at Statoil ASA. The system provides different options, including visual, to formulate queries over ontologies and to display query answers. Optique 1.0 offers installation wizards that allow to extract ontologies from relational schemata, extract and define mappings connecting ontologies and schemata, and align and approximate ontologies. Moreover, the system offers highly optimised techniques for query answering.	align (company);approximation algorithm;big data;data access;end-to-end encryption;ontology (information science)	Evgeny Kharlamov;Martin Giese;Ernesto Jiménez-Ruiz;Martin G. Skjæveland;Ahmet Soylu;Dmitriy Zheleznyakov;Timea Bagosi;Marco Console;Peter Haase;Ian Horrocks;Sarunas Marciuska;Christoph Pinkel;Mariano Rodriguez-Muro;Marco Ruzzi;Valerio Santarelli;Domenico Fabio Savo;Kunal Sengupta;Michael Schmidt	2013			artificial intelligence;data mining;world wide web	DB	-38.2559436106622	2.065713911589491	187363
208c1cabfdb9b21ecbb66786201ec88c6ac973b3	closing the data loop: an integrated open access analysis platform for the mimic database		We describe a new model for collaborative access, exploration, and analyses of the Medical Information Mart for Intensive Care — III (MIMIC III) database for translational clinical research. The proposed model addresses the significant disconnect between data collection at the point of care and translational clinical research. It addresses problems of data integration, preprocessing, normalization, analyses (along with associated compute back-end), and visualization. The proposed platform is general, and can be easily adapted to other databases. The pre-packaged analyses toolkit is easily extensible, and allows for multi-language support. The platform can be easily federated, mirrored at other locations, and supports a RESTful API for service composition and scaling.	addresses (publication format);closing (morphology);data collection;data mart;database normalization;dual loop;genetic translation process;image scaling;imagery;mimic;numerous;preprocessor;published database;representational state transfer;service composability principle;test scaling	Mohammad Adibuzzaman;Ken Musselman;Alistair Edward William Johnson;Paul Brown;Zachary Pitluk Pitluk;Ananth Grama	2016	2016 Computing in Cardiology Conference (CinC)	10.23919/CIC.2016.7868698	computer science;data mining;database;world wide web	DB	-41.291422904565984	1.205667403745584	187434
b944a34776320309887b64921d2eb997f99c83f4	computing techniques for spatio-temporal data in archaeology and cultural heritage - introduction				Alberto Belussi;Roland Billen;Pierre Hallot;Sara Migliorini	2017		10.1007/978-3-319-63946-8_47	data mining;data science;temporal database;computer science;cultural heritage	AI	-40.76913348222731	-7.001726387735771	187530
ed9b7e32f59b4bba3b934b469728a8b449a0d67a	real-time environmental sensor data: an application to water quality using web services	real time data;internet of things iot;sensor networks;web services;adaptive sampling	While real-time sensor feeds have the potential to transform both environmental science and decisionmaking, such data are rarely part of real-time workflows, analyses and modeling tool chains. Despite benefits ranging from detecting malfunctioning sensors to adaptive sampling, the limited number and complexity of existing real-time platforms across environmental domains pose a barrier to the adoption of real-time data. We present an architecture built upon 1) the increasing availability of new technologies to expose environmental sensors as web services, and 2) the merging of these services under recent innovations on the Internet of Things (IoT). By leveraging recent developments in the IoT arena, the environmental sciences stand to make significant gains in the use of real-time data. We describe a use case in the hydrologic sciences, where an adaptive sampling algorithm is successfully deployed to optimize the use of a constrained sensor network resource. © 2016 Elsevier Ltd. All rights reserved. Software and data availability The use case was implemented using the Xively Internet of Things platform and a Flask web-server (written in Python 2.7) running on an Elastic Beanstalk t2.micro instance provided by Amazon Web Services. All experimental data from this study are hosted on a secure Xively feed and available upon request. The source code and implementation parameters are available on a public repository: https://github.com/kLabUM/IoT. As of 2015, all of these tools are available at no cost for a project of the scale discussed in this paper. Web connectivity is required of all hardware and software. For additional information, contact bpwong@umich. edu.	aws elastic beanstalk;adaptive sampling;alan burns (professor);algorithm;amazon web services;chen–ho encoding;computation;data system;david w. bradley;decision support system;ecosystem;entity–relationship model;experiment;flask;internet of things;interoperability;open-source software;pollack's rule;programming language;python;real-time clock;real-time data;real-time locating system;real-time transcription;real-time web;requirement;sampling (signal processing);scalability;sensor;sensor node;server (computing);software deployment;tempest (codename);usability;watershed (image processing);web server;web service	Brandon P. Wong;Branko Kerkez	2016	Environmental Modelling and Software	10.1016/j.envsoft.2016.07.020	web service;real-time data;wireless sensor network;computer science;engineering;data mining;world wide web;computer security;internet of things	Embedded	-35.19747171616762	-5.669341428727761	187577
5226683cc3972261fac328a6180e084165b97e54	"""erratum to """"a new solution for camera calibration and real-time image distortion correction in medical endoscopy-initial technical evaluation"""""""	lenses;calibration;biomedical imaging	Asterisk indicates corresponding author. ∗R. Melo is with the Institute for Systems and Robotics and Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Coimbra, 3030-290 Coimbra, Portugal (e-mail: rmelo@isr.uc.pt). J. P. Barreto is with the Institute for Systems and Robotics and Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Coimbra, 3030-290 Coimbra, Portugal (e-mail: jpbar@isr.uc.pt). G. Falcão is with the Instituto de Telecomunicações and Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Coimbra, 3030-290 Coimbra, Portugal (e-mail: gff@co.it.pt). Digital Object Identifier 10.1109/TBME.2012.2203892	camera resectioning;computer engineering;distortion;email;identifier;real-time transcription;robotics	Rui Melo;João Pedro Barreto;Gabriel Falcão Paiva Fernandes	2012	IEEE Trans. Biomed. Engineering	10.1109/TBME.2012.2203892		Robotics	-46.741943027909414	-9.20288439785264	187790
7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2	storing, tracking, and querying provenance in linked data		The proliferation of heterogeneous Linked Data on the Web poses new challenges to database systems. In particular, the capacity to store, track, and query provenance data is becoming a pivotal feature of modern triplestores. We present methods extending a native RDF store to efficiently handle the storage, tracking, and querying of provenance in RDF data. We describe a reliable and understandable specification of the way results were derived from the data and how particular pieces of data were combined to answer a query. Subsequently, we present techniques to tailor queries with provenance data. We empirically evaluate the presented methods and show that the overhead of storing and tracking provenance is acceptable. Finally, we show that tailoring a query with provenance information can also significantly improve the performance of query execution.	database;information retrieval;linked data;overhead (computing);triplestore;world wide web	Marcin Wylot;Philippe Cudré-Mauroux;Manfred Hauswirth;Paul T. Groth	2017	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2017.2690299	data mining;web search query;rdf;computer science;linked data;database;rdf query language;provenance;query optimization	DB	-34.613293230367645	3.8006228823903254	187857
d259e252d9042af5f39f23c08814863e6d884b01	the netlib mathematical software repository	netlib mathematical software repository	The Netlib repository contains freely available software, documents, and databases of interest to the numerical, scientific computing, and other research communities. The repository is maintained by AT&T Bell Laboratories, by the University of Tennessee and Oak Ridge National Laboratory, and by colleagues world-wide. Many sites around the world mirror the collection and are automatically synchronized to provide reliable and efficient service to the global community through a variety of access mechanisms.	computational science;database;mathematical software;netlib;numerical analysis;software repository	Shirley Browne;Jack J. Dongarra;Eric Grosse;Tom Rowan	1995	D-Lib Magazine		computational science;computer science	HPC	-44.47836948067394	-2.674145796775487	187956
9c622be24e03ed305309e05f7b3dccfbdaa71919	an online platform for focal structures analysis-analyzing smaller and more pertinent groups using a web tool	fsa;data mining;focal structures analysis;smaller and pertinent groups	Biological networks are increasingly becoming available for the researchers and practitioners to mineand analyze meaningful structures. The traditional approaches, such as detecting communities/clusters, do not focus on smaller and more relevant groups of individuals. We design and develop a powerful web tool (focalstructures.net) to identify focal structures, influential sets of individuals, in a given network. This paper demonstrates the online usage of the Focal Structures Analysis (FSA) approach and aims to give a brief understanding what a focal structure is about. Besides biological networks, the FSA approach can be experimented on different application domains (such as social networks) as well. The web tool helps researchers and practitioners to mine smaller and meaningful structures in an easier and more accessible way.	focal (programming language);relevance	Fatih Sen;Naga Satya V Rao Nagisetty;Teeradache Viangteeravat;Nitin Agarwal	2015			computer science;data science;data mining;world wide web	Logic	-43.40357854083084	-1.3290092311637212	187981
bb77b112992786969c97358b37cd0ae3936a445b	the italian cadastral information system: a real-life spatio-temporal dbms	intercambio informacion;base donnee repartie;settore inf 01 informatica;distributed database;systeme information geographique;geographic information system;interoperabilite;integration information;base repartida dato;information organization;organizacion informacion;information integration;echange information;information exchange;integracion informacion;organisation information;systeme gestion base donnee;sistema gestion base datos;database management system;sistema informacion geografica	In this paper we describe the technical and organizational solution that has been designed and implemented in Italy to deal with cadastral data management. The system, named “Sistema di Interscambio Catasto-Comuni” (SICC), allows to exchange cadastral data among the principal entities interested in Italy to the treatment of cadastral information, that are Ministry of Finance, Municipalities, Notaries, and Certified Land Surveyors. The system is accessible nation-wide through a WEB-based interface since September 1998 and the effectiveness of its use is demonstrated by the sharp increase in the number of requests managed during the first months: in January 1999 it has been used by more than 15.000 end-users.	entity;information system;world wide web	Franco Arcieri;Carmine Cammino;Enrico Nardelli;Antonino Venza	1999		10.1007/3-540-48344-6_5	information exchange;computer science;information integration;data mining;database;geographic information system;distributed database	DB	-39.94836645895405	-2.680075561391137	188034
1843440c845ba601a51e56158ae1f650c3dbe449	business intelligence for small and middle-sized entreprises	real time;web interface;data analysis;large scale;decision support system;business intelligence;data warehouse;small business;small enterprise	Data warehouses are the core of decision support systems, which nowadays are used by all kind of enterprises in the entire world. Although many studies have been conducted on the need of decision support systems (DSSs) for small businesses, most of them adopt existing solutions and approaches, which are appropriate for large-scaled enterprises, but are inadequate for small and middle-sized enterprises.  Small enterprises require cheap, lightweight architectures and tools (hardware and software) providing online data analysis. In order to ensure these features, we review web-based business intelligence approaches. For real-time analysis, the traditional OLAP architecture is cumbersome and storage-costly; therefore, we also review in-memory processing.  Consequently, this paper discusses the existing approaches and tools working in main memory and/or with web interfaces (including freeware tools), relevant for small and middle-sized enterprises in decision making.	computer data storage;decision support system;in-memory database;in-memory processing;online analytical processing;real-time web;user interface;web application	Oksana Grabova;Jérôme Darmont;Jean-Hugues Chauchat;Iryna Zolotaryova	2010	SIGMOD Record	10.1145/1893173.1893180	decision support system;computer science;knowledge management;data science;data warehouse;data mining;database;business intelligence;data analysis;user interface	DB	-35.67930332062436	-0.9173062251442886	188200
74f4a4834f157e22621199a464389e95098663b0	a remote meteorological data service supported on corba	distributed systems;corba	The Fisheries-Oceanography Coordinated Investigations (FOCI) is an interdisciplinary research program seeking to understand the influence of the environment on the abundance of various commercially valuable fish and shellfish stocks in Alaskan waters and their role in the ecosystem. To meet the need of the FOCI collaborators, we have built upon two previous projects: OceanShare (Denbo and Windsor, 2000) (http:// www.epic.noaa.gov/collab), a robust prototype collaborative tool based on the CORBA, Java and Habanero technologies, and the Climate Data Portal (Soreide, et al., 2002; Soreide, et al., 2001) (http://www.epic.noaa.gov/ cdp), a powerful, flexible and extensible Data Server. The new collaborative tool expands OceanShare data types from profile-type data to accommodate a wide range of oceanographic and atmospheric in-situ and gridded data types. The FOCI scientists also need a system that will enable text, graphics, data, and OceanShare sessions to be easily shared. This requirement is satisfied by creating the Secure Document Repository (SDR). The SDR is developed using the Web-based Distributed Authoring and Versioning (WebDAV) protocol and software. WebDAV enabled clients can access distributed file systems via the http protocol. The SDR will provide distributed file access from custom Java applications and WebDAV enabled commercial applications, e.g. Internet Explorer. 2. DESIGN GOALS The FOCI collaboration software needs to have the following features and characteristics to meet the needs of the FOCI program. • Secure location to share documents, data, images, and collaborative sessions. • Access control by individual document and/or folder • Document version control. • Extend OceanShare. • Integrate ndEdit (Osborne and Denbo, 2002) data selection tool. • Implement both textual and graphical annotation tools. • Extend data handling to include time series and gridded data. • Implement session save and restore.	access control;collaborative software;common object request broker architecture;etsi satellite digital radio;ecosystem;graphical user interface;graphics;hypertext transfer protocol;internet explorer;java;prototype;time series;version control;webdav;world wide web	Jorge S. C. Mota;José Luís Oliveira;Fernando Ramos	1999			embedded system;computer science;operating system;common object request broker architecture;database	Security	-39.18355454326695	-1.5465740454345527	188207
9f1e33ed272973274797317e0f55e3900482f04c	systematic development of data mining-based data quality tools	actual data;partial quality audit;pollutes artificial benchmark databases;large service-related database;induced schema;standard data;data audit test generator;data quality problem;data mining-based data quality;traditional data;so-called data;systematic development;relational database;machine learning;integrity constraints;data mining;data quality	Data quality problems have been a persistent concern especially for large historically grown databases. If maintained over long periods, interpretation and usage of their schemas often shifts. Therefore, traditional data scrubbing techniques based on existing schema and integrity constraint documentation are hardly applicable. So-called data auditing environments circumvent this problem by using machine learning techniques in order to induce semantically meaningful structures from the actual data, and then classifying outliers that do not fit the induced schema as potential errors. However, as the quality of the analyzed database is a-priori unknown, the design of data auditing environments requires special methods for the calibration of error measurements based on the induced schema. In this paper, we present a data audit test generator that systematically generates and pollutes artificial benchmark databases for this purpose. The test generator has been implemented as part of a data auditing environment based on the well-known machine learning algorithm C4.5. Validation in the partial quality audit of a large service-related database at DaimlerChrysler shows the usefulness of the approach as a complement to standard data scrubbing. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 29th VLDB Conference, Berlin, Germany, 2003	benchmark (computing);c4.5 algorithm;data integrity;data mining;data quality;data scrubbing;database;documentation;interpretation (logic);machine learning;vldb;whole earth 'lectronic link	Dominik Lübbers;Udo Grimmer;Matthias Jarke	2003			data quality;relational database;computer science;data science;data warehouse;data integrity;data mining;database;database schema;database design	DB	-37.88254093598599	-0.31310276251140745	188254
70a8b3e16e672b7be87aeaf61666d5eb4298785f	using native data and automation to perform rapid triage and reporting.				Jeff Doubleday	2006			data mining	Robotics	-40.953681326632505	-8.616911584371621	188465
f3f44d3c7f04f4c2961f995ee54507461fc902a7	the semantic web as a platform against risk and uncertainty in agriculture		In this article, we discuss existing literature on DSS in agriculture, on DSS that use data available in the Semantic Web, and on Semantic Web initiatives focusing on agriculture information. Our goal is to assess the readiness of the Semantic Web as a platform to empower DSS that can keep risk and uncertainty in agriculture under control. Key agricultural activities targeted by DSS reported in literature are nutrient management, insect and pest management, land use and planning, environmental change and forecasting, and water and drought management. The most relevant use of Semantic Web in DSS, is in data analysis, as a means of making DSS more intelligent. There are initiatives to produce vocabularies and semantic repositories in the domain of agriculture. However, data and models are still isolated in specific domain repositories, and interoperability is still weak.	semantic web	Wilmer Henry Illescas Espinoza;Alejandro Fernández;Diego Torres	2017		10.1007/978-3-319-65151-4_67	knowledge management;social semantic web;web standards;integrated pest management;interoperability;semantic analytics;decision support system;semantic web;nutrient management;environmental science	ML	-40.45351028609456	-2.3332202501408617	188608
75f1707d26760fc54491ab85e6dc3a8da610c494	scalable algorithms for mining large databases	large databases;scalable algorithm	A large number of corporations have invested heavily in information technology to manage their businesses more effectively, and vast amounts of critical business data have been stored in database systems. The volume of this data is expected to grow considerably in the near future. Yet many organizations have been unable to collect valuable insights from the data to guide their marketing strategy, investment and management policies. One of the reasons for this is that most information is stored implicitly in the large amounts of data. Fortunately, new and sophisticated techniques being developed in the area of data mining can help companies leverage their data more effectively and extract insightful information from their data. This tutorial describes the fundamental algorithms for data mining, many of which have been proposed in recent years. These techniques include association rules, correlation, causal relationships, clustering, outlier detection, similar time sequences, similar images, sequential patterns and classification. In addition, since we will cover technical material in some degree of depth, the audience will get a good exposure to the results in the area, and also future research directions.	algorithm;anomaly detection;association rule learning;causality;cluster analysis;data mining;database;quantum correlation;statistical classification	Rajeev Rastogi;Kyuseok Shim	1999		10.1145/312179.312187	data mining	DB	-37.607854230603685	-7.8500477434755735	188770
66ce65ede64321da2b3d07c6cf95d96ac7f21e0b	cataloguing and searching musical sound recordings in an ontology-based information system		Although a lot of information is available worldwide, getting the right piece of information is challenging. In addition, integrating information from different sources is still very complicated. To make the different information systems to interoperate, it is necessary to provide common knowledge structures. Ontologies have been developed for making the semantics of a domain explicit. However, using such formal structures is not straightforward and their benefits are not clear to the users. The work presented in this paper explores the use of an ontology for musical sound recordings in order to allow cataloguing and searching for such information. Therefore it investigates how the knowledge society can benefit from ontologies, with focus on the culture, more specifically the music domain. Some possible benefits and pitfalls are also described and a case study shows an evaluation of the proposed approach by users of this domain. DOI: 10.4018/978-1-4666-4157-0.ch024	information system;interoperability;knowledge society;ontology (information science)	Marcelo de Oliveira Albuquerque;Sean W. M. Siqueira;Maria Helena Lima Baptista Braz	2011	IJKSR	10.4018/jksr.2011100102	computer science;knowledge management;artificial intelligence;data mining;database;multimedia;world wide web;information retrieval	AI	-44.776287841492405	3.7539039761117454	188951
1159109450d7f893cb7d3c504a7ba98e4b7a4c1a	improving information standards and remote sensing support for disaster management	semantic web disaster management remote sensing user needs social networking;social networking online emergency management geophysical techniques geophysics computing remote sensing;geospatial systems disaster management remote sensing support data management operational clients service infrastructure custom products software tools online data services social networks product lifecycle;disaster management standards geospatial analysis remote sensing earth sensors social network services	Effective remote sensing support to disaster management may require rethinking the processes, standards, and life-cycles for data management. These have traditionally emphasized the viewpoints and concerns of data providers and their immediate operational clients. But particularly in disaster management, there is a need to make end users more central to the design and evolution of the services infrastructure, so as to account for local variability and rapid evolution of user needs. State of the art online data services now provide opportunities to let users request and receive custom products on demand; manipulate them with ubiquitous software tools; and share them across social networks. The resulting “product lifecycle” offers both challenges and unprecedented opportunities for the design and development of geospatial and remote sensing systems in support of disaster management.	process (computing);social network;spatial variability;ubiquitous computing	John Evans;Patrice Cappelaere;Karen Moe;Stuart Frye;Dan Mandl	2014	2014 IEEE Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2014.6946696	knowledge management;data mining;computer security	Mobile	-40.93019647760206	-2.5092092426133443	188996
afa13e3ee910f8d0372c0a7e917feb7cd2b6cb16	a web-geographical information system to support territorial data integration	geographic information system;data integrity		geographic information system	Giuseppe Pozzi;Fabio A. Schreiber;Letizia Tanca;Luca Tosi	2005			xml;local information systems;data integration;geographic information system;information system;data mining;database;business;entity–relationship model;data integrity;gis and public health	DB	-38.83834188771296	-3.7455891870471425	189039
be828cc46f8c618c2e952d5304f19ccaa3268d64	applying commercial computer vision tools to cope with uncertainties in a citizen-driven archive: the case study [email protected]!		Uncertainties in data, e.g., incomplete data sets, data quality issues or inconsistencies in annotations, are a common phenomenon across disciplines. How to address these issues is context dependent. In this paper, we address uncertainties in the citizen-driven archive Topotheque as a concrete use-case in the Digital Humanities project exploreAT!, and demonstrate, how to deal with uncertainties by benchmarking a set of selected commercial computer vision (CV) tools. The approach aims to enrich Topotheque's data to enable better access, connectivity and analysis for both researchers and citizens. Results show that by applying CV, existing uncertainties are noticeably reduced, but new ones also introduced. Better grounds for semantic structuring are provided, enabling higher connectivity and linking within Topotheque, but also across other data sets. Ultimately, the enrichment of the archive is for the benefit of both researchers and citizens enabled by addressing and tackling apparent uncertainties.	archive;computer vision;data quality;digital humanities;email;gene ontology term enrichment	Amelie Dorn;Eveline Wandl-Vogt;Thomas Palfinger;José Luis Díaz;Barbara Piringer;Alexander Schatek;Rainer Zoubek	2018		10.1145/3284179.3284322		HCI	-41.55788750088809	1.5284147109770423	189300
55e4873443f3714335ea49120214db82b0661cbe	linklion: a link repository for the web of data		Links between knowledge bases build the backbone of the Web of Data. Consequently, numerous applications have been developed to compute, evaluate and infer links. Still, the results of many of these applications remain inaccessible to the tools and frameworks that rely upon it. We address this problem by presenting LinkLion, a repository for links between knowledge bases. Our repository is designed as an open-access and open-source portal for the management and distribution of link discovery results. Users are empowered to upload links and specify how these were created. Moreover, users and applications can select and download sets of links via dumps or SPARQL queries. Currently, our portal contains 12.6 million links of 10 different types distributed across 3184 mappings that link 449 datasets. In this demo, we will present the repository as well as different means to access and extend the data it contains. The repository can be found at http://www.linklion.org.	download;internet backbone;open-source software;sparql;semantic web;upload;world wide web	Markus Nentwig;Tommaso Soru;Axel-Cyrille Ngonga Ngomo;Erhard Rahm	2014		10.1007/978-3-319-11955-7_63	web service;web development;data web;web api;semantic web;web navigation;web page;web 2.0;world wide web	Web+IR	-40.68350667599053	3.5386868498159454	189393
a2b20c96067a1a53930a2fe57eac3bec164968ed	framestep: a framework for annotating semantic trajectories based on episodes		Abstract We are witnessing an increasing usage of location data by a variety of applications. Consequently, information systems are required to deal with large datasets containing raw data to build high level abstractions. Semantic Web technologies offer powerful representation tools for pervasive applications. The convergence of location-based services and Semantic Web standards allows an easier interlinking and annotation of trajectories. However, due to the wide range of requirements on modeling mobile object trajectories, it is important to define a high-level data model for representing trajectory episodes and contextual elements with multiple levels of granularity and different options to represent spatial and temporal extents, as well as to express quantitative and qualitative semantic descriptions. In this article, we focus on modeling mobile object trajectories in the context of Semantic Web. First, we introduce a new version of the Semantic Trajectory Episodes (STEP) ontology to represent generic spatiotemporal episodes. Then, we present FrameSTEP as a new framework for annotating semantic trajectories based on episodes. As a result, we combine our ontology, which can represent spatiotemporal phenomena at different levels of granularity, with annotation algorithms, which allow to create instances of our model. The proposed spatial annotation algorithm explores the Linked Open Data cloud and OpenStreetMap tags to find relevant types of spatial features in order to describe the environment where the trajectory took place. Our framework can guide the development of future expert systems in trajectory analysis. It enables reasoning about knowledge gathered from large trajectory data and linked datasets in order to create several intelligent services.		Tales Paiva Nogueira;Reinaldo Bezerra Braga;Carina Teixeira de Oliveira;Hervé Martin	2018	Expert Syst. Appl.	10.1016/j.eswa.2017.10.004	raw data;machine learning;social semantic web;semantic computing;data mining;linked data;artificial intelligence;semantic web;data model;semantic grid;computer science;annotation	DB	-38.415727206268684	0.8235564217260942	189613
121b32304ac3aec14564ab2468a2729427cf81c4	call for papers issue 3/2019 - data analytics and optimization for decision support				Wolfgang W. Bein;Stefan Pickl;Fei Tao	2017	Business & Information Systems Engineering	10.1007/s12599-017-0503-3	data mining;computer science	DB	-38.760641995202384	-6.265649441031454	189714
9fc9ab98429e2c97d02a080f69721de029705c2b	demonstrating effective all-optical processing in ultrafast data networks using semiconductor optical amplifiers	electrical engineering and computer science;thesis	Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2008.	optical amplifier;semiconductor	Jade P. Wang	2008			electronic engineering;computer science;electrical engineering;computer engineering	Robotics	-46.91842823915913	-6.807964936033677	189785
94bcb1b2c336b818f96e5a770d00e690a189d23f	managing scientific metadata	hypermedia markup languages;knowledge network;database system;file servers;sql;heterogeneous data;ecology;xml relational databases network synthesis information retrieval database systems impedance vocabulary protocols microorganisms spatiotemporal phenomena;internet;metadata needs scientific metadata management network enabled database framework xml documents arbitrary schemas sql compliant relational database systems metadata catalog knowledge network for biocomplexity rdf like methods data sets content standards data confederation ecological research independent agencies heterogeneous data networked access distributed solution open source based components;xml document;relational database system;meta data;relational databases;scientific information systems;java scientific information systems ecology meta data hypermedia markup languages relational databases sql internet file servers;java;open source	Metacat is a network-enabled database framework that lets users store, query, and retrieve XML documents with arbitrary schemas in SQL-compliant relational database systems. The system (available from the Knowledge Network for Biocomplexity, http://knb.ecoinformatics.org/) incorporates RDF-like methods for packaging data sets to allow researchers to customize and revise their metadata. It is extensible and flexible enough to preserve utility and interpretability working with future content standards. Metacat solves several key challenges that impede data confederation efforts in ecological research, or any field in which independent agencies collect heterogeneous data that they wish to control locally while enabling networked access. This distributed solution integrates with existing site infrastructures because it works with any SQL-compliant database system. The framework's open-source based components are widely available, and individual sites can extend and customize the system to support their data and metadata needs.		Matthew B. Jones;Chad Berkley;Jivka Bojilova;Mark Schildhauer	2001	IEEE Internet Computing	10.1109/4236.957896	file server;sql;relational database management system;the internet;xml;relational database;computer science;data mining;database;database catalog;java;metadata;world wide web;metadata repository	HPC	-37.59418733005336	1.8018822661399618	189819
5c415b3a7d9c883b2c4f8a3f946c48110cab3470	characteristics classification of durian varieties images for knowledge representation by using conceptual graph		The importance and interest in agriculture field nowadays contributed to the rapid increasing of the deluge fruits images, for instance; durian images. With hundreds of durian varieties, it will be a challenging task to differentiate the images of this crop. Furthermore, with the existing of semantic gap, the current search engine unable to retrieve only the relevant varieties images to the user. Hence, applying semantic technology became the crucial thing in order to bridge the semantic gap. Thus, this paper will discuss about the durian varieties and its characteristics in order to differentiate and determine the images of a durian variety with other varieties. These characteristics then will be employed later in the Conceptual Graph to construct and semantically represent the knowledge regarding of durian varieties and its characteristics.	conceptual graph;knowledge representation and reasoning	Nur Syafikah Ibrahim;Zainab Abu Bakar;Nurazzah Abdul Rahman	2014		10.1007/978-3-319-12844-3_11	artificial intelligence;machine learning;pattern recognition	AI	-39.36588393154981	3.712306819127165	189898
db12753bf288b639f393d230b92c6097e352b074	towards a data warehouse architecture for managing big data evolution		The problem of designing data warehouses in accordance with user requirements and adapting its data and schemata to changes in these requirements as well as data sources has been studied by many researchers worldwide in the context of relational database environments. However, due to the emergence of big data technologies and the necessity to perform OLAP analysis over big data, innovative methods must be developed also to support evolution of data warehouse that is used to analyse big data. Therefore, the main objective of this paper is to propose a data warehousing architecture over big data capable of automatically or semi-automatically adapting to user needs and requirements as well as to changes in the underlying data	algorithm;big data;constraint (mathematics);emergence;evolution;metadata modeling;online analytical processing;relational database;requirement;semiconductor industry;source data;user requirements document	Darja Solodovnikova;Laila Niedrite	2018		10.5220/0006886100630070	database;architecture;big data;data warehouse;computer science	DB	-35.837267496548826	-1.014434373387743	189958
24e483af8b60e408c651f995e1900ccba1338c60	"""erratum to """"compact and efficient strings for java"""" [science of computer programming 75 (2010) 1077-1094]"""	computer program		computer programming;java	Christian Häubl;Christian Wimmer;Hanspeter Mössenböck	2011	Sci. Comput. Program.	10.1016/j.scico.2011.04.005	computational science;computer science;theoretical computer science;programming language	Logic	-47.25202656268355	-3.660146116400458	190012
ddc10ab780c6b0464147ae63de958c7c3355855a	an architecture of semantic knowledge system for biosafety	keyword based search;feedback mechanism;semantic knowledge system architecture;metadata;customized interactive user interface;domain knowledge based system maintenance;semantic query;knowledge based systems ontologies humans birds influenza knowledge engineering information retrieval semantic web collaboration instruments;data mining;biosafety;ontologies artificial intelligence;domain knowledge;metadata semantic web knowledge system ontology;knowledge systems;domain knowledge based system development;knowledge system;data visualization;ontology engineering;semantic web;meta data;ontologies;humans;interactive user interface;customized interactive user interface semantic knowledge system architecture biosafety domain knowledge based system development domain knowledge based system maintenance keyword based search ontology engineering data acquisition metadata semantic query;data acquisition;ontology;algorithm design and analysis;knowledge based systems;semantic web data acquisition knowledge based systems meta data ontologies artificial intelligence	The development and maintenance of domain knowledge based system need a lot of manual operations, and with the increasing amount of contents in the system, it is more and more difficult to find the relevant information. The keyword based search usually can not return the accurate result. To solve these problems, this paper proposes a semantic knowledge system with ontology engineering approach. It can acquire data and update knowledge system with less human efforts, and the feedback mechanism can help the knowled gesystem automatically correct the inaccurate metadata. The semantic query improves the keyword based query and returns the answer accurately. The customized interactive user interface is implemented with proper technology to display the knowledge hierarchy and data relationship, and improves the efficiency of the system user.	dikw pyramid;feedback;knowledge-based systems;ontology engineering;semantic query;user interface	Xianggen Wang;Tiejian Luo;Chi Zhang;Wei Liu	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.493	computer science;knowledge management;artificial intelligence;knowledge-based systems;data mining;metadata;information retrieval	DB	-38.54047839245632	2.6389653702499722	190725
393f85195b5dc97619613e4048b584838bb439e4	consistency in a stream warehouse		A stream warehouse is a Data Stream Management System (DSMS) that stores a very long history, e.g. years or decades; or equivalently a data warehouse that is continuously loaded. A stream warehouse enables queries that seamlessly range from realtime alerting and diagnostics to long-term data mining. However, continuously loading data from many different and uncontrolled sources into a real-time stream warehouse introduces a new consistency problem: users want results in as timely a fashion as possible, but “stable” results often require lengthy synchronization delays. In this paper we develop a theory of temporal consistency for stream warehouses that allows for multiple consistency levels. We show how to restrict query answers to a given consistency level and we show how warehouse maintenance can be optimized using knowledge of the consistency levels required by materialized views.	data mining;management system;materialized view;real-time transcription;theory;uncontrolled format string	Lukasz Golab;Theodore Johnson	2011			real-time computing;dimensional modeling;computer science;data mining;database;data stream mining	DB	-34.31414291852362	0.918080981206084	190766
e23d7f2bbd2b175cdb363c0afe4431e5ddbd6d9a	expert systems for cpe: on evolution and data analysis.	data analysis;expert system			Bernard Domanski;Sidney W. Soberman	1989			data mining;expert system;computer science	Theory	-39.32577741076324	-6.099252744736866	190999
6f417568f2ad958011870ef0d89c6c742183220f	big data	big data	antees about the optimality of these systems and strate-gies? Even if we can store the data, how do we learn from data sets that we cannot hold on a single computer or even in many computers? Can we learn from data on the fly? Moreover, our data is heterogeneous: We are observing social networks, ad click-throughs, gene sequences, protein concentrations from cells, as well as confidential personal data that must be kept secret. How do we adapt our systems and algorithms for all kinds of data? These are just some of the exciting challenges facing the big data community. For such a diverse topic like big data, it is nearly impossible to provide a comprehensive picture. Instead, in this issue we try to highlight some recent developments organized into three main themes: the theoreti-b ig data is everywhere. In just about every part of the modern world, scientists and engineers are developing new ways to measure events. Whether it's sensors, traffic cameras, sales data, Web usage, gene expression, or just about anything else, we have entered an age of truly massive data. Why do we collect this data? It's simple—to learn. We want to make predictions , quantify reality, or understand the past to optimize the decisions we make. Massive data leads to many challenges for computer scientists. We're recording petabytes of data every day. Before we even think about learning from it, how and where do we store it? What kinds of systems do we build to retrieve and analyze the data? Can we develop theoretical guar-cal foundation providing models and algorithms for reasoning about various data processing tasks, the large-scale computer systems for handling big data, and the range of applications and analyses enabled by big data from a variety of scientific domains. It has been an interesting time for big data with innovations coming simultaneously from theorists, system builders, and scientists or application designers. We hope to provide readers with an idea of the interplay between developments in these three different communities, how ideas and priorities in different communities interact, and together drive forward the development of big data analysis. Theory Opening the issue is an introduction to the theo-interest in big data has given rise to a lot of recent interest in building systems to support queries and transactions over massive quantities of data.	algorithm;big data;computer scientist;confidentiality;on the fly;personally identifiable information;petabyte;sensor;social network	Andrew Cron;Huy L. Nguyen;Aditya G. Parameswaran	2012	ACM Crossroads	10.1145/2331042.2331045	data science;data mining;feature selection;cluster analysis;intelligent decision support system;computer science;big data;government;database transaction	ML	-37.8262423197484	-8.074411439600416	191470
ce0678c32ad7926690e1079f723504b85fe6f6d8	graph-based information exploration over structured and unstructured data	structured;unstructured;graph-based;exploring relations;biomedical	With the rise of the Semantic Web, several public semantic repositories like Knowledge Bases, Ontologies and Taxonomies have been developed in a variety of domains. For specific domains like the biomedical domain they have already formed a huge valuable infrastructure. On the other hand, the development of efficient algorithms for Natural Language Processing gave us access to the massive knowledge hidden in many unstructured resources. Combining and harvesting these two worlds would result into a very productive knowledge fusion applicable in several domains. In this paper, an extensible framework is presented that focuses on accessing and graphically presenting the knowledge coming from all available structured and unstructured resources. An abstraction formalism for representing any type of query based on graphs is the base of this approach. This formalism makes the framework accessible to non-expert users that have no knowledge of constructing queries in any querying language and barely understand what structured and unstructured resources are. The architecture that will allow for the framework to be adaptable to all available resources is described along with a proof of concept implementation in the biomedical domain.	algorithm;analysis of algorithms;categorization;extensibility;formal system;is-a;logical connective;natural language processing;ontology (information science);semantic web;semantic reasoner;semantics (computer science);server (computing);server-side;shattered world;taxonomy (general)	Giannis V. Koumoutsos;Maria Fasli;Ian Lewin;David Milward	2017	2017 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2017.8258145	data mining;data visualization;architecture;ontology (information science);computer science;semantics;semantic web;abstraction;formalism (philosophy);unstructured data	DB	-37.3633058326411	2.3681959553503464	191514
57c5a2128a643f443c4fd6a05395ca0f114fe75c	blo: batata lake (oriximiná/pa) application ontology		This work presents the BLO ontology (Batata Lake Ontology), an application ontology that describes in a structured way the data of research done by limnology researchers of Federal University of Rio de Janeiro (UFRJ) Macaé in Batata Lake (Oriximiná/PA). The main contribution of the BLO is the creation of a research data repository in RDF and the BLS application (Batata Lake System), a semantic web application to support researchers in environmental impact assessments, in preservation areas settings, in species protection and recovery of degraded areas, among other activities.	binary large object;field research;kaby lake;relevance;research data archiving;sampling (signal processing);semantic web;semantic reasoner;vocabulary;web application;winsock	Adriano N. de Souza;Adriana P. de Medeiros	2015			limnology;data mining;semantic web;ontology;rdf;cartography;environmental impact assessment;information repository;geography	Web+IR	-40.0205760661242	-2.440660466270643	191610
a3d80e4acf5585d300d1c3ed07e781b30ba11974	effective retrieval and new indexing method for case based reasoning: application in chemical process design	fuzzy set;decision tree;case base reasoning;genie chimique;sphere indexing algorithm;fuzzy set theory;fuzzy sets;chemical engineering;process design;indexing method;adaptation guided retrieval;indexation;genie des procedes;characteristic function;design;case based reasoning;similarity function;similarity measure;algorithm design	To cite this version: Negny, Stéphane and Riesco, hector and Le Lann, Jean Marc Effective retrieval and new indexing method for case based reasoning: Application in chemical process design. (2010) Engineering Applications of Artificial Intelligence, vol. 23 (n° 6). pp. 880-894. ISSN 0952-1976 Open Archive Toulouse Archive Ouverte (OATAO) OATAO is an open access repository that collects the work of Toulouse researchers and makes it freely available over the web where possible.	algorithm;apple a6;apple a7;applications of artificial intelligence;archive;case-based reasoning;conditional (computer programming);constraint (mathematics);constraint satisfaction problem;decision support system;decision tree;discretization;fuzzy set;hector;heuristic (computer science);international standard serial number;jean;numerical analysis;performance;requirement;set theory;similarity measure	Stéphane Negny;Hector Riesco;Jean-Marc Le Lann	2010	Eng. Appl. of AI	10.1016/j.engappai.2010.03.005	mathematical optimization;computer science;artificial intelligence;machine learning;data mining;fuzzy set;algorithm	AI	-41.19282742215034	-9.853241830485537	191640
585651313bb43a39b6a5615471ce8764c80323b7	temporal provenance model (tpm): model and query language		Provenance refers to the documentation of an object’s lifecycle. This documentation (often represented as a graph) should include all the information necessary to reproduce a certain piece of data or the process that led to it. In a dynamic world, as data changes, it is important to be able to get a piece of data as it was, and its provenance graph, at a certain point in time. Supporting time-aware provenance querying is challenging and requires: (i) explicitly representing the time information in the provenance graphs, and (ii) providing abstractions and efficient mechanisms for time-aware querying of provenance graphs over an ever growing volume of data. The existing provenance models treat time as a second class citizen (i.e. as an optional annotation). This makes time-aware querying of provenance data inefficient and sometimes inaccessible. We introduce an extended provenance graph model to explicitly represent time as an additional dimension of provenance data. We also provide a query language, novel abstractions and efficient mechanisms to query and analyze timed provenance graphs. The main contributions of the paper include: (i) proposing a Temporal Provenance Model (TPM) as a timed provenance model; and (ii) introducing two concepts of timed folder, as a container of related set of objects and their provenance relationship over time, and timed paths, to represent the evolution of objects tracing information over time, for analyzing and querying TPM graphs. We have implemented the approach on top of FPSPARQL, a query engine for large graphs, and have evaluated for querying TPM models. The evaluation shows the viability and efficiency of our approach.	british undergraduate degree classification;database;documentation;entity;experiment;graph (discrete mathematics);object lifetime;query language;scalability;trusted platform module	Seyed-Mehdi-Reza Beheshti;Hamid R. Motahari Nezhad;Boualem Benatallah	2012	CoRR		computer science;data mining;database;programming language;world wide web	DB	-36.107578982731695	3.0847357153907584	192161
c4238d3230aa1ade4427e831b914cbd9d58e0251	template for a system design file using oodpm version 2015		"""Object Oriented Design by Prototype Methodology (OODPM) integrates two known technologies: the object approach and the prototype concept. Object oriented methodology is used for internal system design, and prototype methodology is used for external system design. This document is a template for a system design file using OODPM version 2015 (titles of paragraphs only). For full explanations for each paragraph look at [1]. This version developed after tens of projects that developed and plan using version 6 in a very vast projects for national information systems. This version companion by """"OODPM Methodology for Management Information Systems life Cycle"""" (meanwhile only in Hebrew)."""	management information system;prototype;systems design	Offer Drori	2016	ACM SIGSOFT Software Engineering Notes	10.1145/2853073.2853090		SE	-43.75505907140822	-2.771668209391205	192459
d8c16e8a3738442d35dc200a2c8ecb8ed20cd193	multidimensional mining of big social data for supporting advanced big data analytics		Big social data are now everywhere. They constitute a rich source of knowledge that is prone to be explored and mined in order to support advanced big data analytics. Multidimensional mining identifies a promising collection of tools to this end. Following this recent trend, in this paper, we provide an overview on two state-of-the-art proposals that show how big data analytics over big social data work in practice.	big data;mined	Alfredo Cuzzocrea	2017	2017 40th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)	10.23919/MIPRO.2017.7973630	the internet;data mining;data science;computer science;metadata;big data;analytics	DB	-37.27666552123077	-6.300315219080091	192955
2d52ce7fc821f4ac01b69e3a02b5a6de997e478b	an architectural route through pacs	image storage;architecture systeme;pacs;data processing;optical imaging;solid modeling;image storage optical films optical imaging data processing solid modeling;information system;system architecture;systeme information;optical films	Digital archiving of diagnostic images has been in demand for many years-particularly since optical mass storage technology increased the chances of its realization. Technically, the digital picture archive was regarded as a giant data store with film input and hardcopy or soft-copy output-a simple and straightforward concept. Then, as more and more digital imaging machines were implemented in hospitals, mass storage media for archiving the digital pictures were looked upon as peripheral devices to the picture-generating machines. Meanwhile, it was becoming obvious that digital picture archives would be far more than isolated devices or extensions to existing isolated devices. Instead, they would become the kernel of networks that would connect a variety of imaging machines, interact with hospital databases and administration systems, and, last but not least, facilitate communication of pictorial and other information between medical personnel. This perspective added tremendous complexity to the concept. Complexity is mainly a consequence of looking at PACS from the standpoint of the user, the network engineer, the designer of imaging (sub)systems, or the manufacturer of high-technology key components. For instance, the user's demands conflict with the manufac-turer's product strategy. The user needs a system that is highly adaptable in terms of functions and structure to his individual work situation. The manufacturer, however, for the sake of economy, must produce standard products with a fixed catalog of functions. Another complication is that some well-established, basic data processing concepts have to be thrown overboard when innovative technologies, techniques, and methods are employed to process, store, and communicate a large number of high-resolution pictures with sufficient speed. Hasty development with inadequate concepts can result in the proliferation of confusing and redundant systems. A thorough architectural approach is urgently required to decorrelate user requirements from technical and manufacturing boundary conditions and produce generic concepts for system modules and procedures down to the bottom system level. This article presents a general procedure for an adequate architectural approach and describes in detail concepts for solving the basic technical problems. A conceptual process must be top down, but where is the top and where is the bottom? The structure of a process depends on the observer's perspective. A PACS is a typical example of a system with multiple dimensions, including user, network, system management, data processing , and other dimensions. The system architect's primary mission is to define the system's dimensions such that conceptual models from the different perspectives become …	archive;data store;database;digital camera;digital imaging;image resolution;mass effect trilogy;mass storage;peripheral;picture archiving and communication system;redundancy (engineering);requirement;systems management;top-down and bottom-up design;unicom system architect;user requirements document	Dietrich Meyer-Ebrecht;Thomas Wendler	1983	Computer	10.1109/MC.1983.1654465	embedded system;computer vision;data processing;computer hardware;computer science;optical imaging;solid modeling;picture archiving and communication system;information system	Graphics	-33.92865869579644	-4.6830748298196045	193053
2644318620469002b74061e41f1f28ee97be5bdb	progress on multi-lingual named entity annotation guidelines using rdf (s)		This paper provides a discussion and concise summary of the PIA (Portable Information Access project) guidelines for annotators and tool developers for annotating what we call named entity ‘plus’ (NE+) expressions such as individual names or technical terms that we want to distinguish for whatever reason from the rest of a text. In particular we consider how to annotate locally ambiguous syntactic and semantic structures. We provide notation that conforms to RDF(S) so that annotated documents can have their content accessed on the Semantic Web, i.e. the next generation World Wide Web. In this new framework named entities become instances of concepts in an explicit ontology, and the base text provides links to the annotation and ontology data files.	information access;named entity;resource description framework;semantic web;world wide web	Nigel Collier;Koichi Takeuchi;Chikashi Nobata;Jun-ichi Fukumoto;Norihiro Ogata	2002			social semantic web;web standards;semantic web;web intelligence;rdf;data mining;information retrieval;semantic analytics;semantic web stack;computer science;annotation	Web+IR	-41.31006741646703	3.929288757833964	193401
ff12fd9ac86170e0418e01584a8e8cf22b7d5962	program committee		Main Track P. Athanas (Virginia Tech, US) H. Basson (U. Littoral, FR) T. Basten (TU Eindhoven, NL) N. Bergmann (U Queensland, AU) C. Bouganis (Imp. Coll., UK) P. Carballo (ULPGC, ES) L. Chen (NTU, TW) T. Chen (Colorado St., US) G. Danese (U Pavia, IT) D. Demarchi, (Politechnic of Torino, IT) R. Drechsler (U Bremen, DE) L. Fanucci (U Pisa, IT) J. Ferreira (U Porto, PT) M. Figueroa (U Concepcion, CL) K. Gaj (George Mason U, USA) V. Goulart (U Kyushu, JP) G. Jacquemod (U Nice-Sophia, FR) J. Haid (Infineon, AT) I. Hamzaoglu (U Sabanci, TR) A. Hemani (KTH, SE) D. Houzet (Grenoble IT, FR) L. Jozwiak (TU Eindhoven, NL) B. Juurlink (TU Berlin, DE) K. Kent (U New Brunswick, CN) P. Kitsos (TEI of Western Greece, GR) Z. Kotasek, (TU Brno, CZ) H. Kubatova (CTU Prague, CZ) K. Kuchcinski (U Lund, SE) S. Kumar (U Jonkoping, SE)	coding tree unit;display resolution;entity–relationship model;haplogroup cz (mtdna);interface message processor;mason;mahdiyar;nl (complexity);network interface device;text encoding initiative	Pilar Carballo	2005		10.1109/ICMB.2005.82		Security	-46.08642526450868	-9.661832441630603	193405
