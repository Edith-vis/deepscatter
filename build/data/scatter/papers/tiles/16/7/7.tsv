id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
3487334f409ee11c3c4cc14b3326017e374e66f4	a non-cooperative pareto-efficient solution to a single-shot prisoner's dilemma	non cooperative games;quantum computer;non cooperative game;game theory;prisoner s dilemma	The Prisoner’s Dilemma is a simple model that captures the essential contradiction between individual rationality and global rationality. Although the one-shot Prisoner’s Dilemma is usually viewed simple, in this paper we will categorize it into five different types. For the type-4 Prisoner’s Dilemma game, we will propose a selfenforcing algorithmic model to help non-cooperative agents obtain Pareto-efficient payoffs. The algorithmic model is based on an algorithm using complex numbers and can work in macro applications.	algorithm;categorization;internet;pareto efficiency;prisoner's dilemma;rationality;whole earth 'lectronic link	Mu-rong H. Wu	2010	CoRR		non-cooperative game;superrationality;game theory;cheap talk;win–stay, lose–switch;traveler's dilemma;economics;simultaneous game;repeated game;microeconomics;normal-form game;mathematical economics;algorithmic game theory;welfare economics;deadlock;symmetric game;symmetric equilibrium;strong reciprocity	ECom	-9.075600056992412	-1.6541728157230562	51771
820b54fdbd43706da4ff26c6847d936f8b9ee1a9	an agents and artifacts approach to distributed data mining	multi agent system;collaborative learning;jason;cartago;distributed data mining	This paper proposes a novel Distributed Data Mining (DDM) approach based on the Agents and Artifacts paradigm, as implemented in CArtAgO [9], where artifacts encapsulate data mining tools, inherited from Weka, that agents can use while engaged in collaborative, distributed learning processes. Target hypothesis are currently constrained to decision trees built with J48, but the approach is flexible enough to allow different kinds of learning models. The twofold contribution of this work includes: i) JaCA-DDM: an extensible tool implemented in the agent oriented programming language Jason [2] and CArtAgO [10,9] to experiment DDM agent-based approaches on different, well known training sets. And ii) A collaborative protocol where an agent builds an initial decision tree, and then enhances this initial hypothesis using instances from other agents that are not covered yet (counter examples); reducing in this way the number of instances communicated, while preserving accuracy when compared to full centralized approaches.	agent-based model;agent-oriented programming;centralized computing;data mining;database;decision tree;distributed data management architecture;experiment;jason;programming language;programming paradigm;shared services;simulation;weka	Xavier Limón;Alejandro Guerra-Hernández;Nicandro Cruz-Ramírez;Francisco Grimaldo	2013		10.1007/978-3-642-45111-9_30	collaborative learning;computer science;artificial intelligence;data science;machine learning;data mining	ML	-15.858156306259104	-9.318724067234792	51856
dcaa861c26e77bb17d3126c41c032b49f141d86e	subjective probability over a subjective decision tree	decision tree;subjective probability;decision maker;model uncertainty;state space;preference for flexibility;private information	Since Savage’s seminal work, a state space has been assumed as a primitive and as a complete description of the world. However, the decision maker may have subjective states in her mind, which are distinct from the primitive states, but relevant for her decisions. Dekel, Lipman and Rustichini (2001) derive a unique subjective state space from preference. In a dynamic setting, a state space S and a filtration {Ft}t=0 over S has been taken as primitives. We derive a triplet (S, {Ft}t=0, μ) from preference, where S is a subjective state space, {Ft}t=0 is a subjective filtration over S, and μ is a subjective probability over S. We also show the uniqueness of the representation.	decision tree;mind;state space;triplet state	Norio Takeoka	2007	J. Economic Theory	10.1016/j.jet.2006.09.010	decision-making;private information retrieval;state space;operations management;decision tree;mathematics;subjective expected utility	ECom	-8.664946767042288	-1.1645877899891781	52381
35c1660d40708668b3c0f10bbe09c1132d95bb0c	exploiting max-sum for the decentralized assembly of high-valued supply chains	high value;current state-of-the-art approach;supply chains;high-valued supply chain;low resource usage profile;low value;decentralized assembly;optimal supply chains;low resource;high resource requirement;exploiting max-sum;decentralized supply chain formation;supply chain formation	Supply Chain Formation involves determining the participants and the exchange of goods within a production network. Today’s companies operate autonomously, making local decisions, and coordinating with other companies to buy and sell goods along their Supply Chains. Such temporal interactions need to be formed rapidly and in a decentralized manner. For sufficiently large problems, current state-of-the-art approaches for Decentralized Supply Chain Formation are only capable of either (i) producing Supply Chains of high value at the expense of high resource requirements; or (ii) require low resources at the expense of producing Supply Chains of low value. In this paper we describe an algorithm that is able to produce Supply Chains of high value while keeping a low resource usage profile. Moreover, our method is able to produce near optimal Supply Chains while using up to four orders of magnitude less resources that the state-of-the-art.	algorithm;interaction;requirement	Toni Penya-Alba;Meritxell Vinyals;Jesús Cerquides;Juan A. Rodríguez-Aguilar	2014			teamwork;service management	AI	-8.498467368650287	-7.500914328245093	52463
a211bb0d33e44bf16b2e867cab0d6609bb6ac024	endogenous games and equilibrium adoption of social norms and ethical constraints	subgame perfect equilibrium;behavioral economics;behavioral economics endogenous games bilateral bargaining prisoners dilemma social norms;social norm	We depart from the usual assumption in noncooperative game theory that games arise exogenously. We assume instead that games between agents are formed endogenously in the sense that agents choose their opponents through a costly search process. Since agents are aware in this situation that both parties have the option of rejecting a match, they have an incentive to make themselves as attractive a partner as they can. This is accomplished in a pregame in which agents consider all the potential strategies in a game but choose to learn only a subset. We assume that this choice is observable by their potential partners in the matching game. We motivate this as a proxy for subscribing to a code of ethics, accepting a set of social norms, or being a member of religious, philosophical or political group. We show that agents will sometimes choose to constrain their action sets in the pregame in order to achieve better matches and higher payo¤s. We suggest that this might provide at least a partial explanation for experimental observations that agents apparently choose strategies that do not maximize their payo¤s.	embedded system;exception handling;experiment;game theory;norm (social);observable;proxy server	John P. Conley;William S. Neilson	2009	Games and Economic Behavior	10.1016/j.geb.2008.09.031	economics;microeconomics;mathematical economics;subgame perfect equilibrium;welfare economics;behavioral economics;norm	AI	-7.267730652659647	-3.610504249249804	52600
266afcd414b0ad54185a34500f3a24da2f84ec53	an agent-based system for bilateral contracts of energy	game theory;leader follower concept;stackelberg game;negotiation process;generating company;agent based system;bilateral contract;load company;behavior protocols	In this paper, an agent-based system for bilateral contracts of energy is proposed. The generating companies submit their offers to the demand companies. The demand companies also submit their bids to the generators. Each load or generator's agent wants to match with an opponent, which offers the most valuable proposal. However, the problem of simultaneous decision-making causes decision conflicts among the agents. To overcome this conflict, we assume loads as the leaders and generators as the followers. We use Stackelberg game to match the seller and buyer agents. The negotiation process between a buyer and its potential seller will determine the power price between them. This process is carried out through a proposed combined time-behavioral protocol (TBP). With negligible changes in around the agreed price, this protocol can reduce the negotiation time considerably. After successful negotiation, the seller and buyer agents could sign a bilateral contract of energy if the market conditions allow it. The applicability of the proposed method is illustrated through a case study.	agent-based model;bilateral filter	Hamed Kebriaei;Ashkan Rahimi-Kian;Vahid Johari Majd	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.03.005	game theory;stackelberg competition;negotiation	AI	-7.3581882737524795	-8.132738122066847	52734
1a6bbe8abf26ff789d6da1fe68fba65f43e06fb5	gains from diversification on convex combinations: a majorization and stochastic dominance approach	ordered set;stochastic dominance;selected works;relacion orden;dominancia estocastica;portfolio selection;risk aversion;expected utility;seleccion cartera;ordering;ensemble ordonne;selection portefeuille;relation ordre;majorization;portfolio management;preferencia;bepress;dominance stochastique;preference;utilite attendue;gestion cartera;majorization stochastic dominance portfolio selection expected utility diversification;diversification;gestion portefeuille;diversificacion;utilidad espera;aversion riesgo;aversion risque;convex combination;conjunto ordenado;second order stochastic dominance	By incorporating both majorization theory and stochastic dominance theory, this paper presents a general theory and a unifying framework for determining the diversification preferences of risk-averse investors and conditions under which they would unanimously judge a particular asset to be superior. In particular, we develop a theory for comparing the preferences of different convex combinations of assets that characterize a portfolio to give higher expected utility by second-order stochastic dominance. Our findings also provide an additional methodology for determining the second-order stochastic dominance efficient set.	diversification (finance);expected utility hypothesis;risk aversion	Martin Egozcue;Wing-Keung Wong	2010	European Journal of Operational Research	10.1016/j.ejor.2009.01.007	financial economics;diversification;mathematical optimization;convex combination;risk aversion;economics;expected utility hypothesis;order theory;stochastic dominance;mathematics;mathematical economics;welfare economics;project portfolio management	ML	-6.78367829461332	-0.7120740182107244	52838
182cdbe19416e23379d533ebbf5952327fc0e429	model-based belief merging without distance measures	multiagent system;aggregation function;distance measure;model based approach;satisfiability;integrity constraints and multiagent systems;integrity constraints;belief merging;partial satisfiability	Merging operators try to define the beliefs of a group of agents according to the beliefs of each member of the group. Several model-based propositional belief merging operators have been proposed which use distance measures and aggregation functions. This paper introduces the notion of Partial Satisfiability which is an alternative way of measuring the satisfaction of a formula since this notion lets us have satisfaction values in the interval [0,1]. Partial Satisfiability allows us to define model-based merging operator. The proposal produces similar results to other merging approaches, but while other approaches require many merging operators in order to achieve satisfactory results for different scenarios this proposal obtains similar results for all these different scenarios with a unique operator. Moreover, unlike most of model-based approaches, this approach considers the case where the belief bases are inconsistent. The framework presented is in a preliminary state and further analysis of its properties is needed in order to characterize the proposed merging operator in terms of postulates.	aggregate function;belief revision	Verónica Borja Macías;Maria del Pilar Pozos Parra	2007		10.1145/1329125.1329312	computer science;data integrity;algorithm;satisfiability	DB	-12.810308749731428	-0.85466666502725	52966
466d022f7d0afbd7abb527414454d22862d1aff5	competition among intelligent agents and standard bidders with different risk behaviors in simulated english auction marketplace	software;electronic commerce;software agents artificial intelligence electronic commerce;bidding strategies;risk behaviors english auction simulated marketplace intelligent agents;risk behavior;online auction;hf commerce;software agents;lead jacobian matrices software;intelligent agents;risk preference;lead;english auction;intelligent agent;artificial intelligence;risk behaviors;jacobian matrices;online auctions intelligent agents standard bidders risk behaviors simulated english auction marketplace;qa76 computer software;simulated marketplace	Agents have been used to bid in online auctions to take over the role of human bidders. We can find bidder agents with a variety of bidding strategies that participate in online auction. However, it is not known how the presence of these agents will affect the marketplace in terms of closing price and the chances of winning. In this paper, we study the economic consequence from a simulated English auction market populated by intelligent agents and three groups of standard bidders with different risk preferences. Our study revealed that when intelligent agents compete with the standard bidders, these agents generally perform better than their counterparts. More specifically, we analyse their average winning utility, the average closing price of auctions and the number of auctions won by them. Based on our experimental results, the intelligent agents outperformed the standard bidders in all three aspects.	closing (morphology);intelligent agent;population	Jacob Sow;Patricia Anthony;Chong Mun Ho	2010	2010 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2010.5641740	lead;vickrey auction;unique bid auction;computer science;artificial intelligence;vickrey–clarke–groves auction;software agent;common value auction;english auction;intelligent agent	AI	-8.53639329519855	-8.8758267669914	53108
187061def5097475d033ced986ad45869e99a0ee	a parallel, multi-issue negotiation model in dynamic e-markets	selected works;bepress	Negotiating agents play a key role in e-markets and become more popular. However, in much existing work, the e-markets are assumed to be closed and static, which is unrealistic. To address the issue, this paper developed negotiating agents that can adapt their negotiation strategies, outcome expectations, offer evaluations, and counter-offers generations in dynamic, open e-markets. Also, the proposed agents can generate multiple counter-offers according to different preferences so as to further improve their negotiation outcomes. Finally, the experimental results show the improvements on agents’ profits by employing our negotiation model.	linkage (software);opponent process;the australian	Fenghui Ren;Minjie Zhang;Xudong Luo;Danny Soetanto	2011		10.1007/978-3-642-25832-9_45	simulation;engineering;knowledge management;operations management	AI	-8.700422243787639	-8.613024273130408	53119
e1648dd241ceb749ef8475fca28d7872e3f76cdd	bringing resources into logic	truth values;inference system;perpetual proof;available resources;many valued logics;potential answers;application software;logic;inference mechanisms;logic artificial intelligence humans intelligent systems computational and artificial intelligence application software resumes pattern matching;resumes;artificial intelligent;pattern matching;unknown values;intelligent systems;artificial intelligence;many valued logics inference mechanisms;humans;potential answers lore truth values unknown values available resources perpetual proof inference system;path following;lore;computational and artificial intelligence	We define LORE, a logic with four values, the traditional truth values T and F, and two “Unknown” values, allowing to differentiate between knowing that nothing is known, and not knowing (with the available resources) whether it is known. The purpose of developing this logic is to use it to guide an Artificial Intelligence system. In this kind of system one is engaged in a sort of “perpetual proof” during which propositions are told to the system and the system goal is to answer questions from a user. A computer based on LORE has the capability to remember all the paths followed during an attempt to answer a question. For each path it records the used hypotheses, the missing hypotheses (when the path did not lead to an answer), and why they were assumed missing. An inference system with these capabilities can report answers found and reasons that prevented the inference of other potential answers.	artificial intelligence system;inference engine	Nuno J. Mamede;João P. Martins	1990		10.1109/ISMVL.1990.122624	application software;intelligent decision support system;computer science;artificial intelligence;machine learning;pattern matching;data mining;mathematics;logic;algorithm	AI	-16.119607540647557	2.4531296385487638	53210
2d40b568278e009ca82deff87602c9ca2a6cf7dc	culture, computation, morality		I point to a deep and unjustly ignored relation between culture and computation. First, I interpret Piaget’s theory of child development with the language of theoretical computer science. Then I argue that the two different possible manifestations Piagetian disequilibrium are equivalent to two distinct cultural tendencies (what is commonly referred to as the East-West divide). I argue that this simple characterization of overaccommodation versus overassimilation provides a satisfying explanation as to why the two cultural tendencies differ in the way they empirically do. All such notions are grounded on a firm mathematical framework for those who prefer the computable, and grounded on my personal history for those who prefer the	computability theory;computable function;computation;piaget's theory of cognitive development;theoretical computer science	Jongmin Jerome Baek	2017	CoRR		social science;philosophy;sociology	NLP	-12.351751776755348	2.2793602194745746	53545
d755c975ecded26dfa037b7b79dbe0b965c461e7	preference conditions for multiattribute value functions	value function	This paper examines conditions on preferences that simplify the assessment of multiattribute value functions for use in the analysis of multiobjective decision problems. It is shown that when these conditions hold the value function must have a simple analytic form. A procedure is presented for testing whether the conditions hold and determining the value function when the conditions are found to be valid.		Craig W. Kirkwood;Rakesh K. Sarin	1980	Operations Research	10.1287/opre.28.1.225	mathematical optimization;operations management;mathematics;bellman equation;mathematical economics;welfare economics	DB	-7.162116382329687	-1.682621569528727	53653
897721decfffd24050a0c309d54ebd6dccae39b1	a preview of the new volume		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Kan Chen	1994	J. Intellig. Transport. Systems	10.1080/10248079408903811	simulation;engineering	Robotics	-15.1451460267262	-5.727382572048511	53737
977e801a50dd0abda61b3b8d65155381712910d4	probabilistic conceptual network: a belief representation scheme for utility-based categorization	conceptual knowledge;probabilistic network;knowledge representation scheme;hierarchical knowledge;probabilistic conceptual network;belief representation scheme;utility-based categorization;decision analysis;categorical abstraction;categorization decision model;varying level	Probabilistic conceptual network is a knowl­ edge representation scheme designed for reasoning about concepts and categorical abstractions in utility-based categorization. The scheme combines the formalisms of ab­ straction and inheritance hierarchies from artificial intelligence, and probabilistic net­ works from decision analysis. It provides a common framework for representing con­ ceptual knowledge, hierarchical knowledge, and uncertainty. It facilitates dynamic con­ struction of categorization decision models at varying levels of abstraction. The scheme is applied to an automated machining problem for reasoning about the state of the machine at varying levels of abstraction in support of actions for maintaining competitiveness of the plant.	abductive reasoning;artificial intelligence;categorization;causal filter;competitive analysis (online algorithm);decision analysis;grams;is-a;knowledge representation and reasoning;liquidity at risk;principle of abstraction;principle of maximum entropy;semantic network;semantics (computer science);subsumption architecture	Kim-Leng Poh;Michael R. Fehling	1993			natural language processing;computer science;machine learning;data mining	AI	-17.670956685016055	-3.4795646184802895	53867
94b0c4710c69509b53874bd3c7a6c1495cbb5c4e	exploiting expert knowledge in factored pomdps		Observations and Method Application Passing on information gleaned during method execution: • abstract observation variables associated with abstract actions • method controllers have multiple terminal nodes labeled with abstract observations • method application: match terminal node labels and abstract observation formulas abstract controller: . . . a . . . P ¬P method for a:		Felix Müller;Christian Späth;Thomas Geier;Susanne Biundo-Stephan	2012		10.3233/978-1-61499-098-7-606	computer science;artificial intelligence;machine learning;data mining	Robotics	-18.510969902803414	-4.7560317070981375	54011
02ca2c3a246ff1c290e2bfbabbef124816b5b60a	strategic decompositions of normal form games: zero-sum games and potential games		We study new classes of games, called zero-sum equivalent games and zero-sum equivalent potential games, and prove decomposition theorems involving these classes of games. We say that two games are “strategically equivalent” if, for every player, the payoff differences between two strategies (holding other players’ strategies fixed) are identical. A zero-sum equivalent game is a game that is strategically equivalent to a zero-sum game; a zero-sum equivalent potential game is a zero-sum equivalent game that is strategically equivalent to a common interest game. We also call a game “normalized” if the sum of one player’s payoffs, given the other players’ strategies, is always zero. We show that any normal form game can be uniquely decomposed into either (i) a zero-sum equivalent game and a normalized common interest game, or (ii) a zero-sum equivalent potential game, a normalized zero-sum game, and a normalized common interest game, each with distinctive equilibrium properties. For example, we show that two-player zero-sum equivalent games with finite strategy sets generically have a unique Nash equilibrium and that two-player zero-sum equivalent potential games with finite strategy sets generically have a strictly dominant Nash equilibrium.	list of code lyoko episodes;nash equilibrium	Sung-Ha Hwang;Luc Rey-Bellet	2016	CoRR		bondareva–shapley theorem;non-cooperative game;combinatorial game theory;bayesian game;game theory;mathematical optimization;example of a game without a value;simulation;best response;game tree;extensive-form game;simultaneous game;metagaming;repeated game;mathematics;stochastic game;strategy;screening game;normal-form game;mathematical economics;sequential game;symmetric game	ECom	-5.164241929894659	-1.3854165061547468	54023
461f6d44a1ad5974f80c3f26a55a3d869f336a4f	introspective reasoning in a case-based planner	introspective reasoning;case-based planner	Many current AI systems assume that the reasoning mechanisms used to manipulate their knowledge may be fixed ahead of time by the designer. This assumption may break down in complex domains. The focus of this research is developing a model of introspective reasoning and learning to enable a system to improve its own reasoning as well as its domain knowledge. Our model is based on the proposal of (Birnbaum et al. 1991) to use a model of the ideal behavior of a case-based system to judge system performance and to refine its reasoning mechanisms; it also draws on the research of (Ram & Cox 1994) on introspective failure-driven learning. This work examines introspection guided by expectation failures about reasoning performance. We are developing a vocabulary of failures for the case-based system, an introspective reasoner which uses a hierarchical model of system behavior, and a method of reusing CBR for parts of the case-based planner itself. The system we are developing combines a modelbased introspective reasoner with a case-based planning system. The planner generates high-level plans for navigating city streets, and is similar in structure to the planner CHEF (Hammond 1989). However, we implement components of the planner using the casebased reasoning mechanisms of the planner as a whole. Our primary interest in this approach is the advantage it offers for developing the model for introspective reasoning. We can reuse expectations that apply to the planner as a whole for its case-based parts. During the planning process, the introspective reasoner compares the planner’s reasoning to its assertions about ideal behavior. When a failure is detected, for instance if the system judges that the retrieved case is not the “best” case in memory, the introspective reasoner considers related assertions to pinpoint the source of the failure and to suggest a solution. In this case our system creates a new index to distinguish the true best case from the bad retrieved case. Determining what information to include in the model and how to structure it are central issues. Birnbaum’s model is a set of high-level assertions applicable to many case-based planners (Birnbaum et al. 199 1).	best, worst and average case;case-based reasoning;hierarchical database model;high- and low-level;introspection;random-access memory;semantic reasoner;vocabulary	Susan Fox;David B. Leake	1994			introspection;artificial intelligence;machine learning;planner;computer science;domain knowledge;reuse;semantic reasoner;hierarchical database model;vocabulary	AI	-18.773935900867762	1.9389099548591853	54233
30e3628f6b6b1fa7307ec26922015cfbea7bd018	belief in the opponents' future rationality	belief in future rationality;epistemic game theory;backwards rationalizability procedure;backward dominance procedure;dynamic games;backward induction	For dynamic games we consider the idea that a player, at every stage of the game, believes that his opponents will choose rationally in the future. Not only this, we also assume that players, throughout the game, believe that their opponents always believe that their opponents will choose rationally in the future, and so on. This leads to the concept of common belief in future rationality, which we formalize within an epistemic model. Our main contribution is to present an easy elimination procedure, backwards dominance, that selects exactly those strategies that can rationally be chosen under common belief in future rationality. The algorithm proceeds by successively eliminating strategies at every information set of the game. More specically, in round k of the procedure we eliminate at a given information set h those strategies for player i that are strictly dominated at some player i information set h0 weakly following h, given the opponentsstrategies that have survived at h0 until round k. Key words and phrases: Epistemic game theory, dynamic games, belief in future rationality, algorithms. Journal of Economic Literature Classication: C72 Contact information: Maastricht University, Department of Quantitative Economics P.O. Box 616, 6200 MD Maastricht, The Netherlands a.perea@maastrichtuniversity.nl Webpage: http://www.personeel.unimaas.nl/a.perea/	algorithm;game theory;geforce 6 series;rationality;web page	Andrés Perea	2014	Games and Economic Behavior	10.1016/j.geb.2013.11.008	economics;mathematics;mathematical economics;welfare economics;backward induction	AI	-7.515992244007685	-3.88730284346969	54524
0bafdc8a759732a6fcaa3a9878fad3fa55a4c71e	reactive synthesis without regret		Two-player zero-sum games of infinite duration and their quantitative versions are used in verification to model the interaction between a controller (Eve) and its environment (Adam). The question usually addressed is that of the existence (and computability) of a strategy for Eve that can maximize her payoff against any strategy of Adam. In this work, we are interested in strategies of Eve that minimize her regret, i.e. strategies that minimize the difference between her actual payoff and the payoff she could have achieved if she had known the strategy of Adam in advance. We give algorithms to compute the strategies of Eve that ensure minimal regret against an adversary whose choice of strategy is (1) unrestricted, (2) limited to positional strategies, or (3) limited to word strategies, and show that the two last cases have natural modelling applications. These results apply for quantitative games defined with the classical payoff functions $$\mathsf {Inf}$$ Inf , $$\mathsf {Sup}$$ Sup , $${\mathsf {LimInf}}$$ LimInf , $$\mathsf {LimSup}$$ LimSup , and mean-payoff. We also show that our notion of regret minimization in which Adam is limited to word strategies generalizes the notion of good for games introduced by Henzinger and Piterman, and is related to the notion of determinization by pruning due to Aminof, Kupferman and Lampert.	adversary (cryptography);algorithm;computability;eve;powerset construction;regret (decision theory)	Paul Hunter;Guillermo A. Pérez;Jean-François Raskin	2016	Acta Informatica	10.1007/s00236-016-0268-z	artificial intelligence;mathematics;mathematical economics;algorithm	Logic	-5.488283634280684	3.4935996504624764	54778
6ebba3c9b23efb81c41193b2e4900d7c47a4c63e	necessary and sufficient conditions for the strategyproofness of irresolute social choice functions	gibbard satterthwaite theorem;satisfiability;strategyproofness;necessary and sufficient condition;social choice;social choice theory;preference extensions	While the Gibbard-Satterthwaite theorem states that every non-dictatorial and resolute, i.e., single-valued, social choice function is manipulable, it was recently shown that a number of appealing irresolute Condorcet extensions are strategyproof according to Kelly's preference extension. In this paper, we study whether these results carry over to stronger preference extensions due to Fishburn and Gärdenfors. For both preference extensions, we provide sufficient conditions for strategyproofness and identify social choice functions that satisfy these conditions, answering a question by Gärdenfors [15] in the affirmative. We also show that some more discriminatory social choice functions fail to satisfy necessary conditions for strategyproofness.	kelly criterion	Felix Brandt;Markus Brill	2011		10.1145/2000378.2000394	social choice theory;economics;mathematics;microeconomics;mathematical economics;welfare economics	AI	-6.720574699187146	-1.549219036153724	54820
16c65065275e7a70cef01f288d8c33943ada5ba1	a dialogue game protocol for multi-agent argument over proposals for action	protocols;argumentation;course of action;practical reasoning;denotational semantic;bdi agents;dialogue games	We present the syntax and semantics for a multi-agent dialogue game protocol which permits argument over proposals for action. The protocol, called the Persuasive Argument for Multiple Agents (PARMA) Protocol, embodies an earlier theory by the authors of persuasion over action which enables participants to rationally propose, attack, and defend, an action or course of actions (or inaction). We present an outline of both an axiomatic and a denotational semantics, and discuss implementation of the protocol, in the context of both human and artificial agents.	autonomous robot;communications protocol;denotational semantics;intelligent agent;multi-agent system;situation calculus;software agent	Katie Atkinson;Trevor J. M. Bench-Capon;Peter McBurney	2005	Autonomous Agents and Multi-Agent Systems	10.1007/s10458-005-1166-x	communications protocol;practical reason;computer science;artificial intelligence;algorithm	AI	-17.401574034631068	4.105180877260543	54898
509af5d424923cb5f945ce2ccd61d7a1ee9e841f	algebraic reasoning about reactions: discovery of conserved properties in particle physics	reactions;machine discovery;model building;particle physics;algebraic reasoning	Kocabas (1991) describes a situation from particle physics in which quantum properties and conservation laws are postulated from lists of observed and unobserved reactions. Kocabas also presents a program named BR-3 that can rediscover some accepted quantum properties from textbook data, although it fails on a more difficult example from the same source. This paper describes PAULI, a program that solves the same task as BR-3 but uses a different problem-solving model. PAULI produces different, simpler solutions than does BR-3, and it can also handle the problematic example. After comparing the two programs, we conclude that PAULI offers distinct advantages over its predecessor, which we attribute to analgebraic approach to reasoning about sets of reactions.	problem solving	Raúl E. Valdés-Pérez	1994	Machine Learning	10.1007/BF00993864	model building;chemical reaction;computer science;artificial intelligence;machine learning;pure mathematics;mathematics;conservation law	AI	-16.763543180734	1.9917116240501536	55066
003d74759168f027165ed95e1cee227654d5cbcc	uncertain spatio-temporal reasoning for distributed transportation scheduling problem	spatial reasoning;temporal constraints;scheduling algorithm;distributed artificial intelligence;characteristic function;scheduling problem;situated multi agent system;temporal reasoning	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	agent-based model;algorithm;contract net protocol;francis;high-level programming language;online and offline;primary source;scheduling (computing);shortest path problem;situated;spatial–temporal reasoning	Maroua Bouzid	2007	Applied Artificial Intelligence	10.1080/08839510701492587	job shop scheduling;real-time computing;characteristic function;simulation;computer science;artificial intelligence;machine learning;spatial intelligence;reasoning system;scheduling	Robotics	-14.857911576421747	-6.2892993552646255	55145
03f8634f7f12ae7c8af0d92dde13fd9eefde7cd9	an agent based model of the nash demand game in regular lattices		In this work we propose an agent based model where a fixed finite population of tagged agents play iteratively the Nash demand game in a regular lattice. This work extends the multiagent bargaining model by [1] including the spatial dimension in the game. Each agent is endowed with memory and plays the best reply against the opponent's most frequent demand. The results show that all the possible persistent regimes of the global interaction game can also be obtained with this spatial version. Our preliminary analysis also suggests that the topological distribution of the agents can generate new persistent regimes within groups of agents with the same tag.	agent-based model;computer memory;nash equilibrium	David Poza;José Manuel Galán;José Ignacio Santos;Adolfo López-Paredes	2010		10.1007/978-3-642-14341-0_28	price of stability;epsilon-equilibrium;simulation;best response;economics;subgame;folk theorem;repeated game;chicken;microeconomics;normal-form game;mathematical economics;price of anarchy;nash equilibrium	ECom	-4.75130321526113	-2.2734405518144496	55372
471180a3eb822874f4b1dbe8ee9e346857b7fc50	coping with one-to-many multi-criteria negotiations in electronic markets	electronic commerce;electronic markets;multiple criteria;multi agent systems;negotiation support systems;consumer electronics procurement fuzzy sets information science decision making protocols proposals cost accounting information analysis;partial rank order family one to many multicriteria negotiations one to many multicriterion negotiations electronic markets preference modeling relational analysis;multi agent systems electronic commerce negotiation support systems	We are concerned with negotiators who negotiate on issues (products or services) characterized in terms of multiple criteria. In a typical round of the negotiation, the negotiator receives from various counter-parties responses or offers with specific values on the criteria that characterize the issue at stake. Clearly, the negotiator has certain preferences with respect to these offers. The analyis we present here allows us to reveal the dependencies among the various offers in light of the negotiator’s preferences. In addition, we are able to expose the dependencies among the criteria, as they are implied by the counter-parties’ offers. We make use of basic concepts from preference modeling and relational analysis to obtain a family of partial rank orders illustrating the dependencies among offers and criteria. We believe that, by using this information, the negotiator is better armed to identify the most suitable counter-party to negotiate with, or the criterion on which to focus in this negotiation.	algorithm;append;bilateral filter;computation;counterfactual conditional;decision support system;electronic markets;lateral thinking;multi-agent system;one-to-many (data model);request for quotation;semiconductor industry	Bartel Van de Walle;Sven Heitsch;Peyman Faratin	2001		10.1109/DEXA.2001.953147	e-commerce;computer science;knowledge management;artificial intelligence;multi-agent system;world wide web	ML	-8.852559722028731	-8.426056047145137	55381
e1013f35d89a267ceb9040cda18196b13f512beb	using strong isomorphisms to construct game strategy spaces	game theory;non cooperative games isomorphic probability distributions mixed strategy spaces;probability distribution;probability theory;parameter space;structure preservation;non cooperative game;simple game;likelihood function;fisher information;mixed strategy;c72 noncooperative games	When applied to the same game, probability theory and game theory can disagree on calculated values of the Fisher information, the log likelihood function, entropy gradients, the rank and Jacobian of variable transforms, and even the dimensionality and volume of the underlying probability parameter spaces. These differences arise as probability theory employs structure preserving isomorphic mappings when constructing strategy spaces to analyze games. In contrast, game theory uses weaker mappings which change some of the properties of the underlying probability distributions within the mixed strategy space. In this paper, we explore how using strong isomorphic mappings to define game strategy spaces can alter rational outcomes in simple games, and might resolve some of the paradoxes of game theory.	fisher information;game theory;gradient;jacobian matrix and determinant;samegame	Michael J. Gagen	2012	CoRR		bondareva–shapley theorem;non-cooperative game;combinatorial game theory;implementation theory;probability distribution;bayesian game;game theory;probability theory;minimax;positive political theory;mathematical optimization;combinatorics;example of a game without a value;best response;extensive-form game;fisher information;repeated game;mathematics;strategy;likelihood function;normal-form game;simulations and games in economics education;mathematical economics;parameter space;algorithmic game theory;sequential game;symmetric game;statistics	ECom	-8.15699624972792	0.42341444002572876	55488
1b2ca32beb4b1b3619456fae16b7dba83a962b79	argument relevance as the right kind of epistemic reason	central purpose;epistemic reason;argument relevance;probabilistic criterion;right kind;dialogue-relative understanding	A defense of the view that there is a central purpose in presenting an argument: to establish its conclusion, and so to secure belief in it. Relevance is then relevance to that purpose. To deem a statement relevant to an argument is to claim that it functions as a kind of epistemic reason pertaining to the cogency of an argument to establish its conclusion. Criticism of Walton's dialogue-relative understanding of relevance, as well as the probabilistic criterion of relevance or evidence.	relevance	Jonathan E. Adler	2012	J. Log. Comput.	10.1093/logcom/exp063	epistemology;artificial intelligence;mathematics	NLP	-14.887437045980286	1.1911384761947643	55523
cfb69c44cd71294aca88147edad888ec4496331b	a framework for electronic negotiations based on adjusted-winner mediation	matchmaking facility;electronic market mediation service;electronic commerce;adjusted winner mediation;internet negotiation support systems electronic commerce;procedural framework;utility function;electronic negotiations;adjusted winner procedure;electronic markets;electronic market setting;mutually beneficial agreements;internet;mediation consumer electronics laboratories game theory;negotiating agents;negotiation processes;market agents;negotiation support systems;market agents electronic negotiations adjusted winner mediation procedural framework electronic market mediation service adjusted winner procedure fair division mutually beneficial agreements negotiating agents electronic market setting binding offers mirroring utility functions mediation service matchmaking facility negotiation processes;mirroring utility functions;binding offers;mediation service;fair division	This paper presents the design of a procedural framework for an electronic market mediation service based on the adjusted-winner procedure for fair division. The main benefit of applying this procedure is that mutually beneficial agreements can be reached with minimal effort by the	adjusted winner procedure	Michael Ströbel	2000		10.1109/DEXA.2000.875152	e-commerce;fair division;the internet;party-directed mediation;adjusted winner procedure	AI	-8.529697788672022	-7.53604084235754	55687
27d4ace0e36b504d406a516490e5bab03059d175	the s-shaped utility function	normative;economie;comportement;utility;revolution;utilite;fonction;descriptivisme;utility function;psychology and economics;experimentalism;psychology;function;normativite;descriptivism;crime;experimentalisme;decision;psychologie;economy;loss aversion;economics;behavior;ethique professionnelle;human decision making;ideal	The results generated by experimentalists in psychology and economics haveled to numerous advances in the study of human decision making under risk.Camerer (1995) and Rabin (1998) provide excellent reviews of the relevantliterature. These results clearly display the gap between normative theoriesof ideal behavior and descriptive theories of observed behavior. The mostprominent result is loss aversion – the observation that a loss is given greatervalue than a gain of an equal size – and the resulting S-shaped utility function. Rabin puts the key point as follows: ``Researchers have identified a pervasive feature of reference dependence: In a wide variety of domains, people are significantly more averse to losses than they are attracted to same size gains''(Rabin 1998, 13–14). In what follows, I will show that the ``wide variety of domains'' is wide indeed. In particular, I will review the effects of the S-shaped utility function on the resolution of the decision to commit a crime, to decision to violate professional ethics, and the decision to participate in a rebellion.	decision theory;pervasive informatics;risk aversion;utility	Raymond Dacey	2003	Synthese	10.1023/A:1023465024536	ideal;philosophy;epistemology;normative;utility;function;revolution;behavior	ML	-6.342609225990402	-6.782477644481144	55879
9f6d514eb457fc5a1b19a387c6ef072774dc0a1d	classical logical versus quantum conceptual thought: examples in economics, decision theory and concept theory	quantum physics;quantum mechanics;decision theory;probability theory;classical logic	Inspired by a quantum mechanical formalism to model concepts and their disjunctions and conjunctions, we put forward in this paper a specific hypothesis. Namely that within human thought two superposed layers can be distinguished: (i) a layer guided by an underlying classical deterministic process, giving rise to essentially logical thought and its indeterministic version modeled by classical probability theory; (ii) a layer guided by conceptual weights of different types, such as ‘typicality’, ‘membership’, ‘representativeness’, ‘similarity’, ‘applicability’, ‘preference’ or ‘utility’, giving rise to what we call ‘conceptual thought’, indeterministic in essence, but equally well, although very differently, organized than logical thought. A substantial part of the conceptual thought process can be modeled by quantum mechanical probabilistic structures. We consider examples of three specific domains of research where the effects of the presence of conceptual thought and its deviations from classical logical thought have been noticed and studied, i.e. economics, decision theory, and concept theories and which provide experimental evidence for our hypothesis.	decision theory;intersubjectivity;quantum mechanics;quantum superposition;semantics (computer science)	Diederik Aerts;Bart D'Hooghe	2009		10.1007/978-3-642-00834-4_12	probability theory;combinatorics;classical logic;decision theory;computer science;artificial intelligence;mathematics;algorithm;quantum mechanics;statistics	AI	-14.708739224149756	1.054153396834582	55887
24ead9654305cf84237c3d3b71607d954deb5814	changing the subject: on the subject of subjectivity	personal identity;necessary and sufficient condition	In this paper I shall attempt to argue for the simple view of personal identity. I shall first argue that we often do have warrant for our beliefs that we exist as continuing subjects of experience, and that these beliefs are justified independently of any reductionist analysis of what it means to be a person. This has two important implications that are relevant to the ongoing debate concerning the number of persons that are in existence throughout any duration in time: (1) the lack of logically or metaphysically necessary and sufficient conditions for distinguishing one person from another should imply neither that there is only one person nor that personhood is not individuative; and (2) the lack of such universally applicable identity criteria should not imply that the term ‘person’ is a folk term with no real application. In other words, lack of reductionist analysis does not entail lack of existence.	reductionism	Troy Catterson	2007	Synthese	10.1007/s11229-007-9250-1	personal identity;philosophy;epistemology	Vision	-13.282072128018553	3.87462275325431	55927
0edbe6e9824d90b542a0a41209edadc139d0ebda	public goods, participation constraints, and democracy: a possibility theorem	majority rule;possibility theorem;expected utility;330 wirtschaft;satisfiability;320 politik;public good provision;ex post efficiency;public goods;public goods provision;participation constraints;cost sharing;public good;willingness to pay;majority voting;individual rationality	It is well known that ex post e¢ cient mechanisms for the provision of indivisible public goods are not interim individually rational. However, the corresponding literature assumes that agents who veto a mechanism can enforce a situation in which the public good is never provided. This paper instead considers majority voting with uniform cost sharing as the relevant status quo. E¢ cient mechanisms may then exist, which also satisfy all agentsinterim participation constraints. In this case, ex post ine¢ cient voting mechanisms can be replaced by e¢ cient ones without reducing any individuals expected utility. Intuitively, agents with a low willingness to pay have to contribute more under majority rule than under an e¢ cient mechanism with a balanced budget. This possibility theorem is not universal in the sense of Schweizer (Games and Economic Behavior, 2005). Keywords: Public goods, ex post e¢ ciency, participation constraints, majority voting, possibility theorem. JEL classications: D02, D61, D71, H41.	expected utility hypothesis;indivisible	Hans Peter Grüner;Yukio Koriyama	2012	Games and Economic Behavior	10.1016/j.geb.2011.11.001	majority rule;public good;economics;public economics;microeconomics;mathematical economics;welfare economics	ECom	-7.047738304954075	-3.168976085821988	56106
e6ca83cdb7770906f1bcf17f4051f7cb8400914e	improving multi-agent negotiations using multi-objective pso algorithm	multi agent system;distributed agents;pso;multi agent systems;multi objective particle swarm optimization;high performance;negotiation	Negotiation over limited resources, as a way for the agents to reach agreement, is one of the significant topics in Multi-Agent Systems (MASs). Most of the models proposed for negotiation suffer from different limitations in the number of the negotiation parties and issues as well as some constraining assumptions such as availability of unlimited computational resources and complete information about the participants. In this paper we make an attempt to ease the limitations specified above by means of a distributive agent based mechanism underpinned by Multi-Objective Swarm Optimization (MOPSO), as a fast and effective learning technique to handle the complexity and dynamics of the real-world negotiations. The experimental results of the proposed method reveal its effectiveness and high performance in presence of limited computational resources and tough deadlines.	agent-based model;algorithm;computation;computational resource;effective fitness;env;fitness function;iteration;lateral thinking;num lock;pareto efficiency;particle swarm optimization	Ahmad Esmaeili;Nasser Mozayani	2010		10.1007/978-3-642-13480-7_11	simulation;computer science;artificial intelligence;multi-agent system;management science;negotiation	AI	-10.67972149180921	-8.61873545478831	56127
556585a0f8d66aea5887ff6b6d6488e25b5700e8	strategy-proof voting rules over multi-issue domains with restricted preferences	satisfiability;strategy proofness;lexicographic domains;multi issue domains;voting;voting rule;social choice;is strategy	In this paper, we characterize strategy-proof voting rules when the set of alternatives has a multi-issue structure, and the voters ’ preferences are represented by acyclic CP-nets that follow a common order over iss ue . Our main result is a simple full characterization of strategy-proof vo ting rules satisfying nonimposition for a very natural restriction on preferences in multi-issue domains: we show that if the preference domain is lexicographic, then a voting rule satisfying non-imposition is strategy-proof if and only if it can be decomposed into multiple strategy-proof local rules, one for each issue and each setting of the issues preceding it. We also obtain the following variant of Gi bbard-Satterthwaite: when there are at least two issues and each of the issues can ta ke at least two values, then there is no non-dictatorial strategy-proof vo ting rule that satisfies non-imposition, even when the domain of voters’ preference s is restricted to linear orders that are consistent with acyclic CP-nets followi ng a common order over issues. This impossibility result follows from either one of two more general new impossibility results we obtained.	cp system;directed acyclic graph;lexicographical order	Lirong Xia;Vincent Conitzer	2010		10.1007/978-3-642-17572-5_33	mathematical optimization;social choice theory;voting;economics;arrow's impossibility theorem;mathematics;cardinal voting systems;mathematical economics;preferential block voting;law;welfare economics;anti-plurality voting;condorcet method;satisfiability	ECom	-6.882958116289374	-1.9338700370915372	56142
8c3f02bcde2ba163169e7077de7aecc52ba5871a	combining databases with prioritized information	databases;prioritized data;combining databases	To solve a problem one may need to combine the knowledge of several different experts. It can happen that some of the claims of one or more experts may be in conflict with the claims of other experts. There may be several such points of conflict and any claim may be involved in several different such points of conflict. In that case, the user of the knowledge of experts may prefer a certain claim to another in one conflict-point without necessarily preferring that statement in another conflict-point. Our work constructs a framework within which the consequences of a set of such preferences (expressed as priorities among sets of statements) can be computed. We give four types of semantics for priorities, three of which are shown to be equivalent to one another. The fourth type of semantics for priorities is shown to be more cautious than the other three. In terms of these semantics for priorities, we give a function for combining knowledge from different sources such that the combined knowledge is conflict-free and satisfies all the priorities.	database	Shekhar Pradhan;Jack Minker;V. S. Subrahmanian	1995	Journal of Intelligent Information Systems	10.1007/BF00961654	computer science;knowledge management;data mining;database;management science	DB	-17.678203451825055	2.0691281919729096	56250
45d3890a5d354b31110508c683cffae84513d698	a behavioral model for linguistic uncertainty	behavior modeling;monotonic predicate;possibility measure;natural extension;satisfiability;upper probability;vagueness;imprecise probability;linguistic uncertainty;linguistic information;technology and engineering;numerical model;natural language;imprecise probabilities;prototype theory;plausibility ordering;probability measure;possibility distribution	"""The paper discusses the problem of modelling linguistic uncertainty, which is the uncertainty produced by statements in natural language. For example, the vague statement`Mary is young' produces uncertainty about Mary's age. We concentrate on simple aarmative statements of the typèsubject is predicate', where the predicate satisses a special condition called monotonicity. For this case, we model linguistic uncertainty in terms of upper probabilities , which are given a behavioural interpretation as betting rates. Possibility measures and probability measures are special types of upper probability measure. We evaluate Zadeh's suggestion that possibility measures should be used to model linguistic uncertainty and the Bayesian claim that probability measures should be used. Our main conclusion is that, when the predicate is monotonic, possibility measures are appropriate models for linguistic uncertainty. We also discuss several assessment strategies for constructing a numerical model. 1. Introduction Information is commonly transmitted through statements in natural language. The statement`Mary is young', for instance, provides partial information about Mary's age. If initially we know nothing about Mary's age and we subsequently hear that she is young, our knowledge about her age has increased, although we remain uncertain about her exact age. In this paper we are concerned with the information conveyed by statements in natural language, which we call linguistic information. Linguistic information often involves vague predicates such as`young'. We will restrict our discussion to simple aarmative statements of the typèsubject is predicate' or`subject satisses property'. The linguistic information`Mary is young' produces some uncertainty about Mary's age; it seems likely that Mary is younger than 30 years, but we cannot be certain about that. We will call this type of uncertainty linguistic uncertainty. In general, linguistic uncertainty is the uncertainty about a precisely deened quantity that is produced by linguistic information. The problem considered in this paper is how best to model linguistic uncertainty. In this problem, our uncertainty is due to a lack of knowledge, and not to some physical randomness or indeterminacy. Our model for linguistic uncertainty will therefore be epistemic rather than physical (see 42] for a discussion of this distinction). Why is this important? For one thing, linguistic uncertainty is very common and it is natural from a scientiic or a philosophical point of view to attempt to model it. Another reason is more practical. Human actions are very often based on linguistic information. In order to design systems that emulate, \mechanise"""" or improve what …"""	behavioral modeling;indeterminacy (philosophy);information;logico-linguistic modeling;mathematical model;natural language;randomness;uncertainty principle;vagueness	Peter Walley;Gert de Cooman	2001	Inf. Sci.	10.1016/S0020-0255(01)00090-1	behavioral modeling;imprecise probability;uncertainty analysis;probability measure;artificial intelligence;mathematics;natural language;prototype theory;algorithm;statistics;satisfiability	NLP	-16.104002466025477	0.546032446752953	56404
8b87d3990065f81bf3295ff25a84b9eb66828842	lying opportunities and incentives to lie: reference dependence versus reputation		Abstract Recent experiments on lying behavior show that the lying frequency in case of low outcomes increases in the ex-ante probability of high outcomes. This finding is in line with models consisting of internal lying costs and external reputation costs and rejects certain other models, but does not allow for a clean test of models with reference dependent loss aversion. To compare the explanatory power of reputation models and loss aversion models, we manipulate the ex-ante probability that lying is possible at all. We show that the reputation model predicts that the lying frequency decreases in the probability that lying is possible, while the loss aversion model predicts the opposite. Our experimental results strongly support the reputation model. From an applied perspective, our results suggest that reducing the probability that lying is possible may be counterproductive.		Eberhard Feess;Florian Kerzenmacher	2018	Games and Economic Behavior	10.1016/j.geb.2018.07.003	experimental economics;dishonesty;mathematical economics;economics;truth telling;explanatory power;loss aversion;microeconomics;lying;reputation;incentive	ECom	-5.449156101153278	-6.909267542268405	56592
0e5b5cfa489ad115738b9d2d275dfe35ea329729	on the support size of stable strategies in random games	game theory;normal distribution;evolutionary stable strategy;nash equilibria;evolutionary game;independent random variables;bimatrix game	In this paper we study the support sizes of evolutionary stable strategies (ESS) in random evolutionary games. We prove that, when the elements of the payoff matrix behave either as uniform, or normally distributed independent random variables, almost all ESS have support sizes o(n), where n is the number of possible types for a player. Our arguments are based exclusively on the severity of a stability property that the payoff submatrix indicated by the support of an ESS must satisfy. We then combine our normal–random result with a recent result of McLennan and Berg (2005), concerning the expected number of Nash Equilibria in normal–random bimatrix games, to show that the expected number of ESS is significantly smaller than the expected number of symmetric Nash equilibria of the underlying symmetric bimatrix game. JEL Classification Code: C7 – Game Theory and Bargaining Theory.	berg connector;game theory;nash equilibrium	Spyros C. Kontogiannis;Paul G. Spirakis	2007		10.1007/978-3-540-74871-7_14	implementation theory;game theory;minimax;mathematical optimization;example of a game without a value;best response;coordination game;extensive-form game;simultaneous game;repeated game;mathematics;evolutionarily stable strategy;strategy;chicken;normal-form game;mathematical economics;sequential game;welfare economics;equilibrium selection;symmetric game;nash equilibrium	ECom	-4.85545940811112	-1.1265483220000079	56628
9f6eb997df1f5e76b28d5251cb11a4318e5d1eba	the hotelling-downs model with runoff voting	free entry;nash equilibria;runoff system;downs;equilibrium;multiple equilibria	We consider the Hotelling-Downs model with n ≥ 2 office seeking candidates and runoff voting. We show that Nash equilibria in pure strategies always exist and that there are typically multiple equilibria, both convergent (all candidates are located at the median) and divergent (candidates locate at distinct positions), though only divergent equilibria are robust to free entry. Moreover, two-policy equilibria exist under any distribution of voters’ ideal policies, while equilibria with more than two policies exist generically but under restrictive conditions that we characterize. In this sense, our analysis suggests that two-policy equilibria are the most prominent outcomes.	nash equilibrium;typset and runoff	Sandro Brusco;Marcin Dziubinski;Jaideep Roy	2012	Games and Economic Behavior	10.1016/j.geb.2011.08.004	economics;mathematical economics;welfare economics;nash equilibrium	AI	-5.255130999544858	-4.458350468021876	56738
92de5607884d047b98b821e0cc5c4806654c9ed2	the core of multi-choice ntu games	core;necessary and sufficient condition;multi choice ntu games	We propose a necessary and sufficient condition for the existence of core to the context of multi-choice NTU games.	network interface device	Yan-An Hwang;Wen-Hwa Li	2005	Math. Meth. of OR	10.1007/s001860400398	core;mathematical optimization;mathematical economics	Logic	-5.95034248479269	-0.16436500138019566	56758
548acc908caef3f5a5b685b52c6566e5e9bb790b	on the validity of dempster-shafer theory	dempster shafer theory;cognition;semantics;artificial intelligence;bayesian methods;dst	We challenge the validity of Dempster-Shafer Theory by using an emblematic example to show that DS rule produces counter-intuitive result. Further analysis reveals that the result comes from a understanding of evidence pooling which goes against the common expectation of this process. Although DS theory has attracted some interest of the scientific community working in information fusion and artificial intelligence, its validity to solve practical problems is problematic, because it is not applicable to evidences combination in general, but only to a certain type situations which still need to be clearly identified.	artificial intelligence;case-based reasoning;flaw hypothesis methodology;information source	Jean Dezert;Pei Wang;Albena Tchamova	2012	2012 15th International Conference on Information Fusion		artificial intelligence;machine learning;data mining;mathematics	Robotics	-15.279693110677835	0.782538579652367	56956
aa05a5536dee1ef8f3c6a0e860ffdc4702b819d5	utility, revealed preferences theory, and strategic ambiguity in iterated games	game theory;transfer entropy;multilayer perceptrons;strategic behaviour;logic gates;decision theory;information theory	Iterated games, in which the same economic interaction is repeatedly played between the same agents, are an important framework for understanding the effectiveness of strategic choices over time. To date, very little work has applied information theory to the information sets used by agents in order to decide what action to take next in such strategic situations. This article looks at the mutual information between previous game states and an agent’s next action by introducing two new classes of games: “invertible games” and “cyclical games”. By explicitly expanding out the mutual information between past states and the next action we show under what circumstances the explicit values of the utility are irrelevant for iterated games and this is then related to revealed preferences theory of classical economics. These information measures are then applied to the Traveler’s Dilemma game and the Prisoner’s Dilemma game, the Prisoner’s Dilemma being invertible, to illustrate their use. In the Prisoner’s Dilemma, a novel connection is made between the computational principles of logic gates and both the structure of games and the agents’ decision strategies. This approach is applied to the cyclical game Matching Pennies to analyse the foundations of a behavioural ambiguity between two well studied strategies: “Tit-for-Tat” and “Win-Stay, Lose-Switch”.	information theory;iterated function;iteration;logic gate;mutual information;prisoner's dilemma;relevance;win–stay, lose–switch	Michael S Harré	2017	Entropy	10.3390/e19050201	combinatorial game theory;superrationality;game theory;decision theory;transfer entropy;logic gate;information theory;simultaneous game;artificial intelligence;game mechanics;repeated game;mathematics;screening game;normal-form game;mathematical economics;sequential game;deadlock;symmetric game;statistics	AI	-9.654813830061265	-1.846945251950768	57422
be3324c0ff359d1d1014be8b4962ecc46a40e62f	sensor fusion in integrated circuit fault diagnosis using a belief function model	d s evidence theory;belief function;integrated circuit;journal;information fusion;sensor fusion;fault diagnosis	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;function model;integrated circuit;primary source	Daqi Zhu;Wei Gu	2008	IJDSN	10.1080/15501320701260626	embedded system;fault indicator;computer science;artificial intelligence;stuck-at fault;integrated circuit;machine learning;fault model;sensor fusion	Robotics	-14.824030290252301	-5.355306268862716	57542
60a425f6324b1be784976a609cca0b5d773ed299	mixed nash equilibria in concurrent terminal-reward games	004;concurrent games randomized strategy nash equilibria undecidability	We study mixed-strategy Nash equilibria in multiplayer deterministic concurrent games played on graphs, with terminal-reward payoffs (that is, absorbing states with a value for each player). We show undecidability of the existence of a constrained Nash equilibrium (the constraint requiring that one player should have maximal payoff), with only three players and 0/1-rewards (i.e., reachability objectives). This has to be compared with the undecidability result by Ummels and Wojtczak for turn-based games which requires 14 players and general rewards. Our proof has various interesting consequences: (i) the undecidability of the existence of a Nash equilibrium with a constraint on the social welfare; (ii) the undecidability of the existence of an (unconstrained) Nash equilibrium in concurrent games with terminal-reward payoffs. 1998 ACM Subject Classification F.3.1 Specifying and Verifying and Reasoning about Programs, D.2.4 Software/Program Verification), G.3 Probability and statistics	formal specification;formal verification;maximal set;nash equilibrium;reachability;undecidable problem	Patricia Bouyer;Nicolas Markey;Daniel Stan	2014		10.4230/LIPIcs.FSTTCS.2014.351	price of stability;epsilon-equilibrium;discrete mathematics;best response;coordination game;computer science;mathematics;correlated equilibrium;risk dominance;mathematical economics;equilibrium selection;nash equilibrium	Theory	-5.463479214404346	3.118421660515774	57587
884bd72accc5501e74091367c3a33edb3e7aaacb	the sage handbook of gis and society, edited by timothy l. nyerges, helen couclelis, and robert mcmasterlondon, sage publications ltd., 2011, 576 pp., us$150 (hardcover), isbn 978-1-4129-4645-2		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;geographic information system;international standard book number;primary source	Ick Hoi Kim	2012	Annals of GIS	10.1080/19475683.2012.694262	regional science	Robotics	-15.184133375110559	-5.74060641811127	57719
dec28333adb3ace88ab5dcd6ebcfe8cc886467ff	fair allocation of indivisible goods: the two-agent case	institutional repositories;fedora;vital;vtls;ils	One must allocate a finite set of indivisible goods among two agents without monetary compensation. We impose Pareto-efficiency, anonymity, a weak notion of no-envy, a welfare lower bound based on each agent’s ranking of the subsets of goods, and a monotonicity property w.r.t. changes in preferences. We prove that there is a rule satisfying these axioms. If there are three goods, it is the only rule, together with one of its subcorrespondences, satisfying each fairness axiom and not discriminating between goods.	fairness measure;indivisible;pareto efficiency	Eve Ramaekers	2013	Social Choice and Welfare	10.1007/s00355-012-0684-0	economics;public economics;microeconomics;mathematical economics;welfare economics;commerce	ECom	-5.580080422344744	-2.7533526684443177	57789
55874f05c9d6eddf565af16ea1bb14bdade8634b	gender-based focal points	nationalekonomi;battle of the sexes;convention;gender;journal of economic literature;focal point;discrimination;coordination	The subjects behaved significantly more “hawkish” in an experimental battle of the sexes game when the co-player was a woman compared to when it was a man. Discrimination helped the parties to coordinate and increase the average earnings in the subject group of mixed sex, compared to the unisex groups.	focal (programming language)	Håkan J. Holm	2000	Games and Economic Behavior	10.1006/game.1998.0685	development economics;discrimination;management;battle of the sexes	ECom	-6.701085915457483	-6.636941551957341	57916
0633015006fd8088c9089a94848ef3f21ac3881c	quasy: quantitative synthesis tool	satisfiability;probability distribution;markov decision process	We present the tool Q UASY, a quantitative synthesis tool. Q UASY takes qualitative and quantitative specifications and automatic ally onstructs a system that satisfies the qualitative specification and optimizes the qu antitative specification, if such a system exists. The user can choose between a system that satisfies and optimi zes the specifications (a) under all possible environment behaviors or (b) under th most-likely environment behaviors given as a probability distribution on the po ssible input sequences. QUASY solves these two quantitative synthesis problems by reduct ion to instances of 2-player games and Markov Decision Processes (MDPs) with qua ntitative winning objectives. QUASY can also be seen as a game solver for quantitative games. Most notable, it can solve lexicographic mean-payoff games with 2 players, MDPs with meanpayoff objectives, and ergodic MDPs with mean-payoff parit y objectives.	emoticon;ergodicity;lexicographical order;markov chain;markov decision process;solver	Krishnendu Chatterjee;Thomas A. Henzinger;Barbara Jobstmann;Rohit Singh	2011		10.1007/978-3-642-19835-9_24	markov decision process;probability distribution;simulation;computer science;artificial intelligence;machine learning;mathematics;algorithm;satisfiability	Logic	-5.612434555800322	3.798694256886792	58013
9758d8290322060b76a8f9dac736a5afdff011a0	bootstrap techniques for signal processing	signal processing	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source;signal processing	Gutti J. Babu	2005	Technometrics	10.1198/tech.2005.s292	multidimensional signal processing;space-time adaptive processing;digital signal processing;mathematics	Robotics	-14.440252675539918	-5.731724769613792	58047
812fdcf3cb15f5f21259fe117ea12eccf163122b	a mathematical model for optimal decisions in a representative democracy		Direct democracy, where each voter casts one vote, fails when the average voter competence falls below 50%. This happens in noisy settings when voters have limited information. Representative democracy, where voters choose representatives to vote, can be an elixir in both these situations. We introduce a mathematical model for studying representative democracy, in particular understanding the parameters of a representative democracy that gives maximum decision making capability. Our main result states that under general and natural conditions, 1. for fixed voting cost, the optimal number of representatives is linear; 2. for polynomial cost, the optimal number of representatives is logarithmic.	elixir;ibm notes;mathematical model;polynomial;voter model	Malik Magdon-Ismail;Lirong Xia	2018			direct democracy;machine learning;artificial intelligence;mathematics;voting;representative democracy;special case;classifier (linguistics);logarithm;polynomial	ML	-8.99459626271324	-3.755058360470861	58121
1ee570d0beed71d6195bbc68afe742072ba51ae3	multi-agent negotiation model based-on argumentation in the context of e-commerce		In the e-commerce transactions, there are lots of commodities with the same name, but anyone of these commodities have certain attributes which differ itself from others. During the traditional process of multi-agent negotiation, only one commodity can be selected as the negotiation object from these commodities with same name, if buyer agent want to find an appropriate commodity, the flexibility and efficiency of multi-agent negotiation would be low. This paper studies the multi-agent negotiation model by argumentation for a group of commodities. It firstly defines all kinds of negotiation elements, then establishes a negotiation model based-on argumentation and describes the negotiation agreements and strategies, and finally an example would be presented for testifying the effects of this model.	blinding (cryptography);e-commerce;multi-agent system	Guorui Jiang;Yangwei Xu;Ying Liu	2011			knowledge management	AI	-9.041378107631116	-8.242405460863274	58150
81c274e5048d6f385f01906b254189d4c4099a92	belief control and intentionality	belief;compatibilism;deciding to believe;doxastic attitudes;doxastic voluntarism;intentionality;libertarianism;volitions;voluntary control	In this paper, I argue that the rejection of doxastic voluntarism is not as straightforward as its opponents take it to be. I begin with a critical examination of William Alston’s defense of involuntarism and then focus on the question of whether belief is intentional.	doxastic logic;intentionality;international telecommunications routes;introspection;is-a;machine perception;rejection sampling	Matthias Steup	2011	Synthese	10.1007/s11229-011-9919-3	epistemology	ML	-12.557618455193383	2.7612903380099962	58250
091d3f2f2479328765bf253a739e51e4879b7506	faith, unbelief and evil: a fragment of a dialogue	moral judgment;moral demand;ritual murder;unmoved mover;eternal damnation	The man who is isolated over against God is as such rejected by God. But to be this man can only be the choice of the Godless man himself. The witness of the Community of God to every individual man points in this direction: that this choice of the Godless is null and void, that he belongs to Jesus Christ from eternity and thus is not rejected, but rather chosen by God in Jesus Christ, that the reprobation which he deserves on the basis of his wrong choice is borne and removed by Jesus Christ; that on the basis of the true, the Divine choice he is chosen for eternal life with God. The promise of his election will determine him as a member of the Church to become himself a carrier of its witness to the whole world. And the revelation of his rejection can determine him only to believe in Jesus Christ as Him by whom it is borne and removed. (Fourth of Karl Barth’s “main theses” on God’s Election of Grace in his Dogmatic II/2)1	karl wiegers;rejection sampling;the witness	A. N. Prior	2011	Synthese	10.1007/s11229-011-9945-1	political ponerology;philosophy;epistemology	ECom	-12.107097348929162	2.8629016262094056	58336
04a18834f25e713372f2c66e4cee9e6226e801d6	causal transportability with limited experiments	experimental design;social and behavioral sciences;meta analysis;transportability;causal relations;transfer knowledge;life sciences;medicine and health sciences;completeness	We address the problem of transferring causal knowledge learned in one environment to another, potentially different environment, when only limited experiments may be conducted at the source. This generalizes the treatment of transportability introduced in [Pearl and Bareinboim, 2011; Bareinboim and Pearl, 2012b], which deals with transferring causal information when any experiment can be conducted at the source. Given that it is not always feasible to conduct certain controlled experiments, we consider the decision problem whether experiments on a selected subset Z of variables together with qualitative assumptions encoded in a diagram may render causal effects in the target environment computable from the available data. This problem, which we call z-transportability, reduces to ordinary transportability whenZ is all-inclusive, and, like the latter, can be given syntactic characterization using the do-calculus [Pearl, 1995; 2000]. This paper establishes a necessary and sufficient condition for causal effects in the target domain to be estimable from both the non-experimental information available and the limited experimental information transferred from the source. We further provides a complete algorithm for computing the transport formula, that is, a way of fusing experimental and observational information to synthesize an unbiased estimate of the desired causal relation.	algorithm;causal filter;causality;computable function;decision problem;diagram;experiment	Elias Bareinboim;Judea Pearl	2013			meta-analysis;completeness;computer science;artificial intelligence;machine learning;design of experiments;operations research;statistics	AI	-10.060427245169885	1.0132767726989034	58590
838a1b449b9160865ea433484d6f2282e8d081b1	computing equilibria in two-player timed games via turn-based finite games	timing game	We study two-player timed games where the objectives of the two players are not opposite. We focus on the standard notion of Nash equilibrium and propose a series of transformations that builds two finite turn-based games out of a timed game, with a precise correspondence between Nash equilibria in the original and in final games. This provides us with an algorithm to compute Nash equilibria in two-player timed games for large classes of properties.	algorithm;automata theory;computation;modality (human–computer interaction);nash equilibrium;reachability;tree automaton	Patricia Bouyer;Romain Brenguier;Nicolas Markey	2010		10.1007/978-3-642-15297-9_7	price of stability;combinatorial game theory;game theory;epsilon-equilibrium;simulation;best response;coordination game;turns, rounds and time-keeping systems in games;computer science;folk theorem;repeated game;distributed computing;normal-form game;mathematical economics;equilibrium selection	Logic	-5.4805341895262325	3.133183322920967	58693
7b4dd1d36884b8a28e1d2b2d0704eb429ca031db	a constraint-based nurse rostering system using a redundant modeling approach	constraint propagation;medical administrative data processing;planning artificial intelligence;soft constraints;testing constraint based nurse rostering system redundant modeling approach system design system implementation timetables work shifts hospital rules nurse preference rules disjunction search tree constraint propagation search time;search trees;design and implementation;personnel;scheduling;nurse rostering;constraint handling;tree searching;hospitals humans computer science design engineering system testing process planning;tree searching constraint handling medical administrative data processing human resource management personnel scheduling planning artificial intelligence;human resource management	This paper describes the design and implementation of a nurse rostering system using a redundant modeling approach. Nurse rostering is defined as a process of generating timetables for specifying the work shifts of nurses over a given period of time. This process is difficult because the human roster planner has to ensure that every rostering decision made complies with a mixture of hard hospital rules and soft nurse preference rules. Moreover, some nurse shift pre-assignments often break the regularity of wanted (or un wanted) shifts and reduce the choices for other unfilled slot s. Soft constraints amount to disjunction, which can be modeled as choices in the search tree. This approach, although straightforward, incurs overhead in the search of solution . We propose redundant modeling, an effective way to speed up constraint propagation through cooperations among different models for the same problem, as a means to reduce search time. Experiments and pilot testing of the system confirm the feasibility of our method.	local consistency;nurse scheduling problem;overhead (computing);schedule;search tree;software propagation	B. M. W. Cheng;Jimmy Ho-Man Lee;J. C. K. Wu	1996		10.1109/TAI.1996.560444	nurse scheduling problem;simulation;computer science;artificial intelligence;human resource management;scheduling;local consistency	AI	-19.10751302099195	-5.728728586209336	58717
63c3995994f1a3d5355ff696e7a79f89c839abda	lattices for studying monotonicity of bayesian networks	bayesian network	In many real problem domains, the main variable of interest behaves monotonically in terms of the observable variables, in the sense that higher values for the variable of interest become more likely with higher-ordered observations. Unfortunately, establishing whether or not a Bayesian network exhibits these monotonicity properties is highly intractable in general. In this paper, we present a method that, by building upon the concept of assignment lattice, provides for identifying any violations of the properties of (partial) monotonicity of the output and for constructing minimal offending contexts. We illustrate the application of our method with a real Bayesian network in veterinary science.	bayesian network;observable variable;problem domain	Linda C. van der Gaag;Silja Renooij;Petra L. Geenen	2006			polymer;pattern recognition;antistatic agent;artificial intelligence;semiconductor;lattice (order);coating;computer science;electrical conductor;polyaniline	AI	-16.37950553839195	0.7975463826187875	58750
63789f3de5413cad01c9504693f8d23d9c83b329	a study on multi-agent based resource allocation mechanism for automated enterprise contracting	pareto optimisation;emerging market;market model;agent based;resource allocation;microeconomics;economic theory;multi agent systems;resource management virtual manufacturing distributed computing microeconomics dynamic programming pareto analysis computer simulation internet systems engineering and theory mechanical engineering;resource allocation microeconomics multi agent systems pareto optimisation;pareto optimal solution;agent contracting multiagent resource allocation mechanism automated enterprise contracting enterprise negotiations unbounded self interested agents economic theory agent mediated pareto optimal allocation virtual market model;computer simulation;pareto optimality	A study on multi-agent based resource allocation mechanism for automated enterprise contracting. In this study we focus on automated contracting among enterprise negotiations with unbounded self-interested agents based on economic theory. Each enterprise is modelled as agent, and we consider agent-mediated Pareto optimal allocation of resources through market mechanism. We formulate virtual market model as a discrete resource allocation problem in dynamic situations, and demonstrate the applicability of the market concept with multi-agent paradigm to this framework. In this paper we clarify the proposed mechanism successfully calculates Pareto optimal solutions for the resource allocation in the agent contracting by comparing our method with conventional analytic approaches. Additionally we apply the mechanism into dynamic market environment, and analyse the rationality of the emerged market by computer simulation	agent-based model;algorithm;computer simulation;cyberspace;electronic trading;fixed-point iteration;interaction;mathematical optimization;multi-agent system;pareto efficiency;programming paradigm;rationality	Toshiya Kaihara;Susumu Fujii	2006	2006 IEEE Conference on Emerging Technologies and Factory Automation	10.1109/ETFA.2006.355360	computer simulation;bayesian efficiency;resource allocation;computer science;artificial intelligence;management science;emerging markets	AI	-8.7639075857446	-7.068404793357859	58761
8f07cebcbb30a9a225dce68f7941911887364819	an inspiration for solving grid resource management problems using multiple economic models	economic models;grid;domain of strength;optimization	Economic models can motivate resource providers to share resources across multiple administrations in Grid computing. Our survey on existing economic models in Grid computing identified that different economic models are suitable for different scenarios. In this paper, we conduct an experiment to quantify the strengths and weaknesses of widely proposed economic models in the Grid - Commodity Market, Continuous Double Auction, English Auction, Contract-Net-Protocol and Bargaining. Based on this experimental analysis, we identify regions where a particular economic model outperforms others. Then, we indicate that switching between the economic models could be used to maximize benefits in a specific scenario.		S. M. Aminul Haque;Saadat M. Alhashmi;Rajendran Parthiban	2011		10.1007/978-3-642-28675-9_1	economics;computer science;marketing;economic model;operations management;management science;grid;welfare economics	HPC	-6.132652072970315	-8.320333091076332	58800
116a70da27e2492c5d30dbd77d4885d7641212f8	bounds on the cost of stabilizing a cooperative game				Yoram Bachrach;Edith Elkind;Enrico Malizia;Reshef Meir;Dmitrii V. Pasechnik;Jeffrey S. Rosenschein;Jörg Rothe;Michael Zuckerman	2018	J. Artif. Intell. Res.	10.1613/jair.1.11270		HCI	-6.166504328171694	1.1143871145206177	58890
293af2dc96ffed5435051e0622d6991411690da9	fairness through awareness	monopolist pricing;computational complexity;affirmative action;computers and society;differential privacy;mechanism design;facility location	"""We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of """"fair affirmative action,"""" which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness."""	algorithm;differential privacy;fairness measure	Cynthia Dwork;Moritz Hardt;Toniann Pitassi;Omer Reingold;Richard S. Zemel	2012		10.1145/2090236.2090255	mechanism design;actuarial science;computer science;facility location problem;mathematics;computational complexity theory;differential privacy;algorithm	ML	-6.655500704532716	-4.409732946503412	59068
4982147befcd429803e20e6e5756f5a9757ce292	an algebraic approach for computing equilibria of a subclass of finite normal form games	algebraic approach;game theory;nash equilibrium;galois group;nash equilibria;grobner bases;polynomial algebra;algebraic method;solution concept;normal form game	A Nash equilibrium has become important solution concept for analyzing the decision making in Game theory. In this paper, we consider the problem of computing Nash equilibria of a subclass of generic finite normal form games. We define rational payoff irrational equilibria games to be the games with all rational payoffs and all irrational equilibria. We present a purely algebraic method for computing all Nash equilibria of these games that uses knowledge of Galois groups. Some results, showing properties of the class of games, and an example to show working of the method concludes the paper.	algorithm;game theory;graphical user interface;handy board;linear algebra;nash equilibrium	Samaresh Chatterji;Ratnik Gandhi	2010	CoRR		price of stability;implementation theory;game theory;epsilon-equilibrium;combinatorics;discrete mathematics;best response;trembling hand perfect equilibrium;coordination game;economics;folk theorem;repeated game;mathematics;correlated equilibrium;microeconomics;risk dominance;normal-form game;mathematical economics;equilibrium selection;symmetric game;nash equilibrium	AI	-5.431431023077692	-1.0605058026962944	59132
1e68f199d17d6d7c4f4dd4335874914252699f59	assertion, uniqueness and epistemic hypocrisy	b philosophy general	Engel (Grazer Philos Stud 77: 45–59, 2008) has insisted that a number of notable strategies for rejecting the knowledge norm of assertion are put forward on the basis of the wrong kinds of reasons. A central aim of this paper will be to establish the contrast point: I argue that one very familiar strategy for defending the knowledge norm of assertion—viz., that it is claimed to do better in various respects than its competitors (e.g. the justification and the truth norms)—relies on a presupposition that is shown to be ultimately under-motivated. That presupposition is the uniqueness thesis—that there is a unique epistemic rule for assertion, and that such a rule will govern assertions uniformly. In particular, the strategy I shall take here will be to challenge the sufficiency leg of the knowledge norm in a way that at the same time counts against Williamson’s (Knowledge and its limits, 2000) own rationale for the uniqueness thesis. However, rather than to challenge the sufficiency leg of the knowledge norm via the familiar style of ‘expert opinion’ and, more generally, ‘second-hand knowledge’ cases (e.g. Lackey in Learning from words: testimony as a source of knowledge, 2008), a strategy that has recently been called into question by Benton (Philos Phenomenol Res, 2014), I’ll instead advance a very different line of argument against the sufficiency thesis, one which turns on a phenomenon I call epistemic hypocrisy.	assertion (software development);design rationale;existential quantification;viz: the computer game	J. Adam Carter	2015	Synthese	10.1007/s11229-015-0766-5	philosophy;epistemology;mathematics;algorithm	AI	-12.783108987673844	3.712452865902498	59341
63c9574692e97ffda08212970f3749fc4bc13a48	directed kernel density estimation (dkde) for time series visualization	incremental development;kernel density estimation kde;time series;directed kernel density estimation dkde;visual analysis;kernel density estimate;visual analytics;spatial statistics;dynamic behavior	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;kernel density estimation;primary source;time series	Jukka Matthias Krisp;Stefan Peters	2011	Annals of GIS	10.1080/19475683.2011.602218	kernel;kernel density estimation;econometrics;visual analytics;kernel embedding of distributions;computer science;machine learning;time series;iterative and incremental development;mathematics;spatial analysis;variable kernel density estimation;statistics	Robotics	-14.168169234218071	-5.862255432093456	59396
bad25628768ee78bd11755ddd9895864933c6bee	the dynamics of stable matchings and half-matchings for the stable marriage and roommates problems	stable matching;stable marriage;stable marriage problem;incremental algorithm;matching mechanism;stable roommates problem	We study the dynamics of stable marriage and stable roommates markets. Our main tool is the incremental algorithm of Roth and Vande Vate and its generalization by Tan and Hsueh. Beyond proposing alternative proofs for known results, we also generalize some of them to the nonbipartite case. In particular, we show that the lastcomer gets his best stable partner in both incremental algorithms. Consequently, we confirm that it is better to arrive later than earlier to a stable roommates market. We also prove that when the equilibrium is restored after the arrival of a new agent, some agents will be better off under any stable solution for the new market than at any stable solution for the original market. We also propose a procedure to find these agents.	approximation algorithm;blocking (computing);blum axioms;dynamic problem (algorithms);instability;many-to-many;matching (graph theory);os-tan;one-to-many (data model);realms of the haunting;stable marriage problem;terminate (software)	Péter Biró;Katarína Cechlárová;Tamás Fleiner	2008	Int. J. Game Theory	10.1007/s00182-007-0084-3	mathematical optimization;stable marriage problem;economics;mathematics;microeconomics;stable roommates problem;mathematical economics;welfare economics;algorithm	ECom	-6.255572439941484	-2.3652373020073965	59621
2f38c13adeae6bf2cd0b61111146e8f5c1da539d	how to decrease the degree of envy in allocations of indivisible goods		We consider the problem of fairly distributing a number of indivisible goods among agents with additive utility functions. Among the common criteria of fairness, we focus on envy-freeness and its weaker notions. Instead of concentrating on envy-free allocations which might not always exist, we seek to find an allocation with minimum envy. Based on a notion introduced by Chevaleyre et al. [7], we define several problems of minimizing the degree of envy and study their approximability.	indivisible	Trung Thanh Nguyen;Jörg Rothe	2013		10.1007/978-3-642-41575-3_21	mathematical economics	AI	-5.13271105456863	-0.7981070629048699	59975
cbc17f8d3e9c51235def72ddc6b1e9c67accf411	modelling market sellers in world of warcraft		The virtual economy operates like the one in the physical world, which is realized by players with two functions–demand and supply, only using the form as avatars. Only that in virtual worlds, the prices of virtual items is decided by the players instead of by its quality or scarcity, so the player behavior analyses became an important tool to monitor the persistent virtual worlds. In order to achieve the idea of a more balanced virtual market, we analyze the trading behaviors in three points – sellers, prices and items, and expect to figure out the selling motivation, pricing rules and the over/underpricing conditions. In this paper, we use the auction house data in World of Warcraft (WoW) to analyze these seller behaviors. After applying clustering and classification, a market seller model of WoW is presented.	avatar (computing);cluster analysis;virtual economy;virtual world;world of warcraft	Sheng-Yi Hsu;Chuen-Tsai Sun	2014			simulation;multimedia;computer science	ECom	-7.97093500603021	-7.9370013753050275	59979
7ad01c8b45c8bfe2ed87172113d09b2ae4dc0b8d	too much of a good thing?	natural resources;utility performance;design analysis	This paper explores a seemingly paradoxical phenomenon associated with the use of expected-utility theory in capital-budgeting and risk-sharing decisions under uncertainity. As an investment prospect becomes better and better, decision makers using classic decision-analysis techniques may, in fact, prefer less and less of it. We explore this phenomenon in the very real context of petroleum company drilling-investment decisions and demonstrate that the phenomenon is pervasive. When we examined the prospect inventory of a major oil company, we found that an increase in the upside payoff would lead to a lower optimal working interest for the grand majority of its prospects. We also explore the underlying factors leading to this phenomenon to develop both intuition and understanding. These factors have to do with degree of risk aversion (the more risk averse you are, the less likely you are to increase your holdings of an improving prospect) and managerial perspectives on risk. Once understood, the results no longer seem surprising.		Dana R. Clyman;Michael R. Walls;James S. Dyer	1998	The New York times on the Web	10.1287/opre.47.6.957	economics;operations management;natural resource;management	NLP	-5.202857814073854	-6.780763277783909	59998
63546995f5866298c96913d43b3555aeb44d7204	mixed equilibria are unstable in games of strategic complements	strategic complements;supermodular game;mixed equilibrium;learning in games;selected works;nash equilibria;strategic complementarity;supermodular games;strategic complementarities;journal of economic literature;bepress	In games with strict strategic complementarities, properly mixed Nash equilibria— equilibria that are not in pure strategies—are unstable for a broad class of learning dynamics. Journal of Economic Literature Classification Numbers: C72, C73.	complementarity theory;control theory;nash equilibrium	Federico Echenique;Aaron Edlin	2004	J. Economic Theory	10.1016/j.jet.2003.10.004	strategic complements;best response;economics;microeconomics;mathematical economics;welfare economics;nash equilibrium	ECom	-4.85850831968584	-1.5803189323336784	60198
5cdd4019c20d2348fe9b12c07b4c79921555a4b2	did frege believe frege's principle?	sinn;context principle;compositionality;bedeutung;holism;water table;contextuality	In this essay I will consider two theses that are associated with Frege, and will investigate the extent to which Frege “really” believed them. Much of what I have to say will come as no surprise to scholars of the historical Frege. But Frege is not only a historical figure; he also occupies a site on the philosophical landscape that has allowed his doctrines to seep into the subconscious water table. And scholars in a wide variety of different scholarly establishments then sip from these doctrines. I believe that some Frege-interested philosophers at various of these establishments might find my conclusions surprising. Some of these philosophical establishments have arisen from an educational milieu in which Frege is associated with some specific doctrine at the expense of not even being aware of other milieux where other specific doctrines are given sole prominence. The two theses which I will discuss illustrate this point. Each of them is called “Frege’s Principle,” but by philosophers from different milieux. By calling them “milieux” I do not want to convey the idea that they are each located at some specific socio-politico-geographico-temporal location. Rather, it is a matter of their each being located at different places on the intellectuallandscape. For this reason one might (and I sometimes will) call them “(interpretative) traditions.”	frege;frege's propositional calculus;holographic principle;milieu intérieur	Francis Jeffry Pelletier	2001	Journal of Logic, Language and Information	10.1023/A:1026594023292	water table;holism;kochen–specker theorem;philosophy;epistemology;computer science;artificial intelligence;mathematics;linguistics;algorithm;principle of compositionality	NLP	-12.824594499567224	3.1031969362452645	60249
4558c0af29e431cf8c227df002b6e0a92ff57acd	monotonicity paradoxes in the theory of elections		Abstract   An election procedure based on voter preference rankings is said to be monotonic if the alternative chosen by the procedure for any profile of voter preference rankings is also chosen after it is moved up in one or more of the profile's rankings. Several reasonable-sounding election procedures that are known to violate monotonicity are examined along with some new classes of non-monotonic procedures. Closely-related procedures that are monotonic are also identified. The procedural mechanisms and combinatorial structures that give rise to failures of monotonicity are analyzed in some detail.		Peter C. Fishburn	1982	Discrete Applied Mathematics	10.1016/0166-218X(82)90070-1	mathematics;algorithm	AI	-8.482063009947506	-2.060885432374548	60254
e81353a02510e74eb2fa70e7497c63f4ba526608	collaborative assignment using belief-desire-intention agent modeling and negotiation with speedup strategies	optimal solution;distributed agents;agent based;agent modeling;linear assignment problem;belief desire intention;resource allocation;collaborative linear assignment;objective function;distributed environment;distributed problem solving;belief desire intention bdi negotiation model;control strategy;heuristic algorithm;reasoning control	In this paper, we propose a distributed agent model that applies belief-desire-intention (BDI) reasoning and negotiation for addressing the linear assignment problem (LAP) collaboratively. In resource allocation, LAP is viewed as seeking a concurrent allocation of one different resource for every task to optimize a linear sum objective function. The proposed model provides a basic agent-based foundation needed for efficient resource allocation in a distributed environment. A distributed agent algorithm that has been developed based on the BDI negotiation model is examined both analytically and experimentally. To improve performance in terms of average negotiation speed and solution quality, two initialization heuristics and two different reasoning control strategies are applied, with the latter yielding different variants of the basic algorithm. Extensive simulations suggest that all the heuristic-algorithm combinations can produce a near optimal solution soon enough in some specific sense. The significance and applicability of the research work are also discussed.		Kiam Tian Seow;Kwang Mong Sim	2008	Inf. Sci.	10.1016/j.ins.2007.09.024	heuristic;mathematical optimization;resource allocation;computer science;knowledge management;artificial intelligence;assignment problem;management science;distributed computing environment	AI	-16.828769073271832	-9.309193182739648	60322
5079e41f4dd0a39c33dcf032b54ea009f80db44f	unfinkable dispositions	dispositions · finks · conditional analysis · ceteris paribus laws	This paper develops two ideas with respect to dispositional properties: (1) Adapting a suggestion of Sungho Choi, it appears the conceptual distinction between dispositional and categorical properties can be drawn in terms of susceptibility to finks and antidotes. Dispositional, but not categorical properties, are not susceptible to intrinsic finks, nor are they remediable by intrinsic antidotes. (2) If correct, this suggests the possibility that some dispositions—those which lack any causal basis—may be insusceptible to any fink or antidote. Since finks and antidotes are a major obstacle to a conditional analysis of dispositions, these dispositions that are unfinkable may be successfully analysed by the conditional analysis of dispositions. This result is of importance for those who think that the fundamental properties might be dispositions which lack any distinct causal basis, because it suggests that these properties, if they exist, can be analysed by simple conditionals and that they will not be subject to ceteris paribus laws.	causal filter;causality;fink	Toby Handfield	2006	Synthese	10.1007/s11229-006-9148-3		ML	-14.413809936907699	3.8145704840780685	60511
1075e010278ea8c368ddc127e20979ff694c58c1	simulating belief systems of autonomous agents	conflict detection;autonomous agent;belief revision;defeasible logic;distributed interactive simulation;defeasible reasoning;article;computer simulation;belief generation;belief simulation;knowledge base	A u t o n o m o u s agents in compute r s imulat ions do not have the usual mcchanisms to acquire informat ion as do thei r h u m a n counterpar t s . In many such simulations, it is not desirable tha t the agent have access to complete and correct informat ion about its envi ronment . We examine how impcrfcct ion in available informat ion may be s imula ted in the case of au tonomous agents. We de te rmine probabil ist ically what the agent may detect , th rough hypothet ical sensors, in a given si tuation. These detect ions are combined with the agent ' s knowledge base to infer observat ions and beliefs. Inhe ren t in this task is a degree of unccr ta in ty in choosing the most appropr ia te observat ion or belief. We describe and compare two approaches a numerical approach and one based on defeasible logic f o r s imulat ing an appropr ia te bel ief in light of conflicting detec t ion values at a given point in time. We discuss the appl icat ion of this t echnique to au tonomous forces in combat s imulat ion systems.	autonomous robot;decibel;defeasible logic;knowledge base;numerical analysis;sensor;simulation;ical	Hemant K. Bhargava;William C. Branley	1995	Decision Support Systems	10.1016/0167-9236(94)00036-R	computer simulation;knowledge base;computer science;knowledge management;artificial intelligence;autonomous agent;machine learning;belief revision;defeasible reasoning	AI	-17.95156840133649	3.727111247688836	60574
21fdbbda2c01776c4550a5916b1f92568e0284d8	agent-based air traffic control in airport airspace	particle systems;software prototype;air traffic control;multiagent system;multi agent system;software prototyping;agent based;open environment;multi agent systems;air traffic control airports application specific processors air safety routing software prototyping aircraft collision avoidance software safety traffic control;emergent behavior;airport airspace;zone assistant aircraft crews air traffic control airport airspace multiagent system software prototype;software prototyping air traffic control multi agent systems;zone assistant aircraft crews;organizational structure	The paper presents multi agent system approach and software prototype aimed at the air traffic control in airport airspace. The system consists of agents playing roles of assistant aircraft crews and assistant air traffic control operator of approach zone. According to selected organizational structure of multi agent system two different approaches to air traffic control are used. Within the approach zone coordination of agents' behavior is based on decisions made by assistant operator agent. Within the arrival zone assistant aircraft crews' agents make decision autonomously using a priory agreed behavior policy.	control flow;multi-agent system;prototype;software prototyping	Mariusz Nowostawski;Martin K. Purvis	2007	2007 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT'07)	10.1109/IAT.2007.23	free flight;simulation;engineering;automotive engineering;transport engineering;control zone	Robotics	-14.30409791653287	-9.055205126256809	60646
285c76435ea9765f2cb7ab10361a71282e666881	about the power to enforce and prevent consensus by manipulating communication rules	communication structure;mathematical analysis;curious agents;opinion dynamics;bounded confidence;balancing agents;continuous opinion dynamics	We explore the possibilities of enforcing and preventing consensus in continuous opinion dynamics that result from modifications in the communication rules. We refer to the model of Weisbuch and Deffuant, where n agents adjust their continuous opinions as a result of random pairwise encounters whenever their opinions differ not more than a given bound of confidence ε. A high ε leads to consensus, while a lower ε leads to a fragmentation into several opinion clusters. We drop the random encounter assumption and ask: How small may ε be such that consensus is still possible with a certain communication plan for the entire group? Mathematical analysis shows that ε may be significantly smaller than in the random pairwise case. On the other hand, we ask: How large may ε be such that preventing consensus is still possible? In answering this question, we prove Fortunato’s simulation result that consensus cannot be prevented for ε > 0.5 for large groups. Next, we consider opinion dynamics under different individual strategies and examine their power to increase the chances of consensus. One result is that balancing agents increase chances of consensus, especially if the agents are cautious in adapting their opinions. However, curious agents increase chances of consensus only if those agents are not cautious in adapting their opinions.	consensus (computer science);fork (software development);simulation	Jan Lorenz;Diemo Urbig	2007	Advances in Complex Systems	10.1142/S0219525907000982	computer science;artificial intelligence;data mining;mathematics;management science	ECom	-10.122029857182469	-6.214616799941485	60831
69d6d9b03cd82ee0b4917665afe13034c5eb724f	tracking beliefs and intentions in the werewolf game	computational pragmatics;intention and belief;belief change;situation calculus;dialogue modeling	We propose a model of belief and intention change over the course of a dialogue, in the case where the decisions taken during the dialogue affect the possibly conflicting goals of the agents involved. We use Situation Calculus to model the evolution of the world and an observation model to analyze the evolution of intentions and beliefs. In our formalization, utterances, that only change the beliefs and intentions, are observations. We illustrate our formalization with the game of Werewolf.	accessibility;deterministic algorithm;parsing;semantics (computer science);situation calculus	Codruta Gîrlea;Eyal Amir;Roxana Girju	2014			computer science;knowledge management;artificial intelligence;situation calculus	AI	-17.84432817389682	3.879900965964637	61041
26e6d80a783caff292bbb510a9e9cdd86b124b74	towards flexible negotiation in teamwork	complex dynamics;argumentation;collaboration;multi agent;decision theoretic;negotiation;teamwork	In a complex, dynamic multi -agent setting, coherent team actions are often jeopardized by agents' conflicting beliefs about different aspects of their environment, about resource availabilit y, and about their own or teammates' capabiliti es and performance. Team members thus need to communicate and negotiate to restore team coherence. This paper focuses on the problem of negotiations in teamwork to resolve such conflicts. The basis of such negotiations is inter-agent argumentation based on Toulmin's argumentation pattern. There are several novel aspects in our approach. First, our approach to argumentation exploits recently developed general, explicit teamwork models, which make it possible to provide a generalized and reusable argumentation facilit y based on teamwork constraints. Second, an emphasis on collaboration in argumentation leads to novel argumentation strategies geared towards benefiting the team rather than the individual. Third, our goal, to realize argumentation in practice in an agent team, has led to decision theoretic and pruning techniques to reduce argumentation overhead. Our approach is implemented in a system called CONSA.	argumentation framework;coherence (physics);overhead (computing);theory	Zhun Qiu;Milind Tambe;Hyuckchul Jung	1999		10.1145/301136.301263	simulation;complex dynamics;teamwork;knowledge management;negotiation;collaboration	AI	-17.995274362752184	-8.844991357602568	61133
db7f41d0ac765c0e31a044781e0eab3e033f80f9	dynamic bayesian description logics		It is well known that many artificial intelligence applications need to represent and reason with knowledge that is not fully certain. This has motivated the study of many knowledge representation formalisms that can effectively handle uncertainty, and in particular probabilistic description logics (DLs) [7–9]. Although these logics are encompassed under the same umbrella, they differ greatly in the way they interpret the probabilities (e.g. statistical vs. subjective), their probabilistic constructors (i.e., probabilistic axioms or probabilistic concepts and roles), their semantics, and even their probabilistic independence assumptions. A recent example of probabilistic DLs are the Bayesian DLs, which can express both logical and probabilistic dependencies between axioms [2–4]. One common feature among most of these probabilistic DLs is that they consider the uncertainty degree (i.e., the probability) of the different events to be fixed and static through time. However, this assumption is still too strong for many application scenarios. Consider for example a situation where a grid of sensors is collecting knowledge that is then fed into an ontology to reason about the situation of a large system. Since the sensors might perform an incorrect reading, this knowledge and the consequences derived from it can only be guaranteed to hold with some probability. However, the failure rate of a sensor is not static over time; as the sensor ages, its probability of failing increases. Moreover, the speed at which each sensor ages may also be influenced by other external factors like the weather at the place it is located, or the amount of use it is given. We propose to extend the formalism of Bayesian DLs to dynamic Bayesian DLs, in which the probabilities of the axioms to hold are updated over discrete time steps following the principles of dynamic Bayesian networks. Using this principle, we can not only reason about the probabilistic entailments at every point in time, but also reason about future events given some evidence at different times. This work presents the first steps towards probabilistic reasoning about complex events over time.	applications of artificial intelligence;description logic;dynamic bayesian network;failure rate;knowledge representation and reasoning;probabilistic automaton;semantics (computer science);sensor	Ismail Ilkan Ceylan;Rafael Peñaloza	2015			machine learning;description logic;t-norm fuzzy logics;knowledge representation and reasoning;logical conjunction;dynamic bayesian network;axiom;probabilistic logic;mathematics;artificial intelligence;bayesian probability	AI	-16.889688586149337	0.31288280145247166	61274
8496da1c2eeeccfa4e312542d169bc2a3189f696	a class of permutation tests for the equality of two marginal survival functions using paired censored data	propensity score matching;paired censored data;secondary 62g09;crossing survival functions;nonproportional hazard;primary 62n03;paired survival data;permutation test	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	censoring (statistics);francis;marginal model;primary source;resampling (statistics)	Liang Li	2014	Communications in Statistics - Simulation and Computation	10.1080/03610918.2012.755196	econometrics;resampling;propensity score matching;mathematics;statistics;paired difference test	Robotics	-14.133124533515529	-5.735918610865335	61310
f988afe0c8e5e2b500a3bf1f9b305fde2dcb6319	cognitive hierarchy and voting manipulation		By the Gibbard–Satterthwaite theorem, every reasonable voting rule for three or more alternatives is susceptible to manipulation: there exist elections where one or more voters can change the election outcome in their favour by unilaterally modifying their vote. When a given election admits several such voters, strategic voting becomes a game among potential manipulators: a manipulative vote that leads to a better outcome when other voters are truthful may lead to disastrous results when other voters choose to manipulate as well. We consider this situation from the perspective of a boundedly rational voter, and use the cognitive hierarchy framework (Camerer et al., 2004) to identify good strategies. We then investigate the associated algorithmic questions under the k-approval voting rule, k ≥ 1. We obtain positive algorithmic results for k = 1, 2 and NPand coNP-hardness results for k ≥ 4.	algorithm;co-np;cognition;existential quantification;time complexity	Edith Elkind;Umberto Grandi;Francesca Rossi;Arkadii M. Slinko	2017	CoRR		mathematics;cognitive hierarchy theory;voting;machine learning;artificial intelligence	AI	-8.352633773380154	-1.7192237648327098	61451
77b4ceb7e0c4fe5cae1c2af8919f0b77742e0888	logistics issues in autonomous food production systems for extended duration space exploration	quest software;space vehicles aerospace computing discrete event simulation software agents logistics data processing transportation;extended duration space exploration;transportation traffic flow space missions autonomous food production systems extended duration space exploration autonomous life support system program nasa intelligent agents contract net protocol approach transportation activity scheduling discrete event simulation model quest software;contract net protocol;discrete event simulation model;logistics data processing;contracts;contract net protocol approach;software agents;transport protocols;support system;intelligent agents;transportation traffic flow;logistics;aerospace computing;food products;transportation activity scheduling;robots;transportation;intelligent agent;autonomous food production systems;space missions;production systems;logistics production systems space missions transportation nasa intelligent agent contracts transport protocols discrete event simulation robots;nasa;autonomous life support system program;space vehicles;discrete event simulation	To enable longer space missions, systems for produc of food in space will be necessary. The Autonomo Life Support System (ALSS) program of NASA is a on-going research effort in this direction. This resea uses intelligent agents to relieve the crew of substan efforts relating to the food production tasks . In this paper, we propose a Contract Net Protocol approac schedule transportation activities within th environment. A discrete-event simulation model us QUEST software (by Deneb Robotics, Inc.) is used represent the flow of the transportation traffic within t system.	autonomous robot;contract net protocol;intelligent agent;list of discrete event simulation software;logistics;robotics	Reza Naghshineh-pour;Nicole Williams;Bala Ram	1999		10.1145/324898.325052	robot;logistics;transport;real-time computing;simulation;computer science;engineering;artificial intelligence;discrete event simulation;software agent;space exploration;production system;contract net protocol;intelligent agent;transport layer	AI	-14.3064419016074	-9.054676792394096	61706
6314415c10f54f43f2e8b2f753b123ff9822a998	periodic stopping games	subgame perfect equilibrium;stopping games;stochastic games;dynkin games;dynkin game	Stopping games (without simultaneous stopping) are sequential games in which at every stage one of the players is chosen, who decides whether to continue the interaction or stop it, whereby a terminal payoff vector is obtained. Periodic stopping games are stopping games in which both of the processes that define it, the payoff process as well as the process by which players are chosen, are periodic and do not depend on the past choices. We prove that every periodic stopping game without simultaneous stopping, has either periodic subgame perfect -equilibrium or a subgame perfect 0-equilibrium in pure strategies.	optimal stopping	Ayala Mashiah-Yaakovi	2009	Int. J. Game Theory	10.1007/s00182-008-0143-4	combinatorial game theory;markov perfect equilibrium;combinatorics;economics;stopping time;optional stopping theorem;mathematics;microeconomics;mathematical economics;subgame perfect equilibrium	Theory	-4.614973718611827	-1.8640554742936346	61838
751ae0ddcc45459a46d8a82d18ae1fc6bab39d33	what difference might and may make	humanidades;filosofia etica	How does your information change when you learn that something might be the case, where the modal “might” is epistemic? On the orthodox view, a proposition is added to your information base; on the view defended here, no propositions are added to your information base but some are removed from it. I (i) argue that Stephen Yablo’s recent attempt to define this removal operation as a kind of propositional subtraction fails, (ii) offer a definition of my own in terms of the part–whole relations between the truthmakers of the propositions one accepts, and (iii) argue that a deontic analogue of this account solves a problem about permission posed long ago by David Lewis.	deontic logic;modal logic	Gerhard Nuffer	2014	Synthese	10.1007/s11229-014-0576-1	philosophy;epistemology;mathematics;algorithm	Security	-12.685622349105278	4.03883549690095	61862
68769c25ace3569763fff4b1ba615b79f2b58dd5	noniterative manipulation of discrete energy-based models for image analysis	image restoration;posterior distribution;pattern recognition;image analysis;causal models	With emphasis on the graph structure of energy-based models devoted to image analysis, we investigate eecient procedures for sampling and inferring. We show that triangulated graphs, whom trees are simple instances of, always support causal models for which noniterative procedures can be devised to minimize the energy, to extract probabilistic descriptions, to sample from corresponding prior and posterior distributions, or to infer from local marginals. The relevance and eeciency of these procedures are illustrated for image restoration problems.iterative proc edures, discrete low-level image analysis (URA 227) Université de Rennes 1 – Insa de Rennes et en Automatique – unité de recherche de Rennes Manipulation non-it erative de mod eles energ etiques discrets pour l'analyse d'images R esum e : Nous adoptons ici un point de vue graphique pour explorer des mod eles energ etiques de manipulation peu co^ uteuse, pour l'analyse d'images. Il s'av ere que les mod eles dans lesquels les relations de d ependance directe forment un graphe triangul e (e.g., un arbre dans les cas les plus simples) admettent toujours une repr esentation causale. De ce fait, il est alors possible de calculer des probabilit es li ees au mod ele (en particulier les marginales locales), d' echantillonner ou maximiser de telles probabilit es, ou bien encore de minimiser l' energie globale, et ce de faa con non-it erative. De telles proc edures sont d ecrites en d etail dans le cas des arbres, et compar ees sur dii erents probl emes de restoration d'images. Noniterative manipulation of discrete energy-based models for image analysis 3 1 Introduction and general framework	bibliothèque de l'école des chartes;causal filter;circuit restoration;coded aperture;estdomains;high- and low-level;image analysis;image restoration;modulo operation;naruto shippuden: clash of ninja revolution 3;online shopping;relevance;sampling (signal processing);sensitivity index;vue	Patrick Pérez;Jean-Marc Laferté	1997		10.1007/3-540-62909-2_78	image restoration;computer vision;image analysis;computer science;machine learning;pattern recognition;mathematics;posterior probability;causal model	Vision	-16.283371415260202	-2.961647561165591	61876
92d1212d4555b300decf0ea45f3e447e631aa3e3	ubiquitous manufacturing: overview, framework and further research directions	ubiquitous manufacturing;interpretive structural modelling;ubiquitous computing;micmac	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Rameshwar Dubey;Angappa Gunasekaran;Anindya Chakrabarty	2017	Int. J. Computer Integrated Manufacturing	10.1080/0951192X.2014.1003411	computer science;systems engineering;engineering;knowledge management;management science;ubiquitous computing	Robotics	-15.48411645414485	-5.302152443156845	62022
fa2ba06a47c1fbf9cc00728f1e9e1c313672c971	the core and nucleolus in a model of information transferal	journal;期刊论文	Galdeano et al. introduced the so-called information market game involving n identical firms acquiring a new technology owned by an innovator. For this specific cooperative game, the nucleolus is determined through a characterization of the symmetrical part of the core. The nonemptiness of the symmetrical core is shown to be equivalent to one of each, super additivity, zeromonotonicity, or monotonicity.		Dongshuang Hou;Theo S. H. Driessen	2012	J. Applied Mathematics	10.1155/2012/379848	mathematics;mathematical economics	ECom	-5.798060805981155	-2.786025681224956	62221
32c66fb78b3b82de245fcde47dbc8af70b701ed5	rearing its ugly head: the cosmological constant and newton's greatest blunder		It is folklore that Albert Einstein’s greatest blunder occurred in 1917 when he introduced the cosmological constant term into his theory of general relativity [5]. At the time Einstein believed the universe to be static, and yet his original field equations, which describe how the gravitational influence of matter bends space-time, predicted an expanding (or contracting) one. To resolve this apparent contradiction and counteract the effects of gravity, Einstein introduced into his equations an all-encompassing force term scaled by a constant (originally Λ λ ) that has become known as the cosmological constant [19]. Then along came Edwin Hubble’s observation of galactic red shifts in 1929, which evidenced an expanding universe and made Einstein’s cosmological constant term unnecessary. Einstein thus realized he had missed a great opportunity to predict the expansion of the universe from his original theory, had he only believed in it. As recounted by the cosmologist George Gamow in his 1970 autobiography [4]:	constant term;newton;numerical relativity	Hieu D. Nguyen	2008	The American Mathematical Monthly		calculus;cosmological constant;mathematical analysis;mathematics	Theory	-10.166962914139074	2.8476875673094244	62408
63bbfc6fecb59ec046c4f42c434f11238cb7f99b	mechanical verification of a constructive proof for flp		The impossibility of distributed consensus with one faulty process is a result with important consequences for real world distributed systems e.g., commits in replicated databases. Since proofs are not immune to faults and even plausible proofs with a profound formalism can conclude wrong results, we validate the fundamental result named FLP after Fischer, Lynch and Paterson by using the interactive theorem prover Isabelle/HOL. We present a formalization of distributed systems and the aforementioned consensus problem. Our proof is based on Hagen Volzer’s paper A constructive proof for FLP. In addition to the enhanced confidence in the validity of Volzer’s proof, we contribute the missing gaps to show the correctness in Isabelle/HOL. We clarify the proof details and even prove fairness of the infinite execution that contradicts consensus. Our Isabelle formalization may serve as a starting point for similar proofs of properties of distributed systems.	correctness (computer science);fairness measure;field electron emission;hol (proof assistant);isabelle;precondition;weatherstar;windows fundamentals for legacy pcs	Benjamin Bisping;Paul-David Brodmann;Tim Jungnickel;Christina Rickmann;Henning Seidler;Anke Stüber;Arno Wilhelm-Weidner;Kirstin Peters;Uwe Nestmann	2016		10.1007/978-3-319-43144-4_7	discrete mathematics	Logic	-16.025127966087748	3.491427171037338	62452
90a8a080bc1501722f0ec4c494481060dfa1f509	consciousness and understanding in the chinese room	consciousness;semantics vs syntax;chinese room	"""In this paper I submit that the “Chinese room” argument rests on the assumption that understanding a sentence necessarily implies being conscious of its content. However, this assumption can be challenged by showing that two notions of consciousness come into play, one to be found in AI, the other in Searle’s argument, and that the former is an essential condition for the notion used by Searle. If Searle discards the first, he not only has trouble explaining how we can learn a language but finds the validity of his own argument in jeopardy. In the well-known “Chinese room argument,” John Searle argues against the idea that the process of understanding a language can be tantamount to mere manipulation of formal symbols. Over the years the argument, considered fatal against “strong Artificial Intelligence,” has provoked a number of objections (see commentary to Searle 1980; Carleton 1984; Rey 1986). Here I shall present another possible one. My hope is that this objection will shed some light on the relationship between understanding and consciousness. As I will argue, Searle’s position assumes that to understand a sentence one must necessarily be conscious of its content in a way I will specify later. So, the purpose of this paper, then, is, first, to differentiate understanding from being conscious of understanding and, second, to clarify the role that various notions of being conscious play in the argument. In this way I hope to show that in both cases Searle fails to make his point. Let’s start with the argument itself. Searle’s original purpose was to demonstrate that computer programs, however complex and accurate, will never be able to understand a language. Simplifying somewhat, Searle’s argument goes as follows: Searle, completely ignorant of Chinese, is locked into a room with a book of Chinese symbols and a book, in English, that explains how to combine and transform the Chinese symbols. Every now and then, sheets of paper with Chinese symbols written on them are slipped to him from under the door. His task is to give back a sheet of paper with Chinese symbols whenever he receives one. To do this, he compares the symbols on the incoming sheet with those on the Chinese symbols book, checks which rules are allowed for the occurring symbols, and transforms them accordingly. In this way, Searle transforms the symbols by means of a set of rules in a purely “formal” way, that is, by identifying the symbols just by looking at their shapes. Outside the room there is a Chinese person who is giving the sheets to Searle taking them to be “questions” and the sheets handed over by Searle to be “answers.” As a matter of fact, the Chinese person, a perfectly fluent native speaker, considers Searle’s answers to be adequate responses to the questions. Consequently he believes that inside the room there is somebody who understands Chinese, and grants in this way that Searle has passed the Turing test for Chinese. However, Searle’s comprehension of Chinese is not improved by his symbols processing. Hence, understanding a language is not equivalent to symbols processing, and the Turing test is not sufficient to determine understanding (Searle 1980). Against this argument a number of objections are possible. The most interesting one is the so-called “systems reply.” According to this objection, even if Searle himself does not understand Chinese, the entire system, that is, the books, the room, the execution of the functions, etc., does. Searle has a rebuttal to this line of thinking: even if he memorizes all the contents of the books and the rules, and walks around uttering Chinese, he still wouldn’t be a Chinese speaker, insofar as he would not understand Chinese. Let’s inspect the main argument and this rebuttal more closely. Consider the main argument from Searle’s point of view. It goes as follows (1): i) I’m manipulating formal symbols for Chinese ii) I do not understand Chinese iii) The manipulation of formal symbols is not equivalent to understanding This very argument has been transposed, by Searle himself, also in terms of syntax versus semantics (Searle 1990). The idea is that we may substitute “manipulation of symbols"""" with “syntax"""" and “understanding"""" with “semantics"""". Here is the new version of the argument: i) I’m doing syntax for Chinese ii) I’m not semantically competent with Chinese iii) Syntax is not sufficient for semantic competence Searle’s conclusion, in this new formulation, is that syntax is not sufficient for “taking care” of semantics (Haugeland 1981). Now, how does all this demonstrate the insufficiency of the Turing test as a test for linguistic competence and, hence, for understanding? We saw that, according to Searle, purely syntactical manipulation is sufficient for passing the test. So, in order to pass the Turing test it is not necessary to have semantics, that is, to have an intentional mind. Now, consider the problem of being a judge for a Turing test. The judge is a normal human being that, at the end of the test, supposes that there must be a competent Chinese speaker inside the room or, in the systems reply, that Searle is a competent Chinese speaker. On what basis could the judge evaluate the adequacy of the responses by Searle? Given that he evaluates not only the syntactical correctness of the responses, but also -and primarily -their semantical adequacy to the questions, his judgments must be grounded in a semantical basis too. But since what Searle is doing is nothing but symbol manipulation, we conclude that syntax is sufficient for semantics. Therefore, in order to show that syntax is not sufficient for semantics, Searle has to suppose that syntax is sufficient for semantics. How is such a simple rebuttal of Searle’s argument possible? It seems to me that two different notions of semantics are at stake. >On one side, say from the Turing test perspective, Searle may manage reference for instance, he may perform correctly on questions like “could you indicate a red jacket?” and truth correctly replying to a list of true/false questions. On the other side, say from the first person perspective of Searle himself, he does not know what he is doing; he cannot, as it were, “inspect” the contents of its own utterances. Analogously, since semantics was intended as a substitute for understanding, we have two notions of understanding either. >In one case, that of the Turing test, Searle does understand; in the other, that of the first person perspective, Searle does not understand what he is saying or doing. This second notion, however, is not the notion of understanding per se, but the notion of being conscious of understanding. Searle’s argument rests on the idea that understanding a language necessarily implies being conscious of the contents of utterances or mental states. It is Searle’s task to show that understanding necessarily implies consciousness. Recently Searle has argued exactly along these lines, claiming that we must be able to be conscious of all the mental contents we have, at least in principle (Searle 1992). Now, what relationship between understanding and consciousness may we have in AI? Consider the case of AI program SHRDLU. SHRDLU simulates a robot arm which can move a number of solids, such as cubes, pyramids and spheres, in a fictional world, that is, in a world completely generated by the computer itself (Winograd 1972). A human being gives SHRDLU commands such as “pick up a pyramid and put it over a big blue block,” and SHRDLU reports what it is doing and why. Since SHRDLU may report what its final task is, and what the relevant steps it has to perform to accomplish the task are, I submit that it is conscious of what it is doing in the very simple sense that it is able to keep records of its own steps. This should not be considered a trivial matter: for instance, sometimes we are completely unable to describe how we perform certain actions or what the basic elements of certain skills are. The situation with Searle’s reports, in the reply system response, is substantially the same. Searle may be conscious \it of \rm manipulating a certain symbol, i.e., the same symbol he used yesterday, even if he cannot be conscious \it that \rm he is manipulating a certain symbol, that is, he does not know what the symbol means (2). The very fact that Searle may report his own activity on all this symbols’ manipulation corresponds to being conscious of. One may argue that Searle’s reporting activity actually is, again, symbols’ manipulation, so that no level of consciousness could be reached through this activity. I disagree. When Searle reports his activity about symbols, he is using the symbols to refer to the symbols themselves. To be adequate to the task, Searle has to differentiate between an object-language and a metalanguage, and it is exactly this feature that defines the kind of consciousness I am discussing. In the case of Searle’s native language understanding, on the other hand, the reports would be conscious reports in another sense. Specifically, Searle would be conscious that the content of the proposition he has in mind or has pronounced is such and such. The that clause gives to the report the intentional character Searle considers proper to the domain of real conscious understanding. In this way understanding, an intentional notion, is explained exclusively in terms of conscious understanding or, more specifically, of being conscious that. The problem here is that the distinction between being conscious of p and being conscious that p is not taken into consideration in Searle's use of the notion of consciousness. On the contrary, Searle’s view seems to be committed exclusively with a notion of consciousness as a sort of “certainty” about what is going on in one’s own mind, that is, only with the consciousness that. Considering the way in which we learn a language, this position could be disputed. Suppose it is your first French class. The teacher tells you th"""	altered level of consciousness;artificial intelligence;book;chinese room;computational theory of mind;computer program;convergence insufficiency;coppersmith–winograd algorithm;correctness (computer science);first-person (video games);mental state;natural language understanding;olap cube;object language;pyramid (geometry);report;robotic arm;shrdlu;turing test;whole earth 'lectronic link	Simone Gozzano	1995	Informatica (Slovenia)		natural language processing;computer science;artificial intelligence;consciousness	AI	-12.106879526042285	2.4925992861093533	62551
38c8b1f80390d25844e9edafcc37a8ae76505daf	the logic of conflicts between decision making agents	datavetenskap datalogi;computer science	We present a formal model for the analysis of conflicts in sets of autonomous agents restricted in the sense that they can be described in a (first-order) language and by a transaction mechanism. In this model, we allow for enrichment of agent systems with correspondence assertions, expressing the relationship between different entities in the formal specifications of the agents. Thereafter the specifications are analysed with respect to conflicts. If two specifications are free of conflicts, the formulae of one specification together with the set of correspondence assertions do not restrict the models of the other specification, i.e. the agent system does not restrict the individual agents. The approach takes into account static as well as dynamic aspects of this kind of interaction. Classifications of complexity of determining whether two specifications are free of conflicts are also presented. Furthermore, if the agents are allowed to act in accordance with the result of executions of a decision module, a situation may occur where, for example, subsets of their possible goal sets are consistent, but in actual fact the individual agents may nevertheless always terminate in states that are in conflict. Therefore, the model is also enriched by processes for analysing when specifications are compatible with respect to states for which it is reasonable to assume that they eventually will be reached.	autonomous robot;complexity;database;entity;first-order predicate;formal language;gene ontology term enrichment;terminate (software)	Love Ekenberg	2000	J. Log. Comput.	10.1093/logcom/10.4.583	optimal decision;decision analysis;computer science;artificial intelligence;decision tree;data mining;mathematics;evidential reasoning approach;algorithm;statistics	Logic	-18.34519452851696	3.56358956529454	62666
03b0a172141607f809c0b0b5049c6a5be6cc6ede	pricing and referrals in diffusion on networks		When a new product or technology is introduced, potential consumers can learn its quality by trying the product, at a risk, or by letting others try it and free-riding on the information that they generate. We propose a dynamic game to study the adoption of technologies of uncertain value, when agents are connected by a network and a monopolist seller chooses a policy to maximize profits. Consumers with low degree (few friends) have incentives to adopt early, while consumers with high degree have incentives to free ride. The seller can induce high-degree consumers to adopt early by offering referral incentives rewards to early adopters whose friends buy in the second period. Referral incentives thus lead to a ‘double-threshold strategy’ by which low and high-degree agents adopt the product early while middle-degree agents wait. We show that referral incentives are optimal on certain networks while intertemporal price discrimination (i.e., a first-period price discount) is optimal on others, and discuss welfare implications.	a* search algorithm;approximation;degree distribution;entity–relationship model;experiment;nash equilibrium;network topology;social system;star network	Matt V. Leduc;Matthew O. Jackson;Ramesh Johari	2017	Games and Economic Behavior	10.1016/j.geb.2017.05.011	word of mouth;social learning;actuarial science;economics;marketing;commerce	AI	-5.517100802066771	-6.8224926381801865	62884
6e43e6e6854cf65b5ca3d0b74f965ab1d37bf031	"""editorial to the special issue on """"random variables, joint distribution functions, and copulas"""""""		"""= 0 if x j = 0 for at least one index j, and (3) all n-dimensional differences of C are nonnegative. There he further announced what we now call Sklar's theorem. In the Kybernetika paper he sketched the proof of the above statement, developed some of its consequences, and discussed various connections between copulas and random variables, associative copulas, binary operations on spaces of one-dimensional distribution functions induced by copulas, and applications to probabilistic metric spaces and probabilistic information spaces. At the time copulas were used almost exclusively in the theory of Probabilistic Metric Spaces [15]. Only some years later, after the publication of the paper by Schweizer and Wolff [16], did the statistical community """" discover """" copulas. This had two effects: on the one hand, it was quickly realized that copulas were of paramount importance in dealing with questions concerning the dependence of random variables, and, on the other hand the number of papers dealing with copulas, under both the theoretical and the applied aspects, grew enormously. For a brief history of the early years of copula theory we refer the reader to Schweizer's synthesis [14]. After nearly 50 years from their introduction, copulas can be considered as a standard tool for the constructions of stochastic models, as clearly stated in the books by Joe [9] and Nelsen [12]. As a consequence, they are frequently used in finance: see, just to make few examples, the books by Cherubini et al. [3] and Mc Neil et al. [11], and the recent survey papers by Embrechts [4] and Genest et al. [6]. Moreover, copulas were also found to be of great interest in environmental sciences, especially hydrology: see the book by Salvadori et al. [13] and the paper by Genest and Favre [5]. Outside the field of applied probability and statistics, they appear, for example, in the theory of functional equations and inequalities [1] and fuzzy set"""	book;fuzzy set;spaces;stochastic process;word lists by frequency	Fabrizio Durante;Radko Mesiar;Carlo Sempi	2008	Kybernetika			Theory	-13.985598794661742	-2.7595382751556934	63201
90c9e2097b633d931e189b71d4430d5f465fec94	ultima ratio - a visual language for argumentation	decision support;argumentation;software agents;logic programming visualization engines art animation uniform resource locators counting circuits humans;data visualisation;visual languages;decision support systems visual languages data visualisation logic programming software agents;logic programming;visualisation;decision support systems;visual language;logic programs;reasoning;visualisation unit ultima ratio visual language argumentation rational monologue extended logic programming well founded semantics decision support agent arguments dynamic construction proof trees logical engine;conflict resolution	In the third act of Shakespeare’s Hamlet, the hero is unsure whether to kill Claudius the assassin of Hamlet’s father or not. He argues that if he does kill him, Claudius who is praying at that very moment goes to heaven and if he does not kill him Hamlet’s father is not revenged. A contradiction. Ultima Ratio aims at formalization and visualization of argumentation for agents. An agent is constituted by a set of arguments and assumptions. Facing a particular world, the agent’s believes may be inconsistent triggering a rational monologue to deal with the situation. Formally, we define a framework for argumentation based on extended logic programming under well-founded semantics. The system serves as decision support and is capable of detecting and removing contradictions and deriving conclusions of the agent’s arguments. To demonstrate the structure and dynamics of the agent’s argumentation, we visualise the process as dynamic construction of proof trees. The paper includes screenshots of the logical engine and the visualisation unit as exhibited at the computer arts exhibition Ars Electronica 98.	autodesk softimage;decision support system;interactive storytelling;logic programming;multi-agent system;needham–schroeder protocol;screenshot;sensor;sequence motif;situated;vrml;visual language;well-founded semantics	Michael Schroeder;Daniela Alina Plewe;Andreas Raab	1999		10.1109/IV.1999.781604	computer science;artificial intelligence;theoretical computer science;algorithm	AI	-17.67308568616779	3.769952388573242	63212
8e8c68808c26a56ba44f32b74f11f03fcb8eaa4a	market equilibria with hybrid linear-leontief utilities	finite group;20b05;market equilibrium;complexite calcul;utility function;calculo automatico;68wxx;computing;algorithme;calcul automatique;algorithm;complejidad computacion;equilibre;computational complexity;informatique theorique;ppad;groupe fini;equilibrium;equilibrio;grupo finito;computer theory;algoritmo;informatica teorica	We introduce a new family of utility functions for exchange markets. This family provides a natural and ''continuous'' hybridization of the traditional linear and Leontief utilities and might be useful in understanding the complexity of computing approximating market equilibria, although computing an equilibrium in a market with this family of utility functions, this is PPAD-hard in general. In this paper, we present an algorithm for finding an approximate Arrow-Debreu equilibrium when the Leontief components of the market are grouped, finite and well-conditioned.		Xi Chen;Li-Sha Huang;Shang-Hua Teng	2009	Theor. Comput. Sci.	10.1016/j.tcs.2008.12.030	computing;ppad;computer science;mathematics;mathematical economics;algorithm	ECom	-5.143224524170277	1.147897950578225	63481
bf9bf4c1ccb021c31deaa0d00431a6130974b286	what concepts do	philosophy history;histoire de la philosophie	This paper identifies and criticizes a line of reasoning that has played a substantial role in the widespread rejection of the view that Fodor has dubbed “Concept Atomism”. The line of reasoning is not only fallacious, but its application in the present case rests on a misconception about the explanatory potential of Concept Atomism. This diagnosis suggests the possibility of a new polemical strategy in support of Concept Atomism. The new strategy is more comprehensive than that which defenders of the view, namely Fodor, have employed.	rejection sampling	Kevan Edwards	2009	Synthese	10.1007/s11229-009-9584-y	philosophy;epistemology	AI	-12.096991380860164	2.9529288291707934	63719
16e45baae05b1d5c09f98af6010a4921bfc93865	cooperation in mixed equilibrium	mixed equilibrium;game theory;cooperation;self punishment;disclosure of information	Self-penalties of the players in a two-person game are studied as a cooperative tool: by committing himself to a penalty in case he plays a particular strategy, a player may improve upon the equilibrium payoff of both players. A complete characterization of this phenomenon is given for mixed equilibria.		Hervé Moulin	1976	Math. Oper. Res.	10.1287/moor.1.3.273	non-cooperative game;implementation theory;game theory;traveler's dilemma;best response;sequential equilibrium;trembling hand perfect equilibrium;extensive-form game;repeated game;stochastic game;strategy;correlated equilibrium;chicken;risk dominance;mathematical economics;matching pennies;cooperation;equilibrium selection;solution concept;nash equilibrium;symmetric equilibrium	ECom	-5.158495449597191	-2.1701479394608145	63769
02881eafd6f30ae593b4270fd34195e1b67bcfc3	a systematic comparison of professional exchange rate forecasts with the judgemental forecasts of novices	forecasting;foreign exchange market;judgment;expectation formation;behavioral finance;exchange rate;expertise;human decision making	Professional forecasters in foreign exchange markets are not able to beat naive forecasts. In order to find reasons for this phenomenon we compare the empirical forecasts of experts with the experimentally generated forecasts of novices of the EUR/USD exchange rate in three different forecast horizons. Although the subjects are only provided with the realizations of the exchange rate and are not supported by any statistical procedures they outperform experts in accuracy. Professionals consistently expect a reversal of forgoing exchange rate changes whereas novices extrapolate trends. The judgemental forecasts appear to be unbiased and professionals appear to be biased. We demonstrate that professionals are influenced by the fundamental value an irrelevant anchor in speculative exchange markets. The poor performance of the experts is not a common failure of human decisionmaking in market environments but caused by misleading information.	experiment;extrapolation;foreign exchange service (telecommunications);heuristic;martin kay;rationality;relevance;requirement;speculative execution;statistical model;time series;universal quantification	Johannes Leitner;Robert Schmidt	2006	CEJOR	10.1007/s10100-006-0161-x	financial economics;judgment;actuarial science;economics;forecasting;foreign exchange market;mathematics;management;operations research;behavioral economics;statistics	AI	-5.6928833532972165	-9.28210509262184	63799
d760fb6b69c438b1ca241987faa73a8c07510811	experimentation-driven operator learning	experimentation-driven operator learning	Expert-provided operator descriptions are expensive, incomplete, and incorrect. Given the assumptions of noise-free information and an completely-observable state, OBSERVER can autonomously learn and refines new operators through observation and practice (Wang 1995). WISER, our learning system, relaxes these assumptions and learns operator preconditions through experimentation utilizing imperfect expertprovided knowledge. Our decision-theoretic formula calculates a probably best state S’ for experimentation based on the imperfect knowledge. When a robotic action is executed successfully for the first time in a state S, the corresponding operator’s initial preconditions are learned as parameterized S. We empirically show the number of training examples required to learn the initial preconditions as a function of the amount of injected error. The learned preconditions contain all the necessary positive literals, but no negative literals. Unless given a rule like arm-empty --+ +&ding(X), a robot may believe a state, {(arm-empty), (holding bo~j}, to be possible due to unavoidable perceptual alias. The plan execution in this state is unreliable. To make a planner robust in a noisy state, WISER learns constraining negative preconditions by interconnecting two types of logic systems used in planning systems. The state representation uses two-value logic plus the Closed-World Assumption and the operator representation uses three-value logic. Let L represent all the predicates known to WISER and P the predicates true in S. By CWA, N, the predicates not true in S, is defined as {L P}. WISER induces preconditions from S* = {P U TN}, which prevent inconsistent actions in a noisy state. Let the instantiated operators, iopi and iopj, be obtained from an operator op by instantiating each parameter of op to the same object respectively except that one parameter is instantiated to different objects of the same type, say a for iopi and b for iopj. iqj is obtained by substituting b for a. In any state S, if the preconditions for iopi and iopj are both satisfied (or not satisfied) and their respective actions have the same (parameterized) effects on S, the two objects a	call of duty: black ops;closed-world assumption;cognitive work analysis;decision theory;formal system;observable;precondition;robot;twisted nematic field effect	Kang Soo Tae	1996			machine learning;operator (computer programming);artificial intelligence;computer science	AI	-16.1931933601343	2.519425565376576	63815
34d254602c2de8c21c7203c0c0540417984963db	predictive modeling of interacting agents	electronic mail;communication networks;operational intelligence;information systems;performance evaluation;time measurement;predictive models noise measurement time measurement communication networks electronic mail communication system control performance evaluation broadcasting intelligent agent weapons;predictive modeling;state estimation;noise measurement;ac generators;probabilistic model;technical intelligence;predictive models time measurement ac generators state estimation communication networks communication system control performance evaluation intelligent agent information systems pattern recognition;intelligent agent;technical intelligence predictive modeling interacting agents operational intelligence;pattern recognition;artificial intelligence;predictive models;planning and control;prediction model;process model;broadcasting;sensor fusion artificial intelligence;sensor fusion;communication system control;weapons;interacting agents	A model for characterizing communications behaviors has been generalized to a probabilistic model for intentional behavior. Responsive actions are decomposed into measurement, inference, planning and control components) and are predicted as a function of the estimated capability, opportunity and intent of given agents to perform such component actions. This representational scheme enables the generation of process models for predicting capabilities development (technical intelligence) and tactical operations (operational intelligence).	automated planning and scheduling;control flow;information source;interaction;measurement in quantum mechanics;predictive modelling;statistical model	Alan N. Steinberg	2007	2007 10th International Conference on Information Fusion	10.1109/ICIF.2007.4407979	control engineering;simulation;engineering;artificial intelligence	Robotics	-17.802112451324888	-5.688674767272888	63872
f6dbc7e8a1efca9e8b1c252e1bdd0ed810960993	learning from multiple proofs: first experiments	article in monograph or in proceedings	Mathematical textbooks typically present only one proof for most of the theorems. However, there are infinitely many proofs for each theorem in first-order logic, and mathematicians are often aware of (and even invent new) important alternative proofs and use such knowledge for (lateral) thinking about new problems. In this paper we start exploring how the explicit knowledge of multiple (human and ATP) proofs of the same theorem can be used in learning-based premise selection algorithms in large-theory mathematics. Several methods and their combinations are defined, and their effect on the ATP performance is evaluated on the MPTP2078 large-theory benchmark. Our first findings are that the proofs used for learning significantly influence the number of problems solved, and that the quality of the proofs is more important than the quantity.	algorithm;automated theorem proving;benchmark (computing);first-order logic;first-order predicate;lateral thinking	Daniel Kühlwein;Josef Urban	2012			proofs involving the addition of natural numbers;computer science;artificial intelligence;pure mathematics;mathematics;algorithm	AI	-15.174075352042882	3.246830874234485	63940
3dec4373eb3280a286ef5a4d0fc01dbc6419f99f	distributed problem solving and planning	basic technique;effective solution;right way;tile collective effort;collective effort;multiple problems solvers	Distributed problem solving involves the collective effort of multiple problems solvers to combine their knowledge, information, and capabilities so as to develop solutions to problems that each could not have solved as well (if at all) alone. The challenge in distributed problem solving is thus in marshalling the distributed capabilities in the right ways so that the problem solving activities of each agent complement the activities of the others, so as to lead efficiently to effective solutions. Thus, while working together leads to distributed problem solving, there is also the distributed problem of how to work together that must be solved. We consider that problem to be a distributed planning problem, where each agent must formulate plans for what it will do that take into account (sufficiently well) the plans of other agents. In this paper, we characterize the variations of distributed problem solving and distributed planning, and summarize some of the basic techniques that have been developed to date.	automated planning and scheduling;cooperative distributed problem solving;general-purpose markup language;marshalling (computer science);rendering (computer graphics);sensitivity and specificity	Edmund H. Durfee	2001		10.1007/3-540-47745-4_6	mathematical optimization;computer science;artificial intelligence;management science	AI	-16.84568626434005	-8.788533969172319	63954
1f80023d906aa4be7472e5ce3b47d39651f4a4e0	the stochastic shapley value for coalitional games with externalities	externalities;shapley value;cooperative games	A long debated but still open question in the game theory literature is that of how to extend the Shapley Value to coalitional games with externalities. While previous work predominantly focused on developing alternative axiomatizations, in this article we propose a novel approach which centres around the coalition formation process and the underlying probability distribution from which a suitable axiomatization naturally follows. Specifically, we view coalition formation in games with externalities as a  discrete-time stochastic process . We focus, in particular, on the Chinese Restaurant Process – a well-known stochastic process from probability theory. We show that reformulating Shapley's coalition formation process as the Chinese Restaurant Process yields in games with externalities a unique value with various desirable properties. We then generalize this result by proving that all values that satisfy the direct translation of Shapley's axioms to games with externalities can be obtained using our approach based on stochastic processes.	stable marriage problem	Oskar Skibski;Tomasz P. Michalak;Michael Wooldridge	2018	Games and Economic Behavior	10.1016/j.geb.2017.04.008	externality;economics;microeconomics;shapley value;mathematical economics;welfare economics	ECom	-6.987699814083164	-0.6537583235443193	64107
27e5e57e24185fb6ebc3ac687cc7b191fa34f9a0	multilayered reasoning by means of conceptual fuzzy sets	fuzzy set;pattern recognition;knowledge representation;set theory;fuzzy sets;artificial intelligence	The real world consists of a very large number of instances of events and continuous numeric values. On the other hand, people represent and process their knowledge in terms of abstracted concepts derived from generalization of these instances and numeric values. Logic based paradigms for knowledge representation use symbolic processing both for concept representation and inference. Their underlying assumption is that a concept can be defined precisely. However, as this assumption hardly holds for natural concepts, it follows that symbolic processing cannot deal with such concepts. Thus symbolic processing has essential problems from a practical point of view of applications in the real world. In contrast, fuzzy set theory can be viewed as a stronger and more practical notation than formal, logic based theories because it supports both symbolic processing and numeric processing, connecting the logic based world and the real world. In this paper, we propose multi-layered reasoning by using conceptual fuzzy sets (CFS). The general characteristics of CFS are discussed along with upper layer supervision and context dependent processing.	fuzzy set	Tomohiro Takagi;Atsushi Imura;Hirohide Ushida;Toru Yamaguchi	1996	Int. J. Intell. Syst.	10.1002/(SICI)1098-111X(199602)11:2%3C97::AID-INT2%3E3.0.CO;2-T	knowledge representation and reasoning;computer science;artificial intelligence;machine learning;mathematics;fuzzy set;algorithm;fuzzy control system	AI	-18.562499087868453	1.48261120807254	64538
58e0f34a2ede91ea63019fe4e2be3b7ac1a9f5d6	a blackboard system for planning space missions	space missions	INTRODUCTION This paper reports on the practical application of using a blackboard architecture to combine artificialintelligence planning techniques with conventional models of computing to create an innovative planning and scheduling system for spacecraft mission managers. It describes a mission planning system and a blackboard architecture and proposes a mission planning environment that can be used to conduct satellite coverage analysis, link connectivity analysis, mission planning and performance analysis of existing systems, and constellation tradeoff analysis of candidate designs.	automated planning and scheduling;blackboard system;chuck;interference (communication);radio frequency;scheduling (computing)	G. Pearson	1989		10.1145/66617.66667	computer science;space exploration;in-space propulsion technologies	Robotics	-17.70008770737149	-8.362339551419947	64597
a56df8978d2961b496eb2ecefd9f7669b769a967	standard exponential cure rate model with informative censoring	standard cure rate model;fisher information matrix;coverage probability;informative censoring;62n01	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	censoring (statistics);francis;information;primary source	Luiz Antonio de Freitas;Josemar Rodrigues	2013	Communications in Statistics - Simulation and Computation	10.1080/03610918.2011.627100	censoring;econometrics;fisher information;mathematics;censoring;statistics	Robotics	-14.42531554433228	-5.696260345965976	64647
927fdaa8999d42f080a799ea3d48ce889501102c	erratum to: modeling preventive maintenance of manufacturing processes with probabilistic boolean networks with interventions				Pedro J. Rivera Torres;Eileen I. Serrano Mercado;Orestes Llanes Santiago;Luis Anido Rifón	2018	J. Intelligent Manufacturing	10.1007/s10845-017-1321-7		Robotics	-13.020572271834357	-7.220406072346534	64886
19c8cfecf6f7b7f3b479e532095285bcfd08b417	stochastic timed games revisited	timed automata stochastic games two counter machines;004	Stochastic timed games (STGs), introduced by Bouyer and Forejt, naturally generalize both continuous-time Markov chains and timed automata by providing a partition of the locations between those controlled by two players (Player Box and Player Diamond) with competing objectives and those governed by stochastic laws. Depending on the number of players— 2, 1, or 0—subclasses of stochastic timed games are often classified as 2 1 2 -player, 1 1 2 -player, and 1 2 -player games where the 1 2 symbolizes the presence of the stochastic “nature” player. For STGs with reachability objectives it is known that 1 2 -player one-clock STGs are decidable for qualitative objectives, and that 2 1 2 -player three-clock STGs are undecidable for quantitative reachability objectives. This paper further refines the gap in this decidability spectrum. We show that quantitative reachability objectives are already undecidable for 1 2 player four-clock STGs, and even under the time-bounded restriction for 2 1 2 -player five-clock STGs. We also obtain a class of 1 1 2 , 2 1 2 player STGs for which the quantitative reachability problem is decidable.	automata theory;markov chain;reachability problem;timed automaton;undecidable problem	S. Akshay;Patricia Bouyer;Shankara Narayanan Krishna;Lakshmi Manasa;Ashutosh Trivedi	2016		10.4230/LIPIcs.MFCS.2016.8	combinatorics;discrete mathematics;computer science;mathematics;algorithm	Logic	-5.537225441417065	3.600127584028796	64979
35070d6a3616967c67d2e65faa5729ddd14cf6ac	leading, learning and herding	social learning;reputation;herding;opinion leaders	We analyze a game theoretic model of social learning about a consumption good with endogenous timing and heterogeneous accuracy of private information. We show that if individuals value their reputation for the degree to which they are informed, this reduces the incentive to learn by observing others and exacerbates the incentive to consume the good before others, i.e., to attempt to be an “opinion leader.” Consequently, reputation concerns reduce the average delay of consumption of new goods, and increase (reduce) the probability of herding on consumption (non-consumption). JEL Classification Numbers: C73, D81, D82, D83	game theory;personally identifiable information	Daniel F. Stone;Steven J. Miller	2013	Mathematical Social Sciences	10.1016/j.mathsocsci.2012.12.002	public relations;social learning;herding;actuarial science;reputation;opinion leadership;commerce	Metrics	-5.542480208135984	-6.794496233438352	64998
d8cfda9a000eaad7417af10e97ea36faf7dc3da2	a new eyenet model for diagnosis of diabetic retinopathy	modified probabilistic neural network;modified radial basis function;proposed eyenet model;diabetic retinopathy;modified rbfnn;npdr image;new eyenet model;neural network rbfnn;modified pnn	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	R. Priya;P. Aruna	2013	Applied Artificial Intelligence	10.1080/08839514.2013.848751	machine learning;artificial neural network;diabetic retinopathy;probabilistic neural network;artificial intelligence;computer science;fundus (eye)	Robotics	-14.125069282843574	-6.112823530767928	65521
5c3cf6b75dd86ab888b4e551c9da4070e8633569	combining various types of belief structures	uncertainty modeling;joint variables;fusion;belief structures	We first discuss the basic ideas of the Dempster-Shafer belief structure. We particularly emphasize the associated measures of plausibility and belief. We discuss the cumulative distribution function associated with a belief structure. We next turn to the issue of combining multiple belief structures. Two different types of combination of belief structures are investigated. The first, which we refer to as the fusion of belief structures, occurs when the belief structures being combined are providing information about the same variable. Here we make use of the Dempster rule. The second type, which we refer to as the joining of belief structures, occurs when the belief structures being combined are providing information about different variables. Here we make use of a belief structures cumulative distribution function and the Sklar theorem. The classic belief structures typically have a finite number of focal elements; here we began to look at belief structures which have a continuum of focal elements.		Ronald R. Yager	2015	Inf. Sci.	10.1016/j.ins.2014.12.047	fusion;artificial intelligence;machine learning;mathematics;belief revision	AI	-14.98754006195141	0.21493176232762204	66136
31cc4d0663bb58f3ab879080ec9a8369fe871e9d	agent-based simulation on competition of e-auction marketplaces	electronic commerce;agent based simulation;online auction market;online auction;network effect;retailing;e auction marketplace;internet business information management protocols electronic commerce monopoly lead data analysis analytical models usability;software agents digital simulation electronic commerce retailing;software agents;network externality;online auction market agent based simulation e auction marketplace;digital simulation	In online auction markets, competition exists not only on two sides of a market, i.e. the buyer side and the seller side, but also among auction marketplace operators that provide similar services. Among the factors that determine the result of competition, network effects play an especially prominent role in the participants' decisions regarding the selection of a market. This paper presents an agent-based simulation model to study the competition of e-auction marketplaces. It focuses particularly on the influence of network externalities on the dynamic evolution of markets. The simulation shows that there exists equilibrium on co-existence in which none participant has motivation to move to another market. Furthermore, both the path to achieve such equilibrium and the market partition at equilibrium need not be unique. The model is also easy to be extended with flexibility for investigation on more complicated scenarios of competing markets	agent-based model;simulation	Xin Chen;Juho Mäkiö;Christof Weinhardt	2005	International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)	10.1109/CIMCA.2005.1631584	eauction;combinatorial auction;computer science;artificial intelligence;reverse auction;software agent;network effect;retail;auction theory;forward auction	Robotics	-8.277784121233816	-8.994774950101775	66144
84329c8cb79a36d89b9ed9feb101bfb497d78f49	extensive-form games and strategic complementarities	strategic complements;subgame perfect equilibrium;supermodular game;fixed point theorem;dynamic game;nash equilibria;strategic complementarity;fixed point;complete lattice;normal form game	I prove the subgame-perfect equivalent of the basic result for Nash equilibria in normal-form games of strategic complements: the set of subgame-perfect equilibria is a non-empty, complete lattice. For this purpose I introduce a device that allows the study of the set of subgame-perfect equilibria as the set of fixed points of a correspondence. The correspondence has a natural interpretation. My results are limited because extensive-form games of strategic complementarities turn out— surprisingly—to be a very restrictive class of games. JEL Classification: C72, C73. ∗Department of Economics, University of California at Berkeley and Departamento de Economı́a, Facultad de Ciencias Sociales. email: fede@econ.berkeley.edu. I am very grateful to Ilya Segal and Chris Shannon for many helpful discussions. I also wish to thank Bob Anderson, and Matthew Rabin for their comments. The usual disclaimer applies.	complementarity theory;email;fixed point (mathematics);nash equilibrium;shannon (unit)	Federico Echenique	2004	Games and Economic Behavior	10.1016/S0899-8256(03)00122-2	price of stability;strategic complements;markov perfect equilibrium;game theory;mathematical optimization;best response;trembling hand perfect equilibrium;coordination game;economics;complete lattice;subgame;non-credible threat;folk theorem;repeated game;mathematics;fixed point;microeconomics;fixed-point theorem;normal-form game;mathematical economics;sequential game;subgame perfect equilibrium;welfare economics;equilibrium selection;nash equilibrium	ECom	-5.914477701726099	-1.3609610351347392	66173
44455e54f0973f2bfbd20018b1098fa446ec93be	second degree pareto dominance	stochastic dominance;majorization;lorenz dominance;binary relation;distribution of income;income inequality	We present and discuss a binary relation aimed at the study of the re-distribution of income. We characterize, in a number of ways, the set of income allocations that can be reached from an initial allocation through a sequence of pairwise equalizing transfers, where the sequence contains no flow of income from any donor to anyone else who ends up strictly richer than this donor himself ends up at the outcome of the entire sequence.	pareto efficiency	Ronny Aboudi;Dominique Thon	2008	Social Choice and Welfare	10.1007/s00355-007-0252-1	mathematical optimization;economics;stochastic dominance;binary relation;mathematics;microeconomics;mathematical economics;welfare economics	AI	-5.519494052441446	-2.7177537736552506	66231
b2ce8c3f2521c82253b364b224a66ae543c4176f	using iterative repair to automate planning and scheduling of shuttle payload operations	automated planning;soft x ray;human interaction;satisfiability;planning and scheduling;artificial intelligent;heuristic search;data storage;temporal constraints;ultraviolet;domain specificity;time constraint	This paper describes the DATA-CHASER Automated Planner/Scheduler (DCAPS) system for automated generation and repair of command sequences for the DATA-CHASER shuttle payload. DCAPS uses general Artificial Intelligence (AI) heuristic search techniques, including an iterative repair framework in which the system iteratively resolves conflicts with the state, resource, and temporal constraints of the payload activities. DCAPS was used in the operations of the shuttle payload for the STS-85 shuttle flight in August 1997 and enabled an 80% reduction in mission operations effort and a 40% increase in science return.	artificial intelligence;automated planning and scheduling;heuristic;iteration;iterative method;scheduling (computing)	Gregg Rabideau;Steve A. Chien;Jason Willis;Tobias Mann	1999			interpersonal relationship;real-time computing;simulation;heuristic;computer science;artificial intelligence;computer data storage;satisfiability	AI	-19.042225551402975	-7.349706662935934	66518
15f6b5ec772fd415848a7b59e7bb7f1670c3f20a	development of adaptive group sequential procedure for changing sample size	sample size;62;cui s weighted statistic;power of test;conditional power;sequential test;calculating of necessary sample size;simulation study;40;random numbers;formulation of the power of test;decision rule	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Tomohiro Nakamura;Hideyuki Douke	2010	Communications in Statistics - Simulation and Computation	10.1080/03610911003693029	sequential estimation;sample size determination;econometrics;decision rule;mathematics;statistics	Robotics	-14.207170446985318	-5.68394525903798	66610
4f2792ab999a920a64c282f507d81db89753dac0	application and theory of petri nets and concurrency		Molecular programming is an emerging field concerned with building synthetic biomolecular computing devices at the nanoscale, for example from DNA or RNA molecules. Many promising applications have been proposed, ranging from diagnostic biosensors and nanorobots to synthetic biology, but prohibitive complexity and imprecision of experimental observations makes reliability of molecular programs difficult to achieve. This paper advocates the development of design automation methodologies for molecular programming, highlighting the role of quantitative verification in this context. We focus on DNA ‘walker’ circuits, in which molecules can be programmed to traverse tracks placed on a DNA origami tile, taking appropriate decisions at junctions and reporting the outcome when reaching the end of the track. The behaviour of molecular walkers is inherently probabilistic and thus probabilistic model checking methods are needed for their analysis. We demonstrate how DNA walkers can be modelled using stochastic Petri nets, and apply statistical model checking using the tool Cosmos to analyse the reliability and performance characteristics of the designs. The results are compared and contrasted with those obtained for the PRISM model checker. The paper ends by summarising future research challenges in the field.	cosmos;dhrystone;model checking;nanorobotics;prism (surveillance program);prism model checker;statistical model;stochastic petri net;synthetic biology;traverse	Raymond R. Devillers;Antti Valmari	2015		10.1007/978-3-319-19488-2	computer science;artificial intelligence;operations research	EDA	-17.211738418982748	-1.8899833144112717	66796
9a20eb01574909226978ca762f54ca52e4482006	default values, criteria and constructivism		Wittgenstein, in his later writings, gave an account of the meaning of expressions in terms of criteria for their application. As with many of Wittgenstein's later ideas the notion of a criterion has proved difficult to explicate. A recent account, which ties criteria to the philosophical doctrine of constructivism, provides a link between the concept of a criterion and a series of ideas about language understanding which have emerged in the past few years. It is shown that these ideas can be made to cohere within a general constructivist framework, and that an alternative realist framework is inadequate in this respect.	default logic;social constructivism	Alan Garnham	1980	Cognitive Science	10.1207/s15516709cog0404_4	psychology;social science;philosophy;epistemology;mathematics;sociology;cognitive science	HCI	-13.011535644635051	3.7400812798289413	66996
eba9ce3bb7b34a6055814ab1541d54579f5bfd20	prior’s tonk, notions of logic, and levels of inconsistency: vindicating the pluralistic unity of science in the light of categorical logical positivism	humanidades;filosofia etica	There are still on-going debates on what exactly is wrong with Prior’s pathological “tonk.” In this article I argue, on the basis of categorical inferentialism, that (i) two notions of inconsistency ought to be distinguished in an appropriate account of tonk; (ii) logic with tonk is inconsistent as the theory of propositions, and it is due to the fallacy of equivocation; (iii) in contrast to this diagnosis of the Prior’s tonk problem, nothing is actually wrong with tonk if logic is viewed as the theory of proofs rather than propositions, and tonk perfectly makes sense in terms of the identity of proofs. Indeed, there is fully complete semantics of proofs for tonk, which allows us to link the Prior’s old philosophical idea with contemporary issues at the interface of categorical logic, computer science, and quantum physics, and thereby to expose commonalities between the laws of Reason and the laws of Nature, which are what logic and physics are respectively about. I conclude the article by articulating the ideas of categorical logical positivism and pluralistic unified science as its goal, including the unification of realist and antirealist conceptions of meaning by virtue of the categorical logical basis of metaphysics.	categorical logic;computer science;quantum mechanics;unification (computer science)	Yoshihiro Maruyama	2015	Synthese	10.1007/s11229-015-0932-9	categorical logic;philosophy;epistemology;mathematics;algorithm	Logic	-12.990897463225977	4.154534943396584	67109
9d6bd1712f69040fe75a29ea5769b2221081fd59	infinite-duration bidding games		Two-player games on graphs are widely studied in formal methods as they model the interaction between a system and its environment. The game is played by moving a token throughout a graph to produce an infinite path. There are several common modes to determine how the players move the token through the graph; e.g., in turn-based games the players alternate turns in moving the token. We study the bidding mode of moving the token, which, to the best of our knowledge, has never been studied in infinite-duration games. Both players have separate budgets, which sum up to 1. In each turn, a bidding takes place. Both players submit bids simultaneously, and a bid is legal if it does not exceed the available budget. The winner of the bidding pays his bid to the other player and moves the token. For reachability objectives, repeated bidding games have been studied and are called Richman games [36, 35]. There, a central question is the existence and computation of threshold budgets; namely, a value t ∈ [0, 1] such that if Player 1’s budget exceeds t, he can win the game, and if Player 2’s budget exceeds 1− t, he can win the game. We focus on parity games and mean-payoff games. We show the existence of threshold budgets in these games, and reduce the problem of finding them to Richman games. We also determine the strategy-complexity of an optimal strategy. Our most interesting result shows that memoryless strategies suffice for mean-payoff bidding games. 1998 ACM Subject Classification J.4 Social and Behavioral Sciences, F.1.2 Modes of Computation	computation;formal methods;reachability;unique games conjecture	Guy Avni;Thomas A. Henzinger;Ventsislav Chonev	2017		10.4230/LIPIcs.CONCUR.2017.21	bidding	ECom	-4.82162073515678	2.9799368209778647	67326
68489aa9317e775a9393f4acd903eff9ed27c6cd	how does prospect theory reflect heuristics' probability sensitivity in risky choice?		Two prominent approaches to describing how people make decisions between risky options are algebraic models and heuristics. The two approaches are based on fundamentally different algorithms and are thus usually treated as antithetical, suggesting that they may be incommensurable. Using cumulative prospect theory (CPT; Tversky & Kahneman, 1992) as an illustrative case of an algebraic model, we demonstrate how algebraic models and heuristics can mutually inform each other. Specifically, we highlight that CPT describes decisions in terms of psychophysical characteristics, such as diminishing sensitivity to probabilities, and we show that this holds even when the underlying process is heuristic in nature. Our results suggest that algebraic models and heuristics might offer complementary rather than rival modeling frameworks and highlight the potential role of heuristic principles in information processing for prominent descriptive constructs in risky choice.	algorithm;cpt (file format);heuristic (computer science);information processing;linear algebra;mutual information	Renata Suter;Thorsten Pachur;Ralph Hertwig	2013			prospect theory;social psychology	AI	-14.225088771496166	-0.020506812680722988	67867
2e8886cffb5cee665cb7683f0cf766a850182494	vote manipulation in the presence of multiple sincere ballots	approval voting;gibbard satterthwaite theorem;satisfiability;universiteitsbibliotheek;voting rule	A classical result in voting theory, the Gibbard-Satterthwaite Theorem, states that for any non-dictatorial voting rule for choosing between three or more candidates, there will be situations that give voters an incentive to manipulate by not reporting their true preferences. However, this theorem does not immediately apply to all voting rules that are used in practice. For instance, it makes the implicit assumption that there is a unique way of casting a sincere vote, for any given preference ordering over candidates. Approval voting is an important voting rule that does not satisfy this condition. In approval voting, a ballot consists of the names of any subset of the set of candidates standing; these are the candidates the voter approves of. The candidate receiving the most approvals wins. A ballot is considered sincere if the voter prefers any of the approved candidates over any of the disapproved candidates. In this paper, we explore to what extent the presence of multiple sincere ballots allows us to circumvent the Gibbard-Satterthwaite Theorem. Our results show that there are several interesting settings in which no voter will have an incentive not to vote by means of some sincere ballot.	existential quantification;standardization of office open xml;voter model	Ulrich Endriss	2007		10.1145/1324249.1324268	public relations;bullet voting;exhaustive ballot;spoilt vote;voting;arrow's impossibility theorem;single-member district;computer science;approval voting;contingent vote;mathematics;cardinal voting systems;positional voting system;preferential block voting;welfare economics;anti-plurality voting;condorcet method;satisfiability;disapproval voting	AI	-7.794074348225946	-2.4342912380717334	68021
258013e8cf1ae20460498f4dc7c86faaf680b0b6	survey of dbms use and satisfaction	etude utilisateur;entreprise;user study;empresa;satisfaccion;satisfaction;firm;systeme gestion base donnee;information system;sistema gestion base datos;database management system;systeme information;pc;sistema informacion	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Adam Fadlalla;Narasimhaiah Gorla	1997	IS Management	10.1080/10580539708907051	computer science;database;computer security;information system	Robotics	-16.041735141478107	-5.642232221624854	68084
b2fdca3ce607f042b91ccc3b94315407f985b81f	shared online spreadsheets and hidden profiles: technological effects on dyad decision strategy	group decision strategy;hidden profile task;fast and frugal heuristic;group decision support system	0953-5438/$ see front matter 2012 British Inform http://dx.doi.org/10.1016/j.intcom.2012.06.004 q This paper has been recommended for acceptance ⇑ Corresponding author. Tel.: +34 963983093; fax: E-mail addresses: infabra@uv.es (I. Fajardo), s.j.pay 1 Throughout this article feature and cue are used i properties of alternatives which vary and which determ alternatives. We report a study in which dyads use Instant Messaging to agree a preference among a set of three apartments. The information given to participants is partially overlapping, and contains a ‘‘hidden profile’’ (HP), such that a single apartment emerges as the best according to an unweighted sum of feature values only if dyad members pool information that is presented to only one of them. When dyads were additionally provided with a shared online spreadsheet, their decision strategy was more likely to be compensatory and relatively exhaustive, even if the distribution of importance among the cues in which the apartments vary meant that a ‘‘fast and frugal’’ heuristic such as take-the-best would be a rational strategy. This study shows the potential of classic experimental tasks, the HP task in particular, for understanding technological constraints on group decision making and signals the importance of understanding decision-making strategies, and the potential of fast and frugal heuristics, for informing the design of decision support systems. 2012 British Informatics Society Limited. All rights reserved.	decision support system;decision theory;fax;heuristic (computer science);informatics;instant messaging;spreadsheet	Inmaculada Fajardo;Stephen John Payne	2012	Interacting with Computers	10.1016/j.intcom.2012.06.004	simulation;artificial intelligence	HCI	-8.061654854132456	-6.502094117843013	68255
bce6d160e9cd035b2d45a46e581868b08c8ffc6f	an agent architecture for concurrent bilateral negotiations		We present an architecture that makes use of symbolic decision-making to support agents participating in concurrent bilateral negotiations. The architecture is a revised version of previous work with the KGP model [23, 12], which we specialise with knowledge about the agent’s self, the negotiation opponents and the environment. Our work combines the specification of domain-independent decision-making with a new protocol for concurrent negotiation that revisits the well-known alternating offers protocol [22]. We show how the decision-making can be specialised to represent the agent’s strategies, utilities and preferences using a Prolog-like meta-program. The work prepares the ground for supporting decision-making in concurrent bilateral negotiations that is more lightweight than previous work and contributes towards a fully developed model of the architecture.	agent architecture;bilateral filter;electronic markets;heuristic evaluation;prolog;software deployment;testbed;whole earth 'lectronic link	Bedour Alrayes;Kostas Stathis	2013		10.1007/978-3-319-11364-7_8	interaction protocol;architecture;distributed computing;negotiation;computer science;agent architecture	AI	-18.89747741734445	-9.159603725502722	68293
49f85cbc38d97fb53fb9551c3f1f2c241ecec200	the evolution of cooperation in file sharing p2p systems: first steps	incentive mechanisms;evolutionary algorithm;free riding;p2p systems	In order to cope with the free-riding problem in file sharing P2P systems, two kinds of incentive mechanisms have been proposed: reciprocity based and currency based. The main goal of this work was to study the impact of those incentive mechanisms in the emergence of cooperation in file sharing P2P systems. For each kind of incentive mechanism we designed a game and the outcome of this game was used as a fitness function to carry out an evolutionary process. We were able to observe that the Currency Game obtains an enough cooperative population slightly faster than the Reciprocity Game but, in the long run, the Reciprocity Game outperforms the Currency Game because the final populations under the former are consistently more cooperative than the final populations produced by the latter.	file sharing;the evolution of cooperation	María Esther Sosa-Rodríguez;Elizabeth Pérez Cortés	2012		10.1007/978-3-642-37798-3_14	non-cooperative game;simulation;computer science;artificial intelligence;machine learning;evolutionary algorithm;repeated game;algorithmic game theory;free riding;sequential game;strong reciprocity	Theory	-9.72569604571168	-8.6514091332066	68495
fc29ac45e27364e7b5210ec496b1df3b3749491e	recognizing members of the tournament equilibrium set is np-hard	game theory;solution concept;computational complexity;satisfiability;social science	"""A recurring theme in the mathematical social sciences is how to select the """"most desirable"""" elements given a binary dominance relation on a set of alternatives. Schwartz's tournament equilibrium set (TEQ) ranks among the most intriguing, but also among the most enigmatic, tournament solutions that have been proposed so far in this context. Due to its unwieldy recursive definition, little is known about TEQ. In particular, its monotonicity remains an open problem up to date. Yet, if TEQ were to satisfy monotonicity, it would be a very attractive tournament solution concept refining both the Banks set and Dutta's minimal covering set. We show that the problem of dec iding whether a given alternative is contained in TEQ is NP-hard."""	np-hardness	Felix Brandt;Felix A. Fischer;Paul Harrenstein	2007	CoRR		game theory;mathematical optimization;combinatorics;mathematics;computational complexity theory;solution concept;algorithm;satisfiability	NLP	-5.841910730669845	0.21172505964834365	68719
b55d0a1becd66f04eb165800fb9d71a5ce497250	twisted tales: causal complexity and cognitive scientific explanation	cognitive science;book chapter;information content;philosophy;context dependent;article;environmental factor	Recent work in biology and cognitive science depicts a variety of target phenomena as the products of a tangled web of causal influences. Such influences may include both internal and external factors as well as complex patterns of reciprocal causal interaction. Such twisted tales are sometimes seen as a threat to explanatory strategies that invoke notions such as ‘inner programs’, ‘genes for’ and sometimes even ‘internal representations’. But the threat, I shall argue, is more apparent than real. Complex causal influence, in and of itself, provides no good reason to reject these familiar explanatory notions. To believe otherwise, I suggest, is generally to commit (at least) one of two seductive errors. The first error is to think that the general notion of a state x coding for an outcome y involves the state's constituting a full description of y. This is what I call the ‘myth of the self-contained code’. The second error is to think that the practice of treating certain factors as special (e.g., seeing genes as coding for outcomes in a way environmental factors do not) depends on the (often mistaken) belief that the singled out factor is somehow doing the most real work. Where the amounts of causal influence are evenly spread, it is assumed there can be no reason to treat one factor in a special way. This is what I term the ‘Myth of Explanatory Equality’. Avoiding these errors involves reminding ourselves of (1) the rich context-dependence of even standard, unproblematic uses of the notions of code, program and information content (all three make sense only relative to an assumed ecological backdrop) and (2) the difference between explaining why an event occurred and displaying the full workings of a complex causal system.	backdrop cms;causal filter;causal system;causality;cognitive science;commit (data management);interaction;self-information;tell-tale;twisted	Andy Clark	1998	Minds and Machines	10.1023/A:1008238431527	psychology;self-information;philosophy;epistemology;computer science;artificial intelligence;context-dependent memory;communication;social psychology;algorithm;cognitive science	ML	-12.776149785582579	2.1435930408331263	69034
0457234cf35af596c856637b7a339c11b58cbd69	an axiomatic approach for persuasion dialogs	argumentation;postulates;nonmonotonic reasoning multi agent systems;cognition protocols semantics knowledge based systems abstracts speech computational modeling;nonmonotonic reasoning axiomatic approach public persuasion dialogs agents conflicting opinions abstract argumentation system structured argumentation system;intelligence artificielle;dialog;logique en informatique;multi agent systems;apprentissage;nonmonotonic reasoning;informatique et langage;postulates argumentation dialog	Several systems were developed for supporting public persuasion dialogs where two agents with conflicting opinions try to convince an audience. For computing the outcomes of dialogs, these systems use (abstract or structured) argumentation systems that were initially developed for nonmonotonic reasoning. Despite the increasing number of such systems, there are almost no work on high level properties they should satisfy. This paper is a first attempt for defining postulates that guide the well-definition of dialog systems and that allow their comparison. We propose six basic postulates (including e.g. the finiteness of generated dialogs). We then show that this set of postulates is incompatible with those proposed for argumentation systems devoted for nonmonotonic reasoning. This incompatibility confirms the differences between persuading and reasoning. It also suggests that reasoning systems are not suitable for computing the outcomes of dialogs.	artificial intelligence;content negotiation;dialog system;high-level programming language;knowledge base;mathematical formulation of quantum mechanics;non-monotonic logic;persuasive technology;reasoning system;software incompatibility;universal instantiation;whole earth 'lectronic link	Leila Amgoud;Florence Bannay	2013	2013 IEEE 25th International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2013.97	natural language processing;computer science;artificial intelligence;non-monotonic logic;multi-agent system	AI	-16.874558181677248	3.5969821609652106	69087
d81b99864bf24d52931bea0419b0e41f6714b902	non-standard uses of pirika: pilot of the right knowledge and argument		Argumentation is a dialectical process of knowing things (inquiry) and justifying them (advocacy) in general. Computational argumentation has been recognized as a social computing mechanism or paradigm in the multi-agent systems community. We have developed a computational argumentation framework that basically consists of EALP (Extended Annotated Logic Programming) and LMA (Logic of Multiple-valued Argumentation) constructed on top of EALP. In this paper, we describe some non-standard uses of the implemented argumentation system: PIRIKA (Pilot of the Right Knowledge and Argument) based on EALP and LMA, which is now opened to the public as open source software, and show that those uses can extend further the usefulness and usability of PIRIKA together with the standard use of PIRIKA. PIRIKA allows to us to put forward indefinite agendas (partially unspecified ones) with variables, to represent formal literals as semi-natural language sentences, and to use semi-lattice for annotations of truth-values particularly for the Eastern argumentation.		Yutaka Oomidou;Hajime Sawamura;Takeshi Hagiwara;Jacques Riche	2013		10.1007/978-3-642-44927-7_16	knowledge management;natural language processing;argumentation framework;argumentation theory;software;dialectic;usability;social computing;computer science;artificial intelligence;logic programming	NLP	-17.213465737782123	3.2514212491452374	69097
1b3f6895b884481772ee92331d29edab808765d3	evaluation of workpiece orientation and configuration of multi-axis machine tool using visibility cone analysis	visibility cone analysis;workpiece orientation;path planning;evaluation method;multi axis machine tool;machine tool;manufacturability analysis;process planning	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	algorithm;apache axis;design for manufacturability;eigen (c++ library);francis;national supercomputer centre in sweden;primary source;rotary system;sampling (signal processing);simulation	Rong-Shean Lee;Yan-Hong Lin;Ming-Yang Tseng;Wei-Shiuan Wu	2010	Int. J. Computer Integrated Manufacturing	10.1080/09511921003700861	computer vision;computer science;engineering;operations management;machine tool;motion planning;engineering drawing;mechanical engineering	Robotics	-15.463812713654297	-4.379165515807082	69107
00c4c7fe3b0923aa80ad9cb6a62c8f2c443c5642	analytical geospatial digital earth	journal article	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Bert Veenendaal;Songnian Li;Suzana Dragicevic;Maria A. Brovelli	2014	Int. J. Digital Earth	10.1080/17538947.2014.896967	geology;computer science;world wide web;information retrieval	Robotics	-15.142503778004361	-5.9118870902286735	69296
82d2f6ababd82eb29d4c3a2ca478a5cc638ec1c5	information revelation strategies in abstract argument frameworks using graph based reasoning		The exchange of arguments between agents can enable the achievement of otherwise impossible goals, for example through persuading others to act in a certain way. In such a situation, the persuading argument can be seen to have a positive utility. However, arguments can also have a negative utility  uttering the argument could reveal sensitive information, or prevent the information from being used as a bargaining chip in the future. Previous work on arguing with confidential information suggested that a simple tree based search be used to identify which arguments an agent should utter in order to maximise their utility. In this paper, we analyse the problem of which arguments an agent should reveal in more detail. Our framework is constructed on top of a bipolar argument structure, from which we instantiate bonds  subsets of arguments that lead to some specific conclusions. While the general problem of identifying the maximal utility arguments is NP-complete, we give a polynomial time algorithm for identifying the maximum utility bond in situations where bond utilities are additive.		Madalina Croitoru;Nir Oren	2013		10.1007/978-3-319-04534-4_2	argument;artificial intelligence;mathematics;social psychology;algorithm	AI	-8.617307642552625	-2.2881763180951764	69315
70d236d858e4e9d61eff9c514a18cb9f5692f0b7	the shapley value for the probability game		Abstract The main goal of this paper is to introduce the probability game. On one hand, we analyze the Shapley value by providing an axiomatic characterization. We propose the so-called independent fairness property, meaning that for any two players, the player with larger individual value gets a larger portion of the total benefit. On the other, we use the Shapley value for studying the profitability of merging two agents.		Dongshuang Hou;Genjiu Xu;Panfei Sun;Theo S. H. Driessen	2018	Oper. Res. Lett.	10.1016/j.orl.2018.06.004	mathematical optimization;merge (version control);axiom;profitability index;shapley value;mathematics	ECom	-5.87123034990537	-2.6217083403078405	69334
26bdce5a4810b090395d53e9b88c68ba0c2f80b8	conditional acceptance functions	dung style abstract argumentation	Dung-style abstract argumentation theory centers on argumentation frameworks and acceptance functions. The latter take as input a framework and return sets of labelings. This methodology assumes full awareness of the arguments relevant to the evaluation. There are two reasons why this is not satisfactory. Firstly, full awareness is, in general, not a realistic assumption. Second, frameworks have explanatory power, which allows us to reason abductively or counterfactually, but this is lost under the usual semantics. To recover this aspect, we generalize conventional acceptance, and we present the concept of a conditional acceptance function.	argumentation framework;counterfactual conditional;theory	Richard Booth;Souhila Kaci;Tjitze Rienstra;Leon van der Torre	2012		10.3233/978-1-61499-111-3-470	computer science;knowledge management;artificial intelligence;social psychology;algorithm	AI	-15.643733263733536	3.5566481214509484	69426
163823adc105dcdb58509072ddd551b81be18589	learning in perturbed asymmetric games	zero sum games;potential game;convergence;learning;normal form games;best response dynamics;learning model;social sciences;global convergence;stability;stochastic fictitious play;dynamics;business economics;games;potential games;fictitious play;mixed equilibria;economics;globally asymptotically stable;zero sum game;bimatrix game;mixed strategy equilibria	We investigate the stability of mixed strategy equilibria in 2 person (bimatrix) games under perturbed best response dynamics. A mixed equilibrium is asymptotically stable under all such dynamics if and only if the game is linearly equivalent to a zero sum game. In this case, the mixed equilibrium is also globally asymptotically stable. Global convergence to the set of perturbed equilibria is shown also for (rescaled) partnership games, also known as potential games. Lastly, mixed equilibria of partnership games are shown to be always unstable under all dynamics of this class. Journal of Economic Literature classification numbers: C72, D83.	admissible heuristic;control theory;logical possibility;nash equilibrium;nat friedman;perturbation function;quantum;stochastic process	Josef Hofbauer;Ed Hopkins	2005	Games and Economic Behavior	10.1016/j.geb.2004.06.006	mathematical optimization;best response;economics;business economics;mathematics;zero-sum game;mathematical economics	ECom	-4.89475078819676	-1.431974667195024	69796
bb539c52428685f07709b5eb83ee6613ae06c8ae	level-k reasoning in a generalized beauty contest	coordination games	We study how the predictive power of level-k models changes as we perturb the classical beauty contest setting along two dimensions: the strength of the coordination motive and the information symmetry. We use a variation of the Morris and Shin (2002) model as the unified framework for our study, and find that the predictive power of level-k models varies considerably along these two dimensions. Level-k models are successful in predicting subject behavior in settings with symmetric information and a strong coordination motive. However, the predictive power of level-k models is significantly weakened when private information is introduced or the importance of the coordination motive is decreased.	personally identifiable information;perturbation theory;randomness;unified framework	Dmitry Shapiro;Xianwen Shi;Artie Zillante	2014	Games and Economic Behavior	10.1016/j.geb.2014.04.002	coordination game;economics;mathematics;microeconomics	AI	-6.407672552509792	-6.03227060229707	69890
98a421c7fa02fdf7c4d427dc35e5c203d3fefbd0	categorization: a source of theory and output of research		In a research community, the use of the concept of category and categorization is widespread, generally helpful, but sometimes overly constraining. Despite the wealth of studies that propose new categories, a somewhat static view of categories pervades many disciplines. As we demonstrate on the analysis of a seminal framework by Gregor (2006), a given set of categories can be criticized and challenged in light of potentially valid alternatives. In contrast, we suggest for researchers to adopt the assumption of fluidity of categories, which leads to a different approach to demonstrating the contribution of research that deals with categories.	categorization	Fred Niederman;Roman Lukyanenko	2018		10.1145/3209626.3209717	natural language processing;categorization;artificial intelligence;computer science	Vision	-14.31262233852163	1.0432638772270433	70009
776e7cca4e4711ad3efbd606351c2b0043e10d16	to reveal or not to reveal? strategic disclosure of private information in negotiation	rationalite;experimental economics;modelizacion;confidencialidad;game theory;history;negociation;group decisions and negotiations game theory bounded rationality history consistent rationality experimental economics;asymmetry;economic sciences;social decision;teoria juego;bounded rationality;theorie jeu;asymetrie;distribution cost;confidentiality;modelisation;confidentialite;ciencias economicas;history consistent rationality;racionalidad;cout distribution;negociacion;bargaining;search cost;fictitious play;asimetria;sciences economiques;historia;decision colectiva;search model;decision collective;private information;modeling;group decision;rationality;histoire;group decisions and negotiations	"""Within the bargaining literature, it is widely held that negotiators should never reveal information that will lead to disclosure of their reservation prices. We analyze a simple bargaining and search model in which the informed buyer can choose to reveal his cost of searching for an outside price (which determines his reservation price) to the uninformed seller. We demonstrate that buyers can be made better off by revealing their search cost. More interestingly, we also find that, depending on the assumed distribution of search costs, sometimes buyers with relatively low search costs should reveal their private information whereas in other cases buyers with relatively high search costs should do so. We then test our model experimentally and find that subjects' behavior is not entirely consistent with theoretical predictions. In general, bargainers' behavior is better explained by a bounded rationality model similar to """"fictitious play""""."""	personally identifiable information	Ching Chyi Lee;Michael J. Ferguson	2010	European Journal of Operational Research	10.1016/j.ejor.2010.04.013	game theory;mathematical optimization;systems modeling;private information retrieval;confidentiality;economics;rationality;computer science;public economics;marketing;search cost;mathematics;experimental economics;welfare economics;negotiation;asymmetry;fictitious play;bounded rationality	ECom	-4.9312395319177345	-8.266250515296843	70083
fe0d87982488a81e6c311cabd495c592d00d43c0	game theoretic analysis of the value exchange system	electronic commerce;game theory;local currency;foreign exchange trading;electronic money;games communities vectors mathematical model equations information services game theory;local currency game theoretic analysis value exchange system;game theory foreign exchange trading;game theory local currency electronic commerce electronic money market game;market game	Recently, the idea of using local currency has been attracting attention as a way of circulating goods and services in local communities. However, if users of this currency in the same community have a variety of values, it may be difficult to circulate goods and services. For this reason, we have devised value exchange systems using a local currency embodying the variety of values for exchanging services and values. The systems studied so far increase trading in the community. However, they assume that there are only two users in a community. Moreover, users may not always be satisfied in the previous systems. In this paper, we extend our system to n users by using game theory. Moreover, we examine the condition under which all users in a community can be satisfied by using the core of game theory.	game theory	Sumiko Miyata;Hirotsugu Kinoshita;Tetsuya Morizumi;Li Chao	2013	2013 IEEE 37th Annual Computer Software and Applications Conference Workshops	10.1109/COMPSACW.2013.46	non-cooperative game;implementation theory;game theory;cheap talk;positive political theory;electronic money;computer science;information set;repeated game;screening game;simulations and games in economics education;algorithmic game theory	HCI	-8.046113897544771	-7.663052046586967	70341
a0d80d7d96260353a9dc9b2e9dcbc6ac6ef06bed	intensity of the sense of fairness: measurement and behavioral characterization	journal of economic literature;indivisible good	The analysis of the behavioral and social implications of the intensity of moral sentiments requires that these emotions be quanti...ed. In this paper we quantify the intensity of individual sense of fairness in the context of the model of Karni and Safra (2000). That model depicts selfinterest seeking individuals endowed with intrinsic sense of fairness, who must choose among alternative random allocation procedures to determine who, among a group of eligible individuals, will be given ownership of an indivisible good . For such individuals we develop measures of the intensity of their sense of fairness and explore their behavioral characterization.	fairness measure;indivisible	Edi Karni;Zvi Safra	2002	J. Economic Theory	10.1006/jeth.2001.2845	economics;microeconomics;welfare economics	ECom	-6.231087671839579	-6.440158732813123	70517
406c26a52737dfdc2810c0a8cc366693fd95c0ef	a note on the owen value for glove games	imbalanced market;owen value;glove game;shapley value;computational complexity	A well-known and simple game to model markets is the glove game where worth is produced by building matching pairs. For glove games, different concepts, like the Shapley value, the component restricted Shapley value or the Owen value, yield different distributions of worth. While the Shapley value does not distinguish between productive and unproductive agents in the market and the component restricted Shapley value does not consider imbalancedness of the market, the Owen value accounts for both. As computational effort for Shapley-based allocation rules is generally high, this note provides a computationally efficient formula for the Owen value (and the component restricted Shapley value) for glove games in case of minimal winning coalitions. A comparison of the efficient formulas highlights the above-mentioned differences.	algorithmic efficiency;stable marriage problem	Julia Belau	2015	IGTR	10.1142/S0219198915500140	economics;operations management;microeconomics;shapley value;mathematical economics;computational complexity theory	ECom	-5.420490848539591	-3.574406285901889	70990
cce3a57994ff53ec66af2d805ac559163e0597a8	cognitive autoimmunity: knowledge, ignorance and self-deception		In logic and epistemology, the concept of autoimmunity refers to the partial incapability of the human agent to distinguish between her knowledge and her ignorance, due to an involuntary mechanism which underlies the fixation and revision of beliefs. The idea originated within the project initiated by Dov Gabbay and John Woods of a Naturalization of Logic, which aims at informing elaborated notions of logic and epistemology with well-established results of cognitive science. The term autoimmunity follows from the consideration that the cognitive states of belief, doubt, knowledge and ignorance affect the epistemic status of the agent who experiences them in ways she cannot anticipate nor control. Thus, we contend that the concept of autoimmunity could be usefully employed beyond the epistemological and logical fieldwork, in order to describe the cognitive mechanism supporting what the philosophical literature calls ‘epistemic feelings’, explaining some problematic occurrences of them related to the incorrect analysis of the agent’s own cognition (tip-of-the-tongue experience, misplaced feeling of knowing, etc).	abductive reasoning;cognition;cognitive science;experience;field research;imperative programming;mental state;quantum entanglement	Selene Arfini;Lorenzo Magnani	2016	Logic Journal of the IGPL	10.1093/jigpal/jzw016	social psychology;mathematics;self-deception;algorithm;ignorance;cognition	AI	-14.164575033997895	2.7790714673130816	71024
bbbfae17587e7262dd80d922f993276cf3368a06	comprehensive object-oriented learning - an introduction	object oriented	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Ola Berge;Annita Fjuk;Arne-Kristian Groven;Håvard Hegna;Jens Kaasbøll	2003	Computer Science Education	10.1076/csed.13.4.331.17490	computer science;knowledge management;machine learning;programming language;object-oriented programming	Robotics	-15.23566101495618	-5.612897149828149	71240
ce3c1a7482528c64bfe68f52d771cb1f83e0dcb5	a temporal logic for reasoning about processes and plans	temporal logic	A common disclaimer by an AI author is that he has neglected temporal considerations to avoid complication. The implication is nearly made that adding a temporal dimension to the research (on engineering, medical diagnosis, etc.) would be a familiar but tedious exercise that would obscure the new material presented by the author. Actually, of course, no one has ever dealt with time correctly in an AI program, and there is reason to believe that doing it would change everything. Because time has been neglected, medical diagnosis programs cannot talk about the course of a disease. Story understanding programs have trouble with past events. Problem solvers have had only the crudest models of the future, in spite of the obvious importance of future events. Many researchers have compensated by modeling the course of external time with the program’s own internal time, changing the world model to reflect changing reality. This leads to a confusion between correcting a mistaken belief and updating an outdated belief. Most AI data bases have some sort of operator for removing formulas. (e.g., ERASE in PLANNER, Hewitt, 1972) This operator has tended to be used for two quite different purposes: getting rid of tentative or hypothetical assertions that turned out not to be true, and noting that an assertion is no longer true. The confusion is natural, since some of the same consequences must follow in either case. For example, if ‘‘The car is drivable’’ follows from ‘‘There is gas in the car,’’ then the former statement must be deleted when the latter is, whether you have discovered there to be no gas after all, or the gas has been used up. But in many cases, the two behave quite differently, and efforts to make them the same have resulted in awkward, inextensible programs. For example, from ‘‘x is beating his wife,’’ you are entitled to infer, ‘‘x is a bad man.’’ But if x pauses to catch his breath, only the former statement must be deleted from the data base. Clearly, the proper inference is from ‘‘If x has beat his wife recently, he is a bad man,’’ and ‘‘x is beating his wife,’’ to ‘‘For the next year or so, x will have beaten his wife recently,’’ and hence to	artificial intelligence;assertion (software development);database;natural language understanding;planner;temporal logic	Drew McDermott	1982	Cognitive Science	10.1207/s15516709cog0602_1	psychology;linear temporal logic;temporal logic;interval temporal logic;computation tree logic;artificial intelligence;bunched logic;mathematics;multimodal logic;algorithm;temporal logic of actions	AI	-17.451696504397802	1.0148958179923817	71242
33134d10973a3e147ece71ffa71f535d407f4423	hierarchical plan-based control in open-ended environments: considering knowledge acquisition opportunities		We introduce a novel hierarchical planning approach that extends previous approaches by additionally considering decompositions that are only applicable with respect to a consistent extension of the (open-ended) domain model at hand. The introduced planning approach is integrated into a plan-based control architecture that interleaves planning and execution automatically so that missing information can be acquired by means of active knowledge acquisition. If it is more reasonable, or even necessary, to acquire additional information prior to making the next planning decision, the planner postpones the overall planning process, and the execution of appropriate knowledge acquisition tasks is automatically integrated into the overall planning and execution process.	knowledge acquisition	Dominik Off;Jianwei Zhang	2012		10.1007/978-3-642-36907-0_2	knowledge management	ML	-18.711805474400226	-7.750012422652028	71473
a845aaef08d6b39f58c4d70e4109f81b6006122f	introduction to judea pearl's do-calculus		This is a purely pedagogical paper with no new results. The goal of the paper is to give a fairly self-contained introduction to Judea Pearl’s do-calculus, including proofs of his 3 rules. 1 ar X iv :1 30 5. 55 06 v1 [ cs .A I] 2 6 A pr 2 01 3		Robert R. Tucci	2013	CoRR		computer science;operations research	Theory	-15.240177144470755	2.624899160235966	71483
cca4d2bc40b8aca18903fa3d5cacc35263b33bb4	fuzzy metrics and statistical metric spaces		"""The adjective """"fuzzy"""" seems to be a very popular and very frequent one in the contemporary studies concerning the logical and set-theoretical foundations of mathematics. The main reason of this quick development is, in our opinion, easy to be understood. The surrounding us world is full of uncertainty, the information we obtain from the environment, the notions we use and the data resulting from our observation or measurement are, in general, vague and incorrect. So every formal description of the real world or some of its aspects is, in every case, only an approxima­ tion and an idealization of the actual state. The notions like fuzzy sets, fuzzy orderings, fuzzy languages etc. enable to handle and to study the degree of uncertainty mentioned above in a purely mathematic and formal way. A very brief survey of the most interest­ ing results and applications concerning the notion of fuzzy set and the related ones can be found in [l]. The aim of this paper is to apply the concept of fuzziness to the clasical notions of metric and metric spaces and to compare the obtained notions with those resulting from some other, namely probabilistic statistical, generalizations of metric spaces. Our aim is to write this paper on a quite self-explanatory level the references being necessary only for the reader wanting to study these matters in more details."""	fuzzy set;vagueness	Ivan Kramosil;Jirí Michálek	1975	Kybernetika		product metric;equivalence of metrics	AI	-14.82302757156071	0.7321237675600927	71742
cd6309876d689db922d231903c3e9e5f7c2a4cca	on conjugate information systems: a proposition on how to learn concepts in humane sciences by means of rough set theory	approximate reasoning;attribute value;data mining;humane science;correct understanding;rough set theory;rough set;enough knowledge;conjugate information system;inducing knowledge;data table	Rough sets, the notion introduced by Zdzisław Pawlak in early 80's and developed subsequently by many researchers, have proved their usefulness in many problems of Approximate Reasoning, Data Mining, Decision Making. Inducing knowledge from data tables with data in either symbolic or numeric form, rests on computations of dependencies among groups of attributes, and it is a well-developed part of the rough set theory. Recently, some works have been devoted to problems of concept learning in humane sciences via rough sets. This problem is distinct as to its nature from learning from data, as it does involve a dialogue between the teacher and the pupil in order to explain the meaning of a concept whose meaning is subjective, vague and often initially obscure, through a series of interchanges, corrections of inappropriate choices, explanations of reasons for corrections, finally reaching a point, where the pupil has mastered enough knowledge of the subject to be able in future to solve related problems fairly satisfactorily. We propose here an approach to the problem of learning concepts in humane sciences based on the notion of a conjugate system; it is a family of information systems, organized by means of certain requirements in order to allow a group of students and a teacher to analyze a common universe of objects and to correct faulty choices of attribute value in order to reach a more correct understanding of the concept.		Maria Semeniuk-Polkowska	2007	Trans. Rough Sets	10.1007/978-3-540-71200-8_16	artificial intelligence;mathematics;algorithm	AI	-15.7673071438406	1.672039972367963	71773
78ecfd9c1817b1b6d0bfb17c811e6681d3748d5c	a universal measure of intelligence for artificial agents	conference paper;computer sciences	where ri is the reward in cycle i of a given history, and the expected value is taken over all possible interaction histories of π and μ. The choice of γi is a subtle issue that controls how greedy or far sighted the agent should be. Here we use the near-harmonic γi := 1/i2 as this produces an agent with increasing farsightedness of the order of its current age [Hutter2004]. As we desire an extremely general definition of intelligence for arbitrary systems, our space of environments should be as large as possible. An obvious choice is the space of all probability measures, however this causes serious problems as we cannot even describe some of these measures in a finite way.	greedy algorithm;intelligent agent	Shane Legg;Marcus Hutter	2005			artificial architecture;music and artificial intelligence;applied mathematics;computer science;artificial intelligence;artificial psychology;artificial intelligence, situated approach	AI	-11.526692329653066	-2.543153603857796	71882
6718f51aab474eee8d8d662b1e2bead46c15919d	the limitation of bayesianism	bayes theorem;revision;conditioning;sensitivity;reasoning under uncertainty;jeffrey s rule;ignorance;updating	In the current discussion about the capacity of Bayesianism in reasoning under uncertainty, there is a conceptual and notational confusion between the explicit condition and the implicit condition of a probability evaluation. Consequently, the limitation of Bayesianism is often seriously underestimated. To represent the uncertainty of a belief system where revision is needed, it is not enough to assign a probability value to each belief.		Pei Wang	2004	Artif. Intell.	10.1016/j.artint.2003.09.003	sensitivity;artificial intelligence;conditioning;mathematics;bayes' theorem	AI	-13.680536596140968	2.619622104151811	72148
c9b5e88c20a6ccf622be8f45138faa8fc530d453	reliability engineering: probabilistic models and maintenance methods	probabilistic model	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source;reliability engineering	Richard P. Heydorn	2006	Technometrics	10.1198/tech.2006.s412	reliability engineering;probabilistic-based design optimization;availability;probabilistic design	Robotics	-14.918372820375708	-5.181468803960855	72155
2ba5ef39ccef69d9900ba0184ff2760a56ce2529	who acts more like a game theorist? group and individual play in a sequential market game and the effect of the time horizon	stackelberg market;groups versus individuals;discontinuity effect;experiment	Previous experimental results on one-shot sequential two-player games show that group decisions are closer to the subgame-perfect Nash equilbirum than individual decisions. We extend the analysis of inter-group versus inter-individual decision making to a Stackelberg market game, by running both one-shot and repeated markets. Whereas in the one-shot markets we find no significant differences in the behavior of groups and individuals, we find that the behavior of groups is further away from the subgame-perfect equilibrium of the stage game than that of individuals. To a large extent, this result is independent of the method of eliciting choices (sequential or strategy method) and the method used to account for observed firstand secondmover behavior. We provide evidence on followers’response functions and electronic chats to offer an explanation for the differential effect that the time horizon of interaction has on the extent of individual and group players’(non)conformity with subgame perfectness. JEL Classification numbers: C72, C92, L13.	game theory;nash equilibrium	Wieland Müller;Fangfang Tan	2013	Games and Economic Behavior	10.1016/j.geb.2013.09.007	experiment;economics;repeated game;microeconomics;mathematical economics;sequential game;subgame perfect equilibrium;welfare economics	ECom	-5.113813297518972	-6.051919551651173	72167
1e65041c9fe8293e8c49140b4550b08d58b186ba	a tractable and expressive class of marginal contribution nets and its applications	coalitional games;rule based;read once formulas;marginal contribution networks;shapley value;solution concept;network flow;point of view;series parallel	Coalitional games raise a number of important questions from the point of view of computer science, key among them being how to represent such games compactly, and how to efficiently compute solution concepts assuming such representations. Marginal contribution nets (MC-nets), introduced by Ieong and Shoham, are one of the simplest and most influential representation schemes for coalitional games. MCnets are a rule-based formalism, in which rules take the form pattern −→ value, where “pattern” is a Boolean condition over agents, and “value” is a numeric value. Ieong and Shoham showed that, for a class of what we will call “basic” MC-nets, where patterns are constrained to be a conjunction of literals, marginal contribution nets permit the easy computation of solution concepts such as the Shapley value. However, there are very natural classes of coalitional game that require an exponential number of such basic MC-net rules. We present read-once MC-nets, a new class of MCnets that is provably more compact than basic MC-nets, while retaining the attractive computational properties of basic MC-nets. We show how the techniques we develop for read-once MC-nets can be applied to other domains, in particular, computing solution concepts in network flow games on series-parallel networks.	cobham's thesis;computation;computer science;flow network;kinetic data structure;logic programming;marginal model;semantics (computer science);series-parallel graph;time complexity	Edith Elkind;Leslie Ann Goldberg;Paul W. Goldberg;Michael Wooldridge	2009	Math. Log. Q.	10.1002/malq.200810021	rule-based system;series and parallel circuits;mathematical analysis;flow network;topology;computer science;mathematics;shapley value;mathematical economics;solution concept;algorithm;algebra	Theory	-5.917614711161394	2.7505276933177325	72237
008bf3f2421def1fe9d943da36551a40c81df2d8	environment-dependent content and the virtues of causal explanation		One line of objection to Externalism or Anti-Individualism is that, if it were true, causal explanations citing the content of mental states or events would be redundant. Another is that individuation by intentional properties would fail to be individuation by causal powers. My aim in the present paper is to answer these charges. But first a few terminological matters to get the charges into focus. We need a formulation of Externalism or Anti-Individualism. Let us say that:	causal filter;causality;externalism;mental state	Paul Noordhof	2005	Synthese	10.1007/s11229-005-0579-z	externalism;epistemology;individuation;causality;mathematics	AI	-12.983492876236934	3.4273009796665606	72439
9bd979fac14d546ed5fb4ea71d706b1993a36053	a choice prediction competition for social preferences in simple extensive form games: an introduction	trust;computer program;fairness;social welfare;prediction error;game theory;competition;decision choices and conditions;forecasting and prediction;social preferences;altruism;rational choice;motivation and incentives;behavior;article;reciprocity;level 1	Two independent, but related, choice prediction competitions are organized that focus on behavior in simple two-person extensive form games (http://sites.google.com/site/extformpredcomp/): one focuses on predicting the choices of the first mover and the other on predicting the choices of the second mover. The competitions are based on an estimation experiment and a competition experiment. The two experiments use the same methods and subject pool, and examine games randomly selected from the same distribution. The current introductory paper presents the results of the estimation experiment, and clarifies the descriptive value of some baseline models. The best baseline model assumes that each choice is made based on one of several rules. The rules include: rational choice, level-1 reasoning, an attempt to maximize joint payoff, and an attempt to increase fairness. The probability of using the different rules is assumed to be stable over games. The estimated parameters imply that the most popular rule is rational choice; it is used in about half the cases. To participate in the competitions, researchers are asked to email the organizers models (implemented in computer programs) that read the incentive structure as input, and derive the predicted behavior as an output. The submission deadline is 1 December 2011, the results of the competition experiment will not be revealed until that date. The submitted models will be ranked based on their prediction OPEN ACCESS	baseline (configuration management);computer program;email;experiment;fairness measure;randomness	Eyal Ert;Ido Erev;Alvin E. Roth	2011	Games	10.3390/g2030257	game theory;competition;economics;social preferences;altruism;social welfare;microeconomics;mathematical economics;reciprocity;welfare economics;statistics;behavior	AI	-6.050361722142271	-6.031926813054252	72937
e3e7e761795cc10c3579d692ec253d1228c3c8ba	using expressive dialogues and gradient information to improve trade-offs in bilateral negotiations	conflict of interest;search space;fuzzy constraints	A bilateral negotiation may be seen as an interaction between two parties with the goal of reaching an agreement over a given range of issues which usually involves solving a conflict of interests between the parties involved. In our previous work, we address the problem of automatic bilateral negotiation by using fuzzy constraints as a mean to express participant's preferences, focusing in purchase negotiation scenarios. Other research works have used similarity criteria to perform trade-offs in bilateral bargaining scenarios, without any expressive mechanisms between participants. In this paper, we combine our expressive approach with the traditional positional bargaining schema. In particular, we explore the possibility of using the derivatives of each agent's valuation function to issue direction requests to narrow the solution search space of its counterpart, thus improving the effectiveness and efficiency of the negotiation over traditional positional approaches.	gradient descent	Ivan Marsá-Maestre;Miguel A. López-Carmona;Juan Ramón Velasco;Bernardo Alarcos	2008		10.1007/978-3-540-85717-4_8	searching the conformational space for docking;knowledge management	NLP	-9.402761148234799	-7.651804350764317	72950
d5bcb7b99f90034132c13eb74434874f670cc802	a value via posets induced by graph-restricted communication situations	allocation rule;cooperative game;directed graph;solution concept	This paper provides a new value (solution concept or allocation rule) of cooperative games via posets induced by graphs. Several values in a graph-restricted communication situation have been proposed or introduced by Myerson, Borm, and Hamiache... However, these values have been subjected to some criticisms in certain types of games. The value proposed in this paper withstands these criticisms. Moreover, these existing values have been defined only in situations represented by undirected graphs, while the notion of the value proposed in this paper can be extended to situations represented by directed graphs. Keywords— graph-restricted situations, communication situations, values, posets, cooperative games.	directed graph;graph (discrete mathematics)	Katsushige Fujimoto;Aoi Honda	2009			combinatorics;discrete mathematics;artificial intelligence;mathematics	AI	-7.717761645874784	-2.080721843893728	73624
f0675d5e2342879f2914fa2fde6b0b3a2bd4841b	a critique of the sensitivity rules usually employed for statistical table protection	statistical databases;database security;statistical disclosure control;disclosure risk;security	In statistical disclosure control of tabular data, sensitivity rules are commonly used to decide whether a table cell is sensitive and should therefore not be published. The most popular sensitivity rules are the dominance rule, the p%-rule and the pq-rule. The dominance rule has received critiques based on specific numerical examples and is being gradually abandoned by leading statistical agencies. In this paper, we construct general counterexamples which show that none of the above rules does adequately reflect disclosure risk if cell contributors or coalitions of them behave as intruders: in that case, releasing a cell declared non-sensitive can imply higher disclosure risk than releasing a cell declared sensitive. As possible solutions, we propose an alternative sensitivity rule based on the concentration of relative contributions. More generally, we suggest to complement a priori risk assessment based on sensitivity rules with a posteriori risk assessment which takes into account tables after they have been protected.	numerical analysis;risk assessment;rule 184;rule 90;statistical disclosure control;statistical model;table (information);table cell	Josep Domingo-Ferrer;Vicenç Torra	2002	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488502001636	computer science;artificial intelligence;information security;data mining;computer security;statistics	DB	-11.205290151720032	-4.231729815839763	73651
3ff89bb5fc11dea187b435a062f3ee12eb796640	on obligation rules for minimum cost spanning tree problems	allocation rule;satisfiability;transportation;spanning tree	"""Tijs et al. (2006) introduce the family of obligation rules in minimum cost spanning tree problems. We prove that obligation rules are closely related with the marginalistic values of the irreducible game. We also provide axiomatic characterizations of obligation rules with two basic monotonicity properties, namely population monotonicity (if new agents join a """"society"""" no agent from the """"initial society"""" can be worse off) and strong cost monotonicity (if a number of connection costs increase, no agent can be better off). In this class, the folk rule is the only allocation rule satisfying equal treatment of equals."""	axiomatic system;file spanning;irreducibility;minimum spanning tree	Gustavo Bergantiños;Anirban Kar	2010		10.1145/1807406.1807481	mathematical optimization;mathematics;welfare economics;algorithm	ECom	-7.53658879196708	-2.263892212110464	73697
48bcd865dbf8f81820f4cf0c7047981045f2cab0	load balancing for virtual worlds by splitting and merging spatial regions		The expansion and contraction are the fundamental operations in systems assigning resources strictly based on load. Previously, an aggregation algorithm was proposed for distributing the load of a space (square in shape initially) comprises of a number of sub-spaces as balanced as possible among two servers. It used the SplitCapacity constraint to get contiguous larger spaces for an improved performance. However, the implementation of this algorithm revealed that, it violates the implicit consideration of the continuity constraint, when it distributes the load for non-square grid spaces. The basic flaw is that it allows non-contiguous and odd combinations. Further, the same was discovered with merging that uses only the MergeCapacity constraint to initiate the contraction process. In this paper, we summarise our previous work and examine the limitations in basic aggregation and merging algorithms. It is demonstrated with theoretical arguments and a simple simulation model that the assignment of contiguous spaces has potential benefits. We, therefore, extend these algorithms to incorporate an explicit continuity check to overcome the issues introduced by allowing odd cases. It is demonstrated with the help of results from our prototype that the extended methods strictly achieves the theoretical goals of the proposed methods.		Umar Farooq;John Glauert;Kashif Zia	2018	Informatica (Slovenia)		grid;mathematical optimization;computer science;merge (version control);contiguity (probability theory);load balancing (computing);metaverse;server	HPC	-13.076632001216712	-1.9569298869176224	74070
de1d5d6d93147ca920b71540be7088c72c87739b	belief updating and the demand for information	experiment	How do individuals value noisy information that guides economic decisions? In our laboratory experiment, we find that individuals underreact to increasing the informativeness of a signal, thus undervalue high-quality information, and that they disproportionately prefer information that may yield certainty. Both biases appear to be mainly due to non-standard belief updating. We find that individuals differ consistently in their responsiveness to information – the extent that their beliefs move upon observing signals. Individual parameters of responsiveness to information have explanatory power in two distinct choice environments and are unrelated to proxies for mathematical aptitude.	information;responsiveness;aptitude	Sandro Ambuehl;Shengwu Li	2018	Games and Economic Behavior	10.1016/j.geb.2017.11.009	economics;belief structure;management science;social psychology;welfare economics	ECom	-4.9848067564375595	-8.786467052648637	74087
e62071afa8cf7863af4d2f95c7abaa82a18a8dfb	on dialogue systems with speech acts, arguments, and counterarguments	logica formal;dialogue system;multiagent system;argumentation;intelligent tutoring system;habla;nonmonotonic logic;speech acts;speech;contreargument;formal logic;parole;sistema multiagente;logique formelle;systeme multiagent;systeme tutorial intelligent	This paper proposes a formal framework for argumentative dialogue systems with the possibility of counterargument. The framework allows for claiming, challenging, retracting and conceding propositions. It also allows for exchanging arguments and counterarguments for propositions, by incorporating argument games for nonmonotonic logics. A key element of the framework is a precise definition of the notion of relevance of a move, which enables flexible yet well-behaved protocols.	dialog system	Henry Prakken	2000		10.1007/3-540-40006-0_16	natural language processing;computer science;speech;artificial intelligence;non-monotonic logic;logic;algorithm	NLP	-16.736059844529525	3.7475058519691666	74100
fb25b15b653021e8c40a6b5fbc60addfaf20c085	analyzing phylogenetic trees with timed and probabilistic model checking: the lactose persistence case study		Model checking is a generic verification technique that allows the phylogeneticist to focus on models and specifications instead of on implementation issues. Phylogenetic trees are considered as transition systems over which we interrogate phylogenetic questions written as formulas of temporal logic. Nonetheless, standard logics become insufficient for certain practices of phylogenetic analysis since they do not allow the inclusion of explicit time and probabilities. The aim of this paper is to extend the application of model checking techniques beyond qualitative phylogenetic properties and adapt the existing logical extensions and tools to the field of phylogeny. The introduction of time and probabilities in phylogenetic specifications is motivated by the study of a real example: the analysis of the ratio of lactose intolerance in some populations and the date of appearance of this phenotype.	checking (action);lactose intolerance;model checking;phylogenesis;phylogenetic tree;population;probability;specification;statistical model;temporal logic;trees (plant);verification of theories	José Ignacio Requeno;José Manuel Colom	2014	Journal of integrative bioinformatics	10.2390/biecoll-jib-2014-248	theoretical computer science;algorithm	Logic	-17.14867497900613	-1.8523852022604435	74230
a15613970e405f4c5e45e29dbf1c421d92f79a93	reduction, externalism and immanence in husserl and heidegger	philosophy;philosophie	This paper argues that the Husserl–Heidegger relationship is systematically misunderstood when framed in terms of a distinction between internalism and externalism. Both philosophers, it is argued, employ the phenomenological reduction to immanence as a fundamental methodological instrument. After first outlining the assumptions regarding inner and outer and the individual and the social from which recent epistemological interpretations of phenomenology begin, I turn to the question of Husserl’s internalism. I argue that Husserl can only be understood as an internalist on the assumption that immanence equates with internal. This, however, is not the case as can be seen once the reduction is understood not as setting aside the existence of the world, but rather a reflection on its meaning. Turning to Heidegger, I argue that his commitment to a form of the phenomenological reduction precludes him from being either a semantic or a social externalist. The place of authenticity and the first person perspective in his work derive from his phenomenological commitments, which can be seen in his accounts of discourse and language and of falling (Verfallen). I then go on to briefly outline a more plausible basis for understanding the difference between Husserl’s and Heidegger’s phenomenologies in terms of their respective emphases on logic and on poetics.	externalism;first-person (video games)	Felix O'Murchadha	2006	Synthese	10.1007/s11229-006-9085-1	philosophy;epistemology	AI	-12.96710345980898	2.90007521479974	74325
3bfbb8f136ef88e883a1bd7e59ea8dea9454d81c	the power of paradox: some recent developments in interactive epistemology	game theory;backward induction	Paradoxes of game-theoretic reasoning have played an important role in spurring developments in interactive epistemology, the area in game theory that studies the role of the players’ beliefs, knowledge, etc. This paper describes two such paradoxes – one concerning backward induction, the other iterated weak dominance. We start with the basic epistemic condition of “rationality and common belief of rationality” in a game, describe various ‘refinements’ of this condition that have been proposed, and explain how these refinements resolve the two paradoxes. We will see that a unified epistemic picture of game theory emerges. We end with some new foundational questions uncovered by the epistemic program.	amanda;backward induction;brian henderson-sellers;case-based reasoning;epr paradox;email;experiment;game theory;lexicographical order;rationality;set theory;stuart card;viz: the computer game	Adam Brandenburger	2007	Int. J. Game Theory	10.1007/s00182-006-0061-2	game theory;economics;mathematics;microeconomics;mathematical economics;backward induction	AI	-9.922654543830912	-1.0711359363047799	74349
ba93a66e67ff79ff41d52967a6fbaeaceaceeb40	intransitive indifference in preference theory: a survey		This paper presents a survey of results in preference theory with intransitive indifference and discusses them for the areas of basic preference theory, consumer preference, additive utility, qualitative probability, expected utility, and social choice.		Peter C. Fishburn	1970	Operations Research	10.1287/opre.18.2.207	economics;microeconomics;welfare economics	ECom	-7.556700568114742	-1.4562827395153357	74375
f5529c911af89f5ab0b944dbca0a1c83ca2f1f0f	deterministic, strategyproof, and fair cake cutting		We study the classic cake cutting problem from a mechanism design perspective, in particular focusing on deterministic mechanisms that are strategyproof and fair. We begin by looking at mechanisms that are non-wasteful and primarily show that for even the restricted class of piecewise constant valuations there exists no direct-revelation mechanism that is strategyproof and even approximately proportional. Subsequently, we remove the nonwasteful constraint and show another impossibility result stating that there is no strategyproof and approximately proportional direct-revelation mechanism that outputs contiguous allocations, again, for even the restricted class of piecewise constant valuations. In addition to the above results, we also present some negative results when considering an approximate notion of strategyproofness, show a connection between direct-revelation mechanisms and mechanisms in the Robertson-Webb model when agents have piecewise constant valuations, and finally also present a (minor) modification to the well-known Even-Paz algorithm that has better incentive-compatible properties for the cases when there are two or three agents.	approximation algorithm;cutting stock problem;whole earth 'lectronic link	Vijay Menon;Kate Larson	2017		10.24963/ijcai.2017/50	forensic engineering	AI	-6.273731888659167	-4.369431458324239	74415
7e86b40f2adafce24b478ae4eada81e0e38d08f6	integration of product quality and tool degradation for reliability modelling and analysis of multi-station manufacturing systems	system reliability;integrable model;multi station manufacturing system;reliability modeling;journal;tool degradation;information integration;quality and reliability interaction;interactive system;product quality;tool failure;manufacturing system	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	elegant degradation;francis;multiseat configuration;primary source;reliability engineering	Jiwen Sun;Lifeng Xi;Ershun Pan;Shichang Du;B. Ju	2009	Int. J. Computer Integrated Manufacturing	10.1080/09511920802209058	reliability engineering;computer science;systems engineering;engineering;operations management;information integration;database	Robotics	-15.009239777947156	-4.696918668602852	74609
8637e1e2a8b8d7b7d0028aa46446857156dffd96	equilibrium analysis in cake cutting	game theory;cake cutting;fair division	Cake cutting is a fundamental model in fair division; it represents the problem of fairly allocating a heterogeneous divisible good among agents with different preferences. The central criteria of fairness are proportionality and envy-freeness, and many of the existing protocols are designed to guarantee proportional or envy-free allocations, when the participating agents follow the protocol. However, typically, all agents following the protocol is not guaranteed to result in a Nash equilibrium. In this paper, we initiate the study of equilibria of classical cake cutting protocols. We consider one of the simplest and most elegant continuous algorithms – the Dubins-Spanier procedure, which guarantees a proportional allocation of the cake – and study its equilibria when the agents use simple threshold strategies. We show that given a cake cutting instance with strictly positive value density functions, every envy-free allocation of the cake can be mapped to a pure Nash equilibrium of the corresponding moving knife game. Moreover, every pure Nash equilibrium of the moving knife game induces an envy-free allocation of the cake. In addition, the moving knife game has an -equilibrium which is ε-envy-free, allocates the entire cake, and is independent of the tie-breaking rule.	algorithm;efficient cake-cutting;fairness measure;nash equilibrium	Simina Brânzei;Peter Bro Miltersen	2013			fair division;game theory;zero-sum game	AI	-5.537786802980524	-3.084382028059632	75078
6493aef2279905f674e4cc62462aae8c82fcf791	decision taking versus promise issuing	universiteitsbibliotheek	An alignment is developed between the terminology of outcom e riented decision taking and a terminology for promise issuing. Differ ences and correspondences are investigated between the concepts of decision an d promise. For decision taking, two forms are distinguished: the exter nal outcome delivering form and internalized decision taking. Internali zed decision taking is brought in connection with Marc Slors’ theory of self-progr amming. Examples are produced for decisions and promises in four dif ferent several settings each connected with software technology: instruc tion sequence effectuation, informational money transfer, budget announcement , and division by zero.	division by zero	Jan A. Bergstra	2013	CoRR		decision analysis;computer science;management;business decision mapping	ML	-12.57681804176072	-1.1510960717202103	75111
ad3fad73d527b651d8371c1f8318e7e0b0d44d04	collective coin flipping, robust voting schemes and minima of banzhaf values	protocols;random access memory;mathematics;game theory;atmospheric measurements;fault tolerant;particle measurements;gain;distributed computing;voting game;binary trees;indexes;fault tolerant systems;voting;cryptography;games;fault tolerance;robustness voting game theory computer science distributed computing protocols mathematics power generation economics fault tolerance decision making;process control;randomized algorithm;robustness;decision process;computer science;economics;majority voting;filtering theory;information theory;power generation economics;power measurement	The power of players in a collective decision process is a central issue in Mathematical Economics and Game Theory. Similar issues arise in Computer Science in the study of distributed, fault tolerant computations when several processes, some perhaps faulty, have to reach agreement. In the present article we study voting schemes which are relatively immune to the presence of unfair players. In particular, we discuss how to perform collective coin flipping which is only slightly biased despite the presence of unfair players. Mathematically this corresponds to problems concerning the minima of Banzhaf values in certain n -person games. These are measures of power studied in Game Theory. It is quite remarkable that while dictatorial voting games are, of course, the most sensitive to the presence of unfair players, some voting schemes that we propose here are significantly more robust than majority voting. Coin flipping was selected as a study case because of its simplicity and because collective coin flipping is widely used in randomized algorithms for distributed computations. It is our feeling that Game Theory has much to contribute to Computer Science and we are sure that further applications will be found.	computation;computer science;fault tolerance;game theory;maxima and minima;randomized algorithm	Michael Ben-Or;Nathan Linial	1985	26th Annual Symposium on Foundations of Computer Science (sfcs 1985)	10.1109/SFCS.1985.15	game theory;fault tolerance;combinatorics;simulation;information theory;computer science;theoretical computer science;process control;mathematics;computer security;algorithm;statistics	Theory	-9.511656539902539	-4.250438633458512	75257
315f373f2fea94f2b4af4a99e9362fbeb139abdd	real-time control of decentralised autonomous flexible manufacturing systems by using memory and oblivion	flexible manufacturing systems;memory data;autonomous decentralised system;fms;real time control;manufacturing control;decentralised fms;flexible manufacturing system;autonomous fms;production systems;production planning;agv;automatic guided vehicles;oblivion;reasoning;agvs;memory	This paper describes a method that uses memory to determine a priority ranking for competing hypotheses. The aim is to increase the reasoning efficiency of a system which controls Automatic Guided Vehicles (AGVs) in Autonomous Decentralised Flexible Manufacturing Systems (AD-FMSs). The system includes memory data of past production conditions and AGV actions. Using these memory data, the system reorders hypotheses by giving the highest priority ranking to the hypothesis that is most likely to be true. The system was applied to an AD-FMS constructed on a computer. The results showed that, this reasoning system reduced the number of hypothesis replacements.	autonomous robot;read-only memory;real-time transcription;reasoning system	Hidehiko Yamamoto;Rizauddin Bin Ramli	2007	IJIIDS	10.1504/IJIIDS.2007.016685	real-time control system;computer science;artificial intelligence;production system;memory;reason	Robotics	-12.19586335159086	-5.011367170809201	75268
3ad7bde77795235104ff3b5b48d205feeeeb1f5a	knowledge and use of price distributions by populations and individuals		How much do individuals, compared to the population, know about the distribution of values in the world? Participants reported the prices of consumer goods such as watches and belts and we compared how accurately individuals vs. the overall population knew the mean and dispersion of prices. Although individuals and the population both knew objects’ average prices and relative standard deviations, the population was more sensitive to the absolute standard deviation of prices. In a second experiment, we examined whether individuals’ impoverished distribution knowledge impairs their ability to interpret advertisements. Consistent with people using Bayesian inference, the higher an object’s actual price dispersion, the more participants relied on advertisements; however, this effect is considerably smaller than a simple proportional offset, suggesting again that individuals underestimate dispersion. Thus, despite having a sense of the distribution of real world quantities, individuals tend to know only a fraction of the world distribution.	bayesian approaches to brain function;population	Timothy Lew;Ed Vul	2016			social psychology;psychology	HCI	-4.873104892386199	-9.109131328175911	75398
fb5f01bff9c3687738be92b8dd2833019ee38f85	market integration vs temporal granularity: how to provide needed flexibility resources?		The aim of this paper is to study the implications of the decision of the French regulator to join the FCR Cooperation, a common platform for cross-border procurement of primary reserve. Two aspects will be analyzed: 1) cost of procurement and increase of social welfare by cross-border procurement and 2) implications for entry of new entrants like aggregators of Electric Vehicles. Our results are that joining the FCR Cooperation will impact negatively participation of aggregators and might not result to lower costs for procurement of reserve.	common platform;common weakness enumeration;news aggregator;procurement;requirement	Olivier Borne;Marc Petit;Yannick Perez	2017	2017 14th International Conference on the European Energy Market (EEM)	10.1109/ISGTEurope.2017.8260135	merge (version control);remuneration;procurement;market integration;granularity;regulator;social welfare;microeconomics;business	Robotics	-5.899319698948977	-8.505334043321069	75506
aa8e88145434232cdc5534edf7a93d8da1474905	belief merging using partial satisfactibility: cases studies	satisfiability	Merging operators aim at defining the beliefs or goals of a group of agents from the beliefs or goals of each member of the group. Several model-based propositional belief merging operators have been proposed which use propositional satisfaction. In this paper we introduce the notion of partial satisfactibility which is an alternative way of measure the satisfaction of a formula since this notion let us have satisfaction values on [0,1]. Partial satisfactibility allows us to define modelbased merging operators. An interesting point is that our proposal produces similar results than other merging approaches but without using distance measures. While in the literature it is required many merging operators in order to get satisfying results for different scenarios our proposal obtains similar results for all these different scenarios with a unique operator. Another important point is that our approach unlike most of the model-based approaches considers the case when the belief bases are	belief revision	Maria del Pilar Pozos Parra;Verónica Borja Macías	2006			machine learning;merge (version control);discrete mathematics;distance measures;operator (computer programming);artificial intelligence;satisfiability;mathematics	DB	-12.782307446327577	-0.825003483699327	75643
5cd838fb2e9ac0d6b536adc133f64ef21c9fa9cc	competition fosters trust	trust;reputation;competition	We study the effects of reputation and competition in a stylized market for experience goods. If interaction is anonymous, such markets perform poorly: sellers are not trustworthy, and buyers do not trust sellers. If sellers are identifiable and can, hence, build a reputation, efficiency quadruples but is still at only a third of the first best. Adding more information by granting buyers access to all sellers’ complete history has, somewhat surprisingly, no effect. On the other hand, we find that competition, coupled with some minimal information, eliminates the trust problem almost completely.	trust (emotion)	Steffen Huck;Gabriele K. Lünser;Jean-Robert Tyran	2012	Games and Economic Behavior	10.1016/j.geb.2012.06.010	industrial organization;microeconomics;business;commerce;forward auction	ECom	-4.724470114519583	-7.327661904218845	75689
b9760cfc45638d2ab40ef24d0eca8344f3e935d6	strength of preference in graph models for multiple-decision-maker conflicts	etude sur modele;preference theory;matematicas aplicadas;mathematiques appliquees;resolucion conflicto;resource management;model study;relation binaire;prise decision;decision maker;human behavior;structure stability;stabilite structurale;gestion recursos;estabilidad estructural;resolution conflit;multiple decision;theorie preference;estudio sobre modelo;binary relation;gestion ressources;decision multiple;teoria preferencia;graph model;water resource management;applied mathematics;toma decision;conflict resolution	Models of strength of preference are incorporated into the graph model for conflict resolution to study realistically multi-objective decision making situations in disputes with any finite number of participants. A decision maker’s preference is expressed using a triplet of binary relations that allows preference of one state over another to be strong or weak, and also allows indifference. Four basic stability definitions used in the graph model to represent human behavior in conflicts are extended to the context of the new triplet preference structure, producing strong and weak stabilities. Theorems governing the relationships among standard, strong, and weak stabilities are presented. Finally, application of the new preference representation to an international water resource management conflict confirms the practical utility of the triplet structure and the significance of the strategic insights it can provide. 2005 Elsevier Inc. All rights reserved.	decision support system;information;nash equilibrium;shingled magnetic recording;stable model semantics;triplet state;vertex-transitive graph	Luai Hamouda;D. Marc Kilgour;Keith W. Hipel	2006	Applied Mathematics and Computation	10.1016/j.amc.2005.11.109	decision-making;applied mathematics;artificial intelligence;resource management;conflict resolution;binary relation;mathematics;human behavior	AI	-6.709800858536145	-0.4289830994431517	75761
ede3d9f0ba7b838265c64ecc1ccfb0e1215d8df7	tacit collusion in repeated auctions	asymptotic efficiency;independent private values;repeated game;tacit collusion;folk theorem	Tacit Collusion in Repeated Auctions* by Andreas Blume and Paul Heidhues We study tacit collusion in repeated auctions in which bidders can only observe past winners and not their bids. We adopt a stringent interpretation of tacit collusion as collusion without communication about strategies that we model as a symmetry restriction on repeated game strategies: Strategies cannot discriminate among initially nameless bidders until they have become named through winning an auction. We obtain three classes of results: (1) Completely refraining from using names, i.e. strengthening the symmetry constraint, rules out collusion altogether, and even if naming is permitted, as per our definition of tacit collusion, the lack of communication limits collusive strategies and payoffs among impatient bidders. (2) If communication is allowed, there are sustained improvements over bid rotation and competitive bidding among patient bidders. (3) These gains extend to tacit collusion among patient bidders. However, whether tacit or not, collusion need not be efficient.	tacit programming	Andrzej Skrzypacz;Hugo Hopenhayn	2004	J. Economic Theory	10.1016/S0022-0531(03)00128-5	economics;public economics;folk theorem;repeated game;microeconomics;mathematical economics;welfare economics	Theory	-5.985788281299984	-3.5942014928380854	76096
f387b1792a43a785f3bd8ed39b44fb91fb36e089	herding with costly information		We consider a standard sequential decision to adopt/buy a good in a herding environment. The setup is same as in Sgroi (2002). Contrary to the basic herding case we introduce a cost that the agents have to pay for the information about their predecessorsu0027 actions. All agents receive informative signals as in the standard herding models but do not view the actions taken by their predecessors unless they pay the observation costs. In this set up the first and the second agents rely on their own signals when they make the decision to adopt/buy the good. Only the third agent is willing to buy the information on all of the preceding agentsu0027 actions. All agents following the third agent buy information on only one agentu0027s action and decide to adopt/buy the good after updating their beliefs. What follows is that the two first agentsu0027 actions determine whether the rest of the agents will adopt/buy the good or not when information about the predecessorsu0027 actions is cheap enough. If the cost of the information about the predecessorsu0027 actions is very expensive then all the agents will act according to their own signals. If observing is free one gets the standard results. In essence we identify a discontinuity in the basic herding model since the herding arises deterministically when a small observation cost is introduced.		S. Nageeb Ali	2018	J. Economic Theory	10.1016/j.jet.2018.02.009	microeconomics;social learning;herding;observational learning;financial market;economics;public history;if and only if	ECom	-7.021822407315777	-6.293252492896271	76233
621d783358453bfdcf675b35ef45f017f0cc7d09	an inquiry dialogue system	agent interaction;dialogue system;argumentation;cooperation;dialogue;inquiry;knowledge;shared knowledge;games;dialogue games	The majority of existing work on agent dialogues considers negotiation, persuasion or deliberation dialogues; we focus on inquiry dialogues, which allow agents to collaborate in order to find new knowledge. We present a general framework for representing dialogues and give the details necessary to generate two subtypes of inquiry dialogue that we define: argument inquiry dialogues allow two agents to share knowledge to jointly construct arguments; warrant inquiry dialogues allow two agents to share knowledge to jointly construct dialectical trees (essentially a tree with an argument at each node in which a child node is a counter argument to its parent). Existing inquiry dialogue systems only model dialogues, meaning they provide a protocol which dictates what the possible legal next moves are but not which of these moves to make. Our system not only includes a dialogue-game style protocol for each subtype of inquiry dialogue that we present, but also a strategy that selects exactly one of the legal moves to make. We propose a benchmark against which we compare our dialogues, being the arguments that can be constructed from the union of the agents’ beliefs, and use this to define soundness and completeness properties that we show hold for all inquiry dialogues generated by our system.	benchmark (computing);defeasible logic;dialog system;dialog tree;embedded system;logic programming;tree (data structure)	Elizabeth Black;Anthony Hunter	2008	Autonomous Agents and Multi-Agent Systems	10.1007/s10458-008-9074-5	games;knowledge management;inquiry;artificial intelligence;knowledge;cooperation	AI	-13.197582715372139	-1.1335491873696226	76245
04ee04eb5cdc4ab7d8fedddfd090cd04d57c8614	economic design of cumulative sum control charts for monitoring a process with correlated samples	62k05;cost function;economic design;62p30;swinburne;control chart;process parameters;sensitivity analysis;cumulative sum chart;optimal design;genetic algorithm;cumulant;correlation coefficient;correlated data	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	chart;francis;primary source	M. H. Lee	2010	Communications in Statistics - Simulation and Computation	10.1080/03610918.2010.524333	econometrics;control chart;genetic algorithm;optimal design;mathematics;shewhart individuals control chart;x-bar chart;sensitivity analysis;statistics;cumulant	Robotics	-14.319098237659317	-5.609000449155111	76408
580ce8888e7b8539deaa59c4322142c416d0488b	endogenous networks, social games, and evolution	scale effect;nash equilibrium;equilibrium selection;coordination game;network externality;network formation;stochastic stability	This paper studies a social game where agents choose their partners as well as their actions. Players interact with direct and indirect neighbors in the endogenous network. We show that the architecture of any nontrivial Nash equilibrium is minimally connected, and equilibrium actions approximate a symmetric equilibrium of the underlying game. We apply the model to analyze stochastic stability in 2×2 coordination games. We find that long-run equilibrium selection depends on a trade-off between efficiency and risk dominance due to the presence of scale effects arising from network externalities. Our results suggest a general pattern of equilibrium selection.  2005 Elsevier Inc. All rights reserved. JEL classification: C72; C73	approximation algorithm;nash equilibrium;social network game	Daniel A. Hojman;Adam Szeidl	2006	Games and Economic Behavior	10.1016/j.geb.2005.02.007	price of stability;implementation theory;markov perfect equilibrium;epsilon-equilibrium;best response;mertens-stable equilibrium;sequential equilibrium;trembling hand perfect equilibrium;coordination game;economics;network formation;folk theorem;network effect;repeated game;stochastic game;correlated equilibrium;microeconomics;risk dominance;mathematical economics;subgame perfect equilibrium;welfare economics;equilibrium selection;symmetric game;solution concept;nash equilibrium;symmetric equilibrium	ECom	-4.7940726524890644	-2.4480598781418768	76865
594d2dfffb0811fa8b41f036b9d0f7dfb536cd4f	a simulation of the mission crew workload in a multi mission aircraft	aerospace computing;aircraft;digital simulation;military computing;personnel;extend;imagine that inc.;uk air warfare centre;aircraft technologies;generic simulation approach;manning burden;mission crew workload simulation;model supervisory roles;modular definition;multi-mission aircraft;optimum capability;relief crew;rest periods;sensitivities;skills mixture;training;training burden;variations	The increased tasking and reduced manning of a multi mission aircraft put pressures on the mission-crew in terms of their workload, mixture of skill and the training required. This paper describes the simulation undertake by CORDA for the UK Air Warfare Centre. Within the model, written in EXTEND™ (Imagine That Inc.), the mission is defined in a modular form, which allows the mission tasks to be developed and modified as the study progressed. Workloads are associated with each task which are then distributed amongst the mission-crew. Within the model supervisory roles, relief crew and rest periods are modelled. The design has provided a generic simulation approach to mission-crew loading which allows the analyst to study variations and sensitivities in a controlled manner. This allows the optimum capability to be assessed and the minimum training / manning burden to be associated with the introduction of new aircraft technologies and skills.	email;graphical user interface;high-level programming language;operations research;requirement;simulation	Phillip Martin;Christopher S. Watson;Andrew L Skinner	2001			simulation;modular form;aerospace engineering;engineering;mathematics	Robotics	-12.497341549521789	-8.62047121982666	77079
1b47dc12f24d70189f3256363f4cd3623ecd0fe3	engineering and verifying requirements for programmable self-assembling nanomachines	dna;biology computing;formal specification;formal verification;nanotechnology;probability;prism probabilistic symbolic model checker;chemical kinetics;goal refinements;goal-oriented requirements engineering;molecular components;molecular detectors;programmable dna nanotechnology;programmable self-assembling nanomachines;requirement engineering;software engineering methods;target molecules;dna nanotechnology;requirements engineering;molecular programming;safety;validation and verification	We propose an extension of van Lamsweerde’s goal-oriented requirements engineering to the domain of programmable DNA nanotechnology. This is a domain in which individual devices (agents) are at most a few dozen nanometers in diameter. These devices are programmed to assemble themselves from molecular components and perform their assigned tasks. The devices carry out their tasks in the probabilistic world of chemical kinetics, so they are individually error-prone. However, the number of devices deployed is roughly on the order of a nanomole, and some goals are achieved when enough of these agents achieve their assigned subgoals. We show that it is useful in this setting to augment the AND/OR goal diagrams to allow goal refinements that are mediated by threshold functions, rather than ANDs or ORs. We illustrate this method by engineering requirements for a system of molecular detectors (DNA origami “pliers” that capture target molecules) invented by Kuzuya, Sakai, Yamazaki, Xu, and Komiyama (2011). We model this system in the Prism probabilistic symbolic model checker, and we use Prism to verify that requirements are satisfied. This gives prima facie evidence that software engineering methods can be used to make DNA nanotechnology more productive, predictable and safe.	cognitive dimensions of notations;diagram;goal programming;kinetics internet protocol;model checking;nanorobotics;prism (surveillance program);requirement;requirements engineering;sakai project;sensor;software engineering;verification and validation	Robyn R. Lutz;Jack H. Lutz;James I. Lathrop;Titus H. Klinge;Eric R. Henderson;Divita Mathur;Dalia Abo Sheasha	2012	2012 34th International Conference on Software Engineering (ICSE)		verification and validation;formal verification;shape;computer science;systems engineering;engineering;theoretical computer science;software engineering;probability;formal specification;requirements engineering;probabilistic logic;programming language;self-assembly;dna;dna nanotechnology;computer engineering;satisfiability	SE	-17.295099094218784	-1.967717474444411	77607
3011197f669bc4263e0e60dfef6be421276a23b2	deciding the value 1 problem for $\sharp$ -acyclic partially observable markov decision processes		The value 1 problem is a natural decision problem in algorithmic game theory. For partially observable Markov decision processes with reachability objective, this problem is defined as follows: are there observational strategies that achieve the reachability objective with probability arbitrarily close to 1? This problem was shown undecidable recently. Our contribution is to introduce a class of partially observable Markov decision processes, namely ♯-acyclic partially observable Markov decision processes, for which the value 1 problem is decidable. Our algorithm is based on the construction of a two-player perfect information game, called the knowledge game, abstracting the behaviour of a ♯-acyclic partially observable Markov decision process M such that the first player has a winning strategy in the knowledge game if and only if the value of M is 1.		Hugo Gimbert;Youssouf Oualhadj	2014		10.1007/978-3-319-04298-5_25	markov decision process;mathematical optimization;markov kernel;combinatorics;discrete mathematics;partially observable markov decision process;decision rule;mathematics;markov model	AI	-5.497020054182151	3.4834722380911263	77631
77bd50b1f4e13fed6a08b1fd79859cc96cf93fe3	incentive games and mechanisms for risk management	game theory;risk management;satisfiability;noncooperative games;large scale;mechanism design;distributed algorithm	Incentives play an important role in (security and IT) risk management of a large-scale organization with multiple autonomous divisions. This paper presents an incentive mechanism design framework for risk management based on a game-theoretic approach. The risk manager acts as a mechanism designer providing rules and incentive factors such as assistance or subsidies to divisions or units, which are modeled as selfish players of a strategic (noncooperative) game. Based on this model, incentive mechanisms with various objectives are developed that satisfy efficiency, preference-compatibility, and strategy-proofness criteria. In addition, iterative and distributed algorithms are presented, which can be implemented under information limitations such as the risk manager not knowing the individual units’ preferences. An example scenario illustrates the framework and results numerically. The incentive mechanism design approach presented is useful for not only deriving guidelines but also developing computer-assistance systems for large-scale risk management.	autonomous robot;distributed algorithm;game theory;it risk management;iterative method;numerical analysis	Tansu Alpcan	2010	CoRR		mechanism design;game theory;distributed algorithm;simulation;economics;risk management;knowledge management;management science;microeconomics;mathematical economics;satisfiability	AI	-8.274777580386516	-6.559899494485163	77815
1765f1e313f14014489409efde391265ecabc00a	equilibria in quantitative concurrent games		Synthesis of finite-state controllers from high-level specifications in multi-agent systems can be reduced to solving multi-player concurrent games over finite graphs. The complexity of solving such games with qualitative objectives for agents, such as reaching a target set, is well understood resulting in tools with applications in robotics. In this paper, we introduce quantitative concurrent graph games, where transitions have separate costs for different agents, and each agent attempts to reach its target set while minimizing its own cost along the path. In this model, a solution to the game corresponds to a set of strategies, one per agent, that forms a Nash equilibrium. We study the problem of computing the set of all Pareto-optimal Nash equilibria, and give a comprehensive analysis of its complexity and related problems such as the price of stability and the price of anarchy. In particular, while checking the existence of a Nash equilibrium is NP-complete in general, with multiple parameters contributing to the computational hardness separately, two-player games with bounded costs on individual transitions admit a polynomial-time solution.	anarchy;high- and low-level;multi-agent system;np-completeness;nash equilibrium;pareto efficiency;polynomial;price of stability;robotics;time complexity	Shaull Almagor;Rajeev Alur;Suguman Bansal	2018	CoRR		mathematics;price of stability;mathematical optimization;mathematical economics;nash equilibrium;bounded function;price of anarchy;graph	AI	-5.523048617897122	3.426814698741041	77987
31786b1ef5026421e8056f2e4014990b5dc32c6f	new perspectives on reduction and emergence in physics, biology and psychology		This volume has grown out of a conference on Reduction and Emergence held in Paris, at the Ecole Normale Supérieure, 12–15 November, 2003. Traditionally, and until quite recently, emergence and reduction were taken to be contrary notions: a theory T1—or more often, the phenomena described by that theory—was taken to be emergent with respect to another theory T2 if and only if it is impossible to reduce T1 to T2, although T1 and T2 appear to describe and explain the same natural systems or phenomena. This doctrine has recently been challenged by new conceptions of emergent phenomena which allow them to be scientifically explained and even reduced. If the distinction between emergent and resultant phenomena is to be upheld, the task is then to ground emergence on a new criterion independent of reducibility. This is closely linked to the search for an account of scientific reduction that avoids conceiving it in terms of syntactic derivability, as did Nagel’s classical account. It is well known since Feyerabend’s, Popper’s and Kuhn’s work that Nagel’s standards for reduction were too strong to be met by most real pairs of theories. Indeed, many if not all scientific reductions are accompanied by corrections to the reduced theory. There are now several models of such “approximate” reductions, situated somewhere between full conservation and radical elimination. To mention only a few, one important idea was to abandon the requirement of derivation of the reduced T1 from the reducing theory T2, to replace it by the weaker requirement of derivation from T2 of a theory T1* analogous to T1. Structuralism, which represents relations between theories by way of the relations between their	approximation algorithm;emergence;reduction (complexity);resultant;situated;theory	Max Kistler	2006	Synthese	10.1007/s11229-006-9014-3	applied mathematics	NLP	-10.725697866455496	3.243046563261257	78045
2a4afadd37aa65270610d525fa7dc74727668c15	emergence and evolution of coalitions in buyer-seller networks	consumer electronics;coalition formation;social network;multi agent simulation	Summary. We investigate the dynamics of the creation, development, and breakup of social networks formed by coalitions of agents. As an application, we consider coalition formation in a consumer electronic market. In our model, agents have benefits and costs from establishing a social network by participating in a coalition. Buyers benefit in terms of volume discount and better match of their preferences. Sellers benefit in terms of better predictability of sales volumes. The model allows us to investigate the stability and size of the coalitions as well as the performance of the market in terms of utility of the agents. We find that the system exhibits three dierent dominating regimes: individual purchasing behaviour, i.e., no social network exists among the agents, formation of several heterogenous coalitions, i.e., a number of social networks which are not connected, as well as condensation to a giant coalition, i.e., a social network involving all agents.	emergence;expectation–maximization algorithm;social network;systems design	Frank Edward Walter;Stefano Battiston;Frank Schweitzer	2007		10.1007/978-3-540-71075-2_19	public relations;simulation;political science;socioeconomics	AI	-5.900093179044619	-9.601564647572527	78262
f93b082d8ffaf30b29656ffdf4bfcf2ab04cc626	buyer agent decision process based on automatic fuzzy rules generation methods	electronic commerce;software agents consumer behaviour data mining decision theory electronic commerce fuzzy logic fuzzy set theory;efficient reasoning mechanism;fuzzy knowledge base;buyer behaviour buyer agent decision process automatic fuzzy rule generation method software agent product negotiation electronic marketplace fuzzy logic efficient reasoning mechanism fuzzy knowledge base;software agent;fuzzy rules;rule based;buyer agent decision process;consumer electronics;data mining;fuzzy set theory;electronic marketplace;software agents;fuzzy logic;product negotiation;buyer behaviour;cost accounting;fuzzy rule base;general methods;decision theory;automatic fuzzy rule generation method;consumer behaviour;clustering algorithms fuzzy logic proposals cost accounting consumer electronics algorithm design and analysis humans;clustering algorithms;decision process;humans;proposals;algorithm design and analysis;knowledge base	Software Agents can assume the responsibility of finding and negotiating products on behalf of their owners in an electronic marketplace. In such cases, Fuzzy Logic can provide an efficient reasoning mechanism especially for the buyer side. Agents representing buyers can rely on a fuzzy rule base in order to reason for their next action at every round of the interaction process with sellers. In this paper, we describe a model where the buyer builds its fuzzy knowledge base using algorithms for automatic fuzzy rules generation based on data provided by experts and compare a set of such algorithms. Owing to such algorithms, agent developers spend less time and effort for the definition of the underlying rule base. Moreover, the rule base is efficiently created through the use of the dataset indicating the behaviour of the buyer and, thus, representing its line of actions in the electronic marketplace. In our work, we use such algorithms for the definition of the buyer behaviour and we provide critical insides for every algorithm describing their advantages and disadvantages. Moreover, we present numerical results for every basic parameter of the interaction process, such as the time required for the rule base generation, the Joint Utility of the interaction process or the value of the acceptance degree that each algorithm results.	algorithm;fuzzy logic;fuzzy rule;knowledge base;numerical analysis;rule-based system;software agent;value (ethics)	Roi Arapoglou;Kostas Kolomvatsos;Stathes Hadjiefthymiades	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584416	knowledge base;computer science;knowledge management;artificial intelligence;software agent;data mining	AI	-9.300860247449767	-7.743854521314173	78282
721d41807f6165412579bb4a78080c22eab6aa2c	the art of conversation: eliciting information from experts through multi-stage communication	information transmission;decision maker;strategic interaction;cheap talk;face to face	We examine the strategic interaction between an informed expert and an uninformed decision maker, extending the analysis of Crawford and Sobel (Econometrica 50 (1982) 1431). We modify their model to allow for more extensive communication between the two parties and show that face-to-face communication between the expert and the uninformed decision maker followed by a written report from the expert leads to improved information transmission. In (almost) all cases, there exists an equilibrium in our modified model that ex ante Pareto dominates all of the equilibria identified by Crawford and Sobel. This remains true even if the expert’s bias is so great that in their model no information would be disclosed. r 2004 Elsevier Inc. All rights reserved. JEL classification: C72; C82	pareto efficiency;sobel operator	Vijay Krishna;John Morgan	2004	J. Economic Theory	10.1016/j.jet.2003.09.008	cheap talk;decision-making;economics;microeconomics;mathematical economics	AI	-5.70510914329455	-5.598966762085611	78393
47beacc54d355a479687860d2d02ab1774c1b8ca	credo: a military decision-support system based on credal networks	high level fusion procedures;bayes methods probabilistic logic decision support systems spread spectrum communication software tools visualization;belief networks;software tool;bayes methods;software tools belief networks decision support systems military computing sensor fusion;military experts;visualization;spread spectrum communication;military decision support system;decision support systems;convex sets;nato multinational experiment credo military decision support system credal networks software tool military domains bayesian networks convex sets system variables elicitation procedure military experts qualitative judgements high level fusion procedures space security;artificial intelligence;military domains;software tools;probabilistic logic;sensor fusion;credo;qualitative judgements;elicitation procedure;credal networks;space security;system variables;nato multinational experiment;military computing;bayesian networks	A software tool especially designed for military domains to create and query decision-support systems is presented. Credal networks, which are Bayesian networks whose parameters have the freedom to vary in convex sets, are used to model the relations among the system variables. A novel elicitation procedure of these sets, which allows the military experts to report their knowledge by purely qualitative judgements, is proposed. Two high-level fusion procedures to cope with multiple experts in this framework are also derived. All these features are supported by the software and demonstrated in an application to space security tested during the last NATO multinational experiment.	bayesian network;convex set;decision support system;graphical model;high- and low-level;programming tool;semantic network	Alessandro Antonucci;David Huber;Marco Zaffalon;Philippe Luginbuhl;Ian Chapman;Richard Ladouceur	2013	Proceedings of the 16th International Conference on Information Fusion		engineering;artificial intelligence;data mining;operations research	ML	-17.975232393821	-5.487069928200103	78771
4803e5f071462e907e8dc4c23f38fbb479b3ca8f	erratum to: arguments from authority and expert opinion in computational argumentation systems		It is interesting to note that some of the classic cases of argument from authority combine argument from expert opinion with argument from administrative authority. It is also interesting to see how CAS can model cases of evidential reasoning in law where the argument from expert opinion type of authority can conflict with the argument from administrative authority. In Fig. 4, an argument from expert opinion is used as evidence to support the proposition that Smith is guilty of murder, the ultimate claim to be proved by the prosecution (Fig. 4). Let’s assume that this argument fits the requirements for the scheme for argument from expert opinion as indicated by the notation ?ex in the argument diagram. Here we have simplified the argument for purposes of illustration by omitting the implicit premises that the matching of the DNA samples shows that Smith was at the crime scene and that this evidence, taken along with the other elements of the crime of murder in a given jurisdiction, provides an argument that supports the claim that Smith is guilty of murder. But let’s look at the counter-argument at the bottom of the diagram. Let’s say that this argument is a legitimate instance of the scheme for the administrative argument from authority, as indicated by the notation ?ad in the argument node. The minus sign in the er node indicates that this whole line of argument is a counter-argument attacking the prior argument from expert opinion. In fact, it is shown as an undercutter of that argument, as indicated by the argument from administrative authority being directed to the argument node a1. In CAS, an undercutting argument represents the kind of critical question classified as an exception. In this instance, what it means is that an expert opinion argument based on DNA evidence is generally accepted as a persuasive form of argument in the courts, but a defeasible one that can be refuted if an exception to the general rule is found. In this instance, once the law is stated, along with a citation indicating its source, the undercutting argument -er (exception to a rule) defeats the The online version of the original article can be found under doi:10.1007/s00146-016-0666-3.	argument map;computation;defeasible reasoning;diagram;fits;fractal dimension;login;requirement	Douglas Walton;Marcin Koszowy	2016	AI & SOCIETY	10.1007/s00146-016-0673-4	knowledge management;artificial intelligence;management science	AI	-13.024809741642061	1.233650052632958	79038
91a769115052d7d61b4fba541bb52da9b0f7fe71	discovering patterns in sequences of events	multiple description;decouverte;metodologia;learning;presentation;methodologie;apprentissage;shape;induction;sequence evenement;discoveries;sequence prediction;methodology;forme;inference	Abstract Given a sequence of events (or objects), each characterized by a set of attributes, the problem considered is to discover a rule characterizing the sequence and able to predict a plausible sequence continuation. The rule, called a sequence-generating rule , is nondeterministic in the sense that it does not necessarily tell exactly which event must appear next in the sequence, but rather, defines a set of plausible next events. The basic assumption of the methodology presented here is that the next event depends solely on the attributes of the previous events in the sequence. These attributes are either initially given or can be derived from the initial ones through a chain of inferences. Three basic rule models are employed to guide the search for a sequence-generating rule: decomposition, periodic, and disjunctive normal form (DNF). The search process involves simultaneously transforming the initial sequences to derived sequences and instantiating models to find the best match between the instantiated model and the derived sequence. A program, called SPARC/E, is described that implements most of the methodology as applied to discovering sequence generating rules in the card game Eleusis. This game, which models the process of scientific discovery, is used as a source of examples for illustrating the performance of SPARC/E.		Thomas G. Dietterich;Ryszard S. Michalski	1985	Artif. Intell.	10.1016/0004-3702(85)90003-7	shape;computer science;artificial intelligence;methodology;data mining;algorithm	ML	-18.908754398227487	-2.105109329058361	79089
cf02d19320eb0c774e21eb225c1c54b812e8b3c2	epistemic contextualism defended	humanidades;filosofia etica;epistemology;contextualism;disagreement;context sensitivity	Epistemic contextualists think that the extension of the expression ‘knows’ (and its cognates) depends on and varies with the context of utterance. In the last 15 years or so this view has faced intense criticism. This paper focuses on two sorts of objections. The first are what I call the ‘linguistic objections’, which purport to show that the best available linguistic evidence suggests that ‘knows’ is not context-sensitive. The second is what I call the ‘disagreement problem’, which concerns the behaviour of ‘knows’ in disagreement reports. These may not be the only objections to epistemic contextualism, but they are probably the most influential. I argue that the best current epistemic contextualist response to the linguistic objection is incomplete, and I show how it can be supplemented to deal with the full range of linguistic objections. I also develop a new solution to the disagreement problem. The upshot is that neither sort of objection gives us any reason to reject epistemic contextualism. This conclusion is, in a sense, negative—no new arguments for epistemic contextualism are advanced—but it’s a vital step towards rehabilitating the view.	context-sensitive grammar;context-sensitive language	Robin McKenna	2014	Synthese	10.1007/s11229-014-0572-5	philosophy;epistemology	NLP	-13.332399280720658	3.308436194137453	79151
49a6d80d052bcda1d7c88932328b810601429067	the metalogic of economic predictions, calculations and propositions	economic calculation;social choice;formal systems;economic equilibria;gödel undecidability	Indeterminacy is a matter of concern in the analysis of ideal forms and this paper shows that Giidel incompleteness and undecidability directly pertain to thf analysis of theoretical economic systems specifically, that cereain solution concepts such as ‘predictions of characteristics of policy outcomes guided by a social welfare function’, ‘the existence of equilibrium’, ‘the existence of welfare optima’ are subject to GGdel undecidability. This consideration brings into question the convention of a finite decision unit or economic actor, and the paper considers moreappropriate (metatheoretic) assumption structures and the implications of specifying richer information structures in microeconomics and choice theory.	computable function;decision problem;finite element method;indeterminacy in concurrent computation;linear function;social inequality;sound quality;spontaneous order;subject matter expert turing test;subroutine;turing machine;undecidable problem	Peter S. Albin	1982	Mathematical Social Sciences	10.1016/0165-4896(82)90016-6	social science;economics;public economics;mathematics;microeconomics;mathematical economics;law;welfare economics	ECom	-8.706961527308975	-0.8976721134179565	79340
9526aaa0ce69bd480ecb78d2aa15bbf4ef6a5941	uniform folk theorems in repeated anonymous random matching games	anonymous random matching;repeated games;working paper;uniform equilibria	We study infinitely repeated anonymous random matching games played by communities of players, who only observe the outcomes of their own matches. It is well known that cooperation can be sustained in equilibrium for the prisoner’s dilemma, but little is known beyond this game. We study a new equilibrium concept, strongly uniform equilibrium (SUE), which refines uniform equilibrium (UE) and has additional properties. We establish folk theorems for general games and arbitrary number of communities. We extend the results to a setting with imperfect private monitoring, for the case of two communities. We also show that it is possible for some players to get equilibrium payoffs that are outside the set of individually rational and feasible payoffs of the stage game. As a by-product of our analysis, we prove that, in general repeated games with finite players, actions, and signals, the sets of UE and SUE payoffs coincide.	nash equilibrium;prisoner's dilemma	Joyee Deb;Julio González-Díaz;Jérôme Renault	2016	Games and Economic Behavior	10.1016/j.geb.2016.08.006	economics;folk theorem;repeated game;mathematics;stochastic game;microeconomics;mathematical economics;outcome;welfare economics;equilibrium selection;symmetric game;symmetric equilibrium	ECom	-4.583306949176767	-2.048296086473635	79373
71a50d91b7306c3e5fb738a670b4c7e647df3159	mechanism design with partial verification and revelation principle	truthful implementation;partial verification;social choice;revelation principle;mechanism design	In the case of mechanism design with partial verification, where agents have restrictions on misreporting, the Revelation Principle does not always hold. Auletta et al. (J Auton Agent Multi-Agent Syst, to appear) proposed a characterization of correspondences for which the Revelation Principle holds, i.e., they described restrictions on misreporting under which a social choice function is implementable if and only if it is truthfully implementable. In this paper, we demonstrate that the characterization proposed in [1] is incorrect, and, building on their work, give a correct characterization. We also provide an example that demonstrates that our characterization is different from that of Auletta et al.	auton	Lan Yu	2010	Autonomous Agents and Multi-Agent Systems	10.1007/s10458-010-9151-4	mechanism design;social choice theory;revelation principle	AI	-9.331908839220388	0.6796826735290373	79400
c002b202275c9c14263cb8e1c8321734073eb678	a value based on marginal contributions for multi-alternative games with restricted coalitions		This paper deals with cooperative games with n players and r alternatives which are called multi-alternative games. In the conventional multi-alternative games initiated by Bolger, each player can choose any alternative with equal possibilities. In actual social life, there exist situations in which players have some restrictions on their choice of alternatives. Considering such situations, we study restricted multi-alternative games. A value for a given game is proposed.	existential quantification;marginal model	Satoshi Masuya;Masahiro Inuiguchi	2010	Kybernetika		mathematical optimization;mathematics	ECom	-7.365957631584798	-2.6543651927170737	79614
861d2b7ea542815179ecd842fff8ff022896d0c5	sequential voting with abstention	information aggregation;political economy;information cascades	Dekel and Piccione (2000) have proven that information cascades do not necessarily affect the properties of information aggregation in sequential elections: under standard conditions, any symmetric equilibrium of a simultaneous voting mechanism is also an equilibrium of the correspondent sequential mechanism. We show that when voters can abstain, these results are sensitive to the introduction of an arbitrarily small cost of voting: the set of equilibria in the two mechanisms are generally disjoint; and the informative properties of the equilibrium sets can be ranked. If an appropriate q-rule is chosen, when the cost of voting is small the unique symmetric equilibrium of the simultaneous voting mechanism dominates all equilibria of the sequential mechanism.	information cascade	Marco Battaglini	2005	Games and Economic Behavior	10.1016/j.geb.2004.06.007	sequential equilibrium;economics;microeconomics;cardinal voting systems;mathematical economics;welfare economics;anti-plurality voting;information cascade	AI	-5.074229392732699	-2.909665416246157	79778
eb7fb0c426dd7973556476ad33f8be9e9ef1f5a2	existence and uniqueness of ordinal nash outcomes	expected utility;nash bargaining solution;journal of economic literature;necessary and sufficient condition;existence and uniqueness	"""In tiffs paper we present necessary and sufficient conditions Ibr existence and uniqueness of ordinal Nash outcomes. These outcomes are derived from the ordinal Nash solution a reinterpretation and an extension of the Nash bargaining solution that allows bargainers to have preference relations that are more general than expected utility. Out task is undertaken by the construction of a new notion called """"induced utilities'.Journaltt/Ecommfic LiteratmeClassification Number: C78. ~!, 2000 Academic Press"""	expected utility hypothesis;nash equilibrium;ordinal data	Eran Hanany;Zvi Safra	2000	J. Economic Theory	10.1006/jeth.1999.2601	bargaining problem;epsilon-equilibrium;mathematical optimization;best response;economics;expected utility hypothesis;finance;mathematical economics;welfare economics	Theory	-6.194000885662688	-1.4519096613552445	79779
44b91911ed81b8bf2bd8f2b96de5ddd7dcd18b6e	the role of incentives for opening monopoly markets: comparing gte and boc cooperation with local entrants	arbitration;telecommunications	While the 1996 Telecommunications Act requires all incumbent local telephone companies to cooperate with local entrants, section 271 of the Act provides the Bell companies—but not GTE—additional incentives to cooperate. Using an original data set, I compare the negotiations of AT&T, as a local entrant, with GTE and with the Bell companies in states where both operate. My results suggest that the differential incentives matter: The Bells accommodate entry more than does GTE, as evidenced in quicker agreements, less litigation, and more favorable prices offered for network access. Consistent with this, there is more entry into Bell territories. * I wish to thank my dissertation advisor, Marius Schwartz, and my committee members, John Mayo, Serge Moresi and Steven Olley. I also wish to thank Scott Bohannon, Richard Clarke, Joseph Farrell, Alex Raskovich, Pierre Régibeau, Michael Riordan, and FCC seminar participants. Financial support from AT&T is gratefully acknowledged. All errors are, of course, my own. † Georgetown University and Autorità per le Garanzie nelle Comunicazioni, Servizio Analisi Economiche e di Mercato, Centro Direzionale—Isola B/5, Palazzo Torre Francesco, 80143 Napoli, ITALY. email: f.mini@agcom.it “The big difference between us and them [GTE] is they’re already in long distance...what’s their incentive?” Ameritech’s CEO Richard Notebaert, Washington Post, October 23, 1996.	access network;edmund m. clarke;email;monopoly;plan 9 from bell labs;steven anson coons	Federico Mini	2001	CoRR		industrial organization;economics;marketing;operations management;microeconomics;economy;arbitration;law	ML	-6.974941162179765	-4.499651732527733	79781
85bbd55fbd0a58e1bb52e25e1280cb226214bdc3	machining processes time calculating tool integrated in computer aided process planning (capp) for small and medium enterprises (smes)	design tool;computer aided process planning;real time;concurrent engineering tools;production control;small and medium enterprise;planning and production control;tool integration;process planning;concurrent engineering	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	algorithm;computer-aided design;dots per inch;emoticon;francis;nl (complexity);nos/ve;primary source;usability;whole earth 'lectronic link	R. Blanch;Narcis Pellicer;Maria Luisa Garcia-Romeu;Joaquim Ciurana	2011	Int. J. Computer Integrated Manufacturing	10.1080/09511921003608023	computer science;systems engineering;engineering;operations management;concurrent engineering;manufacturing engineering	Robotics	-15.473982678632622	-4.331567733258241	79847
8b51258403ece17fbf097803797a43add7f71528	negowiki: a set of community tools for the consistent comparison of negotiation approaches	community tool;recent research line;negotiation community;automated negotiation;complex scenario;specific negotiation scenario;benchmark different negotiation approach;negotiation scenario;different research line;different approach;automated complex negotiation;consistent comparison	There is a number of recent research lines addressing automated complex negotiations. Most of them focus on overcoming the problems imposed by the complexity of negotiation scenarios which are computationally intractable, be it by approximating these complex scenarios with simpler ones, or by developing heuristic mechanisms to explore more efficiently the solution space. The problem with these mechanisms is that their evaluation is usually restricted to very specific negotiation scenarios, which makes very difficult to compare different approaches, to re-use concepts from previous mechanisms to create new ones or to generalize mechanisms to other scenarios. This makes the different research lines in automated negotiation to progress in an isolated manner. A solution to this recurring problem might be to create a collection of negotiation scenarios which may be used to benchmark different negotiation approaches. This paper aims to fill this gap by providing a framework for the characterization and generation of negotiation scenarios intended to address this problem. The framework has been integrated in a website, called the Negowiki, which allows to share scenarios and experiment results with the negotiation community, facilitating in this way that researchers compare and share their advancements.		Ivan Marsá-Maestre;Mark Klein;Enrique de la Hoz;Miguel A. López-Carmona	2011		10.1007/978-3-642-25044-6_34	simulation;computer science;knowledge management;artificial intelligence;distributed computing;management science	HCI	-14.69994074656249	-9.690160461139694	79880
9bd5598fce86d48521d5a4f470cb676d650d576d	the bounded core for games with restricted cooperation	ограниченное с ядро;кооперативная игра	A game with restricted (incomplete) cooperation is a triple (N, v, Ω), where N represents a finite set of players, Ω ? 2N is a set of feasible coalitions such that N ? Ω, and v: Ω ? R denotes a characteristic function. Unlike the classical TU games, the core of a game with restricted cooperation can be unbounded. Recently Grabisch and Sudholter [9] proposed a new solution concept--the bounded core--that associates a game (N, v,Ω) with the union of all bounded faces of the core. The bounded core can be empty even if the core is nonempty. This paper gives two axiomatizations of the bounded core. The first axiomatization characterizes the bounded core for the class Gr of all games with restricted cooperation, whereas the second one for the subclass Gbcr ? Gr of the games with nonempty bounded cores.		Elena B. Yanovskaya	2016	Automation and Remote Control	10.1134/S0005117916090162	combinatorics;discrete mathematics;mathematics;mathematical economics;bounded function	EDA	-5.99525580392839	-0.449966763182769	80010
7981f1c17d522b40650372a5885c8358bb4e967e	conspicuous consumption dynamics	shock wave;veblen effects gradient dynamics shock waves;interdependent preferences;conspicuous consumption	We formalize Veblen’s idea of conspicuous consumption as two alternative forms of interdependent preferences, dubbed envy and pride. Agents adjust consumption patterns gradually, in the direction of increasing utility. From an arbitrary initial state, the distribution of consumption among agents with identical preferences converges to a unique equilibrium distribution. When pride is stronger, the equilibrium distribution has a right-skewed density. When envy is stronger, the equilibrium is concentrated at a single point, and the adjustment dynamics involve a shock wave that can be interpreted as a growing, moving, homogeneous “middle class.”	interdependence;markov chain	Daniel Friedman;Daniel N. Ostrov	2008	Games and Economic Behavior	10.1016/j.geb.2007.12.008	economics;conspicuous consumption;economy;mathematical economics;welfare economics;shock wave	ECom	-4.879264910196796	-5.252225471893515	80061
a97fbdf42ca4c455c4ed187f649db630dda4f988	a multi-agent temporal constraint satisfaction system based on allen's interval algebra and probabilities	interval algebra;temporal constraints;distributed information systems;group decision making;heuristics;distributed systems;distributed decision making systems;interactive problem solving	AbstrAct Many real-world problems can be viewed and represented as a constraint satisfaction problem (CSP). In addition, many of these problems are distributed in nature. To this end, we combine agents with a special type of CSP called an Interval Algebra network (IA network). An IA network is a graph where each node represents an interval. Directed edges in the network are labelled with temporal interval relations. A probabilistic IA network has probabilities associated with the relations on the edges that can be used to capture preferences. A probabilistic IA agent (PIA-Agent) is assigned a probabilistic IA network. PIA-Agent's networks are connected via edges. The overall goal is to make each PIA-Agent's network consistent and optimal. Each PIA-Agent is independent and has sole control over its network. But, it must communicate and coordinate with other PIA-Agents when modifying or updating edges that are shared between two PIA-Agents. We present an algorithm which allows the PIA-Agents to collab-oratively solve and recommend a temporal schedule. At the agent level, this schedule is optimal under the given local constraints. Although the global solution may not be optimal, we try to generate near optimal ones. Note that our distributed system is not centrally controlled. Our algorithm decides which PIA-Agent should be given an opportunity to update the solution next. Also, when a conflict is detected, the algorithm modifies the PIA-Agent execution order in order to deal with the inconsistency.	algorithm;allen's interval algebra;constraint satisfaction problem;distributed computing	Elhadi M. Shakshuki;André Trudel;Yiqing Xu	2007	IJITWE	10.4018/jitwe.2007040102	mathematical optimization;group decision-making;computer science;theoretical computer science;heuristics	AI	-13.074216859210997	-2.0778308315988827	80123
199dccaa75d602d4723ebd25dce777f0fb9039dc	decision and coordination in a dual-channel three-layered green supply chain		This paper investigated, for the first time, the game and coordination of a dual-channel, three-layered, green fresh produce supply chain, with regard to its economic, social, and environmental performance. Considering that the market demand is dual-channel priced and sensitive to the degree of greenness and the freshness-level, four game models, under different scenarios have been established. These included a centralized scenario, a decentralized scenario, and two contractual scenarios. The equilibrium solutions under the four scenarios were characterized. From the perspective of a sustainable development, the economic, social, and environmental performance of the supply chain was analyzed. To enhance the supply chain performance, two contract mechanisms were designed and the conditions for a multi-win outcome were obtained. Accordingly, many propositions and management implications were provided. The results showed that, (1) compared to the centralized supply chain case, the performance of the decentralized supply chain case is inferior; (2) in addition to increasing the concentration of the supply chain decisions, the two contracts proposed can effectively coordinate the green supply chain and improve its sustainable performance; and (3) the performance of the supply chain is positively driven by the consumers’ sensitivity to greenness degree and the freshness level of fresh produce. This paper fills a research gap and helps the participants of the channel recognize the operational decision principle of a complex green supply chain, in order to achieve a higher and a long-term sustainable-development performance.		Zilong Song;Shiwei He;Baifeng An	2018	Symmetry	10.3390/sym10110549	combinatorics;sustainable development;supply chain;environmental economics;mathematics;contract theory;communication channel;supply and demand	NLP	-5.967661601962911	-7.614793024494433	80154
02e4c9ca3ae25168b30a433bf0909809e67ca9e9	when more is less: limited consideration		There is well-established evidence that decision makers consistently fail to consider all available options. Instead, they restrict their attention to only a subset of alternatives and then undertake a more detailed analysis of this reduced set. This systematic lack of consideration of available options can lead to a “more is less” effect, where excess of options can be welfare-reducing for a decision-maker (DM). Building on this idea, we model individuals who might pay attention to only a subset of the choice problem presented to them. Within this smaller set, a DM is rational in the standard sense, and she chooses the maximal element with respect to her preference. We provide a choice theoretical foundation for our model. In addition, we show to which alternatives are revealed preferred to which, and discuss welfare implications. JEL Classification: D11, D81.		Juan Sebastián Lleras;Yusufcan Masatlioglu;Daisuke Nakajima;Erkut Y. Ozbay	2017	J. Economic Theory	10.1016/j.jet.2017.04.004	public economics;maximal element;welfare economics;economics;welfare;restrict	ECom	-5.971855630432327	-5.525060593845268	80156
d736265007050759bcf6825dcf59f6f408e607ce	a simulation-based approach for testing market strategies in electronic marketplaces	economic incentive;electronic commerce;pricing algorithm simulation based approach market strategies testing electronic marketplace electronic commerce internet software agents goods exchange buyer behaviour market simulator market strategies seller behaviour;pricing;software agent;electronic marketplace;software agents;marketing strategy;internet;electronic equipment testing consumer electronics pricing software agents algorithm design and analysis knowledge engineering analytical models vehicle dynamics electronic commerce internet;consumer behaviour;pricing electronic commerce internet software agents consumer behaviour	With the increasing importance of electronic commerce across the Internet it is becoming increasingly evident that in a few years the Internet will host large numbers of interacting software agents. A vast number of them will be economically motivated, and will exchange a variety of goods and services. It is therefore important to consider the economic incentives and behaviours of economic software agents, and to use every available means to anticipate their collective interactions. We address this concern by presenting a market simulator designed for analysing agent market strategies based on a complete understanding of buyer and seller behaviours, preference model and pricing algorithms.	simulation	Maria João Viamonte;Carlos Ramos;Fátima Rodrigues;José Carlos Cardoso	2003		10.1109/WI.2003.1241248	e-commerce;computer science;artificial intelligence;software agent;consumer behaviour	ECom	-8.378241917989472	-9.311039985843005	80158
f71f03ffb038924e82ab01fea2aeabe9afb33284	spatial cluster modelling		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Nicholas Rose	2004	Technometrics	10.1198/tech.2004.s748	econometrics;computer science;data science;statistics	Robotics	-14.608289573989694	-5.741298917538671	80175
e34cd9efd115f8806383376c9b19dadacb4668b4	an extension of the h-search algorithm for artificial hex players	game theory;algoritmo busqueda;algorithme recherche;search algorithm;teoria juego;intelligence artificielle;theorie jeu;artificial intelligence;inteligencia artificial	Hex is a classic board game invented in the middle of the twentieth century by Piet Hein and rediscovered later by John Nash. The best Hex artificial players analyse the board positions by deducing complex virtual connections from elementary connections using the H-Search algorithm. In this paper, we extend the H-search with a new deduction rule. This new deduction rule is capable of discovering virtual connections that the H-search cannot prove. Thanks to this new deduction rule, the horizon of the artificial Hex players should move further away.	algorithmic efficiency;computation;esoteric programming language;heuristic (computer science);hex;john collison;nash equilibrium;natural deduction;search algorithm;testbed	Rune Rasmussen;Frédéric Maire	2004		10.1007/978-3-540-30549-1_56	game theory;computer science;artificial intelligence;algorithm;search algorithm	AI	-11.624898616240943	-2.0157178016353137	80204
4d587a1c6a425e0fe93d74dd74922bf529aa54fd	leveraging possibilistic beliefs in unrestricted combinatorial auctions	distinguishable dominance;possibilistic beliefs;extensive form games;unrestricted combinatorial auctions;incomplete information;mutual belief of rationality	In unrestricted combinatorial auctions, we put forward a mechanism that guarantees a meaningful revenue benchmark based on the possibilistic beliefs that the players have about each other’s valuations. In essence, the mechanism guarantees, within a factor of two, the maximum revenue that the “best informed player” would be sure to obtain if he/she were to sell the goods to his/her opponents via take-it-or-leave-it offers. Our mechanism is probabilistic and of an extensive form. It relies on a new solution concept, for analyzing extensive-form games of incomplete information, which assumes only mutual belief of rationality. Moreover, our mechanism enjoys several novel properties with respect to privacy, computation and collusion.	benchmark (computing);computation;privacy;rationality	Jing Chen;Silvio Micali	2016	Games	10.3390/g7040032	economics;extensive-form game;microeconomics;mathematical economics;welfare economics;complete information	AI	-8.037795152958457	-4.212220526131909	80210
1254b79979a7fd35b23dce0e2b342c0a555be88a	markets are dead, long live markets	markets;resource allocation;performance;tycoon;design;economics;mechanism design;management	Researchers have long proposed using economic approaches to resource allocation in computer systems. However, few of these proposals became operational, let alone commercial. Questions persist about the economic approach regarding its assumptions, value, applicability, and relevance to system design. The goal of this paper is to answer these questions. We find that market-based resource allocation is useful, and more importantly, that mechanism design and system design should be integrated to produce systems that are both economically and computationally efficient.	algorithmic efficiency;relevance;systems design	Kevin Lai	2005	SIGecom Exchanges	10.1145/1120717.1120719	mechanism design;design;economics;performance;resource allocation;public economics;microeconomics;management;welfare economics	ECom	-8.388903548009743	-5.824738090983407	80291
94bb3f696a033be9194ab9609dc9de7e9e75a1a2	a type-2 fuzzy embedded agent for ubiquitous computing environments	fuzzy controller;life long learning;fuzzy control;adaptive control;test bed;type 1 fuzzy agents type 2 fuzzy embedded agent ubiquitous computing environments type 2 fuzzy controllers type 2 intelligent agent intelligent dormitory user behaviour;user behaviour;learning artificial intelligence ubiquitous computing control engineering computing fuzzy control adaptive control;intelligent agent;ubiquitous computing;control engineering computing;learning artificial intelligence;ubiquitous computing uncertainty fuzzy sets intelligent agent pervasive computing embedded computing ambient intelligence computer science fuzzy control fuzzy systems;ubiquitous computing environment	We describe a novel system for learning and adapting type-2 fuzzy controllers for intelligent agents that are embedded in ubiquitous computing environments (UCEs). Our type-2 agents operate non intrusively in an online life long learning manner to learn the user behaviour so as to control the UCE on the user's behalf. We have performed unique experiments in which the type-2 intelligent agent has learnt and adapted online to the user's behaviour during a stay of five days in the intelligent dormitory (iDorm) which is a real UCE test bed. We show how our type-2 agent deals with the uncertainty and imprecision present in UCEs to give a very good performance that outperform the type-1 fuzzy agents while using a smaller number of rules.	embedded system;experiment;fuzzy logic;intelligent agent;testbed;ubiquitous computing	Faiyaz Doctor;Hani Hagras;Victor Callaghan	2004	2004 IEEE International Conference on Fuzzy Systems (IEEE Cat. No.04CH37542)	10.1109/FUZZY.2004.1375565	fuzzy electronics;simulation;adaptive control;computer science;artificial intelligence;neuro-fuzzy;lifelong learning;ubiquitous robot;ubiquitous computing;intelligent agent;fuzzy control system;intelligent control;testbed	Robotics	-16.208865451127984	-8.522653251936855	80551
2c4cb020ba6ab076ea25d63cab7523ec2dbc8f26	conarg: argumentation with constraints		ConArg [3,4]4 is a tool based on Constraint Programming that is able to model and solve different problems related to (Abstract) Argumentation Frameworks (AFs) [6]. For the implementation we adopted JaCoP, which is a Java library that provides the user with a Finite Domain Constraint Programming paradigm [7]. Through its graphic interface, it is possible to select the extensions (e.g., admissible) the user wants to find, and to browse the obtained solutions. Constraint Programming (CP) [7] is a powerful paradigm for solving combinatorial search problems, which exploits a wide range of techniques from artificial intelligence and operations research. The basic idea in constraint programming is that the user states the constraints and a general purpose constraint solver is used to solve them. Constraints are just relations, and a Constraint Satisfaction Problem (CSP) [7] states which relations should hold among the variables. ConArg [3,4] is able to find all Dung’s classical extensions [6] (i.e., conflictfree, admissible, complete, stable, grounded and preferred extensions) by defining the properties of these extensions through constraints, and solving the related CSP. To show the feasibility of such solution, in [3,4] we test the tool on different randomly generated small-world networks (i.e., Barabasi and Kleinberg ones) and we report the performance of the search in time. Since the total number of these extensions may explode for large sets of arguments (particularly in case of conflict-free extensions, i.e., the less constrained ones), it is important to use techniques to tackle this inherent complexity, as CP-based ones. Moreover, ConArg can solve different classical hard-problems that concern weighted AFs (where attacks are associated with a “strength” value), as the ones related to weighted grounded extensions presented in [5]. For example, given a weighted argument system, a set of arguments S ⊆ Args and an inconsistency budget β (i.e., the tolerated sum of the considered strength values), to find if β is minimal w.r.t. S represents a co-NP-complete problem [5].	admissible heuristic;artificial intelligence;browsing;co-np;co-np-complete;combinatorial search;constraint logic programming;constraint programming;constraint satisfaction problem;entry point;graphical user interface;jacop (solver);java;np-completeness;operations research;procedural generation;programming paradigm;qp state machine frameworks;solver	Stefano Bistarelli;Francesco Santini	2012				AI	-7.6419828150851234	3.7882601670232	80755
488d861cd5122ae7e4ac89ff082b159c4889870c	ethical artificial intelligence.		This book-length article combines several peer reviewed papers and new material to analyze the issues of ethical artificial intelligence (AI). The behavior of future AI systems can be described by mathematical equations, which are adapted to analyze possible unintended AI behaviors and ways that AI designs can avoid them. This article makes the case for utility-maximizing agents and for avoiding infinite sets in agent definitions. It shows how to avoid agent self-delusion using model-based utility functions and how to avoid agents that corrupt their reward generators (sometimes called perverse instantiation) using utility functions that evaluate outcomes at one point in time from the perspective of humans at a different point in time. It argues that agents can avoid unintended actions (sometimes called basic AI drives or instrumental goals) by accurately learning human values. This article defines a self-modeling agent framework and shows how it can avoid problems of resource limits, being predicted by other agents, and inconsistency between the agentu0027s utility function and its definition (one version of this problem is sometimes called motivated value selection). This article also discusses how future AI will differ from current AI, the politics of AI, and the ultimate use of AI to help understand the nature of the universe and our place in it.	artificial intelligence	Bill Hibbard	2014	CoRR		computer science;machine learning;peer review;management science;infinite set;artificial intelligence	AI	-14.454652079894112	1.6988198325223838	81121
4022e78fcdd4b8986aa8a637acf0246fc60c0861	generating explanations for evidential reasoning	information content;evidential reasoning;sensitivity analysis	In this paper, we present two methods to provide explanations for reasoning with be­ lief functions in the valuation-based systems. One approach, inspired by Strat's method, is based on sensitivity analysis, but its com­ putation is simpler thus easier to implement than Strat 's. The other one is to examine the impact of evidence on the conclusion based on the measure of the information content in the evidence. We show the property of ad­ ditivity for the pieces of evidence that are conditional independent within the context of the valuation-based systems. We will give an example to show how these approaches are applied in an evidential network.	self-information;value (ethics)	Hong A Xu;Philippe Smets	1995			self-information;computer science;artificial intelligence;data mining;mathematics;evidential reasoning approach;sensitivity analysis;algorithm;statistics	NLP	-14.994835532485716	0.41896296893861984	81332
55b56f49adc1e128c0ddd3734e0fe1ce28602921	on the stability of a scoring rules set under the iac		A society facing a choice problem has also to choose the voting rule itself from a set of different possible voting rules. In such situations, the consequentialism property allows us to induce voters’ preferences on voting rules from preferences over alternatives. A voting rule employed to resolve the society’s choice problem is self-selective if it chooses itself when it is also used in choosing the voting rule. A voting rules set is said to be stable if it contains at least one self-selective voting rule at each profile of preferences on voting rules. We consider in this paper a society which will make a choice from a set constituted by three alternatives {a, b, c} and a set of the three well-known scoring voting rules {Borda, Plurality, Antiplurality}. Under the Impartial Anonymous Culture assumption (IAC), we will derive a probability for the stability of this triplet of voting rules. We use Ehrhart polynomials in order to solve our problems. This method counts the number of lattice points inside a convex bounded polyhedron (polytope). We discuss briefly recent algorithmic solutions to this method and use it to determine the probability of stabillity of {Borda, Plurality, Antiplurality} set.	polyhedron;polynomial;triplet state	Vincent Merlin;Mostapha Diss;Ahmed Louichi;Hatem Smaoui	2010			condorcet method;voting;anti-plurality voting;discrete mathematics;cardinal voting systems;algorithm;arrow's impossibility theorem;approval voting;lattice (group);bounded function;mathematics	AI	-8.188561662715994	-2.4393038315258413	82023
8ca7695af9513a5e93e02ef3845374b1877eb899	"""subjective hazard rates rationalize """"irrational"""" temporal preferences"""		Delay discounting refers to decision makers’ tendency to treat immediately consumable goods as more valuable than those only available after some delay. Previous work has focused on a seemingly irrational feature of these preferences: the systematic tendency to exhibit more patience when consequences are far in the future but less patience about those same, identical rewards as time passes. One explanation for delay discounting itself appeals to the risk implicitly associated with delayed rewards. The current study investigates whether the implicit risk hypothesis is capable of explaining the seemingly irrational shifts in patience by having participants make subjective risk judgments regarding a variety of real-world scenarios. To reduce the possibility of task demands, participants judged hazard rates rather than survival rates. Results suggest that the seemingly irrational shifts in patience are quite reasonable once participants’ beliefs about the relationship between delay and risk are taken into account.	consumability	Christian C. Luhmann;Michael Bixter	2014				HCI	-6.380371179081192	-7.4592975199411615	82053
5af9f268ad5560eb3283e432dba1401521585d23	the three bases for the enthymeme: a dialogical theory	dialogue system;formal dialogue systems;implicit commitments;argument visualization;common knowledge;argumentation schemes	In traditional logic, an enthymeme is said to be an argument, or chain of argumentation, with one or more missing (implicit) premises or conclusions. In this paper a new theory of enthymemes, based on recent developments in argumentation technology including argumentation schemes, argument visualization tools and formal dialogue systems, is put forward. The dialogical theory hypothesizes three bases for the enthymeme in a formal dialogue system CBVK: (1) the participants' commitment sets, (2) sets of argumentation schemes (especially including presumptive schemes) shared by both participants, and (3) a set of propositions representing common knowledge shared by both participants. The formal dialogue system CBVK is the backbone of the theory of enthymemes into which these three components are built. Three examples of enthymemes of a kind commonly found in everyday conversational argumentation are used to show how the theory applies.		Douglas Walton	2008	J. Applied Logic	10.1016/j.jal.2007.06.002	artificial intelligence;mathematics;algorithm;common knowledge	Logic	-15.92454279663861	3.476491526960983	82292
4a3a2e156543848ce56f3109e85f82a402da8eea	from individual goals to collective decisions		We introduce the problem of aggregating the individual goals of a group of agents to find a collective decision. Goals are represented by propositional formulas on a finite set of binary issues. We define some rules for carrying out the aggregation of goals and we show how to adapt axiomatic properties from the literature on Social Choice Theory to this setting. The type of problems we are interested in studying for our rules are axiomatic characterizations, as well as the computational complexity of computing the outcome.	axiomatic system;collective intelligence;computational complexity theory;multi-agent system;propositional proof system	Arianna Novaro;Umberto Grandi;Dominique Longin;Emiliano Lorini	2018			machine learning;computer science;artificial intelligence;computational complexity theory;axiom;finite set;binary number;social choice theory	AI	-12.339135074426675	-0.686806351912707	82306
0c6501526f44ab8a218c1258d6860f6c3a676d73	stealthy strategies for deception in hypergames with asymmetric information	game theory;probability;games probability distribution vectors markov processes stability analysis probabilistic logic extraterrestrial measurements;asymmetric information;probabilistic model;worst case max strategy stealthy strategy hypergames deception asymmetric information belief manipulation probabilistic model;vectors;probability distribution;games;markov process;stability analysis;probability game theory;markov processes;probabilistic logic;extraterrestrial measurements	This paper considers games with incomplete asymmetric information, where one player (the deceiver) has privileged information about the other (the mark) and intends to employ it for belief manipulation. We use hypergames to represent the asymmetric information available to players and assume a probabilistic model for the actions of the mark. This framework allows us to formalize various notions of deception in a precise way. We provide a necessary condition and a sufficient condition for deceivability when the deceiver is allowed to reveal information to the mark as the game evolves. For the case when the deceiver acts stealthily, i.e., restricts her actions to those that do not contradict the belief of the mark, we are able to fully characterize when deception is possible. Moreover, we design the worst-case max-strategy that, when such a sequence of deceiving actions exists, is guaranteed to find it. An example illustrates our results.	algorithm;best, worst and average case;directed graph;statistical model	Bahman Gharesifard;Jorge Cortés	2011	IEEE Conference on Decision and Control and European Control Conference	10.1109/CDC.2011.6160979	game theory;simulation;artificial intelligence;machine learning;mathematics;markov process;statistics	Robotics	-9.880094946573578	-3.477674615810678	82333
560e52963d51cc63cc6229a10fd0bebda98a5fa9	node and link allocation in network virtualization based on distributed constraint optimization	distributed constraint optimization problem;max-sum algorithm;virtual network embedding;resource allocation;verification;model checking	Virtual Networks (VNs) offer a flexible and economic approach to deploy customer suited networks. However, defining how resources of a physical network are used to support VNs requirements is a NP-hard problem. For this reason, heuristics have been used on mapping of virtual networks. Although heuristics do not ensure the optimal solution, they implement fast solutions and showed satisfactory results. This work presents a modeling of the node and link allocation problem using Distributed Constraint Optimization Problem (DCOP) with factor graphs, which is a formalism widely used in real distributed optimization problems. In our approach, we use the max-sum algorithm to solve the DCOP. Correctness criteria for this approach are discussed and verifications are conducted through model checking.	algorithm;anytime algorithm;belief propagation;correctness (computer science);dcop;distributed constraint optimization;factor graph;formal grammar;heuristic (computer science);location (geography);mathematical optimization;model checking;optimization problem;promela;requirement;spin model checker;scalability	Alexander Gularte;Odorico Machado Mendizabal;Raquel de Miranda Barbosa;Diana F. Adamatti	2017	Journal of Network and Systems Management	10.1007/s10922-017-9410-7	network virtualization;theoretical computer science;model checking;factor graph;computer science;distributed computing;correctness;heuristics;optimization problem;resource allocation;distributed constraint optimization	AI	-7.961146542022218	3.2982198590855423	82350
84a06810b54aa448f9dbb1ae9345d2d70c6b129a	composition of electricity generation portfolios, pivotal dynamics, and market prices	dynamic change;hd industries land use labor;market power;closed form solution;reseau electrique;electrical network;info eu repo semantics workingpaper;red electrica;technology diversification;monopolio;electricity market;prix marche;concurrence economique;monopole;market price;electricity generation;competition economy;electricity;precio de mercado;diversification;competencia economica;diversificacion;simulation model	We use simulations to study how the diversification of electricity generation portfolios influences wholesale prices. We find that the relationship between technological diversification and market prices is mediated by the supply to demand ratio. In each demand case there is a threshold where pivotal dynamics change. Pivotal dynamics preand post-threshold are the cause of non-linearities in the influence of diversification on market prices. The findings are robust to changes in the main market assumptions.	algorithm;computation;diversification (finance);elasticity (cloud computing);experiment;expressive power (computer science);real life;reinforcement learning;seasonality;simulation;time series;volatility	Albert Banal-Estañol;Augusto Rupérez Micola	2009	Management Science	10.1287/mnsc.1090.1067	diversification;electricity generation;electrical network;market price;economics;electricity market;marketing;electricity;microeconomics;economy;market economy;management;labour economics	ECom	-6.221716278857006	-9.22663168004641	82546
035e82158a324490cce326487c43e8c0d28e28b8	philippe flajolet, the father of analytic combinatorics	obituary;info computer science cs;math mathematics math;philippe flajolet	Philippe Flajolet, mathematician and computer scientist extraordinaire, suddenly passed away on March 22, 2011, at the prime of his career. He is celebrated for opening new lines of research in analysis of algorithms, developing powerful new methods, and solving difficult open problems. His research contributions will have impact for generations, and his approach to research, based on curiosity, a discriminating taste, broad knowledge and interest, intellectual integrity, and a genuine sense of camaraderie, will serve as an inspiration to those who knew him for years to come. The common theme of Flajolet's extensive and far-reaching body of work is the scientific approach to the study of algorithms, including the development of requisite mathematical and computational tools. During his forty years of research, he contributed nearly 200 publications, with an important proportion of fundamental contributions and representing uncommon breadth and depth. He is best known for fundamental advances in mathematical methods for the analysis of algorithms, and his research also opened new avenues in various domains of applied computer science, including streaming algorithms, communication protocols, database access methods, data mining, symbolic manipulation, text-processing algorithms, and random generation. He exulted in sharing his passion: his papers had more than than a hundred different co-authors and he was a regular presence at scientific meetings all over the world. His research laid the foundation of a subfield of mathematics, now known as analytic combinatorics. His lifework Analytic Combinatorics (Cambridge University Press, 2009, co-authored with R. Sedgewick) is a prodigious achievement that now defines the field and is already recognized as an authoritative reference. Analytic combinatorics is a modern basis for the quantitative study of combinatorial structures (such as words, trees, mappings, and graphs), with applications to probabilistic study of algorithms that are based on these structures. It also strongly influences other scientific domains, such as statistical physics, computational biology, and information theory. With deep historic roots in classical analysis, the basis of the field lies in the work of Knuth, who put the study of algorithms on a firm scientific basis starting in the late 1960s with his classic series of books. Flajolet's work takes the field forward by introducing original approaches in combinatorics based on two types of methods: symbolic and analytic. The symbolic side is based on the automation of decision procedures in combinatorial enumeration to derive characterizations of generating functions. The analytic side treats those functions as functions in the complex …	analysis of algorithms;applied computer science;book;computation;computational biology;computer scientist;data mining;information theory;mega man (original series);philippe kruchten;procedural generation;streaming algorithm	Bruno Salvy;Robert Sedgewick;Michèle Soria;Wojciech Szpankowski;Brigitte Vallée	2011	Theor. Comput. Sci.	10.1016/j.tcs.2011.05.048	mathematics;algorithm	Theory	-14.08806647644006	-2.6730626372551898	82800
ec3f942b74087c51a575632155e71a15c59b7512	multiattribute utility functions satisfying mutual preferential independence	grupo de excelencia;archimedean utility copula;multiattribute utility;ciencias basicas y experimentales;matematicas;preferential independence;grupo a	The construction of a multiattribute utility function is an important step in decision analysis. One of the most widely used conditions for constructing the utility function is the assumption of mutual preferential independence where trade-offs among any subset of the attributes do not depend on the instantiations of the remaining attributes. Mutual preferential independence asserts that ordinal preferences can be represented by an additive function of the attributes. This paper derives the most general form of a multiattribute utility function that (i) exhibits mutual preferential independence and (ii) is strictly increasing with each argument at the maximum value of the complement attributes. We show that a multiattribute utility function satisfies these two conditions if and only if it is an Archimedean combination of univariate utility assessments. This result enables the construction of multiattribute utility functions that satisfy additive ordinal preferences using univariate utility assessments and a single generating function. We also provide a nonparametric approach for estimating the generating function of the Archimedean form by iteration.		Ali E. Abbas;Zhengwei Sun	2015	Operations Research	10.1287/opre.2015.1350	econometrics;mathematics;mathematical economics;welfare economics;von neumann–morgenstern utility theorem	ML	-7.1703791492460835	-1.689011283053956	82935
4d47428ade37e7c9c3dd94525539d0bbf511650f	debugging with intelligence via probabilistic inference		We aim to debug a single failing execution without the assistance from other passing/failing runs. In our context, debugging is a process with substantial uncertainty - lots of decisions have to be made such as what variables shall be inspected first. To deal with such uncertainty, we propose to equip machines with human-like intelligence. Specifically, we develop a highly automated debugging technique that aims to couple human-like reasoning (e.g., dealing with uncertainty and fusing knowledge) with program semantics based analysis, to achieve benefits from the two and mitigate their limitations. We model debugging as a probabilistic inference problem, in which the likelihood of each executed statement instance and variable being correct/faulty is modeled by a random variable. Human knowledge, human-like reasoning rules and program semantics are modeled as conditional probability distributions, also called probabilistic constraints. Solving these constraints identifies the most likely faulty statements. Our results show that the technique is highly effective. It can precisely identify root causes for a set of real-world bugs in a very small number of interactions with developers, much smaller than a recent proposal that does not encode human intelligence. Our user study also confirms that it substantially improves human productivity.	commonsense reasoning;debugging;encode;failure;interaction;semantics (computer science);software bug;statement (computer science);usability testing	Zhaogui Xu;Shiqing Ma;Xiangyu Zhang;Shuofei Zhu;Baowen Xu	2018	2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)	10.1145/3180155.3180237	computer science;real-time computing;conditional probability;machine learning;debugging;probabilistic logic;human intelligence;small number;random variable;inference;python (programming language);artificial intelligence	SE	-17.59469183105992	0.34811044044227185	82945
65fa516a31d8200efb954d9772795445c35de42d	the elements of general equilibrium theory	fixed point theorem;general equilibrium theory;existence of equilibrium	The lecture will be an introduction to the model of economic equilibrium. The basic concepts: preferences, initial endowments and market clearing prices will discussed in general and by means of examples. I will indicate how fixed point theorems are used to demonstrate the existence of equilibrium prices and sketch an algorithm for Brouwers theorem. If time permits, there will be some remarks on equilibrium models with production. C. Papadimitriou and S. Zhang (Eds.): WINE 2008, LNCS 5385, p. 18, 2008. c © Springer-Verlag Berlin Heidelberg 2008	algorithm;fixed point (mathematics);lecture notes in computer science;nash equilibrium;springer (tank)	Herbert E. Scarf	2008		10.1007/978-3-540-92185-1_10	markov perfect equilibrium;sequential equilibrium;general equilibrium theory;economics;mathematics;microeconomics;fixed-point theorem;mathematical economics;welfare economics;equilibrium selection;solution concept;neoclassical economics	AI	-5.670791705850666	-1.5858323895449047	83074
99633de6f44820daa309a408258811f1e49b86cc	case reconstruction before assessment	knowledge based system;information retrieval;legal reasoning;real world application;first order;trade union	Traditionally Legal Knowledge Based System (LKBS) research concentrates on the matching of abstracted case facts with some formal representation of the law. How these facts are obtained, and how the abstraction that is necessary to ‘match’ the facts with terms mentioned in legsl rules or precedents, is accomplished, is left unmentioned (typically the user is assigned this task). The paper will describe this ‘reconstruction’ phase that precedes legal assessment of the case at hand. An elaboration of exactly this subtask can solve many problems of developing LKBS’S. The development of a practical application for first order legal aid to members of one of the largest trade unions in The Netherlands by non professional volunteers is described. A LKBS is being developed to improve referral to professionals and to induce a better understanding of the law by the volunteers when handling ‘dismisssl’ cases. Analysis of the tssks executed by the volunteers reveals the neceesity of extending the core of legal reasoning, assessment, wit h a practical part, aimed at supporting the reconstruction of the csse. The support is baaed on stereotypicrd plans (scenario’s), and an interface that reflects these scenarios. Furthermore, the application is embedded in a tra&ionaJ information retrieval environment. A successful prototype has been developed, and we are now in the phase of building the actual system.	embedded system;information retrieval;knowledge-based systems;prototype	Henk de Bruijn;Radboud Winkels	1995		10.1145/222092.222248	simulation;computer science;artificial intelligence;knowledge-based systems;first-order logic;management science	AI	-17.271720491297835	1.4719438315077258	83304
acb4a5a29329486a564bbbf30b7483416181c99e	word-of-mouth learning	social learning;global convergence;satisfiability;biased sampling;word of mouth	This paper analyzes a model of rational word-of-mouth learning, in which successive gene of agents make once-and-for-all choices between two alternatives. Before making a decisio new agent samples N old ones and asks them which choice they used and how satisfied they with it. If (a) the sampling rule is “unbiased” in the sense that the samples are representative overall population, (b) each player samples two or more others, and (c) there is any informa all in the payoff observations, then in the long run every agent will choose the same thing addition the payoff observation is sufficiently informative, the long-run outcome is efficient. We investigate a range of biased sampling rules, such as those that over-represent popular or s choices, and determine which ones favor global convergence towards efficiency.  2003 Elsevier Inc. All rights reserved. JEL classification: C72; D83	emoticon;information;local convergence;population;sampling (signal processing)	Abhijit Banerjee;Drew Fudenberg	2004	Games and Economic Behavior	10.1016/S0899-8256(03)00048-4	word of mouth;social learning;sampling bias;economics;mathematics;mathematical economics;statistics;satisfiability	AI	-7.229664156329259	-5.1307333833392565	83330
a510f2c4c54e8accf76fd6c32c96db0beceab732	culture, technology, communication: towards an intercultural global village, edited by charles ess with fay sudweeks, albany: state university of new york press, 2001, 355 pp, isbn 0-7914-5016-3		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;global village (telecommunications);international standard book number;primary source	Ewa Callahan	2004	Inf. Soc.	10.1080/01972240490456999	political economy;media studies	Robotics	-15.416116314725596	-5.643479187192982	83354
b875afa9e3bb49a48c844a2cc9eb69365775b7b2	tax contracts, party bargaining, and government formation		We explore how tax contracts affect government formation and welfare of voters in a democracy with proportional elections, four parties and sincere voting. A tax contract specifies a range of tax rates a party is committed to if in government. We develop a new model of party competition and formation of the government which chooses tax rates, public-good provision, and perks. We show that the introduction of tax contracts has two effects: a perks effect and a policy-shift effect. The former plays a central role in societies with a low degree of political polarization, where it tends to reduce politicians’ perks. In highly polarized societies, tax contracts can yieldmoremoderate political outcomes. However, there exist circumstances in which tax contracts induce more extreme policies. © 2012 Elsevier B.V. All rights reserved. ‘‘Read my lips: no new taxes.’’ George H.W. Bush, Republican National Convention on Aug. 18, 1988, New Orleans, LA.	existential quantification;polarization (waves)	Hans Gersbach;Maik T. Schneider	2012	Mathematical Social Sciences	10.1016/j.mathsocsci.2012.04.004	ad valorem tax;double taxation;state income tax;tax avoidance;tax credit;direct tax;economics;value-added tax;public economics;deferred tax;microeconomics;market economy;indirect tax;tax reform;economic policy;international taxation	AI	-5.172360635006046	-6.8362182924843795	83443
7a998a87e4524f64ffe38d1e081c911ef07cdba8	bayesian data assimilation based on a family of outer measures		A flexible representation of uncertainty that remains within the standard framework of probabilistic measure theory is presented along with a study of its properties. This represe ntation relies on a specific type of outer measure that is based on the measure of a supremum, hence combining additive and highly sub-additive components. It is shown that this type of outer measure enables the introduction of intuitive concepts suc h as pullback and general data assimilation operations.	data assimilation;utility functions on indivisible goods	Jeremie Houssineau;Daniel E. Clark	2016	CoRR		outer measure;mathematical optimization;discrete mathematics;infimum and supremum;probabilistic logic;data assimilation;mathematics;pullback;measure (mathematics);bayesian probability	Robotics	-9.828072052248492	0.3298205512535361	83673
b89fc95455131406fc1669e03ff7a33dc98e9e94	explaining actual causation via reasoning about actions and change		In causality, an actual cause is often defined as an event responsible for bringing about a given outcome in a scenario. In practice, however, identifying this event alone is not always sufficient to provide a satisfactory explanation of how the outcome came to be. In this paper, we motivate this claim using well-known examples and present a novel framework for reasoning more deeply about actual causation. The framework reasons over a scenario and domain knowledge to identify additional events that helped to “set the stage” for the outcome. By leveraging techniques from Reasoning about Actions and Change, the approach supports reasoning over domains in which the evolution of the state of the world over time plays a critical role and enables one to identify and explain the circumstances that led to an outcome of interest. We utilize action language AL for defining the constructs of the framework. This language lends itself quite naturally to an automated translation to Answer Set Programming, using which, reasoning tasks of considerable complexity can be specified and executed. We speculate that a similar approach can also lead to the development of algorithms for our framework. 2012 ACM Subject Classification Computing methodologies → Knowledge representation and reasoning, Computing methodologies → Causal reasoning and diagnostics, Computing methodologies → Temporal reasoning	action language;algorithm;answer set programming;causal filter;causality;correlation does not imply causation;knowledge representation and reasoning;machine translation;stable model semantics	E. LeBlanc	2018		10.4230/OASIcs.ICLP.2018.16		AI	-18.96644198601885	2.219863332633945	83730
0af8d5a8672b98d2ceb4ae33cb1abc29f4520870	managing online self-adaptation in real-time environments	approximate algorithm;real time;dynamic program;gold standard;intelligent control;cooperative intelligent real time control architecture;polynomial time;markov decision process;hard real time	Suppose we have an autonomous aircraft flying a complex mission broken into several different phases such as takeoff, ingress, target surveillance, egress, and landing. For each mission phase, the aircraft’s control system will have prepared a plan (or controller) specifying particular actions and reactions during the phase. Now suppose that the autonomous control system onboard this aircraft is self-adaptive: that is, it can modify its own behavior (plans) to improve its performance. Why might it have to adapt? Perhaps because the mission is changed in-flight, perhaps because some aircraft equipment fails or is damaged, perhaps because the weather does not cooperate, or perhaps because its original mission plans were formed quickly and were never optimized. In any case, the aircraft’s self-adaptive control system is now facing a deliberation scheduling problem. It must decide which mission phase’s plan to try to improve via self-adaptation, how to improve that plan, and how much time to spend on that self-adaptation process itself.	autonomous robot;control system;egress filtering;real-time transcription;scheduling (computing);unmanned aerial vehicle	Robert P. Goldman;David J. Musliner;Kurt D. Krebsbach	2001		10.1007/3-540-36554-0_2	mathematical optimization;real-time computing;simulation;computer science	Robotics	-19.034235317338183	-7.739353756224538	84224
5d9251fea6947530875467f3f72f42d761fcd39f	optimizing advisor network size in a personalized trust-modelling framework for multi-agent systems	electronic commerce;multi agent system;satisfiability;modelling framework;social network	Zhang [1] has recently proposed a novel trust-based framework for systems including electronic commerce This system relies on a model of the trustworthiness of advisors (other buyers offering advice to the current buyer) which incorporates estimates of each advisor's private and public reputations Users create a social network of trusted advisors, and sellers will offer better rewards to satisfy trusted advisors and thus build their own reputations.	multi-agent system;optimizing compiler	Joshua Gorner	2010		10.1007/978-3-642-13059-5_62	e-commerce;computer science;knowledge management;artificial intelligence;multi-agent system;social network;satisfiability	AI	-7.716087637887542	-7.58064889083374	84269
24a47ca6f5b2ebec9e809bf19d2b0f0da3dcab81	combining case-based and model-based reasoning for predicting the outcome of legal cases	instance based learning;model based reasoning;raisonnement base sur cas;raisonnement base sur modele;razonamiento fundado sobre caso;learning algorithm;intelligence artificielle;algorithme apprentissage;aspecto juridico;legal aspect;inferencia;artificial intelligence;aspect juridique;inteligencia artificial;case based reasoning;algoritmo aprendizaje;inference	This paper presents an algorithm called IBP that combines case-based and model-based reasoning for an interpretive CBR application, predicting the outcome of legal cases. IBP uses a weak model of the domain to identify the issues raised in a case, and to combine the analyses for these issues; it reasons with cases to resolve conflicting evidence related to each issue. IBP reasons symbolically about the relevance of cases and uses evidential inferences. Experiments with a collection of historic cases show that IBP’s predictions are better than those made with its weak model or with cases alone. IBP also has higher accuracy compared to standard inductive and instance-based learning algorithms. 1 Reasoning with Cases for Prediction 1.1 Predicting the Outcome of Legal Cases In determining the outcome of a new case, courts in common-law jurisdictions like the US are bound by past decisions. Given that courts are supposed to follow the relevant precedents, it is reasonable to expect that to some extent, one might predict the outcomes of new cases based on past precedents. There is, moreover, a large, and growing, body of data. Legal cases are published on the WWW; tens of thousands of full-text opinions are available in commercial databases like Westlaw and Lexis; and many law firms maintain their own case management systems, where case documents are indexed and stored for future reference. It may appear, then, that the law is a prime candidate for applying machine learning (ML) or data mining techniques to predict the outcomes of new cases or induce rules about the domain. However, purely statistical methods are of limited use in the law, because they generate numerical scores, rather than a human-interpretable explanation. Likewise, rule-learning algorithms have serious limitations, even though their output is easier to understand for a human. These algorithms tend to focus on a subset of features to find a small set of rules to capture regularities in the data. We found that these induced regularities K.D. Ashley and D.G. Bridge (Eds.): ICCBR 2003, LNAI 2689, pp. 65–79, 2003. c © Springer-Verlag Berlin Heidelberg 2003 66 S. Brüninghaus and K.D. Ashley may lead to correct predictions, but they do not correspond to legal reasoning, and the output of such systems is not an acceptable explanation for human experts. Instead, case-based methods are most appropriate. The evidence in legal disputes often favors both sides, but there usually are no rules how to weigh the conflicting features. Advocates and courts make arguments how to resolve these conflicts by analogizing the new case to similar precedents, thereby considering the features of a case in context. In addition, the features and arguments are described textually in long complex texts, and it is very difficult automatically to extract predictive features. Doing so manually, on the other hand, requires knowledge and becomes prohibitively expensive for very large collections. We have developed a hybrid algorithm for predicting the outcomes of legal cases that avoids some of these pitfalls and problems. The algorithm, called IBP for issue-based prediction, integrates CBR techniques and a weak domain model to predict outcomes with a comparatively small case database and provides meaningful explanations. More specifically, IBP (1) uses its model of the domain to identify which issues were raised, (2) applies CBR techniques to evaluate evidence which side is favored for each issue, and (3) derives a prediction from this issue-analysis following its model of the domain. An evaluation shows that IBP achieves more accurate predictions using this combination of actual cases and a weak predictive theory compared to predictions made with either knowledge source alone. IBP’s performance is also better than standard ML algorithms. 1.2 Case-Based Reasoning in the Law IBP is based on the approach developed in the interpretive CBR programs HYPO and CATO (Ashley 1990; Aleven 1997), which are designed to make arguments with cases and implemented for trade secret law. The cases are represented in terms of Factors, stereotypical patterns of fact that strengthen or weaken a plaintiff’s claim. The outcome of a case may either be for the plaintiff, the party that brings the lawsuit, or for the defendant, the party that is being sued. (In the remainder of the paper, we refer to Factors as Fi, and use subscript π and δ to indicate which side they favor.) HYPO’s argumentation model introduced Factors to reason symbolically about cases, and defined similarity by the inclusiveness of the set of Factors a case shares with another problem. HYPO does not employ schemes for assigning quantitative feature weights, which are problematic. They are not sensitive to a problem’s particular context or do not support reasonable legal explanations. CATO extended HYPO’s model by introducing the Factor Hierarchy, a representation that links the Factors to intermediate legal conclusions and higher-level legal issues and that provides a specially designed inference mechanism. With the Factor Hierarchy, CATO can make arguments about the relevance of distinctions between cases. Recently, methods from CATO and HYPO have been applied to predict the outcome of cases (Aleven 2003). In this approach, prediction was based on a simple criterion: If all cases that satisfy a relevance criterion R were won by the same side, then predict that side, else abstain. The relevance criteria used Combining Case-Based and Model-Based Reasoning 67 in the experiment are based on HYPO’s definitions of most-on-point and bestuntrumped cases (HYPO-BUC), and on CATO’s reasoning about the relevance of distinctions between cases (NoSignDist/BUC). Both methods made few mistakes, at cost of abstaining frequently; see Table 1, Nrs. 3 and 8. 1.3 Combining Model-Based and Case-Based Reasoning The integration of CBR with other techniques has long been a focus of CBR research; see (Marling et al. 2002) for a comprehensive overview. Probably most similar to IBP is CARMA (Branting, Hastings, & Lockwood 2001), which also integrated a weakly predictive model and historical cases for prediction. In CARMA’s domain, expert models were too weak for accurate prediction from scratch but they could be used successfully to adapt predictions based on prototypical cases. The program broke down a new case into subcases based on its model, used CBR to retrieve an approximate solution, and applied the model to refine and adapt that solution. Evaluation demonstrated that CARMA’s hybrid of model and cases did better than either method alone. Anapron, another hybrid program, improved the accuracy of its predictions of how names are pronounced by combining independent case-based and rulebased evidential sources (Golding & Rosenbloom 1996). Before applying a rule whose antecedents were satisfied, the program checked for any case exceptions to the rule. If the program found a “compelling analogy” between the problem and a case exception, it applied the exception rather than rule in making a prediction. Experiments showed that the combination improved accuracy over a purely rule-based or case-based version of the program. Anapron differs from IBP in that both reasoners can generate competing solutions. CASEY (Koton 1989) combined case-based reasoning and a strong causal model of heart disease, not to fill gaps in the causal model, but to make it more efficient to reason with the computationally expensive model. Evidence principles and a justified match procedure determined under what circumstances a past case’s explanation could be adapted to diagnose a new problem. Interestingly, two other hybrid approaches, GREBE (Branting 1999) and CABARET (Rissland & Skalak 1989) were developed for the legal domain. Both programs use a combination of rule-based and case-based reasoning for generating arguments. IBP is similar to these programs in that its model summarizes requirements for a claim in its domain. However, CABARET’s domain, home office deduction, is statutory, which means that its model of domain is stronger than in IBP. Like Anapron, it has a more complete set of rules which can be applied at the same time as CBR, and in each reasoning step, meta-knowledge is used to pick the best reasoner. Likewise, GREBE was developed for a domain with a stronger set of rules, compared to IBP’s weak model. Moreover, GREBE’s representation is more structured than CATO’s and includes intermediate conclusions, which are similar to issues in IBP. Most importantly, CABARET and GREBE generate arguments, not predictions. SHYSTER (Popple 1993) was an early attempt to use a nearest-neighbor approach in the law. It used the outcome of the most similar case as prediction 68 S. Brüninghaus and K.D. Ashley for a problem, and generated a quasi-argument by listing shared and unshared features, which is overly simplistic compared to IBP’s output; see Fig. 3. The rest of this paper is organized as follows. In Sec. 2, we describe the knowledge represented in IBP. In Sec. 3, we discuss how IBP works and illustrate it with an example. In Sec. 4, we focus on further details of the algorithm. In Sec. 5, we present the results of our experiments. In Sec. 6, we conclude by summarizing the most interesting observations from a CBR point-of-view. 2 Knowledge Sources for IBP 2.1 Cases and Factors The most important knowledge source in common-law domains, such as trade secret law, are cases. The outcome of a new problem is influenced by the existing body of case law. Courts are bound by their decisions, and may only be overruled by higher courts. Judges and attorneys reason with cases, trying to find the most relevant and favorable precedents. We are using CATO’s case database, which comprises 148 trade secret law cases. These cases come from a wide range of jurisdictions and procedural settings. Most were selected for their pedagogical utility, and the collection was compiled long before the work on IBP 	akaike information criterion;analysis of algorithms;approximation algorithm;authentication;cabaret mechanical theatre;case-based reasoning;causal filter;causal model;compiler;data mining;database;domain model;eisenstein's criterion;exception handling;expect;experiment;feature vector;fitts's law;hybrid algorithm;instance-based learning;logic programming;machine learning;model-based reasoning;natural deduction;numerical analysis;relevance;requirement;rule 90;rule induction;shyster;semantic reasoner;standard ml;station hypo;subject-matter expert;www	Stefanie Brüninghaus;Kevin D. Ashley	2003		10.1007/3-540-45006-8_8	case-based reasoning;instance-based learning;computer science;artificial intelligence;model-based reasoning;operations research;algorithm	AI	-16.740524821352565	2.252905819968387	84371
2a9403c0db6a2e3789c39811fde1c9e990b164f5	cycles of aggregate behavior in theory and experiment	theoretical model;evolutionary dynamics;evolutionary model;nash equilibrium;random matching;structural change;journal of economic literature	Abstract   We test in the laboratory the potential of evolutionary dynamics as predictor of actual behavior. To this end, we propose an asymmetric game (which we interpret as a borrower–lender relation), we study its evolutionary dynamics in a random matching setup, and we test its predictions. The theoretical model provides conditions for changes in qualitative aggregate behavior in response to variations in structural parameters. While it turns out that Nash equilibrium is not a reliable predictor of average aggregate behavior, the experiment seems to confirm the qualitative predictions of the evolutionary model under structural changes. Journal of Economic Literature Classification Numbers: C7, C9, E3.	aggregate function	Antoni Bosch-Domènech;Maria Saez-Marti	2001	Games and Economic Behavior	10.1006/game.2000.0821	econometrics;economics;structural change;evolutionary dynamics;mathematical economics;welfare economics;nash equilibrium	ECom	-5.938013275910857	-6.211581170239329	84895
2d0061b20fe0a291d9947c7f61898d057ba27ef5	agent based model of fisher behavior as a dynamic constraint resolution problem		An agent-based approach for modelling fisher behavior as a dynamic constraint resolution problem is proposed. The fishers are modeled as agents tasked with optimizing different multi-objective utility functions over a search space subjected to ecological, social, and political constraints derived from existing ecological and social models. The agents search for a satisfactory strategy by using a guided local search algorithm modified to allow for competition or cooperation in varying degrees, and the utility function is modified to mimic perfect rationality, as well as to include well-known behavioral strategies such as repetition, imitation, and social comparison. The goal of the model is to allow analysis and comparison of fisher strategies and their impact on the environment under different ecological limitations, fishing policies and assumptions of rationality on the part of the fishers.	agent-based model;guided local search;local search (optimization);rationality;search algorithm;utility	Cezara Pastrav;Sigríður Sigurðardóttir;Jónas R. Viðarsson;Anna K. Daníelsdóttir	2015	2015 Winter Simulation Conference (WSC)		data modeling;aquaculture;mathematical optimization;ecosystem;mathematics;management science	AI	-9.581549710775038	-9.527410848221429	85013
21063e0ac60df12ccea1b43217b11255c6022b5e	the flow network method		We present a method which associates with any network a binary relation on its vertices which is complete and quasi-transitive, so that it admits at least a maximum. Such a method, called flow network method, is based on the concept of maximum flow. As an application, given a competition involving two or more players, we identify it with a network whose vertices are the players and use the flow network method to build a relation on the set of players. Such a relation can be interpreted as a way to establish, for every ordered pair of players, if the first one did at least as good as the second one in the competition and its maxima can be interpreted as the winners of the competition. That way to select the winners is proved to satisfy many desirable properties. Further, relying on the flow network method, we define a voting system where each individual is allowed to express as preference relation any binary relation on the set of alternatives. That voting system is proved to be decisive, anonymous, neutral, monotonic, efficient and immune to the reversal bias. Moreover, it coincides with the Borda count for preference profiles made up by linear orders.	flow network;maxima;maximum flow problem;ordered pair;small-bias sample space;vertex (geometry)	Daniela Bubboloni;Michele Gori	2018	Social Choice and Welfare	10.1007/s00355-018-1131-7	discrete mathematics;artificial intelligence;mathematics;algorithm	ECom	-6.786674577613493	-2.4581320000334483	85050
940fbc841aef2dfeea8fc628581a4dc2142ec5f7	cognitive reasoning and trust in human-robot interactions		We are witnessing accelerating technological advances in autonomous systems, of which driverless cars and home-assistive robots are prominent examples. As mobile autonomy becomes embedded in our society, we increasingly often depend on decisions made by mobile autonomous robots and interact with them socially. Key questions that need to be asked are how to ensure safety and trust in such interactions. How do we know when to trust a robot? How much should we trust? And how much should the robots trust us? This paper will give an overview of a probabilistic logic for expressing trust between human or robotic agents such as “agent A has 99% trust in agent B’s ability or willingness to perform a task” and the role it can play in explaining trust-based decisions and agent’s dependence on one another. The logic is founded on a probabilistic notion of belief, supports cognitive reasoning about goals and intentions, and admits quantitative verification via model checking, which can be used to evaluate trust in human-robot interactions. The paper concludes by summarising future challenges for modelling and verification in this important field.	autonomous car;autonomous robot;autonomous system (internet);cognitive computing;embedded system;interaction;model checking	Marta Z. Kwiatkowska	2017		10.1007/978-3-319-55911-7_1	cognitive science;model checking;robot;probabilistic logic;computer science;autonomy;human–robot interaction;autonomous system (internet)	AI	-18.40674493312251	-1.8636233632553254	85163
a626d823dad698afb50e91d453073b8d4e25489e	a time-constrained sla negotiation strategy in competitive computational grids	computational grid;intelligent strategies;bayesian learning;sla negotiation;time constraints	Automated and intelligent negotiation solutions for reaching service level agreements (SLA) represent a hot research topic in computational grids. Previous work regarding SLA negotiation in grids focuses on devising bargaining models where service providers and consumers can meet and exchange SLA offers and counteroffers. Recent developments in agent research introduce strategies based on opponent learning for contract negotiation. In this paper we design a generic framework for strategical negotiation of service level values under time constraints and exemplify the usage of our framework by extending the Bayesian learning agent to cope with the limited duration of a negotiation session. We prove that opponent learning strategies are worth for consideration in open competitive computational grids, leading towards an optimal allocation of resources and fair satisfaction of participants.	service-level agreement	Gheorghe Cosmin Silaghi;Liviu Dan Serban;Cristian Marius Litan	2012	Future Generation Comp. Syst.	10.1016/j.future.2011.11.002	knowledge management;management science;bayesian inference	NLP	-9.552202538716955	-7.9807726023520065	85179
3273a557c410cd600e13e16523bfe0118ac72086	a kantorovich-monadic powerdomain for information hiding, with probability and nondeterminism	macquarie university institutional repository;quantitative information flow;probability;refinement orders;researchonline;entropy hidden markov models testing probabilistic logic algebra extraterrestrial measurements;digital repository;macquarie university;semantics;observers;data encapsulation;information leakage kantorovich monadic powerdomain information hiding probability nondeterminism domain theoretic model hidden state information flow smyth like relation structural refinement like order semantic elements testing order extant entropy based techniques principal theorem giry kantorovich monads partially observable markov decision processes observer knowledge;security of data data encapsulation decision making entropy markov processes observers probability;quantitative information flow semantics probabilistic domains refinement orders probabilistic monads;probabilistic monads;entropy;markov processes;security of data;probabilistic domains	We propose a novel domain-theoretic model for nondeterminism, probability and hidden state, with relations on it that compare information flow. One relation is Smyth-like, based on a structural, refinement-like order between semantic elements; the other is a testing order that generalises several extant entropy-based techniques. Our principal theorem is that the two orders are equivalent. The model is based on the Giry/Kantorovich monads, and it abstracts Partially Observable Markov Decision Processes by discarding observables' actual values but retaining the effect they had on an observer's knowledge. We illustrate the model, and its orders, on some small examples, where we find that our formalism provides the apparatus for comparing systems in terms of the information they leak.	domain theory;information flow (information theory);markov chain;partially observable markov decision process;power domains;refinement (computing);semantics (computer science)	Annabelle McIver;Larissa Meinicke;Carroll Morgan	2012	2012 27th Annual IEEE Symposium on Logic in Computer Science	10.1109/LICS.2012.56	entropy;combinatorics;discrete mathematics;computer science;probability;mathematics;semantics;markov process;algorithm	Logic	-10.48665581945804	-3.2145697271463254	85186
c00139c317e701168e993981011b73dff224a212	on a participation structure that ensures representative prices in prediction markets		The logarithmic market scoring rule (LMSR) is now the de facto market-maker mechanism for prediction markets. We show how LMSR can have more representative final prices by simply imposing a participation structure where the market proceeds in rounds and, in each round, traders can only trade up to a fixed number of contracts. Focusing on markets over binary outcomes, we prove that under such a participation structure, the market price converges after a finite number of rounds to the median of traders’ private information for an odd number of traders, and to a point in the median interval for an even number of traders. Thus, the final market price effectively represents all agents’ private information since those equilibria are measures of central tendency. We also show that when traders use market price data to revise their private information, the aforementioned equilibrium prices do not change for a broad class of learning methods. © 2017 Elsevier B.V. All rights reserved.	personally identifiable information;traders	Arthur Carvalho	2017	Decision Support Systems	10.1016/j.dss.2017.09.008	factor market;central tendency;market price;parity (mathematics);financial economics;scoring rule;private information retrieval;microeconomics;economics	ECom	-4.932757308912803	-4.508997112959443	85330
e5dd7e5bd871d69723cd1250e8d1d73c81958eae	paul feyerabend and software technology	software technology { methodology { the- ory of science { constructivism	The following contribution is a plea for more stringent methodological standards in software technology. Certain basic scientific principles are often neglected, principles such as the fact that predictions need to be falsifiable. This appears to confirm the theses of the socalled “constructivists” that “objective truths” are in reality just social constructs. It is thus argued here that one needs a stronger empirical foundation for software technology.	computer science;social constructivism;solidity	Gregor Snelting	1998	International Journal on Software Tools for Technology Transfer	10.1007/s100090050013	software;software engineering;computer science	SE	-13.43009226775371	1.4951041076002616	85449
93d31b293ce20c9eb36fefd5c81c7e66059d9b49	on the accessibility of core-extensions	core;accessibility;core extensions	Sengupta and Sengupta (1996) study the accessibility of the core of a TU game and show that the core, if non-empty, can be reached from any non-core allocation via a finite sequence of successive blocks. This paper complements the result by showing that when the core is empty, a number of non-empty core-extensions, including the least core and the weak least core (Maschler et al., 1979), the positive core (Orshan and Sudholter, 2001) and the extended core (Bejan and Gomez, 2009), are accessible in a strong sense, namely each allocation in each of the foregoing core-extensions can be reached from any allocation through a finite sequence of successive blocks.	accessibility	Yi-You Yang	2012	Games and Economic Behavior	10.1016/j.geb.2011.08.007	core;operations management;accessibility	Vision	-6.442015229579826	-2.3662705668869775	85461
b2c0f01764954a5f5b72542e492bd19faeb6b9b9	cybermaterialism, or why there is no free lunch in cyberspace	use;reseau information;universite;critical study;edition electronique;electronic communication;information technology;technologie information;etude critique;information network;utilizacion;estudio critico;cybermaterialisme;utilisation;internet;edicion electronica;university;electronic publishing;tecnologia informacion;communication;universidad;comunicacion;no free lunch;red informacion;communication electronique	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	cyberspace;francis;primary source	Steve Fuller	1995	Inf. Soc.	10.1080/01972243.1995.9960206	no free lunch in search and optimization;the internet;computer science;sociology;electronic publishing;law;information technology	Robotics	-15.516522060041222	-5.623504657948379	85472
6c373181c344ef6d98a9aa283db32eb2145b6548	generating multi-agent plans by distributed intersection of finite state machines		Deterministic multi-agent planning described by MA-STRIPS formalism requires mixture of coordination and synthesis of local agents' plans. All agents' plans, as sequences of actions, can be implicitly described by an appropriate generative structure. Having all local plans of all participating agents described by such a structure and having a merged process of such structures, we can induce a global multi-agent plan by successive elimination of unfeasible combinations of local agents' plans.#R##N##R##N#We use Nondeterministic Finite State Machines (NFSs) as the generative structure for plans and a well-known process of intersection of NFSs to prune the plan spaces towards globally feasible multi-agent plan. Since the numbers of the plans can be large, we use iterative process of building of the NFSs using a state-of-the-art classical planner interleaved with the NFS intersecting process. We have evaluated the algorithm on an extensive number of problems from International Planning Competition extended for multi-agent planning.		Jan Tozicka;Jan Jakubuv;Antonín Komenda	2014		10.3233/978-1-61499-419-0-1111	mathematical optimization	AI	-11.327598318154223	-5.927157254604801	85630
265d71c069d3dba9f436589341868f9c79027ae0	strong mediated equilibrium	mediated equilibrium;multiagent system;game theory;multi agent system;mediador;coalicion;teoria juego;intelligence artificielle;theorie jeu;satisfiability;multi agent systems;mediator;coalition;mediateur;artificial intelligence;inteligencia artificial;sistema multiagente;systeme multiagent;strong equilibrium	Providing agents with strategies that will be robust against deviations by coalitions is central to the design of multi-agent agents. However, such strategies, captured by the notion of strong equilibrium, rarely exist. This paper suggests the use of mediators in order to enrich the set of situations where we can obtain stability against deviations by coalitions. A mediator is a reliable entity, which can ask the agents for the right to play on their behalf, and is guaranteed to behave in a prespecified way based on messages received from the agents. However, a mediator can not enforce behavior; that is, agents can play in the game directly without the mediator’s help. We prove some general results about mediators, and concentrate on the notion of strong mediated equilibrium; we show that desired behaviors, which are stable against deviations by coalitions, can be obtained using mediators in a rich class of settings.	matchware mediator;mediator pattern;multi-agent system;nash equilibrium	Dov Monderer;Moshe Tennenholtz	2006	Artif. Intell.	10.1016/j.artint.2008.10.005	simulation;computer science;artificial intelligence;mediator;multi-agent system;equilibrium selection;satisfiability	AI	-9.941746568903891	-6.360402352533805	86302
736d26b889f59e7ef45e5215c15d5e8978476583	measures of information	truth;truthlikeness;information quality;measures;amount of information;information	This paper builds an integrated framework of measures of information based on the Model for Information (MfI) developed by the author. Since truth is expressed using information, an analysis of truth depends on the nature of information and its limitations. These limitations include those implied by the geometry of information and those implied by the relativity of information. This paper proposes an approach to truth and truthlikeness that takes these limitations into account by incorporating measures of the quality of information. Another measure of information is the amount of information. This has played a role in two important theoretical difficulties—the Bar-Hillel Carnap paradox and the “scandal of deduction”. This paper further provides an analysis of the amount of information, based on MfI, and shows how the MfI approach can resolve these difficulties.	natural deduction;quantities of information	Paul Walton	2015	Information	10.3390/info6010023	information;measure;epistemology;computer science;data mining;mathematics;information quality;interaction information;statistics	NLP	-15.505861355188452	0.7851580362708143	86401
2251779607af1948e6a026bb0e7e668a3d094bda	selfish bin covering	social welfare;nash equilibrium;selfish bin covering;price of anarchy;weighted majority game;price of stability;weighted majority games;covering problem	In this paper, we address the selfish bin covering problem, which is greatly related both to the bin covering problem, and to the weighted majority game. What we are mainly concerned with is howmuch the lack of central coordination harms social welfare. Besides the standard PoA and PoS, which are based on Nash equilibrium, we also take into account the strong Nash equilibrium, and several new equilibrium concepts. For each equilibrium concept, the corresponding PoA and PoS are given, and the problems of computing an arbitrary equilibrium, as well as approximating the best one, are also considered. © 2011 Elsevier B.V. All rights reserved.	covering problems;nash equilibrium	Zhigang Cao;Xiaoguang Yang	2011	Theor. Comput. Sci.	10.1016/j.tcs.2011.09.017	price of stability;epsilon-equilibrium;mathematical optimization;best response;social welfare;correlated equilibrium;mathematical economics;equilibrium selection;solution concept;price of anarchy;nash equilibrium	AI	-5.305316623794485	-0.7158231960814666	87154
0de086b60e8e3b6c266b068f815b8f58bfc13644	tractable dynamic global games and applications	welfare;dynamic game;unique equilibrium;taxation;global games;coordination	We present a family of tractable dynamic global games and its applications. Agents privately learn about a fixed fundamental, and repeatedly adjust their investments while facing frictions. The game exhibits many externalities: payoffs may depend on the volume of investment, on its volatility, and on its concentration. The solution is driven by an invariance result: aggregate investment is (in a pivotal contingency) invariant to a large family of frictions. We use the invariance result to examine how frictions, including those similar to the Tobin tax, affect equilibrium. We identify conditions under which frictions discourage harmful behavior without compromising investment volume. ∗Early drafts of this manuscript circulated under the title “Sand in the Wheels: A Dynamic GlobalGame Approach”. We have benefited from comments by Sylvain Chassang, Federico Echenique, Eugen Kováč, Filip Matějka, David Myatt, Alessandro Pavan, József Sákovics, Ennio Stacchetti, Colin Stewart, Jonathan Weinstein, and from audiences at Edinburgh, NYU, HECER in Helsinki, the University of Texas at Austin, the Kansas Theory Workshop, the Midwest theory conference at Vanderbilt, and the conferences “Information and Coordination: Theory and Applications”, EEA-ESEM in OSLO, and SED in Cyprus. Rossella Argenziano and Siyang Xiong provided excellent comments as discussants. †lmath@mail.utexas.edu ‡j-steiner@kellogg.northwestern.edu	aggregate data;alessandro vespignani;cobham's thesis;comparison of canadian-tax preparation software for personal use;extended euclidean algorithm;volatility;wheels	Laurent Mathevet;Jakub Steiner	2013	J. Economic Theory	10.1016/j.jet.2013.07.015	economics;welfare;macroeconomics;microeconomics;mathematical economics;market economy;sequential game;global game;welfare economics	ML	-7.552401656914348	-4.642849829930414	87457
c941525ce6ee5fc74a36383609844684635718b7	performance evaluation of repeated auctions for robust task execution	performance evaluation	We present empirical results of an auction-based algorithm for dynamic allocation of tasks to robots. The results have been obtained both in simulation and using real robots. A distinctive feature of our algorithm is its robustness to uncertainties and to robot malfunctions that happen during task execution, when unexpected obstacles, loss of communication, and other delays may prevent a robot from completing its allocated tasks. Therefore tasks not yet achieved are resubmitted for bids every time a task has been completed. This provides an opportunity to improve the allocation of the remaining tasks, enabling the robots to recover from failures and reducing the overall time for task completion.	performance evaluation	Maitreyi Nanjanath;Maria L. Gini	2008		10.1007/978-3-540-89076-8_31	real-time computing;simulation;computer science;distributed computing	ECom	-15.48941270694402	-8.592096403883705	87581
fbe57b934b0265af279c62f9ba9d1c82c1108501	voxel-based probabilistic space-time prisms for analysing animal movements and habitat use	landscape ecology;spatial analysis;mobile objects	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;habitat;primary source;voxel	Joni A. Downs;Mark W. Horner;Garrett Hyzer;David S. Lamb;Rebecca Loraamm	2014	International Journal of Geographical Information Science	10.1080/13658816.2013.850170	landscape ecology;geography;spatial analysis;cartography;statistics	Robotics	-15.006412083077477	-6.162688955529662	87600
83ceb143be3c7e931a90ac344de1901666697a3d	stable solutions for multiple scenario cost allocation games with partial information	cooperative games;cost allocation;multiple scenario;core concepts	Multiple scenario cooperative games model situations where the worth of the coalitions is simultaneously valued in several scenarios or under different states of nature. For multiple scenario cost allocation games, we focus on the identification of those allocations which are stable in the sense that agents have no incentives to abandon the group. The stability of an allocation depends on how the quantities assigned to the coalitions are compared with the vector-valued worth of the coalition. Therefore, different extensions of the notion of core emerge, depending on how these comparisons are made. In this paper different notions of core for multiple scenario cost allocation games are studied, and the relationships between them and the existing core solutions are analyzed. We also address the inclusion in the model of partial information about the relative importance of the scenarios which often refers to the probabilities of occurrence. In order to identify allocations which are also stable in the presence of partial information, we extend the notions of core to this new setting, and provide results which permit the calculation of the corresponding sets of allocations. The potential applicability of our analysis is illustrated with the allocation of the total cost of the waste collection service between the different urban areas in the city of Seville.	characteristic function (convex analysis);expected utility hypothesis;lexicographical order;maxima and minima;requirement	D. V. Borrero;Miguel A. Hinojosa;Amparo M. Mármol	2016	Annals OR	10.1007/s10479-014-1755-7	mathematical optimization;simulation;economics;operations management;welfare economics	AI	-5.461907767623488	-3.1972305676839072	87967
41b0af2d8bbb8a9fc9c20825f98e3c3d6cafec86	i. althöfer: on pathology in game tree and other recursion tree models			recursion	L. Victor Allis	1992	ICGA Journal	10.3233/ICG-1992-15207	game tree;machine learning;artificial intelligence;recursion;computer science	NLP	-13.356617602807042	-8.070538675777216	87982
c7dd0c6c567cf8c8af1aa26af3678edcd181edf3	a project report on np: an assumption-based nl plan inference system that uses feature structures	plan recognition;assumption based truth maintenance system;side effect;rewrite systems;pattern matching;natural language;knowledge base	"""This paper presents a project report on NP, a working Natural language Plan inference system that uses feature structures and is based on assumptions. Input to the system is in the form of feature structures, which can be taken directly from the output of a semantic parser. Plan actions are represented by feature-structure plan schemata with preconditions, hierarclfical decompositions, and effects. Output is in the form of a network of believed assertions represented in a knowledge base, and can be reported, used to answer generation-system queries, or drive side-effecting demons. The plan inference component is implemented using a feature-structure-based inference engine and models of plan recognition, prediction, and inference. The inference engine is implemented using a rewriting system for patteru-matctfing, and an Assumption-based Truth Maintenance System (ATMS) for conjunctions. The ArMS allows pre-instantiation of hypothetically known assertions and implications, which can significantly reduce processing time. The ATMS also permits simultaneous consideration of multiple possible inputs or multiple possible inferred plan outputs; these can be mutually conflicting or supportive. This capability will be important for disambiguation. The NP system is used to infer dialogand domaln-level plans, among other types. Original contributions include: a plan inference system that works directly from feature structures; a plan inference system that uses an ATMS and plan schema actions with preconditions and effects to infer hierarchical and chained plans; and, an inference engine that works with multiple featurestructure assertions and rules. Pro jec t Goal. This project is aimed toward a dialog understanding system that can be used as part of an automatic interpreting telephone system. Interpretation will be performed by parsing, transferring, and generating utterances. Thus, dialog understanding will be used to recognize speech acts and illocutionary acts, resolve ellipses, and provide required missing information, among other tasks. The understanding system will use the output of the semantic parser, and provide information to the transfer module and generation system. Therefore, feature structures should be used as the basic data representation scheme. Dialog understanding requires a general-purpose plan inference engine that can work with dialog plans, domain plans, commonsense knowledge plans, and so forth. The system must also in the future be able to perform disambiguation of possible utterances. Background: Assumpt ions . The plans, intentions, beliefs, etc., of a human are menial concepts which cannot be perceived directly', they are unobservable[Mye88]. There is insufficient information to represent these concepts with certainty. Therefore, the system must be able to represent concepts in an uncertain manner, using assumptions. Communication between two people is inherently an assumption-based process. Since it is never cornpletely possible to directly know the concepts of another person, it is necessary in the course of a conversation to take a stance and rely on assumptions about the other person's thoughts [Den87]. Thus, in a dialog understanding system, there are at least two kinds of assumptions that must be represented: assumptions that the two speakers make, which must be modeled by the system, and assumptions that the system makes about the situation, 1 the two speakers and their plans, intentions, etc. Design. Plan inference and other knowledgebased reasoning tasks require that multiple conjunctive implications be matched against large sets of unordered assertions. The system will have a catalog of world knowledge, common-sense knowledge, and assertions which are believed by the speakers. These must be accessed non-sequentially and used for reasoning. In other words, language understanding should be done by using an """"expert system"""" inference engine, Computer languages should be used according to their strengths and weaknesses. Feature-structure systems are strong in representing complex, incomplete, or underspecified information, and in performing unification. However, they are extremely inefficient at list processing and numeric calculations (e.g. for evidential reasoning), and don't represent multiple possible worlds. Lisp and other languages can fulfil these needs. One solution is to build a hybrid system. An inference engine was built which uses a featurestructure language fo r representation and patternmatching tasks, while using an ATMS to perform conjunctive implications, represent assumptions, represent possible worlds, and maintain the truth of derived belief networks when nonmonotonic changes occur. The ATMS allows the system to represent, and reason with, all consistent possibilities at the same time-not just the current best choice. In particular, this permits multiple possible inferred plans to be output, and multiple possible observations to be input. This capability will become important for possible utterance disambiguation. The system interprets the results of the ATMS by using a five-valued uncertainty logic consisting of the uncertain belief values ACTUAL, POSSIBLE, HYPOTHETICAL, INCONSISTENT, or NULL. E a c h asser1 Currently, most dialog understanding systems start with the assumptions that the hearer and speaker always unders tand each other perfectly, that they automatically want to cooperate as much as possible, and that they have absolutely no other commitments outside of the conversation. Clearly some of these assumptions can occasionally be incorrect."""	bayesian network;commonsense knowledge (artificial intelligence);data (computing);expert system;feature model;general-purpose modeling;hybrid system;inference engine;interpreter (computing);knowledge base;knowledge representation and reasoning;lisp;nl (complexity);np (complexity);natural language understanding;parser combinator;parsing;possible world;precondition;reason maintenance;rewriting;unification (computer science);universal instantiation;word-sense disambiguation;dialog	John K. Myers	1990		10.3115/991146.991240	knowledge base;computer science;pattern matching;data mining;database;natural language;programming language;side effect;algorithm	AI	-19.029632674067173	1.9019325653014223	88071
8240cc83bb87b607392cbbc077169f21ccf06e4d	methodology guide to task analysis with the goal of extracting relevant characteristics for human-computer interfaces	trace analysis;satisfiability;task analysis;human computer interface	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source;task analysis	Suzanne Sebillotte	1995	Int. J. Hum. Comput. Interaction	10.1080/10447319509526130	human–computer interaction;computer science;artificial intelligence;theoretical computer science;task analysis;satisfiability	Robotics	-15.84184547317702	-5.933739357259938	88589
819f683347b1f1470a27c50fd2ca4c99832bf59c	a composite run-to-the-bank rule for multi-issue allocation situations	allocation;satisfiability;cooperative game;cooperative games;self duality;multi issue allocation;consistency;bankruptcy	In this paper, we propose a new extension of the run-to-the-bank rule for bankruptcy situations to the class of multi-issue allocation situations. We show that this rule always yields a core element and that it satisfies self-duality. We characterise our rule by means of a new consistency property, issue-consistency.		Carlos González-Alcón;Peter Borm;Ruud Hendrickx	2007	Math. Meth. of OR	10.1007/s00186-006-0123-z	bankruptcy;mathematical optimization;mathematics;mathematical economics;consistency;satisfiability	AI	-6.432651906221938	-1.7646997855529476	89132
32a2e87c5e0c0d5cfb7f94cfc940f39b8770ecb3	conditions for quantum interference in cognitive sciences	quantum probability;quantum measurements;decisions under uncertainty;conditions for interference	We present a general classification of the conditions under which cognitive science, concerned, e.g. with decision making, requires the use of quantum theoretical notions. The analysis is done in the frame of the mathematical approach based on the theory of quantum measurements. We stress that quantum effects in cognition can arise only when decisions are made under uncertainty. Conditions for the appearance of quantum interference in cognitive sciences and the conditions when interference cannot arise are formulated.	cognitive science;decision making;interference (communication);mathematics;quantum hall effect;statistical interference	Vyacheslav I. Yukalov;Didier Sornette	2014	Topics in cognitive science	10.1111/tops.12065	quantum probability;quantum information science;mathematics;quantum limit;statistics	Theory	-13.87076816081488	1.193075263596595	89177
6ee03422707b6310ba1e209603dcd6e26cdd3ada	strong and correlated strong equilibria in monotone congestion games	congestion trafic;game theory;congestion trafico;coalicion;videojuego;teoria juego;economic model;theorie jeu;video game;jeu video;modelo economico;modele economique;internet;traffic congestion;coalition;informatique theorique;jeu ordinateur;computer games;congestion game;congestion games;computer theory;informatica teorica;strong equilibrium	The study of congestion games is central to the interplay between computer science and game theory. However, most work in this context does not deal with possible deviations by coalitions of players, a significant issue one may wish to consider. In order to deal with this issue we study the existence of strong and correlated strong equilibria in monotone congestion games. Our study of strong equilibrium deals with monotone-increasing congestion games, complementing the results obtained by Holzman and Law-Yone on monotone-decreasing congestion games. We then present a study of correlated-strong equilibrium for both decreasing and increasing monotone congestion games.	computer science;game theory;nash equilibrium;network congestion;monotone	Ola Rozenfeld;Moshe Tennenholtz	2006		10.1007/11944874_8	game theory;the internet;simulation;economics;economic model;mathematical economics;operations research	ECom	-5.1298843158171135	0.2687438892629935	89667
5fff53138a1ae6d82747e28e605f52dbfa462594	improving expert meta-schedulers for grid computing through weighted rules evolution	bio inspired algorithms;fuzzy rule based systems;knowledge acquisition;grid computing	Grid computing is an emerging framework which has proved its effectiveness to solve large-scale computational problems in science, engineering and technology. It is founded on the sharing of distributed and heterogeneous resources capabilities of diverse domains to achieve a common goal. Given the high dynamism and uncertainty of these systems, a major issue is the workload allocation or scheduling problem which is known to be NP-hard. In this sense, recent works suggest the consideration of expert schedulers based on Fuzzy Rule-Based Systems (FRBSs) able to cope with the imprecise and changing nature of the grid system. However, the dependence of these systems with the quality of their expert knowledge makes it relevant to incorporate efficient learning strategies offering the highest accuracy. In this work, fuzzy rulebased schedulers are proposed to consider two learning stages where good quality IF-THEN rule bases acquired with a successful and well-known strategy rule learning approach, i.e., Pittsburgh, are subject to a second learning stage where the evolution of rule weights is entailed through Particle Swarm Optimization. Simulations results show that evolution of rule weights through this swarm intelligence -based strategy allows the improvement of the expert system schedules in terms of workload completion and increase the accuracy of the classical genetic learning strategy in FRBSs.	grid computing;meta-scheduling	Rocío Pérez de Prado;J. Enrique Muñoz Expósito;Sebastian García Galán	2011		10.1007/978-3-642-23713-3_26	legal expert system;conflict resolution strategy;computer science;knowledge management;artificial intelligence;machine learning;management science;grid computing	HPC	-15.839776260374164	-9.832408627393704	89751
aed17b6f70853a4d12700ad89db2b004f1fd6229	is the mystery an illusion? papineau on the problem of consciousness	functional properties;filosofi;explanatory gap;consciousness;philosophy;papineau;phenomenal concepts;science philosophy;physicalism;philosophie des sciences	A number of philosophers have recently argued that (i) consciousness properties are identical with some set of physical or functional properties and that (ii) we can explain away the frequently felt puzzlement about this claim as a delusion or confusion generated by our different ways of apprehending or thinking about consciousness. This paper examines David Papineau’s influential version of this view. According to Papineau, the difference between our “phenomenal” and “material” concepts of consciousness produces an instinctive but erroneous intuition that these concepts can’t co-refer. I claim that this account fails. To begin with, it is arguable that we are mystified about physicalism even when the account predicts that we shouldn’t be. Further, and worse, the account predicts that an “intuition of distinctness” will arise in cases where it clearly does not. In conclusion, I make some remarks on the prospects for, constraints on, and (physicalist) alternatives to, a successful defence of the claim (ii).	consciousness;physicalism	Pär Sundström	2007	Synthese	10.1007/s11229-007-9193-6	philosophy;epistemology;consciousness;mathematics;physicalism	ML	-12.689624884877377	2.9422049428177073	89889
2f47b5b0b425d5340d28ada91aa4d38388c57629	market design and motivated human trading behavior in electricity markets	experimental design;real time pricing;experimental method;financial data processing;continuous double auction;market power;federal energy regulatory commission;energy demand;controlled experiment;distribution company market design motivated human trading behavior electricity markets wholesale electricity trading cash motivated students rented computer laboratory facilities institutional arrangements electric power day ahead sealed bid trading simultaneous continuous double auctions bilateral trading eight node network transmission capacity wholesale generator company;electricity market;market design;electric power system;united kingdom;transmission loss;financial data processing electricity supply industry power distribution economics;institutional arrangement;power distribution economics;profitability;experimental research;electric power;electricity supply industry;new zealand;energy markets;humans power markets costs nuclear power generation displays fuels portfolios petroleum electricity supply industry content addressable storage;bilateral trade;free trade	This paper is based on a series of controlled experiments in the trading of wholesale electricity that expands substantially the scope of the research program reported previously. (Backerman, Rassenti and Smith, 1998; Backerman, Denton, Rassenti and Smith, 1998; Denton, Rassenti and Smith, 1998). The experiments employed cash motivated students and rented computer laboratory facilities of the University of Arizona. The primary objective of these experiments was to compare two alternative institutional arrangements for the trading of electric power. The first employed day-ahead sealed bid trading of energy for all periods in the subsequent day; the second employed simultaneous continuous double auctions for bilateral trading of energy up to the hour before delivery. All trading was executed on an eight-node network with limited transmission capacity. Each node was to be thought of as a control area, with one large wholesale generator company and one large distribution company resident there. 1. Modeling Generators Generator companies (Gencos) consisted of portfolios of generator units of various technological types (coal, oil, gas, hydro, nuclear), including multiple generator units of the same type. Parameters that characterized each type of generator, and the companies that owned them, are shown in Table 1. Large coal and nuclear units were represented by high capacity, low marginal costs, large start-up costs, large minimum loads (50% and 100% of maximum capacity, respectively), large fixed costs per hour, and long start-up times (10 and 60 hours respectively). Gas and oil fired turbines varied considerably in capacities and costs, but generally had high marginal costs, low fixed cost, and low minimum loads (5% of maximum capacity or less), but represented quick-start sources of reserve power in the event of unscheduled outages. If all generators in the network were arrayed from lowest to highest, according to their marginal cost and corresponding capacities, the resulting step function was the minimum willingness-to-accept supply schedule of power in the network, as shown in Figure 1 which abstracts from losses and transmission constraints. A portion of the screen display for the Genco at node 7 is shown in Figure 2. (The screen also displays the network and the Genco’s location in the network, but this is shown separately in Figure 6 below). Genco 7 controlled six classes of generator units designated A7, B7,..., G7. Each class consisted of multiple individual units, each unit represented by it’s own lightening icon. By clicking on the icon, the essential data for that generator was displayed in the box titled “A Costs”. For example, the 5 th unit in A7 is highlighted in the upper panel, and the data for that unit is indicated in the left lower panel. (The data are identical for all other class A units in Genco 7’s A portfolio). The first box displays the startup cost of this unit, 1500 (measured in ¶ units of experimental money, “pesos”). Then the ramp (start up) time, 10 hr, followed by the fixed (sunk) cost 389¶/H, pesos per hour. Under Fuel Cost, the first lists the fuel cost, 15 (¶/MWH), for the unit’s minimum loaded capacity step, 80 MW. Subsequent capacity steps, up to the maximum loaded capacity, are listed next. In this example, there is one additional step at the same fuel cost and capacity, the unit having a capacity of 160MW at a constant fuel cost up to capacity. All loaded generators were assigned forced outage rates based on NERC data for units of different types. If an outage was realized, it’s duration was then specified, and during those periods that unit could not be offered to the market. Proceedings of the 32nd Hawaii International Conference on System Sciences 1999 0-7695-0001-3/99 $10.00 (c) 1999 IEEE Proceedings of the 32nd Hawaii International Conference on System Sciences 1999	apple a7;bilateral filter;computer lab;downtime;experiment;marginal model;microwave;ramp simulation software for modelling reliability, availability and maintainability;thinking outside the box	Mark A. Olson;Stephen J. Rassenti;Vernon L. Smith;Mary L. Rigdon;Michael J. Ziegler	1999		10.1109/HICSS.1999.772842	free trade;open outcry;electric power;high-frequency trading;electricity market;electrical engineering;pairs trade;trading strategy;electric power system;design of experiments;trading turret;alternative trading system;statistics;profitability index;algorithmic trading	ML	-12.435163807791175	-8.558546923352399	89900
d011be69b134daec62e3ed88a2611f10c7612316	dynamic communication with biased senders		We study dynamic games in which senders with state-independent payoffs communicate to a single receiver. Senders’ private information evolves according to an aperiodic and irreducible Markov chain. We prove an analog of a folk theorem—that any feasible and individually rational payoff can be approximated in a perfect Bayesian equilibrium if players are sufficiently patient. In particular, there are equilibria in which the receiver makes perfectly informed decisions in almost every period, even if no informative communication can be sustained in the stage game. We conclude that repeated interaction can overcome strategic limits of communication.	approximation algorithm;irreducibility;markov chain;nash equilibrium;personally identifiable information	Chiara Margaria;Alex Smolin	2018	Games and Economic Behavior	10.1016/j.geb.2017.10.017	mathematical economics;welfare economics;mechanism design;bayesian game;aperiodic graph;folk theorem;private information retrieval;repeated game;stochastic game;markov chain;mathematics	ECom	-7.379741317464964	-5.950846759226391	89926
a4dedb26cee44e60bb641f4b4757335331d45596	causal reasoning for algorithmic fairness		In this work, we argue for the importance of causal reasoning in creating fair algorithms for decision making. We give a review of existing approaches to fairness, describe work in causality necessary for the understanding of causal approaches, argue why causality is necessary for any approach that wishes to be fair, and give a detailed analysis of the many recent approaches to causality-based fairness.	algorithm;causality;counterfactual conditional;fairness measure	Joshua R. Loftus;Chris Russell;Matt J. Kusner;Ricardo Silva	2018	CoRR		artificial intelligence;computer science;machine learning;causality;causal reasoning	ML	-15.33691876632985	2.5238988292637523	90059
a13ed8f56f007d2e3a758c37e5e8d37b89bace52	a non-zero-sum no-information best-choice game	optimal selection;secretary problem;optimal stopping game;optimal stopping;equilibrium values;interactive approach	A given number of n applicants are to be interviewed for a position of secretary. They present themselves one-by-one in random order, all n! permutations being equally likely. Two players I and II jointly interview the i-th applicant and observe that his (or her) relative rank is y for I and z for II, relative to i − 1 applicants that have already seen (rank 1 is for the best). Each player chooses one of the two choices Accept or Reject. If choice-pair is R-R, then the i-th is rejected, and the players face the next i+1-th applicant. If A-A is chosen, then the game ends with payoff to I (II), the expected absolute rank under the condition that the i-th has the relative rank y (z). If players choose different choices, then arbitration comes in, and forces players to take the same choices as I’s (II’s) with probability p (p̄ = 1 − p). Arbitration is fair if p = 1/2. If all applicants except the last have been rejected, then A-A should be chosen for the last. Each player aims to minimize the expected payoff he can get. Explicit solution is derived to this n stage game, and numerical results are given for some n and p. The possibility of an interactive approach in this selection problem is analyzed.	iterated function;numerical analysis;selection algorithm	Minoru Sakaguchi;Vladimir V. Mazalov	2004	Math. Meth. of OR	10.1007/s001860400366	simulation;optimal stopping;mathematics;secretary problem;mathematical economics;statistics	Theory	-5.957504215151792	-4.481370860447072	90060
126ae758309d96b9485aeaf933a1506ca424edab	answer set programming for computing decisions under uncertainty	conference report;conference proceeding	Possibility theory offers a qualitative framework for modeling decision under uncertainty. In this setting, pessimistic and optimistic decision criteria have been formally justified. The computation by means of possibilistic logic inference of optimal decisions according to such criteria has been proposed. This paper presents an Answer Set Programming (ASP)-based methodology for modeling decision problems and computing optimal decisions in the sense of the possibilistic criteria. This is achieved by applying both a classic and a possibilistic ASP-based methodology in order to handle both a knowledge base pervaded with uncertainty and a prioritized preference base.	answer set programming;computation;decision problem;knowledge base;possibility theory	Roberto Confalonieri;Henri Prade	2011		10.1007/978-3-642-22152-1_41	computer science;artificial intelligence;data mining;management science	AI	-16.098702641674286	-0.40272596726169285	90075
736dd888e797f89c1f23ca0d50971d71b7fac0c4	an experiment examining insider trading with respect to different market structures		As theoretical and empirical approaches suffer some shortcomings if it comes to analyzing insiders’ behavior, we conducted an adequate experiment. The experimental design incorporates traders with perfect information of the fundamental value of the tradeable asset. These insiders compete with regular uninformed participants on different market structures, in particular on a transparent and on an intransparent call market as well as on a continuous double auction. The results shed first light on a couple of interesting issues, including whether and how insiders try to stay undetected, how their profits are accumulated and what market structure is more advantageous for insiders.		Philipp Hornung;Gernot Hinterleitner;Ulrike Leopold-Wildburger;Roland Mestel;Stefan Palan	2011		10.1007/978-3-642-29210-1_35	market microstructure	ECom	-5.160462834553711	-7.566480794669407	90145
9bdfaa8d11cd8996686249dccb6725af3b102221	pivotal voting and the emperor's new clothes	feddersen;game theory;banks;pesendorfer;austin smith;pivotal voting	A leading recent line of work in game theory applied to politics exploits the “pivotal voting” insight introduced by Austen-Smith and Banks [1]. The most prominent follow-on papers have been by Feddersen and Pesendorfer [2, 3, 4, 5], where a particularly striking result is that in a large election, the winner with many poorly-informed voters will be identical to the winner under full information. But of course any result whatever can be proven if sufficiently implausible assumptions are allowed. This review provides simple derivations of the elections result for both the FP 1996 model [2] and then for the (generalized) FP 1999 model [4]. The resulting transparency makes it easy to see the relation between the models, but also why neither result is relevant to actual elections.		Howard Margolis	2002	Social Choice and Welfare	10.1007/s355-002-8327-0	game theory;economics;mathematics;sociology;microeconomics;mathematical economics;operations research;law	ECom	-8.212514869708556	-4.551106786869088	90211
f5d40049c71300d44767fb4fb92cf7d1406f1ef8	subgame perfect equilibria in model with bargaining costs varying in time	equilibre parfait sous jeu;subgame perfect equilibrium;game theory;negociation;bargaining costs;time variation;teoria juego;theorie jeu;variation temporelle;time varying system;cout negociation;jeu 2 personnes;juego 2 personas;negociacion;systeme parametre variable;two person game;bargaining;sistema parametro variable;key words bargaining costs;subgame perfect equilibria;variacion temporal	The paper presents the bargaining model in which preferences of each player are expressed by the sequence of bargaining costs varying in time. There are theorems describing subgame perfect equilibria for some models with the bargaining costs varying in time. In the class of strategies independent of the former history, a delay in reaching an agreement of subgame perfect equilibrium is impossible. However, if strategies depend on the former history of the game, then an agreement can be reached with delay. An adequate example in which a delay appears is presented.		Agnieszka Rusinowska	2002	Math. Meth. of OR	10.1007/s001860200219	bargaining problem;markov perfect equilibrium;game theory;simulation;subgame;mathematical economics;subgame perfect equilibrium;negotiation	Theory	-5.1503433510356444	0.460311593722101	90321
cb20c6bfeac5ff83a749a588878b804ad22cc534	conditional submodular coherent risk measures		A family of conditional risk measures is introduced by considering a single period financial market, relying on a notion of conditioning for submodular capacities, which generalizes that introduced by Dempster. The resulting measures are expressed as discounted conditional Choquet expected values, take into account ambiguity towards uncertainty and allow for conditioning to “null” events. We also provide a characterisation of consistence of a partial assessment with a conditional submodular coherent risk measure. The latter amounts to test the solvability of a suitable sequence of linear systems.	coherent;submodular set function	Giulianella Coletti;Davide Petturiti;Barbara Vantaggi	2018		10.1007/978-3-319-91476-3_20	financial economics;submodular set function;expected value;financial market;conditioning;ambiguity;coherent risk measure;linear system;mathematics	ML	-7.795431950237751	-0.27825549130764404	90346
05444b0cc8aac81f6d5545bb5a8e53fc18bda62d	inertia in social learning from a summary statistic	asymptotics;belief;unawareness;learning;action learning;social learning;rate of learning;heterogeneous preferences;slow learning;information and knowledge;d83 search;herding;d81 criteria for decision making under risk and uncertainty;private information;communication;speed of learning;echo chamber	We model normal-quadratic social learning with agents who observe a summary statistic over past actions, rather than complete action histories. Because an agent with a summary statistic cannot correct for the fact that earlier actions influenced later ones, even a small presence of old actions in the statistic can introduce very persistent errors. Depending on how fast these old actions fade from view, social learning can either be as fast as if agents’ private information were pooled (rate n) or it can slow to a crawl (rate lnn). We also examine extensions to learning from samples of actions, learning about a moving target, heterogeneous preferences, and biases toward own information.	persistent data structure;personally identifiable information	Nathan Larson	2015	J. Economic Theory	10.1016/j.jet.2015.06.003	social learning;herding;private information retrieval;belief;mathematics;mathematical economics;action learning;statistics	ML	-7.258636640456331	-5.369172114720257	90444
414a1f2225fe64f0b846ed825ecd47032de5e13c	the application of statistical reliability theory in the context of intelligent environments: a tutorial review	computer science and informatics;applied mathematics	Intelligent Environments often require the integration of multi-modal sensing and actuating technologies with high performance real-time computation, including artificial intelligence systems for analysis, learning patterns and reasoning. Such systems may be complex, and involve multiple components. However, in order to make them affordable, Intelligent Environments sometimes require many of their individual components to be low-cost. Nevertheless, in many applications—including safety-critical systems, and systems monitoring the health and well-being of vulnerable individuals, it is essential that these Intelligent Environment systems are reliable, which the issue of affordability must not compromise. If such environments are to find real application and deployment in these types of domain, it is necessary to be able to obtain accurate predictions of how probable any potential failure of the system is in any given timeframe, and of statistical parameters regarding the expected time to the first, or between successive, failures. Such quantities must be kept within what are deemed to be acceptable tolerances if the Intelligent Environment is to be suitable for applications in these critical areas, without requiring excessively high levels of human monitoring and/or intervention. In this paper, an introductory overview of statistical reliability theory is presented. The applicability of this to the context of Intelligent Environments—particularly those involving safety critical or other sensitive issues—is discussed, along with how such reliability modelling can be used to influence the design, implementation and application of an Intelligent Environment.	artificial intelligence;average-case complexity;complex systems;computation;intelligent environment;modal logic;real-time locating system;reliability engineering;software deployment	Gordon Hunter	2015	Journal of Reliable Intelligent Environments	10.1007/s40860-015-0004-4	simulation;applied mathematics;intelligent decision support system;computer science;artificial intelligence;software engineering;mathematics;management science	AI	-18.109454293148993	-3.1525384366464726	90445
f6c3c32e0154a87129d984607c5b31d09cc1e28e	contracts as games on event structures	session types;contract calculus;contracts;games;compliance;event structures	Event structures are one of the classical models of concurrent systems. The idea is that an enabling  X⊢eX⊢e represents the fact that the event e can only occur after all the events in the set X have already occurred. By interpreting events as actions promised by some participants, and by associating each participant with a goal (a function on sequences of events), we use event structures as a formal model for contracts. The states of a contract are sequences of events; a participant has a contractual obligation (in a given state) whenever some of its events is enabled in such a state. To represent the fact that participants are mutually distrusting, we study concurrent games on event structures; there, participants may play by firing events in order to reach their goals, and eventually win, lose or tie.#R##N##R##N#A crucial notion arising in this setting is that of agreement: a participant agrees on a set of contracts if she has a strategy to reach her goals in all the plays conforming to her strategy (or to make another participant sanctionable for not honouring an obligation). Another relevant notion is protection: a participant is protected by her contract when she has a strategy to avoid losing in any contexts, even in those where she has not reached an agreement. We study conditions for obtaining agreement and protection, and we show that these properties mutually exclude each other in a certain class of contracts. We then relate the notion of agreement in contracts with that of compliance in session types. In particular, we show that compliance corresponds to the fact that eager strategies lead to agreement.		Massimo Bartoletti;Tiziana Cimoli;G. Michele Pinna;Roberto Zunino	2016	J. Log. Algebr. Meth. Program.	10.1016/j.jlamp.2015.05.001	games;computer science;mathematics;computer security	Logic	-11.760426201175097	-0.7469244858375762	90446
f902b0a98e706aad83e1a24f46aa51925d7b4ca5	not all explanations predict satisfactorily, and not all good predictions explain	no keywords	"""This short comment on Epstein's (2008) paper and on the response by Thompson and Derr (2008) argues that the symmetry between explanation and prediction cannot satisfactorily be discussed without making clear what prediction means—depending on which connotations the authors have with 'prediction' their arguments can or cannot be accepted. 1.1 Both Epstein (2008) and the authors of response (Thompson and Derr 2008, this issue of JASSS) miss one important point: Both take """"prediction"""" as a term with a fixed and unambiguous meaning—which it is not. Epstein seems to understand prediction in the sense this word has in the sentence """"the orbits of the planets will never be predicted."""" By the way, in this argumentation he forgets that 2000 years before Newton solar and lunar eclipses could be predicted with surprisingly high precision with a model which we now find wrong (which means that a good prediction is not necessarily a good explanation). Thompson's and Derr's reply accepts a much weaker meaning of """"predict"""" as they call """"earthquakes occur"""" a prediction (for which no theory is necessary, after the first earthquake that ever occurred it was clear to humankind that such things happen). Of course, the predictive power of plate tectonics is even higher as it predicts that in certain parts of the planet earthquakes will be more frequent than in other regions. This finding makes it necessary to argue that there are at least three different meanings of prediction, namely as an answer to three quite different questions (see Troitzsch 1997, p. 46): 1 """"Which kinds of behaviour can be expected [from a system like this] under arbitrarily given parameter combinations and initial conditions? 2 Which kind of behaviour will a given target system (whose parameters and previous states may or may not have been precisely measured) display in the near future? 3 Which state will the target system reach in the near future, again given parameters and previous states which may or may not have been precisely measured?"""" 1.2 With this distinction between different kinds of prediction, the controversy between Epstein and his critics is easily ended: Any kind of good explanation, under all circumstances will yield a prediction of type 1 (and perhaps also of type 2), but not every good explanation will yield a prediction if type 3. The prediction of type 3 can even be subdivided in a stochastic or statistical version and a deterministic one (see Troitzsch 1997, p. 46-47): 3a Which are the expected value and the confidence interval around the expected value of the state the target system will reach in the near future, again given parameters and previous states which may or may not have been precisely measured? 3b Which exact value will the state of the target system have at a certain point of time in the near future, again given parameters and previous states which have been precisely measured? (This version was not even mentioned in Troitzsch 1997 and seems to be excluded in Epstein's paper, paragraph 1.8, as well!) The prediction of a solar or lunar eclipse is obviously of type 3b, as everybody knows who ever observed one of these phenomena. The prediction of the temperature at some time during the next few days is obviously of type 3a, as we know from weather forecasts on the TV (where they usually show the trajectory of the expected temperature for the next few days, surrounded by a funnel which shows the growing confidence interval). 1.3 But there is another claim to make: it is not the case that Carl Hempel's symmetry hypothesis was never contested. Some fifty years ago there was a long discussion (see Scriven 1959 and Grünbaum 1962, and see also Stegmüller 1966, where he points to the vagueness and ambiguity of the terms explanation and prediction, and Stegmüller 1969, chapter II, with a very detailed analysis of explanation, retrodiction and prediction). Hempel's argumentation with respect to the symmetry thesis after this discussion (Hempel 1965) is much more detailed, and he argues only that an adequate explanation is a potential prediction (a thesis which he defends strongly) and that an adequate prediction is a potential explanation (here he is more retentive, in principle accepting that the precise predictions of solar eclipses by ancient astronomers were based on wrong explanations). The problem here is the word """"adequate"""". It is quite obvious that his notion of adequacy is similar to Zeigler's (1985, p. 5) """"structural validity"""" since whenever a model is structurally valid (""""if it not only reproduces the observed real system behaviour, but truly reflects the way in which the real system operates to produce its behaviour"""") it can be expected to make predictions of any of the types 1 through 3a (of type 3b only in the entirely deterministic case)—but even non-adequate explanations are potential and even good predictions: see the case of solar and lunar eclipses predicted by Babylonian astronomers. 1.4 In the case of living, cognitive and social systems (and in other complex systems as well) cause-effect relations are rarely as simple as in the examples referred to by Hempel, Grünbaum, Scriven and Stegmüller mentioned above (their arguments are mainly about artillery, astronomy, hydraulics, only Scriven refers to medicine, namely the case of late symptoms of a particular disease which may or may not occur—but Scriven is rather an opponent of the symmetry thesis). In these complex systems sometimes the same effect can be produced by different combinations of different causes (see the discussion in Hempel 1965, footnote 46, where he argues that in the medical cases referred to by opponents of the symmetry thesis the available information is often not sufficient to explain the observed outcome). And this is in line with Epstein's argumentation that """"Explanation Does Not Imply Prediction""""—if and only if we interpret Epstein's prediction as a prediction of type 3b; if we restrict the meaning of prediction to type-1 or type-2 prediction, then a claim that an explanatory theory can predict is certainly possible."""	complex systems;eclipse;initial condition;journal of artificial societies and social simulation;make:;nsa product types;newton;social system;spherical basis;vagueness	Klaus G. Troitzsch	2009	J. Artificial Societies and Social Simulation		computer science;artificial intelligence;mathematics;sociology;social psychology	NLP	-12.056339112752031	1.0247987916318941	90936
02c522e1371c85bccd8e6a425275ab43bafc71e4	economic decision-making in free-to-play games: a laboratory experiment to study the effects of currency conversion		We present initial results from a controlled laboratory experiment where the economic decisionmaking typical in free-to-play games was studied. The participants were presented with a series of scenarios, where they rated how much they were willing to pay (in euros, in hard currency, or in soft currency) for common in-app virtual goods (booster, unlock, timer). The goal of the study was to examine how the multiple currency conversions and the amount of resources affect the perceived value of the virtual goods and the willingness to pay for them. The results don’t support the notion that the currency conversions would lead to increased spending. When comparing the willingness to pay in different currencies by first converting them to a unitary currency, the participants were willing to use highest amount of resources when considering purchases in euros and least when considering purchases in soft currency. However, when considering purchases with euros, the participants were willing to pay most when they had moderate amount of virtual currency. But with gold the willingness to pay was highest when the amount of resources was the highest. This finding highlights the differences in how the players may process real money and in-game currencies. In addition, the results imply that regardless of the currency type, the participants were willing to pay most for unlocking of new game content. It is suggested that the economic decision making in freeto-play games could be studied also with abstract and simplified laboratory experiments.	booster (electric power);experiment;money;purchasing;sim lock;timer;virtual currency;virtual goods;virtual world	Mikko Salminen;Simo Järvelä;Niklas Ravaja	2018				HCI	-6.8202404790415185	-8.46833778833595	91081
6c27c088b11ca17aae71574e10bd79bc8651b53f	symmetric zero-sum games with only asymmetric equilibria	symmetric game;zero sum game;symmetric equilibrium	We know that a) two-player symmetric zero-sum games with non-empty equilibrium sets always admit symmetric equilibria and that b) two-player and multiplayer symmetric non-zero-sum games might have only asymmetric equilibria (Fey, 2012). But what about multiplayer symmetric zero-sum games? This paper shows that these games might also have only asymmetric equilibria. One of the examples employed to illustrate this point is the three-candidate version of the popular Hotelling–Downs model of electoral competition. This demonstrates that symmetric games with only asymmetric equilibria are not technical paradoxes but are integrated in economics and political science literature for quite a while.		Dimitrios Xefteris	2015	Games and Economic Behavior	10.1016/j.geb.2014.12.001	mathematical optimization;economics;microeconomics;zero-sum game;mathematical economics;symmetric game;symmetric equilibrium	ECom	-5.228483177894866	-1.5111286236180863	91135
60127c9ef83e8d16c4f2a93d1ea0dc1920b3f22b	fair and group strategy-proof good allocation with money	no envy;group strategy proof mechanism;anonymity in welfare	We completely characterize the class of fair and group strategy-proof mechanisms. We consider two notions of fairness, anonymity in welfare and no-envy. Both fairness axioms, when applied with strategy-proofness, imply decision efficiency, and lead to the same class of group strategy-proof mechanisms (where the group size is restricted to two). We find that the only feasible mechanism satisfying a mild zero transfer axiom, in this class, is the Pivotal mechanism. Copyright Springer-Verlag Berlin Heidelberg 2014	money	Conan Mukherjee	2014	Social Choice and Welfare	10.1007/s00355-013-0733-3	economics;mathematical economics;social psychology;welfare economics	ECom	-6.18557625645571	-2.917588468884588	91441
d1c0e807ae45084feeae872c63aa0077a5c5c169	an optimal approach for low-power migraine prediction models in the state-of-the-art wireless monitoring devices		Wearable monitoring devices for ubiquitous health care are becoming a reality that has to deal with limited battery autonomy. Several researchers focus their efforts in reducing the energy consumption of these motes: from efficient micro-architectures, to on-node data processing techniques. In this paper we focus in the optimization of the energy consumption of monitoring devices for the prediction of symptomatic events in chronic diseases in real time. To do this, we have developed an optimization methodology that incorporates information of several sources of energy consumption: the running code for prediction, and the sensors for data acquisition. As a result of our methodology, we are able to improve the energy consumption of the computing process up to 90% with a minimal impact on accuracy. The proposed optimization methodology can be applied to any prediction modeling scheme to introduce the concept of energy efficiency. In this work we test the framework using Grammatical Evolutionary algorithms in the prediction of chronic migraines.	clock signal;data acquisition;design automation and test in europe;evolutionary algorithm;low-power broadcasting;mathematical optimization;multi-objective optimization;predictive modelling;real-time computing;run time (program lifecycle phase);sensor;wearable computer	Josué Pagán;Ramin Fallahzadeh;Hassan Ghasemzadeh;José Manuel Moya;José Luis Risco-Martín;José Luis Ayala	2017	Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017		control engineering;microcontroller;embedded system;mathematical optimization;electronic engineering;real-time computing;simulation;computer science;engineering;sensor;predictive modelling	EDA	-13.386760104052394	-3.257088891608277	92228
146a1b47dc28a43a8632e83ffcb6bf2b1c4f89e4	on the status quo sets induced by the raiffa solution to the two-person bargaining problem	bargaining problem;curva;game theory;negociation;axiomatic;raiffa solution;courbe;teoria juego;theorie jeu;prise decision;curve;jeu 2 personnes;axiomatico;solution raiffa;juego 2 personas;negociacion;two person game;bargaining;axiomatique;toma decision	We prove that the status quo sets induced by the Raiffa Solution to the two-person Bargaining Problem are curves , except when the bargaining domain is rectangular. This property is used in a separate paper by Livne (Livne, Z. 1987. Axiomatic characterization at the Raiffa solution to the Nash bargaining problem. Columbia Business School. Forthcoming in Oper. Res. ) in the axiomatization of the Raiffa Solution.		Zvi A. Livne	1989	Math. Oper. Res.	10.1287/moor.14.4.688	bargaining problem;game theory;artificial intelligence;mathematics;curve;axiom;mathematical economics;negotiation	Theory	-6.3968951577685225	-1.2798238697865914	92242
632a65429f28587e0f6fa6ab402f6931d763a674	node-consistent core for games played over event trees	computacion informatica;time consistency;nucleolus;grupo de excelencia;core;s adapted strategies;ciencias basicas y experimentales;imputation distribution procedure;node consistency;s;stochastic games	We consider a class of dynamic games played over an event tree, where the players cooperate to optimize their expected joint payoff. Assuming that the players adopt the core as the solution concept of the cooperative game, we devise a node-decomposition of the imputations in the core such that each player finds it individually rational at each node to stick to cooperation rather than switching to a noncooperative strategy. We illustrate our approach with an example of pollution control.	fault tree analysis	Elena M. Parilina;Georges Zaccour	2015	Automatica	10.1016/j.automatica.2015.01.007	core;simulation;mathematics;mathematical economics;nucleolus;time consistency	Logic	-5.526379099038985	-3.1377037426180223	92318
71d6fb04fb07506c02f99bffd198ede2fe4e2bdd	integrated motion planning and coordination for industrial vehicles	constraint based reasoning;robotics;scheduling;multi robot systems;robotteknik och automation;motion planning;datavetenskap;industrial application;computer science;coordination	A growing interest in the industrial sector for autonomous ground vehicles has prompted significant investment in fleet management systems. Such systems need to accommodate on-line externally imposed temporal and spatial requirements, and to adhere to them even in the presence of contingencies. Moreover, a fleet management system should ensure correctness, i.e., refuse to commit to requirements that cannot be satisfied. We present an approach to obtain sets of alternative execution patterns (called trajectory envelopes) which provide these guarantees. The approach relies on a constraint-based representation shared among multiple solvers, each of which progressively refines trajectory envelopes following a least commitment principle.	algorithm;autonomous robot;correctness (computer science);execution pattern;fleet management system;high- and low-level;industrial pc;motion planning;online and offline;requirement;simulation;solver;state space	Marcello Cirillo;Federico Pecora;Henrik Andreasson;Tansel Uras;Sven Koenig	2014			mathematical optimization;real-time computing;simulation;computer science;artificial intelligence;motion planning;robotics;scheduling	Robotics	-18.922667053766983	-8.269524347324598	92433
66741918f9945941173db801b6b6d20d879dffed	common belief foundations of global games	higher order beliefs;rationalizability;rank beliefs;risk dominance;global games	We provide a characterization of when an action is rationalizable in a binary action coordination game in terms of beliefs and higher order beliefs. The characterization sheds light on when a global game yields a unique outcome. In particular, we can separate those features of the noisy information approach to global games that are important for uniqueness from those that are merely incidental. We derive two su¢ cient conditions for uniqueness that do not make any reference to the relative precision of public and private signals. Preliminary version. We acknowledge support from the NSF under the grant 0648806.	ibm notes	Stephen Morris;Hyun Song Shin;Muhamet Yildiz	2016	J. Economic Theory	10.1016/j.jet.2016.03.007	economics;rationalizability;mathematics;microeconomics;risk dominance;mathematical economics;global game	Theory	-6.060034615888384	-1.431744708752084	92452
da5fc74d37a3d403287314a468bf498031fe20ed	learning of task allocation method based on reorganization of agent networks in known and unknown environments	特集 multiagent based societal systems distributed task allocation;reorganization;learning;team formation;distributed task allocation;coordination	We propose a team formation method that integrates the estimating of the resources of neighboring agents in a tree-structured agent network in order to allocate tasks to the agents that have sufficient capabilities for doing tasks. A task for providing the required service in a distributed environment is often achieved by a number of subtasks that are dynamically constructed on demand in a bottom-up manner and then done by the team of appropriate agents. A number of studies were conducted on efficient team formation for quality services. However, most of them assume that resources in other agents are known, and this assumption is not adequate in real world applications. The contribution of this paper is threefold. First, we extend the conventional method by combining the learning of task allocation and the reorganization of agent networks. In particular, we introduce the elimination of links as well as the generation of links in the reorganization. Second, we revise the learning method so as to use only information available locally. Finally, we omitt the assumption that all resource information in other agents is given in advance. Instead, we extend the task allocation method by combining it with the resource estimation of neighboring agents. We experimentally show that this extension can considerably improve the efficiency of team formation compared with the conventional method even though it does not require knowledge of resources in other agents. We also show that it can make the agent network adaptive to environmental changes.	bottom-up parsing;coefficient;experiment;internet;top-down and bottom-up design;tree structure	Kazuki Urakawa;Toshiharu Sugawara	2014	JIP	10.2197/ipsjjip.22.289	simulation;computer science;knowledge management;distributed computing	AI	-16.52234901653099	-9.706755132915589	92537
fe69f5a23c854cb927983a9a5b9479fd9bed4877	evolution of energy awareness using an open carbon footprint calculation platform				Farzana Rahman;Sheikh Iqbal Ahamed;Casey O'Brien;He Zhang;Lin Liu	2012		10.1201/b16631-60	waste management;environmental science;carbon footprint	Robotics	-12.846500783490626	-3.5387754578298978	92588
d9c1878f460dea59ab9520cedf6e5a8a147efc4f	rational agents and the processes and states of negotiation	rational agent	This thesis shows how a verified and unambiguous theory of a protocol with known properties enables rational agents to interact in a negotiation process and to finally satisfy their goals using strategies and plans. This is achieved through an application of an extended form of propositional dynamic logic in the verification, validation and reasoning about interaction protocols in a multi-agent system. Agent interaction, as a key aspect in multi-agent systems and automated negotiation, has lead to a number of proposed agent communication languages and protocols. In contrast to a language, a rational agent can reason about a protocol to strategically plan possible courses of action in a bid to achieve its goals. Existing techniques for specifying protocols have resulted in faulty and ambiguous interaction protocols, leading to contradictory beliefs between agents. There remains a need for formally specifying and validating sharable interaction protocols with desirable properties. This thesis specifies, verifies and analyses protocols for automated negotiation through the application of Artificial Intelligence techniques. To this end, a meta-language called ANML is specified as an extension of propositional dynamic logic. The syntax and the semantics of ANML, including axioms and inference rules that hold in this normal modal system, are defined. Interaction protocols between agents can be concisely and completely specified in ANML allowing representation and reasoning about the states and processes of a negotiation. Examples of protocols for various types of negotiation are given as logical theories in ANML. This thesis verifies interaction protocols proposed in statecharts and AUML, [Odell and al. 2000], and show the inadequacy of these notations for specifying communication in multi-agent systems. For each verified protocol, its correct version is given in ANML. In addition to correctness, a protocol may exhibit safety, liveness or game theoretic properties. Axioms in ANML for a range of safety and liveness	artificial intelligence;communications protocol;correctness (computer science);dynamic logic (modal logic);formal specification;game theory;interaction protocol;liveness;modal logic;multi-agent system;rational agent	Shamimabi Paurobally	2002			management science;rational agent;negotiation;computer science	AI	-17.561103417371825	3.8835628273271134	92727
ddc682f2f0e3285cb7616ff1e2ce8a67479ff468	special issue on vision-based mobile robots	mobile robot	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.		Jun'ichi Takeno	1993	Advanced Robotics	10.1163/156855394X00086	mobile robot;computer science;artificial intelligence;social robot;mobile manipulator;robot control;mobile robot navigation	Robotics	-15.026404787778906	-6.789077950364146	92894
99e7789c4fdb4b9962ae6c96b89cde28c11d9ae9	shannon revisited: considering a more tractable expression to measure and manage intractability, uncertainty, risk, ignorance, and entropy	risk management;knowledge acquisition;probability measure;information theory	Building on Shannon‟s lead, let‟s consider a more malleable expression for tracking uncertainty, and states of “knowledge available” vs. “knowledge missing,” to better practice innovation, improve risk management, and successfully measure progress of intractable undertakings. Shannon‟s formula, and its common replacements (Renyi, Tsallis) compute to increased knowledge whenever two competing choices, however marginal, exchange probability measures. Such and other distortions are corrected by anchoring knowledge to a reference challenge. Entropy then expresses progress towards meeting that challenge. We introduce an „interval of interest‟ outside which all probability changes should be ignored. The resultant formula for Missing Acquirable Relevant Knowledge (MARK) serves as a means to optimize intractable activities involving knowledge acquisition, such as research, development, risk management, and opportunity exploitation.	cobham's thesis;distortion;knowledge acquisition;marginal model;resultant;risk management;shannon (unit)	Gideon Samid	2010	CoRR		probability measure;risk management;information theory;artificial intelligence;data mining;mathematics;management science;statistics	AI	-9.902970557641636	-3.8106658987441824	93011
4c934f7c141502c5de24c1d76a60c6f5ad54ca1a	"""""""count forward three score and ten..."""""""		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Blanchard Hiatt	1977	Cryptologia	10.1080/0161-117791832850		Robotics	-14.9461151179894	-5.701195717519757	93194
47f62b2bf9df45e6922978cbeceb6f04aa610e15	an algorithmic analysis of simulation strategies	algorithm analysis;time management;discrete event simulation	Each discrete event simulation language incorporates a time control procedure to conduct timing management and next event selection. Each time control procedure embodies, and thus imposes, a strategy (approach, method) for next event selection- and thereby determines the world view of a language. The three generally recognized strategies are event scheduling, activity scanning and process interaction. This paper presents algorithmic formulations of the three strategies and their modeling routines, as well as detailed discussions and comparisons of the strategies. The algorithmic formulations serve to aid understanding by describing essential aspects of the strategies while excluding implementation details which are not strategy-dependent, and which tend to detract from the essential concepts. A significant practical application of the formulations is discussed. This consists of merging the algorithms for the event scheduling and process interaction strategies into one algorithm, which then served as a model for combining GPSS and GASP IV into a simulation system providing the individual capabilities of both language, and the capability to intermix GPSS and GASP within a single model.	algorithm;gpss;scheduling (computing);simulation language	James W. Hooper;Kevin D. Reilly	1982	International Journal of Computer & Information Sciences	10.1007/BF00995526	real-time computing;simulation;time management;discrete event dynamic system;computer science;complex event processing;discrete event simulation;distributed computing;event;simulation language	Robotics	-19.077719714072988	-4.692398418376617	93229
15efd540f86d41ddb45aad25d1d8b94ec777bfa6	selecting winners with partially honest jurors		We consider the effect of “partially honest” jurors (along the lines of Dutta and Sen (2012)) in a model of juror decisions developed in Amoros (2013). We analyse the problem of choosing the w contestants who will win a competition within a group of n>w competitors. All jurors know who the w best contestants are. All the jurors commonly observe who the w best contestants are, but they may be biased (in favour of or against some contestants). We assume that some of these jurors are partially honest. A partially honest individual has a strict preference for revealing the true state over lying when truth-telling does not lead to a worse outcome (according to preferences in the true state) than that which obtains when lying. The socially optimal rule is to always select the w best contestants, in every possible state of the world. We first look at the many person implementation, when the jury consists of at least two partially honest jurors, whose identity is not known to the planner. We find that the socially optimal rule is Nash implementable if for each pair of contestants, there are two jurors who treat the pair in an unbiased manner and one of these jurors is partially honest. However it is not necessary for the planner to know the identity of the jurors who are fair over a given pair. The result shows that the presence of partially honest jurors expands the scope of implementation. We also analyse the problem, when there are only two jurors and consider cases both with and without the assumption of partial honesty.		Sonal Yadav	2016	Mathematical Social Sciences	10.1016/j.mathsocsci.2016.05.005	welfare economics	AI	-6.975429225111876	-3.7195729964211495	93241
f7ddb6edc36f5319976fefc67ea8f4a064651133	order of play in strategically equivalent games in extensive form	extensive form;equivalent game;statistical significance;coordination game	“Can we find a pair of extensive form games that give rise to the same strategic form game such that, when played by a reasonable subject population, there is a statistically significant difference in how the games are played?” (Kreps, 1990, p. 112). And if yes, “can we organize these significant differences according to some principles that reflect recognizable differences in the extensive forms?” Both questions are answered positively by reporting results from three different experiments on public goods provision, resource dilemmas, and pure coordination games.		Amnon Rapoport	1997	Int. J. Game Theory	10.1007/BF01262516	coordination game;economics;operations management;repeated game;statistical significance;sequential game;welfare economics	Logic	-5.653722722826507	-4.16263421856699	93391
3e7ca8c4cde4a1902a8fa3fc02a5209f72c8df00	axiomatic characterization of belief merging by negotiation	swinburne;belief merging;belief negotiation	Belief merging has been an active research field with many important applications. The major approaches for the belief merging problems, considered as arbitration processes, are based on the construction of the total pre-orders of alternatives using distance functions and aggregation functions. However, these approaches require that all belief bases are provided explicitly and the role of agents, who provide the belief bases, are not adequately considered. Therefore, the results are merely ideal and difficult to apply in the multi-agent systems. In this paper, we approach the merging problems from other point of view. Namely, we treat a belief merging problem as a game, in which rational agents participate in a negotiation process to find out a jointly consistent consensus trying to preserve as many important original beliefs as possible. To this end, a model of negotiation for belief merging is presented, a set of rational and intuitive postulates to characterize the belief merging operators are proposed, and a representation theorem is presented.	aggregate function;belief revision;multi-agent system;rational agent	Trong Hieu Tran;Ngoc Thanh Nguyen;Quoc Bao Vo	2012	Multimedia Tools and Applications	10.1007/s11042-012-1136-7	artificial intelligence;management science	AI	-12.617002236226671	-0.8340756522280693	93632
f0602d208a458826db053d1ebcfbdeec3395c74f	the logical handling of threats, rewards, tips, and warnings	argumentation;cognitive psychology;threats rewards;tips warnings;negotiation	Previous logic-based handling of arguments has mainly focused on explanation or justification in presence of inconsistency. As a consequence, only one type of argument has been considered, namely the explanatory type; several argumentation frameworks have been proposed for generating and evaluating explanatory arguments. However, recent investigations of argument-based negotiation have emphasized other types of arguments, such as  threats, rewards, tips , and  warnings . In parallel, cognitive psychologists recently started studying the characteristics of these different types of arguments, and the conditions under which they have their desired effect. Bringing together these two lines of research, we present in this article some logical definitions as well as some criteria for evaluating each type of argument. Empirical findings from cognitive psychology validate these formal results.		Leila Amgoud;Jean-François Bonnefon;Henri Prade	2007		10.1007/978-3-540-75256-1_23	knowledge management;management science;negotiation	Crypto	-14.81840729423544	2.2752361879530225	93917
fdd51b9209af00ccc4a539f93ef8e5f974dd2c67	iterated belief revision in the face of uncertain communication	information sources;belief revision;representation theorem	This paper offers a formalization of iterated belief revision for multiagent communication using the logic of communication graphs introduced in [15]. In this study we consider an agent (i.e., information source) capable of sending two types of message. In the first type, he tells that he knowsa proposition, but in the second type, he tells that he b lievesa proposition. Consequently, iterated belief revision is brought about through a sequence of communication events (i.e., a history), and we propose a variation of the AGM rational postulates for history based belief revision. As we will show, a representation theorem is verified only for a class of restricted communication graphs. We consider this result to be a weak point in the application of the AGM postulates to multiagent communication, and propose a viable alternative.	agent-based model;belief revision;information source;iterated function;iteration	Yoshitaka Suzuki;Satoshi Tojo;Stijn De Saeger	2008		10.1007/978-3-540-93920-7_12	computer science;artificial intelligence;belief revision;algorithm	AI	-17.515291975417778	4.034444388175239	94036
9c89325bec33a156b669fb94b6fbd54abcedfe92	a logical account of institutions: from acceptances to norms via legislators	legislation	The aim of this paper is to provide a logical framework which enables reasoning about institutions and their dynamics. In our approach an institution is grounded on the acceptances of its members. We devote special emphasis to the role of legislator. We characterize the legislator as the role whose function is the creation and the modification of legal facts (e.g. permissions, obligations, etc.): the acceptance of the legislators that a certain norm is valid ensures that the norm is valid. The second part of the paper is devoted to the logical characterization of two important notions in the domain of legal and social theory: the notion of constitutive rule and the notion of norm of competence. A constitutive rule is a rule which is responsible for the creation of new kinds of (institutional) facts. A norm of competence is a rule which assigns powers to the agents playing certain roles within the institution. We show that norms of competence provide the criteria for institutional change.	logical framework	Emiliano Lorini;Dominique Longin	2008			computer science;artificial intelligence	AI	-15.07590749895234	3.916991513842077	94104
b1060446bcde2dc368f4b7796e2eb2b0fb6b75b5	the eyes of things project	embedded vision;computer vision;eyes of things	"""The Eyes of Things (EoT) EU H2020 project envisages a computer vision platform that can be used both standalone and embedded into more complex artifacts, particularly for wearable applications, robotics, home products, surveillance etc. The core hardware will be based on a Software on Chip (SoC) that has been designed for maximum performance of the always-demanding vision applications while keeping the lowest energy consumption. This will allow """"always on"""" and truly mobile vision processing. This demo presents the first prototype applications developed within EoT. First, example vision processing applications will be shown. Additionally, an RTSP server implemented in the device will be demonstrated. This server can capture and stream images. Finally, connectivity will be shown using a minimal MQTT broker specifically implemented for the device."""	computer vision;embedded system;high availability;mqtt;prototype;robotics;server (computing);wearable computer;wearable technology	Noelia Vállez;José Luis Espinosa-Aranda;Oscar Déniz-Suárez;Daniel Aguado-Araujo;Gloria Bueno García;Carlos Sanchez-Bueno	2015		10.1145/2789116.2802648	embedded system;computer vision;simulation;active vision;computer science	Robotics	-14.899264991320974	-2.2926077392132798	94134
c81720ee4fd50237f538bc2f5301c7837e5c3472	inferential modelling and decision making with data		In this paper, we introduce the main concepts of a new maximum livelihood evidential reasoning (MAKER) framework for data-driven inferential modelling and decision making under different types of uncertainty. It consists of two types of model: state space model (SSM) and evidence space model (ESM), driven by the data that reflects the relationships between system inputs and output. SSM is constructed to describe different system states and changes. ESM is established by mapping data to a set of evidence that is partitioned into evidential elements each pointing to a system state set and together represents system behaviours in a probabilistic and distributed manner. The reliability of evidence and interdependence between a pair of evidence are explicitly measured. It is in the joint evidence-state space that multiple pieces of evidence with different degrees of interdependence and reliability are acquired from system inputs and combined to inference system output. A general optimal learning model is constructed, where evidence reliability can be learnt from historical data by maximising the likelihood of true state. In the MAKER framework, different types of uncertainty can be taken into account for inferential modelling, probabilistic prediction and decision making.	inference engine;inferential programming;inferential theory of learning;interdependence;machine learning;state space;state-space representation	Jian-Bo Yang;Dong-Ling Xu	2017	2017 23rd International Conference on Automation and Computing (ICAC)	10.23919/IConAC.2017.8082048	data mapping;probability distribution;probabilistic logic;evidential reasoning approach;machine learning;data modeling;artificial intelligence;inference;state-space representation;computer science	Robotics	-17.628882021446067	-3.7104840300091606	94342
4a2a08356eaabdf3cd60f106e97b6fc7e4842d21	token-based incentive protocol design for online exchange systems	efficiency;token protocols;agents;repeated games	In many online exchange systems, agents provide services to satisfy others agents’ demands. Typically, the provider incurs a (immediate) cost and hence, it may withhold service. As a result, the success of the exchange system requires proper incentive mechanisms to encourage service provision. This paper studies the design of such systems that are operated based on the exchange of tokens, a simple internal currency which provides indirect reciprocity among agents. The emphasis is on how the protocol designer should choose a protocol a supply of tokens and suggested strategies to maximize service provision, taking into account that impatient agents will comply with the protocol if and only if it is in their interests to do so. Agents’ interactions are modeled as a repeated game. We prove that the these protocols have a simple threshold structure and the existences of equilibria. Then we use this structural property to design exchange strategies that maximize the system efficiency. Among all protocols with the same threshold, we find that there is a unique optimal supply of tokens that balances the token distribution in the population and achieves the optimal efficiency. Such token protocols are proven to be able to achieve full efficiency asymptotically as agents become sufficient patient or the cost becomes sufficient small.	existential quantification;interaction;mind;software agent	Jie Xu;William R. Zame;Mihaela van der Schaar	2012		10.1007/978-3-642-35582-0_19	real-time computing;distributed computing;business;computer security	ECom	-7.7046369552140845	-6.648637505970718	94401
8fee72e1b6d78fdd9026f829144582aa55c883e9	automed: an automated mediator for bilateral negotiations under time constraints	satisfiability;automatic mediation;mediation tools;negotiation;time constraint	Engaging in negotiations is a daily activity. Some negotiations require the involvement of a mediator in order to be concluded in a satisfying manner. In such cases, the objective is to help the negotiators reach a mutually beneficial agreement [6, 4]. Our research focuses on mediation tools for dealing with bilateral negotiations under time constraints.	bilateral filter	Michal Chalamish;Sarit Kraus	2007		10.1145/1329125.1329425	party-directed mediation;negotiation;satisfiability	HCI	-10.705931253461292	-6.755211729681194	94479
58a6325220e01223f38a9725a50107315ff5b56d	towards a logical framework for reasoning about risk		Evaluating the effectiveness of the security measures undertaken to protect a distributed system (e.g., protecting privacy of data in a network or in an information system) is a difficult task that, among other things, requires a risk assessment. We introduce a logical framework that allows one to reason about risk by means of operators that formalize causes, effects, preconditions, prevention and mitigation of events that may occur in the system. This is work in progress and we describe a number of interesting variants that could be considered.	distributed computing;information system;logical framework;precondition;risk assessment	Matteo Cristani;Erisa Karafili;Luca Viganò	2012		10.1007/978-3-642-32498-7_46	opportunistic reasoning;logical reasoning;analytic reasoning;logical consequence;qualitative reasoning;adaptive reasoning;model-based reasoning;reasoning system;deductive reasoning	Security	-17.53884031932298	-0.46963028115198974	94663
5719d25454e4ca888ff9539aa85e26faa21f24e9	the representation of derivable information in memory: when what might have been left unsaid is said	construction process;mental representation;incomplete data;knowledge structure;possibility theory	It is now widely accepted that natural language comprehension is a constructive process. Information in discourse interacts with a variety of impinging contextual factors (including, most prominently, the comprehender's pre-existing knowledge) in an active, creative process that results in understandings not derivable by any solely linguistic or logical analysis (c.f., Bransford & McCarrell, 1975; Spiro, 1977, in press). Acceptance of the constructive view of comprehension entails a concomitant delimitation of the range of possible theories of mental representation. Knowledge structures must possess some capability for detecting the pragmatic, as well as logical, implications of the incomplete data contained in discourse (c.f., Charniak, 1974; Mlnsky, 1975; Rumelhart & Ortony, 1977; Schank & Abelson, 1977). In other words, knowledge structures must contain considerable information about the way the world usually works. This characteristic of representation is useful and efficient because natural and social contexts do produce sufficient constraints on worldly events and ideas as to make them, to a limited extent, orderly and predictable.	mental representation;multiple-instance learning;natural language;problem solving;sensor;theory	Rand J. Spiro;Joseph Esposito;Richard J. Vondruska	1978		10.3115/980262.980299	discrete mathematics;mathematics;algorithm	AI	-14.345562110675361	1.9478790721657004	94675
44ee27fa0b41b1f2246862b5c29e7ed6c9c33af2	aggregation of binary evaluations: a borda-like approach	judgment aggregation;mean rule;d71 social choice;binary evaluations;clubs;d70 general;borda;associations;committees;tension	We characterize a rule for aggregating binary evaluations – equivalently, dichotomous weak orders – similar in spirit to the Borda rule from the preference aggregation literature. The binary evaluation framework was introduced as a general approach to aggregation by Wilson (J. Econ. Theory 10 (1975) 63-77). In this setting we characterize the “mean rule,” which we derive from properties similar to those Young (J. Econ. Theory 9 (1974) 43-52) used in his characterization of the Borda rule. Complementing our axiomatic approach is a ∗Financial support from the Spanish Ministry of Science and Innovation through MICINN/FEDER grant ECO2010-21624, the NUI Galway Millennium Fund and the Irish Research Council co-funding from the European Commission is gratefully acknowledged. Conal Duddy conducted this research while visiting the Centre for Philosophy of Natural and Social Science at the London School of Economics. He is grateful for their hospitality. Thanks to Felix Brandt, Franz Dietrich, Nicholas Houy, Marcus Pivato and the participants at PET 13 for their helpful comments. This joint project began after we attended a workshop on New Developments in Judgement Aggregation and Voting Theory, held in Freudenstadt, September 2011. The workshop was organised by Clemens Puppe and Klaus Nehring. We are extremely grateful to them for organizing this workshop, and for providing financial support. †J.E. Cairnes School of Business and Economics and the Whitaker Institute, National University of Ireland Galway, Ireland. Email: conal.duddy@nuigalway.ie ‡J.E. Cairnes School of Business and Economics and the Whitaker Institute, National University of Ireland Galway, University Road, Galway, Ireland. Email: ashley.piggins@nuigalway.ie §Department of Mathematics, Union College, Schenectady, NY 12308, USA. Email: zwickerw@union.edu	axiomatic system;clemens c. j. roothaan;email;fits;franz lisp;klaus samelson;like button;natural user interface;organizing (structure);partition problem;peano axioms;polyethylene terephthalate;rule 184;monotone	Conal Duddy;Ashley Piggins;William S. Zwicker	2016	Social Choice and Welfare	10.1007/s00355-015-0914-3	tension;mathematics;mathematical economics;law;statistics	ECom	-10.626291063405251	-2.2897938258316652	94693
351105195a84449913b6715974334f9926f7cddb	sub-coalitional approach to values		The behavioral models of classical values (like the Shapley and Banzhaf values) consider the contributions to coalition S as contributions delivered by the players individually joining such a coalition as it is being formed; i.e., v(S) – v(S  {i}). In this paper, we propose another approach to values where these contributions are considered as given by sets of players: (v(S) – v(S  R)), where S, R are subsets of the set of all players involved in cooperative game v. Based on this new approach, several sub-coalitional values are proposed, and some properties of these values are shown.		Izabella Stach	2017	Trans. Computational Collective Intelligence	10.1007/978-3-319-70647-4_6		HCI	-6.559395549059403	-2.567584171449623	95033
8a05711a9c9402293c912ba8efe0577f6e244a07	the danger of using axioms in software metrics	software metrics;logical deduction;complexity metrics;mathematics;set theory;group theory;software measure validity;axiom sets;software metric;group theory axiom sets software metrics mathematics logical deduction software measure validity complexity metrics set theory	The authors contrast axiom sets used in mathematics with those used in software metrics research. There are striking differences in the way the two types of axiom sets are derived and their intended purpose. In particular, axiom sets in software metrics are primarily used for purposes of definition, whereas in mathematics they are used for logical deduction. In addition, software metrics axiom sets have been developed without a consensus as to valid example measures and without a common understanding of the data to which they apply. The authors conclude that there is a non-negligible risk that inappropriate axiom sets will not be detected which may lead to an incorrect assessment of the validity of some software measures.	software metric	Barbara A. Kitchenham;John G. Stell	1997	Software Engineering - IEE Proceedings	10.1049/ip-sen:19971723	zermelo–fraenkel set theory;axiom of extensionality;software engineering;naive set theory;equivalence of metrics;group theory;software metric;constructive set theory	SE	-14.499616410477765	0.7802920844189853	95432
48c93bb52f2fcfcf9a16920696dc41505e41ca8a	a note on the uniqueness of models in social abstract argumentation		Social abstract argumentation is a principled way to assign values to conflicting (weighted) arguments. In this note we discuss the important property of the uniqueness of the model.		Leila Amgoud;Elise Bonzon;Marco Correia;Jorge Cruz;Jérôme Delobelle;Sébastien Konieczny;João Leite;Alexis Martin;Nicolas Maudet;Srdjan Vesic	2017	CoRR		pure mathematics;mathematics;mathematical economics	AI	-8.07350978367053	-0.4438611300436494	95651
47c9e105f75ed307e432a25c385906982b21d6fc	multiagent self-assembly simulation environment	self assembly;software systems;self assembly visualization system testing software systems autonomous agents multiagent systems animation real time systems intelligent systems intelligent agent;visualization;design technique;intelligent agents;autonomous agent;animation;intelligent systems;intelligent agent;system testing;genetic algorithm;resource conflicts;autonomous agents;simulation environment;multiagent systems;real time systems	We describe a software system1 to model and visualize 3D or 2D self-assembly of groups of autonomous agents. The system makes a physically accurate estimate of the interaction of agents represented as rigid cubic or tetrahedral structures with variable electrostatic charges on the faces and vertices. Local events cause the agentsý charges to change according to user-defined rules or rules generated by genetic algorithms. The system is used as an experimental environment for theoretical and practical study of autonomous agent self-assembly. In particular, the system is used to further develop and test self-assembly properties of meso-blocks. The software system will be applied to the analysis, prediction and design of self-assembly behavior of agents from atomic- to macro-scales. In particular, it will be a platform for developing design techniques that can be implemented in real nano-scale systems to achieve useful structures.	agent-based model;autonomous agent;autonomous robot;cubic function;gnu nano;genetic algorithm;mesoscopic physics;self-assembly;simulation;software system;vertex (graph theory)	Vadim Gerasimov;Ying Guo;Geoff James;Geoff Poulton;Philip Valencia	2004	Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.	10.1109/AAMAS.2004.189	real-time computing;simulation;computer science;artificial intelligence;autonomous agent;intelligent agent	Robotics	-17.36362320612313	-2.025275677999251	96009
19fcb7a41deb075d9a7d9881ec8130e122d222ff	lifted first-order probabilistic inference	first-order level;propositional representation level;propositional exact inference;lifted first-order probabilistic inference;exact inference algorithm;first-order specification;probabilistic inference algorithm;inference stage;graphical model;propositional level;first-order model	Most probabilistic inference algorithms are specified and processed on a propositional level. In the last decade, many proposals for algorithms accepting first-order specifications have been presented, but in the inference stage they still operate on a mostly propositional representation level. [Poole, 2003] presented a method to perform inference directly on the first-order level, but this method is limited to special cases. In this paper we present the first exact inference algorithm that operates directly on a first-order level, and that can be applied to any first-order model (specified in a language that generalizes undirected graphical models). Our experiments show superior performance in comparison with propositional exact inference.	algorithm;experiment;first-order predicate;graph (discrete mathematics);graphical model;mental representation;propositional calculus	Rodrigo de Salvo Braz;Eyal Amir;Dan Roth	2005			backward chaining;variable elimination;fiducial inference;frequentist inference;adaptive neuro fuzzy inference system;computer science;artificial intelligence;theoretical computer science;disjunction introduction;machine learning;first-order logic;mathematics;graphical model;probabilistic logic network;algorithm;statistics	AI	-8.701794470829318	3.0292317370216795	96047
d91ab8ac04c4b2fa10dd7e888faf42298f8dcfea	epistemic logics, probability, and the calculus of evidence	belief function;satisfiability;rational agent;modal logic;dempster shafer theory;epistemic logic;dempster shafer;knowledge integration;foundations of probability;possible worlds	I In t roduc t ion This paper presents results of the application to epistemic logic structures of the method proposed by Carnap for the development of logical foundations of probability theory. These results, which provide firm conceptual bases for the Dempster-Shafer calculus of evidence, are derived by exclusively using basic concepts from probability and modal logic theories, without resorting to any other theo­ retical notions or structures. A form of epistemic logic (equivalent in power to the modal system S5), is used to define a space of possible worlds or states of affairs. This space, called the epistemic universe, consists of all possible combined descriptions of the state of the real world and of the state of knowledge that certain rational agents have about it. These repre­ sentations generalize those derived by Carnap, which were confined exclusively to descriptions of possible states of the real world. Probabilities defined on certain classes of sets of this universe, representing different states of knowledge about the world, have the properties of the major functions of the Dempster-Shafer calculus of evidence: belief functions and mass assignments. The importance of these epistemic probabilities lies in their ability to represent the ef­ fect of uncertain evidence in the states of knowledge of rational agents. Furthermore, if an epistemic probability is extended to a probability function defined over subsets of the epistemic universe that represent true states of the real world, then any such extension must satisfy the wellknown interval bounds derived from the Dempster-Shafer theory. Application of this logic-based approach to problems of knowledge integration results in a general expression, called the additive combination formula, which can be applied to a wide variety of problems of integration of dependent and independent knowledge. Under assumptions of probabilis­ tic independence this formula is equivalent to Dempster's rule of combination. The research work described in this paper was sponsored by the U.S. Army Signal Warfare Center, under Contract DAAL02-85-C0082, and by the National Science Foundation, under Grant DCR85-13139. The research work presented here was motivated by the need to improve the understanding of issues in the analy­ sis and interpretation of evidence. In the context of this paper, the term evidence is used to describe the informa­ tion, usually imprecise and uncertain, that is conveyed by observations and measurements of real-world systems. We have sought to gain such an understanding by examining the basic concepts, structures, and ideas relevant to the characterization of imprecise and uncertain knowledge. Our approach is strongly based on Carnap's method­ ology (Carnap 1956, 1962) for the development of logical foundations of probability theory. In his formulation, Car­ nap developed an universe of possible worlds that encom­ passes all possible valid states of a real-world system. Infor­ mation about that system, if precise and certain, identifies its actual state (e.g., a detailed diagnosis of a disease). If imprecise but certain, this information identifies a subset of possible system states (e.g., a number of possible diag­ noses). If uncertain, then the information induces a pro­ bability distribution over system states (e.g., probability values for specific diagnoses). It is important to note, however, that in Carnap's char­ acterization no distinction is drawn between degrees of precision or detail when the information is uncertain. This representational shortcoming renders impossible the mod­ eling of information that only assigns degrees of likelihood values to some subsets of possible states (i.e., instead of prescribing those values over all such subsets that are of relevance to the modeler). This type of information, pro­ viding some knowledge about the underlying probability distributions but not all the distribution values, is quite common in practical applications (e.g., in a medical diag­ nosis problem, tests and existing medical knowledge indi­ cate that there is a 60% chance of liver disease but fail to provide any information about the likelihood of individual instances thereof). Seeking to generalize Carnap's approach to allow for the treatment of this type of uncertain information we di­ rected our attention to epistemic logics — a form of modal logics developed to deal wi th problems of representation and manipulation of the states of knowledge of rational	device independence;epistemic modal logic;knowledge integration;observations and measurements;possible world;rational agent;relevance;rendering (computer graphics);theory;utility functions on indivisible goods;world-system	Enrique H. Ruspini	1987		10.1007/978-3-540-44792-4_17	normal modal logic;discrete mathematics;epistemic modal logic;knowledge integration;dempster–shafer theory;artificial intelligence;machine learning;mathematics;epistemic possibility;multimodal logic;algorithm	AI	-15.854650647560481	0.5796231863454822	96088
748dc717e68cd5e8ba2573dbefb207858bfeaba7	a decision tree algorithm for distributed data mining: towards network intrusion detection	distributed data;decision tree;agent based;data collection;intrusion detection;method integration;network intrusion detection;distributed learning;induction generator;classification rules;distributed data mining	This paper presents preliminary works on an agent-based approach for distributed learning of decision trees. The distributed decision tree approach is applied to intrusion detection domain, the interest of which is recently increasing. In the approach, a network profile is built by applying a distributed data analysis method for the collection of data from distributed hosts. The method integrates inductive generalization and agent-based computing, so that classification rules are learned via tree induction from distributed data to be used as intrusion profiles. Agents, in a collaborative fashion, generate partial trees and communicate the temporary results among them in the form of indices to the data records. Experimental results are presented for military network domain data used for the network intrusion detection in KDD cup 1999. Several experimental results show that the performance of distributed version of decision tree is much better than that of non-distributed version with data collected manually from distributed hosts.		Sung Wook Baik;Jerzy W. Bala	2004		10.1007/978-3-540-24768-5_22	intrusion detection system;distributed algorithm;induction generator;distributed data store;decision tree learning;computer science;machine learning;decision tree;incremental decision tree;data mining;database;computer security;statistics;data collection	ML	-15.773122588616026	-9.26722925158127	96184
39e08f6d114d7267a0ac0c71ab5f1aef4399c9dd	delay-predictability trade-offs in reaching a secret goal		Copyright: © 2018 INFORMS Abstract. We formulate a model of sequential decision making, dubbed the Goal Prediction game, to study the extent to which an overseeing adversary can predict the final goal of an agent who tries to reach that goal quickly, through a sequence of intermediate actions. Our formulation is motivated by the increasing ubiquity of large-scale surveillance and data collection infrastructures, which can be used to predict an agent’s intentions and future actions, despite the agent’s desire for privacy. Our main result shows that with a carefully chosen agent strategy, the probability that the agent’s goal is correctly predicted by an adversary can be made inversely proportional to the time that the agent is willing to spend in reaching the goal, but cannot be made any smaller than that. Moreover, this characterization depends on the topology of the agent’s state space only through its diameter.	adversary (cryptography);institute for operations research and the management sciences;state space	John N. Tsitsiklis;Kuang Xu	2018	Operations Research	10.1287/opre.2017.1682	data collection;secrecy;mathematical optimization;simulation;predictability;state space;adversary;distributed computing;computer science	ML	-10.01598489919426	-4.382120659459023	96381
95d08e2d6de56750d7bb8488da995f7c0f587efa	the value of approaching bad things		Adaptive decision making often entails learning to approach things that lead to positive outcomes while avoiding things that are negative. The decision to avoid something removes the risk of a negative experience but also forgoes the opportunity to obtain information, specifically that a seemingly negative option is actually positive. This paper explores how people learn to approach or avoid objects with uncertain payoffs. We provide a computational-level analysis of optimal decision making in this problem which quantifies how the probability of encountering an object in the future should impact the decision to approach or avoid it. A large experiment conducted online shows that most people intuitively take into account both their uncertainty and the value of information when deciding to approach seemingly bad things.	computation;decision theory;digital audio workstation;experiment;interaction;machine learning;multi-armed bandit;word lists by frequency	Alexander Rich;Todd M. Gureckis	2014			social psychology;conversation;value of information;prior probability;optimal decision;beta (finance);decision problem;stochastic game;deci-;mathematics	AI	-9.098080299296202	-4.408960134227536	96383
5d5e5a329e2c2f21885870d5dbaca4061bae499c	a distributed planning approach for web services composition	optimal solution;distributed heuristic function;business to business applications;search space;color;distributed planning approach;planning artificial intelligence;semantics;data exchange;b2b applications;web services planning artificial intelligence software agents;web service;web services planning color argon semantics heuristic algorithms merging;argon;multi agent;distributed heuristic function distributed planning approach web services composition b2b applications business to business applications search space explosion decentralised planner web service agents;scaling up;web service composition;software agents;search space explosion;web services composition;heuristic algorithms;web services;merging;decentralised planning web services composition multi agent;distributed planning;decentralised planning;planning;web service agents;decentralised planner	The ability to automatically answer a request that requires the composition of a set of web services has received much interest in the last decade, as it supports B2B applications. Planning techniques are used widely in the literature to describe the web services composition problem but they don’t scale up well. This weakness is due to the search space explosion caused by the large ranges of data exchanged among services. In addition, it is more interesting to use a decentralised planner because the nature of the problem is distributed. In this paper, we consider a set of web service agents where each agent has a set of services organised in a graph. To respond to a request, agents propose their best local partial plans which are partial paths in the graph. They then coordinate their partial plans to provide the global plan for the submitted request using an algorithm based on a distributed heuristic function. This function ensures the optimality and the completeness of the algorithm. Indeed, it is based not only on the agent capabilities to respond to a request, but also taking into account the plans proposed by other agents. The complexity of the algorithm is polynomial. The experiments show the ability of our approach to find the optimal solutions for automated web services composition taking into account the dependencies betwen the agents.	algorithm;computation;experiment;graph (discrete mathematics);heuristic (computer science);multi-agent system;online and offline;polynomial;response time (technology);web service	Mohamad El Falou;Maroua Bouzid;Abdel-Illah Mouaddib;Thierry Vidal	2010	2010 IEEE International Conference on Web Services	10.1109/ICWS.2010.13	web service;simulation;computer science;ws-policy;data mining;database;semantics;law	Robotics	-13.057228542598002	-2.591986345932144	96393
546bf3168d80a41eb698de10ef2ece8950811534	experiments in multi agent learning	evolutionary computation;multi agent system;mas;ensemble techniques;machine learning;distributed data mining;multi agent learning;knowledge modeling;evolutionary computing	Data sources are often dispersed geographically in real life applications. Finding a knowledge model may require to join all the data sources and to run a machine learning algorithm on the joint set. We present an alternative based on a Multi Agent System (MAS): an agent mines one data source in order to extract a local theory (knowledge model) and then merges it with the previous MAS theory using a knowledge fusion technique. This way, we obtain a global theory that summarizes the distributed knowledge without spending resources and time in joining data sources. The results show that, as a result of knowledge fusion, the accuracy of initial theories is improved as well as the accuracy of the monolithic solution.	algorithm;computer file;experiment;knowledge representation and reasoning;machine learning;multi-agent system;principle of locality;real life	Maria Cruz Gaya;J. Ignacio Giráldez	2008		10.1007/978-3-540-87656-4_11	computer science;artificial intelligence;data science;machine learning;data mining;evolutionary computation	AI	-15.899108237593667	-9.387505462719561	96523
fbf01b5597a9dc5cff06a0be41d90d52eabd4315	agent-based outpatient scheduling for diagnostic services	patient diagnosis;dynamic change;iterative bidding dynamic outpatient scheduling diagnostic service scheduling multiagent systems;dynamic outpatient scheduling;multiagent system;agent based;software agents medical diagnostic computing patient diagnosis scheduling;system performance;iterative bidding;software agents;distributed scheduling;diagnostic service scheduling;large scale;computational modeling atmospheric modeling automation;computational modeling;scheduling;atmospheric modeling;standby waiting patient agent based outpatient scheduling diagnostic service hospital setting diagnostic timeslot agent based dynamic scheduling system iterative bidding procedure;medical diagnostic computing;dynamic scheduling;multiagent systems;automation	This paper presents an agent-based distributed scheduling approach for diagnostic services in a hospital setting. In particular, we focus on how to effectively allocate the last-minute available diagnostic timeslots and how to deal with dynamic changes in existing outpatient schedules. We propose an agent-based dynamic scheduling system which allocates available diagnostic services timeslots to outpatients through an iterative bidding procedure. We also evaluate and analyze the possibility of implementing the proposed system in large scale outpatient scheduling environments. Our experiments show that the system performs well at a scale of managing a thousand standby waiting patients.	agent-based model;experiment;iteration;scheduling (computing)	Patrice Godin;Chun Wang	2010	2010 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2010.5642281	fair-share scheduling;atmospheric model;real-time computing;simulation;dynamic priority scheduling;computer science;artificial intelligence;software agent;automation;multi-agent system;computer performance;computational model;scheduling	Robotics	-13.147089107689228	-9.08698750592948	96586
5306e5e9c920c71115dccb68426933787a8d79a3	solving the crowdsourcing dilemma using the zero-determinant strategy: poster	game theory;crowdsourcing;zero determinant strategy	As a promising technology, crowdsourcing aims to accomplish a complex task via eliciting services from a large group of workers. However, recent observations indicate that the success of crowdsourcing is being hindered by the malicious behaviors of the workers. In this paper, we analyze the attack problem using an iterated prisoner's dilemma (IPD) game and propose an zero-determinant (ZD) strategy based algorithm. Simulation results demonstrate that the requestor can incentivize the worker to keep on cooperating.	algorithm;crowdsourcing;interpupillary distance;iteration;malware;prisoner's dilemma;simulation	Qin Hu;Shengling Wang;Liran Ma;Xiuzhen Cheng;Rongfang Bie	2016		10.1145/2942358.2942402	game theory;simulation;computer science;knowledge management;crowdsourcing	AI	-10.093598138152048	-8.697054680817644	96662
6d68e7b37f428d20b4937530a0f7356f70fce4ec	egalitarianism and utilitarianism in committees of representatives	voting rule	We consider a committee of representatives that makes dichotomous choices (acceptance/rejection) by vote. Given the size of each group represented, what is the most adequate voting rule for the committee? We provide answers based on each of the two principles commonly used to make normative assessments in different contexts: egalitarianism and utilitarianism. To that end, we introduce utilities into the model and adopt a normative approach.	rejection sampling	Annick Laruelle;Federico Valenciano	2010	Social Choice and Welfare	10.1007/s00355-009-0438-9	economics;social psychology;law;welfare economics	AI	-7.790371926772977	-3.6463831037992858	96797
3842f90dac3ff15bd06e31322f1e9ac57f93068b	behavior and deliberation in perfect-information games: nash equilibrium and backward induction	perfect-information game;behavioral model;nash equilibrium outcome;backward-induction outcome	Doxastic characterizations of the set of Nash equilibrium outcomes and of the set of backward-induction outcomes are provided for general perfectinformation games (where there may be multiple backward-induction solutions). We use models that are behavioral, rather than strategy-based, where a state only specifies the actual play of the game and not the hypothetical choices of the players at nodes that are not reached by the actual play. The analysis is completely free of counterfactuals and no belief revision theory is required, since only the beliefs at reached histories are specified.	backward induction;belief revision;counterfactual conditional;doxastic logic;nash equilibrium	Giacomo Bonanno	2018	Int. J. Game Theory	10.1007/s00182-017-0595-5	mathematics;welfare economics;mathematical economics;best response;epsilon-equilibrium;subgame perfect equilibrium;equilibrium selection;nash equilibrium;repeated game;sequential equilibrium;trembling hand perfect equilibrium	ECom	-9.093280615378909	-1.8963681195774964	96997
f96ca82736219076893903b647b58847c8c62985	a cooperative game of information trading: the core, the nucleolus and the kernel	game theory;technological innovation;inovacion;technology;juego cooperativo;teoria juego;economic model;theorie jeu;cooperative game;informacion;modelo economico;modele economique;innovation;informed trading;jeu cooperatif;technologie;information;tecnologia	A certain trade of the information about a technological innovation between the initial owner of the information and n identical producers is studied by means of a cooperative game theoretic approach. The information trading situation is modelled as a cooperative (n + 1)-person game with side payments. The symmetrical strong e-cores (including the core), the nucleolus and the kernel of the cooperative game model are determined. Interpretations of these game theoretic solutions and their implications for the information trading problem are given. 1 An Economic Model of Information Trading We consider an industry consisting of a fixed number of identical producers of which all produce the same homogeneous output with the same technology. The profit level of each producer is identical and expressed in terms of monetary units. In addition to the producers there is another agent who possesses the information about a technological innovation. The new technology may increase the monetary profit level of the producers who acquire and utilize it, but may decrease the profit level of the producers who don't purchase it and continue to use the old technology. Due to an external diseconomy, the profit level of every producer decreases as the number of producers who purchase the new technology grows. The initial owner of the information about the technological innovation has no means of production. As such, we regard the initial owner of the information as the seller of the technological innovation and the producers as the potential buyers of the new technology. To be exact, the initial owner attempts to sell the information about the new technology to all or some of the producers. We aim to describe the profit that can be realized by the initial owner of the information if a trade of the information has been carried out. Concerning the pro1 Theo Driessen, Department of Applied Mathematics, University of Twente, Enschede, The Netherlands. 2 Shigeo Muto, Faculty of Economics, Tohoku University, Kawauchi, Sendai, Japan. 3 Mikio Nakayama, Department of Economics, Hosei University, Tokyo, Japan. 03409422/92/1/55 72 $2.50 9 1992 Physica-Verlag, Heidelberg 56 T. Driessen et al. ducers, the relevant comparison is between the profit level with the use of the new technology and the profit level obtained before the technological innovation. Our analysis of the above information trading situation will be based on a cooperative game model in which the players consist of the seller and the potential buyers of the new technology. In other words, we define a so-called cooperative game in characteristic function form in such a way that the corresponding characteristic function reflects the cooperative behaviour between the seller and the potential buyers. In the context of the relevant game theoretic approach, it is permitted that the players communicate with one another in order to make binding agreements regarding the coalition formation. For instance, the producers can make a binding agreement not to purchase the new technology if they want to do so. Further, a trade of the information can only be carried out on the understanding that resales of the new technology are completely prohibited. The exact cooperative game model of the information trading situation will be treated in Section 2. Following the cooperative setting of Section 2, the profit shares to the seller and the producers can be prescribed by basic solution concepts. The general notions of three main solution concepts the strong e-cores (including the core), the nucleolus and the kernel are discussed in Section 3. Our attention is mainly directed to the core and the nucleolus solution concepts. In the Sections 4, 5 and 6 respectively, we determine the strong e-cores, the nucleolus (as well as the core) and the kernel of the cooperative game model of the information trading situation. Section 7 deals with the monopolistic structure of the information trading situation and the implications for the solution concepts of the corresponding cooperative game model. Concluding remarks are given in Section 8. The most important results can be summarized as follows. The symmetrical strong e-core elements can be described in terms of two realvalued functions f and g of the real number e (cf. Lemma 4.2 and Theorem 4.4). The sign of the critical number e* satisfying f ( e* )= g(e*) determines whether the core is empty or not (cf. Theorem 5.2). According to the nucleolus concept, the profit share to any producer equals either the amount f(e*) or the bound E* ( n 1) induced by the individual rationality principle (cf. Theorem 5.1). As a consequence, the core is nonempty if and only if the potential nucleolus profit share f(e*) to any producer exceeds his monetary profit level obtained before the technological innovation. Further, the nonemptiness of the core implies that the new technology is actually acquired by at least one producer or even all producers (cf. Theorem 5.3). Finally, the kernel coincides with the nucleolus (cf. Theorem 6.2) and the monopolistic structure of the information trading situation yields an empty core (cf. Theorem 7.2). A Cooperative Game of Information Trading: The Core, the Nucleolus and the Kernel 57 2 A Cooperative Game Model of Information Trading First of all, we specify four assumptions on the underlying economic model of the information trading situation as presented in Section 1. Recall that the initial owner seeks to sell the information about the technological innovation to all or some of the n producers (n_> 2). (A.1) Given any nontrivial number of actual buyers of the new technology, an actual buyer attains at least as many profits as a nonbuyer. (A.2 3) The external diseconomy applies to both the actual buyers and the nonbuyers. That is, the profit level of any type of a producer decreases as the number of actual buyers grows. (A.4) The profit level of a unique buyer is more than the positive profit level obtained before the technological innovation. If t producers have purchased the new technology, then E(t) and E* (t) respectively denote the monetary profit level of any producer who has purchased the new technology and not. The collection {E(t),E* (t)l t ~ {0,1, . . . , n}} is called a profit structure of the information trading situation if it satisfies the above four assumptions, i.e., (A.1) E(t)>_E*(t) for all t~{1,2 . . . . . n l } , (A.2) E(t)>_E(t+l) for all t~{1,2 . . . . . n l } , (A.3) E*(t)>_E*(t+l) for all t~{O, 1 . . . . . n l } , (A.4) E ( 1 ) > E * ( 0 ) > 0 , E ( 0 ) = E * ( n ) : = 0 . For the sake of mathematical convenience, we put E(0) = E* ( n ) : = 0. Next we model the information trading situation as a cooperative (n + 1)-person game where its player set N O consists of the seller 0 and the potential buyers 1,2 . . . . . n of the new technology. That is, N O = [0} u N where N: = {1,2 . . . . . n} represents the set of all n producers. Any nonempty subset S of the player set N O (notation: S C NO) is called a coalition. A coalition which includes the seller is always denoted by S O in such a way that S O = {0} u S with S C N. The number of potential buyers in a coalition S O and S is denoted by s. The cornerstone of the cooperative game model is the so-called characteristic function v: 2N~ which assigns to every coalition its maximal joint profit. To be exact, the worth v(S ~ of any coalition S O represents the largest possible monetary profit what the producers in S O can achieve by the cooperative behaviour between themselves and the initial owner of the information about the technological innovation. Notice that the coalition S O must determine the profit independently of the members of the complementary coalition N O S O because 58 T. Driessen et al. resales of the new technology are completely prohibited. In point of fact, the new technology is acquired and utilized by a suitable number of producers in S O so as to maximize the joint profit of production. Moreover, if the initial owner of the information is not a member of a coalition S, then the producers in S can not acquire the new technology by cooperation within S. Consequently, the worth v(S) of any coalition S represents the joint profit of production under the worst conceivable circumstance that all producers outside S do purchase the new technology. Definition 2.1: The cooperative game (NO; v) of the information trading situation is given by v(S ~ : = max (tE(t) + ( s t)E* ( t ) l t e {0, 1 . . . . . s}) for all S O C N O , v ( S ) : = s E * ( n s ) for all S C N . Obviously, the above information trading game (NO; v) satisfies v (N ~ _> n E* (0) = v (N) , v ({0}) = 0 , v({i})=E*(n-1) , v({O,i})=E(1) for all i e N . 3 Game Theoretic Notions The profit shares to the seller and the n producers will be described by means of a specific (n + D-dimensional payoff vector x = (Xo, Xl . . . . . xn) e [R n+l where xi represents the payoff (the profit share) to player i. Following the cooperative game model of Section 2 and the solution part of cooperative game theory, it is customary to require that any payoff vector meets the efficiency and individual rationality principles. In other words, any payoff vector for the information trading game (NO; v) should belong to the imputation set I(v) which is defined to be I (v) : = [(Xo,Xl . . . . . xn) e Nn+l xj= v(N ~ and j ~ N ~ xi>-v({i}) for all i e N ~ 1 A Cooperative Game of Information Trading: The Core, the Nucleolus and the Kernel 59	characteristic function (convex analysis);dna binding site;emoticon;game theory;geo-imputation;maximal set;rationality;universal quantification	Theo S. H. Driessen;Shigeo Muto;Mikio Nakayama	1992	ZOR - Meth. & Mod. of OR	10.1007/BF01541032	bondareva–shapley theorem;non-cooperative game;innovation;game theory;information;simultaneous game;economic model;information set;repeated game;screening game;normal-form game;simulations and games in economics education;mathematical economics;sequential game;technology	Theory	-5.688317071145228	-3.225035126573134	97380
e3a71cc753058f9e672400fed3cf99ec6ad0b5c2	unilateral commitments in finitely repeated games		In this paper, we consider finitely repeated games (without discounting) in which players can make unilateral commitments (UC) regarding their sets of strategies. More precisely, we suppose that each player can restrict his original set of strategies in a preliminary round of the repeated game. In this round all of the players choose (simultaneously) a subset of their strategy sets. These choices (to whom we refer as UC) are publicly announced before the repeated game starts, and in the repeated game every player is committed to use strategies only from the subset chosen in the preliminary round. We are interested in the Nash equilibria (NE) of the overall game, for which we prove some kind of “folk theorem”. These kinds of UC have already been considered in Fáıña-Med́ın et al. (1998) who proved that, if such a preliminary round is included in a finitely repeated prisoners’ dilemma, then there is a symmetric subgame perfect equilibrium in which both players act cooperatively throughout the post-commitment stages of the game (if the number of repetitions is large enough). Here, we take a much more general framework because: (a) we consider finite repetitions of general n-person strategic games; (b) we analyse both the case in which only the preferences of the players are available and the case in which these preferences are represented by utility functions; and (c) we also study the effect of a preliminary round of commitments on one-shot games. It is clear that UC, as described above, try to catch an idea which is well-known (as testified, e.g. by sentences like “burning one’s boats out”), i.e. in nonzero-sum	emoticon;nash equilibrium;uc browser	Ignacio García-Jurado;Luciano Méndez-Naya;Fioravante Patrone	2000	IGTR	10.1142/S0219198900000147	operations management;mathematical economics	ECom	-6.710927320623331	-3.636882139886149	97455
d3957c420eb6659c23a7e6e9900602152211ffa2	a pheromone-based negotiation mechanism for lot-sizing in supply chains	multilevel uncapacitated lot sizing problem;benchmark problem;borda maximin voting rule;planning artificial intelligence;contracts;joints;decision maker;pheromone based negotiation mechanism;companies;supply chains;asymmetric information;lot sizing supply chains contracts proposals voting electronic mail production planning search methods simulated annealing genetic algorithms;multi agent systems;voting rule;centralized planning;lot sizing;planning;supply chain;proposals;centralized planning pheromone based negotiation mechanism supply chains multilevel uncapacitated lot sizing problem borda maximin voting rule;supply chain management;supply chain management lot sizing multi agent systems planning artificial intelligence;ant system	A new generic negotiation mechanism to coordinate decentralized planning of a group of independent and self-interested decision makers or agents, who are searching for an agreeable contract regarding multiple interdependent issues, in the case of asymmetric information is presented. The basic idea of the mechanism is that the group members carry out a joint search based on a mediated pheromone matrix. By means of the Borda maximin voting rule, the agents determine contract proposals in each round of the negotiation process which are then used for the adaptation of pheromones or the generation of new contracts, respectively. The new negotiation mechanism is applied to the problem of decentralized lot-sizing in a supply chain. The resulting solution approach for the multi-level uncapacitated lot-sizing problem (MLULSP) is evaluated on the basis of 40 benchmark problems taken from the relevant literature. The derived results come close to those results which are obtained by centralized planning. Moreover, the new mechanism is competitive with other negotiation approaches which have been presented in the literature so far.	benchmark (computing);centralized computing;intelligent agent;interdependence;minimax	Jörg Homberger;Hermann Gehring	2010	2010 43rd Hawaii International Conference on System Sciences	10.1109/HICSS.2010.26	supply chain management;economics;marketing;operations management;microeconomics;supply chain;commerce	AI	-9.225965272515413	-7.384045136288779	97635
dcf111889d37f53c3913d45d7f219dd4b8d6cab1	feedback and performance in crowd work: a real effort experiment		Online labor markets gain momentum: Frequently, requesters post micro-tasks and workers choose which tasks to complete for a payment. In virtual, short-lived, and commonly one-shot labor relations, one challenge is to properly incentivize worker effort and quality of work. We present a real effort experiment on a crowd work platform studying the effect of feedback on worker performance. Rank order tournaments might or might not disclose a worker’s current competitive position. One might expect that feedback on the competitive position spurs competition and, in effect, effort and performance. On the contrary, we find evidence that in rank order tournaments, performance feedback tends to have a negative impact on workers’ performance. This effect is mediated by task completion. Furthermore when playing against strong competitors, feedback makes workers more likely to quit the task altogether and, thus, show lower performance. When the competitors are weak, workers tend to complete the task but with reduced effort. Thus, providing performance feedback might not be advisable in crowd labor markets.	computer performance;crowdsourcing;feedback	Tim Straub;Henner Gimpel;Florian Teschner;Christof Weinhardt	2014			real-time computing;simulation;multimedia	AI	-10.285208552912332	-8.246784375793945	97841
09a6dfaab5d302cf56ce2fd5ee5868d147e90d45	correlated equilibria in two-player repeated games with nonobservable actions	game theory;equilibrio juego;2 person repeated games;teoria juego;theorie jeu;jeu 2 personnes;repeated game;juego 2 personas;jeu repete;two person game;equilibrium payoff set;equilibrium payoff sets;correlated equilibrium;equilibre jeu;game equilibrium	JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. INFORMS is collaborating with JSTOR to digitize, preserve and extend access to Mathematics of Operations Research. Four kinds of correlated equilibrium payoff sets in undiscounted repeated games with nonobservable actions are studied. Three of them, the upper, the uniform, and Banach lead to the same payoff set, whereas the lower one in general is associated with a larger set. The extensive form correlated equilibrium is also explored. It turns out that both the regular and extensive form correlated equilibria yield the same sets of payoffs. 1. Introduction. In repeated games with nonobservable actions a player gets, after each stage, a signal that depends on the joint action played. This signal does not reveal necessarily the opponents' actions nor does it reveal their payoffs. The question naturally arises: What are the possible equilibrium outcomes and how do players use the information they collected during the game? We confine ourselves to undiscounted repeated games, where the payoffs are determined by the limit of partial average of the stage payoffs. This model enables one to examine the long-run impact of imperfect monitoring. The paper characterizes several types of long-term correlated equilibrium payoffs in two-player repeated games with nonobservable actions. Correlated equilibrium (introduced by Aumann in [Al]) allows the players to utilize an exogenous mediator who provides each one with private information. The players, based on this private information, adopt a pure strategy to be played in the repeated game. Such coordination between players may, in general, sustain equilibrium payoffs that were not supportable by an equilibrium without it (namely, by regular Nash equilibrium). The correlated equilibrium can be thought of also as a Nash equilibrium of an extended game in which a mediator sends messages to the players and then they choose strategies. Correlated equilibrium is a more attractive solution concept than Nash equilibrium for several reasons: (1) it better reflects real-life phenomena in which players may condition their behavior on their private information; (2) it allows for coordination excluded by Nash equilibrium; and (3) it is simpler to compute (see [GZ] and [HS]). In repeated games with imperfect monitoring the introduction of a …	archive;institute for operations research and the management sciences;mathematics of operations research;nash equilibrium;netbsd gzip / freebsd gzip;personally identifiable information;real life	Ehud Lehrer	1992	Math. Oper. Res.	10.1287/moor.17.1.175	game theory;traveler's dilemma;simulation;repeated game;mathematics;stochastic game;correlated equilibrium;mathematical economics	ECom	-5.1351800585995395	-4.753965429072668	97920
c6f8474e16806ddac67f84dff929b4c83700b389	coalition values derived from methods for comparison of coalition influence for games in characteristic function form	game theory	Abstract   The authors introduce new values which express coalition influence in the situation of coalition formation. Some examples which show how introduced values work are given. Propositions in this paper provide some properties that proposed values satisfy in the framework of games in characteristic function form.	characteristic function (convex analysis);game theory	Kentaro Kojima;Takehiro Inohara	2012	Applied Mathematics and Computation	10.1016/j.amc.2012.07.040	simulation;core	AI	-6.676928245720703	-1.8734574360806844	98114
dcbfea9fcd007f55789a45ef51ed90b1cda1d0c6	invariance properties of persistent equilibria and related solution concepts	maastricht university;solutions;satisfiability;digital archive;invariance;noncooperative games;open access;ordinality;solution concept;publication;scientific;ha statistics;institutional repository	Kohlberg and Mertens argued that a solution concept to a game should be invariant under the addition or deletion of an equivalent strategy and not require the use of weakly dominated strategies. In this paper we study which of these requirements are satis ̄ed by Kalai and Samet's concepts of persistent equilibria and persistent retracts. While none of these concepts has all the invariance properties, we show that a slight rephrasing of the notion of a persistent retract leads to a notion satisfying them all. JEL classi ̄cation: C72	persistent world;requirement	Dieter Balkenborg;Mathijs Jansen;Dries Vermeulen	2001	Mathematical Social Sciences	10.1016/S0165-4896(00)00056-1	social science;economics;invariant;publication;mathematics;linguistics;mathematical economics;solution concept;satisfiability	Theory	-7.486634978502083	-0.509982713522655	98128
7ba85052e31c65fce79216d9af165d74c1d08cd4	the data market: policies for decentralised visual localisation		This paper presents a mercantile framework for the decentralised sharing of navigation expertise amongst a fleet of robots which perform regular missions into a common but variable environment. We build on our earlier work [1] and allow individual agents to intermittently initiate trades based on a real-time assessment of the nature of their missions or demand for localisation capability, and to choose trading partners with discrimination based on an internally evolving set of beliefs in the expected value of trading with each other member of the team. To this end, we suggest some obligatory properties that a formalisation of the distributed versioning of experience maps should exhibit, to ensure the eventual convergence in the state of each agent’s map under a sequence of pairwise exchanges, as well as the uninterrupted integrity of the representation under versioning operations. To mitigate limitations in hardware and network resources, the “data market” is catalogued by distinct sections of the world, which the agents treat as “products” for appraisal and purchase. To this end, we demonstrate and evaluate our system using the publicly available Oxford RobotCar Dataset [2], the hand-labelled data market catalogue (approaching 446 km of fully indexed sections-of-interest) for which we plan to release alongside the existing raw stereo imagery. We show that, by refining market policies over time, agents achieve improved localisation in a directed and accelerated manner.		Matthew Gadd;Paul Newman	2018	CoRR		engineering;simulation;management science;valuation (finance);resource (disambiguation);software versioning;pairwise comparison;convergence (routing)	ML	-7.257349473816742	-6.40697968987153	98367
929217953b8aee5c86974ef5462048f3334266d1	membership separability: a new axiomatization of the shapley value		The paper shows that Shapley’s axiomatic characterization of his value can be strengthened considerably. Indeed, his additivity axiom can be replaced by a simple accounting property whereby a player’s payoff is the difference of a reward based on the worth of coalitions to which she belongs, and a tax based on the worth of coalition to which she does not belong, without placing any restriction whatsoever on the functional relationship between the reward or the tax and the worths that determine them.	axiomatic system;linear separability;stable marriage problem	Geoffroy de Clippel	2018	Games and Economic Behavior	10.1016/j.geb.2017.09.004	economics;welfare economics;mathematical economics;axiom;shapley value;microeconomics;stochastic game	ECom	-6.212666873466004	-1.9701075036055034	98529
55725a04a5621aad085db435be10996c67354867	ideological parsimony	theoretical virtues;ontology;ideology;parsimony;simplicity	The theoretical virtue of parsimony values the minimizing of theoretical commitments, but theoretical commitments come in two kinds: ontological and ideological. While the ontological commitments of a theory are the entities it posits, a theory’s ideological commitments are the primitive concepts it employs. Here, I show how we can extend the distinction between quantitative and qualitative parsimony, commonly drawn regarding ontological commitments, to the domain of ideological commitments. I then argue that qualitative ideological parsimony is a theoretical virtue. My defense proceeds by demonstrating the merits of qualitative ideological parsimony and by showing how the qualitative conception of ideological parsimony undermines two notable arguments from ideological parsimony: David Lewis’ defense of modal realism and Ted Sider’s defense of mereological nihilism.	entity;epistemic modal logic;maid-droid;maximum parsimony (phylogenetics);mereological nihilism;mereology;occam's razor;ted	Sam Cowling	2012	Synthese	10.1007/s11229-012-0231-7		AI	-13.382473380910078	3.576121354749586	98695
d503197f897c7dbca8b2972b8c61db4ac02f58aa	mechanism design theory: how to implement social goals	economic theory;mechanism design	The theory of mechanism design can be thought of as the engineering side of economic theory. One begins by identifying a social or economic goal. The theory then addresses the question of whether or not an appropriate institution or procedure (that is, a mechanism) could be designed to attain that goal.	intellect;leo (computer);lively kernel	Eric Maskin	2008		10.1007/978-3-540-92185-1_1	mechanism design;economics;computer science;process theory;management science;mathematical economics;social psychology;welfare economics	HCI	-10.099342691768031	-1.4050694880597587	98812
23baba614de6f6ff40df89e18cb83dd99319318b	on the power of imperfect information	infinite games;blow up;004;imperfect information;infinite games imperfect information;polynomial time	We present a polynomial-time reduction from parity games with imperfect information to safety games with imperfect information. Similar reductions for games with perfect information typically increase the game size exponentially. Our construction avoids such a blow-up by using imperfect information to realise succinct counters which cover a range exponentially larger than their size. In particular, the reduction shows that the problem of solving imperfect-information games with safety conditions is EXPTIME-complete.	exptime;polynomial-time reduction;time complexity	Dietmar Berwanger;Laurent Doyen	2008		10.4230/LIPIcs.FSTTCS.2008.1742	combinatorial game theory;time complexity;combinatorics;discrete mathematics;computer science;perfect information;mathematics;mathematical economics;algorithm	Theory	-5.073765543025235	2.902848696778046	99095
0ed500cae4d707dc2ac39453355bebe924aa8cc7	a method of meta-context ontology modeling and uncertainty reasoning in swot	metacontext;dbn reasoning;semantic web of things;context modeling	To model contexts and provide inference mechanism in Semantic Web of Things (SWoT), a generic and extensible meta-context ontology model (MCOnt) is proposed to model the common semantics to all dimensions of an information space. It can provide not only high-level meta-contexts which are used to capture basic context concepts, but also extensible domain-specific contexts in a hierarchical manner. Meanwhile, to adapt to dynamic and uncertain contexts in SWoT, an combined inference algorithm with context and Dynamic Bayesian Networks (DBNConU) is proposed.	algorithm;dynamic bayesian network;high- and low-level;ontology (information science);semantic web;vii;web of things	Zhong-Jun Lu;Guan-Yu Li;Ying Pan	2016	2016 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC)	10.1109/CyberC.2016.34	computer science;knowledge management;data mining;database;context model	DB	-18.869774255688107	-2.9658214109421017	99171
c98bf666af5955e9a99b18a5c1583164251a3d3e	word-of-mouth versus experts and reputation in the individual dynamics of wine purchasing	threshold dynamics;wine purchasing;social interaction;market share;experts impact;opinion dynamics;word of mouth;expert judgment;modeling	The dynamics of wine purchasing behavior is studied focusing on the respective impacts of the word-to-mouth versus wine expert judgements and reputations. To investigate the problem we apply the Galam model of opinion dynamics to agents who have to select a preference about which type of wine to buy given expert judgements, individual preferences and wine reputations. It could be, for instance, a preference between Bordeaux and Burgundy. The main novelty of the work is not about the building of a new model but indeed the construction of a scheme to confront the Galam model to a specific problem of the real world. Accordingly we design a commercial strategy to hold on to a share of the wine market. It provides a novel understanding on how, given some established reputation, the competitive interplay between social interactions and expert judgments affects the market shares distribution. The financial implications of the practical implementation of these results are discussed. In particular it is found that sample distribution of bottles could be drastically reduced from the usual levels practiced by producers. We hope our results will convince some wine producers to test our predictions.	cobham's thesis;cognition;existential quantification;intelligent agent;interaction;numerical analysis;purchasing;reputation;social network	Tatiana Bouzdine-Chameeva;Serge Galam	2011	Advances in Complex Systems	10.1142/S0219525911003475	public relations;market share;word of mouth;social relation;systems modeling;marketing;advertising	ECom	-7.3527761105578175	-9.103305653459183	99226
e3f422492566e74a0567423ec61b39d298d9be14	utilitarianism, prioritarianism, and intergenerational equity: a cake eating model		We use a simple consumption model, the so-called cake eating model, to study the interaction of equity, time and risk in social decision making. Total consumption, the ‘‘cake’’, is uncertain. The social planner allocates consumption between two agents (representing two generations), by assigning the first a determinate amount, with the second receiving the risky remainder. We study this consumption allocation decision using three socialwelfare functions: utilitarianism, ex ante prioritarianism, and ex post prioritarianism. Under standard assumptions, ex ante prioritarianism allocates more consumption to the first generation than utilitarianism. Thus, a concern for equity, in the ex ante prioritarian sense, means less concern for the risky future. By contrast, ex post prioritarianism normally chooses less consumption for the first generation than utilitarianism. We discuss the robustness of these optimal consumption allocations to learning and to more complicated social welfare functions. © 2017 Elsevier B.V. All rights reserved.	12-hour clock;eating your own dog food;property (philosophy)	Matthew D. Adler;Nicolas Treich	2017	Mathematical Social Sciences	10.1016/j.mathsocsci.2017.03.005	economics;microeconomics;welfare economics	AI	-6.408099356657795	-3.528818101913785	99306
c50e86bc04bd450c032cd95f2d092cfefdd40c84	fundamentals of simple games from a viewpoint of blockability relations	analisis numerico;game theory;matematicas aplicadas;mathematiques appliquees;teoria juego;prise de decision;theorie jeu;blockability relation;analyse numerique;numerical analysis;coalition;simple games;group decision making;applied mathematics;simple game;toma decision	This paper shows some elementary facts on simple games with respect to blockability relations. It is verified in this paper that fundamental concepts on simple games as null players, dictators, veto players, and so on can be expressed in terms of blockability relations. More, some new concepts as ''conflict-free'' and so on, are introduced from the viewpoint of blockability relations into the framework of simple games.		Keitarou Ishikawa	2009	Applied Mathematics and Computation	10.1016/j.amc.2008.12.069	game theory;group decision-making;applied mathematics;numerical analysis;artificial intelligence;mathematics;algorithm	Theory	-6.61090701733453	-0.4795561645831623	99580
74d5b2c382439a2051cbaf4ce0b11d5a2be1e9f5	decision-theoretic foundations of qualitative possibility theory	uncertainty;expected utility;decision maker;uncertainty aversion;decision under uncertainty;decision theory;decision theoretic;possibility theory	This paper presents a justi®cation of two qualitative counterparts of the expected utility criterion for decision under uncertainty, which only require bounded, linearly ordered, valuation sets for expressing uncertainty and preferences. This is carried out in the style of Savage, starting with a set of acts equipped with a complete preordering relation. Conditions on acts are given that imply a possibilistic representation of the decision-maker uncertainty. In this framework, pessimistic (i.e., uncertainty-averse) as well as optimistic attitudes can be explicitly captured. The approach thus proposes an operationally testable description of possibility theory. Ó 2001 Elsevier Science B.V. All rights reserved.	expected utility hypothesis;possibility theory;uncertainty principle;value (ethics)	Didier Dubois;Henri Prade;Régis Sabbadin	2001	European Journal of Operational Research	10.1016/S0377-2217(99)00473-7	uncertainty theory;possibility theory;decision-making;uncertainty analysis;optimal decision;uncertainty;economics;decision theory;expected utility hypothesis;decision analysis;decision rule;mathematics;mathematical economics;evidential decision theory;expected value of perfect information;welfare economics;expected value of including uncertainty;statistics	AI	-9.141450664540269	-0.7211524147604075	99786
1ac3a14a312f9b7b7401df2954b84078116fb027	incentive compatibility for the stable matching model with an entrance criterion	incentive compatibility;stable matching;satisfiability;dormitory assignment	A case study of matching students with dormitory-groups at the Technion lead recently to the study of a variant of the stable matching model with a “qualifying criterion” for the inclusion of a student among those getting an assignment. A notion of stability was introduced for the model and a (student-courting) algorithm which finds a matching that satisfied this criterion and has desired properties was described. Here, we show that students cannot benefit from misrepresenting preferences in an extension of the model that allows dormitory-groups to have different preferences over students and allows these preferences to be incomplete.		Nitsan Perach;Uriel G. Rothblum	2010	Int. J. Game Theory	10.1007/s00182-009-0210-5	mathematical optimization;stable marriage problem;economics;incentive compatibility;mathematics;microeconomics;welfare economics;satisfiability	AI	-6.499732806845655	-1.5679041370474085	99803
9b10231069cf1468b87f212c99ebb6c5187e622e	welfare for economy under awareness	economie;economia;optimum pareto;belief;teorema existencia;incertidumbre;uncertainty;existence theorem;exchange economy;croyance;preferencia;preference;economy;incertitude;creencia;welfare theorems;pareto optimum;theoreme existence;optimo pareto;pareto optimality	We present the extended notion of pure exchange econom $\mathrm{y}$ under uncertainty, called an economy with awa.ren $ess$ structur\^e where each trader having a strictly monotone preference makes decision under $\mathrm{h}\mathrm{i}\mathrm{s}/\mathrm{h}\mathrm{e}\mathrm{r}$ awareness and belief, and we introduce a generalized notion of equilibrium for the economy, called an expectations equilibrium in awareness. We show the existence theorem of the equilibrium and the fundamental welfare theorem for the economy, $\mathrm{i}.\mathrm{e}_{)}$ . an allocation in the economy is $\mathrm{e}\mathrm{x}$-ante Pareto optimal if and only if it is an expectations equilibrium allocation in awareness.	pareto efficiency;traders;monotone	Ken Horie;Takashi Matsuhisa	2005		10.1007/11428862_78	uncertainty;belief;mathematical economics;statistics	AI	-6.559956583319095	-1.2055234397366055	99970
71f6e5204745ed392dbf04bb6a13cb35227f4b5b	the impact of preference structures in multi-issue negotiations - an empirical analysis	empirical analysis			Rudolf Vetschera	2005		10.1007/3-540-32539-5_122	mathematics	SE	-6.378759120364584	-9.653896381088881	100065
06e07335a28d7057ab694529d6ef3ac848c643ed	uniform proofs of order independence for various strategy elimination procedures	mixed strategy	We provide elementary and uniform proofs of order independence for various strategy elimination procedures for finite strategic games, both for dominance by pure and by mixed strategies. The proofs follow the same pattern and focus on the structural properties of the dominance relations. They rely on Newman’s Lemma (see Newman [1942]) and related results on the abstract reduction systems. On leave from CWI, Amsterdam, the Netherlands and University of Amsterdam.	abstract interpretation;elementary;rewriting;samuel newman	Krzysztof R. Apt	2004	CoRR		game theory;economics;strategy;microeconomics;mathematical economics	AI	-6.0850382459815435	-1.3329534285803843	100082
be3b94e795f67090324b0813ed87f779e22af175	implementation by self-relevant mechanisms: applications	single peaked preference;exchange economy;satisfiability;social choice correspondence	Abstract   Tatamitani [J. Math. Econ. 35 (2001) 427] characterized Nash implementation by self-relevant mechanisms, where each agent announces only his preferences with an outcome. Self-relevancy is imposed as a condition informationally decentralized mechanisms should satisfy. The aim of this paper is to investigate the effects of imposing self-relevancy by examining implementability of well-known social choice correspondences. To this end, we first give algorithms for checking implementability by self-relevant mechanisms, and then provide some simple sufficient conditions for it. Using these, we examine implementation by self-relevant mechanisms in pure exchange economies, rationing problems under single-peaked preferences, and matching problems.		Yoshikatsu Tatamitani	2002	Mathematical Social Sciences	10.1016/S0165-4896(02)00037-9	economics;public economics;microeconomics;mathematical economics;welfare economics;satisfiability	Theory	-6.005306340122211	-1.82687628092002	100344
e39a273dc41c33c18c1934aeb2a0c6b434e679bc	developing constraint-based applications with spreadsheets	search problem;optimisation;spreadsheet;optimizacion;intelligence artificielle;problema investigacion;feedback;artificial intelligence;optimization;inteligencia artificial;boucle reaction;probleme recherche;retroalimentacion;tableur	Spreadsheets are in wide-spreadindustrial usefor light-weight businessapplications, whereby the broadacceptanceis both foundedon the underlying intuitive interactionstylewith immediatefeedbackanda ”programmingmodel” comprehensible for non-programmers.In this paperwe show how the spreadsheet developmentparadigmcan be extendedto modelandsolve a specialclassof search andoptimizationproblemsthat occur in many applicationdomainsandwould otherwiserequirethe involvementof specializedknowledgeengineers. We present a development framework that extendsa state-of-the-art spreadsheet environmentby smoothly incorporatinga light-weight Constraint Satisfactionreasonerinto the spreadsheet development paradigm. Basedon a real-world example from the domainof productconfiguration,we describethecomponentsof thedevelopedframework anddiscussthetypesof problemsthatcanbetackledwith thisapproach.Finally, acomparisonof our approachwith relatedwork in thefield andalternativeapproachesis given.	memoization;programmer;programming paradigm;smoothing;spreadsheet	Alexander Felfernig;Gerhard Friedrich;Dietmar Jannach;Christian Russ;Markus Zanker	2003		10.1007/3-540-45034-3_20	search problem;computer science;artificial intelligence;machine learning;feedback;algorithm	SE	-17.55341358780978	-2.602106660097806	100402
b0bd7679a3a56ef957e8d35606edcf1f2c5dbca0	steadfastness, deference, and permissive rationality		Recently, Levinstein (Philos Phenomenol Res, 2015) has offered two interesting arguments concerning epistemic norms and epistemic peer disagreement. In his first argument, Levinstein claims that a tension between Permissivism and steadfast attitudes in the face of epistemic peer disagreement generally leads us to conciliatory attitudes; in his second argument, he argues that, given an ‘extremely weak version of a deference principle,’ Permissivism collapses into Uniqueness. However, in this paper, I show that when we clearly distinguish among several types of Permissivism (what I call Permissivism $$_{1}$$ 1 , Permissivism $$_{2}$$ 2 , and Permissivism $$_{3}$$ 3 ), we can see that any type of Permissivism fits well with steadfast attitudes. Further, even though Levinstein’s ‘extremely weak version of a deference principle’ does rule out a possibility for some types of Permissivism (Permissivism $$_{1}$$ 1 and Permissivism $$_{2}$$ 2 ), it is still compatible with the other type of Permissivism (Permissivism $$_{3}$$ 3 ), so we may regard at least that version of a deference principle as a viable position in connection with that particular type of permissive rationality.	doxastic logic;fits;my life as a teenage robot;rational agent;rationality;windows preinstallation environment	Jaemin Jung	2016	Synthese	10.1007/s11229-016-1197-7	epistemology;mathematics	NLP	-12.303095115980955	4.116299302298878	100649
a5a5dd0236743829812c85dcf238406365b12ab8	distributed data fusion in the dempster-shafer framework		Dempster-Shafer theory is a formal framework for reasoning and decision-making under uncertainty. A cornerstone in this formalism is Dempster's rule, which provides a mechanism for combining belief functions representing independent pieces of evidence. This rule has been used extensively in information fusion applications. In this paper, we consider the situation where several agents are located at the nodes of a network and can communicate only with their neighbors. We show that synchronous or asynchronous linear consensus mechanisms can be used to combine the agents' belief functions in a distributed way. After convergence, a consensus state is reached, in which all agents hold the same belief, which is equal to the orthogonal sum of the agents' initial belief functions. Simulation results with a simple distributed classification are reported.	graph coloring;semantics (computer science);simulation;social network	Orakanya Kanjanatarakul;Thierry Denoeux	2017	2017 12th System of Systems Engineering Conference (SoSE)	10.1109/SYSOSE.2017.7994954	dempster–shafer theory;sensor fusion;formalism (philosophy);machine learning;cognition;asynchronous communication;computer science;wireless ad hoc network;convergence (routing);artificial intelligence	AI	-17.783109067580465	3.3335701220929668	100913
9236f4d7801f8c7b9ed00a62ca3b3c042a8398cb	algorithmic challenges in learning path metrics from observed choices	explicit knowledge;polynomial time algorithm;optimal path;mathematical model	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	András Faragó	2008	Applied Artificial Intelligence	10.1080/08839510802164234	mathematical optimization;fast path;any-angle path planning;longest path problem;computer science;word metric;explicit knowledge;intrinsic metric;machine learning;mathematical model;shortest path problem	Robotics	-14.698608113982736	-6.691334977847318	100941
1e3c94aa9b27807023314a97311493558111852e	approval quorums dominate participation quorums	institutional repositories;underdog compensation;fedora;proportional influence;information aggregation;implementation;vital;direct democracy;social choice;electoral system;vtls;winner take all;approval quorum;ils;preference aggregation;participation quorum	We study direct democracy with population uncertainty. Voters’ participation is often among the desiderata by the election designer. We show that with a participation quorum, i.e. a threshold on the fraction of participating voters below which the status quo is kept, the status quo may be kept in situations where the planner would prefer the reform, or the reform is passed when the planner prefers the status quo. On the other hand, using an approval quorum, i.e. a threshold on the number of voters expressing a ballot in favor of the reform below which the status quo is kept, we show that those drawbacks of participation quorums are avoided. Moreover, an electoral system with approval quorum performs better than one with participation quorum even when the planner wishes to implement the corresponding participation quorum social choice function.		François Maniquet;Massimo Morelli	2015	Social Choice and Welfare	10.1007/s00355-014-0804-0	direct democracy;winner-take-all;public relations;aggregation problem;social choice theory;economics;microeconomics;public administration;implementation;law	AI	-7.203422793361687	-3.9034837806613183	101010
46f21d2a6f2ee8b031a41ba7d33c27d1c0fb4d66	a qualitative linear utility theory for spohn's theory of epistemic beliefs	spohn s theory of epistemic beliefs;book chapter;expected utility;order of magnitude probabilities;decision maker;decision problem;decision theory;utility theory	"""In this paper, we formulate a qualitative """"lin­ ear"""" utility theory for lotteries in which un­ certainty is expressed qualitatively using a Spohnian disbelief function. We argue that a rational decision maker facing an uncertain decision problem in which the uncertainty is expressed qualitatively should behave so as to maximize """"qualitative expected utility."""" Our axiomatization of the qualitative util­ ity is similar to the axiomatization developed by von Neumann and Morgenstern for prob­ abilistic lotteries. We compare our results with other recent results in qualitative de­ cision making."""	approximation;axiomatic system;decision problem;expected utility hypothesis;minimax;rationality;regret (decision theory)	Phan Hong Giang;Prakash P. Shenoy	2000			decision-making;optimal decision;decision theory;two-moment decision model;expected utility hypothesis;decision analysis;decision tree;decision problem;decision rule;mathematics;subjective expected utility;mathematical economics;evidential decision theory;von neumann–morgenstern utility theorem;utility;expected value of including uncertainty;weighted sum model;statistics	AI	-9.064236679472904	-0.5347822538687091	101297
72fdd503913a7735423fb6f08b5686e37eb621b9	a new epistemic characterization of ε-proper rationalizability	epistemic game theory;incomplete information;proper rationalizability	"""For a given """" > 0; the concept of """"-proper rationalizability (Schuhmacher (1999)) is based on two assumptions: (1) every player is cautious, i.e., does not exclude any opponents choice from consideration, and (2) every player satises the """"-proper trembling condition, i.e., the probability he assigns to an opponents choice a is at most """" times the probability he assigns to b whenever he believes the opponent to prefer b to a. In this paper we provide a new epistemic foundation for """"-proper rationalizability within an incomplete information framework, where players are uncertain about the opponents utilities. We show that a belief hierarchy is """"-properly rationalizable in the complete information framework, if and only if, there is an equivalent belief hierarchy within the incomplete information framework that expresses common belief in the events that (1) players are cautious, (2) players choose rationally, and (3) the playersbeliefs about the opponents utilities are centered around the actual utilitiesin some specic way parametrized by """". JEL classication: C72"""	information framework	Andrés Perea;Souvik Roy	2017	Games and Economic Behavior	10.1016/j.geb.2017.04.009	economics;rationalizability;mathematics;mathematical economics;welfare economics;complete information;fictitious play	AI	-8.4769990169472	-2.1550826098261253	101411
7f20889bc1958b89c32fc58f13c8a7557533b408	sat-based abductive diagnosis		Increasing complexity and magnitude of technical systems demand an accurate fault localization in order to reduce maintenance costs and system down times. Resting on solid theoretical foundations, model-based diagnosis provides techniques for root cause identification by reasoning on a description of the system to be diagnosed. Practical implementations in industries, however, are sparse due to the initial modeling effort and the computational complexity. In this paper, we utilize a mapping function automating the modeling process by converting fault information available in practice into propositional Horn logic sentences to be used in abductive model-based diagnosis. Furthermore, the continuing performance improvements of SAT solvers motivated us to investigate a SAT-based approach to abductive diagnosis. While an empirical evaluation did not indicate a computational benefit over an ATMS-based algorithm, the potential to diagnose more expressive models than Horn theories encourages future research in this area.	abductive reasoning;algorithm;compiler;computation;computational complexity theory;horn clause;set cover problem;sparse matrix	Roxane Koitz;Franz Wotawa	2015			implementation;machine learning;computational complexity theory;root cause;magnitude (mathematics);artificial intelligence;computer science	AI	-18.264021829808282	0.3186511458112842	101416
d3153c878a37de1f592ed6e79f8c1edd11b622eb	an introduction to default logic	default logic	"""One of Hubert Dreyfus's more biting criticisms of Artificial Intelligence has been that its success stories have been largely confined to toy environments. Some researchers are unperturbed by this criticism, apparently believing that an accretion of little success stories will eventually reward us with a cumulative global success story. Others have turned to renewed efforts to understand what Herbert Simon calls """"weak methods"""" of inference, that is methods of inference which do not presuppose specific knowledge of a domain. I too believe that knowledge-poor methods of inference offer greater long-term promise for the development of AI. Philippe Besnard's book is a codification of efforts to formalize one species (genus?) of knowledge-poor reasoning, what has been variously called default, defeasible, and nonmonotonic reasoning."""	artificial intelligence;default logic;defeasible reasoning;genus (mathematics);non-monotonic logic;philippe kruchten	Philippe Besnard	1989		10.1007/978-3-662-05689-9	predicate logic;dynamic logic;zeroth-order logic;discrete mathematics;description logic;higher-order logic;many-valued logic;intermediate logic;artificial intelligence;non-monotonic logic;predicate functor logic;mathematics;signature;default logic;substructural logic;multimodal logic;algorithm;philosophy of logic;autoepistemic logic	AI	-11.857806847110929	3.5386602661136966	101493
a41869e5f8be44641fafd66f4a9897de67a19007	the social media dispositive and monetization of user-generated content	althusser;marx;theory of ideology;information society;social media dispositive;free labor	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	autonomous robot;dbpedia;exploit (computer security);francis;monetization;personally identifiable information;primary source;social media;subject (philosophy);theory;user-generated content	Melita Zajc	2015	Inf. Soc.	10.1080/01972243.2015.977636	social science;economics;sociology;law	Robotics	-15.36355450053186	-5.769905075646638	101609
9517ddbd9756838298afb943df04c04b4dc670f5	meta-epistemology and the varieties of epistemic infinitism	science philosophy;philosophie des sciences	I will assume here the defenses of epistemic infinitism are adequate and inquire as to the variety standpoints within the view. I will argue that infinitism has three varieties depending on the strength of demandingness of the infinitist requirement and the purity of its conception of epistemic justification, each of which I will term strong pure, strong impure, and weak impure infinitisms. Further, I will argue that impure infinitisms have the dialectical advantage.	pure function	Scott F. Aikin	2007	Synthese	10.1007/s11229-007-9196-3	philosophy;epistemology;mathematics;infinitism	AI	-12.663964588339146	4.130541685093928	101680
ff0fcc247c1b895f3b5f1804bda43e653a7fd58a	auctioning over probabilistic options for temporal logic-based multi-robot cooperation under uncertainty		Coordinating a team of robots to fulfill a common task is still a demanding problem. This is even more the case when considering uncertainty in the environment, as well as temporal dependencies within the task specification. A multi-robot cooperation from a single goal specification requires mechanisms for decomposing the goal as well as an efficient planning for the team. However, planning action sequences offline is insufficient in real world applications. Rather, due to uncertainties, the robots also need to closely coordinate during execution and adjust their policies when additional observations are made. The framework presented in this paper enables the robot team to cooperatively fulfill tasks given as temporal logic specifications while explicitly considering uncertainty and incorporating observations during execution. We present the effectiveness of our ROS implementation of this approach in a case study scenario.	algorithm;goal programming;online and offline;robot;temporal logic	Philipp Schillinger;Mathias Bürger;Dimos V. Dimarogonas	2018	2018 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2018.8462967	resource management;task analysis;real-time computing;probabilistic logic;control engineering;robot;temporal logic;robot kinematics;engineering	Robotics	-18.915992843907077	-8.303823411946315	101897
45ca2c387eebf53c539ab6e8aa51e7f8c55a993e	a counter-example to karlin's strong conjecture for fictitious play	zero sum games;convergence;nash equilibrium;games convergence vectors nash equilibrium linear programming heuristic algorithms;karlin s strong conjecture zero sum games natural dynamic equilibrium play payoff matrix fictitious play;karlin s conjecture;electrical engineering and computer science;karlin s conjecture fictitious play zero sum games;thesis;vectors;heuristic algorithms;games;linear programming;fictitious play;matrix algebra game theory;article	Fictitious play is a natural dynamic for equilibrium play in zero-sum games, proposed by Brown [6], and shown to converge by Robinson [33]. Samuel Karlin conjectured in 1959 that fictitious play converges at rate O(t-1/2) with respect to the number of steps t. We disprove this conjecture by showing that, when the payoff matrix of the row player is the n × n identity matrix, fictitious play may converge (for some tie-breaking) at rate as slow as Ω(t-1/n).	converge	Constantinos Daskalakis;Qinxuan Pan	2014	2014 IEEE 55th Annual Symposium on Foundations of Computer Science	10.1109/FOCS.2014.10	games;mathematical optimization;combinatorics;convergence;economics;computer science;linear programming;mathematics;zero-sum game;mathematical economics;nash equilibrium;fictitious play	Theory	-4.943061207810802	-0.4045054329776718	102164
e39830ce7ff6d7f046aa3159124def3c8fcd82b4	the pseudo-average rule: bankruptcy, cost allocation and bargaining	balanced contributions;claims problem;cost allocation;rationing;equal area;bargaining;bankruptcy cost;pseudo average solution;bankruptcy	A division rule for claims problems, also known as bankruptcy or rationing problems, based on the pseudo-average solution is studied (for 2-person problems). This solution was introduced in Moulin (Jpn Econ Rev 46:303–332, 1995) for discrete cost allocation problems. Using the asymptotic approach, we obtain a division rule for claims problems. We characterize the division rule axiomatically and show that it coincides with the rule associated to the equal area bargaining solution (this is not true for n = 3). Moreover, following Moulin and Shenker (J Econ Theor 64:178–201, 1994), we show that its associated solution for continuous homogeneous goods is precisely the continuous pseudo-average solution.		Txus Ortells;Juan Carlos Santos	2011	Math. Meth. of OR	10.1007/s00186-010-0333-2	bankruptcy;actuarial science	ECom	-6.227742264903826	-2.7487625329997916	102412
6abc693a5693b372e3a40fae961448b3fe75ee42	bounds on direct and indirect effects of treatment on a continuous endpoint	mediation analysis;bound;direct and indirect effects;causal inference	Direct effect of a treatment variable on an endpoint variable and indirect effect through a mediate variable are important concepts for understanding a causal mechanism. However, the randomized assignment of treatment is not sufficient for identifying the direct and indirect effects, and extra assumptions and conditions are required, such as the sequential ignorability assumption without unobserved confounders or the sequential potential ignorability assumption. But these assumptions may not be credible in many applications. In this article, we consider the bounds on controlled direct effect, natural direct effect, and natural indirect effect without these extra assumptions. Cai et al. [2008] presented the bounds for the case of a binary endpoint, and we extend their results to the general case for an arbitrary endpoint.	causal filter;communication endpoint	Peng Luo;Zhi Geng	2015	ACM TIST	10.1145/2668134	mediation;causal inference;statistics	AI	-14.019625963215816	-1.6972132349450877	102416
0f19610fecf2a46ee2aebf462c484463070a5d10	from computing sets of optima, pareto sets, and sets of nash equilibria to general decision-related set computations	computing sets;generic algorithm;pareto set;nash equilibria;sets of optima;pareto sets	Several algorithms have been proposed to compute sets of optima, Pareto sets, and sets of Nash equilibria. In this paper, we present a general algorithm for decision-related set computations that includes all these algorithms as particular cases. To make our algorithm understandable to people working in optimization and in game theory, we also provide motivations and explanations for our formalizations of the corresponding problems and for the related notions of computable mathematics.	algorithm;computable function;computation;game theory;mathematical optimization;nash equilibrium;pareto efficiency	Vladik Kreinovich;Bartlomiej Jacek Kubica	2010	J. UCS	10.3217/jucs-016-18-2657	mathematical optimization;genetic algorithm;computer science;nash equilibrium	Theory	-6.994941731631123	2.3834495937222777	102457
3c7b00bff56da1e2f7e1c6c5aadb1a3fe5f7218c	graph based representation of dynamic planning	theoretical model;dynamic program;communication conference;dynamic environment;graph representation;environmental change;dynamic planning;auctions	"""profit, time, pleasure, etc.) by achieving this goal and consequently to define the efficient actions for this end. Abstract. Dynamic planning concerns the planning and execution of actions in a dynamic, real world environment. Its goal is to take into account changes generated by unpredicted events occurred during the execution of actions. In this paper we develop the theoretic model of dynamic planning presented in [12]. This model proposes a graph representation of possible, efficient and best plans of agents acting in a dynamic environment. Agents have preferences among consequences of their possible actions performed to reach a fixed goal. Environmental changes and their consequences are taken into account by several approaches proposed in the so-called """"reactive planning"""" field. The dynamic planning approach we propose, handles in addition changes on agents ́ preferences and on their methods to evaluate them; it is modeled as a multi-objective dynamic programming problem. Further on by introducing some additional information concerning his preferences, it is possible to define the best plan as the preferred compromise. During the execution of a single action the agent may modify his evaluations (a revision is necessary) or the world may be modified after an unanticipated event (an update is necessary). Such changes (how these are perceived is not considered in this paper) may invalidate the plan under execution in the sense that it could be impossible to follow it or it could be no more convenient. So, the aim of our dynamic planning model is to take into account such changes and to decide what the agent should do. In the following, section 2 outlines our formalism and describes the multi-criteria planning model. Section 3 presents several algorithmic aspects of the dynamic planning model we propose. We conclude by comparing our research to related work and discussing some problems."""	dynamic programming;graph (abstract data type);reactive planning;theory	Pavlos Moraitis;Alexis Tsoukiàs	2000			simulation;environmental change;computer science;artificial intelligence;graph	AI	-11.970585686153544	-1.1388299906116937	102461
cdc5eee4ab797f205c45e758cbb96b58eeade9f6	the complexity of angel-daemons and game isomorphism		The analysis of the computational aspects of strategic situations is a basic field in Computer Sciences. Two main topics related to strategic games have been developed. First, introduction and analysis of a class of games (so called angel/daemon games) designed to asses web applications, have been considered. Second, the problem of isomorphism between strategic games has been analysed. Both parts have been separately considered.	daemon (computing);nash equilibrium;turing completeness;web application	Alina García-Chacón	2012	Bulletin of the EATCS		subgraph isomorphism problem;maximum common subgraph isomorphism problem;game complexity	AI	-6.912030854827429	1.7826920868001297	102506
0efa37621e379a7c93484f90488d6ebe7ac99026	learning in games with unstable equilibria	tasp;learning process;game theory;nash equilibrium;learning;learning in games;best response dynamics;games learning best response dynamics stochastic fictitious play mixed strategy equilibria;nash equilibria;price dispersion;stochastic fictitious play;games;fictitious play;convergence to equilibrium;games learning best response dynamics stochastic fictitious play mixed strategy equilibria tasp;mixed strategy;mixed strategy equilibria	We propose a new concept for the analysis of games, the TASP, which gives a precise prediction about non-equilibrium play in games whose Nash equilibria are mixed and are unstable under fictitious play-like learning. We show that, when players learn using weighted stochastic fictitious play and so place greater weight on recent experience, the time average of play often converges in these “unstable” games, even while mixed strategies and beliefs continue to cycle. This time average, the TASP, is related to the cycle identified by Shapley [L.S. Shapley, Some topics in two person games, in: M. Dresher, et al. (Eds.), Advances in Game Theory, Princeton University Press, Princeton, 1964]. The TASP can be close to or quite distinct from Nash equilibrium. © 2008 Elsevier Inc. All rights reserved. JEL classification: C72; C73; D83	control theory;game theory;nash equilibrium	Michel Benaïm;Josef Hofbauer;Ed Hopkins	2009	J. Economic Theory	10.1016/j.jet.2008.09.003	game theory;mathematical optimization;economics;microeconomics;mathematical economics;nash equilibrium	AI	-4.5694169993598726	-1.405365523732597	102749
7e613db3c5273aeacbcd7a8d5e7074ab4754ee9a	handling uncertain rules in composite event systems		In recent years, there has been an increased need for active systems systems that are required to act automatically based on events, or changes in the environment. In many cases, the events to which the system should respond to, have to be inferred from other events based on complex temporal predicates. However, none of the existing composite event systems created to enable such inference can deal with cases in which an event cannot be inferred with absolute certainty based on the reported events. Therefore, in this paper, we describe how a deterministic event composition system can be extended to manage such uncertainty, and specify the principles of a formal framework for such inference. The contribution of this framework is twofold: It extends the semantics of event composition in a natural manner for probabilistic settings, and it enables the application of these extensions to the quantification of the occurrence probability of events.		Segev Wasserkrug;Avigdor Gal;Opher Etzion	2005			machine learning;natural language processing;artificial intelligence;computer science;composite number	DB	-18.629234419198838	3.2822092102297353	102802
8a886c04bac7ee0aa74d4ec9cdb71ef91b030c3f	alternating-time temporal logics with irrevocable strategies	alternating time temporal logic	"""In Alternating-time Temporal Logic (ATL), one can express statements about the strategic ability of an agent (or a coalition of agents) to achieve a goal φ such as: """"agent i can choose a strategy such that, if i follows this strategy then, no matter what other agents do, φ will always be true"""". However, strategies in ATL are revocable in the sense that in the evaluation of the goal φ the agent i is no longer restricted by the strategy she has chosen in order to reach the state where the goal is evaluated. In this paper we consider alternative variants of ATL where strategies, on the contrary, are irrevocable. The difference between revocable and irrevocable strategies shows up when we consider the ability to achieve a goal which, again, involves (nested) strategic ability. Furthermore, unlike in the standard semantics of ATL, memory plays an essential role in the semantics based on irrevocable strategies."""	alternating-time temporal logic	Thomas Ågotnes;Valentin Goranko;Wojciech Jamroga	2007		10.1145/1324249.1324256	economics;computer science;artificial intelligence;operations management	AI	-11.829760778195569	-1.1928334343527287	102824
0a7bbb675967d9158d5e5d4abd29822ccea75b9a	pandering and electoral competition	elections;information aggregation;pandering	We study an election with two perfectly informed candidates. Voters share common values over the policy outcome of the election, but possess arbitrarily little information about which policy is best for them. Voters elect one of the candidates, effectively choosing between the two policies proposed by the candidates. We explore under which conditions candidates always propose the voters’ optimal policy. The model is extended to include strategic voting, policy-motivated candidates, imperfectly informed candidates, and heterogeneous preferences.		Gabriele Gratton	2014	Games and Economic Behavior	10.1016/j.geb.2014.01.006	public relations;public economics;political science	AI	-6.571316627122887	-4.368865415728522	102929
8d75baeec415d90cd60d3663999eff29f3e59fa0	individuation and the semantics of demonstratives		Obsessed by the cases where things go wrong, we pay too little attention to the vastly more numerous cases where they go right, and where it is perhaps easier to see that the descriptive content of the expression concerned is wholly at the service of this function [of identifying reference], a function which is complementary to that of predication and contains no element of predication in itself (Strawson [1974], p. 66).		Martin Davies	1982	J. Philosophical Logic	10.1007/BF00293432	artificial intelligence	Logic	-12.751185442201065	3.087986670401658	102960
2db57c29f657ab8d2c4c4251418d77660982ed9a	a novel cognitive cycle for fault diagnosis in infrastructural systems		Abstract Remote sensing techniques are being increasingly used for periodic structural health monitoring of vast infrastructures such as power transmission systems. The current efforts concentrate on analysis of visual and other signals captured from the sensing devices, to diagnose the faults. Such data collection and analysis is expensive in terms of both computational overheads as well as towards robotic maneuvering of the data collection platform, such as a UAV. In this paper, we model the data gathering platform as an intelligent situated agent, and propose to autonomously control its data gathering and analysis activities through a cognitive cycle, to optimize the cost of efforts in identifying the faults that may exist. In this context, we explore use of less expensive qualitative reasoning with the background knowledge expressed as a Qualitative Bayesian Network (QBN). We introduce a reactive, economical planning algorithm around QBN that controls the sequence of data collection and analysis, much like how human inspectors do. We substantiate our claims with the results of simulation of the corresponding cognitive cycle.		Hrishikesh Sharma;Hiranmay Ghosh;P. Balamuralidhar	2017		10.1016/j.procs.2017.08.062	data mining;situated;data collection;machine learning;artificial intelligence;structural health monitoring;reactive planning;qualitative reasoning;overhead (business);computer science;bayesian network;cognition	EDA	-18.186863367799248	-3.2168724698768534	103169
d2f29a8f0beb7f355678c62fa24cef9c987ef6c1	bilateral contracting in multi-agent energy markets: forward contracts and risk management		Electricity markets are systems for effecting the purchase and sale of electricity using supply and demand to set energy prices. Pool prices tend to change quickly and variations are usually highly unpredictable. Bilateral contracts allow market participants to set the terms and conditions of agreements independent of a market operator. This paper describes on-going work that uses the potential of agent-based technology to help addressing several important issues related to market models. Specifically, the paper is devoted to risk management in bilateral contracting of electricity. Two agents interact and trade according to the rules of an alternating offers protocol. The paper focuses on both risk attitude and risk asymmetry and how they can influence price negotiation. In particular, it describes the trading process, introduces strategies that model typical patterns of concessions, and presents several concession tactics. The article also presents a case study on forward bilateral contracting involving risk management: a producer agent and a retailer agent negotiate a three-rate tariff.	risk management	Hugo Algarvio;Fernando Lopes;João Santana	2015		10.1007/978-3-319-19033-4_22	forward market;finance;business;commerce	AI	-6.763283810981766	-8.25879455107948	103247
36dd517d0c89abd99ce133b3ea85e3f21e9b546a	pure and stationary optimal strategies in perfect-information stochastic games with global preferences	parity games;pure and stationary optimal strategies;stochastic games	We consider two-players zero-sum perfect information stochastic games with finitely many states and actions and examine the problem of existence of pure stationary optimal strategies. We show that the existence of such strategies for one-player games (Markov decision processes) implies the existence of such strategies for two-player games. The result is general and holds for any payoff mapping.	markov chain;markov decision process;stationary process	Hugo Gimbert;Wieslaw Zielonka	2009	CoRR		mathematical optimization;mathematics;mathematical economics;welfare economics	AI	-4.66349770147833	-1.4254392021227118	103263
e6e1f5df203a12e7e6633fb39c0626df915a0014	prediction of software reliability using an auto regressive process	akaike information criterion;prediccion;ar model;decomposition valeur singuliere;modelo autorregresivo;information criterion;numerical method;singular value decomposition;auto regressive;qr factorization;autoregressive model;critere information akaike;comparative study;defaillance;criterio informacion akaike;decomposicion valor singular;failures;fiabilite logiciel;fiabilidad logicial;schwartz information criterion;modele autoregressif;software reliability;prediction;fallo	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source;software quality;software reliability testing	S. Chatterjee;R. B. Misra;S. S. Alam	1997	Int. J. Systems Science	10.1080/00207729708929380	econometrics;machine learning;mathematics;autoregressive model;statistics	Robotics	-14.142554234712268	-5.62330276113105	103366
2a43080b95a8005555f150caeacadd6bab611e19	modeling crime scenarios in a bayesian network	case law databases;information overabundance;case law authority	Legal cases involve reasoning with evidence and with the development of a software support tool in mind, a formal foundation for evidential reasoning is required. Three approaches to evidential reasoning have been prominent in the literature: argumentation, narrative and probabilistic reasoning. In this paper a combination of the latter two is proposed.  In recent research on Bayesian networks applied to legal cases, a number of legal idioms have been developed as recurring structures in legal Bayesian networks. A Bayesian network quantifies how various variables in a case interact. In the narrative approach, scenarios provide a context for the evidence in a case. A method that integrates the quantitative, numerical techniques of Bayesian networks with the qualitative, holistic approach of scenarios is lacking.  In this paper, a method is proposed for modeling several scenarios in a single Bayesian network. The method is tested by doing a case study. Two new idioms are introduced: the scenario idiom and the merged scenarios idiom. The resulting network is meant to assist a judge or jury, helping to maintain a good overview of the interactions between relevant variables in a case and preventing tunnel vision by comparing various scenarios.	bayesian network;holism;interaction;numerical analysis	Charlotte S. Vlek;Henry Prakken;Silja Renooij;Bart Verheij	2013		10.1145/2514601.2514618	artificial intelligence;data mining	AI	-14.927769150545675	1.9052815967925945	103370
dfe692e23390aff704ae072ba4d399b9a029a72c	self-stabilization in preference-based systems	speed of convergence;p2p;stable marriages;decentralized system	Participants of a decentralized system often use some local ranking informations, for selection of effective collaborations. We say that such systems are preference-based. For most practical types of preferences, such systems converge towards a unique stable configuration. In this paper, we investigate the speed and quality of the convergence process with respect to the model parameters. Our results provide an interesting insight into the design of system parameters, such as the number of connections or the algorithm for choosing new partners.	self-stabilization	Fabien Mathieu	2008	Peer-to-Peer Networking and Applications	10.1007/s12083-008-0009-3	mathematical optimization;decentralised system;computer science;peer-to-peer;database	Networks	-9.956115894888926	-6.836610191970191	103427
68c74464277349a0ddc979827d5b0bcf198998cc	causal reasoning with neuron diagrams	programming language semantics;syntax;domain specific visual language;causal inference causal reasoning neuron diagrams causation domain specific visual language causal relationships syntax semantics dynamic execution static definition causal effect cause identification;causal relationships;causal effect;causal reasoning;neurons semantics medical services cognition toxicology mathematical model equations;semantics;inference mechanisms;diagrams;toxicology;cause identification;causal inference;visual languages;medical services;visual languages causality diagrams inference mechanisms programming language semantics;cognition;neuron diagrams;visual language;mathematical model;language extension;neuron diagrams visual languages causation;static definition;dynamic execution;neurons;causation;domain specificity;causality	The principle of causation is fundamental to science and society and has remained an active topic of discourse in philosophy for over two millennia. Modern philosophers often rely on ``neuron diagrams'', a domain-specific visual language for discussing and reasoning about causal relationships and the concept of causation itself. In this paper we formalize the syntax and semantics of neuron diagrams. We discuss existing algorithms for identifying causes in neuron diagrams, show how these approaches are flawed, and propose solutions to these problems. We separate the standard representation of a dynamic execution of a neuron diagram from its static definition and define two separate, but related semantics, one for the causal effects of neuron diagrams and one for the identification of causes themselves. Most significantly, we propose a simple language extension that supports a clear, consistent, and comprehensive algorithm for automatic causal inference.	algorithm;artificial neuron;causal filter;causal inference;causality;diagram;out-of-order execution;visual language	Martin Erwig;Eric Walkingshaw	2010	2010 IEEE Symposium on Visual Languages and Human-Centric Computing	10.1109/VLHCC.2010.23	natural language processing;computer science;communication;algorithm	AI	-18.778242987457	0.5114057021617826	103475
1f51620110dab23cb886a7025b30c2db9f52a089	on arbitrage-free pricing for general data queries		Data is a commodity. Recent research has considered the mathematical problem of setting prices for different queries over data. Ideal pricing functions need to be flexible – defined for arbitrary queries (select-project-join, aggregate, random sample, and noisy privacy-preserving queries). They should be fine-grained – a consumer should not be required to buy the entire database to get answers to simple “lowinformation” queries (such as selecting only a few tuples or aggregating over only one attribute). Similarly, a consumer may not want to pay a large amount of money, only to discover that the database is empty. Finally, pricing functions should satisfy consistency conditions such as being “arbitrage-free” – consumers should not be able to circumvent the pricing function by deducing the answer to an expensive query from a few cheap queries. Previously proposed pricing functions satisfy some of these criteria (i.e. they are defined for restricted subclasses of queries and/or use relaxed conditions for avoiding arbitrage). In this paper, we study arbitrage-free pricing functions defined for arbitrary queries. We propose new necessary conditions for avoiding arbitrage and provide new arbitrage-free pricing functions. We also prove several negative results related to the tension between flexible pricing and avoiding arbitrage, and show how this tension often results in unreasonable prices.	aggregate data;interaction;money	Bing-Rong Lin;Daniel Kifer	2014	PVLDB	10.14778/2732939.2732948	data mining;database	DB	-10.790528464009917	-3.887004585470863	103504
6c3f6d56a4eca945dc568ab353f13567d2dd2427	the ungrounded argument		There is an argument that has yet to be made wholly explicit though it might be one of the most important in contemporary metaphysics. This paper is an attempt to rectify that omission. The argument is of such high importance because it involves a host of central concepts, concerning actuality, modality, groundedness and powers. If Ellis’s (2001) assessment is correct, the whole debate between Humean and anti-Humean metaphysics might rest on this viability of the argument. The argument, which I call the Ungrounded Argument (abbreviated to UA), has in various implicit forms been discussed or defended by Blackburn (1990), Molnar (1999, 2003, ch. 8) and Ellis (2001, 114 and 2002, 74–75). It concerns the alleged possibility of ungrounded dispositional properties or causal powers. It is an argument against a thesis that might be called universal or global groundedness; namely, that every dispositional property is grounded in some property other than itself. In Section 2 I formulate, for the first time, an explicit version of the Ungrounded Argument and present the evidence and reasons for its premises. Along the way, I will clarify some of the key concepts and issues. In Section 3 I consider the likely responses to UA and identify the main basis on which it might be challenged. In Section 4, I try to distil the issue down to its central core and show what must be overcome, and what must be acknowledged, if the argument is to be accepted. The main aim of this paper is the explicit articulation of the argument. Sections 3 and 4 are briefer, therefore, and give only an indication of the lines that may have to be developed for the argument’s ultimate acceptance.	biconnected component;bruce ellis;causality;emoticon;modality (human–computer interaction);user agent	Stephen Mumford	2005	Synthese	10.1007/s11229-005-0570-8	epistemology;mathematics	AI	-13.454602933874206	3.30842564986874	103641
5d91b7241039b610b6a062102138de47d9156503	fishery management games: how to admit new members and reduce harvesting levels	subject classification 91b74;game theory;nash equilibrium;proportional rule;subject classification 91a40;subject classification 91a80;shapley value;transboundary fishery management;profitability;fishery management;subject classification 91b76;population monotonic allocation scheme	This paper applies game theory to address the problem of allocating profits among fishing nations, once the countries concerned have expressed an interest in achieving an agreement through a Regional Fishery Management Organization (RFMO). Proposing the population monotonic allocation scheme as management rule for division of profits, we argue that existing RFMOs can be expanded by means of the Shapley value. We also show that adjustment from the Nash equilibrium to sustainable or more efficient can be achieved by means of the proportional rule without harming any of the countries involved.	game theory;nash equilibrium;non-monotonic logic	Kim Hang Pham Do;Henk Folmer;Henk Norde	2008	IGTR	10.1142/S0219198908001960	game theory;economics;fisheries management;public economics;operations management;microeconomics;shapley value;mathematical economics;welfare economics;nash equilibrium;profitability index	AI	-6.2473867823414295	-3.172834795820899	103698
779a633ec9b62db1ade432b9df95de0963f1f7e1	statistical analysis of designed experiments	statistical analysis	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	experiment;francis;primary source	Peter Wludyka	2003	Technometrics	10.1198/tech.2003.s160	econometrics;computer science;statistics	Robotics	-14.491256572364794	-5.681604450353303	103834
4b11955c8ce6e996f3a882a8853a69716acc7467	decision making under uncertainty comprising complete ignorance and probability	probability;order of variables;ignorance	This paper investigates a model of decision making under uncertainty comprising opposite epistemic states of complete ignorance and probability. In the first part, a new utility theory under complete ignorance is developed that combines Hurwicz-Arrow’s theory of decision under ignorance with AnscombeAumann’s idea of reversibility and monotonicity used to characterize subjective probability. The main result is a representation theorem for preference under ignorance by a particular one-parameter function – the τ -anchor utility function. In the second part, we study decision making under uncertainty comprising an ignorant variable and a probabilistic variable. We show that even if the variables are independent, they are not reversible in the Anscombe-Aumann’s sense. This insight leads to the development of a new proposal for decision under uncertainty represented by a preference relation that satisfies the weak order and monotonicity assumptions but rejects the reversibility assumption. A distinctive feature of the new proposal is that the certainty equivalent of a mapping from the state space of uncertain variables to the prize space depends on the order in which the variables are revealed. Explicit modeling of the order of variables explains some of the puzzles in multiple-prior model and the models for decision making with Dempster-Shafer belief function.	decision theory;explicit modeling;state space;utility	Phan Hong Giang	2015	Int. J. Approx. Reasoning	10.1016/j.ijar.2015.05.001	optimal decision;probability;mathematics;statistics	AI	-9.183592395314054	-1.1158953741834257	103939
53b2ed9db47a5ea74352bcc9c34363963ba0212f	a generic agent architecture for cooperative multi-agent games	dissertacao;engenharia electrotecnica electronica e informatica	Traditional search techniques are difficult to apply to cooperative negotiation games, due to the often enormous search trees and the difficulty in calculating the value of a players position or move. We propose a generic agent architecture that ensembles negotiation, trust and opponent modeling, simplifying the development of agents capable of playing these games effectively by introducing modules to handle these challenges. We demonstrate the application of this modular architecture by instantiating it in two different games and testing the designed agents in a variety of scenarios; we also assess the role of the negotiation, trust and opponent modeling modules in each of the games. Results show that the architecture is generic enough to be applied in a wide variety of games. Furthermore, we conclude that the inclusion of the three modules allows for more effective agents to be built.	agent architecture;brane;dec alpha;formal language;general game playing;knowledge base;multi-agent system;search tree;server (computing);zillions of games	João Marinheiro;Henrique Lopes Cardoso	2017		10.5220/0006253101070118	computer science	AI	-17.428449342545868	-8.559371477302738	104124
23753ebd7d0aa8104c685adf2ef3829d8a0b04c3	when warrant transmits and when it doesn’t: towards a general framework	epistemic transmission;crispin wright;transmission of warrant;transmission failure;propositional warrant	In this paper we focus on transmission and failure of transmission of warrant. We identify three individually necessary and jointly sufficient conditions for transmission of warrant, and we show that their satisfaction grounds a number of interesting epistemic phenomena that have not been sufficiently appreciated in the literature. We then scrutinise Wright’s analysis of transmission failure and improve on extant readings of it. Nonetheless, we present a Bayesian counterexample that shows that Wright’s analysis is partially incoherent with our analysis of warrant transmission and prima facie defective. We conclude exploring three alternative lines of reply: developing a more satisfactory account of transmission failure, which we outline; dismissing the Bayesian counterexample by rejecting some of its assumptions; reinterpreting Wright’s analysis to make it immune to the counterexample.	counterfactual conditional;wright (adl)	Luca Moretti;Tommaso Piazza	2011	Synthese	10.1007/s11229-011-0018-2	epistemology;mathematical economics	NLP	-13.72693551136772	3.2180253496320885	104417
febad80a67bad670770643cb9bdb9e904602876f	clustering approach using artificial bee colony algorithm for healthcare waste disposal facility location problem			artificial bee colony algorithm;facility location problem	Zeynep Gergin;Nükhet Tunçbilek;Sakir Esnaf	2019	IJORIS	10.4018/IJORIS.2019010104		Theory	-12.33996016444881	-7.848091657722977	104506
e43ecf14650653be4d9f2b3feef9690ecf8ff671	is scientific theory-commitment doxastic or practical?	praxis;competition;engagement;non epistemique;individu;doxastic logic;theorie scientifique;individual;bayesianism;pratique;commitment;communaute scientifique;practice;logique doxastique;scientific theory;bayesianisme	Associated with Bayesianism is the claim that insofar as thereis anything like scientific theory-commitment, it is not a doxastic commitment to the truth of the theory or any proposition involving the theory, but is rather an essentiallypractical commitment to behaving in accordance with a theory. While there are a number of a priori reasons to think that this should be true, there is stronga posteriori reason to think that it is not in fact true of current scientific practice.After outlining a feature that distinguishes doxastic from practical commitment, I presentempirical evidence that suggests that, like perhaps all other theoretical commitment,scientific theory-commitment is doxastic.	doxastic logic	Ward E. Jones	2003	Synthese	10.1023/B:SYNT.0000004901.93310.03	competition;philosophy;epistemology;bayesian probability;doxastic logic;scientific theory;quantum mechanics	NLP	-12.724318383265317	3.279428072671555	104626
a1dad118a47f1e596df7289ca4d151eb07398d3d	rating protocols in online communities	online communities;rating schemes;recommended strategy;incentive schemes;whitewashing	Sustaining cooperation among self-interested agents is critical for the proliferation of emerging online communities. Providing incentives for cooperation in online communities is particularly challenging because of their unique features: a large population of anonymous agents having asymmetric interests and dynamically joining and leaving the community, operation errors, and agents trying to whitewash when they have a low standing in the community. In this article, we take these features into consideration and propose a framework for designing and analyzing a class of incentive schemes based on rating protocols, which consist of a rating scheme and a recommended strategy. We first define the concept of sustainable rating protocols under which every agent has the incentive to follow the recommended strategy given the deployed rating scheme. We then formulate the problem of designing an optimal rating protocol, which selects the protocol that maximizes the overall social welfare among all sustainable rating protocols. Using the proposed framework, we study the structure of optimal rating protocols and explore the impact of one-sided rating, punishment lengths, and whitewashing on optimal rating protocols. Our results show that optimal rating protocols are capable of sustaining cooperation, with the amount of cooperation varying depending on the community characteristics.	online community	Yu Zhang;Jaeok Park;Mihaela van der Schaar	2014	ACM Trans. Economics and Comput.	10.1145/2560794	environmental resource management;marketing;whitewash	Web+IR	-6.236572211088112	-7.735998702274175	104663
065d3e025175b19af6f936052d99151894c87b3f	incentives for sharing in peer-to-peer networks	e commerce;price dispersion;information value;shopbots	We consider the free-rider problem that arises in peer-to-peer file sharing networks such as Napster: the problem that individual users are provided with no incentive for adding value to the network. We examine the design implications of the assumption that users will selfishly act to maximize their own rewards, by constructing a formal game theoretic model of the system and analyzing equilibria of user strategies under several novel payment mechanisms. We support and extend upon our theoretical predictions with experimental results from a multi-agent reinforcement learning model.	centralized computing;game theory;multi-agent system;napster;peer-to-peer file sharing;reinforcement learning	Philippe Golle;Kevin Leyton-Brown;Ilya Mironov	2001		10.1145/501158.501193	e-commerce;computer science;marketing;value of information;microeconomics;mathematical economics;world wide web;commerce	ML	-7.326650258762334	-7.447455853263326	104681
3a19f9b5445b0a63ad17f4289f9f76fdd701e129	information transparency in prediction markets	forecasting;information aggregation;field experiment;prediction market;information transparency;market efficiency	Prediction markets are designed and conducted for the primary purpose of aggregating information so that market prices forecast future events. In such markets, a group of traders buy and sell contracts and the payoff depends on unknown future events. Information is the key in a prediction market and the success of prediction markets depends on their design. In this paper, we theoretically develop and empirically test the effects of IT-enabled information transparency on prediction market performance (information aggregation efficiency and predictive accuracy) through traders' behavior (traders' participation activity and traders' dynamic interactions). We developed twelve prediction markets and empirically tested our hypotheses using a field experiment. The results suggest that improved information transparency (disclosure of different traders' buy and sell orders) can lead to higher levels of traders' dynamic interactions. Increases in traders' participation activity and dynamic interactions lead to higher information aggregation efficiency and greater market predictive accuracy. Interestingly, however, full disclosure of information and complete transparency do not necessarily further improve traders' activities. This paper is one of the first to take an information-based view to study prediction markets and highlights the importance of information transparency in the design of prediction markets. We further discuss the managerial implications, limitations and future research. How does information transparency influence online prediction market performance?This paper theoretically develops and empirically tests the impact of information transparency in prediction markets.A fully-transparent market impedes traders'?dynamic interactions?in a prediction market rather than further improving them.Information transparency affects the information aggregation efficiency of a market through traders' dynamic interactions.Increases in traders' participation activity and dynamic interactions in a prediction market enhance market performance.		ShengYun Yang;Ting Li;Eric van Heck	2015	Decision Support Systems	10.1016/j.dss.2015.05.009	financial economics;field experiment;economics;forecasting;marketing;efficient-market hypothesis;statistics;commerce	ECom	-4.548175535307081	-8.016173184269304	105017
cb46196b9b2012ec75a87563cdeaf2f3ffc2e8ab	a market-based multi-issue negotiation model considering multiple preferences in dynamic e-marketplaces	dynamic change;electronic commerce;selected works;autonomous agent;bepress;profitability	Electronic commerce has been a significant commercial phenomenon in recent years and autonomous agents have made the advantages of e-markets more distinct. However, as e-market environments become open and dynamic, existing agent negotiation approaches expose some limitations. Static negotiation strategies and offer evaluation approaches might fail to capture dynamic changes of market situations, as well as changes of negotiators’ expectations on negotiation outcomes. When market situations change, agents may need modify their negotiation strategies, expectations and criteria on offer evaluations as well as counter-offer generations in order to maximize their profits. Furthermore, in multi-issue negotiations, agents may have multiple preferences, which might not be delivered by most of existing negotiation approaches. In this paper, we propose a market-based multi-issue negotiation model to capture the dynamic changes of negotiation environments and impacts on negotiation strategies, counter-offer generations and offer evaluations. Also, the proposed model allows negotiators to deliver multiple offers to match their different preferences and negotiators would have more chances to reach agreements. Experimental results illustrate improvements of the proposed model on negotiators’ utilities and efficiencies of the whole negotiation system by comparing with the performance of NDF negotiation model.	autonomous agent;autonomous robot;e-commerce	Fenghui Ren;Minjie Zhang;Chunyan Miao;Zhiqi Shen	2009		10.1007/978-3-642-11161-7_1	e-commerce;computer science;knowledge management;artificial intelligence;autonomous agent;profitability index	AI	-8.780052535152068	-8.687916611220608	105062
304cc5bdd1f43769babcfc8937675789ea4c6452	a multi-agent model of deceit and trust in intercultural trade	trading agents;agent modeling;deceit;transaction cost;transaction cost economics;trust and reputation management;national culture;multi agent simulation;trade partner selection;self esteem;culture;supply chain;product quality;negotiation	Trust is a sine qua non for trade. According to transaction cost economics, a contract always offers some opportunity to defect. In the case of asymmetric product information, where the seller is better informed about product quality than the buyer is, the buyer either has to rely on information provided by the seller or has to check the information by testing the product or tracing the supply chain processes, thus incurring extra transaction cost. An opportunistic seller who assumes the buyer to trust, may deliver a lower quality product than agreed upon. In human decisions to deceive and to show trust or distrust toward business partners, issues like morality, shame, self-esteem, and reputation are involved. These factors depend strongly on trader’s cultural background. This paper develops an agent model of deceit and trust and describes a multi-agent simulation where trading agents are differentiated according to Hofstede’s dimensions of national culture.	agent-based model;distrust;experiment;intelligent agent;multi-agent system;rationality;simulation;software bug;traders	Gert Jan Hofstede;Catholijn M. Jonker;Tim Verwaart	2009		10.1007/978-3-642-04441-0_18	transaction cost;express trust	AI	-4.9428071829957325	-7.212464478940902	105133
ff1530f35ba2e6e067ef421562d8cfe324ec8843	applied logistic regression	logistic regression	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;logistic regression;primary source	Joseph D. Conklin	2002	Technometrics	10.1198/tech.2002.s650	generalised logistic function;logistic regression;factor regression model;multinomial logistic regression;cross-sectional regression	Robotics	-14.510517921183364	-5.747154987502855	105262
130219a61479257c84aded141fa835b6b53f8142	how effective are electronic reputation mechanisms? an experimental investigation	trust;feedback mechanism;moral hazard;electronic markets;reputation;reputation mechanism;internet marketing;public good;organizational behavior;feedback system;reciprocity	E reputation or “feedback” mechanisms aim to mitigate the moral hazard problems associated with exchange among strangers by providing the type of information available in more traditional close-knit groups, where members are frequently involved in one another’s dealings. In this paper, we compare trading in a market with online feedback (as implemented by many Internet markets) to a market without feedback, as well as to a market in which the same people interact with one another repeatedly (partners market). We find that while the feedback mechanism induces quite a substantial improvement in transaction efficiency, it also exhibits a kind of public goods problem in that, unlike in the partners market, the benefits of trust and trustworthy behavior go to the whole community and are not completely internalized. We discuss the implications of this perspective for improving feedback systems.	feedback;goto;internet;moral hazard;trust (emotion)	Gary E. Bolton;Elena Katok;Axel Ockenfels	2004	Management Science	10.1287/mnsc.1030.0199	public relations;economics;marketing;feedback;microeconomics;management;commerce;organizational behavior	ECom	-4.897101558495194	-7.524141843166126	105446
fb62ed5fed4700416004c79188ff5d8bb9948e7b	how incomputable is finding nash equilibria?	nash equilibria	We investigate the Weihrauch-degree of several solution concepts from noncooperative game theory. While the consideration of Nash equilibria forms the core of our work, also pure and correlated equilibria, as well as various concepts of iterated strategy elimination, are dealt with. As a side result, the Weihrauch-degree of solving systems of linear inequalities is settled.	game theory;iteration;linear inequality;nash equilibrium;undecidable problem	Arno Pauly	2010	J. UCS	10.3217/jucs-016-18-2686		ECom	-5.306791005328576	-0.5988409112156238	105525
316bf53cfa38875f81f3fab319134e55cd21322f	the indeterminacy of equilibrium city formation under monopolistic competition and increasing returns	indeterminacy of city systems;smooth economies;city formation;transport costs;monopolistic competition;increasing returns	We study the indeterminacy of equilibrium in the Fujita-Krugman (1995) model of city formation under monopolistic competition and increasing returns. Both the number and the locations of cities are endogenously determined. Assuming smooth transportation costs, we examine equilibria in city-economies where a nite number of cities form endogenously. For any positive integer K, the set of equilibria with K distinct cities has a smooth manifold of dimension The authors thank an anonymous referee, Alex Anas, Glen Ellison, Masahisa Fujita, Jang-Ting Guo, John Nachbar, Esteban Rossi-Hansberg, Norman Schoeld, Jacques-François Thisse, and the participants at the North American Meetings of the Regional Science Association International 2001 in Charleston, the 2003 North American Winter Meeting of the Econometric Society in Washington, D.C., the 13th European Workshop on General Equilibrium Theory in Venice, and a seminar at the University of Kansas for comments, but retain responsibility for any errors.	françois lionet;indeterminacy in concurrent computation;norman margolus	Marcus Berliant;Fan-chin Kung	2006	J. Economic Theory	10.1016/j.jet.2005.05.009	returns to scale;economics;monopolistic competition;macroeconomics;microeconomics;economy;mathematical economics;market economy	NLP	-7.375843672024458	-4.506902649096718	105639
74d85bf991f7bed019cfd909e4bbed7fe7360589	on the difference between the cardinalities of measurable value functions and von neumann-morgenstern utility functions	fonction valeur;funcion utilidad;decomposition;algebra von neumann;fonction utilite;utility function;multiattribute;von neumann algebra;utility preference;algebre von neumann;theory;transformation lineaire;multiattribute measurable value functions;linear transformation;value function;descomposicion;transformacion lineal	Dyer and Sarin Dyer, J. S., R. K. Sarin. 1979. Measurable multiattribute value functions. Opns. Res.27 810-822. proposed an important generalization of the additive-multiplicative decomposition theorem for von Neumann-Morgenstern vNM utility functions to measurable value functions based on the cardinality property of measurable value functions. However, because of a difference between the cardinality of measurable value functions and the cardinality of vNM utility functions, an additional technical condition is needed, and presented here, for the decomposition theorem to hold.		Jean M. Deichtmann;François Sainfort	1997	Operations Research	10.1287/opre.45.2.307	von neumann algebra;mathematical optimization;measurable function;mathematical analysis;simple function;calculus;mathematics;linear map;bellman equation;decomposition;theory	Crypto	-6.843173768889302	-0.3835742353356622	105641
634b54138805bd2162f6ac1a6cbea0f57fefc25a	parmenides: facilitating democratic debate	administracion electronica;liverpool;government policy;course of action;administration publique;repository;administration electronique;electronic government;world wide web;civil service;university;administracion publica	This paper describes PARMENIDES, a system which facilitates structured debate about government policy. 1 The PARMENIDES System The last two decades have seen a deliberative turnin the study of democracy in political philosophy [1]. Prior theories of democracy viewed ordinary citizens as no more than passive consumers of political information and argument, acting only when called upon to vote. In contrast, deliberative theories view citizens as producers of information, engaging as consenting and rational participants in reasoned argument with one another and with their political representatives. Thus, in this view, democracy is not simply a matter of periodic voting: it should also engage governments and the People in a process of continuous debate. Today, with the opportunities provided by the World Wide Web, communication is physically easier than ever before, but the long-standing problems that bedevil the effectiveness of communication remain. To be effective, communication must be clear, unambiguous and structured so that misunderstandings are minimised. In [2] we proposed a structure for persuasive argument that was intended to ease these communication problems, and to promote informed debate. In this paper, we describe a program which exploits this structure, and illustrate it with an example. We start from an assumption that one party (say, the Government) has proposed an action or course of action, and presents a justification for this proposal to the other party, who may respond. The structure for the interaction between the two parties involves: a clear statement of the justification for an action, which makes explicit all the components of the reasoning underlying the argument; an opportunity to challenge any of the components and any of the inferential links between them; an opportunity for the proponent to respond to these challenges. Within this dialogue structure, we see the justification for an action as involving the following argument scheme: an understanding of the current situation; a view of the situation which will result from performance of the action; features of the new situation which are considered desirable (the aspects which the action was performed in order to realise); the social goals which are promoted by these features (the reasons why they are desirable). In [2], we advanced this structure for discussion and identified a number of ways in which it could be attacked. There, we identified fifteen distinct types of attack, several of Table 1. Table of Attacks AttackVariantsDescription 1 2 Disagree with the description of the current situation 2 7 Disagree with the consequences of the proposed action 3 6 Disagree that the desired features are part of the consequences 4 4 Disagree that these features promote the desired value 5 1 Believe the consequences can be realized by some alternative action 6 1 Believe the desired features can be realized through some alternative action 7 1 Believe that an alternative action realizes the desired value 8 1 Believe the action has undesirable side effects which demote the desired value 9 1 Believe the action has undesirable side effects which demote some other value 10 2 Agree that the action should be performed, but for different reasons 11 3 Believe the action will preclude some more desirable action 12 1 Believe the action is impossible 13 2 Believe the circumstances or consequences as described are not possible 14 1 Believe the desired features cannot be realized 15 1 Disagree that the desired value is worth promoting which had a number of variants according to the extent to which the attacker advanced a positive position in reply. Table 1 shows the attacks and the number of variants. It is this variety of attacks which causes many of the problems in communication of views when using traditional means of correspondence. Our original intention was to implement a program controlling a computer mediated dialogue, in which the locutions would represent moves implementing the above attacks: this would ensure that each move was unambiguously identified with its intended effect. This program has been successfully implemented in JAVA, but evaluation has shown that, for casual users, many problems remain. Selecting the correct moves to realize a desired attack on a position is a task almost as difficult as correctly phrasing an attack in natural language. Essentially there is too much freedom of expression provided, and hence an overwhelming variety of options to select between. For this reason we have decided that if support is to be given to enable the general public to express their views as cogently as possible, some simpler form of interaction is required. These are exactly the problems encountered by earlier systems which have attempted to support democratic debate and dialogue. We address these usability problems by leading the user through a fixed series of moves; by constraining the choice of the user, the need for the user to understand the underlying model so as to make informed selection of moves is removed. Additionally, wherever possible statements are presented for approval or disapproval, reducing the problems associated with expressing the content of the various locutions. PARMENIDES is intended to realize these objectives. The idea is to provide a simple web based interface which will guide the user in a structured fashion through a justification of an action giving opportunities to disagree at selected points. Each of these disagreements will represent one of the attacks above, so that the exact nature of the disagreement can be unambiguously identified. The users responses are written to a database so that information of what points of the argument are more strongly supported than others can be gathered. In the program described in this paper, PARMENIDES (Persuasive ARguMENt In DEmocracieS), we focus on negative criticism of the argument: in future work we will provide a similar interface to allow the construction of positive alternative arguments. Figure 1: Introductory screen PARMENIDES is implemented using PHP scripts and can be used at http://www.csc.liv.ac.uk/ ∼katie/Parmenides.html . The example debate concerns the invasion of Iraq in 2003. The aim of PARMENIDES is to present users with a position justifying a particular action and give them the opportunity to make a number of attacks on that position. We do not realize all of the attacks. Some of the attacks are directed against the soundness of the argument, and we here rely on the proponent of the position to produce only well formed arguments. Thus attacks 12, 13 and 14 are considered unnecessary, since we assume that the states of affairs and actions described are possible. Similarly we ignore attack 3: whether the features are entailed by the consequences is a matter of logic, and we rely on the proponent to produce a sound position. Attacks 7, 8, 9 and 11 involve the proposal of some counter position: here we do not provide facilities to allow the statement of alternative positions, but concentrate on gathering a critique of the original position. Finally we ignore attack 10: this is a subtle matter and required in some domains, but since it does not vitiate the proposed action it is not required here. This leaves six attacks which we wish to allow. After an introductory screen, Figure 1, which takes some information about the user and provides some explanation about the purpose and use of the system, the user is presented with a structured statement of the position to be considered. At this point users can simply accept the argument in which case they are sent to a farewell screen. Otherwise, the user is then lead through a series of forms where they are given the opportunity to agree or disagree with the following elements, which comprise the initial position: – the social values of the position (Attack 15), – the promotion of the values by the desired consequences of the proposed action (Attack 4). Here they also have the opportunity to state consequences of the action which they believe compromises the desired value (attack 8), – the consequences of the proposed action (Attack 2), – the suggestion of alternative actions to realize the desired consequences (Attack 5). – the description of the current situation (Attack 1). – the user is then taken to the exit screen. The navigation above realizes six of the fifteen attacks possible against a position listed in Table 1. Each of these attacks proposes no positive information, and thus represents the simplest variant where several variants are possible. The six attacks represent a critique of the position proposed: if none of them can be made, then, provided the position is well formed, the position does represent a justification of the proposed action. Of the nine attacks not provided, four challenge the well formedness of the position (which we assume to be in order here), and, apart from the special case of attack 10, which does not dispute the action, the remaining attacks contest the action by developing a justification of an alternative action. We propose that these attacks are best provided by giving an opportunity to continue from the current exit screen, being prompted to extend the information already provided so as to develop a new position. We are satisfied that PARMENIDES is usable by its target audience, and that it can effectively identify points of disagreement, and record them so that weight of opinion on various issues can be gauged. This is achieved without requiring the user of the system to have any particular familiarity with the underlying model of argument: the attacks are constructed from simple responses without any need for attacks to be explicitly formulated. Using PARMENIDES we can examine the acceptability of various parts of the position. For example, we are able to discriminate between those who support invasion for regime change from those who are concerned with international securit	database;emoticon;inferential programming;java;natural language;php;side effect (computer science);theory;usability;value (ethics);whole earth 'lectronic link;world wide web	Katie Atkinson;Trevor J. M. Bench-Capon;Peter McBurney	2004		10.1007/978-3-540-30078-6_52	public policy;computer science;world wide web	AI	-12.875257611270712	1.6851243302278733	106085
a1fac80ad3221b1f7027c6bccc244d25ce7137d4	intransitive indifference and rationalizability of choice functions on general domains	choice function	Abstract   This paper examines various intransitive preferences in terms of a behavior of a chooser in the general-domain framework, where a choice function may not be defined over all possible subsets of the fixed universe of alternatives. In characterizing the quasi-transitive rationalization of a choice function, the results of this paper show the trade off between the assumption on the domain of a choice function and the requirement that the choice set is single-valued or multi-valued.		Taradas Bandyopadhyay;Kunal Sengupta	2003	Mathematical Social Sciences	10.1016/S0165-4896(03)00039-8	discrete mathematics;choice function;economics;mathematics;mathematical economics;welfare economics	ECom	-7.323769613967629	-1.3793530954974687	106242
90232b363b9429f97af25ef60dce8e1bffe29087	the role of seed money and threshold size in optimizing fundraising campaigns: past behavior matters!	threshold size;conditional cooperation;fundraising;charitable giving;seed money;charity;field experiments;business and economics;differentiated communication	Fund raising appeals often announce that some funds have already been raised in order to reach a certain threshold. This article reports results from a field experiment examining the role of seed money (i.e., no, 50%, and 67%) in combination with threshold size (i.e., low versus high) in fundraising appeals across different targets (i.e., prospects, low fidelity donors, and high fidelity donors). Based on a 2x3x3 between-subjects design we investigate charitable behavior of 25,617 households. Findings reveal a novel qualification of using seed contributions as well as the necessity of a communication differentiation by considering past behavior. We show that seed money works well if the threshold is high but with a low threshold it could have a baleful influence. More specifically, in campaigns targeted at prospects and low fidelity donors, the announcement of seed money increases donations regardless of the threshold level. However, in campaigns targeted at high fidelity donors, seed money is an effective strategy only when the threshold is rather high.	money;optimizing compiler	Griet Alice Verhaert;Dirk Van den Poel	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.04.088	field experiment;seed money	ECom	-5.2726672616928605	-6.874474082037906	106281
0015634113362efcc10de2e4e0df36463739c51f	the likelihood of a condorcet winner in the logrolling setting		The purpose of this note is to compute the probability of logrolling for three different probabilistic cultures. The primary finding is that the restriction of preferences to be in accord with the condition of separable preferences creates enough additional structure among voters’ preference rankings to create an increase in the likelihood that a Condorcet winner will exist with both IC and IAC-based scenarios. Classification JEL : D71, D72.		William V. Gehrlein;Michel Le Breton;Dominique Lepelley	2017	Social Choice and Welfare	10.1007/s00355-017-1063-7	dodgson's method;economics;approval voting;mathematical economics;welfare economics;condorcet method;kemeny–young method	AI	-6.9351831509240025	-3.6093356240358903	106309
1efbed3614dfa2d3ca44102fbbefc3b6349de9aa	consolidating snomed ct's ontological commitment	universiteitsbibliotheek	SNOMED CT is a clinical terminology that uses logical axioms to provide terms with meaning. This enforces precise agreements about the ontological nature of the entities denoted by the terms, commonly described as ontological commitment. We demonstrate that SNOMED CT implicitly supports at least three different kinds of ontological commitments: (i) commitments to independently existing entities, (ii) commitments to representational artifacts and (iii) commitments to clinical situations. Our analysis shows how the truth-value of a sentence changes according to one of these perspectives. We argue that a clear understanding of to what kind of entities SNOMED CT terms denote is crucial for the proper use and maintenance of SNOMED CT. We argue that the three kinds of commitment can co-exist but need to be clearly distinguished.	ct scan;commitment scheme;entity;systematized nomenclature of medicine	Stefan Schulz;Ronald Cornet;Kent A. Spackman	2011	Applied Ontology	10.3233/AO-2011-0084	computer science;knowledge management;data mining;algorithm	AI	-17.221933356301932	1.9983800609299969	106410
0cfb4d83c5923c346b94a559544e53b7a2347fe4	two puzzles about computation		Turing’s classical analysis of computation [13] gives a compelling account of the nature of the computational process; of how we compute. This allows the notion of computability, of what can in principle be computed, to be captured in a mathematically precise fashion. The purpose of this note is to raise two different questions, which are rarely if ever considered, and to which, it seems, we lack convincing, systematic answers. These questions can be posed as: • Why do we compute? • What do we compute? The point is not so much that we have no answers to these puzzles, as that we have no established body of theory which gives satisfying, systematic answers, as part of a broader understanding. By raising these questions, we hope to stimulate some thinking in this direction. These puzzles were raised in [2]; see also [3].	computability;computation;turing	Samson Abramsky	2014	CoRR		mathematics;algorithm	Theory	-12.77687805816243	1.4613671930752479	106430
42ecb7a062d98a4454e7afbd3141021561f4dac4	a computational trust model for e-commerce systems		Trust is a complex and multidimensional concept, which plays a key role in the success of electronic commerce. Assessing trust, specifically in the beginning of a commercial relation and the formulation of trust in general is a complex and difficult task. The researchers are often focused on a specific context for trust formulation and the relevant literature does not clearly distinguish between the factors involving in trust decision making process. With the aim of providing a basis for computational trust models and by consolidating a large body of studied contexts in the trust literature, this paper first tries to present a conceptual trust model for electronic commerce. Four types of trust that are used in the conceptual trust model are as follows: (1) institutional trust, (2) technological trust, (3) trading party trust, and (4) propensity trust. Then, a computational trust model is proposed in which the agents involved in a commercial transaction can consult with a trust manager agent (TMA), which is considered in a distributed fashion in the network. The proposed model is capable of evaluating a broad range of trust contexts and has two main features: (1) trust is evaluated dynamically (i.e., a change in any of the trust’s parameters will result in the re-calculation of trust values) and (2) the proposed model is capable of making partial studies for the trust contexts presented in the conceptual model of trust. Finally, the proposed model is evaluated and the results are presented in this paper. KeywordsTrust model, trust evaluation, computational trust model, electronic commerce, trust manager agent (TMA)	computation;computational model;computational trust;e-commerce;electronic business;in the beginning... was the command line;prototype;rationality;tower mounted amplifier	Mehrnoosh Divsalar;Mohammad Abdollahi Azgomi	2009		10.1007/978-3-642-11532-5_2	public relations;knowledge management;political science;social psychology;computational trust	AI	-9.151028973479809	-8.389272608155018	106528
1e6bbb810952a82b3a2b80b8854d9f6668d09ab6	online bargaining as a form of dynamic pricing and the sellers' advantage from information assymmetry	revenue management;e commerce;online auction;information asymmetry;online bargaining;dynamic pricing;intelligent agent;profitability	Among the means of implementing dynamic pricing strategies in e-commerce, online bargaining is found to be better than revenue management and online auction, because each deal actually reaches a “win-win” situation for both the buyer and the seller in the sense that the mutually agreed deal price is higher than the seller’s reserved price but lower than the buyer’s reserved price. Such feature brings profit to the seller, as well as savings to the buyer. Meanwhile when bargaining online, there is an information asymmetry between the seller side, i.e. the company side, and the buyer side, which grants a great advantage to the sellers over the buyers. This information asymmetry can be captured and exploited for financial gains through adopting a properly designed online bargaining algorithm.	algorithm;bargain buddy;e-commerce;intelligent agent;online shopping;software agent;value (ethics)	Yihua Philip Sheng;Zhong Chen	2005			credit note;microeconomics;business;commerce	AI	-7.397537310589389	-8.137428940757701	107074
9fff486de22421b0f06ff5834fc7b03739cdd801	a framework for aggregating influenced cp-nets and its resistance to bribery	cp nets;bribery;influence;preferences	We consider multi-agent settings where a set of agents want to take a collective decision, based on their preferences over the possible candidate options. While agents have their initial inclination, they may interact and influence each other, and therefore modify their preferences, until hopefully they reach a stable state and declare their final inclination. At that point, a voting rule is used to aggregate the agents’ preferences and generate the collective decision. Recent work has modeled the influence phenomenon in the case of voting over a single issue. Here we generalize this model to account for preferences over combinatorially structured domains including several issues. We propose a way to model influence when agents express their preferences as CP-nets. We define two procedures for aggregating preferences in this scenario, by interleaving voting and influence convergence, and study their resistance to bribery. People often exchange opinions before taking a decision (Krackhardt 1987; Grabisch and Rusinowska 2012). In assemblies, “rules of order” typically prescribe a debate to take place before actual voting. When managers discuss whether to introduce new technologies in a company, the discussion may take many rounds, in which the initial opinions may change because of the influence of what others say, until a stable set of opinions is formed; at that point, voting can take place and a collective decision is taken. Polls in political elections provide a representative sample of the opinions of the voters, and may induce some voters to change their mind about the candidates. Also, in a marketing context involving complex choices, some people may be identified as followers or prescribers for certain features. Moreover, influences can be used to model hierarchical organizations. We consider scenarios where a set of agents need to take a collective decision by voting over the possible candidate decisions, and may exchange information before actually declaring their final vote. We assume that the information agents exchange is the mere observation of others’ vote: agents may revise their vote on the basis of the observed votes of others. In other words, agents may influence each other, leading to their preferences be modified accordingly. Influence is usually an iterative process, during which agents Copyright c © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. can be at the same time influencing and influenced entities, so they may change their inclination more than once based on the changes in the preferences of other agents. Some influence schemes may converge, while others may loop. The concept of influence has been widely studied in psychology, economics, sociology, and mathematics (DeGroot 1974; DeMarzo and Vayanos 2003; Krause 2000). An overview of dynamic models of social influence can be found in (Jackson 2008). Recent work has formally modelled and studied the influence phenomenon in the case of taking a decision over a single two-values issue (Grabisch and Rusinowska 2011). In the context of human users these influence schemes arise from well-studied social models that are estimated via polls, while in the context of artificial agents, the influences can represent natural hierarchical organizations of agents. Here we generalize these schemes and models to account for preferences over combinatorially structured domains including several issues that may be dependent on each other. In fact, the set of possible decisions, over which agents express their preferences, may have a combinatorial structure, that is, each candidate decision can be seen as the combination of certain issues, where each issue has a set of possible instances. Even if there are few issues and instances, they could give rise to a large number of candidate decisions. A compact way to express one’s preferences over such a large set is preferable, otherwise too much space would be needed to rank all possible alternatives. CP-nets are a successful framework that allows one to do this (Boutilier et al. 2004). They exploit the independence among some issues to give conditional preferences over small subsets of them. CP-nets have already been considered in a multi-agent voting setting (Rossi, Venable, and Walsh 2004; Lang and Xia 2009; Purrington and Durfee 2007; Xia, Conitzer, and Lang 2008; Mattei et al. 2013). Here we adapt such frameworks to incorporate influences among agents, by allowing influences to be over the same issue or also among different issues. An interesting feature of our model is that influence is embedded smoothly in the multi-agent CP-net profile, and there is a convenient coincidence between the optimal outcomes of certain CP-nets and the stable states of the influence iterative process. To aggregate preferences in this framework, we propose two procedures, called Finally Aggregation (FA) and Level Aggregation (LA), to find a collective decision by interleaving voting and influence convergence. FA performs influence iteration at each level of the CP-nets and it aggregates agents’ preferences only at the end, while LA performs influence iteration and preference aggregation at each level. We then evaluate such procedures in terms of resistance to bribery. Bribery in voting may be regarded as a type of influence, although it does not involve an iterative process: an external agent (the briber) wants to influence the result of the voting process by convincing some agents to change their vote, in order to get a collective result which is more preferred to him; there is usually a limited budget to be spent by the briber to convince agents (Faliszewski, Hemaspaandra, and Hemaspaandra 2009). We show that the presence of inter-agent influence can make bribery computationally difficult, even in a very restrictive setting, both for LA and FA. On the other hand, there are cases where bribery can be computationally easy for LA. The paper is a revised and extended version of (Maudet et al. 2012).	aggregate data;algorithm;artificial intelligence;boolean network;cp system;cp/m;computational complexity theory;converge;decision theory;embedded system;entity;forward error correction;gene regulatory network;hadamard transform;intelligent agent;iteration;iterative method;jackson;multi-agent system;smoothing	Alberto Maran;Nicolas Maudet;Maria Silvia Pini;Francesca Rossi;Kristen Brent Venable	2013			machine learning;artificial intelligence;computer science;voting;interleaving;convergence (routing);phenomenon	AI	-9.897460027585291	-5.121333500625752	107086
0a05201adbbbb3feac622d4a6dff468284c1fa8e	statistics in the pharmaceutical industry		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Tulay Koru-Sengul	2007	Technometrics	10.1198/tech.2007.s487	psychology;alternative medicine;operations research;performance art	Robotics	-15.10411630684395	-5.658240510013125	107316
9b168da61e7d84963df03f4bc6aad04c4343ad5c	glück, nachhaltigkeit und ökonometrie - zur einbeziehung sozialer aspekte in makroökonometrische modelle	model selection;social aspects and economic models;economic model;structural macroeconometric models;strukturelle makrookonometrische modelle;kriterien zur modellauswahl;soziale komponenten in okonomischen modellen	The strong interrelation between economic and social aspects suggests the necessity to include social components into macroeconometric models. Neglecting these aspects will certainly lead to misspecification and consequently unreliabilities in prediction and policy evaluation. This could be the main reason for the minor role such models are today playing in economic policy. In this paper it is advocated to identify such missing social aspects utilizing information on individuals’ happiness and the discussion on sustainability, and to include them into the models. Such “comprehensive” models would certainly be very large. A discussion of existing modelling strategies such as vector autoregressive models (VAR), dynamic stochastic general equilibrium models (DSGE), and structural macroeconometric models (SMM) suggests to use the latter. The well known critique towards SMM—be it in principle (f.i. the Lucas-critique, identification problems) or on more methodological grounds (especially because of the lack of a sufficient data base) are discussed, and a pragmatic procedure emphasizing the contents rather than methodological aspects is proposed.	autoregressive model;database;whole earth 'lectronic link	Joachim Frohn	2011	AStA Wirtschafts- und Sozialstatistisches Archiv	10.1007/s11943-010-0087-6	economic model;mathematics;model selection;statistics	NLP	-13.164363038838239	-0.024438289879970935	107440
2e7cfedbf9ef974214834ed04b16247fc7bff51a	impacts of software agents in ecommerce systems on customer's loyalty and on behavior of potential customers	software agent	Most of the software agents only perform simple product price comparisons; some support the purchase of products or the negotiation over multiple terms of a transaction, such as, e. g., warranties, return policies, delivery times and loan options. Auctions help to find an effective pricing mechanism in electronic commerce. The active technologies enabling customers to purchase efficiently force the merchants to offer high personalized, value-added and complementary services. The techniques such as rule-based matching or collaborative filtering provide contents that are appropriate to the customers preferences or analyze past purchases of other clients. The oneto-one marketing may be especially useful for sophisticated products demanding explanation or to enable cross-selling of other products. The merchants might achieve additional reduction of transaction costs (especially transport, storage and safety measures) using electronic money systems.	collaborative filtering;e-commerce;logic programming;personalization;purchasing;software agent	Jürgen Seitz;Eberhard Stickel;Krzysztof Woda	2000			loyalty business model;marketing;business;customer retention;commerce	ECom	-7.644079456273875	-8.81845102523354	107685
a40c98f64019141a50a39bf12551678dbd3e7c61	evolutionary games and local dynamics	91a22;evolutionary games;local interactions;replicator dynamics	Evolutionary games have been introduced by Maynard Smith and Price [1973] in the early 70’s with the aim to study the stability of populations in time. In the general model, a population consists of finitely many different types that interact randomly with each other, where each interaction leads to fitness payoffs for each of the types involved. Consequently, the population distribution over the different types is changing in time. When assuming that the fraction of a type changes in a Darwinian way proportionally to its current fraction and proportionally to the difference in fitness with average population members, then the population development can be represented by the so-called replicator dynamics, introduced by Taylor and Jonker [1978]. Although many other population dynamics have been studied for evolutionary games (see Hofbauer and Sigmund [1998] or Sandholm [2011] for a review of all kinds of dynamics), we would like to take the replicator dynamics as our starting point in view of its relation to the Darwinian theory. However, we want to switch from a global to a local perspective, because one of the underlying assumptions in the model of Maynard Smith and Price [1973] and in that of Taylor and Jonker	bsd;maynard electronics;population dynamics;randomness	Philippe Uyttendaele;Frank Thuijsman	2015	IGTR	10.1142/S0219198915400162	social dynamics;economics;microeconomics;mathematical economics;replicator equation	ECom	-5.73503076226415	-5.148342103635175	107802
73f60fd89e58438220533fa6e00dab181cbabfd0	the cournot model with coalition forms	allocation rule;coalition structure cournot model coalition forms cooperative game shapley allocation rule;investments;game theory;shapley allocation rule;nash equilibrium;coalition structure;cournot model;bismuth;resource management;data mining;cooperative game;stability;stability cournot model shapley value coalition;shapley value;coalition;games;stability analysis;coalition forms;bismuth stability computational modeling costs production	The Cournot model is extended to game with coalition forms, where the players within coalition are cooperative. A Shapley allocation rule according to the model is given. Furthermore, the stability of Cournot model with coalition structure is discussed, and some relative properties are given.		Shujin Li;Xiaoning Li;Xianglan Qiao	2009	2009 Fifth International Conference on Natural Computation	10.1109/ICNC.2009.516	cournot competition;economics;core;microeconomics;mathematical economics;welfare economics	Robotics	-4.681535650797403	-3.17507340779904	107903
24c46e308b2fd92dd171598fd480cfa7de0e0b70	an application oriented multi-agent based approach to dynamic load/truck planning	less than truckload operations;vehicle routing;agent based modeling;multi agent systems;negotiation	Truck operations decisions for transportation logistics pose challenges especially when loads are lessthan-truckload (LTL). Within a dynamic business environment load planners should consider effective utilization of resources and profitability of their operations. Multi-agent based system provides effective mechanisms for the management of dynamic operations in transportation. The algorithms for transportation domain that are available in the literature are generally focusing on generation of effective solutions for planning/scheduling problems without considering real transportation systems dynamics. Multiagent based design of the load/truck planning problems is supposed to be helpful for integration of algorithms with real-time logistics controlling systems. The cooperative structure of the multi-agent based approach is motivated by real-world third party logistics (3PL) company operations. Negotiation mechanism among the agents is used to handle the dynamic events. The proposed approach is tested via simulation by using LTL data from a 3PL logistics company. The approach generates feasible and profitable decisions under dynamic circumstances by using negotiation/bidding mechanisms. Proposed approach is implemented by using JACKTM, an agent development framework. A multi-agent based dynamic load/truck control system (MABDLCS) is also developed along with this approach. MABDLCS could be used for both testing some transportation scenarios and for real time vehicle/load control purposes. The solutions obtained by using the proposed approach demonstrated that MAS is contributing on problem solution quality while generating real-time schedules. 2015 Elsevier Ltd. All rights reserved.	agent-based model;algorithm;content negotiation;control system;list of 4000 series integrated circuits;load management;logistics;multi-agent system;open reading frame;real-time transcription;schedule (computer science);scheduling (computing);simulation;system dynamics;third-party software component	Adil Baykasoglu;Vahit Kaplanoglu	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.04.011	simulation;computer science;artificial intelligence;vehicle routing problem;multi-agent system;negotiation	AI	-13.372386043424365	-8.839645206163809	108101
fad6574d63cface57557ae0067cc037540d8400d	models and algorithms for genome evolution	genome evolution;authoritative text;comparative genomics;broad spectrum;founding father;international selection;essential collection;david sankoff;computational biology;computer scientist;current status	The BLAST search engine was published and released in 1990. It is a heuristic that uses the idea of a neighborhood to find seed matches that are then extended. This approach came from work that this author was doing to lever these ideas to arrive at a deterministic algorithm with a characterized and superior time complexity. The resulting O(enpow(e/p) logn) expected-time algorithm for finding all e-matches to a string of length p in a text of length n was completed in 1991. The function pow( ) is 0 for = 0 and concave increasing, so the algorithm is truly sublinear in that its running time is O(n) for c < 1 for sufficiently small. This paper reviews the history and the unfolding of the basic concepts, and it attempts to intuitively describe the deeper result whose time complexity, to this author’s knowledge, has yet to be improved upon.	blast;concave function;deterministic algorithm;heuristic;time complexity;unfolding (dsp implementation);web search engine	Cédric Chauve;Nadia El-Mabrouk;Eric Tannier	2013		10.1007/978-1-4471-5298-9	biology;zoology;bioinformatics	ML	-14.136254351927574	-2.708886008924791	108313
c2fc099e5eb3dd637dd39a66f7e0abde60298c40	a new rule for the problem of sharing the revenue from museum passes	axioms;resource allocation;proportional;museum passes;marginality	We present a new rule for the problem of sharing the revenue from museum passes. The rule allocates the revenue from each pass proportionally to the product of the admission fee and the number of total visits (with and without pass) of the museums. We provide a systematic study of the properties of the rule, in comparison with other rules in the literature.		Gustavo Bergantiños;Juan D. Moreno-Ternero	2016	Oper. Res. Lett.	10.1016/j.orl.2016.01.004	simulation;resource allocation;proportionality;mathematics;axiom	ECom	-5.2038967119479524	-3.927109869534046	108869
b6e21ec1255bab20790bc7c8a7e508ec343758a8	the diffusion approximation of stochastic evolutionary game dynamics: mean effective fixation time and the significance of the one-third law	stochastic process;fixation probability;mutation rate;diffusion approximation;nash equilibria;finite population;average fitness;evolutionary game;stationary distribution;one third law;mean effective fixation time	The one-third law introduced by Nowak et al. (Nature 428:646–650, 2004) for the Moran stochastic process has proven to be a robust criterion to predict when weak selection will favor a strategy invading a finite population. In this paper, we investigate fixation probability, mean effective fixation time, and average and expected fitnesses in the diffusion approximation of the stochastic evolutionary game. Our main results show that in two-strategy games with strict Nash equilibria A and B: (i) the one-third law means that, if selection favors strategy A when a single individual is using it initially, then one-third of the opponents one meets before fixation are A-individuals; and (ii) the average fitness of strategy A about the mean effective fixation time is larger than that of strategy B. The analysis reinforces the universal nature of the one-third law as of fundamental importance in models of selection. We also connect risk dominance of strategy A to its larger expected fitness with respect to the stationary distribution of the diffusion approximation that includes a small mutation rate between the two strategies.	approximation	Xiu-Deng Zheng;Ross Cressman;Yi Tao	2011	Dynamic Games and Applications	10.1007/s13235-011-0025-4	stochastic process;mutation rate;econometrics;mathematical optimization;stationary distribution;mathematics;mathematical economics;nash equilibrium;statistics	ECom	-7.289784249682321	-5.628564197263	109027
d547215ae800bb67640b8f469e83d0dcbfc3f86a	integrating human input for decision making with informative bayesian beliefs			information	Robert Lew;Hongsheng Wu;Chen-Hsiang Yu	2017				ML	-18.02256279022185	-4.740811977074907	109154
159b12d2439b42f94ed67d39973715ffa57b1f15	coordination of epidemic control policies: a game theoretic perspective		We consider two neighbouring countries in which a pandemic disease spreads. Countries face a trade-off between the social costs of the epidemic diffusion and the monetary costs in order to avoid the insurgence of pandemics. However, due to migration of people across countries, the treatment efforts by one country generate a positive externality for the neighbouring country. Both countries can negotiate on the healthcare cost that each has to sustain. But, they do so subject to a central authority (CA) who can impose penalties to both countries whenever they cannot reach an agreement. We analyse the outcome of such situation via the Nash bargaining concept. Next we show how the CA should design penalties to i) ensure that revealing the true migration flow data is a self-enforcing behaviour, and to ii) enforce that the NB solution adheres to certain fairness properties.	certificate authority;fairness measure;game theory;naive bayes classifier;nash equilibrium	Lorenzo Maggi;Francesco De Pellegrini;Alexandre Reiffers;P. Jean-Jacques Herings;Eitan Altman	2014	2014 7th International Conference on NETwork Games, COntrol and OPtimization (NetGCoop)		economics;public economics;economy;welfare economics	AI	-5.0885997836631915	-6.400428815757976	109174
c5c15f83f3598b2fc78fb529d67d9de378696006	remarks on counterpossibles	essential properties;counterfactuals;subjunctive conditionals;indicative conditionals;limit assumption;journal article;keywords accessibility relation;accessibility relation;impossible worlds;lewis semantics;non trivial counterpossibles;contingent counterpossibles	Since the publication of David Lewis’ Counterfactuals, the standard line on subjunctive conditionals with impossible antecedents (or counterpossibles) has been that they are vacuously true. That is, a conditional of the form ‘If p were the case, q would be the case’ is trivially true whenever the antecedent, p, is impossible. The primary justification is that Lewis’ semantics best approximates the English subjunctive conditional, and that a vacuous treatment of counterpossibles is a consequence of that very elegant theory. Another justification derives from the classical lore than if an impossibility were true, then anything goes. In this paper we defend non-vacuism, the view that counterpossibles are sometimes non-vacuously true and sometimes non-vacuously false. We do so while retaining a Lewisian semantics, which is to say, the approach we favor does not require us to abandon classical logic or a similarity semantics. It does however require us to countenance impossible worlds. An impossible worlds treatment of counterpossibles is suggested (but not defended) by Lewis (Counterfactuals. Blackwell, Oxford, 1973), and developed by Nolan (Notre Dame J Formal Logic 38:325–527, 1997), Kment (Mind 115:261–310, 2006a: Philos Perspect 20:237–302, 2006b), and Vander Laan (In: Jackson F, Priest G (eds) Lewisian themes. Oxford University Press, Oxford, 2004). We follow this tradition, and develop an account of comparative similarity for impossible worlds.	blackwell (series);centrality;counterfactual conditional;impossible world;jackson;map projection;modal logic;possible world;priest;theme (computing)	Berit Brogaard;Joe Salerno	2012	Synthese	10.1007/s11229-012-0196-6	counterfactual conditional;philosophy;epistemology;mathematics;impossible world;accessibility relation;algorithm	PL	-12.803912792874144	4.1383396915740605	109216
49adde0e013f555e1ae38a8a8627a820280996a0	the new millennium		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Jeffrey L. Ott	2000	Information Systems Security	10.1201/1086/43307.8.4.20000101/31074.1	environmental ethics;medicine	Robotics	-15.120301297982072	-5.682181201790972	109223
ce2bf277b4cb5d8a19d2953ae63f9de453460f27	on the uses of the monotonicity and independence axioms in models of ambiguity aversion	canonical model;ambiguity aversion;monotonicity independence ambiguity	This paper suggests an alternative axiomatization of two canonical models of ambiguity aversion. Instead of relaxing the independence axiom to accommodate uncertainty aversion, we impose independence on constant acts only. Maxmin and variational preferences are characterized by different strengthenings of the monotonicity axiom.	risk aversion	Leandro Nascimento;Gil Riella	2010	Mathematical Social Sciences	10.1016/j.mathsocsci.2010.01.003	financial economics;axiom independence;economics;ambiguity aversion;canonical model;mathematics;mathematical economics;welfare economics	ECom	-7.213174833042223	-0.8580697835886453	109627
e08f0bc3d545d7f9a739440e3779c669e7446f48	multi-agent bidding mechanism with contract log learning functionality		This paper addresses the agent-based bidding mechanism under trading actions from supplier sites to demand sites. Bidding includes unit price, amount, and storage cost. This trading environment assumes to be completely competitive, which means an agent cannot detect the competitive agent information. To increase the success rate of bidding, the agent must learn its bidding strategy from the past trading log. Our agent estimates the appropriate bidding price from the past “success bids” and “failure bids” by using statistical analysis. Experimental results shows an agent with such learning functionality increases its rate of “success bids” by 45.6%, compared to the agent without such functionality.		Kazuhiro Abe;Masanori Akiyoshi;Norihisa Komoda	2012		10.1007/978-3-642-28765-7_25	data mining;computer science;unit price;bidding;negotiation;microeconomics	ECom	-8.358107240168586	-9.007227175403905	109845
c135a74d387d49424d0c81657a57d39f7b7ec43f	the gamma-core and coalition formation	game theory;nash equilibrium;strategic games;repeated game;coalition formation;core;strategic game	This paper reinterprets the γ -core (Chander and Tulkens in Int Tax Pub Financ 2:279–293, 1995; in Int J Game Theory 26:379–401, 1997) and justifies it as well as its prediction that the efficient coalition structure is stable in terms of the coalition formation theory. The problem of coalition formation is formulated as an infinitely repeated game in which the players must choose whether to cooperate or not. It is shown that a certain equilibrium of this game corresponds to the γ -core assumption that when a coalition forms the remaining players form singletons, and that the grand coalition is an equilibrium coalition structure. JEL Classification numbers C71, C72, D62	bloch sphere;game theory;geo-imputation;robustness (computer science);supratik chakraborty;symmetric multiprocessing	Parkash Chander	2007	Int. J. Game Theory	10.1007/s00182-006-0067-9	non-cooperative game;implementation theory;game theory;core;example of a game without a value;economics;simultaneous game;repeated game;core;microeconomics;normal-form game;mathematical economics;sequential game;welfare economics;equilibrium selection;nash equilibrium	AI	-5.824946983795324	-2.1078466470118227	109942
db2b163146acc99cfb0bc624ad557b0b34f0757f	language acquisition in the mdl framework	language acquisition;minimum description length;statistical model	The Minimum Description Length (MDL) principle provides guidance to the fundamental question of determining what a given set of observed data tells us about the underlying data generating machinery. Hence, in the broadest sense the MDL principle relates to the central question of all science, although its most useful applications have been to the more practical problem of tting statistical models to data. In this article, we review the MDL principle and demonstrate how it may be pro tably applied to the logical problem of language acquisition.	mdl (programming language);minimum description length;statistical model	Jorma Rissanen;Eric Sven Ristad	1992			language acquisition;natural language processing;minimum description length;artificial intelligence;statistical model;computer science	ML	-13.790333685416181	1.578145574784589	110213
4cdb8ce2710b808c519826106833a3ec31427bf5	state-of-the-art cryogenic machining and processing	cryogenic machining;cryogenic processing;machinability;cryogenic cooling	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Alireza Shokrani;V. G. Dhokia;Patricia Muñoz-Escalona;Stephen T. Newman	2013	Int. J. Computer Integrated Manufacturing	10.1080/0951192X.2012.749531	engineering drawing;metallurgy;manufacturing engineering	Robotics	-15.30777040851778	-4.630902596878018	110260
161c904ac6ba9e2867c92998537c40a1154006ea	grammatical inference as a tool for constructing self-learning syntactic pattern recognition-based agents	real time processing;syntactic pattern recognition;intelligent system;grammatical inference	Syntactic pattern recognition-based agents have been proven to be a useful tool for constructing real-time process control intelligent systems. In the paper the problem of self-learning schemes in the agents is discussed. Learning capabilities are very important if practical applications of the agents are considered, since the agents should be able to accumulate knowledge about the environment and flexible react to the changes in the environment. As it is shown in the paper, the learning scheme in the agents can be based on a suitable grammatical inference algorithm.	grammar induction;syntactic pattern recognition	Janusz Jurek	2008		10.1007/978-3-540-69389-5_79	natural language processing;grammar induction;computer science;artificial intelligence;machine learning	Logic	-18.822790587287678	-5.387226061712165	110375
451cf6650c01f14b2194019667207089de73fbe1	on a class of computationally efficient feature selection criteria	trust;competence;benevolence;브랜드몰입;판매원 신뢰;communication quality;선의;feature selection;능력;커뮤니케이션 품질;brand commitment	본 연구는 교환관계에서 판매원에 대한 신뢰 특성(능력, 선의)이 판매원에 대한 소비자#N#들의 신뢰에 미치는 영향력을 검증하였다. 다음으로 소비자 신뢰를 결정하는 주요 변인#N#으로 고려되어온 커뮤니케이션을 질적인 측면에서 어떤 영향을 미치는지를 밝히기 위해#N#커뮤니케이션 품질이라는 개념을 도입하였다. 마지막으로 커뮤니케이션 품질과 판매원#N#에 대한 소비자들의 신뢰가 브랜드몰입에 미치는 영향력을 검증하였다. 이러한 연구목#N#적을 해결하기 위해 서울 및 수도권에 거주하는 500명의 소비자들을 대상으로 SPA 브랜#N#드 구매상황을 가정해서 온라인 서베이를 실시하였다. 분석결과, 커뮤니케이션 품질과#N#판매원에 대한 신뢰 특성은 판매원에 대한 소비자들의 신뢰를 결정하는 변인인 것으로#N#밝혀졌으며, 커뮤니케이션 품질과 판매원에 대한 소비자들의 신뢰는 브랜드몰입에 까지#N#긍정적인 영향을 미치는 변인이었다.	algorithmic efficiency;feature selection	C. H. Chen	1975	Pattern Recognition	10.1016/0031-3203(75)90018-7	computer science;machine learning;competence;trustworthy computing;feature selection	Vision	-12.405808477601086	-6.647692209119363	110515
24e6c5b22a9f20db28d0cea9b3c19e684a27f010	a semantics for speech acts	speech acts;distributed computing;formal semantics;distributed artificial intelligence;natural language processing;speech act theory;electronic data interchange	Speech act theory is important not only in Linguistics, but also in Computer Science. It has applications in Distributed Computing, Distributed Artificial Intelligence, Natural Language Processing, and Electronic Data Interchange protocols. While much research into speech acts has been done, one aspect of them that has largely been ignored is their semantics, i.e. their conditions of satisfaction. A formal semantics for speech acts is motivated and presented here that relates their satisfaction to the intentions, know-how, and actions of the participating agents. This makes it possible to state several potentially useful constraints on communication and provides a basis for checking their consistency.	complementarity (physics);computation;computational linguistics;computer science;debugging;distributed artificial intelligence;distributed computing;electronic data interchange;natural language processing;semantics (computer science);theory	Munindar P. Singh	1993	Annals of Mathematics and Artificial Intelligence	10.1007/BF02451549	natural language processing;formal semantics;computer science;artificial intelligence;machine learning;electronic data interchange;formal semantics;linguistics;programming language;computational semantics	AI	-16.74918847764303	3.7090027181567056	110560
a8de5e6207ec524aa443569ace161cb2559d68c4	wearlock: unlocking your phone via acoustics using smartwatch		Smartphone lock screens are implemented to reduce the risk of data loss or compromise given the fact that increasing amount of person data are accessible on smartphones nowadays. Unfortunately, many smartphone users abandon lock screens due to the inconvenience of unlocking their phones many times a day. With the wide adoption of wearables, token-based approaches have gained popularity in simplifying unlocking and retaining security at the same time. To this end, we propose to take advantage of the smartwatch for easy smartphone unlocking. In this paper, we have designed WearLock, a system that uses acoustic tones as tokens to automate the unlocking securely. We build a sub-channel selection and an adaptive modulation in the acoustic modem to maximize unlocking success rate against ambient noise only when those two devices are nearby. We leverage the motion sensor on the smartwatch to reduce the unlock frequency. We offload smartwatch tasks to the smartphone to speed up computation and save energy. We have implemented the WearLock prototype and conducted extensive evaluations. Results achieved a low average bit error rate (BER) as 8% in various experiments. Compared to traditional manual personal identification numbers (PINs) entry, WearLock achieves at least 18% unlock speedup without any manual effort.	acoustic cryptanalysis;bit error rate;computation;experiment;lock screen;modem;modulation;motion detector;personal identification number;prototype;sim lock;smartphone;smartwatch;speedup;wearable computer;x.690	Shanhe Yi;Zhengrui Qin;Nancy Carter;Qun Li	2017	2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)	10.1109/ICDCS.2017.183	lock (computer science);distributed computing;computer network;wireless sensor network;wearable computer;smartwatch;computer science;link adaptation;speedup;embedded system;security token;compromise	Mobile	-15.50984053307234	-2.5606073946054058	110785
cd2eddb3a6d9d265c8887088f0bd523d6bb7498a	adapting a kidney exchange algorithm to align with human values		The efficient allocation of limited resources is a classical problem in economics and computer science. In kidney exchanges, a central market maker allocates living kidney donors to patients in need of an organ. Patients and donors in kidney exchanges are prioritized using ad-hoc weights decided on by committee and then fed into an allocation algorithm that determines who get what---and who does not. In this paper, we provide an end-to-end methodology for estimating weights of individual participant profiles in a kidney exchange. We first elicit from human subjects a list of patient attributes they consider acceptable for the purpose of prioritizing patients (e.g., medical characteristics, lifestyle choices, and so on). Then, we ask subjects comparison queries between patient profiles and estimate weights in a principled way from their responses. We show how to use these weights in kidney exchange market clearing algorithms. We then evaluate the impact of the weights in simulations and find that the precise numerical values of the weights we computed matter little, other than the ordering of profiles that they imply. However, compared to not prioritizing patients at all, there is a significant effect, with certain classes of patients being (de)prioritized based on the human-elicited value judgments.		Rachel Freedman;Jana Schaich Borg;Walter Sinnott-Armstrong;John P. Dickerson;Vincent Conitzer	2018		10.1145/3278721.3278727	artificial intelligence;machine learning;computer science;ask price;market clearing;algorithm;market maker	AI	-8.66353382719305	-6.475225987292739	110920
5305b387402d776ee88ecf6d2045f092ec1d5221	programs as the language of science		Currently it is widely accepted that the language of science is mathematics. This book explores an alternative idea where the future of science is based on the language of algorithms and programs. How such a language can actually be implemented in the sciences is outlined in some detail. We start by constructing a simple formal system where statements are represented as programs and inference is based on computability as opposed to the classical notion of truth value assignments. The focus is on theories where the intrinsic properties and dynamic state of real world objects can be defined in terms of information and subject to laws based on simple deterministic rules and finite state arithmetic. Such models, it is argued, not only offer alternative simulation tools, as opposed to those based on discrete approximations of conventional continuum theories, but in themselves can be regarded as a language that describes the physical laws at a fundamental level. This book does not examine any specific application in detail but rather attempts to lay down a foundation for the validation of such theories by employing the inference scheme based on computability logic.	algorithm;approximation;computability logic;formal system;simulation;theory;triune continuum paradigm	Garry Pantelis	2018	CoRR			PL	-14.856599202946926	2.9753877759941054	111120
5547877cae4a7ecddcf59541d70cac54b8aa3bde	the object allocation problem with random priorities		The existing priority-based object allocation literature restricts objectsu0027 priorities to be deterministic. However, agents might be probabilistically prioritized ex-ante, for instance, through a non-uniform tie-breaking rule. This paper generalizes the deterministic setting by allowing priorities to be random. In this probabilistic environment, we first introduce a fairness notion called claimwise stability in the spirit of the usual stability of Gale and Shapley (1962). We show that the natural generalization of the deferred acceptance mechanism (Gale and Shapley, 1962), so called probabilistic deferred acceptance mechanism, is claimwise stable, but with the downside that it is stochastically dominated by another claimwise stable rule. We then introduce a new mechanism called the constrained probabilistic serial, which is built on the probabilistic serial mechanism of Bogomolnaia and Moulin (2001). It is both claimwise stable and constrained sd-efficient. The paper then systematically compares the probabilistic deferred acceptance and constrained probabilistic serial mechanisms in terms of their strategic and fairness properties.		Mustafa Oguz Afacan	2018	Games and Economic Behavior	10.1016/j.geb.2018.03.010	mathematical economics;welfare economics;probabilistic logic;economics	ECom	-5.190730483332227	-3.4413304710625456	111214
f23c01bc974df6607cb1b231d3f1e5fe59b239c8	primaries with strategic voters: trading off electability and ideology		This paper presents a spatial model of primary election to analyze strategic voting and its effect on the policy outcome. Primary voters care for the electability of the candidates as well as their offered policies. The trade off between these two factors might make the preferences of the primary voters non-single-peaked. I show the median voter is still decisive when the preferences are quadratic. Moreover, I use comparative statics and numerical analysis to evaluate the conditions under which the position of the Condorcet winner in the primary election shifts toward the center. Among the conditions that contribute to such a shift are radical policies by the incumbent, public opinion shift toward the incumbent party, and accurate information about the population median. Copyright Springer-Verlag Berlin Heidelberg 2015		Mohammad Reza Mirhosseini	2015	Social Choice and Welfare	10.1007/s00355-014-0845-4	public relations;blanket primary;economics;public economics;microeconomics;mathematical economics;public administration;law	ECom	-5.303094151459231	-5.838611502698675	111344
5cadd163251ff781f1eea0def5eafce7de34a3e0	sequential decision aggregation with social pressure		This paper proposes and characterizes a sequential decision aggregation system consisting of agents performing binary sequential hypothesis testing, and a fusion center which collects the individual decisions and reaches the global decision according to some threshold rule. Individual decision makers’ behaviors in the system are influenced by other decision makers, through a model for social pressure; our notion of social pressure is proportional to the ratio of individual decision makers who have already made the decisions. For our proposed model, we obtain the following results: First, we derive a recursive expression for the probabilities of making the correct and wrong global decisions as a function of time, system size, and the global decision threshold. The expression is based on the individual decision makers’ decision probabilities and does not rely on the specific individual decision making policy. Second, we discuss two specific threshold rules: the fastest rule and the majority rule. By means of a mean-field analysis, we relate the asymptotic performance of the fusion center, as the system size tends to infinity, to the individual decision makers’ decision probability sequence. In addition to theoretical analysis, simulation work is conducted to discuss the speed/accuracy tradeoffs for different threshold rules.	algorithm;fastest;mathematical optimization;network topology;recursion;rule 110;simulation	Wenjun Mei;Francesco Bullo	2016	MCSS	10.1007/s00498-016-0174-5	optimal decision;decision theory;decision analysis;decision field theory;decision tree;data mining;decision rule;admissible decision rule;management science;evidential reasoning approach;evidential decision theory;weighted sum model;business decision mapping	ML	-10.686774862123821	-4.513872244307626	111457
bea5bbbd89c1bc61d49bf2212c1e3df3ac1082fc	the intrinsic quantum nature of nash equilibrium mixtures	humanidades;filosofia etica	In classical game theory the idea that players randomize between their actions according to a particular optimal probability distribution has always been viewed as puzzling. In this paper, we establish a fundamental connection between n-person normal form games and quantum mechanics (QM), which eliminates the conceptual problems of these random strategies. While the two theories have been regarded as distinct, our main theorem proves that if we do not give any other piece of information to a player in a game, than the payoff matrix—the axiom of “no-supplementary data” holds—then the state of mind of a rational player is algebraically isomorphic to a pure quantum state. The“no supplementary data”axiom is captured in a Lukasiewicz’s three-valued Kripke semantics wherein statements about whether a strategy or a belief of a player is rational are initially indeterminate i.e. neither true, nor false. As a corollary, we show that in a mixed Nash equilibrium, the knowledge structure of a player implies that probabilities must verify the standard “Born rule” postulate of QM. The puzzling “indifference condition” wherein each player must be rationally indifferent between all the pure actions of the support of his equilibrium strategy is resolved by his state of mind being described by a “quantum superposition” prior a player is asked to make a definite choice in a “measurement”. Finally, these results demonstrate that there is an intrinsic limitation to the predictions of game theory, on a par with the “irreducible randomness” of quantum physics.	contingency (philosophy);game theory;gist;indeterminacy in concurrent computation;nash equilibrium;quantum	Yohan Pelosse	2016	J. Philosophical Logic	10.1007/s10992-015-9349-7	bayesian game;epsilon-equilibrium;philosophy;epistemology;computer science;strategic dominance;pure mathematics;repeated game;mathematics;strategy;correlated equilibrium;mathematical economics;equilibrium selection;nash equilibrium	AI	-13.11602020918005	2.3033092021433945	111498
234e004075097584e9b084b7e2d1ccc8a39d2503	achieving cooperation with many prisoners in the nipd	cross entropy;iterated prisoners 39;game theory;cooperation;reinforcement learning;dilemma;many prisoners;ipd	This paper discusses an empirical investigation into the N-person's iterated prisoners' dilemma NIPD, a standard problem from game theory. We use both the cross entropy method and reinforcement learning and achieve cooperation with much greater sizes of population than we have previously been able to do with genetic algorithms and artificial immune systems. Our experimental results give some insight into the circumstances where cooperation might develop.		Juan Enrique Agudo;Colin Fyfe	2016	IJCSE	10.1504/IJCSE.2016.074560	integrated project delivery;game theory;actuarial science;computer science;artificial intelligence;cross entropy;reinforcement learning;cooperation	Robotics	-9.429862531256173	-9.223411750789333	111525
c1f259c2370f817a74e361fb85ecbdb4539fbb67	incorporating rhetorical and plausible reasoning in a system for simulating argumentation	argumentation;rhetoric;computer model;simulation;collaboration;plausibility;formal logic;meeting	The article introduces argumentation theory, some examples of computational models of argumentation, some application examples, considers the significance and problems of argumentation systems, and outlines the significance and difficulties of the field. Also, the article describes a system which used rhetorical reasoning rules such as fairness, reciprocity, and deterrence which was used to simulate the text of a debate. The text was modelled using modern argumentation theory, and this model was used to build the system. The article discusses the system with regard to several aspects: its ability to deal with meaningful contradiction such as claim X supporting claim Y, yet claim Y attacking claim X; recursive arguments; inconsistency maintenance; expressiveness; encapsulation, the use of definitions as the basis for rules, and making generalisations using taxonomies. The article concludes with a discussion of domain dependence, rule plausibility, and some comparisons with formal logic.	simulation	Masoud Saeedi;John A. A. Sillince	1999	Knowl.-Based Syst.	10.1016/S0950-7051(99)00010-6	computer simulation;rhetoric;computer science;artificial intelligence;machine learning;probabilistic argumentation;logic;algorithm;collaboration	AI	-15.39031579108562	2.9147531724755247	111726
e7ebecb22eb94319ff14036da9bca5a091f51679	need for data processing naturally leads to fuzzy logic (and neural networks): fuzzy beyond experts and beyond probabilities		Fuzzy techniques have been originally designed to describe imprecise (“fuzzy”) expert knowledge. Somewhat surprisingly, fuzzy techniques have also been successfully used in situations without expert knowledge, when all we have is data. In this paper, we explain this surprising phenomenon by showing that the need for optimal processing of data (including crisp data) naturally leads to fuzzy and neural data processing techniques. This result shows the potential of fuzzy data processing. To maximally utilize this potential, we need to provide an operational meaning of the corresponding fuzzy degrees. We show that such a meaning can be extracted from the above justification of fuzzy techniques. It turns out that, in contrast to probabilistic uncertainty, the natural operational meaning of fuzzy degrees is indirect – similarly to the operational meaning of geometry and physics in General Relativity.	fuzzy concept;fuzzy logic;neural networks;numerical relativity	Vladik Kreinovich;Hung T. Nguyen;Songsak Sriboonchitta	2016	Int. J. Intell. Syst.	10.1002/int.21785	fuzzy electronics;adaptive neuro fuzzy inference system;artificial intelligence;neuro-fuzzy;machine learning;data mining	ML	-18.355819291657163	1.2609978949813396	111866
040c3088bd4bb76fb56618ff66501f1291fd512d	informative narrowcasting with consumer search	consumer search;electronic mail;advertising data processing;electronic markets;advertising internet consumer electronics information technology cost function data mining target tracking collaboration information filtering information filters;internet;search cost;market participation;electronic mail advertising data processing internet;consumer reservation utility consumer addressability electronic markets focused promotional messages targeted advertising approach narrowcasting market participation decision optimal targeting decision nontrivial search costs	Improved consumer addressability in electronic markets allows vendors to send focused promotional messages to specific customers, facilitating a targeted advertising approach that we call “narrowcasting”. We characterize a rational consumer’s market participation decision and identify a narrowcasting firm’s optimal targeting decision when buyers face nontrivial search costs. In the special case where consumer reservation utility is relatively low and search cost is relatively high, consumers will neither search nor purchase blindly without search, and the market breaks down. Our main contribution here is to show that narrowcasting can help a seller revitalize this market by replacing learning by consumers with learning and targeting by the seller.	electronic markets;information	Rajiv M. Dewan;Bing Jing;Abraham Seidmann	2002		10.1109/HICSS.2002.994263	the internet;marketing;search cost;advertising;search advertising;law;commerce	ECom	-4.577331335372758	-8.259903040812267	111951
1326b4802dabbb4d1f4d109a5c507aa354404716	using multiattribute utility theory to avoid bad outcomes by focusing on the best systems in ranking and selection	jos;multi objective;simulation;conceptual modellling;journal of simulation;output analysis;design and analysis of simulation experimetns;input modelling;decision analysis;ranking and selection;applications of simulation and modelling;simulation visualisation;simulation tutorials;parallel and distributed simulation;discrete event simulation	When making decisions under uncertainty, it seems natural to use constraints on performance to avoid the selection of a particularly bad system. However that intuition has been shown to impair good recommendations as demonstrated by some well-known results in the stochastic optimization literature. Our work on multiattribute ranking and selection procedures demonstrates that Pareto and constraint-based approaches could be used as part of a successful decision process; but a tradeoff-based approach, like multiattribute utility theory, is required to identify the true best system in all but a few special cases. We show that there is no guaranteed strategic equivalence between utility theory and constraint-based approaches when constraints on the means of the performance measures are used in the latter. Hence, a choice must be made as to which is appropriate. In this paper, we extend well-known results in the decision analysis literature to ranking and selection. Journal of Simulation (2015) 9(3), 238–248. doi:10.1057/jos.2014.34; published online 27 February 2015	decision analysis;mathematical optimization;pareto efficiency;simulation;stochastic optimization;turing completeness;utility	Jason R. W. Merrick;Douglas J. Morrice;John C. Butler	2015	J. Simulation	10.1057/jos.2014.34	simulation;decision analysis;computer science;discrete event simulation;machine learning;management science	AI	-9.652873213335457	-2.8267447995789277	112152
a427ff2f0101ea352746a89fd9984aee546e69b3	managing the toughest transition: part one	profitability;team work;customer relations;it management	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Stewart L. Stokes	2003	IS Management	10.1201/1078/43204.20.2.20030301/41464.2		Robotics	-15.311049233119691	-5.341455241351234	112159
ce2da524f823e7ca109e305a9626c4e453c8f65c	agent-based dimensioning to support the planning of intra-logistics systems	complexity theory;waste materials;time measurement;layout;multi agent systems;logistics;planning	The increasing cost pressure on Intra-Logistics systems due to the global competition in Intra-Logistics systems motivates planners to decrease planning durations in order to stay cost efficient. Conversely, the rough planning of Intra- Logistics systems is a time consuming, complex task, whose progress is primarily driven by the experience of the planning team. Consequently, the planning is opaque. In order to reduce the planning duration and to systemize the planning this paper presents a rule-based concept for an agent-based assistance system to support the planner during the rough planning of Intra-Logistics systems. The assistance system provides an overview of possible solutions regarding to specified requirements by generating and evaluating all possible solution alternatives.	agent-based model;assistive technology;cost efficiency;logic programming;logistics;requirement	Theresa Beyer;Peter Göhner;Ramin Yousefifar;Karl-Heinz Wehking	2016	2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA)	10.1109/ETFA.2016.7733647	planning;layout;logistics;simulation;computer science;systems engineering;engineering;artificial intelligence;multi-agent system;material requirements planning;time	Robotics	-13.790240190533407	-9.028309784299749	112322
a8cf2895c1780d402ace84df75cd167222836ab1	using multiagent negotiation to model water resources systems operations		The operations of water resources infrastructures, such as dams and diversions, often involve multiple conflicting interests and stakeholders. Among the approaches that have been proposed to design optimal operating policies for these systems, those based on agents have recently attracted an increasing attention. The different stakeholders are represented as different agents and their interactions are usually modeled as distributed constraint optimization problems. Those few works that have attempted to model the interactions between stakeholders as negotiations present some significant limitations, like the necessity for each agent to know the preferences of all other agents. To overcome this drawback, in this paper we contribute a general monotonic concession protocol that allows the stakeholders-agents of a regulated lake to periodically reach agreements on the amount of water to release daily, trying to control lake floods and to supply water to agricultural districts downstream. In particular, we study two specific instances of the general protocol according to their ability to converge, reach Pareto optimal agreements, limit complexity, and show good experimental performance.	agent-based model	Francesco Amigoni;Andrea Castelletti;Paolo Gazzotti;Matteo Giuliani;Emanuele Mason	2016		10.1007/978-3-319-46882-2_4	knowledge management;management science	NLP	-6.62485752728202	-8.356904316333582	112397
fc2fa421935e7443fa319d4dd64caa5dcaa88341	a user interface for a knowledge-based planning and scheduling system	knowledge bases artificial intelligence;user interfaces aerospace computing expert systems scheduling;expert systems;user interface;planning and scheduling;mission planning;nasa programs;aerospace computing;scheduling;lisp programming language;user interfaces;system usability assessment aerospace computing expert system user interface empress expert mission planning and replanning scheduling system space shuttle zetalisp 3600 series symbolics lisp machine;user interfaces payloads scheduling space shuttles usability nasa knowledge based systems space missions monitoring resource management;human factors engineering;knowledge base	The objective of EMPRESS (expert mission planning and replanning scheduling system) is to support the planning and scheduling required to prepare science and application payloads for flight aboard the US Space Shuttle. EMPRESS was designed and implemented in Zetalisp on a 3600 series Symbolics LISP machine. Initially, EMPRESS was built as a concept demonstration system. The system has since been modified and expanded to ensure that the data have integrity. Issues underlying the design and development of the EMPRESS-I interface, results from a system usability assessment, and consequent modifications are described. >	automated planning and scheduling;scheduling (computing);user interface	Alice M. Mulvehill	1988	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.17369	knowledge base;real-time computing;simulation;computer science;artificial intelligence;user interface	Robotics	-19.115344525737036	-7.32201429001511	112773
1eddc66556a61f10e97ce5fd6cb74d1918c1d5d9	learning control knowledge for forward search planning	learning control;forward search	A number of today’s state-of-the-art planners are based on forward state-space search. The impressive performance can be attributed to progress in computing domain independent heuristics that perform well across many domains. However, it is easy to find domains where such heuristics provide poor guidance, leading to planning failure. Motivated by such failures, the focus of this paper is to investigate mechanisms for learning domain-specific knowledge to better control forward search in a given domain. While there has been a large body of work on inductive learning of control knowledge for AI planning, there is a void of work aimed at forward-state-space search. One reason for this may be that it is challenging to specify a knowledge representation for compactly representing important concepts across a wide range of domains. One of the main contributions of this work is to introduce a novel feature space for representing such control knowledge. The key idea is to define features in terms of information computed via relaxed plan extraction, which has been a major source of success for non-learning planners. This gives a new way of leveraging relaxed planning techniques in the context of learning. Using this feature space, we describe three forms of control knowledge—reactive policies (decision list rules and measures of progress) and linear heuristics—and show how to learn them and incorporate them into forward state-space search. Our empirical results show that our approaches are able to surpass state-of-the-art non-learning planners across a wide range of planning competition domains.	automated planning and scheduling;decision list;feature vector;heuristic (computer science);inductive reasoning;knowledge representation and reasoning;linear logic;nist hash function competition;state space search	Sung Wook Yoon;Alan Fern;Robert Givan	2008	Journal of Machine Learning Research	10.1145/1390681.1390705	computer science;knowledge management;artificial intelligence;machine learning	AI	-18.079480043132907	-6.659044202254394	112939
faa5a4ae8276cb7c97e9d4a8bfb6f33899128bbd	support-vector-based emergent self-organising approach for emotional understanding	performance measure;self organisation map;connectionist models;support vector;emotion;social psychology;computer analysis;support vector machine;self organising map	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	connectionism;embedded system;emergence;francis;map;mental state;primary source;profiling (computer programming);self-organization	Yok-Yen Nguwi;Siu-Yeung Cho	2010	Connect. Sci.	10.1080/09540091.2010.521936	support vector machine;computer science;artificial intelligence;machine learning	Robotics	-14.92685009756721	-6.355150055386157	113031
ea16e7c22689d0781682cd05b0308465ba8a1903	learning hierarchical task network domains from partially observed plan traces	weighted max sat;hierarchical task networks;intermediate state;max sat;hierarchical task network htn planning;action model learning;htn planning;action modeling;learning htns;期刊论文;partial decomposition	Hierarchical Task Network (HTN) planning is an effective yet knowledge intensive problem-solving technique. It requires humans to encode knowledge in the form of methods and action models. Methods describe how to decompose tasks into subtasks and the preconditions under which those methods are applicable whereas action models describe how actions change the world. Encoding such knowledge is a difficult and time-consuming process, even for domain experts. In this paper, we propose a new learning algorithm, called HTNLearn, to help acquire HTN methods and action models. HTNLearn receives as input a collection of plan traces with partially annotated intermediate state information, and a set of annotated tasks that specify the conditions before and after the tasks’ completion. In addition, plan traces are annotated with potentially empty partial decomposition trees that record the processes of decomposing tasks to subtasks. HTNLearn outputs are a collection of methods and action models. HTNLearn first encodes constraints about the methods and action models as a constraint satisfaction problem, and then solves the problem using a weighted MAX-SAT solver. HTNLearn can learn methods and action models simultaneously from partially observed plan traces (i.e., plan traces where the intermediate states are partially observable). We test HTNLearn in several HTN domains. The experimental results show that our algorithm HTNLearn is both effective and efficient.	algorithm;boolean satisfiability problem;codi;constraint satisfaction problem;encode;hierarchical task network;max;maximum satisfiability problem;partially observable system;precondition;problem solving;solver;subject-matter expert;tracing (software)	Hankui Zhuo;Hector Muñoz-Avila;Qiang Yang	2014	Artif. Intell.	10.1016/j.artint.2014.04.003	computer science;artificial intelligence;maximum satisfiability problem;machine learning;data mining;hierarchical task network	AI	-18.445999096481483	-6.022033864077697	113141
6a4a673d292d1d150759215593fe7b9be1b88776	atomic cournotian traders may be walrasian	cournot nash equilibrium;walras equilibrium;core;mixed exchange economies;working paper	In a bilateral oligopoly, with large traders, represented as atoms, and small traders, represented by an atomless part, when is there a non-empty intersection between the sets of Walras and Cournot-Nash allocations? Using a two commodity version of the Shapley window model, we show that a necessary and sufficient condition for a CournotNash allocation to be a Walras allocation is that all atoms demand a null amount of one of the two commodities. We provide two examples which show that this characterization holds non-vacuously. When our condition fails to hold, we also confirm, through some examples, the result obtained by Okuno, Postlewaite, and Roberts (1980): small traders always have a negligible influence on prices, while the large traders keep their strategic power even when their behavior turns out to be Walrasian in the cooperative framework considered by Gabszewicz and Mertens (1971) and Shitovitz (1973). Journal of Economic Literature Classification Numbers: C71, C72, D51.	atom;bilateral filter;bilateral sound;nash equilibrium;traders;trading room;turing completeness	Giulio Codognato;Sayantan Ghosal;Simone Tonin	2015	J. Economic Theory	10.1016/j.jet.2015.05.005	core;economics;macroeconomics;microeconomics;mathematical economics;market economy	ECom	-5.68432906612617	-2.5864502767178164	113197
ac56a4997f39590a11adcebc5d568d6e0c27c712	recycling privileged learning and distribution matching for fairness		Equipping machine learning models with ethical and legal constraints is a serious issue; without this, the future of machine learning is at risk. This paper takes a step forward in this direction and focuses on ensuring machine learning models deliver fair decisions. In legal scholarships, the notion of fairness itself is evolving and multi-faceted. We set an overarching goal to develop a unified machine learning framework that is able to handle any definitions of fairness, their combinations, and also new definitions that might be stipulated in the future. To achieve our goal, we recycle two well-established machine learning techniques, privileged learning and distribution matching, and harmonize them for satisfying multi-faceted fairness definitions. We consider protected characteristics such as race and gender as privileged information that is available at training but not at test time; this accelerates model training and delivers fairness through unawareness. Further, we cast demographic parity, equalized odds, and equality of opportunity as a classical two-sample problem of conditional distributions, which can be solved in a general form by using distance measures in Hilbert Space. We show several existing models are special cases of ours. Finally, we advocate returning the Pareto frontier of multi-objective minimization of error and unfairness in predictions. This will facilitate decision makers to select an operating point and to be accountable for it.	faceted classification;fairness measure;hilbert space;machine learning;operating point;pareto efficiency	Novi Quadrianto;Viktoriia Sharmanska	2017			operating point;computer science;machine learning;distance measures;hilbert space;artificial intelligence;pareto principle;parity (mathematics);conditional probability distribution;odds	ML	-8.60950039613117	-4.7024266896652245	113236
8b49aded7fdc4827159f0d7f0b46e79023ef66c1	public good and private good valuation for waiting time reduction: a laboratory study	conditional cooperation;relative performance evaluation;nash equilibrium;delegation;lindahl equilibrium;waiting time;cartel stability;voluntary contributions;experiments;laboratory experiment;private provision of public goods;public good;willingness to pay;second price auction	In a laboratory experiment with salient rewards, subjects were endowed with money and waiting time. Preferences for waiting time were elicited both as a private good by means of a series of second price auctions, i.e., in a non-induced values framework, and as a public good in the scope of a voluntary contribution game. Our data supports the Lindahl equilibrium as a descriptive model of behavior. Private goods valuations turned out to be good predictors of public goods valuations. Furthermore, positive expectations about other subjects’ contributions lead to a significant increase of one’s own contribution. We did not find evidence of waiting time being discounted. JEL classification: H41, D61, C90	money;nash equilibrium;value (ethics)	Tibor Neugebauer;Stefan Traub	2012	Social Choice and Welfare	10.1007/s00355-011-0544-3	delegation;vickrey auction;actuarial science;public good;economics;public economics;microeconomics;nash equilibrium	ECom	-5.732612286075723	-6.15127945344205	113327
913c5d5c9bf2c45f905cd0d70c0f16837d9da34e	a new probabilistic scheme for information retrieval in hypertext	information retrieval;indexing terms;probabilistic model;document frequency;test collection	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.		Jacques Savoy	1995	The New Review of Hypermedia and Multimedia	10.1080/13614569508914662	statistical model;index term;relevance;computer science;data mining;okapi bm25;world wide web;vector space model;information retrieval;divergence-from-randomness model	Robotics	-14.911311262838025	-5.481366801309368	113493
b19d26f918848da4889dd230cb63735e547511c7	using petri net plans for modeling uav-ugv cooperative landing	uav;ugv;cooperation;petri nets plans	Aerial and ground vehicles working in corporation are crucial assets for several real world applications, ranging from search and rescue to logistics. In this paper, we consider a cooperative landing task problem, where an unmanned aerial vehicle (UAV) must land on an unmanned ground vehicle (UGV) while such ground vehicle is moving in the environment to execute its own mission. To solve this challenging problem we consider the Petri Net Plans (PNPs) framework, an advanced planning specification framework that allows to design and monitor multi robot plans. Specifically, we use the PNP framework to effectively use different controllers in different conditions and to monitor the evolution of the system during mission execution so that the best controller is always used even in face of unexpected situations. Empirical simulation results show that our system can properly monitor the joint mission carried out by the UAV/UGV team, hence confirming that the use of a formal planning language significantly helps in the design of such complex scenarios.	aerial photography;high-level programming language;interrupt;legacy plug and play;logistics;petri net;risk management;robot;simulation;specification language;unmanned aerial vehicle	Andrea Bertolaso;Masoume M. Raeissi;Alessandro Farinelli;Riccardo Muradore	2016		10.3233/978-1-61499-672-9-1720	simulation;cooperation	Robotics	-18.810797538349878	-8.590325910072538	113887
9f62c948b2c0fa3e30574ac3a4aeb66e18643f03	effect of pavement marking retroreflectivity on the performance of vision-based lane departure warning systems	retroreflectivity;ran off road crashes;wet weather;lane departure warning systems;road markings;laboratory studies;performance improvement;field studies;visibility;intelligent vehicles;warning systems;integrated vehicle based safety systems;visual perception;pavement markings	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	erdős–rényi model;francis;item unique identification;primary source	Mohammed Hadi;Prasoon Sinha	2011	J. Intellig. Transport. Systems	10.1080/15472450.2011.544587	simulation;visual perception;visibility;retroreflector;warning system;transport engineering;forensic engineering;field research	Robotics	-15.325846815870456	-6.429954222728308	114035
00209d9fcc76ed86d4df569ddaaa6985d80b6f06	games where you can play optimally without any memory	economie;modelizacion;distributed system;economia;parite;systeme reparti;game theory;estrategia optima;pago;simultaneidad informatica;teoria juego;strategie joueur;theorie jeu;parity;payment;system modelling;positional infinite games;estrategia jugador;sistema reactivo;jeu 2 personnes;modelisation;optimal strategy;ciencias economicas;paiement;concurrency;sistema repartido;juego 2 personas;two person game;reactive system;paridad;systeme reactif;sciences economiques;economy;economics;modeling;simultaneite informatique;strategie optimale;player strategy	Reactive systems are often modelled as two person antagonistic games where one player represents the system while his adversary represents the environment. Undoubtedly, the most popular games in this context are parity games and their cousins (Rabin, Streett and Muller games). Recently however also games with other types of payments, like discounted or mean-payoff [5,6], previously used only in economic context, entered into the area of system modelling and verification. The most outstanding property of parity, mean-payoff and discounted games is the existence of optimal positional (memoryless) strategies for both players. This observation raises two questions: (1) can we characterise the family of payoff mappings for which there always exist optimal positional strategies for both players and (2) are there other payoff mappings with practical or theoretical interest and admitting optimal positional strategies. This paper provides a complete answer to the first question by presenting a simple necessary and sufficient condition on payoff mapping guaranteeing the existence of optimal positional strategies. As a corollary to this result we show the following remarkable property of payoff mappings: if both players have optimal positional strategies when playing solitary one-player games then also they have optimal positional strategies for two-player games.	adversary (cryptography);ω-automaton	Hugo Gimbert;Wieslaw Zielonka	2005		10.1007/11539452_33	game theory;simulation;systems modeling;concurrency;parity;reactive system;computer science;artificial intelligence;mathematics;mathematical economics;algorithm;payment	ECom	-4.940340586835069	1.0573813347048542	114326
dbac6f8f3a6ea753dd0d44e51dad1bc5082d51b3	coalitional beliefs in cournot oligopoly tu games	91a12;cooperative game;externalities;core;cournot competition;j belief	In cooperative games, due to computational complexity issues, deviant agents are not able to base their behavior on the outsiders’ status but have to follow certain beliefs as to how it is in their strategic interest to act. This behavior constitutes the main interest of this paper. To this end, we quantify and characterize the set of coalitional beliefs that support cooperation of such agents. Assuming that they are engaged in a differentiated Cournot competition, for every belief of the deviants we define a TU-game, the solution to which characterizes the set of coalitional beliefs that support core nonemptiness. For this we fix the number of coalitions that deviants S will face to, say, j in number and introduce the notion of j-beliefa of S as the least number of coalitions into which the outsiders N\S will reorganize. We then define for every j-belief a TU-game and the j-belief core of it. We prove that the worth of S is minimized when the n−s agents split approximately equally among the j coalitions, while the worth of S is maximized when j − 1 agents have one member and one coalition has n− s− (j − 1) members. Given the above, we prove that when goods are substitutes, the j-belief core is nonempty, provided that S believe the N\S will form a sufficiently large number of coalitions, while when goods are complements, the j-belief core is nonempty irrespective of the beliefs of the agents in S. Finally, in the case of homogeneous goods we prove that the j-belief core is nonempty and depends only on the number of the outsider coalitions and not on their size.	computational complexity theory;the outsider	Paraskevas V. Lekeas	2013	IGTR	10.1142/S0219198913500047	externality;core;cournot competition;economics;public economics;microeconomics;mathematical economics;welfare economics	Theory	-5.747289171773462	-3.4699616889829312	114377
d8f26c24ef3c59a2b22547298e120df441ccc291	sampling dynamics of a symmetric ultimatum game	sampling dynamics;ultimatum game;social norm	We propose a dynamic three-strategy symmetric model of the Ultimatum Game with players using a sampling procedure. We allow an intermediate strategy, interpreted as a social norm, to evolve in time according to beliefs of players about an average offer. We show that a social norm converges to a self-consistent offer of about 15 % in the unique globally asymptotically stable equilibrium of our model. Copyright The Author(s) 2013		Jacek Miekisz;Michal Ramsza	2013	Dynamic Games and Applications	10.1007/s13235-012-0064-5	ultimatum game;dictator game;economics;mathematics;microeconomics;mathematical economics;welfare economics;symmetric game;norm	ECom	-4.968348666032068	-2.8295830708009437	114402
df29c41dc7307a79a9401595fcf8859e83e3a201	perspectives of systems informatics		A Petri net is distributed if, given an allocation of transitions to (geographical) locations, no two transitions at different locations share a common input place. A system is distributable if there is some distributed Petri net implementing it. This paper addresses the question of which systems can be distributed, while respecting a given allocation. The paper states the problem formally and discusses several examples illuminating – to the best of the authors’ knowledge – the current status of this work.	informatics;petri net	Edmund Clarke Irina Virbitskaite;Andrei Voronkov	2011		10.1007/978-3-642-29709-0	combinatorics;discrete mathematics;mathematics;linguistics;algorithm;satisfiability	AI	-9.774708500811476	1.6156098621293342	114799
db7a32d81003df259830969e5dbddf72e57ebf2a	allocation rules for land division	allocation rule;satisfiability	This paper studies the classical land division problem formalized by Steinhaus [27] in a multi-profile context. We propose a notion of an allocation rule for this setting. We discuss several examples of rules and properties they may satisfy. Central among these properties is division independence: a parcel may be partitioned into smaller parcels, these smaller parcels allocated according to the rule, leaving a recommended allocation for the original parcel. In conjunction with two other normative properties, division independence is shown to imply the principle of utilitarianism. Journal of Economic Literature Classification Numbers: C71, D63, D71 ∗Assistant Professor of Economics, Division of the Humanities and Social Sciences, Mail Code 228-77, California Institute of Technology, Pasadena, CA 91125. Email: chambers@hss.caltech.edu. Phone: (626) 395-3559. I would like to thank Marcus Berliant, John Duggan, Larry Epstein, and William Thomson for detailed comments and suggestions. I would also especially like to thank three anonymous referees whose valuable observations led to corrections and improvements that greatly improved this paper. All errors are my own.	email	Christopher P. Chambers	2005	J. Economic Theory	10.1016/j.jet.2004.04.008	economics;operations management;mathematics;mathematical economics;welfare economics;algorithm;satisfiability	ML	-7.533078100328308	-4.492159259414242	114831
a71fcae194d406d5f71fc83d5556b50406497f39	williamson on counterpossibles		A counterpossible conditional is a counterfactual with an impossible antecedent. Common sense delivers the view that some such conditionals are true, and some are false. In recent publications, Timothy Williamson has defended the view that all are true. In this paper we defend the common sense view against Williamson’s objections.		Francesco Berto;Rohan French;Graham Priest;David Ripley	2018	J. Philosophical Logic	10.1007/s10992-017-9446-x	epistemology;mathematics;counterfactual thinking;common sense	NLP	-13.183584342938092	3.6251335466578145	114876
95e6287f2c658ea8d1d9d8927517afa8fde2bd9c	a trichotomy of attitudes for decision-making under complete ignorance	institutional repositories;fedora;satisfiability;vital;probability distribution;complete ignorance quasi transitivity independence duplication protective criterion;vtls;scale invariance;ils	We study decision criteria under complete ignorance, that is, when there is no available information regarding plausible probability distributions over the possible outcomes. We characterize the set of criteria satisfying quasi-transitivity, Savage’s independence, duplication, a strong version of dominance and scale invariance. Only three criteria satisfy these requirements. These criteria are the well-known protective criterion, its dual criterion which we call hazardous, and a neutral criterionwhich is the composition of both (a decision is strictly preferred according to this criterion to another one if both the protective and the hazardous criteria strictly prefer the former). © 2009 Elsevier B.V. All rights reserved.	decision problem;neutral monism;requirement;vertex-transitive graph;whole earth 'lectronic link	Ronan Congar;François Maniquet	2010	Mathematical Social Sciences	10.1016/j.mathsocsci.2009.10.001	probability distribution;scale invariance;mathematics;welfare economics;statistics;satisfiability	AI	-7.297028188697427	-1.4630867611585021	114917
7fafe16983b318973f5b032a253a696a62c0b946	time series analysis and inverse theory for geophysicists	time series analysis	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source;time series	Donald E. Myers	2005	Technometrics	10.1198/tech.2005.s291	mathematical analysis;calculus;mathematics	Robotics	-14.507852944614465	-5.7735999012521875	115169
bb34a038d7f208473872548d4845052c46c03f90	max-prob: an unbiased rational decision making procedure for multiple-adversary environments	utility value;possible utility value;adversarial binary-utility game;resource-bounded environment;winning probability;rational action;unbiased rational decision;binary-utility game;unbiased rational agent;final state;multiple-adversary environment	In binary-utility games, an agent can have only two possible utility values for final states, 1 (win) and 0 (lose). An adversarial binary-utility game is one where for each final state there must be at least one winning and one losing agent. We define an unbiased rational agent as one that seeks to maximize its utility value, but is equally likely to choose between states with the same utility value. This induces a probability distribution over the outcomes of the game, from which an agent can infer its probability to win. A single adversary binary game is one where there are only two possible outcomes, so that the winning probabilities remain binary values. In this case, the rational action for an agent is to play minimax. In this work we focus on the more complex, multiple-adversary environment. We propose a new algorithmic framework where agents try to maximize their winning probabilities. We begin by theoretically analyzing why an unbiased rational agent should take our approach in an unbounded environment and not that of the existing Paranoid or MaxN algorithms. We then expand our framework to a resource-bounded environment, where winning probabilities are estimated, and show empirical results supporting our claims.	adversary (cryptography);algorithm;minimax;parsing;rational agent	Anat Hashavit;Shaul Markovitch	2011		10.5591/978-1-57735-516-8/IJCAI11-048	artificial intelligence;machine learning;mathematics;management science;statistics	AI	-9.474205960613142	-3.9463402247225545	115193
36cfd51453ce5616b88cba5e47c2449f3c156579	sequential contracting with multiple principals	incentive compatibility;dynamic game;contracts;sequential common agency;degeneration;endogenous types;common agency;mechanism design;private information;sequential common agency mechanism design contracts endogenous types	This paper considers dynamic games in which multiple principals contract sequentially and non-cooperatively with the same agent. We rst show that when contracting is private, i.e. when downstream principals observe neither the mechanisms o¤ered upstream nor the decisions taken in these mechanisms, then all PBE outcomes can be characterized through pure-strategy proles in which the principals o¤er menus of contracts and delegate to the agent the choice of the contractual terms. We then show that, in most cases of interest for applications, the characterization of the equilibrium outcomes is further facilitated by the fact that the principals can be restricted to o¤er incentive-compatible extended direct mechanisms in which the agent reports the endogenous payo¤-relevant decisions contracted upstream in addition to his exogenous private information. Finally we show how the aforementioned results must be adjusted to accommodate alternative assumptions about the observability of upstream histories and/or the timing of contracting examined in the literature. Keywords: Sequential common agency, mechanism design, contracts, endogenous types. JEL Classication Numbers: D89, C72. This paper builds on a previous working paper that circulated under the title A Markovian Revelation Principle for Common Agency Games.For discussions and useful comments, we thank seminar participants at various institutions and conferences where this paper has been presented. A special thank is for Eddie Dekel, Mike Peters, Marciano Siniscalchi, Jean Tirole, an associate editor and four anonymous referees for suggestions that helped us improve the paper. We are also thankful to Itai Sher for excellent research assistance. yDepartment of Economics, Northwestern University. Email: alepavan@northwestern.edu, Corresponding author. Fax: +1 847 491 7001. zDepartment of Economics, University of Bologna. E-mail: giacomo.calzolari@unibo.it	2-satisfiability;downstream (software development);eddie (text editor);email;fax;jean;like button;personally identifiable information;upstream (software development)	Alessandro Pavan;Giacomo Calzolari	2009	J. Economic Theory	10.1016/j.jet.2008.07.003	mechanism design;private information retrieval;economics;incentive compatibility;public economics;operations management;microeconomics;sequential game;welfare economics	ECom	-6.9137838506789455	-4.308172889162214	115296
7643fd433c83f170ee97040ba1b986b3c0020b5c	meaning and dialogue coherence: a proof-theoretic investigation	coherence;dialogue modelling;natural deduction;multi-agent inference;proof-theoretic semantics	This paper presents a novel proof-theoretic account of dialogue coherence. It focuses on an abstract class of cooperative information-oriented dialogues and describes how their structure can be accounted for in terms of a multi-agent hybrid inference system that combines natural deduction with information transfer and observation. We show how certain dialogue structures arise out of the interplay between the inferential roles of logical connectives (i.e., sentence semantics), a rule for transferring information between agents, and a rule for information flow between agents and their environment. The order of explanation is opposite in direction to that adopted in game-theoretic semantics, where sentence semantics (or a notion of valid inference) is derived from winning dialogue strategies. That approach and the current one may, however, be reconcilable, since we focus on cooperative dialogue, whereas the game-theoretic tradition concentrates on adversarial dialogue.	abstract type;european summer school in logic, language and information;game semantics;game theory;generative systems;inference engine;inferential programming;information flow (information theory);information processing;logical connective;multi-agent system;natural deduction;separable polynomial	Paul Piwek	2008	Journal of Logic, Language and Information	10.1007/s10849-008-9069-2	natural language processing;information transfer;coherence;computer science;artificial intelligence;proof-theoretic semantics;mathematics;linguistics;programming language;natural deduction;algorithm	AI	-16.71993471775376	3.6136986537857254	115339
94f33dd8a2f4dd4725da3246874f8d25c7912e4a	goals and bracketing under mental accounting	self control;goals;choice bracketing;mental accounting;reference dependent preferences;quasi hyperbolic discounting	Behavioral economics struggles to explain why people sometimes evaluate outcomes separately (narrow bracketing of mental accounts) and sometimes jointly (broad bracketing). We develop a theory of endogenous bracketing, where people set goals to tackle self-control problems. Goals induce reference points that make substandard performance painful. Evaluating goals in a broadly bracketed mental account insulates an individual from exogenous risk of failure; but because decisions or risks in different tasks become substitutes there are incentives to deviate from goals that are absent under narrow bracketing. Extensions include goal revision, naïveté about self-control, income targeting, and firms’ bundling strategies. © 2016 Elsevier Inc. All rights reserved. JEL classification: D03; D81; D91	naivety;theory	Alexander K. Koch;Julia Nafziger	2016	J. Economic Theory	10.1016/j.jet.2016.01.001	self-control;actuarial science;economics;mental accounting;microeconomics;welfare economics	AI	-5.015454534002795	-6.872124120433656	115476
af771374c9bddd8f852ea4af12d68dc48f1d140f	strategic advice for decision-making under conflict based on observed behaviour		An improvement in the inverse engineering of preferences approach for the Graph Model for Conflict Resolution is introduced. In addition to providing decision-makers and analysts with up-to-date preference information about opponents, the methodology is now equipped with an Advice function which enriches the decision-making process by providing important information regarding potential moves. Decision-makers who use the methods introduced in this paper are provided with the expected value of each of their possible moves, with the probability of the opponent’s next response, and with the opponent reachable states. This insightful information helps establish an accurate picture of the conflict situation and in so doing, aids stakeholders in making strategic decisions. © 2018 Elsevier Inc. All rights reserved.	advice (complexity)	Amanda Garcia;Amer Obeidi;Keith W. Hipel	2018	Applied Mathematics and Computation	10.1016/j.amc.2018.03.031	mathematical optimization;management science;game theory;expected value;mathematics;adversary;conflict resolution;graph	AI	-9.809107485801269	-4.1422508975962415	115498
fb5827cf3b11053d7a4eaf12c4b2ecca6eba8ea8	a simple distributed conflict-driven answer set solver	empirical study;answer sets;faculty of science environment engineering and technology;280301;message passing;pre2009 programming techniques;off the shelf	We propose an approach to distributed Answer Set Solving based on Message Passing. Our approach aims at taking advantage of modern ASP solvers rather than proposing a genuine yet involved parallel ASP solver. To this end, we rely upon a simple master-worker architecture in which each worker amounts to an off-the-shelf ASP solver augmented with a separate communication module being only lightly connected to the actual solver. The overall communication is driven by the workers’ communication modules, which asynchronously exchange messages with the master. We have implemented our approach and report upon an empirical study demonstrating its computational impact.	answer set programming;message passing;solver;stable model semantics	Enrico Ellguth;Martin Gebser;Markus Gusowski;Benjamin Kaufmann;Roland Kaminski;Stefan Liske;Torsten Schaub;Lars Schneidenbach;Bettina Schnor	2009		10.1007/978-3-642-04238-6_47	message passing;simulation;computer science;artificial intelligence;theoretical computer science;distributed computing;programming language;empirical research;algorithm	AI	-17.148872138599007	-7.961126623333493	115540
e5d5afd6e96beeb0d7d8ee37306bcfb5941e16b2	epistemic versus all things considered requirements	humanidades;filosofia etica	Epistemic obligations are constraints on belief stemming from epistemic considerations alone. Booth (Synthese 187: 509–517, 2012) is one of the many philosophers who deny that there are epistemic obligations. Any obligation pertaining to belief is an all things considered obligation, according to him—a strictly generic, rather than specifically epistemic, requirement. Though Booth’s argument is valid, I will try to show that it is unsound. There are two central premises: (1) S is justified in believing that P iff S is blameless in believing that P; (2) S is blameless in believing that P iff S has not violated an all things considered duty in believing that P. Both premises are false. My argument against (1) depends on my own theory of epistemic obligations. My argument against (2) does not. This paper is part of a larger project—defending epistemic requirements in general against a series of objections and advancing a particular theory that solves various problems.	requirement;stemming	Scott Stapleford	2015	Synthese	10.1007/s11229-015-0660-1	philosophy;epistemology;mathematics;algorithm	AI	-13.093101971093866	3.7931024059004965	115865
370bda1bf9f54f229ae48a8685f2d5418fbeeda8	sla as a complementary currency in peer-2-peer markets	qa75 electronic computers computer science	Service Level Agreements (SLAs) provide a basis for establishing contractually binding interactions within Peer-2-Peer systems. Such SLAs are particularly useful when considering interactions in environments with limited trust between participants. Complementary currencies, on the other hand, have proven to be useful for facilitating exchange among selfish peers. We identify how an SLA can itself be used as a complementary currency to encourage resource sharing between peers. Our work demonstrates how an SLA can be used as a medium of exchange and used to establish a market for computational resources. The value of an SLA can vary based on demand for particular types of resources. Simulate a process of trade we investigate several economic indicators qualified for their significance in small economies. We demonstrate how the economic benefit (in terms of profit and loss) evolves based on varying levels of demand.	darknet market;service-level agreement	Ioan Petri;Omer F. Rana;Gheorghe Cosmin Silaghi	2010		10.1007/978-3-642-15681-6_11	industrial organization;economics;computer science;marketing;operations management;database;distributed computing;management;commerce	ECom	-6.161179772287279	-7.761129347344123	115870
4fcab67ae42a01204afa371273fbcc9c7ac55d63	nonreductive physics		This paper documents a wide range of nonreductive scientific treatments of phenomena in the domain of physics. These treatments strongly resist characterization as explanations of macrobehavior exclusively in terms of behavior of microconstituents. For they are treatments in which macroquantities are cast in the role of genuine and irreducible degrees of freedom. One is driven into reductionism when one is not cultivated to possess an array of distinctions rich enough to let things be what they are. In contrast, making the decisive distinction has an illuminating and liberating effect because it lets the concrete occurrence stand forth for what it is. We understand it not in terms of a decipherment, but on its own terms. –Robert Sokolowski (“Making Distinctions”, 1992) One is driven into reductionism when one is not cultivated to possess an array of distinctions rich enough to let things be what they are. In contrast, making the decisive distinction has an illuminating and liberating effect because it lets the concrete occurrence stand forth for what it is. We understand it not in terms of a decipherment, but on its own terms.	decipherment;irreducibility;reductionism	Mariam Thalos	2004	Synthese	10.1007/s11229-004-6251-1		ML	-12.312339993137538	2.99154917445586	116326
5afe3182619991446457ee9692a8a86f85a1f391	implementation without commitment in moral hazard environments		Interdependent-choice equilibrium is defined as an extension of correlated equilibrium in which the mediator is able to choose the timing of her signals, and observe the actions taken by the players. The set of interdependent-choice equilibria is a nonempty, closed and convex polytope. It characterizes all the outcomes that can be implemented in single shot interactions without repetition, side payments, binding contracts or any other form of delegation.	interaction;interdependence;moral hazard;name binding	Bruno Salcedo	2013	CoRR		simulation;operations management	ECom	-4.743503319811814	-2.965054594022468	116468
47fe88d0ce6456b33164ecd837842eefd9ef0ae9	extracting tsk-type neuro-fuzzy model using the hunting search algorithm	hunting search algorithm hus;online control;modeling;tsk type neuro fuzzy model	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;neuro-fuzzy;primary source;search algorithm	Sana Bouzaida;Anis Sakly;Faouzi M'Sahli	2014	Int. J. General Systems	10.1080/03081079.2013.848355	systems modeling;artificial intelligence;machine learning;operations research	Robotics	-14.748086887150697	-5.742733849783721	116869
4cc18e6a5538374434bec828a805e89ca3ee4f7c	an axiomatization of the core of market games	optimum pareto;market games;game theory;propiedad;game core;coalicion;axiomatic;axiomatic approach;noyau jeu;teoria juego;theorie jeu;axiomatico;core;jeu marche;coalition;characterization;propriete;nucleo juego;market game;caracterisation;properties;axiomatique;pareto optimum;optimo pareto;caracterizacion	As shown by Peleg (1993), the core of market games is characterized by nonemptiness, individual rationality, superadditivity, the weak reduced game property, the converse reduced game property, and weak symmetry. It was not known whether weak symmetry was logically independent. With the help of a certain transitive 4-person TU game, it is shown that weak symmetry is redundant in this result. Hence, the core on market games is axiomatized by the remaining five properties, if the universe of players contains at least four members.	axiomatic system;rationality	Bezalel Peleg	1989	Math. Oper. Res.	10.1287/moor.14.3.448	bondareva–shapley theorem;axiomatic system;game theory;core;simulation;mathematics;axiom;property;mathematical economics;algorithm	ECom	-6.230734706886293	-1.3179644153793448	117140
88e8e0bb0f29961cbd252e28ac8f1032ac51b8e0	interval methods for judgment aggregation in argumentation	qa75 electronic computers computer science	Given a set of conflicting arguments, there can exist multiple plausible opinions about which arguments should be accepted, rejected, or deemed undecided. Recent work explored some operators for deciding how multiple such judgments should be aggregated. Here, we generalize this line of study by introducing a family of operators called interval aggregation methods, and show that they contain existing operators as instances. While these methods fail to output a complete labelling in general, we show that it is possible to transform a given aggregation method into one that does always yield collectively rational labellings. This employs the down-admissible and up-complete constructions of Caminada and Pigozzi. For interval methods, collective rationality is attained at the expense of a strong Independence postulate, but we show that an interesting weakening of the Independence postulate is retained by this transformation. Our results open up many new directions in the study of generalized judgment aggregation in argumentation. aA short version of this paper appears in the proceedings of KR 2014.	graph labeling;interval arithmetic;rationality	Richard Booth;Edmond Awad;Iyad Rahwan	2014			computer science;artificial intelligence;algorithm	AI	-13.213135206645122	4.09237710436223	117338
7c5fd0cfcf2e2db18aa2c298b371d137fd5a02c1	one-to-many non-cooperative matching games	subgame perfect equilibrium;many to one matching;labor economics;stability;non cooperative game	We study a strategic model of wage negotiations between rms and workers. First, we dene the stability of an allocation in an environment where rms can employ more than one worker. Secondly, we develop a one-to-many non-cooperative matching game, which is an extension of Kameckes one-to-one non-cooperative matching game. The main result shows that the equivalence between the stable allocations and the outcomes of the subgame equilibria in the matching game: for any stable allocation there is a subgame perfect equilibrium in this game which induces the allocation on the equilibrium path, and every subgame perfect equilibrium induces a stable allocation on the equuilibrium path. Furthermore, as for the existence of a stable allocation, we argue that a stable allocation, so a subgame perfect equilibrium, does not always exist, but it exists under some conditions, using Kelso and Crawfords modelling.	one-to-many (data model);one-to-one (data model);turing completeness	Yujiro Kawasaki	2013	Int. J. Game Theory	10.1007/s00182-013-0369-7	non-cooperative game;markov perfect equilibrium;game theory;mertens-stable equilibrium;sequential equilibrium;trembling hand perfect equilibrium;stability;economics;extensive-form game;subgame;non-credible threat;folk theorem;information set;repeated game;microeconomics;mathematical economics;stackelberg competition;subgame perfect equilibrium;welfare economics;backward induction;equilibrium selection;solution concept;nash equilibrium;centipede game;statistics	ECom	-4.575418439971465	-3.105413590949616	117628
f9f02be52daa19d15b3113ee46d44712fe56aaee	a framework for fraud discovery via illicit agreements in energy markets		Fundamental changes in the electrical energy sector are drawing on serious implications leading to high heterogeneity in both supply and demand. One focal point in transitioning to the smart grid vision is the liberalization and deregulation of electricity markets, which are undergoing a transformation towards accommodating a more decentralized and sustainable provision of energy. As the number of traders in the market is increasing steadily and the trading activities are becoming more complex, the energy markets are becoming more exposed to potential fraud. In this paper we address the problem of detecting collusive behavior, where a group of individual traders act together, inconsistently with the competitive model, to artificially manipulate the market and elicit illegal profits. We investigate collusion attacks in the energy market and propose a novel mechanism, showing the effectiveness and practical applicability of our method to real scenarios. Thus, while economic analysis does not imply proving guilt, it can be instrumental in making decisions about identifying market participants worthy of closer inspection from appointed antitrust authorities.	algorithmic trading;breakpoint;exploit (computer security);focal (programming language);sensor;traders;trading room	Radu-Casian Mihailescu;Sascha Ossowski	2015	AI Commun.	10.3233/AIC-140625	artificial intelligence;machine learning;industrial organization;deregulation;profit (economics);smart grid;energy market;collusion;computer science;focal point;liberalization;supply and demand	AI	-6.039824677424609	-8.85509521214246	117653
d58687700dd002969a6273573be4bb60490967cb	the demon, the gambler, and the engineer - reconciling hybrid-system theory with metrology		Hybrid discrete-continuous system dynamics arises when discrete actions, e.g. by a decision algorithm, meet continuous behaviour, e.g. due to physical processes and continuous control. Various flavours of hybrid automata have been suggested as a means to formally analyse such dynamical systems, among them deterministic automata models facilitating reasoning about their normative behaviour, nondeterministic automata under a demonic interpretation supporting worst-case analysis, and stochastic variants enabling quantitative verification. In this article, we demonstrate that all these variants provide imprecise, in the sense of either overly pessimistic or overly optimistic, verdicts for engineered systems operating under uncertain observation of their environment due to, e.g., measurement error. We argue that even the most elaborate models of hybrid automata currently available ignore wisdom from metrology and game theory concerning environmental state estimation to be pursued by a rational player, which a control system obviously ought to constitute. We consequently suggest a revised formal model, called Bayesian hybrid automata, that is able to represent state tracking and estimation in hybrid systems and thereby enhances precision of verdicts obtained from the model.		Martin Fränzle;Paul Kröger	2017		10.1007/978-3-030-01461-2_9		Logic	-16.32950188907613	-0.5234797777255966	117770
95119f2dd4816a4f02e93362730318e1edbd273e	integration and conditioning in numerical possibility theory	possibility measure;satisfiability;fuzzy integral;technology and engineering;choquet integral;possibility theory	The paper discusses integration and some aspects of conditioning in numerical possibility theory, where possibility measures have the behavioural interpretation of upper probabilities, that is, systems of upper betting rates. In such a context, integration can be used to extend upper probabilities to upper previsions. It is argued that the role of the fuzzy integral in this context is limited, as it can only be used to define a coherent upper prevision if the associated upper probability is 0–1-valued, in which case it moreover coincides with the Choquet integral. These results are valid for arbitrary coherent upper probabilities, and therefore also relevant for possibility theory. It follows from the discussion that in a numerical context, the Choquet integral is better suited than the fuzzy integral for producing coherent upper previsions starting from possibility measures. At the same time, alternative expressions for the Choquet integral associated with a possibility measure are derived. Finally, it is shown that a possibility measure is fully conglomerable and satisfies Walley's regularity axiom for conditioning, ensuring that it can be coherently extended to a conditional possibility measure using both the methods of natural and regular extension.	coherence (physics);numerical analysis;possibility theory	Gert de Cooman	2001	Annals of Mathematics and Artificial Intelligence	10.1023/A:1016705331195	possibility theory;mathematical analysis;computer science;artificial intelligence;calculus;fuzzy measure theory;mathematics;choquet integral;satisfiability	AI	-11.047936958749588	1.4594914651688247	117859
937c7f0bf6df4f9f1f7a61d435d6f82976b7a3d5	addressing concept drift in reputation assessment		Evaluating trust and reputation in environments where agent behaviours are highly dynamic is challenging. In this paper, we address the limitations of existing methods when agent behaviours can change at varying speeds and times across a system. Modelling such environments as a multi-agent system, we propose a method that expands on existing trust, reputation and stereotype models and uses concept drift detection to identify and exclude unrepresentative past experiences. Our method improves the selection of relevant data for evaluating trust and reputation, without excluding relevant historical data (as is the case with sliding windows and forgetting factors). We show that our approach enables agents to achieve higher utility than existing methods, by more accurately and robustly evaluating trust and reputation with respect to dynamic agent behaviours.	concept drift;microsoft windows;multi-agent system	Caroline Player;Nathan Griffiths	2018			simulation;computer science;concept drift;distributed computing;reputation	AI	-10.957052821395397	-8.897903187803653	117935
96b31f19139a30109b0c7adf1152177cfb356fc9	towards elearning 2.0 university	social software;tencompetence;web 2 0;university;interactive learning environment;article;elearning 2 0	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Adriana J. Berlanga;Francisco J. García-Peñalvo;Peter B. Sloep	2010	Interactive Learning Environments	10.1080/10494820.2010.500498	human–computer interaction;computer science;knowledge management;multimedia;web 2.0;world wide web	Robotics	-16.11445120532029	-5.98569688204969	118334
fad26b790f23df512833b801381713adad2f68d1	expertness measuring in cooperative learning	object pushing systems;mathematics;multi agent system;intelligent robots;expertness criteria;multi robot systems learning artificial intelligence multi agent systems;expertness measurement;expert agents;learning systems;physics;multi agent systems;object pushing systems cooperative learning learning quality learning speed expert agents weighted strategy sharing expertness measurement expertness criteria hunter prey problems;shared knowledge;multi robot systems;immune system;learning quality;intelligent systems;weighted strategy sharing;system testing;humans;intelligent robots humans multiagent systems immune system laboratories intelligent systems physics mathematics learning systems system testing;learning speed;learning artificial intelligence;hunter prey problems;cooperative learning;multiagent systems	Cooperative Learning in a multi-agent system can improve the learning quality and learning speed. The improvement can be gained if each agent detects the expert agents and use their knowledge properly. In this paper, a new cooperative learning method, called Weighted Strategy Sharing (WSS) is introduced. Also some criteria are introduced to measure the expertness of agents. In WSS, based on the amount of its teammate expertness, each agent assigns a weight to their knowledge. These weights are used in sharing knowledge among agents in our system. WSS and the expertness criteria are tested on two simulated Hunter-Prey problem and Object Pushing systems.	experience;gradient;in the beginning... was the command line;intelligent agent;multi-agent system;prey;reinforcement learning;simulation	Majid Nili Ahmadabadi;Masoud Asadpour;Seyyed H. Khodanbakhsh;Eiji Nakano	2000		10.1109/IROS.2000.895305	simulation;immune system;computer science;knowledge management;artificial intelligence;multi-agent system;system testing	AI	-16.85838681515128	-9.69716896029491	118390
6f33942e8be9e133ff134ccc444cddba03661e90	the stability set as a social choice correspondence	game theory;voting rules;voting games;voting game;stability;voting rule;social choice;social choice correspondence;stability set	The stability set has been introduced in the game theory and social choice literature by Rubinstein [J. Econ. Theory 23 (1980) 150], as a possible resolution of the problem of the emptiness of the core of voting games. In this context, it models the idea that individuals may be, in some sense, prudent: a voter  i  will never vote for candidate  a  against candidate  b  in a pairwise comparison if  a  is afterwards beaten by a candidate  c  that is worse than  a  and  b  in her preference ordering. Most of the literature on the stability set focused on its non-existence, but almost nothing is said on its relationships with other social choice correspondences and its normative properties. This is what we intend to do here in a rather extensive study. We first prove that the stability set offers a specific way to select the best alternatives, different from many other voting rules. Secondly, we show that this prudent behavior, in the context of voting, may lead to the selection of undesired alternatives.		Mathieu Martin;Vincent Merlin	2002	Mathematical Social Sciences	10.1016/S0165-4896(02)00010-0	bullet voting;game theory;may's theorem;social choice theory;stability;economics;arrow's impossibility theorem;public economics;approval voting;calculus of voting;microeconomics;cardinal voting systems;welfare economics;anti-plurality voting;condorcet method;statistics;disapproval voting	ECom	-7.906707112107856	-2.2067753154436507	118428
392cf15b71cfcbbccde020f129f6164cd30dd22d	a general-purpose defeasible reasoner	defeasible reasoning	"""In its present incarnation, OSCAR is a fully implemented programmable architecture for a rational agent. If we just focus upon the epistemic reasoning in OSCAR, we lzave a powoful general-purpose defeasible reasoner. The purpose of this paper is to describe that reasoner. OSCAR's defeasible reasoner is based upon seven fundamental ideas. These are (I) an argument-based account of defeasible reasoning, (2) an analysis of defeat-status given a set of interrelated arguments, ( 3) a general adequacy criterion for automated defeasible reasoners, called """"d.e.-adequacy"""", (4) the claim tlzat a generalpurpose defeasible reasoner must have the kind of structure I have called """"flag-based"""", ( 5) an af,;;orithm for compttting defeat-status, (6) an interest-driven monotonic reasoner, and (7) an account of degrees of justification, degrees of interest, and their interactions. These will each be discussed in tum. In its present incarnation, OSCAR is a fully implemented programmable architecture for a rational agent. This architecture and the underlying theory are described in detail in [POL95]. It is an important characteristic of OSCAR that most practical reasoning is reduced to epistemic reasoning, through a process I call """"doxastification"""". In particular, most (but not quite all) of the details of planning, plan adoption, and plan execution are carried out by epistemic reasoning. If we focus on the epistemic reasoning in OSCAR, we have a powerful general-purpose defeasible reasoner. The purpose of this paper is to describe that reasoner. OSCAR's defeasible reasoner is based upon seven fundamental ideas. These are (I) an argument-based account of defeasible reasoning, (2) an analysis of defeat-status given a set of interrelated arguments, (3) a general adequacy criterion for automated defeasible reasoners, called """"i.d.e.-adequacy"""", (4) the claim that a general-purpose defeasible reasoner must have the kind of structure I have called """"t1ag-based"""", (5) an algorithm for computing defeat-status, (6) an interest-driven monotonic reasoner, and (7) an account of degrees of justification, degrees of interest, and their interactions. I will discuss each of these in turn. Journal of Applied Non-Classical Logics. Volume 6no l/1996. pages 89 a 113 D ow nl oa de d by [ N ew Y or k U ni ve rs ity ] at 1 8: 03 1 1 M ay 2 01 5 90 Journal of Applied Non-Classical Logics. Vol. 6 no 1/1996 1. Argument-Based Defeasible Reasoning The basic conception of defeasible reasoning embodied in OSCAR can, in its most general form, be regarded as the received view in philosophical epistemology. It is the argument-based approach pioneered by Roderick Chisholm and myself in the late 60's and early 70's (see [CHI66) and [CHI77), and [POL67], [POL70), [POL71), and [POL74]), and developed further by myself ([POL86), [POL87), [POL90c], [POL91 ], [POL92), [POL94]), and a number of people in the intervening years (e.g., [KYB83], [LOU87], and [NUT88] and [NUT90]). The basic idea is that reasoning consists of the construction of arguments, where reasons are the atomic links in arguments. Defeasibility arises from the fact that some reasons are subject to defeat. I call these """"prima facie reasons"""", and the considerations that defeat them are """"defeaters"""". Although this is somewhat controversial, I have long argued and will here assume that there are only two kinds of defeaters. A """"rebutting defeater"""" is a reason for denying the conclusion of the reason. An """"undercutting defeater"""" attacks the connection between the premises and the conclusion, and can be regarded as a reason for denying that the premises wouldn't be true unless the conclusion were true. It will be convenient to abbreviate """"(P wouldn't be true unless Q were true)"""" as """"(P ® Q)"""". Reasons that are not defeasible are """"conclusive"""". A convenient way to encode arguments is as """"inference-graphs"""". The nodes of an inference-graph represent premises and conclusions, and the links between the nodes represent dependency relations. The """"node-basis"""" of a node is the set of nodes from which it is inferred. We can combine all of the reasoning of the reasoner into a single global inference-graph, and that will be the central data-structure used by the reasoner. Defeat relations can be encoded by adding a separate category of """"defeat links"""" to the inference-graph. An important point about OSCAR's inference-graphs is that if the same conclusion is obtained by two different arguments, this is represented by two different nodes. In this way, we can take the nodes to have univocal defeat-statuses. In effect, each node encodes an argument for the conclusion represented at that node. Some simple inference-graphs are diagrammed in figures l4. Because OSCAR engages in """"suppositional reasoning"""", wherein conclusions are drawn relative to suppositions, the nodes of the inference-graph will be taken to encode sequents rather than formulas. A sequent is a pair (r,P) where P is a formula and r is a set of formulas (the supposition). This will be discussed further in section seven. 2. Justification and Defeat Given a set of arguments some of which may support defeaters for inferences contained in others, we require an account of which of these arguments are defeated and which are undefeated. My analysis of defeat-status has evolved over the years, largely in an attempt to handle some intuitively complicated cases like the paradox of the preface and cases involving self-defeat (where an argument may defeat some of its own steps). I now believe that I have an adequate account of these phenomena, and my account is defended in a recent paper [POL94]. The analysis is as follows. We first define: D ow nl oa de d by [ N ew Y or k U ni ve rs ity ] at 1 8: 03 1 1 M ay 2 01 5"""	algorithm;automated planning and scheduling;case-based reasoning;data structure;defeasible reasoning;encode;general-purpose markup language;inference engine;interaction;p (complexity);rational agent;type inference;user (computing)	John L. Pollock	1996	Journal of Applied Non-Classical Logics	10.1080/11663081.1996.10510868	epistemology;computer science;artificial intelligence;defeasible reasoning	AI	-14.12973280142137	3.9349477867114993	118755
0058ea3000d48253d19e3be16f6657cc9c78ecc1	enabling computer to negotiate with human in e-commerce: a strategy model	computers;electronic commerce;collaboration;strategy automated negotiation agent;software agents electronic commerce electronic trading human computer interaction internet;software agents;strategy;agent;computational modeling;business;computers computational modeling collaboration business software agents context electronic commerce;context;strategy model human computer negotiation online dynamic trading b2c e commerce software agent design online merchants trading cost automated negotiation;automated negotiation	Human-computer negotiation plays an important role in dynamic trading online, especially in B2C e-commerce. Further scientific investigations about designing the software agent that can deal with the human's random and inconsistent offer is in need, which is crucially useful for the online merchants to achieve better trading outcomes and save vast trading cost. The lack of such studies has decelerated the process of applying automated negotiation to real world applications. To address the critical issue, this paper develops a strategy model. To demonstrate the effectiveness of this model, we develop a prototype and conduct human-computer negotiation experiments over 121 participants. The experimental result shows that the agent with our newly designed strategy model can significantly increase the agreement rate and joint outcome of the both sides, and even can outperform human negotiators.	e-commerce payment system;experiment;prototype;software agent	Mukun Cao;Lifang Peng	2016	2016 49th Hawaii International Conference on System Sciences (HICSS)	10.1109/HICSS.2016.52	e-commerce;strategy;computer science;knowledge management;marketing;software agent;management;computational model;world wide web;commerce;collaboration	AI	-8.973380503714099	-8.8469146877374	118902
bb8cb48d7fe1baf3ba342dbe8bb83c85e3b87161	coalition-based pricing in ascending combinatorial auctions	bidder behavior;coordination problem;coalition based pricing;multi object auctions	Bidders in larger ascending combinatorial auctions face a substantial coordination problem, which has received little attention in the literature. The coordination problem manifests itself by the fact that losing bidders need to submit non-overlapping package bids which are high enough to outbid the standing winners. We propose an auction format, which leverages the information that the auctioneer collects throughout the auction about the preferences of individual bidders and suggests prices for the members of losing bidder coalitions, which in total would make a given coalition winning. We model the bidder’s bundle selection problem as a coordination game, which provides a theoretical rationale for bidders to agree to these prices, and highlights the role of the auctioneer in providing relevant information feedback. Results of extensive numerical simulations and experiments with human participants demonstrate that this type of pricing substantially reduces the number of auction rounds and bids necessary to find a competitive equilibrium, and at the same time significantly increases auction efficiency in the lab. This rapid convergence is crucial for the practical viability of combinatorial auctions in larger markets.	decision problem;design rationale;experiment;numerical analysis;selection algorithm;simulation;ws-coordination	Martin Bichler;Zhen Hao;Gediminas Adomavicius	2017	Information Systems Research	10.1287/isre.2016.0681	industrial organization;social science;combinatorial auction;coordination game;economics;vickrey–clarke–groves auction;common value auction;english auction;microeconomics;commerce	AI	-4.565670232765925	-7.15662288052434	118904
441313fe192d158054d44704ebaba4f735b76a8a	system integration for kinematic data exchange	cadcam;production engineering human work science and ergonomics;implementation;produktionsteknik arbetsvetenskap och ergonomi;industrial engineering and management;kinematics;step;industriell ekonomi och organisation	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source;system integration	Yujiang Li;Mikael Hedlind;Torsten Kjellberg;Gunilla Sivard	2015	Int. J. Computer Integrated Manufacturing	10.1080/0951192X.2014.941937	kinematics;simulation;computer science;engineering;operations management;implementation;engineering drawing;mechanical engineering	Robotics	-15.419848053686607	-4.573125351928146	119141
e12d91802ba7ab987b12409d181c7b638cc97223	evolutionary game analysis and regulatory strategies for online group-buying based on system dynamics		ABSTRACTThe emergence of online group-buying provides a new consumption pattern for consumers in e-commerce era. However, many consumers realize that their own interests sometimes can’t be guaranteed in the group-buying market due to the lack of being regulated. This paper aims to develop effective regulation strategies for online group-buying market. To the best of our knowledge, most existing studies assume that three parties in online group-buying market, i.e. the retailer, the group-buying platform and the consumer, are perfectly rational. To better understand the decision process, in this paper, we incorporate the concept of bounded rationality into consideration. Firstly, a three-parties evolutionary game model is established to study each player’s game strategy based on bounded rationality. Secondly, the game model is simulated as a whole by adopting system dynamics to analyze its stability. Finally, theoretical analysis and extensive computational experiments are conducted to obtain the managerial...	system dynamics	Zhong-Zhong Jiang;Na He;Xuwei Qin;Andrew W. H. Ip;Chun-Ho Wu;K. L. Yung	2018	Enterprise IS	10.1080/17517575.2017.1412503	data mining;management science;computer science;system dynamics;bounded rationality;group buying	Robotics	-8.644789299036498	-9.757184307707908	119253
6b2ff8f1abf80c56b773b963d4186688f079ddc5	computationally effective reasoning about goal interactions	agent platform;conflict detection;x jack system;ai;statistical significance;goal interactions;plan merging;reasoning algorithms;intelligent agent;artificial intelligence;empirical evaluation;data structure;agent reasoning;stress testing;jack	It is important that intelligent agents are able to pursue multiple goals in parallel, in a rational manner. In this work we have described the careful empirical evaluation of the value of data structures and algorithms developed for reasoning about both positive and negative goal interactions. These mechanisms are incorporated into a commercial agent platform and then evaluated in comparison to the platform without these additions. We describe the data structures and algorithms developed, and the X-JACK system, which incorporates these into JACK, a state of the art agent development toolkit. There are three basic kinds of reasoning that are developed: reasoning about resource conflicts, reasoning to avoid negative interactions that can happen when steps of parallel goals are arbitrarily interleaved, and reasoning to take advantage of situations where a single step can help to achieve multiple goals. X-JACK is experimentally compared to JACK, under a range of situations designed to stress test the reasoning algorithms, as well as situations designed to be more similar to real applications. We found that the cost of the additional reasoning is small, even with large numbers of goal interactions to reason about. The benefit however is noticeable, and is statistically significant, even when the amount of goal interactions is relatively small.	algorithm;complexity;computation;consumability;data structure;experiment;heuristic (computer science);intelligent agent;interaction;interference (communication);iteration;jack audio connection kit;jack intelligent agents;jack edmonds;petri net;recursion;recursive language;requirement;stress testing;synergy	John Thangarajah;Lin Padgham	2010	Journal of Automated Reasoning	10.1007/s10817-010-9175-0	opportunistic reasoning;simulation;qualitative reasoning;data structure;computer science;artificial intelligence;adaptive reasoning;statistical significance;management science;reasoning system;intelligent agent;stress testing	AI	-17.288198889245365	-7.73068726849737	119289
26d4dcf2adee4d9b16c5d04c8f18cc860777da3a	strategic argumentation: a game theoretical investigation	normative reasoning;argumentation;predictive force;game theory;probability;defeasible logic	Argumentation is modelled as a game where the payoffs are measured in terms of the probability that the claimed conclusion is, or is not, defeasibly provable, given a history of arguments that have actually been exchanged, and given the probability of the factual premises. The probability of a conclusion is calculated using a standard variant of Defeasible Logic, in combination with standard probability calculus. It is a new element of the present approach that the exchange of arguments is analysed with game theoretical tools, yielding a prescriptive and to some extent even predictive account of the actual course of play. A brief comparison with existing argument-based dialogue approaches confirms that such a prescriptive account of the actual argumentation has been almost lacking in the approaches proposed so far.	defeasible logic;game theory;provable security	Bram Roth;Régis Riveret;Antonino Rotolo;Guido Governatori	2007		10.1145/1276318.1276333	game theory;artificial intelligence;probability;probabilistic argumentation;algorithm	AI	-15.007058836877205	2.5290427621602953	119385
11121470faddad91e469c1c4bb2e5f625629594e	on notions of causality and distributed knowledge	satisfiability;computer security;information flow	The notion of distributed knowledge is used to express what a group of agents would know if they were to combine their information. The paper considers the application of this notion to systems in which there are constraints on how an agent’s actions may cause changes to another agent’s observations. Intuitively, in such a setting, one would like that anything an agent knows about other agents must be distributed knowledge to the agents that can causally affect it. In prior work, we have argued that the definition of intransitive noninterference — a notion of causality used in the literature on computer security — is flawed because it fails to satisfy this property, and have proposed alternate definitions of causality that we have shown to be better behaved with respect to the theory of intransitive noninterference. In this paper we refine this understanding, and show that in order for the converse of the property to hold, one also needs a novel notion of distributed knowledge, as well as a new notion of what it means for a proposition to be “about” other agents.	asynchronous system;bloom;causal filter;causality;computer security;directed acyclic graph;fagin's theorem;information flow (information theory);message passing;moses;operating system;oracle machine	Ron van der Meyden	2008			information flow;computer science;artificial intelligence;algorithm;satisfiability	AI	-17.151301250806483	3.943106510602899	119433
5d50ec7d1e7fd20eaac4fc07f54de94009dd770c	arch-comp18 category report: stochastic modelling		This report presents the results of a friendly competition for formal verification and policy synthesis of stochastic models. The friendly competition took place as part of the workshop Applied Verification for Continuous and Hybrid Systems (ARCH) in 2018. In this first edition, we present five benchmarks with different levels of complexities and stochastic flavours. We make use of six different tools and frameworks (in alphabetical order): Barrier Certificates, FAUST, FIRM-GDTL, Modest, SDCPN modelling & MC simulation and SReachTools; and attempt to solve instances of the five different benchmark problems. Through these benchmarks, we capture a snapshot on the current state-of the art tools and frameworks within the stochastic modelling domain. We also present the challenges encountered within this domain and highlight future plans which will push forward the development of more tools and methodologies for performing formal verification and optimal policy synthesis of stochastic processes.	benchmark (computing);faust;formal verification;hybrid system;simulation;snapshot (computer storage);stochastic modelling (insurance);stochastic process	Alessandro Abate;Henk Blom;Nathalie Cauchi;Sofie Haesaert;Arnd Hartmanns;Kendra Lesser;Meeko M. K. Oishi;Caroline Auricoste;Sadegh Soudjani;Cristian Ioan Vasile;Abraham P. Vinod	2018				AI	-12.762833651113205	-3.868633591675979	119467
5dfe4e4b7a4c665272ed055a08976a6a69e48b44	from qualitative to quantitative physics		Sacks and Doyle have done the A1 community a service by spelling out the limitations of the qualitative physics/reasoning/simulation approach (SPQR). SPQR is one of those approaches that initially looked neat, but did not pan out in practice. Sacks and Doyle note that “. . . there are other simple and common systems that SPQR still cannot comprehend after IS years of investigation.” I believe these meager achievements are largely a result of an artificial distinction between qualitative and quantitative reasoning built into the SPQR approach. This distinction inhibits smooth mixing of qualitative to quantitative reasoning as needed. An alternative approach is to regard a qualitative prediction (e.g., pressure P1 will increase) as a weakly informative quantitative prediction, by representing our knowledge of P1 as a flat probability density between the minimum and maximum pressure. Such a representation says in effect that we know nothing about P1 other than that it is positive. Better knowledge about the value of P1 can be expressed as a peaked distribution around some mean value-the better the knowledge, the sharper the peak. Such a probabilistic representation allows reasoning at a level of detail appropriate to the problem at hand, and in effect claims that all system knowledge is quantitative to some degree. As Sacks and Doyle point out, an expert trying to design/diagnose/understand/controVmodeY . . . a complex system uses an intimate mixture of both qualitative and quantitative reasoning. However, instead of further SPQR bashing, I want to expand on the method of combining both quantitative and qualitative reasoning into a coherent whole, as a constructive extension of Sacks and Doyle’s critique. Attempts have been made to graft quantitative knowledge onto basically qualitative systems using ranges and inequalities (e.g., Simmons 1986; Kuipers and Berleant 1988). While this is clearly an improvement over purely qualitative approaches, it has a number of problems. For example, the all-or-nothing nature of a range means that a slight change in the range associated with say a pressure variable can lead to qualitatively different predictions (e.g., that a relief valve will open)-brittle behavior that is purely the result of using ranges and logical inference where it is not appropriate. In a smooth probabilistic representation of the same situation, a11 that changes is the probability that the relief valve will open. A desirable criterion for any system that represents and reasons about complex situations is that the system should be able to smoothly adjust the accuracy of its reasoning to one that is appropriate for answering the current question. Unfortunately, even SPQR augmented with ranges does not meet this criterion, because if the ranges are reduced to points (no uncertainty) the SPQR system is still doing range arithmetic, not numerical simulation. In other words, when uncertainty is added to a standard differential equation system model, SPQR requires an abrupt change in the type of inference performed. Much more serious is that the proposed inference procedures for these quantitative ranges can lead to erroneous conclusions. For any set of mutually constrained variables	coherence (physics);complex system;computer simulation;grafting (decision trees);information;level of detail;spqr tree;smoothing	Peter C. Cheeseman	1992	Computational Intelligence	10.1111/j.1467-8640.1992.tb00348.x	computer science;machine learning;artificial intelligence	AI	-13.736126671632714	2.1873105739929914	119537
d517c9b83e80dec214f0c7c769bda7dda5023e26	arrow's possibility theorem for one-dimensional single-peaked preferences	single peaked preference;satisfiability;arrovian social choice one dimensional continuum single peaked preferences;social choice;social welfare function;independence of irrelevant alternatives;majority voting	In one-dimensional environments with single-peaked preferences we consider social welfare functions satisfying Arrow’s requirements, i.e. weak Pareto and independence of irrelevant alternatives. When the policy space is a one-dimensional continuum such a welfare function is determined by a collection of 2N strictly quasi-concave preferences and a tie-breaking rule. As a corollary we obtain that when the number of voters is odd, simple majority voting is transitive if and only if each voter’s preference is strictly quasiconcave. © 2008 Elsevier Inc. All rights reserved. JEL classification: D70; D71	concave function;pareto efficiency;quasiconvex function;relevance;requirement;triune continuum paradigm	Lars Ehlers;Ton Storcken	2008	Games and Economic Behavior	10.1016/j.geb.2008.02.005	may's theorem;majority rule;unrestricted domain;social choice theory;economics;arrow's impossibility theorem;mathematical economics;welfare economics;satisfiability	AI	-6.523431729095639	-1.8079932604484095	119546
b1690e751c3039fc84386a5ecd4e964eb8cfc37a	the proportional lottery protocol is strongly β-participatory and vnm-strategy-proof	stable set	A voting protocol is said to be strongly participatory if for any player i and any strategy profile either the outcome is i‘s preferred one or i has a strategy which would ensure her a better outcome, and VNMstrategy proof if at any preference profile the set of sincere strategies of each player is a VNM-stable set. It is shown that the proportional lottery (PL) modular voting protocol is both strongly participatory and VNMstrategy proof. Jel Classification: D70,D71 Stefano Vannucci, Dipartimento di Economia Politica, Università degli Studi di Siena	expected utility hypothesis	Stefano Vannucci	2008	IGTR	10.1142/S021919890800187X	independent set;economics;mathematics;welfare economics	AI	-6.437828968651616	-2.8402956251298845	119761
264ef4001db964929792479367c2ba26488674b0	determining causes and severity of end-user frustration	user interface;user perception;time delay;network connectivity;user experience;technical report;web browsing;word processing	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Irina Ceaparu;Jonathan Lazar;Katie Bessière;John P. Robinson;Ben Shneiderman	2004	Int. J. Hum. Comput. Interaction	10.1207/s15327590ijhc1703_3	user experience design;simulation;human–computer interaction;computer science;technical report;operating system;user interface;world wide web	Robotics	-16.366350284306474	-6.067134934628708	119859
cbddd6312003887cad14734fa01157b52882a62c	fairness in group identification		We study the problem of classifying individuals into groups, using agents’ opinions on who belong to which group as input. Our focus is on the rules that satisfy equal treatment of equals, a minimal fairness property, in addition to independence of irrelevant opinions and non-degeneracy. We show that a rule satisfies the three axioms if and only if it is the liberal rule, a strong one-vote rule, a one-row rule, or a one-column rule. The last three families of rules can be ruled out by simple, intuitive properties. Thus, invoking equal treatment of equals, which is substantially weaker than symmetry, we obtain a characterization of the liberal rule.	fairness measure	Wonki Jo Cho	2018	Mathematical Social Sciences	10.1016/j.mathsocsci.2018.04.002	mathematical economics;welfare economics;mathematics;liberalism;axiom;if and only if	ECom	-6.722729259424084	-2.5674161464086045	120087
0271b18539b50c2aaeb14d0c9d2a586f14128367	welfare ratios in one-sided matching mechanisms		We study the Price of Anarchy of mechanisms for the well-known problem of one-sided matching, or house allocation, with respect to the social welfare objective. We consider both ordinal mechanisms, where agents submit preference lists over the items, and cardinal mechanisms, where agents may submit numerical values for the items being allocated. We present a general lower bound of ?(?n) on the Price of Anarchy, which applies to all mechanisms and we show that a very well-known mechanisms, Probabilistic Serial achieves a matching upper bound. We extend our lower bound to the Price of Stability of a large class of mechanisms that satisfy a common proportionality property	pattern matching	George Christodoulou;Aris Filos-Ratsikas;Søren Kristoffer Stiil Frederiksen;Paul W. Goldberg;Jie Zhang;Jinshan Zhang	2015	CoRR		economics;operations management;mathematical economics;welfare economics	ECom	-5.117446466605354	-3.0943890318508718	120831
564cd051359814a9b8eee9250128b291efeb83ea	reactive and automatic behavior in plan execution	probability of error;average case analysis	"""Much of the work on execution assumes that the agent constantly senses the environment, which lets it respond immediately"""" to errors or unexpected events. In this paper, we argue that this purely reactive strategy is only optimal if sensing is inexpensive, and wc formulate a simple model of execution that incorporates the cost of sensing. We present, an average-case analysis of this model, which shows that in domains with high sensing cost or low probability of error, a more ’automatic’ strategy one with long inter~als between sensing can lead to less expensive execution. The analysis also shows that the distance to the goal has no effect on the optimal sensing interval. These results run counter to the prevailing wisdom in the planning community, but they promise a more balanced approach to the interleaving of execution and sensing. Reactive and Automatic Execution Much of the recent research on plan execution and control has focused on reactive systems. Onc central characteristic of such approaches i that the agent senses the environment on each time step, thus ensuring that it can react promptly to any errors or othcr unexpected events. This holds whether the agent draws on largescale knowledge structures, such as plans (Howe & Cohen, 1991)or cases (Hammond, Converse, & Marks, 1988), or bases its decisions on localized structures, such as control rules (Bresina, Drummond, & Kedar, 1993: Grefenstette, Ramsey, & Schultz, 1990) or neural networks (Sutton, 1988; Kaelbling, 1993). However, human beings still provide the best examples of robust physical agents, and the psychological literature reveals that humans do not always behave in a reactive manner. People can certainly operate in reactive or ’closed-loop’ mode, which closely couples execution with sensing (Adams, 1971). But at least in some domains, humans instead operate in automatic or ’openloop’ mode, in which execution proceeds in the absence of sensory feedback (Schmidt, 1982). Thus, at least some contexts, successful agents appear to prefer nonreactive strategies to reactive ones) One explanation for this phenomenon is that there exists a tradeoff between the cost of sensing, which models of reactivc agents t~-pically ignore, and the cost of errors that occur during execution. For some situations, the optimal sensing strategy is completely reactive behavior, in which the agent observes the environment after each execution step. For other situations, the best strategy is completely automatic behavior, in which execution occurs without sensing. In most cases, the optimum wiU presumably fall somewhere between these two extremes, with the agent sensing the world during execution, but not after every step. There exist other reasons for preferring automatic to reactive behavior in some situations. At least for humans, the former appears to require fewer attentional resources, which lets them execute multiple automatic procedures in parallel. Humans also exhibit a well-known tradeoff between speed and accuracy, and in some cases one may desire an automated, rapid response to a reactive, accurate one. However, our goal here is not to provide a detailed account of human execution strategies, but to better understand the range of such strategies and the conditions under which they are appropriate. Thus, we wiU focus on the first explanation above, which assigns an explicit cost to the sensing process. In the following pages, we attempt to formalize the tradeoff between the cost of sensing and the cost of errors, and to identify the optimal position for an agent to take along the continuum from closed-loop, reactive behavior to open-loop, automatic behavior. In the next section, we present an idealized model of execution that takes both factors into account, followed by an analysis 1 Note that the distinction between reac.ti~e and automatic behavior is entirely different from the more common distinction between reaction and deliberation. The former deals cxclusively with sensing strategies during execution, whereas the latter contrasts execution with plan generation. LANGLEY 299 From: AIPS 1994 Proceedings. Copyright © 1994, AAAI (www.aaai.org). All rights reserved. of this model. After this, we present some theoretical curves that illustrate the behavioral implications of the analysis. Finally, we discuss related work on sensing and execution, along with some prospects for future research. A Model of Execution Cost We would like some model of execution that lets us understand the tradeoff between the cost of sensing and the cost of errors. Of course, any model is necessarily an idealization of some actual situation, and from the many possible models, we must selcct one that is simultaneously plausible and analytically tractable. Thus, we will begin with a realistic scenario and introduce some simplifying assumptions that we hope retain the essential characteristics. One common problem that involves physical agents is robot navigation. In some approaches, the agent retrieves or generates a plan for moving through an environment, then executes this plan in the physical world. Unfortunately, the plan does not always operate as desired. One source of divergence from the planned path comes from actuator error: a command to turn 35 degrees or to move 5.2 meters ahead may not execute exactly as specified. Another source of divergence comes from external forces: another agent may bump into the robot or aa unexpected slope may alter its direction. Similar issues arise in the control of planes and boats, where malfunctioning effectors or unpredictable forces like wind can take the craft off the planned course. In the standard reactive control regimen, the agent senses the environment on every time step, detects errors or divergences as soon as they occur, anti takes action to put the agent back on the desired path.2 The quality of the resulting plan execution takes into account the number of steps required to reach the goal or some similar measure. In a more general framework, execution quality also takes into account the cost c of sensing, which discourages a rational agent from unnecessary sensing and leads it to sample the environment only after every s time steps. This sensing cost m~."""" actually add to the execution time, or it may draw on other resources; here we care only that it somehow contributes to the overall"""	aips;artificial neural network;best, worst and average case;cobham's thesis;feedback;forward error correction;humans;physical symbol system;probabilistic analysis of algorithms;rational agent;risk management;robotic mapping;run time (program lifecycle phase);schmidt decomposition;thermal copper pillar bump;triune continuum paradigm	Pat Langley;Wayne Iba;Jeff Shrager	1994			real-time computing;simulation;computer science;operations management	AI	-11.80921360066117	-2.613333337037778	121014
4a9e5a32dd6c0d2d5274318d254682d3b500395c	an empirical study of diagrammatic inference process by recording the moving operation of diagrams		"""In this study, we investigate how people manipulate diagrams in logical reasoning, especially no valid conclusion (NVC) tasks. In NVC tasks, premises are given and people are asked to judge whether """"no consequence can be drawn from the premises."""" Here, we introduce a method of asking participants to directly manipulate instances of dia- grammatic objects as a component of inferential processes. We observed how participants move Euler diagrams, presented on a PC monitor, to solve syllogisms with universally quantified sentences. In the NVC tasks, 88.6% of our participants chose to use an enumeration strategy with mul- tiple configurations of conclusion diagrams and/or a partial-overlapping strategy of placing two circles. Our results provide evidence that NVC judgment for tasks with diagrams can be reached using an efficient way of counter-example construction."""	diagram	Yuri Sato;Yuichiro Wajima;Kazuhiro Ueda	2014		10.1007/978-3-662-44043-8_21	computer science;artificial intelligence;data mining;algorithm	HCI	-14.115013885847535	2.1760499608057153	121091
873822ec98765dc783dff1e2fd2390955fd857a7	aspects of dependence in generalized farlie-gumbel-morgenstern distributions	farlie gumbel morgenstern family;negative and positive quadrant dependence;secondary 62e15 62e20;u statistics;primary 62g10 62g05 62h20;copula functions	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Mohammad Amini;H. Jabbari;Gholamreza Mohtashami Borzadran	2011	Communications in Statistics - Simulation and Computation	10.1080/03610918.2011.568149	u-statistic;econometrics;calculus;mathematics;statistics	Robotics	-14.227052912066888	-5.7064632511114155	121259
a8d1b1008c72dbb0df3a427975ac1bb563342936	content reference: reasoning about rules		In a companion paper [1] we described the concept of saturation, the situation in a program in which so many knowledge sources (KSs) are potentially useful at each invocation cycle that it is unrealistic to consider unguided, exhaustive invocation. We argued that saturation appears almost inevitable in large AI programs.#R##N##R##N#We suggested that the process of invocation can be viewed as occurring in three steps: retrieval (selecting some subset of KSs from the knowledge base), refinement (pruning or reordering that subset), and execution (executing one or more of the KSs). We then argued that one useful approach to dealing with saturation is by embedding intelligence in the refinement phase, and described meta-rules, a means of encoding knowledge used to effect refinement.#R##N##R##N#In this paper we consider a more detailed, ‘engineering’ issue, but one with a number of interesting implications: While there are many ways to implement refinement, we suggest that one particular technique—which we call content reference—offers a number of advantages, including giving the system some ability to reason about the content of its knowledge.		Randall Davis	1980	Artif. Intell.	10.1016/0004-3702(80)90044-2	computer science;artificial intelligence;data mining;algorithm	AI	-18.879291399971795	2.166951314297353	121414
1cb83ea524bced585267fafbfb9cacba01ab922d	theories of coalitional rationality	normal form games coalitional agreements coordination epistemic foundations of solution concepts;satisfiability;solution concept;normal form game	This paper generalizes the concept of best response to coalitions of players and offers epistemic definitions of coalitional rationalizability in normal form games. The (best) response of a coalition is defined to be an operator from sets of conjectures to sets of strategies. A strategy is epistemic coalitionally rationalizable if it is consistent with rationality and common certainty that every coalition is rational. A characterization of this solution set is provided for operators satisfying four basic properties. Special attention is devoted to an operator that leads to a solution concept that is generically equivalent to the iteratively defined concept of coalitional rationalizability.	drew mcdermott;eddie (text editor);microsoft lumia;pareto efficiency;principle of rationality;requirement;tomotaka takahashi;weak formulation	Attila Ambrus	2009	J. Economic Theory	10.1016/j.jet.2007.03.010	economics;rationalizability;mathematics;normal-form game;mathematical economics;welfare economics;solution concept;satisfiability	AI	-6.45485662165752	-1.043554551648091	121548
20bb07549825b7a87d49421f8db8fb7710c4fa8a	a mutual influence algorithm for multiple concurrent negotiations - a game theoretical analysis	bargaining power;nash equilibrium;theoretical analysis	Buyers always want to obtain goods at the lowest price. To achieve this, a buyer agent can have multiple concurrent negotiations with all the sellers. It is obvious that when the buyer obtains a new low offer from one of the sellers during negotiation, the buyer should have more bargaining power in negotiating with other sellers, as other sellers should offer a even lower price in order to make a deal. In this way, the concurrent negotiations mutually influence one another. In this paper, we present an algorithm to enable mutual influence among multiple concurrent negotiations. We show that the outcome of this algorithm is in Nash equilibrium when we model and analyze the negotiation as a game.	algorithm;game theory	Ka-man Lam;Ho-fung Leung	2005		10.1007/11552451_136	industrial organization;economics;microeconomics;commerce	AI	-8.116320918249274	-7.3346539867729685	121567
d0b22bbe0e642c28eca1829ddb3c6e9e3258dcfd	distributed coordination of emergency medical service for angioplasty patients	angioplasty;68w15;ems coordination;emergency medical assistance;distributed optimization;ambulance coordination;90c27;90b50;pci;auction algorithm	In this paper we study the coordination of Emergency Medical Service (EMS) for patients with acute myocardial infarction with ST-segment elevation (STEMI). This is a health problem with high associated mortality. A “golden standard” treatment for STEMI is angioplasty, which requires a catheterization lab and a highly qualified cardiology team. It should be performed as soon as possible since the delay to treatment worsens the patient’s prognosis. The decrease of the delay is achieved by coordination of EMS, which is especially important in the case of multiple simultaneous patients. Nowadays, this process is based on the First-Come-First-Served (FCFS) principle and it heavily depends on human control and phone communication with high proneness to human error and delays. The objective is, therefore, to automate the EMS coordination while minimizing the time from symptom onset to reperfusion and thus to lower the mortality and morbidity resulting from this disease. In this paper, we present a multi-agent decision-support system for the distributed coordination of EMS focusing on urgent out-of-hospital STEMI patients awaiting angioplasty. The system is also applicable to emergency patients of any pathology needing pre-hospital acute medical care and urgent hospital treatment. The assignment of patients to ambulances and angioplasty-enabled hospitals with cardiology teams is performed via a three-level optimization model. At each level, we find a globally efficient solution by a modification of the distributed relaxation method for the assignment problem called the auction algorithm. The efficiency of the proposed model is demonstrated by simulation experiments.	assignment problem;auction algorithm;decision support system;experiment;global positioning system;guidance system;human error;instability;iterative method;linear programming relaxation;mathematical optimization;mobile phone;multi-agent system;onset (audio);relaxation (approximation);requirement;responsiveness;seamless3d;simulation;software agent;ws-coordination	Marin Lujak;Holger Billhardt;Sascha Ossowski	2016	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-016-9507-9	auction algorithm;simulation;computer science;conventional pci	AI	-13.152833967077429	-9.10241834747965	121777
e11914569935758677c448eb9438d7213e70ded7	uncertainty, efficiency and incentive compatibility: ambiguity solves the conflict between efficiency and incentive compatibility		Abstract A fundamental result of modern economics is the conflict between efficiency and incentive compatibility, that is, the fact that some Pareto optimal (efficient) allocations are not incentive compatible. This conflict has generated a huge literature, which almost always assumes that individuals are expected utility maximizers. What happens if they have other kind of preferences? Is there any preference where this conflict does not exist? Can we characterize those preferences? We show that in an economy where individuals have complete, transitive, continuous and monotonic preferences, every efficient allocation is incentive compatible if and only if individuals have maximin preferences.		Luciano Irineu de Castro;Nicholas C. Yannelis	2018	J. Economic Theory	10.1016/j.jet.2018.02.008	expected utility hypothesis;microeconomics;economics;incentive compatibility;ambiguity aversion;ambiguity;mechanism design;pareto principle;transitive relation;information asymmetry	AI	-5.726183518884428	-2.6391498594263507	121835
bfe6a64548cfaa793bdc86d6beff38e9be877cca	games on convex geometries with a coalition structure	coalition structure;cooperative game;convex geometry;core;shapley value	In this study, the model of games on convex geometries with a coalition structure is introduced, where the players can participate in different unions. The Shapley value for games on convex geometries with a coalition structure is researched, which can be seen as an extension of the Shapley value for games on convex geometries and that for the traditional games, respectively. By establishing the associated axiomatic systems, the existence and uniqueness of the given Shapley value is proved. Meantime, the core for this kind of games is defined, and an equivalent form is studied. When the given game is convex, some properties are researched.	antimatroid	Fanyong Meng	2013	IJMIC	10.1504/IJMIC.2013.055431	bondareva–shapley theorem;convex geometry;core;mathematical optimization;mathematics;shapley value;mathematical economics	ECom	-6.200475704708658	-0.40325767872705676	121843
e82659aca4f2122cc8c16ff34cf25ab9c80666e1	ai rules: okay?	normativity rules skill habit ai simulation	This remark exactly expresses the attitude I propose to adopt to the philosophical critique of AI. In this case the oversimplification to which I wish to draw attention concerns the way in which the normativity of human cognition is dealt with in rule-AI. The upshot is not some jejune criticism of the idea of modelling cognition in machines, but a closer look at some of the assumptions involved in doing it. An examination of these assumptions discloses certain limitations to the precision with which moment by moment exercises of cognitive skills can be simulated.		Rom Harre	1996	J. Exp. Theor. Artif. Intell.	10.1080/095281396147401	computer science;artificial intelligence	ML	-13.636687639090491	2.5079634861436833	121856
383e89ea95c7b777a3eab972bcaca2bae044c34a	group learning and opinion diffusion in a broadcast network	multiarmed bandit problems group learning opinion diffusion broadcast network randomly generated rewards statistics expected total reward maximization online learning process regret learning problem setting;statistical analysis;learning artificial intelligence;statistical analysis learning artificial intelligence	We analyze the following group learning problem in the context of opinion diffusion: Consider a network with M users, each facing N options. In a discrete time setting, at each time step, each user chooses K out of the N options, and receive randomly generated rewards, whose statistics depend on the options chosen as well as the user itself, and are unknown to the users. Each user aims to maximize their expected total rewards over a certain time horizon through an online learning process, i.e., a sequence of exploration (sampling the return of each option) and exploitation (selecting empirically good options) steps. Different from a typical regret learning problem setting (also known as the class of multi-armed bandit problems), the group of users share information regarding their decisions and experiences in a broadcast network. The challenge is that while it may be helpful to observe others' actions in one's own learning (i.e., second-hand learning), what is considered desirable option for one user may be undesirable for another (think of restaurant choices), and this difference in preference is in general unknown a priori. Even when two users happen to have the same preference (e.g., they agree one option is better than the other), they may differ in their absolute valuation of each individual option. Within this context we consider two group learning scenarios, (1) users with uniform preferences and (2) users with diverse preferences, and examine how a user should construct its learning process to best extract information from others' decisions and experiences so as to maximize its own reward. Performance is measured in weak regret, the difference between the user's total reward and the reward from a user-specific best single-action policy (i.e., always selecting the set of options generating the highest mean rewards for this user). Within each scenario we also consider two cases: (i) when users exchange full information, meaning they share the actual rewards they obtained from their choices, and (ii) when users exchange limited information, e.g., only their choices but not rewards obtained from these choices. We show the gains from group learning compared to individual learning from one's own choices and experiences.	experience;exploit (computer security);multi-armed bandit;online machine learning;procedural generation;regret (decision theory);sampling (signal processing);value (ethics)	Yang Liu;Mingyan Liu	2013	2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/Allerton.2013.6736706	preference learning;computer science;machine learning;mathematics;management science;statistics	AI	-7.052402335458865	-5.704391093409795	122090
047ca1935aa5dd25459285e9a0bcbdec8c7c80bb	probabilistic event calculus for event recognition	uncertainty;probabilistic inference;machine learning;events	Symbolic event recognition systems have been successfully applied to a variety of application domains, extracting useful information in the form of events, allowing experts or other systems to monitor and respond when significant events are recognised. In a typical event recognition application, however, these systems often have to deal with a significant amount of uncertainty. In this article, we address the issue of uncertainty in logic-based event recognition by extending the Event Calculus with probabilistic reasoning. Markov logic networks are a natural candidate for our logic-based formalism. However, the temporal semantics of the Event Calculus introduce a number of challenges for the proposed model. We show how and under what assumptions we can overcome these problems. Additionally, we study how probabilistic modelling changes the behaviour of the formalism, affecting its key property—the inertia of fluents. Furthermore, we demonstrate the advantages of the probabilistic Event Calculus through examples and experiments in the domain of activity recognition, using a publicly available dataset for video surveillance.	activity recognition;closed-circuit television;event calculus;experiment;fluent (artificial intelligence);formal system;markov chain;markov logic network;probabilistic database;semantics (computer science);statistical model	Anastasios Skarlatidis;Georgios Paliouras;Alexander Artikis;George A. Vouros	2015	ACM Trans. Comput. Log.	10.1145/2699916	uncertainty;computer science;artificial intelligence;complex event processing;machine learning;data mining;mathematics;probabilistic logic;event tree analysis	AI	-18.744718037724343	3.1594714209116264	122095
5007d72ee678a24ba09400f35a242a31935365f1	developing a distributed drone delivery system with a hybrid behavior planning system		The demand for fast and reliable parcel shipping is globally rising. Conventional delivery by land requires good infrastructure and causes high costs, especially on the last mile. We present a distributed and scalable drone delivery system based on the contract net protocol for task allocation and the ROS hybrid behaviour planner (RHBP) for goal-oriented task execution. The solution is tested on a modified multi-agent systems simulation platform (MASSIM). Within this environment, the solution scales up well and is profitable across different configurations.		Daniel Krakowczyk;Jannik Wolff;Alexandru Ciobanu;Dennis Julian Meyer;Christopher-Eyk Hrabia	2018		10.1007/978-3-030-00111-7_10	last mile;real-time computing;scalability;drone;contract net protocol;multi-agent system;systems simulation;business	Robotics	-13.506267843937682	-8.849932284540074	122155
49e569f1509b66fdeb930268c607e20f3dceb28d	ramification analysis using causal mapping	ramification analysis;causal mapping;knowledge engineering;knowledge representation and reasoning;default logic;intelligent agent;non monotonic logic	"""To operate in the real-world, intelligent agents constantly need to absorb new information, and to consider the ramiications of it. This raises interesting questions for knowledge representation and reasoning. Here we consider ramiication analysis in which we wish to determine both the likely outcomes from events occuring and the less likely, but very signiicant outcomes, from events occuring. To formalize ramiication analysis, we introduce the notion of causal maps for modelling \causal relationships"""" between events. In particular, we consider exis-tential event classes, for example presidential-election, with instances being true, false, or unknown, and directional events classes, for example inflation, with instances being increasing , decreasing or unchanging. Using causal maps, we can propagate new information to determine possible ramiications. These ramiications are also described in terms of events. Whilst causal maps ooer a lucid view on ramiications, we also want to support automated reasoning, to address problems of incompleteness, and to represent further conditions on ram-iications. To do this, we translate causal maps into default logic, and use the proof theory and automated reasoning technology of default logic. In this paper, we provide a syntax and semantics for causal mapping, and a translation into default logic, and discuss an integration of the approach with langauge engineering."""	automated reasoning;causal filter;default logic;intelligent agent;knowledge representation and reasoning;lucid;mind map;ramification problem;random-access memory;reasoning system;while	Anthony Hunter	2000	Data Knowl. Eng.	10.1016/S0169-023X(99)00030-0	knowledge representation and reasoning;computer science;artificial intelligence;non-monotonic logic;knowledge engineering;data mining;database;programming language;default logic;intelligent agent;algorithm	AI	-18.535175208995746	2.312992885383187	122291
4a969406a6cc876c95e15c9f71106f1130643f8a	porting of a wildfire risk and fire spread application into a cloud computing environment	fire behavior;forest fires;risk assessment;parallel processing;cloud computing	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	cloud computing;francis;primary source	Kostas Kalabokidis;Nikolaos Athanasis;Christos Vasilakos;Palaiologos Palaiologou	2014	International Journal of Geographical Information Science	10.1080/13658816.2013.858257	risk assessment;parallel processing;simulation;cloud computing;geography;computer science;remote sensing	Robotics	-15.205662305906717	-5.91589629065154	122333
0c3ac09fa55d7d0c231a7d20a8f93adc72158522	on the rationality of our response to testimony	philosophy of science;truth;belief;epistemologie;credulite;verite;personne;disposition;acceptability;proposition;raison;croyance;temoignage;person;epistemology;contexte;acceptabilite;testimony;reason;context	The assumption that we largely lack reasons for accepting testimony has dominated its epistemology. Given the further assumption that whatever reasons we do have are insufficient to justify our testimonial beliefs, many conclude that any account of testimonial knowledge must allow credulity to be justified. In this paper I argue that both of these assumptions are false. Our responses to testimony are guided by our background beliefs as to the testimony as a type, the testimonial situation, the testifier's character and the truth of the proposition testified to. These beliefs provide reasons for our responses. Thus, we usually do have reasons, in the sense of propositions believed, for accepting testimony and these reasons can provide evidence for the testimonial beliefs we form.	rationality	Paul Faulkner	2002	Synthese	10.1023/A:1016116728471	philosophy of science;philosophy;epistemology;belief;disposition;person;reason	NLP	-13.584776953725532	3.9604893743395957	122566
3577a7f837c919e9fcc34b841178e5838000294e	double implementation of linear cost share equilibrium allocations	cost sharing;public good	In this paper we consider the problem of double implementation of Linear Cost Share Equilibrium (LCSE) allocations by a feasible and continuous mechanism whose Nash allocations and strong Nash allocations coincide with Linear Cost Share Equilibrium allocations. The mechanism presented here allows preferences and initial endowments as well as coalition patterns to be privately observed, a feature missing from much recent work in implementation theory. Since LCSE contains Lindahl equilibrium and Ratio equilibrium as special cases, it doubly implements these two equilibria allocations. Further, if one reinterprets the commodity space, this mechanism also doubly implements Walrasian allocations for private goods economies. Thus, the mechanism given in the paper appears to represent a ‘generic’ mechanism to doubly implement market-type allocations in private and/or public goods economies.  2000 Elsevier Science B.V. All rights reserved. JEL classification: C72; D61; D78	nash equilibrium	Guoqiang Tian	2000	Mathematical Social Sciences	10.1016/S0165-4896(99)00047-5	public good;economics;public economics;microeconomics;mathematical economics;welfare economics	AI	-4.917255819371956	-3.389128148971389	122601
b37505d562c8e79ab63aa96e36bf28d86918ca75	disagreement, evidence, and agnosticism		In this paper, I respond to recent attempts by philosophers to deny the existence of something that is both real and significant: reasonable disagreements between epistemic peers. In their arguments against the possibility of such disagreements, skeptical philosophers typically invoke one or more of the following: indifference reasoning, equal weight principles, and uniqueness theses. I take up each of these in turn, finding ample reason to resist them. The arguments for indifference reasoning and equal weight principles tend to overlook the possibility of a certain kind of agnostic credal state which I call deep agnosticism, the possibility of which derails the arguments. The arguments for uniqueness theses tend to invoke a flawed understanding of the evidential support relation. When these problems and misunderstandings are brought into the light and corrected, the threat to reasonable disagreement vanishes.	uniqueness type	Jason Decker	2011	Synthese	10.1007/s11229-011-0010-x	philosophy;epistemology;mathematics	AI	-13.166852892101902	3.3098288745654285	122799
15c7d9b90adfd347dd3a252be6ea8b2dcfa242eb	evidential reasoning compared in a network usage prediction testbed: preliminary report	network usage prediction;preliminary report;evidential reasoning	Publisher Summary This chapter reports on empirical work aimed at comparing evidential reasoning techniques. While there is prima facie evidence for some conclusions, this is work in progress. The domain is a network of UNIX* cycle servers and the task is to predict the properties of the state of the network from partial descriptions of the state. Actual data from the network are taken and used for blindfold testing in a betting game that allows abstention. The focal technique has been Kyburgu0027s method for reasoning with data of varying relevance to a particular query, though the aim is to be able eventually to compare various uncertainty calculi. The conclusions are not novel but are instructive. (1) All of the calculi performed better than human subjects; therefore, unbiased access to sample experience is apparently of value. (2) Performance depends on metric: (a) when trials are repeated, net = gains − losses favors methods that place many bets, if the probability of placing a correct bet is sufficiently high; that is, it favors point-valued formalisms; (b) yield = gains /(gains + losses) favors methods that bet only when sure to bet correctly; that is, it favors interval-valued formalisms. (3) Among the calculi, there were no clear winners or losers. Methods are identified for eliminating the bias of the net as a performance criterion and for separating the calculi effectively: in both cases by posting odds for the betting game in the appropriate way.	testbed	Ronald Prescott Loui	1988			epistemology;computer science;artificial intelligence;machine learning;data mining;mathematics;evidential reasoning approach;algorithm;statistics	AI	-14.979263954323596	-0.05295491216524777	122931
ead043721045de61ee2e548009d9fd2200efca1c	cusum procedures for monitoring process mean and variability	62p30;statistical process control;cumulative sum;65c20;likelihood ratio test	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	cartesian perceptual compression;chart;francis;heart rate variability;mean shift;outlook.com;primary source;recursion;sampling (signal processing)	Xin Zhang;Jiujun Zhang	2013	Communications in Statistics - Simulation and Computation	10.1080/03610918.2012.665550	econometrics;process capability;likelihood-ratio test;cusum;mathematics;shewhart individuals control chart;statistical process control;statistics	Robotics	-14.124790533647651	-5.729284691535338	123087
0e9bdc5df968152c2f4356d3f9d116703432cd27	stability in role based hedonic games	conp complete;computational complexity;np complete;hedonic games	In the hedonic coalition formation game model Roles and Teams Hedonic Games (RTHG) (Spradling et al. 2013), agents view teams as compositions of available roles. An agent’s utility for a partition is based upon which role she fulfills within the coalition and which additional roles are being fulfilled within the coalition. Goals for matchmaking in this setting include forming partitions which optimize some function of the utility. Optimization problems related to finding Perfect, MaxSum and MaxMin partitions in RTHG are all known to be NP-hard. In this paper, we introduce a Role Based Hedonic Game model (RBHG) which has no fixed team size and a more relaxed set of compositions. We consider the related problem of stability in RBHG. Given a set of available movements for agents, a partition is stable iff no agent would choose to move from the partition to another partition. We show NP-completeness for several RBHG stability problems and coNP-completeness for two verification prob-	algorithm;approximation;co-np;heuristic;mathematical optimization;minimax;np-completeness;np-hardness;nash equilibrium;pareto efficiency;perfect;software bug;time complexity;utility;word lists by frequency	Matthew Spradling;Judy Goldsmith	2015			mathematical optimization;np-complete;computer science;computational complexity theory	AI	-5.556350652980024	0.4166087911120189	123146
6dd1c8f198334aab1b639df9d15d664f2c8ae2c6	a theory of kindness, reluctance, and shame for social preferences	behavior modeling;social preferences;dictator game	Recent experimental evidence from dictator games suggests that proposers take money from receivers when taking is an option, and that many proposers are reluctant to play the game. This paper proposes a behavioral model with two components: a choice correspondence that depends on the endowed allocation and the menu of allocations available, and a preference ordering over endowment/menu pairs. The choice correspondence governs behavior when the proposer actually plays a game, and the preference ordering governs the proposer’s willingness to play a particular game. The model is then used to characterize notions of proposer kindness, reluctance, and shame. © 2008 Elsevier Inc. All rights reserved. JEL classification: C72; D64	behavioral modeling;behavioral pattern;context-sensitive language;quasiconvex function;random hacks of kindness;risk aversion;social inequality;utility	William S. Neilson	2009	Games and Economic Behavior	10.1016/j.geb.2008.04.004	behavioral modeling;dictator game;economics;social preferences;microeconomics;welfare economics	AI	-6.50064993498212	-3.0521267392772033	123281
4b491394336cc4ff6a3068f89df4364c8839722e	moderate growth time series for dynamic combinatorics modelisation	symbolic computation;multiagent system;time series	Here, we present a family of time series with a simple growth constraint. This family can be the basis of a model to apply to emerging computation in business and micro-economy where global functions can be expressed from local rules. We explicit a double statistics on these series which allows to establish a one-to-one correspondence between three other ballot-like strunctures.	computation;one-to-one (data model);time series	Luaï Jaff;Gérard Duchamp;Hatem Hadj Kacem;Cyrille Bertelle	2006	CoRR		mathematical optimization;combinatorics;discrete mathematics;symbolic computation;time series;mathematics;algebra	ML	-5.345560786530121	1.566537575702987	123496
098c7bcfbec0ee9c8a2a6340f2a261a10a48fbbf	computers play the beer game: can artificial agents manage supply chains?	business strategy;lead time;bullwhip effect;automated supply chains;stochastic demand;genetic algorithm;genetic algorithms;supply chain;beer game;artificial agents	We model an electronic supply chain managed by artificial agents. We investigate whether artificial agents do better than humans when playing the MIT Beer Game. Can the artificial agents discover good and effective business strategies in supply chains both in stationary and non-stationary environments? Can the artificial agents discover policies that mitigate the Bullwhip effect? In particular, we study the following questions: Can agents learn reasonably good policies in the face of deterministic demand with fixed lead time? Can agents cope reasonably well in the face of stochastic demand with stochastic lead time? Can agents learn and adapt in various contexts to play the game? Can agents cooperate across the supply chain?	intelligent agent;stationary process	Steven Orla Kimbrough;Dong-Jun Wu;Fang Zhong	2002	Decision Support Systems	10.1016/S0167-9236(02)00019-2	genetic algorithm;economics;computer science;marketing;operations management;microeconomics;commerce;strategic management	AI	-9.222118620649075	-9.343719088999716	123515
66dfaef868e0278c8b5a0a476bca9befc88ee517	modeling the effects of weather and traffic on the risk of secondary incidents	statistical approach;partial derivatives;pedestrian safety;neural networks;poison control;injury prevention;incident management;safety literature;traffic flow;traffic safety;injury control;home safety;injury research;safety abstracts;human factors;logit;occupational safety;safety;rainfall intensity;weather condition;weather conditions;safety research;mutual information;traffic incidents;accident prevention;violence prevention;bicycle safety;neural network model;poisoning prevention;falls;ergonomics;rainfall;suicide prevention;secondary incidents;neural network;logits	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	activation function;artificial neural network;cobham's thesis;francis;logistic regression;memory-level parallelism;primary source;statistical model	Eleni I. Vlahogianni;Matthew G. Karlaftis;Foteini P. Orfanou	2012	J. Intellig. Transport. Systems	10.1080/15472450.2012.688384	simulation;computer science;engineering;suicide prevention;injury prevention;machine learning;traffic flow;partial derivative;logit;mutual information;forensic engineering;computer security;artificial neural network;incident management	Robotics	-15.135966233652274	-6.45253390823294	123612
9eb06c4eb5bf6d6745e0207f13c513378ad3d6a7	meta-envy-free cake-cutting and pie-cutting protocols	game theory;pie cutting;envy free;cake cutting;meta envy free	This paper discusses cake-cutting protocols when the cake is a heterogeneous good, represented by an interval on the real line. We propose a new desirable property, the meta-envy-freeness of cake-cutting, which has not been formally considered before. Meta-envy-free means there is no envy on role assignments, that is, no party wants to exchange his/her role in the protocol with the one of any other party. If there is an envy on role assignments, the protocol cannot be actually executed because there is no settlement on which party plays which role in the protocol. A similar definition, envy-freeness, is widely discussed. Envy-free means that no player wants to exchange his/her part of the cake with that of any other player’s. Though envy-freeness was considered to be one of the most important desirable properties, envy-freeness does not prevent envy about role assignment in the protocols. We define metaenvy-freeness to formalize this kind of envy. We propose that simultaneously achieving meta-envy-free and envy-free is desirable in cake-cutting. We show that current envy-free cake-cutting protocols do not satisfy meta-envy-freeness. Formerly proposed properties such as strong envy-free, exact, and equitable do not directly consider this type of envy and these properties are very difficult to realize. This paper then shows cake-cutting protocols for two and three party cases that simultaneously achieves envy-free and meta-envy-free. Last, we show meta-envy-free pie-cutting protocols.	communications protocol;cutting stock problem;efficient cake-cutting	Yoshifumi Manabe;Tatsuaki Okamoto	2012	JIP	10.2197/ipsjjip.20.686	game theory;envy-free	AI	-5.907345921226221	-3.2476617260566893	123963
6a3e7720f0858ce15ab13f5c4b649ef4877b410a	algorithmic game theory	cybernetics;game theory;systems theory	s and Brief Announcements Designing Matching Mechanisms under Constraints: An Approach from Discrete Convex Analysis (Extended Abstract) . . . . . . . . . . . . . . . . . . . . . . 291 Fuhito Kojima, Akihisa Tamura, and Makoto Yokoo Monotonicity, Revenue Equivalence and Budgets . . . . . . . . . . . . . . . . . . . . . 292 Ahuva Mu’alem The Price of Spite in Spot-Checking Games (Brief Announcement) . . . . . 293 Guillaume Sagnol, Ralf Borndörfer, Thomas Schlechte, and Elmar Swarat Brief Announcement: A Model for Multilevel Network Games . . . . . . . . . . 294 Sebastian Abshoff, Andreas Cord-Landwehr, Daniel Jung, and Alexander Skopalik Complexity of Optimal Lobbying in Threshold Aggregation (Brief Announcement) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295 Ilan Nehama Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297 Social Welfare in One-Sided Matchings: Random Priority and Beyond Aris Filos-Ratsikas, Søren Kristoffer Stiil Frederiksen, and Jie Zhang 1 Department of Computer Science, Aarhus University {filosra,ssf}@cs.au.dk 2 Department of Computer Science, University of Oxford jie.zhang@cs.ox.ac.uk Abstract. We study the problem of approximate social welfare maxiWe study the problem of approximate social welfare maximization (without money) in one-sided matching problems when agents have unrestricted cardinal preferences over a finite set of items. Random priority is a very well-known truthful-in-expectation mechanism for the problem. We prove that the approximation ratio of random priority is Θ(n−1/2) while no truthful-in-expectation mechanism can achieve an approximation ratio better than O(n−1/2), where n is the number of agents and items. Furthermore, we prove that the approximation ratio of all ordinal (not necessarily truthful-in-expectation) mechanisms is upper bounded by O(n−1/2), indicating that random priority is asymptotically the best truthful-in-expectation mechanism and the best ordinal mechanism for the problem.	aris express;algorithmic game theory;approximation algorithm;computer science;convex analysis;entropy maximization;liang-jie zhang;ordinal data;turing completeness	Ron Lavi	2014		10.1007/978-3-662-44803-8	combinatorial game theory;implementation theory;algorithmic mechanism design;game theory;minimax;algorithmic learning theory;mathematical game;algorithmic game theory;sequential game;game complexity	Theory	-4.625328429333623	0.9946259760755107	124362
4ccfcee35ac6002b17fa045f0e276b9566a507f9	can domain specific knowledge be generalized?	domain specific knowledge	The MECHO p r o j e c t (see Cl]) cons is ts of w r i t ­ ing a computer program which can colve a wide va r ­ i e t y o f simple mechanics problems s ta ted in Eng l i sh . This program is being used as a veh ic le f o r s tudy­ ing methods f o r gu id ing search in a semant ica l ly r i c h domain. Our methodology is to f i n d genera l , j u s t i f i a b l e , in ference ru les which can be combined to ca r ry out the reasoning necessary to solve the mechanics problems. As is we l l known, when ru les l i k e these are run on a general in ference machine the r e s u l t is o f ten a combinator ia l exp los ion . Rules are combined in unexpected ways and the search f o r a s o l u t i o n is developed along unreason­ able paths . These f a i l u r e s are used to debug the ru les by adding to them l o c a l , domain s p e c i f i c c o n t r o l i n fo rma t i on . F i n a l l y these techniques are genera l ized and incorporated in the in ference mechanism. We hope t h a t t h i s methodology w i l l lead us to the design of a computat ional l og i c f o r na tu ra l reasoning. In t h i s paper one such t r a n s i t i o n from domain s p e c i f i c to general in ference technique w i l l be descr ibed. We w i l l use t h i s example to emphasize the importance of t h i s genera l i za t i on stage. W i th ­ out i t one might be led to s u p e r f i c i a l and fa l se conclusions about the nature of na tu ra l reasoning. Suppose we have ava i l ab le the fo l l ow ing r e ­ l a t i o n s : Ve l (Ob jec t , v, t ime) (v is the ve loc ­ i t y o f ob jec t dur ing t i m e ) ; A t (Ob jec t , p l ace , moment) (object is at place at moment); F i n a l ( p e r i o d , moment) (moment is the f i n a l moment of t ime i n t e r v a l p e r i o d ) . We may have discovered the f o l l ow ing domain s p e c i f i c i n ­ format ion enabl ing us to guide the search fo r problem so lu t i ons along successful l i n e s , ( i ) I f the program i s desperate to s a t i s f y Ve l (Ob jec t , v, t i m e ) , ob jec t and t ime being known, but a l l in ferences have ground to a h a l t , then a new in termedia te unknown v can be created and as­ ser ted to be the v e l o c i t y o f ob jec t a t t ime , ( i i ) I f the program i s asked to conf i rm t h a t A t ( o b j e c t , p l a c e l , moment), but i t a l ready knows t ha t ob jec t is at some d i f f e r e n t place (place2) at moment, then the attempt to prove A t ( o b j e c t , p l a c e l , moment) can be abandoned, be­ cause ob jec ts can on ly be in one place at a t ime , ( i i i ) I f the f i n a l moment o f pe r iod is found to be momentl, but l a t e r processing f a i l s , i t i s no use backing up to r eca l cu la te F i n a l ( p e r i o d , ?x ) , since the same answer is bound to be g iven . A t f i r s t glance i t might seem as i f these ex­ amples argued f o r the i n t e r v e n t i o n of r i c h domain s p e c i f i c in fo rmat ion a t a l l c o n t r o l po in t s and t h a t a programming language which f a c i l i t a t e d such i n t e r v e n t i o n was requ i red . But these conclusions are not j u s t i f i e d from the examples ( i ) ( i i i ) above. In f a c t ( i ) ( i i i ) represent d i f f e r e n t facets o f a general phenomenon. To see t h i s no t i ce t h a t V e l , At and F i na l are a l l r e a l l y f unc t i ons : Vel is a f unc t i on from ob jec ts and t imes to v e l o c i t i e s ; At is a f unc t i on from ob jec ts and moments to places and F i n a l is a f unc t i on from per iods to moments. What separates func t ions from other r e l a t i o n s is t ha t they are s ing le va lued, tha t i s t h e i r value is guaranteed to e x i s t and to be unique♦ ( i ) is an example of t h i s existence proper ty being used and ( i i ) and ( i i i ) d i f f e r e n t uses of the unique­ ness p roper t y . The genera l i za t ions o f ( i ) , ( i i ) and ( i i i ) are ( i ) ' I f the program is desperate to f i n d a func­ t i o n value given i t s arguments, and a l l in ferences have f a i l e d then a new e n t i t y can be created and asserted to be t ha t va lue . (Note tha t we do not want the program to create a new e n t i t y whenever it is l ega l to do so as t h i s con t r ibu tes to the com­ b i n a t o r i a l exp los ion . The no t ion of being des­ perate f o r the answer can be genera l ized (see [21). ( i i ) * I f the program is t r y i n g to conf i rm a func­ t i o n va lue , but i t a lready has a con t rad i c to ry value s tored then the conf i rmat ion attempt is to be abandoned. ( i i i ) ' I f the program has ca lcu la ted a f unc t i on value then i t should not reca lcu la te t h i s on back up. Note t h a t ( i ) ' ( i i i ) ' improve on the p red ica te ca lcu lus representa t ion of the existence and unique­ ness of func t ion va lues , by g i v i ng procedural i n ­ format ion about when t h i s in fo rmat ion is to be used. They also improve on the l o c a l domain s p e c i f i c em­ bodiment of ( i ) ( i i i ) by represent ing a large amount of such in fo rmat ion in concise form, e .g . the existence proper ty can now be used on At and F ina l and the uniqueness proper ty can be used on V e l . A l l t h a t i s necessary to share the bene f i t s o f the con t ro l i n fo rmat ion embodied in ( i ) ' ( i i i ) ' i s to spec i fy which r e l a t i o n s have func t i on va lues . For func t ions of one argument the implement­ a t i on of ( i ) ' ( i i i ) ' can be ass is ted by ma in ta in ­ ing a s ing le s l o t on the proper ty l i s t of the argu­ ment. However, something more complicated is needed f o r func t ions of more than one argument. Some r e l a t i o n s may be func t ions in more than one way, e . g . i f T imesys(per iod, ini tmom, finmom) means t h a t initmom is the i n i t i a l moment and finmom is the f i n a l moment of per iod then Timesys is a func t i on in 3 ways: (a) from per iod to Initmom (b) from per iod to finmom (c) from initmom and finmom to pe r iod	apl;artificial intelligence;automatic computing engine;backup;case-based reasoning;co-ment;computer program;exptime;earthbound;emoticon;enhanced graphics adapter;fo (complexity);genera;lu decomposition;large eddy simulation;linear algebra;lookahead carry unit;naruto shippuden: clash of ninja revolution 3;numerical aperture;programming language;radio frequency;spec#	Alan Bundy	1977				AI	-11.487977557787863	2.3826328346614396	124425
be5f6973f4285c14c14df280e61eb6db206c9906	the cognitive act and the first-person perspective: an epistemology for constructive type theory	cognitive act;truth and error;theory of knowledge;type theory;judgement;constructive type theory;point of view	The notion of cognitive act is of importance for an epistemology that is apt for constructive type theory, and for epistemology in general. Instead of taking knowledge attributions as the primary use of the verb ‘to know’ that needs to be given an account of, and understanding a first-person knowledge claim as a special case of knowledge attribution, the account of knowledge that is given here understands first-person knowledge claims as the primary use of the verb ‘to know’. This means that a cognitive act is an act that counts as cognitive from a first-person point of view. The method of linguistic phenomenology is used to explain or elucidate our epistemic notions. One of the advantages of the theory is that an answer can be given to some of the problems in modern epistemology, such as the Gettier problem.	cognition;correctness (computer science);first-person (video games);intuitionistic type theory;receiver operating characteristic;eric	Maria van der Schaar	2009	Synthese	10.1007/s11229-009-9708-4	evolutionary epistemology;epistemology;type theory	NLP	-13.130481216057303	3.0847211644575423	124466
56882e2fe1be33347374f677ede84fed229b8698	evolutionary stability and lexicographic preferences	game theory;nash equilibrium;evolutionarily stable strategy;evolutionary stability;nash equilibria	Abstract   We explore the interaction between evolutionary stability and lexicographic preferences. To do so, we define a  limit Nash equilibrium  for a lexicographic game as the limit of Nash equilibria of nearby games with continuous preferences. Nash equilibria of lexicographic games are limit Nash equilibria, but not conversely. Modified evolutionarily stable strategies (Binmore and Samuelson, 1992. J. Econ. Theory 57, 278–305) are limit Nash equilibria. Modified evolutionary stability differs from “lexicographic evolutionarily stability” (defined by extending the common characterization of evolutionary stability to lexicographic preferences) in the order in which limits in the payoff space and the space of invasion barriers are taken.	lexicographic preferences	Larry Samuelson;Jeroen M. Swinkels	2003	Games and Economic Behavior	10.1016/S0899-8256(03)00049-6	price of stability;game theory;epsilon-equilibrium;mathematical optimization;best response;coordination game;economics;non-credible threat;evolutionarily stable strategy;correlated equilibrium;chicken;microeconomics;risk dominance;normal-form game;mathematical economics;welfare economics;equilibrium selection;nash equilibrium	ECom	-5.004202970791476	-1.819711529267862	124532
5a48dc86599254fc2776f7a1a33a3ad95817c193	implementation and axiomatization of discounted shapley values	91a12;91a10	In this paper we introduce discounting in the bidding mechanism of Perez-Castrillo and Wettstein (J Econ Theory 100:274–294, 2001 ) who implemented the Shapley value for cooperative transferable utility games. This modification of the mechanism yields the corresponding discounted Shapley value as the payoff distribution in every subgame perfect equilibrium. The class of discounted Shapley values contains the Shapley value and equal division solution as its extreme cases. Interestingly, we obtain axiomatizations of each solution in this class by generalizing the null player property (of the Shapley value) and nullifying player property (of the equal division solution) to the so-called $$\delta $$ δ -reducing player property. Copyright The Author(s) 2015	axiomatic system;stable marriage problem	René van den Brink;Yukihiko Funaki	2015	Social Choice and Welfare	10.1007/s00355-015-0899-y	mathematical optimization;economics;microeconomics;shapley value;mathematical economics;welfare economics	ECom	-5.499884816514841	-2.489135073518794	124562
fe7a9cc87d1f69949f605bfd66d3f594e48d3cb0	logical model of guilt as a part of a structure of crime	defeater;lkif;guilt;justification;structure of crime;excuse;defeasible rules;mensrea	We model guilt as a part of crime using an ontology which is an extension of the LKIF ontology, and to put its elements into context of particular criminal jurisdictions common law and the laws of EU countries codes. Differences between existing multipartite guilt frameworks are analysed. We list entities defining these models. An ontological comparative analysis of guilt frameworks in various jurisdictions reveals 7 types of frameworks. Rules of defeasible reasoning are necessary to decide blameworthiness using justification or excuse arguments as we show on an example.	argument map;code;defeasible reasoning;entity;logical data model;qualitative comparative analysis;quilt	Jakub Nowakowski;Czeslaw Jedrzejek	2014			psychology;epistemology;criminology;social psychology	NLP	-13.104309444357161	1.2639043134979604	124858
96c6a1dba6dae412ce798f170db04e27fc750781	nash equilibria conditions for cyclic games with p players	nash equilibria;zero sum game	Abstract Cyclic games with p players which extend zero-sum games of two players are studied and conditions for the existence of Nash equilibria in such games are formulated.	nash equilibrium	Dmitrii Lozovanu;Stefan Pickl	2006	Electronic Notes in Discrete Mathematics	10.1016/j.endm.2006.06.081	price of stability;epsilon-equilibrium;best response;coordination game;folk theorem;repeated game;mathematics;risk dominance;normal-form game;zero-sum game;mathematical economics;equilibrium selection;nash equilibrium	ECom	-5.284745077523158	-0.9656575437910154	124898
8d42443ca00b9628cbfcdc804ad72d580fae0d40	surveyor's forum: stochastic or operational?		"""tests will fail dismally even though the system is well behaved. The alternative is to test interdeparture times for the exponential distribution-a complete capitulation to the stochastic point of view! Things get worse when we come to prediction. Since the operational assumptions are certain to be refuted by existing data, how can we(h~e them to predict? At best we can say that the departures from the operational assumptions have certain statistical properties, which will presumably be the same in the future. But here again we are capitulating to the stochastic point of view. Is that any better than assuming exponential or hyperexponential or some such distribution of service times? Dr. Bard raises four main points. First, he suggests that distribution-free stochastic results such as Little's law hardly require validation. But distribution-free results express relationships among stochastic parameters and expectations of random variables. Validation involves substituting observed values for these stochastic quantities. There is no certainty that the values actually measured during a finite observation period will be the same as the underlying stochastic quantities, or even that they will obey the same mathematical relationships. Hence, distribution-free sto-chastic results do require validation. In contrast , operational laws such as Little's law express invariant relationships among observable quantities. They require no validation. Second, Bard notes correctly that the homogeneity assumptions used in certain operational derivations are virtually certain to be violated by real systems during most observation periods. However, the same is true of many important models found in science and engineering-e.g., Newton's model of an ideal fluid or Boyle's model of a perfect gas. Such models are useful because their abilities to estimate the behavior of real systems have been well validated, even though precise experimental procedures are likely to refute their underlying assumptions. Queueing network models are useful for the same reason, even though their homogeneity assumptions may not be satisfied exactly. The comment that sto-chastic models can neither be conclusively validated nor conclusively refuted by any experiment strikes us as a metaphysical embarrassment to the stochastic analyst, rather than a consolation. Third, Bard considers a hypothetwal system in which devices have exponential service times. If observed for a long time, this system will satisfy operational homogeneity approximately. If observed for a short time, this same system is likely to fail the homo-geneity test """"dismally."""" But the weakness Bard has noted is really the inability of stochastic theory to resolve …"""	little's law;newton;newton's method;observable;operational semantics;phelim boyle;queueing theory;stochastic modelling (insurance);stochastic process;time complexity	Jeffrey P. Buzen;Peter J. Denning	1979	ACM Comput. Surv.	10.1145/356757.356767	simulation;operations research	Metrics	-11.856419203416243	0.8569046728475864	125016
012c851ce80f9015fd249f322b795a040f4a415d	a lean pull system design analysed by value stream mapping and multiple criteria decision-making method under demand uncertainty	cycle time;liquid crystal display;technique for order preference by similarity to ideal solution;topsis;multiple criteria decision making;lean production;production control;system design;continuous improvement;work in process;thin film transistor;manufacturing system;taguchi method;value stream mapping;control strategy;demand uncertainty;pull strategy	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source;systems design	Jiunn-Chenn Lu;Taho Yang;Cheng-Yi Wang	2011	Int. J. Computer Integrated Manufacturing	10.1080/0951192X.2010.551283	topsis;mathematical optimization;simulation;thin-film transistor;taguchi methods;cycle time variation;engineering;artificial intelligence;marketing;operations management;work in process;liquid-crystal display;management;value stream mapping;engineering drawing;lean manufacturing;systems design;mechanical engineering	Robotics	-15.3377901593313	-4.484031546919878	125144
05ae681067ada0c784623e28f24deeb7b22c7afb	implications of a logical paradox for computer-dispensed justice		We argue that the paradox of the preface suggests a reason why computers, no matter how expert, should not be permitted to judge criminal cases. This column normally comments on the work of others; we make an exception just this once.		Joseph S. Fulda	1994	AI & SOCIETY	10.1007/BF01195058	logical consequence;management science	AI	-13.0398826812344	3.3299673723443126	125353
c11fe251d1b55d146a2ced4a887940ae1d82ef2d	a two-phase dialogue game for skeptical preferred semantics	qa75 electronic computers computer science;conference item	In this paper we propose a labelling based dialogue game for determining whether a single argument within a Dung argumentation framework is skeptically preferred. Our game consists of two phases, and determines the membership of a single argument within the extension, assuming optimal play by dialogue participants. In the first phase, one player attempts to advance arguments to construct an extension not containing the argument under consideration, while the second phase verifies that the extension is indeed a preferred one. Correctness within this basic game requires perfect play by both players, and we therefore also introduce an overarching game to overcome this limitation.	argumentation framework;correctness (computer science);human–computer interaction	Zohreh Shams;Nir Oren	2016		10.1007/978-3-319-48758-8_41	non-cooperative game;game design;simulation;extensive-form game;simultaneous game;computer science;artificial intelligence;information set;repeated game;strategy;screening game;normal-form game;sequential game;social psychology	AI	-11.139156189215008	-0.6482925298864032	125379
0296d56c7faff1d3423f91c40414239b01cabd45	bandit algorithms for e-commerce recommender systems: extended abstract		We study bandit algorithms for e-commerce recommender systems. The question we pose is whether it is necessary to consider reinforcement learning effects in recommender systems. A key reason to introduce a recommender system for a product page on an e-commerce site is to increase the order value by improving the chance of making an upsale. If the recommender system merely predicts the next purchase, there might be no positive effect at all on the order value, since the recommender system predicts sales that would have happened independent of the recommender system. What we really are looking for are the false negatives, i.e., purchases that happen as a consequence of the recommender system. These purchases entail the entire uplift and should be present as reinforcement learning effects. This effect cannot be displayed in a simulation of the site, since there are no reinforcement learning effects present in a simulation. The attribution model must capture the uplift to guarantee an increased order value. However, such an attribution model is not practical, due to data sparsity. Given this starting point, we study some standard attribution models for e-commerce recommender systems, and describe how these fare when applied in a reinforcement learning algorithm, both in a simulation and on live sites.	algorithm;e-commerce;purchasing;recommender system;reinforcement learning;simulation;sparse matrix	Björn Brodén;Mikael Hammar;Bengt J. Nilsson;Dimitris Paraschakis	2017		10.1145/3109859.3109930	artificial intelligence;thompson sampling;data mining;machine learning;recommender system;e-commerce;computer science;ensemble learning;attribution;reinforcement learning;algorithm	ML	-10.963875514687246	-7.477729022654264	125765
3dfdabf723419873534a244f79ff40fd169839c0	a bdd-based polytime algorithm for cost-bounded interactive configuration	mass customization;real time;online interaction;decision support system;uct;polynomial time;constraint satisfaction problem;user interaction;binary decision diagram	Interactive configurators are decision support systems assisting users in selecting values for parameters that respect given constraints. The underlying knowledge can be conveniently formulated as a Constraint Satisfaction Problem where the constraints are propositional formulas. The problem of interactive configuration was originally inspired by the product configuration problem with the emergence of the masscustomization paradigm in product manufacturing, but has also been applied to other tasks requiring user interaction, such as specifying services or setting up complex equipment. The user-friendly requirements of complete, backtrack-free and real-time interaction makes the problem computationally challenging. Therefore, it is beneficial to compile the configuration constraints into a tractable representation such as Binary Decision Diagrams (BDD) (Bryant 1986) to support efficient user interaction. The compilation deals with the NPhardness such that the online interaction is in polynomial time in the size of the BDD. In this paper we address the problem of extending configurators so that a user can interactively limit configuration choices based on a maximum cost (such as price or weight of a product) of any valid configuration, in a complete, backtrack-free and real-time manner. The current BDD compilation approach is not adequate for this purpose, since adding the total cost information to the constraints description can dramatically increase the size of the compiled BDD. We show how to extend this compilation approach to solve the problem while keeping the polynomial time guarantees.	algorithm;backtrack;backtracking;binary decision diagram;cobham's thesis;compiler;constraint satisfaction problem;decision support system;emergence;interactivity;knowledge-based configuration;maxima and minima;np-hardness;polynomial;programming paradigm;real-time clock;requirement;time complexity;usability	Tarik Hadzic;Henrik Reif Andersen	2006			time complexity;mathematical optimization;decision support system;mass customization;computer science;artificial intelligence;theoretical computer science;machine learning;constraint satisfaction problem;binary decision diagram;algorithm	AI	-7.862066772770672	3.2840519792450085	125784
3e72241d81f6cdc41658e01c42d555d87b5344d0	probability theory		In this chapter, the authors make use of the formalizations of measure theory and Lebesgue integration in HOL4 to provide a higher-order-logic formalization of probability theory (Mhamdi, 2013). For illustration purposes, they also present the probabilistic analysis of the Heavy Hitter problem using HOL.	hol (proof assistant);probabilistic analysis of algorithms	Alexander A. Grigoryan	2017		10.1007/978-3-319-17885-1_101005	probability distribution;random variable;inverse probability;probability axioms;probability mass function;imprecise probability;conditional probability;probability measure;convolution of probability distributions;normalizing constant;probability interpretations;raised cosine distribution;probability;tree diagram;law of total probability;conditional mutual information;law of the unconscious statistician	Logic	-15.587460002510342	0.6179749490617621	125916
1ad0f7e0e02e56642e2f9dba3332b20de176cdb3	an axiomatic framework for influence diagram computation with partially ordered utilities	sequential decision making;variable elimination;partially ordered preferences;algorithm;conference item;partially ordered utilities;influence diagrams;influence diagram computation;axiomatic framework;sequential variable	This paper presents an axiomatic framework for influence diagram computation, which allows reasoning with partially ordered values of utility. We show how an algorithm based on sequential variable elimination can be used to compute the set of maximal values of expected utility (up to an equivalence relation). Formalisms subsumed by the framework include decision making under uncertainty based on multi-objective utility, or on interval-valued utilities, as well as a more qualitative decision theory based on order-of-magnitude probabilities and utilities.	algorithm;axiomatic system;computation;decision theory;expected utility hypothesis;influence diagram;maximal set;turing completeness;variable elimination	Nic Wilson;Radu Marinescu	2012			variable elimination;mathematical optimization;influence diagram;computer science	AI	-9.35706017687223	-0.005534120036623415	126086
6cf975b92ea222da57e57d26887a5c49963c0ec9	strategy synthesis for stochastic games with multiple long-run objectives		We consider turn-based stochastic games whose winning conditions are conjunctions of satisfaction objectives for long-run average rewards, and address the problem of finding a strategy that almost surely maintains the averages above a given multi-dimensional threshold vector. We show that strategies constructed from Pareto set approximations of expected energy objectives are ε-optimal for the corresponding average rewards. We further apply our methods to compositional strategy synthesis for multi-component stochastic games that leverages composition rules for probabilistic automata, which we extend for long-run ratio rewards with fairness. We implement the techniques and illustrate our methods on a case study of automated compositional synthesis of controllers for aircraft primary electric power distribution networks that ensure a given level of reliability.	approximation;automata theory;fairness measure;pareto efficiency;probabilistic automaton	Nicolas Basset;Marta Z. Kwiatkowska;Ufuk Topcu;Clemens Wiltsche	2015		10.1007/978-3-662-46681-0_22	mathematical optimization;simulation;artificial intelligence;mathematics;algorithm	Logic	-5.45307254753206	3.757306504816727	126318
4d35d224b7f942b743226ba4b0071459e7ce74cd	epistemic expressivism and the argument from motivation	epistemic expressivism;meta epistemology;meta ethics;humanidades;filosofia etica;epistemology;epistemic motivation;ecumenical expressivism;argument from motivation	This paper explores in detail an argument for epistemic expressivism, what we call the Argument from Motivation. While the Argument from Motivation has sometimes been anticipated, it has never been set out in detail. The argument has three premises, roughly, that certain judgments expressed in attributions of knowledge are intrinsically motivating in a distinct way (P1); that motivation for action requires desire-like states or conative attitudes (HTM); and that the semantic content of knowledge attributions cannot be specified without reference to the intrinsically motivating judgments that such attributions express (P2). We argue that these premises entail a version of ecumenical expressivism. Since the argument from motivation has not been explicitly stated before, there is no current discussion of the argument. In this paper we therefore consider and reject various objections that one might propose to the argument, including some that stem from the idea that knowledge is factive, or that knowledge involves evidence that rules out relevant alternatives. Other objections to (P1) specifically might be derived from cases of apparent lack of epistemic motivation considered in in Kvanvig (The value of knowledge and the pursuit of understanding, 2003) and Brown (Nous 42(2):167–189, 2008), as well as from general forms of externalism about epistemic motivation. We consider these and find them wanting. Finally, the paper offers some critical remarks about the prospect of denying (P2).	externalism;html;judgment (mathematical logic)	Klemens Kappel;Emil Frederik Lundbjerg Moeller	2013	Synthese	10.1007/s11229-013-0347-4	meta-epistemology;philosophy;epistemology;mathematics	NLP	-13.485914086439022	3.1602868330644047	126531
ba68d3cc2ab3aae55a255686dfe336db180fed99	the gtr-model: a universal framework for quantum-like measurements		We present a very general geometrico-dynamical description of physical or more abstract entities, called the general tension-reduction (GTR) model, where not only states, but also measurement-interactions can be represented, and the associated outcome probabilities calculated. Underlying the model is the hypothesis that indeterminism manifests as a consequence of unavoidable fluctuations in the experimental context, in accordance with the hidden-measurements interpretation of quantum mechanics. When the structure of the state space is Hilbertian, and measurements are of the universal kind, i.e., are the result of an average over all possible ways of selecting an outcome, the GTR-model provides the same predictions of the Born rule, and therefore provides a natural completed version of quantum mechanics. However, when the structure of the state space is non-Hilbertian and/or not all possible ways of selecting an outcome are available to be actualized, the predictions of the model generally differ from the quantum ones, especially when sequential measurements are considered. Some paradigmatic examples will be discussed, taken from physics and human cognition. Particular attention will be given to some known psychological effects, like question order effects and response replicability, which we show are able to generate non-Hilbertian statistics. We also suggest a realistic interpretation of the GTR-model, when applied to human cognition and decision, which we think could become the generally adopted interpretative framework in quantum cognition research.	born rule;entity;interaction;interpretations of quantum mechanics;quantum cognition;state space	Diederik Aerts;Massimiliano Sassoli de Bianchi	2015	CoRR	10.1142/9789813146280_0005	mathematics;physics;quantum mechanics	ML	-13.824576530295682	0.3074756842713226	126545
ba4bf401872f2508a184d51690ca00184977a60a	a scientific inquiry fusion theory for high-level information fusion	threat assessment scientific inquiry fusion theory high level information fusion joint directors of laboratories fusion model logical inference probabilistic graphical models statistical queries learning algorithms probability theory mathematical logic machine learning automated maritime security situation;statistical analysis inference mechanisms learning artificial intelligence probability query processing sensor fusion;hidden markov models probabilistic logic predictive models phase locked loops graphical models markov random fields joints;logic based fusion statistical relational learning frameworks for multi level fusion processes situation and intent assessment behaviour classification probabilistic graphical models	The Joint Directors of Laboratories fusion model is adequate as a functional description, but falls short as a formal design guide for the application of logical inference under uncertainty for high-level information fusion. We propose a formal construct called the Scientific Inquiry Fusion Theory, with three stages of explanation, prediction and generalisation aligned with the corresponding inferences of abduction, deduction and induction. We first define fusion as formal models without uncertainty, in which the corresponding logical inference patterns can be used for solving fusion problems. Then, we extend these formal models with uncertainty through probabilistic graphical models, where fusion processes are realised by statistical queries and learning algorithms based on the sound unification of Probability Theory, Mathematical Logic and Machine Learning. Finally, we demonstrate the application of this formal high-level information fusion framework with an example of automated maritime security situation and threat assessment.	abductive reasoning;algorithm;anomaly detection;graphical model;hidden markov model;high- and low-level;machine learning;markov chain;markov logic network;mathematical induction;natural deduction;scale-invariant feature transform;unification (computer science)	Zhuoyun Ao;Jason B. Scholz;Martin Oxenham	2014	17th International Conference on Information Fusion (FUSION)		statistical relational learning;computer science;machine learning;pattern recognition;data mining;graphical model	AI	-18.473098772459718	-2.7616158236474107	126589
2c7602688453a19ee805cfb0060a3fb182b13061	sincere-strategy preference-based approval voting broadly resists control	approval voting;complexity theory;artificial intelligent;control problem;polynomial time;artificial intelligence;complexity of procedural control	We study sincere-strategy preference-based approval voting (SP-AV), a system proposed by Brams and Sanver [8], with respect to procedural control. In such control scenarios, an external agent seeks to change the outcome of an election via actions such as adding/deleting/partitioning either candidates or voters. SP-AV combines the voters’ preference rankings with their approvals of candidates, and we adapt it here so as to keep its useful features with respect to approval strategies even in the presence of control actions. We prove that this system is computationally resistant (i.e., the corresponding control problems are NP-hard) to at least 16 out of 20 types of constructive and destructive control. Thus, for the 20 control types studied here, SP-AV has more resistances to control, by at least two, than is currently known for any other natural voting system with a polynomial-time winner problem.	av-test;np (complexity);polynomial;time complexity	Gábor Erdélyi;Markus Nowak;Jörg Rothe	2008		10.1007/978-3-540-85238-4_25	time complexity;bullet voting;computer science;artificial intelligence;approval voting;cardinal voting systems;computer security;algorithm	AI	-7.513997418141793	1.4314600751407174	126671
a7bd639000a8260a27f3630e715848c13f9f8216	testing randomness by matching pennies		In the game of Matching Pennies, Alice and Bob each hold a penny, and at every tick of the clock they simultaneously display the head or the tail sides of their coins. If they both display the same side, then Alice wins Bob’s penny; if they display different sides, then Bob wins Alice’s penny. To avoid giving the opponent a chance to win, both players seem to have nothing else to do but to randomly play heads and tails with equal frequencies. However, while not losing in this game is easy, not missing an opportunity to win is not. Randomizing your own moves can be made easy. Recognizing when the opponent’s moves are not random can be arbitrarily hard. The notion of randomness is central in game theory, but it is usually taken for granted. The notion of outsmarting is not central in game theory, but it is central in the practice of gaming. We pursue the idea that these two notions can be usefully viewed as two sides of the same coin.	alice and bob;game theory;randomness;tails	Dusko Pavlovic	2015	CoRR		simulation;artificial intelligence;advertising	Crypto	-8.900331476329644	-3.638244757756873	126746
167b91b873cd9cbd36a1bab4fd96f15eca7f9d74	power measures derived from the sequential query process	journal article	We study a basic sequential model for the formation of winning coalitions in a simple game, well known from its use in defining the Shapley-Shubik power index. We derive in a uniform way a family of measures of collective and individual decisiveness in simple games, and show that, as for the Shapley-Shubik index, they extend naturally to measures for TU-games. These individual measures, which we call weighted semivalues, form a class whose intersection with that of the class of weak semivalues yields the class of all semivalues. We single out the simplest measure in this family for more investigation, as it is new to the literature as far as we know. Although it is very different from the Shapley value, it is closely related in several ways, and is the natural analogue of the Shapley value under a nonstandard, but natural, definition of simple game. We illustrate this new measure by calculating its values on some standard examples.	decibel;stable marriage problem;technical standard;value (computer science)	Geoffrey Pritchard;Reyhaneh Reyhani;Mark C. Wilson	2013	Mathematical Social Sciences	10.1016/j.mathsocsci.2012.11.003	economics;operations management;mathematics;mathematical economics;welfare economics;statistics	ML	-7.2040233982530495	-1.7288529477072025	126802
379921d8a06fe0fb88aa782dc98ba4a1c832ded6	choice under uncertainty with the best and worst in mind: neo-additive capacities	choquet expected utility;subjective expected utility;convex combination	The concept of a non-extreme-outcome-additive capacity (neo-additive capacity ) is introduced. Neo-additive capacities model optimistic and pessimistic attitudes towards uncertainty as observed in many experimental studies. Moreover, neo-additive capacities can be applied easily in economic problems, as we demonstrate by examples. This paper provides an axiomatisation of Choquet expected utility with neo-capacities in a framework of purely subjective uncertainty. JEL Classification: D81	axiomatic system;decision theory;expected utility hypothesis;utility functions on indivisible goods	Alain Chateauneuf;Jürgen Eichberger;Simon Grant	2007	J. Economic Theory	10.1016/j.jet.2007.01.017	mathematical optimization;convex combination;economics;expected utility hypothesis;mathematics;subjective expected utility;mathematical economics;welfare economics;von neumann–morgenstern utility theorem	AI	-8.352215496800437	-0.9463702210970653	126806
b910ae0f47a2630430afd4e0702968c879b02e47	non-cumulative learning in metaxa.3	cumulant	Knowledge revision in incremental learning systems w i l l usually be restr icted by some external c r i t e r i a to achieve a conservative behavior of the system. Unfortunately, conservatism has some well known drawbacks. Therefore, it can become necessary to drop these restr ict ions and to change over to a non-cumulative learning mode. In this paper the incremental learning system METAXA.3 is described which is able to perform a special kind of noncumulative knowledge revision enabling it to learn without requiring unrestricted resources or the absence of noisy data. The generalization approach is sketched and knowledge revision in METAXA.3 is described.	multi-task learning;signal-to-noise ratio	Werner Emde	1987				AI	-16.45892290479007	2.515481685596161	127109
3ac9b13804581e79776d310967e4318437ddd374	an attempt to formalise a non-trivial benchmark problem in common sense reasoning	event calculus;reasoning about action;cognitive robotics;common sense reasoning;benchmark problem;naive physics;common sense	Most logic-based AI research works at a meta-theoretical level, producing new logics and studying their properties. Little effort is made to show how these logics can be used to formalise object-level theories of common sense. In the spirit of Pat Hayes’s Naive Physics Manifesto, the present paper supplies a formalisation of a non-trivial benchmark problem in common sense physical reasoning, namely how to crack an egg. The formalisation is based on the event calculus, a well-known formalism for reasoning about action. Along the way, a number of methodological issues are raised, such as the question of how the symbols deployed in the formalisation might be grounded through a robot’s interaction with the world.  2003 Published by Elsevier B.V.	benchmark (computing);commonsense reasoning;event calculus;formal system;hayes microcomputer products;robot;theory	Murray Shanahan	2004	Artif. Intell.	10.1016/j.artint.2003.05.001	commonsense reasoning;computer science;artificial intelligence;mathematics;event calculus;algorithm;cognitive robotics	AI	-15.025166277659133	3.3976038629827583	127197
de70410840a4902f095f8259fd4ecdba09b26a0e	partition equilibrium always exists in resource selection games	resource selection;social context;nash equilibrium;load balance	We consider the existence of Partition Equilibrium in Resource Selection Games. Super-strong equilibrium, where no subset of players has an incentive to change their strategies collectively, does not always exist in such games. We show, however, that partition equilibrium (introduced in [4] to model coalitions arising in a social context) always exists in general resource selection games, as well as how to compute it efficiently. In a partition equilibrium, the set of players has a fixed partition into coalitions, and the only deviations considered are by coalitions that are sets in this partition. Our algorithm to compute a partition equilibrium in any resource selection game (i.e., load balancing game) settles the open question from [4] about existence of partition equilibrium in general resource selection games. Moreover, we show how to always find a partition equilibrium which is also a Nash equilibrium. This implies that in resource selection games, we do not need to sacrifice the stability of individual players when forming solutions stable against coalitional deviations. In addition, while super-strong equilibrium may not exist in resource selection games, we show that its existence can be decided efficiently, and how to find one if it exists.		Elliot Anshelevich;Bugra Caskurlu;Ameya Hate	2010		10.1007/978-3-642-16170-4_5	markov perfect equilibrium;social environment;epsilon-equilibrium;mathematical optimization;sequential equilibrium;economics;load balancing;mathematical economics;welfare economics;equilibrium selection;nash equilibrium;symmetric equilibrium	AI	-4.686562474451152	-0.07779795044327235	127439
836e1c1acbbda1b8e2d5e1960b5ba6db3d314b31	strong nash equilibrium in multistage games	time consistent imputation distribution procedure;regularization;strong transferable nash equilibrium;game;equilibrium	"""Infinite multistage games G with games Γ(·) played on each stage are considered. The definition of path and trajectory in graph tree are introduced. For infinite multistage games G a regularization procedure is introduced and in the regularizied game a strong Nash Equilibrium (coalition proof) is constructed. The approach considered in this paper is similar to one used in the proof of Folk theorems for infinitely repeated games. The repeated n-person """"Prisoner's Dilemma"""" game is considered, as a special case. For this game a strong Nash Equilibrium is found."""	multistage amplifier;nash equilibrium	Leon A. Petrosjan;L. V. Grauer	2002	IGTR	10.1142/S0219198902000689	price of stability;markov perfect equilibrium;games;game theory;epsilon-equilibrium;minimax;regularization;mathematical optimization;best response;sequential equilibrium;trembling hand perfect equilibrium;subgame;folk theorem;repeated game;mathematics;stochastic game;correlated equilibrium;normal-form game;mathematical economics;subgame perfect equilibrium;welfare economics;equilibrium selection;symmetric game;solution concept;nash equilibrium;symmetric equilibrium	ECom	-5.258826074573953	-1.091439546178174	127632
fc0d5f5ce347508153409e2b97b86e88581d4d9c	exploiting imu sensors for iot enabled health monitoring	healthcare;imu;fall detection;stride length;calorie	Inertial Measurement Units (IMUs) embedded in commercial mobile devices are a good choice for continuous monitoring in healthcare domain due to their attractive form factor and low power consumption. We present improved and accurate sensing algorithms using a single IMU to sense basic events like step count, stride length, fall, immobility etc. Our algorithms have been shown to perform better than the state of the art algorithms, and are implemented in such a way that IMU is not bound to any specific position or orientation with respect to the user. We propose a 3-layer based framework for a complete end-to-end system architecture for IoT enabled health monitoring, useful for application in areas like individual fitness monitoring and elderly care.	algorithm;embedded system;end system;end-to-end principle;mobile device;sensor;systems architecture	Vivek Chandel;Arijit Sinharay;Nasimuddin Ahmed;Avik Ghose	2016		10.1145/2933566.2933569	embedded system;real-time computing;simulation;engineering	Mobile	-14.861456124322597	-2.286882216815684	127697
a2868ec1dfc1d8ca21d063573068d9d185a8dd5e	fault identification through the combination of symbolic conflict recognition and markov chain-aided belief revision	pattern recognition fault location inference mechanisms uncertainty handling markov processes;belief updating;dk atira pure researchoutput researchoutputtypes contributiontojournal article;general diagnostic engine;inference mechanisms;conflict recognition;uncertainty handling;fault diagnosis engines humans standards development production fault detection maintenance engineering turbines information technology;belief revision;pattern recognition;dempster shafer;general diagnostic engine fault identification symbolic conflict recognition markov chain aided belief revision fault diagnosis;markov processes;gde;fault identification;fault diagnosis;markov chains;markov chain;fault location	Fault identification is a search for possible behaviors that would explain the observed behavior of a physical system. During this search, different possible models are considered and information about the interaction between possible behaviors is derived. Much of this potentially useful information is generally ignored in conventional pure symbolic approaches to fault diagnosis, however. A novel approach is presented in this paper that exploits uncertain information on the behavioral description of system components to identify possible fault behaviors in physical systems. The work utilizes the standard conflict recognition technique developed in the framework of the general diagnostic engine (GDE) to support diagnostic inference through the production of both rewarding and penalizing evidence. In particular, Markov matrices are derived from the given evidence, thereby enabling the use of Markov chains to implement the diagnostic process. This work has resulted in a technique, which maximizes the use of derived information, for identifying candidates for multiple faults that is demonstrated to be very effective.	belief revision;constraint satisfaction;dynamical system;endeavour (supercomputer);markov chain;simulation;software propagation;type system	F. S. Smith;Qiang Shen	2004	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2004.832826	markov chain;artificial intelligence;machine learning;data mining;mathematics;statistics	AI	-18.315021848996	-4.3517724085253295	128086
945ba8ef6a7205b35332f7defd5c7090165b57b0	measuring lifetime poverty	snapshot poverty;poverty measurement;lifetime poverty;chronic poverty	This paper presents an axiomatic framework for measuring lifetime poverty over multiple periods. For an individual, we argue that lifetime poverty is inuenced by both the snapshot poverty of each period and the poverty level of the permanent lifetime consumption; it is also inuenced by how poverty spells are distributed over the lifetime. Two obvious candidates for aggregation are to aggregate over time and then across individuals, or vice versa. For a society, we consider a path-independence requirement to make these two approaches of aggregation consistent. We axiomatically characterize classes of lifetime poverty indices and derive dominance conditions of poverty orderings for both individual and societal lifetime poverty measurements. JEL Classications: I32 Key Words: Lifetime poverty, snapshot poverty, chronic poverty, poverty measurement. Acknowledgement : For very helpful comments we thank Walter Bossert, Conchita DAmbrosio, Serge Kolm and other participants of our presentation at the 8th International Meeting of the Society for Social Choice and Welfare (July 2006, Istanbul). The rst author thanks SSHRC for nancial support.	axiomatic system;snapshot (computer storage)	Michael Hoy;Buhong Zheng	2011	J. Economic Theory	10.1016/j.jet.2011.10.011	economics;socioeconomics;welfare economics;economic growth	Metrics	-10.704936445000724	-2.381907441768575	128195
c9c1e31981adb42775b21a68da99ccfb9db18e8c	communication, correlation and cheap-talk in games with public information	distributed computing;communication equilibrium;correlated equilbirium;repeated game;cheap talk;distributed computation;normal form;correlated equilibrium;public information;normal form correlated equilibrium;normal form correlation;stochastic games;c72 noncooperative games	This paper studies extensive form games with public information where all players have the same information at each point in time. We prove that when there are at least three players, all communication equilibrium payo s can be obtained by unmediated cheap-talk procedures. The result encompasses repeated games and stochastic games.		Yuval Heller;Eilon Solan;Tristan Tomala	2012	Games and Economic Behavior	10.1016/j.geb.2011.05.002	combinatorial game theory;markov perfect equilibrium;cheap talk;sequential equilibrium;economics;repeated game;mathematics;correlated equilibrium;microeconomics;mathematical economics;welfare economics	ECom	-4.875138759563279	-1.9825204984095832	128476
bd8bbfbdce9df7679c6d729f40daa155b576556a	a game theoretic perspective on network topologies	game theory;cost function;network topology;statistical physics;degree sequence;collaborative networks;network formation;complete graph	We extend the results of Goyal and Joshi (S. Goyal and S. Joshi. Networks of collaboration in oligopoly. Games and Economic behavior, 43(1):57-85, 2003), who first considered the problem of collaboration networks of oligopolies and showed that under certain linear assumptions network collaboration produced a stable complete graph through selfish competition. We show with nonlinear cost functions and player payoff alteration that stable collaboration graphs with an arbitrary degree sequence can result. As a by product, we prove a general result on the formation of graphs with arbitrary degree sequences as the result of selfish competition. Simple motivating examples are provided and we discuss a potential relation to Network Science in our conclusions.	degree (graph theory);game theory;network science;network topology;nonlinear system	Shaun Lichter;Christopher Griffin;Terry L. Friesz	2011	CoRR		game theory;mathematical optimization;combinatorics;simulation;network formation;mathematics;complete graph;network topology	ECom	-4.759741461771423	-0.04635101225725363	128519
9bacf6dabc38fec502f22f5c4fa63353f421ae32	optimal preference clustering technique for scalable multiagent negotiation(short paper)	distributed artificial intelligence;artificial intelligence;coherence and coordination;automated negotiation	We propose protocol for automated negotiations between multiple agents over multiple and interdependent issues. We consider the situation in which the agents have to agree upon one option contract among many possible ones contract space. Interdependency between issues prevents us from applying negotiation protocols that have linear time complexity cost like Hill Climbing implementing mediated text negotiation protocolHC. As a result most previous works propose methods in which the agents use non linear optimizers like simulated annealing to generate proposals. Then a central mediator can be used to match the proposals in order to find an intersection. But this matching process usually has exponential time cost complexity. We propose multi round HCMR-HC for negotiations with multiple and interdependent issues. In each round the normal HC is used to determine a negotiation deal region to be used by the next round. We propose that the agents should cluster their constraints by the cardinality of the constraints in order to get socially optimal contracts before applying MR-HC. To showcase that our proposed clustering technique is an essential one, we evaluate the optimality of our proposed protocol by running simulations at different cluster sizes.	agent-based model;computer cluster	Raiye Hailu;Takayuki Ito	2014		10.1007/978-3-319-07455-9_5	computer science;artificial intelligence;machine learning;data mining	AI	-10.0330763462552	-6.630737977196635	128806
21245256f3cd279831e1ae30b87a5b0c8d8e3435	analysis of asymmetric two-sided matching: agent-based simulation with theorem-proof approach	social simulation;mate search problem;agent based models abm;job matching;two sided matching;theorem proof approach	This paper discusses an extended version of the matching problem which includes the mate search problem; this version is a generalization of a traditional optimization problem. The matching problem is extended to a form of the asymmetric two-sided matching problem. An agent-based simulation model is built and simulation results are presented. Todd and Miller (1999) simulated the two-sided matching problem in a symmetric setting. In his model, there are the same number of agents in both parties (groups), each of whom has his/her own mate value. Each agent in a party tries to find his/her mate in the other party, based on his/her candidate's mate value and his/her own aspiration level for his/her partner's mate value. Each agent learns his/her own mate value and adjusts his/her aspiration level through the trial period (adolescence). Todd and Miller (1999) tried several search rules and learning mechanisms that are symmetric for both parties. In the present paper, Todd and Miller's (1999) model is extended to an asymmetric setting where the two parties have different numbers of agents, and the search rule and the learning mechanism for the two parties differ. Through the simulation, the search rules and the learning mechanisms which were identified to be appropriate in a symmetric setting are revealed to be inappropriate in the asymmetric setting and the reason why this is so is discussed. Furthermore, some general facts are derived using a mathematical theorem-proof approach. Some of these facts are used to direct a revision of the model, and a revised simulation model is presented. An implication is obtained for practical situations in asymmetric matching setting. For example, in the job hunting case, if job applicants want to finish their job hunting successfully, they should be modest at the beginning of the hunt.	simulation	Naoki Shiba	2013	J. Artificial Societies and Social Simulation		psychology;social science;simulation;economics;computer science;artificial intelligence;mathematics;social simulation;sociology;social psychology;statistics	AI	-8.515933820056633	-6.804102020411598	128946
c6e6eca0de108973261eeba6cdea7124f3f7f539	the puzzle of conditionals with true clauses: against the gricean account		Indicative conditionals, that is sentences of the form “If p, then q,” belong to the most puzzling phenomena of language. On the majority of accounts of indicative conditionals, the truth of p and q suffices for “If p, then q” to be true or highly acceptable. Yet, many conditionals with true clauses, even if there is a meaningful connection between them, sound odd. The most common reaction to this phenomenon is to attribute the oddity of conditionals with true clauses to natural language pragmatics. We present an experimental study investigating how the presence or absence of a connection between the clauses affects the assertability of conditionals and conjunction expressing generic and specific kind of content. The results refute the standard pragmatic explanation.	experiment;natural language	Karolina Krzyzanowska;Peter Collins;Ulrike Hahn	2017			psychology;cognitive psychology	AI	-13.739499800319798	3.6795548620009684	129091
a5a5863ebcfde4b184a71c1ce69ca2d9f3770ce5	characterizing the existence of potential functions in weighted congestion games	cost function;potential functions;exponential function;nash equilibria;potential function;pure nash equilibria;congestion game	Since the pioneering paper of Rosenthal a lot of work has been done in order to determine classes of games that admit a potential. First, we study the existence of potential functions for weighted congestion games. Let $\mathcal{C}$ be an arbitrary set of locally bounded functions and let $\mathcal{G}(\mathcal{C})$ be the set of weighted congestion games with cost functions in $\mathcal{C}$. We show that every weighted congestion game $G\in\mathcal{G}(\mathcal{C})$ admits an exact potential if and only if $\mathcal{C}$ contains only affine functions. We also give a similar characterization for w-potentials with the difference that here $\mathcal{C}$ consists either of affine functions or of certain exponential functions. We finally extend our characterizations to weighted congestion games with facility-dependent demands and elastic demands, respectively.	network congestion	Tobias Harks;Max Klimm;Rolf H. Möhring	2009		10.1007/978-3-642-04645-2_10	mathematical optimization;combinatorics;discrete mathematics;economics;exponential function;mathematics;mathematical economics;nash equilibrium	ECom	-4.598019348592629	0.010823257216890854	129112
ca347fd8591b35f0cc424bca174409b3f5d207a6	social evaluation functionals: a gateway to continuity in social choice		This paper develops social choice theory aggregating individual utility functions to a social utility function. Such a tool allows me to deal with a natural notion of continuity in social choice theory. In addition, and in order to have the choice problem as close as possible to its beginnings, the social evaluation functionals considered are assumed to satisfy both ordinal measurability and interpersonal non-comparability, and unanimity. I present two results concerning the characterization of projective social evaluation functionals (which means that the social utility function is exactly the utility of the dictator). The first one needs a strong form of welfarism called social state separability. The second one uses continuity in combination with a new axiom called ordinal-scale-preserving. Copyright Springer-Verlag Berlin Heidelberg 2015	scott continuity	Juan Carlos Candeal	2015	Social Choice and Welfare	10.1007/s00355-014-0832-9	economics;mathematics;sociology;mathematical economics;social psychology;welfare economics	ECom	-7.203165532346993	-1.5511549566846354	129286
78e2a4dee6714d003470b7703723c121e83362be	computation with imprecise probabilities	imprecise probability	An imprecise probability distribution is an instance of second-order uncertainty, that is, uncertainty about uncertainty, or uncertainty for short. Another instance is an imprecise possibility distribution. Computation with imprecise probabilities is not an academic exercise—it is a bridge to reality. In the real world, imprecise probabilities are the norm rather than exception. In large measure, real-world probabilities are perceptions of likelihood. Perceptions are intrinsically imprecise, reflecting the bounded ability of human sensory organs, and ultimately the brain, to resolve detail and store information. Imprecision of perceptions is passed on to perceived probabilities. This is why real-world probabilities are, for the most part, imprecise.	computation;exception handling	Lotfi A. Zadeh	2008		10.1109/IRI.2008.4582989	imprecise probability;computer science;machine learning;data mining;statistics	AI	-16.194581878339143	0.8078461035373494	129318
1d3df79fd50d3f2ab4a01bd9bfff39d4da1c43bb	the demand bargaining set: general characterization and application to majority games	proportional payoffs;satisfiability;stable demands;weighted majority game;bargaining sets;weighted majority games;solution concept;undominated coalition structures	The cooperative solution concept introduced here, the demand bargaining set, contains the core and is included in the Zhou bargaining set, eliminating the “dominated” coalition structures. The demand vectors belonging to the demand bargaining set are self-stable. In the class of constant-sum homogeneous weighted majority games the demand bargaining set is non-empty and predicts a unique demand vector, namely a proportional distribution within minimal winning coalitions. The noncooperative implementation of the demand bargaining set is obtained for all the games that satisfy the one-stage property.		Massimo Morelli;Maria Montero	2003	Games and Economic Behavior	10.1016/S0899-8256(02)00532-8	bargaining problem;economics;microeconomics;mathematical economics;welfare economics;solution concept;satisfiability	ECom	-4.829388190902364	-3.1184437020783737	129547
a468d70f5f00a958100b23a3e47e7b4257233f02	the role of visualisation in the choice of stationary non-separable space-time covariance functions: an application to air pollution data	visualisation;air pollution;spatio temporal covariance;model fitting;kriging;carbon monoxide	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;interaction;primary source;stationary process	José-María Montero-Lorenzo;Gema Fernández-Avilés	2015	International Journal of Geographical Information Science	10.1080/13658816.2015.1063152	matérn covariance function;econometrics;carbon monoxide;data mining;mathematics;kriging;ecology;rational quadratic covariance function;statistics;air pollution	Robotics	-14.358231910730026	-5.8467334406847264	129730
c2afc6ff4a41ad525d73a710acc4d7784b6e068b	vacant technology forecasting using new bayesian patent clustering	ensemble method;patent analysis;vacant technology forecasting;new hybrid bayesian clustering	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	bayesian network;ct scan;cluster analysis;computer engineering;computer science;data mining;francis;information engineering;information management;jun wang (scientist);nl (complexity);primary source	Seongyong Choi;Sunghae Jun	2014	Techn. Analysis & Strat. Manag.	10.1080/09537325.2013.850477	data science;data mining;operations research	Robotics	-14.381293967211105	-5.686075486832762	129866
a523d8c59b1b47bf9af41c5cc6f693576a9b4c41	developing a mechanism to support decisions for products missing from the shelf	chaine de distribution;classification methods;rupture de stocks;supply chain;control d inventaire;methode de classification;grocery sector;inventory control;distributed control;secteur alimentaire;out of shelf	The issue of products missing from the shelf is of high importance in a retailing supply chain. A decision system enabling store managers to decrease the out-of-shelf rate would be valuable. This paper examines two different approaches in building such a decision system: (a) utilization of inventory control models and (b) incorporation of classification methods. Both share criticism regarding their applicability, thus a further exploration of this issue is valuable. This paper examines four different experiments in order to evaluate each approach and record its applicability in the grocery sector. It comes out that both research approaches have limitations and strengths, thus it is suggested that such a decision system should be designed by utilizing both of them. RÉSUMÉ. La rupture de stock de produits est un problème important dans la grande distribution. Un système d’aide à la décision visant à réduire les problèmes d’inventaire serait très utile pour les managers. Cet article considère deux approches pour construire un tel système : a) utilisation de modèles de contrôle d’inventaire et b) une méthode de classification. Ces deux approches proposent une solution imparfaite et nécessitent une discussion. Nous avons conduit quatre expériences utilisant les deux méthodes et avons mesuré leur impact dans le secteur de la distribution alimentaire. Nous en concluons que ces approches ont toutes deux des forces et des faiblesses et doivent être utilisées en tandem.	decision support system;emoticon;experiment;inventory control;linear algebra;steam rupture	Dimitris Papakiriakopoulos	2011	Journal of Decision Systems	10.3166/JDS.20.417-441	inventory control;marketing;operations management;supply chain;management;operations research	ML	-16.841817575142596	-3.0469292596983952	130081
385953b9cd0d112c89d7a8f7139e361e053e6217	the uncovered set and indifference in spatial models: a fuzzy set approach	fuzzy preferences;fuzzy set;procesamiento informacion;social sciences;cycling;conjunto difuso;ensemble flou;pareto set;spatial models;social science;probleme recouvrement;problema recubrimiento;information processing;06axx;uncovered set;sistema difuso;systeme flou;covering problem;traitement information;05b40;fuzzy system;spatial model	In spite of substantial criticism [1], a great number of spatial models are grounded in the rational choice assumption that political actors are self-interested, utility maximizers possessing Euclidean preferences over alternatives. Were it not for the substantial incongruence between the predictions made by these models and empirical reality, criticism of the assumptions behind the models might be more easily dismissed. The most well-known incongruence is the majority cycling problem. McKelvey [2] found that in the absence of Plotts [3] radial symmetry, outcomes of majority rule games under assumptions of sincere voting should lead anywhere. The problem is that there is little empirical evidence of cycling in political life [4], a fact that has led many to question the validity of spatial models. In more formal terms, McKelvey argues that there is no core, or set of majority preferred outcomes. A core is dened as	fuzzy set;radial (radio);spatial analysis	John N. Mordeson;Terry D. Clark;Nicholas R. Miller;Peter C. Casey;Michael B. Gibilisco	2011	Fuzzy Sets and Systems	10.1016/j.fss.2010.10.016	information processing;computer science;artificial intelligence;data mining;mathematics;cycling;fuzzy set;algorithm;fuzzy control system	AI	-8.104234766778847	-0.3690511903527441	130089
b4b81ed60c028596948fc9e7f3829cace4d7ebbb	a relaxation of a semiring constraint satisfaction problem using combined semirings	semiring constraint satisfaction;single semiring;combined semirings;solution tuples;partial constraint satisfaction problem;combined semiring structure;original problem;original scsp;abstract solution;acceptable solution;semiring value;semiring constraint satisfaction problem;constraint satisfaction problem	The Semiring Constraint Satisfaction Problem (SCSP) framework is a popular approach for the representation of partial constraint satisfaction problems. In this framework preferences (semiring values) can be associated with tuples of values of the variable domains. Bistarelli et al. [1] define an abstract solution to a SCSP which consists of the best set of solution tuples for the variables in the problem. Sometimes this abstract solution may not be good enough, and in this case we want to change the constraints so that we solve a problem that is slightly di↵erent from the original problem but has an acceptable solution. In [2] we propose a relaxation of a SCSP where we define a measure of distance (a semiring value from a second semiring) between the original SCSP and a relaxed SCSP. In this paper we show how the two semirings can be combined into a single semiring. This combined semiring structure will allow us to use existing tools for SCSPs to solve Combined Semiring Relaxations of SCSPs. At this stage our work is preliminary and needs further investigation to develop into a useful algorithm.	algorithm;constraint satisfaction problem;linear programming relaxation;principle of good enough;yamaha ymf292	Louise Leenen;Thomas Andreas Meyer;Peter Harvey;Aditya K. Ghose	2006		10.1007/11801603_104	mathematical optimization;combinatorics;discrete mathematics;constraint satisfaction;computer science;artificial intelligence;semiring;mathematics;constraint satisfaction problem	AI	-7.570483410589068	3.709508105267416	130115
5a31f616e49fe920d2d9e7f42012139d86afb60b	dominant strategy implementation of stable rules	deferred acceptance algorithm;assignment problem;weak nonbossiness;satisfiability;stability;dominant strategy implementation;multiple equilibria	Most priority-based assignment problems are solved using the deferred acceptance algorithm. Kojima (2010) shows that stability and nonbossiness are incompatible. We show that the deferred acceptance algorithm satisfies a weaker notion of nonbossiness for every substitutable priority structure. We also discuss the multiplicity of dominant strategy equilibria of the preference revelation game induced by the deferred acceptance algorithm. We show that even untruthful dominant strategy equilibria lead to the truthful equilibrium outcome. In other words, the deferred acceptance algorithm is dominant strategy implementable.		Taro Kumano;Masahiro Watabe	2012	Games and Economic Behavior	10.1016/j.geb.2011.11.008	mathematical optimization;stability;mathematics;assignment problem;mathematical economics;welfare economics;statistics;satisfiability	ECom	-6.380078223411371	-1.6737270300349198	130123
256d7d31c20232b29d1b974452abc01302d15ab4	on the strategic origin of brownian motion in finance	institutional repositories;continuous time;fedora;stock market;brownian motion;vital;asymmetric information;repeated game;insider trading;games of incomplete information;insider trading game of incomplete information brownian motion;vtls;private information;side information;ils	This paper is concerned with the stategic use of a private information on the stock market. A repeated auction model is used to analyze the evolution of the price system on a market with asymmetric information. The model turns out to be a zero-sum repeated game with one-sided information, as introduced by Aumann and Maschler. The stochastic evolution of the price system can be explicitly computed in the n times repeated case. As n grows to ∞, this process tends to a continuous time martingale related to a Brownian Motion. This paper provides in this way an endogenous justification for the appearance of Brownian Motion in Finance theory. ESSTIN & Institut Elie Cartan, Université Henri Poincaré (Nancy 1), France and CORE, Université Catholique de Louvain, Belgium. E-mail: demeyer@iecn.u-nancy.fr Institut Elie Cartan, Université Henri Poincaré (Nancy 1), France. E-mail: moussa@iecn.u-nancy.fr This paper presents research results of the Belgian Program on Interuniversity Poles of Attraction initiated by the Belgian State, Prime Minister’s Office, Science Policy Programming. The scientific responsibility is assumed by the authors.	brownian motion;database;louvain modularity;personal message;personally identifiable information;policy-based design;triune continuum paradigm;windows insider	Bernard De Meyer;Hadiza Moussa Saley	2003	Int. J. Game Theory	10.1007/s001820200120	financial economics;information asymmetry;private information retrieval;actuarial science;economics;repeated game;brownian motion;microeconomics;mathematical economics	AI	-7.062233263774675	-4.897818321995778	130177
d7b1fcf83599d42cd8c2c5a97ebf483f9b83bb9a	on coalitional semivalues	transferable utility	In this paper we propose a characterization of the coalitional value for transferable utility games (Owen, 1977), and we define and study coalitional semivalues, which are generalizations of semivalues (Dubey, Neyman and Weber, 1981).		M. Josune Albizuri;José Manuel Zarzuelo	2004	Games and Economic Behavior	10.1016/j.geb.2004.01.001	economics;microeconomics;transferable utility;mathematical economics;welfare economics	ECom	-5.769010100880372	-1.5296197012966495	130399
c22c92e16aa1811215133351bb8d48480a0998bc	an axiomatization of the consistent non-transferable utility value	non transferable utility	The Maschler-Owen consistent value for non-transferable utility games is axiomatized, by means of a marginality axiom.	axiomatic system	Sergiu Hart	2005	Int. J. Game Theory	10.1007/s00182-005-0204-x	economics;isoelastic utility;cardinal utility;subjective expected utility;microeconomics;mathematical economics;welfare economics;von neumann–morgenstern utility theorem	ECom	-6.589627740164887	-1.242911890449202	130474
04c6734c154486b3c73c8ae7c4c3cff88bba1d48	gale-shapley stable marriage problem revisited: strategic issues and applications	strategic issues;gale shapley algorithm;stable matching;stable marriage;secondary school;strategic behavior;student posting exercise;stable marriage problem;ministry of education;two sided matching	We study strategic issues in the Gale-Shapley stable marriage model. In the first part of the paper, we derive the optimal cheating strategy and show that it is not always possible for a woman to recover her women-optimal stable partner from the men-optimal stable matching mechanism when she can only cheat by permuting her preferences. In fact, we show, using simulation, that the chances that a woman can benefit from cheating are slim. In the second part of the paper, we consider a two-sided matching market found in Singapore. We study the matching mechanism used by the Ministry of Education (MOE) in the placement of primary six students in secondary schools, and discuss why the current method has limited success in accomodating the preferences of the students, and the specific needs of the schools (in terms of the “mix” of admitted students). Using insights from the first part of the paper, we show that stable matching mechanisms are more appropriate in this matching market, and explain why the strategic behavior of the students need not be a major concern.	moe;matching (graph theory);simulation;stable marriage problem	Chung-Piaw Teo;Jay Sethuraman;Wee-Peng Tan	2001	Management Science	10.1287/mnsc.47.9.1252.9784	simulation;stable marriage problem;economics;marketing;operations management;stable roommates problem;management;welfare economics	AI	-5.890767985576986	-4.604729526135804	130770
07cd777416a0c14c97bb5dd9c0e645d6ad00dd0f	bidding patterns, experience, and avoiding the winner's curse in online auctions	bidder strategy;economic outcome;online auction;bidding strategy;online auctions;different bidder;curse adjustment;domain-specific experience;bidding pattern;bidding patterns;bidder experience;general auction experience	The design and implementation of online auctions has given rise to a unique set of bidding strategies that has stimulated a growing body of research. We make use of a theoretically grounded, well-understood, and empirically observable bidder behavior-the winner's curse adjustment for the expected number of bidders in an auction-to examine the relationships between bidder experience, bidding patterns, and the winner's curse adjustment in rare coin online auctions. We also examine the impact of uncertainty on the winner's curse adjustment, both by using precise measures of uncertainty and by considering seller and bidder strategies for reducing that uncertainty. We analyze a complete record of all auctions in a three-month period for rare U. S. coins, examining 284,681 bids from 62,625 auctions hosted by eBay, the market leader in online auctions. One of the main contributions of this paper is to demonstrate that the bidding patterns associated with different bidders are strongly related to whether they calculate their bids to take into account the number of competing bidders, as predicted for common-value auctions. This is a substantial extension and empirical confirmation of prior work that has explored the implications of different observed patterns of bidding. We also explore new territory by examining the relationships between bidder experience, bidding patterns observed, and the economic outcomes for bidders. We are able to show that bidders with more domain-specific experience (rather than general auction experience) make better adjustments for the winner's curse, that experience has an effect on the type of bidding strategy, and that the type of bidding strategy has a significant effect on the economic outcomes for the bidders.	din's curse	Robert F. Easley;Charles A. Wood;Sharad Barkataki	2011	J. of Management Information Systems		economics;winner's curse;marketing;common value auction;microeconomics;statistics;commerce	OS	-5.171070845089076	-7.9633385424039895	130849
5e39a6b57d1d9b9447c980cafd7d0a1e2f5f2785	optimal budget allocation in social networks: quality or seeding?	resource management games equations social network services nash equilibrium computational modeling vectors;balanced graph optimal budget allocation social networks strategic model marketing product consumption myopic best response dynamics linear update local update product quality nash equilibrium seeding budget;social networking online game theory graph theory marketing data processing product quality	In this paper, we study a strategic model of marketing and product consumption in social networks. We consider two competing firms in a market providing two substitutable products with preset qualities. Agents choose their consumptions following a myopic best response dynamics which results in a local, linear update for the consumptions. At some point in time, firms receive a limited budget which they can use to trigger a larger consumption of their products in the network. Firms have to decide between marginally improving the quality of their products and giving free offers to a chosen set of agents in the network in order to better facilitate spreading their products. We derive a simple threshold rule for the optimal allocation of the budget and describe the resulting Nash equilibrium. It is shown that the optimal allocation of the budget depends on the entire distribution of centralities in the network, quality of products and the model parameters. In particular, we show that in a graph with a higher number of agents with centralities above a certain threshold, firms spend more budget on seeding in the optimal allocation. Furthermore, if seeding budget is nonzero for a balanced graph, it will also be nonzero for any other graph, and if seeding budget is zero for a star graph, it will be zero for any other graph too. We also show that firms allocate more budget to quality improvement when their qualities are close, in order to distance themselves from the rival firm. However, as the gap between qualities widens, competition in qualities becomes less effective and firms spend more budget on seeding.	a* search algorithm;centrality;coefficient;mathematical optimization;nash equilibrium;random seed;social network	Arastoo Fazeli;Amir Ajorlou;Ali Jadbabaie	2014	53rd IEEE Conference on Decision and Control	10.1109/CDC.2014.7040084	budget constraint	ECom	-5.669759607209424	-7.091777874554426	131131
22461776f5b51ed87b8f2a50242695929f589cd4	complementarity with complete but p-acyclic preferences	monotone comparative statics;hb economic theory;single crossing property	This paper extends the formulation of complementarity in Milgrom and Shannon (1994) to the case of complete but P-acyclic preferences. In such a case, quasi-supermodularity and the single-crossing property on their own do not guarantee monotone comparative statics or equilibrium existence: an additional condition, monotone closure, is required.	complementarity (physics);directed acyclic graph	Sayantan Ghosal	2011	Mathematical Social Sciences	10.1016/j.mathsocsci.2011.06.003	mathematical optimization;comparative statics;mathematics;mathematical economics;welfare economics	ECom	-6.402998022555837	-1.0904662654612578	131526
6a6d06663233958cf1af7fc50fe0004efbbf4eb3	communication, consensus, and order.: who wants to speak first?	consensus;bayesian games;unawareness;type space;agent communication;interactive epistemology;agreement;rational agent;incomplete information;speculative trade;ordre de parole;awareness;common knowledge;connaissance commune;communication protocol;equilibrium;common prior	Parikh and Krasucki [1990] showed that if rational agents communicate the value of a function f according to a protocol upon which they have agreed beforehand, they will eventually reach a consensus about the value of f, provided a fairness condition on the protocol and a convexity condition on the function f hold. In this article, we address the issue of the influence of the protocol on the outcome of the communication process, in the case where agents communicate in order to learn information. We show that if it is common knowledge among a group of agents that some of them disagree about two protocols, then the consensus value of f must be the same according to the two protocols.	consensus (computer science);fairness measure;rational agent	Nicolas Houy;Lucie Ménager	2007		10.1145/1324249.1324277	rational agent;communications protocol;consensus;awareness;economics;epistemology;computer science;artificial intelligence;mathematics;microeconomics;mathematical economics;complete information;algorithm;common knowledge	ECom	-8.500340078058214	-2.778561210798477	131986
685ef8a65686d3dab9923797e1eab29fc98dc482	agent-based dynamic rescheduling of daily activities		When simulating individuals’ daily plan, lots of unexpected events need to be considered, like traffic jam and weather changes. Therefore, there will be a mismatch between the original plan and the executed one. Faced with this situation, individuals need to adjust the rest of the activities to make a new schedule. This paper analyzes the causes of rescheduling, and establishes a new rescheduling model, combining strengths of existing rescheduling models. The model in this paper considers the rescheduling possibilities and choices as much as possible. It takes time pressure and schedule similarity into consideration when updating a schedule. Furthermore, this paper analyzes joint trip/activity execution by studying the cooperation between agents during the rescheduling process. c © 2016 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Conference Program Chairs.		Hui Zhaoa;Stéphane Gallanda;Luk Knapenb;Tom Bellemansb;Ansar-Ul-Haque Yasarb	2018		10.1016/j.procs.2018.04.100	data mining;real-time computing;activities of daily living;unexpected events;computer science	AI	-15.303309668698814	-8.632790770892303	132162
6abff59a50611053823f5350e664da907805b656	a simulation-based decision support system for workforce management in call centers	customer satisfaction level;largest call center;workforce management;performance reporting;predefined level;workforce plan;call center;call arrival rate;call center agent;simulation-based decision support system;user-friendly dss environment	Workforce management is critical in call centers, where thousands of calls are handled by hundreds of agents every day. In a call center, where the call arrival rates tend to fluctuate during the day, the agent allocation plans are required to be planned flexibly and the number of operating call center agents ought to be updated whenever needed, in order to keep the customer satisfaction level over a predefined level. Workforce plans are usually generated by the use of queuing models that are based on Erlang C calculations. However, they have assumptions that oversimplify the real system and jeopardize the validation of the model. At this point, the simulation models, which do not have such restrictive assumptions, are effective in calculating the required number of agents for each time period and measuring the performance of a given shift schedule. In this study, a simulation-based decision support system (DSS) is developed that runs on real-time data for one of the largest call centers in Turkey. The graphical user interfaces (GUIs) are designed in accordance with the man-machine interaction consideration to increase the usability, functionality and effectiveness of the DSS. It is shown that the combination of the advantages of simulation with a flexible and user-friendly DSS environment provides more effective and efficient workforce planning and performance reporting in call centers.	decision support system;simulation	Asli Senser Erdem;Birgul Basarir	2010			decision support system;systems engineering;knowledge management;workforce planning;management science	Metrics	-13.032775433315441	-9.148165902663717	132210
5f2b3552a01681ede668221e592179f65d210d67	comparing influence theories in voting games under locally generated measures of dissatisfaction	dominance;voting game;desirability relation;power relation;simple game;influence relation	The Isbell desirability relation (I), the Shapley–Shubik index (SS) and the Banzhaf–Coleman index (BC) are power theories that grasp the notion of individual influence in a yes–no voting rule. Also, a yes–no voting rule is often used as a tool for aggregating individual preferences over any given finite set of alternatives into a collective preference. In this second context, Diffo Lambo and Moulen (DM) have introduced a power relation which ranks the voters with respect to how ably they influence the collective preference. However, DM relies on the metric d that measures closeness between preference relations. Our concern in this work is: do I, SS, BC and DM agree when the same yes–no voting rule is the basis for collective decision making? We provide a concrete and intuitive class of metrics called locally generated (LG). We give a characterization of the LG metrics d for which I, SS, BC and DM agree on ranking the voters.	centrality;shapley–folkman lemma;stable marriage problem;theory;turing completeness;video graphics array;word lists by frequency	Lawrence Diffo Lambo;Bertrand Tchantcho;Joël Moulen	2012	Int. J. Game Theory	10.1007/s00182-012-0342-x	mathematics;dominance;cardinal voting systems;mathematical economics;welfare economics	ECom	-7.630033569794792	-1.741561529817509	132339
67bb19d948596831b4c74197f6060e0507243956	anonymous, yet trustworthy auctions	auction;distributed systems.;trust;anonymity;credential;distributed system;profitability;business strategy	An auction is an inevitable market mechanism to setup prices of goods or services in a competitive and dynamic environment. Anonymity of bidders is required to conceal their business strategies from competitors. However, it is also essential to provide the seller guarantees that a bidder is trustworthy and competent enough to perform certain tasks (e.g transports). This paper proposes an auction protocol where bidders will participate anonymously, yet prove to be trustworthy and competent and can be held accountable towards auctioneers and sellers. Moreover, the protocol introduces promises, bonuses and compensations to ensure the best price for the sellers, extra profit for bidders and opportunities for newcomers in the business. It also handles ties, and copes with last minute bidding. Finally, the auction’s fair proceedings and outcome can be verified by everyone.	trust (emotion);trustworthy computing	Prasit Bikash Chanda;Vincent Naessens;Bart De Decker	2009		10.1007/978-3-642-04280-5_19	vickrey–clarke–groves auction;generalized second-price auction;eauction;common value auction;auction theory;reverse auction;english auction;microeconomics;business;commerce;forward auction	Security	-9.777601695080701	-8.116307040263862	132392
8b351ef01182b5259c1eea76fb5dba989bc77f8b	crowdselect: increasing accuracy of crowdsourcing tasks through behavior prediction and user selection	malicious worker;task assignment;crowdsourcing	Crowdsourcing allows many people to complete tasks of various difficulty with minimal recruitment and administration costs. However, the lack of participant accountability may entice people to complete as many tasks as possible without fully engaging in them, jeopardizing the quality of responses. In this paper, we present a dynamic and time efficient solution to the task assignment problem in crowdsourcing platforms. Our proposed approach, CrowdSelect, offers a theoretically proven algorithm to assign workers to tasks in a cost efficient manner, while ensuring high accuracy of the overall task. In contrast to existing works, our approach makes minimal assumptions on the probability of error for workers, and completely removes the assumptions that such probability is known apriori and that it remains consistent over time. Through experiments over real Amazon Mechanical Turk traces and synthetic data, we find that CrowdSelect has a significant gain in term of accuracy compared to state-of-the-art algorithms, and can provide a 17.5\% gain in answers' accuracy compared to previous methods, even when there are over 50\% malicious workers.	activity selection problem;amazon mechanical turk;apriori algorithm;assignment problem;cost efficiency;crowdsourcing;experiment;malware;synthetic data;the turk;tracing (software)	Chenxi Qiu;Anna Cinzia Squicciarini;Barbara Carminati;James Caverlee;Dev Rishi Khare	2016		10.1145/2983323.2983830	simulation;computer science;artificial intelligence;machine learning;data mining;database;world wide web;crowdsourcing	HCI	-10.39503017967582	-8.349804497655983	132413
303647792dcb56cd5b6962664d6903264fb36148	uncertainty in probabilistic trust models	history;uncertainty;hidden markov model;computer model;risk management;hidden markov models uncertainty computational modeling stability analysis history mathematical model probabilistic logic;trust model;risk reduction;risk management data privacy hidden markov models;utility maximization;trusted computing;confidence interval;computational modeling;hidden markov models;data privacy;probabilistic trust model;stability analysis;mathematical model;risk reduction probabilistic trust model trust computational model beta trust model hmm trust model hidden markov model uncertainty driven method trust based utility maximization;probabilistic logic;risk reduction probabilistic trust model uncertainty;virtual worlds	Computational models of trust try to transfer the concept of trust from the real to the virtual world. While such models have been widely investigated in the past decade, the uncertainty involved in trust computation has been overlooked in the literature. In this paper, uncertainty of probabilistic trust models is quantified using confidence intervals and its factors are determined through simulation. The results confirm the importance and highlight the amount of uncertainty in the Beta and HMM (Hidden Markov Model) trust models. In addition, an uncertainty-driven method is proposed which reduces the risk involved in the trust-based utility maximization according to uncertainty.	beta;computation;entropy maximization;hidden markov model;markov chain;simulation;trust metric;virtual world	Sadegh Dorri Nogoorani;Rasool Jalili	2012	2012 IEEE 26th International Conference on Advanced Information Networking and Applications	10.1109/AINA.2012.73	von neumann stability analysis;confidence interval;uncertainty analysis;uncertainty;computer science;machine learning;mathematical model;data mining;absolute risk reduction;probabilistic logic;trustworthy computing;computational model;sensitivity analysis;hidden markov model;statistics	SE	-11.89390260178411	-9.676302489724177	133049
a351ad1b253f280a5fa8ab4d57400f4ad24a7aee	ordinal invariance in multicoalitional bargaining	satisfiability;non transferable utility;h social sciences general	A multicoalitional bargaining problem is a non-transferable utility game and for each coalit bargaining rule. We look for ordinally invariant solutions to such problems and discover a su of Bennett’s (1997, Games Econ. Behav. 19, 151–179) that satisfies the property. On a s of problems that is closely related to standard bargaining problems and allocation problem majority decision-making, the two rules coincide. Therefore, Bennett solutions to such pro are immune to misrepresentation of cardinal utility information. We also show that Shapley–S solution to any bargaining problem is the limit of a sequence of unique Bennett solutions to ass multicoalitional problems.  2003 Elsevier Inc. All rights reserved. JEL classification: C71; C78; D30	ordinal data;refinement (computing);rule 110;stable marriage problem	Özgür Kibris	2004	Games and Economic Behavior	10.1016/S0899-8256(03)00046-0	bargaining problem;mathematical optimization;mathematics;mathematical economics;welfare economics;satisfiability	AI	-6.392243044621469	-1.5838194524079525	133109
0d977e65c860d468a13af25e717d3476622bae41	interpreting statutory predicates	legislation;case base reasoning;hybrid approach;rule based reasoning;expert system	In this paper we discuss a hybrid approach to the problem of statutory interpretation that involves combining our past approach to case-based reasoning (“CBA”), as exemplified in our previous HYPO and TAX-HYPO systems, with traditional rule-based reasoning (“RBR”), as exemplified by expert systems. We do not tackle the fullblown version of statutory interpretation, which would include reasoning with legislative intent or other normative aspects (the “ought”), but confine ourselves to reasoning with explicit cases and rules. We discuss strategies that can be used to guide interpretation, particularly the interleaving of CBR and RBR, and how they are used in an agenda-based architecture, called CABARET, which we are currently developing in a general way and experimenting with in the particular area of Section §280A(c)(1) of the U.S. Internal Revenue Code, which deals with the so called “home office deduction”.	case-based reasoning;experiment;expert system;forward error correction;logic programming;natural deduction;station hypo	Edwina L. Rissland;David B. Skalak	1989		10.1145/74014.74021	case-based reasoning;analytic reasoning;qualitative reasoning;computer science;knowledge management;artificial intelligence;model-based reasoning;data mining;reasoning system;deductive reasoning;expert system	AI	-17.659659863256454	2.3933058576270625	133139
5c2c4aaa239c46ba70c4ad6109da2a92a2aeaa59	an autonomous cooperative system for high speed sorting systems	high speed sorting systems;software holons;self contained software entities;autonomous cooperative system;software holons autonomous cooperative system high speed sorting systems flexible control holonic manufacturing system distributed artificial intelligence self contained software entities intelligent agent technology;flexible control;software agents multi agent systems production control materials handling intelligent control;intelligent control;software agents;multi agent systems;control system;production control;cooperative systems;materials handling;distributed artificial intelligence;normal operator;intelligent agent technology;holonic manufacturing system;cooperative systems sorting control systems automatic control humans automation electrical equipment industry intelligent agent optimal control environmental economics;high speed	In a high speed sorting system, the ability of the control system to optimally utilize the available resources and to react to changes in a timely manner determines the economic viability of that system. An Autonomous Cooperative System (ACS) for flexible control of high speed sorting systems is presented. Each section of the sorting system is provided with sufficient intelligence and autonomy to flexibly control the operations of the physical equipment. Control is carried out while the overall operation of the system is optimized through cooperation among the controlled sections. The operation of the sorting system is observed during conditions of normal operation and equipment failures. The results show how an ACS can be used to reduce the impact of perturbations in sorting systems.	sorting	Sivaram Balasubramanian;Francisco P. Maturana;Dave Vasko	2000		10.1109/DEXA.2000.875036	simulation;computer science;control system;artificial intelligence;software agent;normal operator;intelligent control	Robotics	-12.293303803746081	-5.022529382734993	133233
6a96d6cda5864543c39162a33ba4b2977606b5cb	cake, death, and trolleys: dilemmas as benchmarks of ethical decision-making		Artificial intelligence (AI) systems are becoming part of our lives and societies. The more decisions such systems make for us, the more we need to ensure that the decisions they make have a positive individual and societal ethical impact. How can we estimate how good a system is at making ethical decisions? Benchmarking is used to evaluate how good a machine or a process performs with respect to industry bests. In this paper we argue that (some) ethical dilemmas can be used as benchmarks for estimating the ethical performance of an autonomous system. We advocate that an open source repository of such dilemmas should be maintained. We present a prototype of such a repository available at https://imdb. uib.no/dilemmaz/articles/all1.		Edvard P. Bjørgen;S. Madsen;Therese S. Bjørknes;Fredrik V. Heimsæter;Robin Håvik;Morten Linderud;Per-Niklas Longberg;Louise A. Dennis;Marija Slavkovik	2018		10.1145/3278721.3278767	machine learning;management science;artificial intelligence;benchmarking;autonomous system (mathematics);computer science;machine ethics;ethical decision	AI	-10.67155995303796	-9.203548779175192	133300
1426d519b21c1bd9d7db60407cafdfd5cb203521	techreport: time-sensitive probabilistic inference for the edge		In recent years the two trends of edge computing and artificial intelligence became both crucial for information processing infrastructures. While the centralized analysis of massive amounts of data seems to be at odds with computation on the outer edge of distributed systems, we explore the properties of eventually consistent systems and statistics to identify sound formalisms for probabilistic inference on the edge. In particular we treat time itself as a random variable that we incorporate into statistical models through probabilistic programming.	artificial intelligence;centralized computing;computation;distributed computing;edge computing;eventual consistency;information processing;statistical model	Christian Weilbach;Annette Bieniusa	2017	CoRR		probabilistic analysis of algorithms;distributed computing;divergence-from-randomness model;variable elimination;probabilistic logic;machine learning;probabilistic ctl;inference;computer science;statistical model;artificial intelligence;edge computing	ML	-9.414258681607086	2.800781974021378	133606
abb5facc7a57a009aa59158c4adc90c13e1266d3	analyses on continuance of unpopular international airline by local government	local government airports aerospace engineering regression analysis job shop scheduling power generation economics ieee news costs cybernetics usa councils;game theory;policy analysis;airports;international flight continuation;data mining;companies;local government;travel industry game theory regression analysis;asiana airlines;regression analysis unpopular international airline continuance analysis asiana airlines tottori prefecture international flight continuation nash bargaining model;bargaining game;policy analysis bargaining game mutiple regression analyses;regression analysis;economics;tottori prefecture;nash bargaining model;mutiple regression analyses;unpopular international airline continuance analysis;travel industry	The bargaining between Asiana Airlines and Tottori Prefecture in Japan on continuation of international flight between Seoul in Korea and Yonago of Tottori in Japan in 2007 is analyzed. Nash bargaining model is employed to validate the bargaining power of both parties. Moreover, Regression Analyses are executed to find significant factors that have effects on flight users. Among others, the study finds that Tottori Prefecture had reasonable bargaining power at the bargaining process, and the two Tottori policies of User Promotion for Korean Travelers and Attraction Projects for Foreigners have been positively affected the number of users of the flights.	continuation;nash equilibrium	Kei Fukuyama;Yuki Ikeda	2009	2009 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2009.5346000	game theory;policy analysis;tourism;regression analysis	Robotics	-5.051929536022705	-9.620705467905857	133662
5cc96daf76dc913c178ebdab8d39d4615ee783fb	reassessing logical hylomorphism and the demarcation of logical constants	hylomorphism;demarcation of logic;logical constants;history of logic	The paper investigates the propriety of applying the form versus matter distinction to arguments and to logic in general. Its main point is that many of the currently pervasive views on form and matter with respect to logic rest on several substantive and even contentious assumptions which are nevertheless uncritically accepted. Indeed, many of the issues raised by the application of this distinction to arguments seem to be related to a questionable combination of different presuppositions and expectations; this holds in particular of the vexed issue of demarcating the class of logical constants. I begin with a characterization of currently widespread views on form and matter in logic, which I refer to as ‘logical hylomorphism as we know it’—LHAWKI, for short—and argue that the hylomorphism underlying LHAWKI is mereological. Next, I sketch an overview of the historical developments leading from Aristotelian, non-mereological metaphysical hylomorphism to mereological logical hylomorphism (LHAWKI). I conclude with a reassessment of the prospects for the combination of hylomorphism and logic, arguing in particular that LHAWKI is not the only and certainly not the most suitable version of logical hylomorphism. In particular, this implies that the project of demarcating the class of logical constants as a means to define the scope and nature of logic rests on highly problematic assumptions.	andrew donald booth;betti's theorem;demarcation point;logical constant;mereology;pervasive informatics;eric	Catarina Dutilh Novaes	2010	Synthese	10.1007/s11229-010-9825-0	philosophy;epistemology;mathematics;algorithm	Logic	-13.084770383532259	3.911444869219089	133920
37d4b2207d8086c3ee8e30b507f5673b803de5e9	reducing price of anarchy of selfish task allocation with more selfishness	selfish task allocation;price of anarchy;selfish scheduling;期刊论文	In this paper we consider the task allocation problem from a new game theoretic perspective. We assume that tasks and machines are both controlled by selfish agents with two distinct objectives, which stands in contrast to the passive role of machines in the traditional model of selfish task allocation. To characterize the outcome of this new game where two classes of players interact, we introduce the concept of dual equilibrium. We prove that the price of anarchy with respect to dual equilibria is 1.4, which is considerably smaller than the counterpart 2 in the traditional model. Our study shows that activating more freedom and selfishness in a game may bring about a better global outcome.	anarchy	Xujin Chen;Xiao-Dong Hu;Weidong Ma;Changjun Wang	2013	Theor. Comput. Sci.	10.1016/j.tcs.2013.02.019	price of stability;mathematical economics;price of anarchy	ECom	-5.654963665651656	-0.27127633661991934	133923
989326a5d73c829e3d3c72d601d51269b5fc73d5	reasoning with differing tasks and response formats		This study investigated the role probabilistic and deductive relations play in the reasoning process. It was predicted that when taking an analytic stance to a problem, it would take longer to evaluate inferences when asked how probable it is that the conclusion is true, than when asked whether the conclusion follows or not from the premises. Contrary to this prediction, people responded faster when the response format was continuous. However, there was no effect of argument type with continuous response format, suggesting people did not assess entailment relations in this condition. Options to address the issue further are discussed.	deductive database	Nicole Cruz de Echeverría Loebell;Markus Knauff	2013			inductive reasoning;cognitive psychology;deductive reasoning;social psychology;belief revision;psychology;experimental psychology;probabilistic logic;logical consequence;inference;norm (social)	AI	-14.026490599945713	3.2508087481285353	133962
b25abedd74278720e7bec09dd20d788484bc4e6e	on the behavior of competing markets populated by automated traders	trading agents;continuous double auction;multiple markets;agent based;stock exchange	Real market institutions, stock and commodity exchanges for example, do not occur in isolation. Company stock is frequently listed on severa l stock exchanges, allowing traders to potentially trade such stock in different ma rkets. While there has been extensive research into agent-based trading in indi vidual markets, there is little work on agents that trade in such multiple market scena rios. Our work seeks to address this imbalance. Here we provide an initial analy sis of the behavior of trading agents that are free to move between a number of parallel markets, where markets are able to charge traders in a variety of ways. We show the movement of traders between markets, sketch some adaptive strate gies that markets may use to adjust charges, evaluate the effectiveness of thes e s rategies, and give some results which show the effect of trader movement on pr operties of the markets.	agent-based model;commodity money;darknet market;emoticon;evaporation;experiment;flaming (internet);game theory;population;steady state;traders	Jinzhong Niu;Kai Cai;Simon Parsons;Elizabeth Sklar	2007		10.1007/978-3-540-88713-3_14	financial economics;variable pricing;market system;capital market;business;commerce;algorithmic trading	AI	-7.623145676218226	-9.246027788549	134051
ace93538b314351006e92a921bd56c362acf946b	mere faith and entitlement	skepticism;justification;entitlement;dogmatism	The scandal to philosophy and human reason, wrote Kant, is that we must take the existence of material objects on mere faith. In contrast, the skeptical paradox that has scandalized recent philosophy is not formulated in terms of faith, but rather in terms of justification, warrant, and entitlement. I argue that most contemporary approaches to the paradox (both dogmatist/liberal and default/conservative) do not address the traditional problem that scandalized Kant, and that the status of having a warrant (or justification) that is derived from entitlement is irrelevant to whether we take our beliefs on mere faith. For, one can have the sort of warrant that most contemporary anti-skeptics posit while still taking one’s belief on mere faith. An alternative approach to the traditional problem is sketched, one that still makes use of contemporary insights about “entitlement.”	moravec's paradox;relevance	Yuval Avnur	2011	Synthese	10.1007/s11229-011-0053-z	philosophy;epistemology	AI	-13.207422021170949	3.490987904491425	134358
a91ad923f5a347750a7b4df7f96fdf305e258b8f	automata-based bottom-up design of conflict-free security policies specified as policy expressions		Security policies (or more briefly: policies) are used to filter accesses to computing resources. A policy is usually specified by a table of rules, where each rule specifies conditions to accept or reject an access request. Since the acceptance of malicious requests or the rejection of legitimate requests may lead to serious consequences, the correct design of policies is very important. The present paper is inspired by two works: the first one uses an automata-based method to design policies, while the second one suggests a bottom-up design method of policies specified as policy expressions. A policy expression looks like a boolean expression, where policies are composed using three operators: (lnot ), (wedge ), (vee ). In this paper, we generalize the automata-based method for the bottom-up design of policies specified as policy expressions. In our context, designing a policy specified as a policy expression ( PE ) amounts to constructing an automaton (varGamma _{ PE }) that models the access control specified in ( PE ). To respect the essence of bottom-up design, the automaton (varGamma _{ PE }) is constructed incrementally, by first constructing the automata that model the basic policies that compose ( PE ), and then constructing incrementally the automata that model the subexpressions that compose ( PE ), until we obtain (varGamma _{ PE }). Then we show how to use (varGamma _{ PE }) to determine whether ( PE ) verifies several properties, namely adequacy, implication, and equivalence. Also, we study the problem of conflicting rules, i.e. policy rules that do not agree on whether some request must be accepted or rejected. We show that our bottom-up design supports any strategy of conflict resolution.		Ahmed Khoumsi;Mohammed Erradi	2018		10.1007/978-3-030-05529-5_23	equivalence (measure theory);operator (computer programming);access control;discrete mathematics;boolean expression;security policy;conflict resolution;expression (mathematics);automaton;computer science	Logic	-11.770946499585326	0.13714083398986301	134373
58739cd80fa8f08cb53adef7dd2002b615641cb7	or forum - perspectives on queues: social justice and the psychology of queueing	sistema fila espera;file attente;systeme attente;social justice;queue;572 queueing in service industries;681 queues as experienced by customers;queueing system;fila espera;531 the many attributes of queues	Queues involve waiting, to be sure, but one's attitudes toward queues may be influenced more strongly by other factors. For instance, customers may become infuriated if they experience social injustice, defined as violation of first in, first out. Queueing environment and feedback regarding the likely magnitude of the delay can also influence customer attitudes and ultimately, in many instances, a firm's market share. Even if we focus on the wait itself, the “outcome” of the queueing experience may vary nonlinearly with the delay, thus reducing the importance of average time in queue, the traditional measure of queueing performance. This speculative paper uses personal experiences, published and unpublished cases, and occasionally “the literature” to begin to organize our thoughts on the important attributes of queueing. To flesh out more of these issues, the author asks for your cards and letters.		Richard C. Larson	1987	Operations Research	10.1287/opre.35.6.895	real-time computing;simulation;computer science;operations management;queue	Metrics	-11.973480635570098	-3.7214087807401737	134414
d84f8c795266ffe451276aa8ab5bfc4899dc0ef4	how videogames express ideas		What are the exact aspects of the videogame medium, the precise features or combinations of features that lend themselves to expressing ideas and meaning? To chart this out, I begin with an American legal case that serves as a foundation for the basic issues involved and then move on to show how this relates to some of the broader attitudes the world of videogame discourse. Based on this, I break down the expressive strategies of videogames into three aspects—non-playable sequences, rule-based systems, and the relationship between the two—which I then illustrate with examples proving that videogames can indeed be an expressive medium.	rule-based system	Matthew Weise	2003			management science;legal case;computer science;chart	NLP	-14.550829769905304	2.8222189113132674	134432
03d45df42ac5298dc33e78c786b1de63e9a14415	on cooperative games, inseparable by semivalues	semivalue;cooperative game;shared hernel;shapley value;shuffle game	Semivalues like the Shapley value and the Banzhaf value may assign the same payoff vector to different games. It is even possible that two games attain the same outcome for all semivalues. Due to the linearity of the semivalues, this exactly occurs in case the difference of the two games is an element of the kernel of each semivalue. The intersection of these kernels is called the shared kernel, and its game theoretic importance is that two games can be evaluated differently by semivalues if and only if their difference is not a shared kernel element. The shared kernel is a linear subspace of games. The corresponding linear equality system is provided so that one is able to check membership. The shared kernel is spanned by specific -valued games, referred to as shuffle games. We provide a basis with shuffle games, based on an a-priori given ordering of the players.	kernel (operating system);linear equation;separable polynomial;theory	Rafael Amer;Jean Derks;José Miguel Giménez	2003	Int. J. Game Theory	10.1007/s001820300144	combinatorics;discrete mathematics;economics;mathematics;shapley value;mathematical economics	ECom	-6.627392200098045	-2.549368483546574	134876
dd0c2de0c115852126bb3579f46082b455cebea6	the peer review game: an agent-based model of scientists facing resource constraints and institutional pressures	peer review;cooperation;game theory;scientist strategies;agent-based model	This paper looks at peer review as a cooperation dilemma through a game-theory framework. We built an agent-based model to estimate how much the quality of peer review is influenced by different resource allocation strategies followed by scientists dealing with multiple tasks, i.e., publishing and reviewing. We assumed that scientists were sensitive to acceptance or rejection of their manuscripts and the fairness of peer review to which they were exposed before reviewing. We also assumed that they could be realistic or excessively over-confident about the quality of their manuscripts when reviewing. Furthermore, we assumed they could be sensitive to competitive pressures provided by the institutional context in which they were embedded. Results showed that the bias and quality of publications greatly depend on reviewer motivations but also that context pressures can have a negative effect. However, while excessive competition can be detrimental to minimising publication bias, a certain level of competition is instrumental to ensure the high quality of publication especially when scientists accept reviewing for reciprocity motives.	agent-based model;assumed;deny (action);display resolution;embedded system;embedding;fairness measure;game theory;manuscripts;multi-institutional systems;peer review;quantity;rejection sampling;resource allocation;review [publication type]	Federico Bianchi;Francisco Grimaldo;Giangiacomo Bravo;Flaminio Squazzoni	2018		10.1007/s11192-018-2825-4	agent-based model;computer science;game theory;dilemma;data mining;management science;reciprocity (social psychology);resource allocation;publication bias;publishing	AI	-10.946796239796633	-9.668592744278723	134922
7f90c14254e4842c6d46ee2f5134808ecd80599f	the total belief theorem		In this paper, motivated by the treatment of conditional constraints in the data association problem, we state and prove the generalisation of the law of total probability to belief functions, as finite random sets. Our results apply to the case in which Dempster’s conditioning is employed. We show that the solution to the resulting total belief problem is in general not unique, whereas it is unique when the a-priori belief function is Bayesian. Examples and case studies underpin the theoretical contributions. Finally, our results are compared to previous related work on the generalisation of Jeffrey’s rule by Spies and Smets.	coherence (physics);correspondence problem;disjunctive normal form;linear system;marginal model;matroid	Chunlai Zhou;Fabio Cuzzolin	2017			artificial intelligence;machine learning;computer science	AI	-9.364459391077494	-0.10960223433956828	135021
7d84e7cca75b14f0b3ff04a480ed01063571166c	when does a 'visual proof by induction' serve a proof-like function in mathematics?		A proof by mathematical induction demonstrates that a general theorem is necessarily true for all natural numbers. It has been suggested that some theorems may also be proven by a ‘visual proof by induction’ (Brown, 2010), despite the fact that the image only displays particular cases of the general theorem. In this study we examine the nature of the conclusions drawn from a visual proof by induction. We find that, while most university-educated viewers demonstrate a willingness to generalize the statement to nearby cases not depicted in the image, only viewers who have been trained in formal proof strategies show significantly higher resistance to the suggestion of large-magnitude counterexamples to the theorem. We conclude that for most university-educated adults without proof-training the image serves as the basis of a standard inductive generalization and does not provide the degree of certainty required for mathematical proof. 6475 Alvarado Road, Suite 206 San Diego, CA 92120 619-594-1579 http://crmse.sdsu.edu	formal proof;mathematical induction	Josephine Relaford-Doyle;Rafael Núñez	2017			cognitive psychology;psychology;mathematical induction	ML	-11.35251873653577	2.7591966065692466	135166
d50d15dbd752256c9af76fc881ffd9487a8b7703	moore’s paradox is not just another pragmatic paradox	g e moore;belief;pragmatic paradox;i;self refutation;first person;satisfiability;higher order;moore s paradox	One version of Moore’s Paradox is the challenge to account for the absurdity of beliefs purportedly expressed by someone who asserts sentences of the form ‘p & I do not believe that p’ (‘Moorean sentences’). The absurdity of these beliefs is philosophically puzzling, given that Moorean sentences (i) are contingent and often true; and (ii) express contents that are unproblematic when presented in the third-person. In this paper I critically examine the most popular proposed solution to these two puzzles, according to which Moorean beliefs are absurd because Moorean sentences are instances of pragmatic paradox; that is to say, the propositions they express are necessarily false-when-believed. My conclusion is that while a Moorean belief is a pragmatic paradox, it is not just another pragmatic paradox, because this diagnosis does not explain all the puzzling features of Moorean beliefs. In particularly, while this analysis is plausible in relation to the puzzle posed by characteristic (i) of Moorean sentences, I argue that it fails to account for (ii). I do so in the course of an attempt to formulate the definition of a pragmatic paradox in more precise formal terms, in order to see whether the definition is satisfied by Moorean sentences, but not by their third-person transpositions. For only an account which can do so could address (ii) adequately. After rejecting a number of attempted formalizations, I arrive at a definition which delivers the right results. The problem with this definition, however, is that it has to be couched in first-person terms, making an essential use of ‘I’. Thus the problem of accounting for first-/third-person asymmetry recurs at a higher order, which shows that the Pragmatic Paradox Resolution fails to identify the source of such asymmetry highlighted by Moore’s Paradox.	contingency (philosophy);epr paradox	Timothy Chan	2008	Synthese	10.1007/s11229-008-9403-x	higher-order logic;philosophy;epistemology;belief;mathematics;algorithm;satisfiability	NLP	-13.402111164408662	3.8896893505191983	135271
4b175e09cbb2b9b2238688cbfee1c37f9c57ceaf	the theory of implementation of social choice rules	decentralization;dominance;implementation theory;game theory;nash equilibrium;asymmetric information;monotonicity;mechanism design	Suppose that the goals of a society can be summarized in a social choice rule, i.e., a mapping from relevant underlying parameters to final outcomes. Typically, the underlying parameters (e.g., individual preferences) are unknown to the public authority. The implementation problem is then formulated: Under what circumstances can one design a mechanism so that the unknown information is truthfully elicited and the social optimum ends up being implemented? In designing such a mechanism, appropriate incentives must be given to the agents so that they do not wish to misrepresent their information. The theory of implementation or mechanism design formalizes this “social engineering” problem and provides answers to the question just posed. I survey the theory of implementation in this article, emphasizing the results under two different benchmarks (that agents have dominant strategies and that they play a Nash equilibrium). Examples discussed include voting, and the allocation of private and public goods under complete and incomplete information.	benchmark (computing);nash equilibrium;social engineering (security)	Roberto Serrano	2004	SIAM Review	10.1137/S0036144503435945	implementation theory;economics;public economics;mathematical economics;welfare economics	ECom	-8.232097908473085	-5.185866745120763	135312
7ebe56fd183c4db94305107922e836c20f56f17e	the divine essence and the conception of god in spinoza	god;substance;infini;dieu;logic;spinoza b;totalite;ethics;totality;absolute;pouvoir;infinite;attributes;philosophy;epistemology;metaphysics;ethique;eternite;spinoza;attribute;essence;power;absolu;eternity;attribut	I argue against a prevailing view that the essence of Godis identical with the attributes. I show that given what Spinoza says in 2d2 – Spinoza'spurported definition of the essence of a thing – the attributes cannot be identical withthe essence of God (whether the essence of God is understood as the distinct attributesor as a totality of indistinct attributes). I argue that while the attributes do notsatisfy the stipulations of 2d2 relative to God, absolutely infinite and eternal power does satisfythose stipulations. Hence, I conclude that absolutely infinite and eternal power is God'sessence and that the attributes are expressions of that power.		Sherry Deveaux	2003	Synthese	10.1023/A:1023594617953	eternity;ethics;philosophy;epistemology;metaphysics;power;logic	DB	-12.189695972795112	3.726987227968126	135390
b0b5fa1dc3df0bdde6c8aa98ed80c133da48bebe	boolean games revisited	game theory;formal model;utility function;nash equilibria;communication conference;strategic interaction;boolean games	Game theory is a widely used formal model for studying strategical interactions between agents. Boolean games[8] are two players, zero-sum static games where players’ utility f unctions are binary and described by a single propositional formula, and the strategies available to a player consist of truth assignmen ts to each of a given set of propositional variables (the variables controlledby the player.) We generalize the framework to n-players games which are not necessarily zero-sum. We give simple characterizat ions of Nash equilibria and dominated strategies, and investigate the computational complexity of the related problems.	bitwise operation;boolean algebra;boolean circuit;cp/m;computational complexity theory;formal language;game theory;interaction;nash equilibrium;numerical analysis;ordinal data;propositional variable;service-oriented architecture	Elise Bonzon;Marie-Christine Lagasquie-Schiex;Jérôme Lang;Bruno Zanuttini	2006			combinatorial game theory;bayesian game;game theory;best response;coordination game;repeated game;screening game;normal-form game;boolean function;sequential game;nash equilibrium	AI	-8.24584848493604	2.1580012343874166	135843
c4be70804d935ab2f4daf26669c9601405dd62b2	learning to scale payments in crowdsourcing with properboost	online learning;incentive schemes;reputation systems;crowdsourcing	Motivating workers to provide significant effort has been recognized as an important issue in crowdsourcing. It is important not only to compensate worker effort, but also to discourage low-quality workers from participating. Several proper incentive schemes have been proposed for this purpose; they are either based on gold tasks or on peer consistency in individual tasks. As the rewards cannot become negative, these schemes have difficulty in achieving zero expected reward for random answers. We describe a novel boosting scheme, ProperRBoost, that improves the efficiency of existing incentive schemes by making a better separation between incentives for high and low quality work, and effectively discourages random answers by assigning them near minimal average rewards. We show the actual performance of the boosting scheme through simulations of various worker strategies.	boosting (machine learning);crowdsourcing;display resolution;experiment;interaction;peer-to-peer;simulation;spamming	Goran Radanovic;Boi Faltings	2016			computer science;knowledge management;data mining;crowdsourcing	ML	-10.36283060887794	-8.28324482365419	135904
7617b0efc530b933af4fe3957805f1a94706f666	"""a """"quantal regret"""" method for structural econometrics in repeated games"""		"""We suggest a general method for inferring players' values from their actions in repeated games. The method extends and improves upon the recent suggestion of (Nekipelov et al., EC 2015) and is based on the assumption that players are more likely to exhibit sequences of actions that have lower regret. We evaluate this """"quantal-regret"""" method on two different datasets from experiments of repeated games with controlled player values: those of (Selten and Chmura, AER 2008) on a variety of two-player 2x2 games and our own experiment on ad-auctions (Noti et al., WWW 2014). We find that the quantal-regret method is consistently and significantly more precise than either """"classic"""" econometric methods that are based on Nash equilibria, or the """"min-regret"""" method of (Nekipelov et al., EC 2015)."""	experiment;maxima and minima;nash equilibrium;quantum;regret (decision theory);www;word lists by frequency	Noam Nisan;Gali Noti	2017		10.1145/3033274.3085111	econometrics;mathematics;mathematical economics;welfare economics;regret	AI	-6.9159792357176935	-5.36900133361107	136424
71044c011853b465fff26d76a5d17db509eba838	lexicographic choice functions without archimedeanicity	choice functions;technology and engineering;archimedeanicity;lexicographic probabilities;maximality	We investigate the connection between choice functions and lexicographic probabilities, by means of the convexity axiom considered by [7] but without imposing any Archimedean condition. We show that lexicographic probabilities are related to a particular type of sets of desirable gambles, and investigate the properties of the coherent choice function this induces via maximality. Finally, we show that the convexity axiom is necessary but not sufficient for a coherent choice function to be the infimum of a class of lexicographic ones.	coherence (physics);lexicographical order;lexicography	Arthur Van Camp;Enrique Miranda;Gert de Cooman	2016		10.1007/978-3-319-42972-4_59	mathematical optimization;discrete mathematics;mathematics;lexicographic preferences	Web+IR	-7.2276859900405475	-1.0964315242575442	136425
86314646058ddf49f91d624384cf073b4690340a	a fcm analysis for supply chain management	supply chain management fuzzy reasoning production engineering computing;fuzzy cognitive maps fcm analysis supply chain management business relations political environment economic environment sc performance;fuzzy logic;fuzzy cognitive maps;event management supply chain management fuzzy logic fuzzy cognitive maps;event management;supply chains biological system modeling pragmatics companies fuzzy cognitive maps;supply chain management	Supply chain consists of many participants across multiple levels of business world. From producers to suppliers, to transporters and to retailers the number of interacting participants rises as the supply chain expands its boundaries. As the size of supply chain grows the problems within it increased too. The modern supply chain is not limited in a small national market where the participants are limited and each one knows its contributor. Modern markets spread their boundaries across continents, where new opportunities and new threats appear. Business relations, political and economic environment determine more or less the performance of the supply chain form a wider sense. In this environment with many concepts causally interrelated, we used the knowledge of the experts and we suggest a FCM that models the key concepts of SC performance showing the exogenous and internal concepts that believed that affect the SC performance.	fuzzy cognitive map;interaction;knowledge representation and reasoning;nash equilibrium;supply chain attack;threat (computer)	George Vlahakis;Dimitris Apostolou	2015	2015 6th International Conference on Information, Intelligence, Systems and Applications (IISA)	10.1109/IISA.2015.7387986	supply chain management;service management;knowledge management;marketing;operations management;business	AI	-7.019157156810234	-7.821557835864208	136483
013d76e134666149bc41b3b9eee98f9f9ad5dc3a	leximin rules for bankruptcy problems under uncertainty	uncertainty;multi dimensional;bankruptcy rules;preferences	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	algorithmic efficiency;francis;primary source;utility functions on indivisible goods	Miguel A. Hinojosa;Amparo M. Mármol;F. J. Sánchez	2014	Int. J. Systems Science	10.1080/00207721.2012.659705	actuarial science;uncertainty;mathematics;statistics	Robotics	-14.361774120535237	-5.599765728532562	136893
de8a8478cede309c59c083ccce72c26200b32f88	terms for talking about information and communication	index;icon;symbol;coding;communication;context;information	This paper offers terms for talking about information and how it relates to both matter-energy and communication, by: (1) Identifying three different levels of signs: Index, based in contiguity, icon, based in similarity, and symbol, based in convention; (2) examining three kinds of coding: Analogic differences, which deal with positive quantities having contiguous and continuous values, and digital distinctions, which include “either/or functions”, discrete values, and capacities for negation, decontextualization, and abstract concept-transfer, and finally, iconic coding, which incorporates both analogic differences and digital distinctions; and (3) differentiating between “information theoretic” orientations (which deal with data, what is “given as meaningful” according to selections and combinations within “contexts of choice”) and “communication theoretic” ones (which deal with capta, what is “taken as meaningful” according to various “choices of context”). Finally, a brief envoi reflects on how information broadly construed relates to probability and entropy.	a mathematical theory of communication;ampersand	Corey Anton	2012	Information	10.3390/info3030351	information;epistemology;computer science;artificial intelligence;machine learning;mathematics;symbol;coding;statistics	Web+IR	-10.121171149736444	4.15044585829706	137119
8b9914c371aa6a589be6bef06704389aed9d2519	min, max, and sum	utilitarianism;symmetry;journal of economic literature;rawls social welfare function;social choice;monotonicty;maximin utility	This paper provides characterization theorems for preferences that can be represented by U(x1; : : : ; xn) = minfxkg, U(x1; : : : ; xn) = maxfxkg, U(x1; : : : ; xn) = P u(xk), or combinations of these functionals. The main assumption is partial separability, where changing a common component of two vectors does not reverse strict preferences, but may turn strict preferences into indi erence. We discuss applications of our results to social choice. Uzi Segal thanks SSHRCC and Joel Sobel thanks NSF and CASBS for nancial support. Department of Economics, Boston College, Chestnut Hill, MA 02467, U.S.A. E-mail: segalu@bc.edu Department of Economics, University of California, San Diego, La Jolla, CA 92093, U.S.A. E-mail: jsobel@weber.ucsd.edu	characterization test;ibm notes;jolla;linear algebra;linear separability;sobel operator	Uzi Segal;Joel Sobel	2002	J. Economic Theory	10.1006/jeth.2001.2859	social choice theory;economics;utilitarianism;public economics;macroeconomics;mathematics;microeconomics;symmetry;mathematical economics;welfare economics	ML	-5.911947630319595	-1.4383607144167325	137255
85884fb2d8aebd1e15faa41ff8dea99faa4dd7eb	a framework for performance evaluation of communication alternatives for intelligent transportation systems	south carolina;communication system;performance evaluation;wireless;greenville south carolina;case studies;measures of effectiveness;intelligent transport system;communication systems;intelligent transportation systems;public sector;evaluation;system architecture;architecture;evaluation framework	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	bandwidth (signal processing);closed-circuit television;communication complexity;cost efficiency;francis;human factors and ergonomics;performance evaluation;primary source;requirement;serial experiments lain;simulation;software deployment;vehicle-to-vehicle	Yongchang Ma;Yan Zhou;Mashrur Chowdhury;Kuang-Ching Wang;Ryan Fries	2009	J. Intellig. Transport. Systems	10.1080/15472450903084212	embedded system;simulation;telecommunications;computer science;systems engineering;engineering;civil engineering;communications system;computer network;systems architecture	Robotics	-16.013503654591908	-5.355777310191146	137272
5d69170e5d7478441c014371a62b9ec63bd4fdd1	noise channels: glitch and error in digital culture, by peter krapp. minneapolis, mn: university of minnesota press, 2011, 162 pp. $22.50 paper. isbn 9780816676255 (paper)	minnesota press;digital culture;peter krapp;noise channels	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;glitch;international standard book number;primary source	Kathryn Thompson	2014	Inf. Soc.	10.1080/01972243.2014.856671	media studies	Robotics	-15.33555208585624	-5.62640717217359	137807
a69e5a19dd3fb3dfe4d63c2ef234c03b7abb55f7	the logic of priori and a posteriori rationality in strategic games		We propose a logic for describing the interaction between knowledge, preference, and the freedom to act, and their interactions with the norms of Priori and a Posteriori rationality, which we have argued for in previous work [3]. We then apply it to strategic games to characterise weak dominance and Nash equilibrium.	rationality	Meiyun Guo;Jeremy Seligman	2013		10.1007/978-3-642-40948-6_17	mathematics;mathematical economics;social psychology;welfare economics	ECom	-9.175009585594571	-1.9597280173491634	137920
e10f93d8cc49e120c18f883028fa7614336e58f7	trust prediction in online communities employing neurofuzzy approach		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	adaptive neuro fuzzy inference system;bayesian network;binary classification;c4.5 algorithm;computation;decision tree;distrust;f1 score;francis;inference engine;interaction;list of algorithms;logistic regression;online community;primary source	Hossein Abbasimehr;Mohammad Jafar Tarokh	2015	Applied Artificial Intelligence	10.1080/08839514.2015.1051894	artificial intelligence;machine learning;data mining	Robotics	-14.50682814338933	-5.732412401152116	138032
71fbf6bdcad9274d8f8ac9c750924ae913590983	agent-oriented and distributed assembly task planning for multiple manipulators	manipulators;manipulators industrial control cooperative systems assembling industrial manipulators;generic algorithm;cooperative computational model agent oriented distributed assembly task planning multiple manipulators subtasks feasibility reliability;computer model;automatic generation;computational modeling assembly systems computer science system recovery distributed computing information management contracts;cooperative systems;assembling;industrial control;industrial manipulators	Generally, algorithms for assembly task planning are apt to become complicated. Moreover, it is very difficult to create plans for multiple manipulators, because complicated tasks to divide an obtained plan into some subtasks and allocate them to each manipulator have to be executed. We propose a new assembly task planning system for multiple manipulators in order to avoid these complicated problems and improve the feasibility and the reliability of the whole system. In this system, each of parts composing a machine as regarded as an agent, and a plan for an assembly task is automatically generated as results of autonomous behaviors of these agents. The outline of the planning system based on this concept is described, some simulation results using a cooperative computational model are given. >		Jun-ichi Hirai;Tadashi Nagata	1994		10.1109/IROS.1994.407402	computer simulation;control engineering;real-time computing;simulation;genetic algorithm;computer science;engineering;artificial intelligence	Robotics	-18.384089043471977	-9.849434071825085	138168
d64183b285d0a160ef9d0d104c1d5695cba58187	a power analysis of linear games with consensus	power analysis;91a12;91b12;91a80;voting by count and account;upper bound;shapley shubik power index;indexation;weighted majority game;linear game;91a40;simple game	Linear simple games with consensus are considered. These games are obtained by intersecting a linear simple game (a game where the desirability order for players is total) and a symmetric ( k -out-of- n ) game. We investigate the behavior of the Shapley–Shubik power index when passing from a linear game with consensus level  q  1  to the same game with consensus level  q  2 > q  1 . We also introduce a “range notion”, which intuitively represents the egalitarianism of the Shapley–Shubik index, obtain an upper bound on this measure, and characterize when it is achieved. Finally, the theory developed here is illustrated with several real-world voting systems.	consensus (computer science)	Francesc Carreras;Josep Freixas	2004	Mathematical Social Sciences	10.1016/j.mathsocsci.2004.03.004	bondareva–shapley theorem;minimax;example of a game without a value;power analysis;shapley–shubik power index;extensive-form game;repeated game;mathematics;stochastic game;normal-form game;mathematical economics;upper and lower bounds;sequential game;welfare economics;symmetric game;statistics	Theory	-7.638680950119699	-1.89036439271345	138224
e3ead6978dc82d96f8cd3d20ffaed36f5f5d16de	multivariate reduced-rank regression		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Neil H. Timm	1999	Technometrics	10.1080/00401706.1999.10485939	data mining;statistics	Robotics	-14.475651158837758	-5.7219275558586675	138305
8cba82bd630f41c027c8871be6c42052eb653aa8	pareto-optimal nash equilibrium: sufficient conditions and existence in mixed strategies		This paper considers the Nash equilibrium strategy profiles that are Pareto optimal with respect to the rest Nash equilibrium strategy profiles. The sufficient conditions for the existence of such pure strategy profiles are established. These conditions employ the Germeier convolutions of the payoff functions. For the non-cooperative games with compact strategy sets and continuous payoff functions, the existence of the Pareto optimal Nash equilibria in mixed strategies is proved.	nash equilibrium;pareto efficiency	Vladislav I. Zhukovskiy;Konstantin N. Kudryavtsev	2016	Automation and Remote Control	10.1134/S0005117916080154	price of stability;epsilon-equilibrium;mathematical optimization;traveler's dilemma;best response;trembling hand perfect equilibrium;coordination game;correlated equilibrium;chicken;risk dominance;mathematical economics;matching pennies;equilibrium selection;solution concept;nash equilibrium;symmetric equilibrium	Robotics	-4.844606028533999	-1.5775292351201322	138355
8ff2b71c525640ba68c1899ec36a04f330084cb8	strategy effectiveness of game-theoretical solution concepts in extensive-form general-sum games	software;game theory;solution concepts;artificial intelligence;control and systems engineering;general sum games	Game theory describes the conditions for the strategies of rational agents to form an equilibrium. However, game theory can fail from the prescriptive viewpoint and can serve only as a heuristic recommendation for agents. There exists a plethora of game theoretic solution concepts, however, their effectiveness has never been compared; hence, there is no guideline for selecting correct algorithm for a given domain. Therefore, we compare the effectiveness of solution-concept strategies and strategies computed by Counterfactual regret minimization (CFR) and Monte-Carlo tree search in practice. Our results show that (1) CFR strategies are typically the best, and (2) the effectiveness of the refinements of NE depends on the utility structure of the game.	algorithm;counterfactual conditional;game theory;heuristic;monte carlo tree search;rational agent	Jiri Cermak;Branislav Bosanský;Nicola Gatti	2015			combinatorial game theory;implementation theory;game theory;minimax;positive political theory;simulation;computer science;artificial intelligence;repeated game;management science;screening game;normal-form game;simulations and games in economics education;algorithmic game theory;outcome;sequential game;equilibrium selection;solution concept	AI	-9.617749788500838	-2.907269847888352	138500
e2ff7730a30c6a438932c03aa8b2d42892f219f3	mercury hotspot identification in water conservation area 3, florida, usa	getis ord;mercury;hotspot;r emap	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;mercury;mercury-arc valve;primary source	Paul Julian	2013	Annals of GIS	10.1080/19475683.2013.782469	environmental engineering;geography;hydrology;ecology	Robotics	-15.046192546268149	-6.014351310347902	138526
1a43ec4e20e74734cb0ca1a740a83838e0ee37f6	physicalism and strict implication	a priori knowledge · application conditionals · physicalism · reduction · strict implication · supervenience;functional model;a priori knowledge	The aim of this paper is to determine the plausibility of Robert Kirk’s strict implication thesis as an explication of physicalism and its relation to Jackson and Chalmer’s notion of application conditionals, to the notion of global supervenience and to a posteriori identities. It is argued that the strict implication thesis is subject to the same objection that affects the notion of global supervenience. Furthermore, reference to an idealised physics in the formulation of strict implication threatens to make the thesis vacuous. Third, Kirk’s claim that the strict implication thesis does not entail reduction of the mental to the physical (excluding phenomenal properties) is untenable if a functional model of reduction is preferred over Nagel’s classical model. Finally, Kirk’s claim that the physical facts entail in an a priori way the fact that certain brain states feel somehow seems to be unfounded.	function model;initial condition;jackson;physicalism;plausibility structure;reductionism;strict conditional;supercomputer	Jürgen Schröder	2006	Synthese	10.1007/s11229-006-9024-1	a priori and a posteriori;philosophy;epistemology;function model;mathematics;algorithm	PL	-12.988119102640335	3.7470520866257035	138603
705ee49ec76f6d90f145cd8ba84ae55cec9fdc69	constraints on social dilemmas: an institutional approach	prisoner s dilemma;social dilemma;acid rain;profitability;collective action;environmental problem	Many contemporary political and economic problems have attributes of social dilemmas. These dilemmas are simply characterized as settings in which individuals have a dominant strategy to not cooperate in collective action. However, this choice results in a Pareto-inferior outcome. Likewise, a dominated strategy exists that results in a Pareto-superior outcome. Where cooperation is absent, this problem has been described as ann-person prisoner's dilemma. This paper discusses the environmental problem of acid rain as such a social dilemma. Relying on a series of laboratory experimental settings, the argument is that many social dilemmas can be resolved through the construction of institutional mechanisms allowing for the coordination of the participant's joint strategies. Policy analysts in particular can profit from focusing on institutional solutions to social dilemmas.		Rick K. Wilson	1984	Annals OR	10.1007/BF01874738	superrationality;acid rain;economics;socioeconomics;social dilemma;welfare economics;prisoner's dilemma;profitability index	ML	-7.09285271296747	-7.339447113466874	138862
35c9065806dc1e27cbd81651390595d0fbe75686	majority rip-off in referendum voting	income distribution;universiteitsbibliotheek;direct democracy;majority voting	Many democracies complement a parliamentarian system with elements of direct democracy, where the electorate decides on single issues by majority voting. A well-known paradox states that in a sequence of referenda one can get from an arbitrary original income distribution to one in which one player gets almost all the cake. In this paper we design a three-player game modelling the sequential modification mechanism. The strategic analysis reveals that the paradox survives even with rational strategic voters and though the right to propose is allocated to each player once: the last player receives almost the entire cake. The result can be extended to the three-party n-voter case and is for some cases similar when we consider a random rather than fixed sequence of proposers.		Klaus Abbink	2006	Social Choice and Welfare	10.1007/s00355-005-0023-9	direct democracy;majority rule;income distribution;economics;public economics;cardinal voting systems;law;economic system	ECom	-6.393821523095314	-3.733905077703193	139016
0e5721f10e11180f8b56327d2c8f99199bc8c354	optimal decision-making with minimal waste: strategyproof redistribution of vcg payments	location problem;vickrey clarke groves;social choice;group decision making;vcg;mechanism design;individual rationality	Mechanisms for coordinating group decision-making among self-interested agents often employ a trusted center, capable of enforcing the prescribed outcome. Typically such mechanisms, including the ubiquitous Vickrey Clarke Groves (VCG), require significant transfer payments from agents to the center. While this is sought after in some settings, it is often an unwanted cost of implementation. We propose a modification of the VCG framework that---by using domain information regarding agent valuation spaces---is often able to achieve redistribution of much of the required transfer payments back among the agents, thus coming closer to budget-balance. The proposed mechanism is strategyproof, ex post individual rational, no-deficit, and leads to an efficient outcome; we prove that among all mechanisms with these qualities and an anonymity property it is optimally balanced, in that no mechanism ever yields greater payoff to the agents. We provide a general characterization of when strategyproof redistribution is possible, and demonstrate specifically that substantial redistribution can be achieved in allocation problems.	balanced ternary;edmund m. clarke;maximal set;value (ethics)	Ruggiero Cavallo	2006		10.1145/1160633.1160790	mechanism design;social choice theory;group decision-making	AI	-5.335381477659814	-3.7175222875457172	139129
4d9c2f8a451f9a7aaa6d7513393fe3a87cef5c1a	the strategic cores α, β, γ and δ	pure exchange game;strategic cores;γ core;subject classification c72;s pareto nash equilibrium;dominant strategy;δ core;commons game;subject classification c71	In a strategic coalitional game, we consider the relations among four cores α, β, γ, and the one we call δ which is obtained by slightly weakening the conjectural cooperative equilibria due to Currarini and Marini. We show that the α-core and the γ-core are refined by the δ-core; and, moreover that if every player has a dominant strategy, the β-core is refined by the γ-core, so that the four cores refine themselves in the greek alphabetical order. Two economic games will be considered to show that the refinement of the α-core can vary from the weakest to the strongest. While the four cores are equal in the pure exchange game, a radical reduction of the α-core is obtained in the commons game, a simple version of the Cournot game, bringing about a single strategy profile as the δ-core.		Takashi Harada;Mikio Nakayama	2011	IGTR	10.1142/S0219198911002836	bondareva–shapley theorem;example of a game without a value;best response;economics;operations management;strategic dominance;chicken;microeconomics;normal-form game;mathematical economics;symmetric game;nash equilibrium	Logic	-5.586445011878702	-2.457419768300028	139139
b2e38994e0e309b081244d1c7e07f6789e25ae8c	a model for safety case confidence assessment	belief theory;uncertainty;safety case;confidence;quantitative estimation;bayesian belief network	Building a safety case is a common approach to make expert judgement explicit about safety of a system. The issue of confidence in such argumentation is still an open research field. Providing quantitative estimation of confidence is an interesting approach to manage complexity of arguments. This paper explores the main current approaches, and proposes a new model for quantitative confidence estimation based on Belief Theory for its definition, and on Bayesian Belief Networks for its propagation in safety case networks.	bayesian network;hippi;open research;software propagation;tornado	Jérémie Guiochet;Quynh Anh Do Hoang;Mohamed Kaâniche	2015		10.1007/978-3-319-24255-2_23	uncertainty;computer science;artificial intelligence;bayesian network;data mining;management science;confidence;statistics	AI	-16.430817979416005	-0.5861951724714263	139692
48a7baab08221e949c4258f4f6e024702b7390d8	simulating the behaviour of electronic marketplaces with an agent-based approach	analytical models;economic incentive;agent based;economic forecasting;pricing;software agent;simulation;consumer electronics economic forecasting software agents algorithm design and analysis data mining internet analytical models pricing data analysis feedback;consumer electronics;data mining;greedydual;electronic marketplace;software agents;marketing strategy;algorithm;data analysis;global economy;feedback;internet;cache replacement;web caching;algorithm design and analysis	We forecast a future in which the global economy and the Internet will host a large number of interacting software agents. Most of them will be economically motivated, and will negotiate a variety of goods and services. It is therefore important to consider the economic incentives and behaviours of economic software agents, and to use all available means to anticipate their collective interactions. This paper addresses this concern by presenting a multi-agent market simulator designed for analysing agent market strategies based on a complete understanding of buyer and seller behaviours, preference models and pricing algorithms. The results of the negotiations between agents will be analysed by data mining tools in order to extract rules that will give the agents feedback to improve their strategies.	algorithm;data mining;interaction;internet;multi-agent system;software agent	Maria João Viamonte;Carlos Ramos;Fátima Rodrigues;José Carlos Cardoso	2004	IEEE/WIC/ACM International Conference on Web Intelligence (WI'04)	10.1109/WI.2004.127	simulation;computer science;artificial intelligence;software agent;economic forecasting;data mining;world wide web	AI	-8.370324131762022	-9.326805315652955	139795
02a1266f5a83a696d578168d9654b8248687c88b	development of a reduced human user input task allocation method for multiple robots	fuzzy inference systems;multi robot systems;task specification;tk electrical engineering electronics nuclear engineering;ta engineering general civil engineering general;task allocation	Task allocation mechanisms are employed by multi-robot systems to efficiently distribute tasks between different robots. Currently, many task allocation methods rely on detailed expert knowledge to coordinate robots. However, it may not be feasible to dedicate an expert human user to a multi-robot system. Hence, a non-expert user may have to specify tasks to a team of robots in some situations. This paper presents a novel reduced human user input multi-robot task allocation technique that utilises Fuzzy Inference Systems (FISs). A two-stage primary and secondary task allocation process is employed to select a team of robots comprising manager and worker robots. A multi-robot mapping and exploration task is utilised as a model task to evaluate the task allocation process. Experiments show that primary task allocation is able to successfully identify and select manager robots. Similarly, secondary task allocation successfully identifies and selects worker robots. Both task allocation processes are also robust to parameter variation permitting intuitive selection of parameter values.		Praneel Chand;Dale Anthony Carnegie	2012	Robotics and Autonomous Systems	10.1016/j.robot.2012.07.004	real-time computing;simulation;artificial intelligence;function;task analysis	Robotics	-18.43416018635028	-8.419893658437674	139838
29129e3629c78b91447824319a39c1ada968791a	decision making in multi-issue e-market auction using fuzzy techniques and negotiable attitudes	linguistic variables;online auction;bidding strategy;different strategy;multi-issue e-market auction;agents;procuring goods;fuzzy technique;different criterion;attitudes;software agent;fuzzy sets;autonomous bidding strategy;dynamic assessment;salable goods;negotiable attitude;help agent;fuzzy set;software testing	Online auctions are one of the most effective ways of negotiation of salable goods over the internet. Software agents are increasingly being used to represent humans in online auctions. These agents can systematically monitor a wide variety of auctions and can make rapid decisions about what bids to place in what auctions. To be successful in open multi-agent environments, agents must be capable of adapting different strategies and tactics to their prevailing circumstances. This paper presents a software test-bed for studying autonomous bidding strategies in simulated auctions for procuring goods. It shows that agents’ bidding strategy explore the attitudes and behaviors that help agents to manage dynamic assessment of prices of goods given the different criteria and scenario conditions. Our agent also uses fuzzy techniques for the decision making: to make decisions about the outcome of auctions, and to alter the agent’s bidding strategy in response to the different criteria and market conditions.	autonomous robot;fuzzy set;internet;multi-agent system;software agent;software testing;testbed	Madhu Lata Goyal;Jie Lu;Guang Zhang	2008	JTAER		computer science;marketing;operations management;software agent;fuzzy set;commerce	AI	-8.954654830629567	-9.745980095136835	139884
5823eafcb226760504119ab3f34832ff721efeb1	evals is not enough: why we should report wall-clock time	genetic improvement gi;genetic improvement;genetic programming;conference paper;genetic programming gp;genetic improvement programming	Have you ever noticed that your car never achieves the fuel economy claimed by the manufacturer? Does this seem unfair? Unscientic? Would you like the same situation to occur in Genetic Improvement? Comparison will always be difficult [9], however, guidelines have been discussed [3, 5, 4]. With two GP [8] approaches, comparing the number of evaluations of the fitness function is reasonably fair. This means you are comparing the GP systems, and not how well they are implemented, how fast the language is. However, the situation with GI [6, 1] is unique. With GI we will typically compare systems which are applied to the same application written in the same language (i.e. a GI systems targeted at Java, may not even be applied to C). Thus, wall-clock time becomes more relevant. Thus, this paper asks if reporting number of evaluations is enough, or if wall-clock time is also important, particularly in the context of GI. It argues that reporting time is even more important when doing GI when compared to traditional GP	fitness function;genetic improvement (computer science);geographic information system;java	John R. Woodward;Alexander E. I. Brownlee;Colin G. Johnson	2016		10.1145/2908961.2931695	genetic programming;computer science;artificial intelligence;machine learning;operations research;algorithm	PL	-16.197389945594498	-1.659924326458671	139945
0fe29adfab1268a83b11a4e8bff764fc77f11a16	foundations of information theory	kpbc;naukowa;opis bibliograficzny;kujawy;bydgoszcz;starodruki;rekopisy;biblioteka cyfrowa;copernicana;regionalia;ksiązka;pomorze;pomeranica;akademicka;vilniana;torun;dziedzictwo kulturowe;kujawsko pomorska	Information has become the most precious resource of society. At the same time, there is no consensus on the meaning of the term “information,” and many researchers have considered problems of information definition. This results in a quantity of contradictions, misconceptions, and paradoxes related to the world of information. To remedy the situation, a new approach in information theory, which is called the general theory of information, is developed. The main achievement of the general theory of information is explication of a relevant and adequate definition of information. This theory is built on an axiomatic base as a system of two classes of principles and their consequences. The first class consists of the ontological principles, which are revealing general properties and regularities of information and its functioning. Principles from the second class explain how to measure information.	british undergraduate degree classification;first-class function;information theory	Mark Burgin	2008	CoRR		applied mathematics;computer science;artificial intelligence	Theory	-14.895966403623714	1.1361300194608273	140038
bf6f85895a02f17099bcc75e7ddfe1a797d9d2e7	the case for the philosophy of chemistry	mecanique quantique;philosophy of science;explanation;explication;philosophie de l esprit;philosophy of language;reductionism;physique;asymmetry;loi;logic;computationalism;supervenience;philosophy of mind;dependance;asymetrie;periodicite;law;dependency;physics;chimie;social science;quantum mechanics;philosophie des geistes;chemistry;philosophy;epistemology;science philosophy;metaphysics;philosophie des sciences sociales;reductionnisme;computationnalisme;philosophie de la chimie;survenance;philosophie des sciences	The philosophy of chemistry has been sadly neglected by most contempory literature in the philosophy of science. This paper argues that this neglect has been unfortunate and that there is much to be learned from paying greater philosophical attention to the set of issues defined by the philosophy of chemistry. The potential contribution of this field to such current topics as reduction, laws, explanation, and supervenience is explored, as are possible applications of insights gained by such study to the philosophy of mind and the philosophy of social science.	philosophy of mind	Eric R. Scerri;Lee McIntyre	1997	Synthese	10.1023/A:1004949814965	dependency;philosophy of science;philosophy of psychology;reductionism;philosophy of sport;philosophy of mind;western philosophy;philosophy;epistemology;philosophy of language;supervenience;metaphysics;social philosophy;mathematics;economic methodology;logic;philosophy of biology;philosophy of computer science;asymmetry	HCI	-11.10671753932251	3.3926316595324564	140120
4de1ccf298ea03c55e889f8b8b3b3456a5e14476	statistics and truth		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Robert G. Easterling	1999	Technometrics	10.1080/00401706.1999.10485597	genetics;zoology;biology	Robotics	-14.913676211297442	-5.732415800469324	140160
21c0e3cfa46de6297aa787977c9af9ba0dd50cd8	multi-period multi-attribute group decision-making under linguistic assessments	dynamic linguistic weighted geometric operator;investment decision;unbalanced multiplicative linguistic label set;time series;decision maker;aggregation;multi period multi attribute group decision making;90b50;91b06;group decision making;medical diagnosis	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	aggregate data;book;computing with words and perceptions;francis;fuzzy control system;information theory;management science;nl (complexity);operations research;primary source;programmable logic array;real life;spatial variability;three utilities problem;time series	Zeshui Xu	2009	Int. J. General Systems	10.1080/03081070903257920	decision-making;group decision-making;optimal decision;decision analysis;decision engineering;weighted product model;artificial intelligence;time series;medical diagnosis;data mining;mathematics;weighted sum model;statistics;business decision mapping	Robotics	-14.2429833011167	-5.57623561418434	140246
8a4658ec0cc7ba76094b3b64f331d7b1978bf48b	best-response dynamics in a birth-death model of evolution in games	journal_article;learning;best response dynamics;population size;mutation rate;coordination game;birth death processes;birth death process;stochastic stability;coordination games;mutation;decision rule	We consider a model of evolution with mutations as in Kandori et al (1993) [Kandori, M., Mailath, G.J., Rob, R., 1993. Learning, mutation, and long run equilibria in games. Econometrica 61, 29-56], where agents follow best-response decision rules as in Sandholm (1998) [Sandholm, W., 1998. Simple and clever decision rules for a model of evolution. Economics Letters 61, 165-170]. Contrary to those papers, our model gives rise to a birth-death process, which allows explicit computation of the long-run probabilities of equilibria for given values of the mutation rate and the population size. We use this fact to provide a direct proof of the stochastic stability of riskdominant equilibria as the mutation rate tends to zero, and illustrate the outcomes of the dynamics for positive mutation rates.	computation;models of dna evolution;mutation (genetic algorithm)	Carlos Alós-Ferrer;Ilja Neustadt	2010	IGTR	10.1142/S021919891000260X	coordination game;economics;mathematical economics	ML	-7.066777555417346	-5.313831966405457	140486
edb4f2b7751aa8b4d479d8333c63101305bc8b9b	plausibility of diagnostic hypotheses: the nature of simplicity		In general diagnostic problems multiple disorders can occur simultaneously. AI systems have traditionally handled the potential combinatorial explosion of possible hypotheses in such problems by focusing attention on a few “most plausible” ones. This raises the issue of establishing what makes one hypothesis more plausible than others. Typically a hypothesis (a set of disorders) must not only account for the given manifestations, but it must also satisfy some notion of simplicity (or coherency, or parsimony, etc) to be considered. While various criteria for simplicity have been proposed in the past, these have been based on intuitive and subjective grounds. In this paper, we address the issue of if and when several previously-proposed criteria of parsimony are reasonable in the sense that they are guaranteed to at least identify the most probable hypothesis. Hypothesis likelihood is calculated using a recent extension of Bayesian classification theory for multimembership classification in causal diagnostic domains. The significance of this result is that it is now possible to decide objectively a priori the appropriateness of different criteria for simplicity in developing an inference method for certain classes of general diagnostic problems. 1. Diagnostic Problem-Solving During the last decade, a number of artificial intelligent (AI) systems have been developed that use an “abductive” * approach to diagnostic problem-solving [Pople73, 821 [Pauker76] [ReggiaSl, 831 [Miller821 [Josephson841 [Basili85]. These systems use an associative knowledge base where causal associations between disorders and manifestations are the central component, and inferences are made through a sequential hypothesize-and-test process. An important but as yet unresolved issue in abductive systems for diagnostic problem-solving is what characteristics make a set of disorders a plausible, “best”, or “simplest” explanatory hypothesis for observed manifestations. This issue has long been an important one in philosophy [Peirce55] [Thagard78] [Joseph son821 as well as in AI [Rubin75] [Pople73] [Pauker76] [Reggia83] [Josephson84], and is not only of relevance to diagnostic problem-solving but also to many other areas in AI (natural language processing, machine learning, etc. [Charniak85] [Reggia85a]). In particular, to * Abductive inference is ing to the best explanation.” s enerally defined to be “reasonor a given set of facts, and is distinguished from deductive and inductive inference (see [Reggia85a\). Peirce55 Thagard781 [Pople73] [Josephson82] [Charniak85] the authors’ knowledge, all previous suggestions of hypothesis plausibility have generally been proposed primarily on intuitive rather than formal grounds. Over the last few years we have been studying a formal model of diagnostic problem-solving referred to as parsimonious covering theory [Reggia83,85b] [Peng86a]. Recently, we have successfully integrated into this causal reasoning model the ability to calculate the relative likelihood of any evolving or complete diagnostic hypothesis [Peng86b]. As a result an objective measure (relative likelihood) can now be used to examine several previous subjective criteria of hypothesis plausibility. The rest of this paper examines this issue, and is organized as follows. First, the parsimonious covering model of problemsolving, which is based on an underlying causal relationship and the use of probability theory in this context, are briefly summarized in Sections 2 and 3. Section 4 then examines several different criteria for hypothesis plausibility used in AI systems with respect to whether they lead to the most probuble diagnostic hypothesis. Situations where the use of each criterion is/is not appropriate are identified. Section 5 concludes by summarizing the implications of these results for AI system development. 2. Parsimonious Covering Theory Causal associations between disorders and manifestations are the central element of diagnostic knowledge bases in many real-world systems, and parsimonious covering theory is based on a formalization of causal associative knowledge [Peng86a] [Reggia85b]. The simplest type of diagnostic problems in this model, and the one we use in this paper, is defined to be a 4-tuple P = <D,M,C,M+> where D = {di, . . . , d,} is a finite non-empty set of disorders; M = {ml, . . . , mk} is a finite non-empty set of manifestations (symptoms); C C D x M is a relation with domain(C) = D and range(C) = M; and M+C M is a distinguished subset of M. The relation C captures the intuitive notion of causal associations in a symbolic form, where <d; ,mj > E C iff “disorder di may cause manifestation mj “. Note that <di ,mi > E C does not imply that mj always occurs when di is present, but only that mj may occur. D, M, and C together correspond to the knowledge base in an abductive expert system. M+, a special subset of M, represents the features (manifestations) which are present 140 / SCIENCE From: AAAI-86 Proceedings. Copyright ©1986, AAAI (www.aaai.org). All rights reserved. in a specific problem. Fig. 1 graphically illustrates the symbolic causal knowledge of a tiny abstract diagnostic problem of this type. dl d2 d3 d4 ci I ml m2 m3 m4 Fig. 1. An example of a very simple abstraction of a diagnostic problem. Two functions, “causes” and “effects”, can be defined in the above framework: for all mjE M, causes(mj ) = {di 1 <d i ,mj > E C}, representing all possible causes of manifestation mj ; for all di E D, effects(di ) = {mj 1 <di ,rni > E C}, representing all manifestations which may be caused by di. A set of disorders D1 E D is then said to be a cover of a set of manifestations MJ 5 M if MJ & effects(D1), where by definition effects(D, ) = U effects(di ). Also, we define causes(MJ) = 4~ Dr	abductive reasoning;artificial intelligence;causal filter;causality;covering space;expert system;inductive reasoning;knowledge base;machine learning;mathematical model;maximum parsimony (phylogenetics);natural language processing;norm (social);occam's razor;plausibility structure;problem solving;relevance;stellar classification;world-system	Yun Peng;James A. Reggia	1986			artificial intelligence;statistics	AI	-14.842408817624904	2.0793936737995877	140488
1eeb4be135e6c25e1819e09f066e2ef079c812f8	the interpretation and application of belief functions	theory of evidence;sistema experto;belief function;inference incertaine;intelligence artificielle;raisonnement;artificial intelligent;approximate reasoning;uncertain inference;dempster shafer theory;probability theory;razonamiento;dempster shafer;theorie probabilite;artificial intelligence;teoria probabilidad;inteligencia artificial;systeme expert;reasoning;bayesian analysis;theorie dempster shafer;expert system	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Richard E. Neapolitan	1993	Applied Artificial Intelligence	10.1080/08839519308949983	dempster–shafer theory;computer science;artificial intelligence;machine learning;expert system	Robotics	-14.785799768210401	-5.3123225596892505	140827
310cd0041a6bb5f8967e548bdc6002b4ecdaf0ca	negotiating flexible agreements by combining distributive and integrative negotiation	batna;social science literature;automated negotiation;computational framework;sensitive information;social science;multi-criteria decision making;integrative negotiation;reserve price;co-operative value;software agent;disadvantaging oneself;flexible agreement;integrative and distributive bargaining;management literature deal;value added;gains from trade	This paper presents an approach to automated negotiation between agents which attempts to combine the advantages of a co-operative value adding approach, with the reality that negotiating agents are also competing. We use the concept of a trusted mediator to facilitate openness regarding what one values, without disadvantaging oneself by revealing sensitive information (such as a reserve price) to the other party. Social science and management literature deals with negotiation between people, and so can be both more complex, and less well defined than automated negotiation between software agents. We take inspiration from the social science literature and develop a computational framework to support negotiating software agents. The framework includes recognition that agents are self interested, and therefore will manipulate the system to their advantage if possible. We include mechanisms to discourage this kind of manipulation in the form of a transaction cost associated with making only small concessions, and a bias in dividing the pie which is the gain from trade which favours the agent who is most “honest”	information sensitivity;openness;software agent	Quoc Bao Vo;Lin Padgham;Lawrence Cavedon	2007	Intelligent Decision Technologies		knowledge management;value added;artificial intelligence;software agent;management science;best alternative to a negotiated agreement;gains from trade	AI	-9.00703650075122	-8.298290335730108	140904
3719bc5ab38709d171e27edd573c65962f2b1770	segmenting reactions to improve the behavior of a planning/reacting agent	numerical range	An agent operating in a real-world environment will inevitably encounter some events that dcmand very quick responses----the deadline for an action is short, and the consequences of not acting are high. For these types of events, the agent has a better chance of acting appropriately if it has a pre-stored set of reactions that can be quickly retrieved based on features of the situation. In this paper, we examine the problem of selecting the best set of reactions for an agent to store. In paxticulaJr, we examine the benefit of including intervals of reactions--.i.e., executing some reactions only across segments of the complete numeric ranges over which they are defincxt. We present a decision-theoretic algorithm for selecting the optimal set of reaction intervals, a~d we present experintents with a computer implementation of that algorithm, called KNEE JERK. Tlw experiments show that the benefit of breaking down reactions into intervals is quite high under a wide range of circumstances.	algorithm;decision theory;experiment;kneeling chair;norm (social)	Michael Wolverton;Richard Washington	1996			market segmentation;data mining;real-time computing;numerical range;computer science	AI	-11.624737405610128	-4.680133072944787	141059
746a2d99655eb16ad758b696c29d7227f423e629	fair distribution of efficiency gains in supply networks from a cooperative game theory point of view	fairness;game theory;t value;cooperative game theory;supply networks;planning models;point of view;efficiency gains	"""In this paper, the authors address the distribution of efficiency gains among partially autonomous supply network actors in a manner they will accept as fair and as an incentive to cooperation. The problem is economically significant because it requires substantiating efficiency gains in an understandable manner. Moreover, supply networks suffer from a conflict potential because the partially autonomous actors seek to maximize their own shares of the efficiency gain. The method applied appropriates a model from cooperative game theory involving the τ-value. The special nature of the τ-value ensures that it seems rational to the actors to cooperate in the supply network. The proposed method for the distribution problem offers a fair distribution of efficiency gains in the supply network and ensures that the distribution results can be communicated easily. DOI: 10.4018/jisscm.2010040101 2 International Journal of Information Systems and Supply Chain Management, 3(2), 1-24, April-June 2010 Copyright © 2010, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. al., 2005; McCullen & Towill, 2002; Metters, 1997). The bullwhip effect describes especially how companies build inventory buffers based on the demand of their customers: the further the company is from the final customer the greater the “safety stock” is in times of rising demand. The cost of capital invested in oversized inventory buffers in the stocks causes inefficiency and thus efficiency gains can be realized by avoiding or reducing the bullwhip effect. Evidence of the practical relevance of the bullwhip effect to supply chain management is provided by studies of its financial consequences (McCullen & Towill, 2002; Metters, 1997). Based on available estimates of the cost of the bullwhip effect, companies should be able to increase their profits— depending on the source—by 8.4 to 20.1% (McCullen & Towill, 2002) or by 10 to 30% (Metters, 1997) by avoiding it. When efficiency gains are realized in supply networks, a distribution problem arises. The cooperating actors know that they are realizing the efficiency gains by mutually coordinating their activities. Moreover, each actor is interested in maximizing his own gain at the expense of the other actors in the supply network. Thus, supply networks suffer from a built-in conflict between cooperation and defection. The problem lies in distributing efficiency gains among partially autonomous actors in a manner that the actors will accept as fair and advantageous to cooperation. If, on the other hand, it would be advantageous for at least one of the actors to leave the grand coalition, the supply network would collapse. With this scenario in mind, a stability requirement can be posited for the solutions of efficiency gain distribution problems. These problem solutions are regarded as desirable only, if they ensure that all actors in a supply network are willing to cooperate with each other. The fulfillment of this stability requirement is often circumscribed by the actors’ acceptance of the distribution of the efficiency gains as fair. In economic literature, significant research efforts have been devoted to developing concepts of fairness (e.g., Fehr & Schmidt, 1999; Pazner, 1977; Varian, 1976). Unfortunately, to the knowledge of the authors, these concepts have not yet been adapted to solve the distribution problem outlined above. Furthermore, cooperative game theory offers a number of solution concepts for distribution problems worthy of closer consideration. These are especially the Shapley value (Shapley, 1953; see Derks & Tijs, 2000) and the Nucleolus (Schmeidler, 1969; see also Meertens & Potters, 2006). Yet these solution concepts suffer from a serious drawback. The fairness of the distribution results is hard to justify, since the ‘logic’ of these approaches is difficult to communicate. The central issue of this paper is to present a solution concept from cooperative game theory for the problem of distributing efficiency gains in supply networks that aims at distributing efficiency gains in a fair manner. From a management point of view, this concern is motivated by the fact that the (game) theoretic solution concepts have to withstand acceptance problems in management practice, since they are not intuitively comprehensible. This is why there is a considerable management demand for solutions of the efficiency gain distribution problem that are both theoretically sound and involve an easily, if not intuitively comprehensible operationalization of fairness. The issue of an intuitively and easily comprehensible fairness of efficiency gain distributions is tackled by means of cooperative game theory. In contrast to the literature, no “completed” solution concepts are taken as starting points and analyzed with respect to their theoretical characteristics. Furthermore, no abstract mathematical axioms are taken as bases for showing then that a certain game theoretic concept is implicated. Instead, a novel and primarily economic, argumentation is presented. This argumentation seeks to derive a known solution concept – the so-called τ-value – from economically plausible assumptions by means of which the space of possible solution concepts is restricted until the τ-value results as the solution concept for the fair distribution of efficiency gains. The main results of this paper concern three aspects which serve to advance the findings in the pertinent literature. Firstly, the τ-value solution concept is derived in a novel, non mathemati22 more pages are available in the full version of this document, which may be purchased using the """"Add to Cart"""" button on the product's webpage: www.igi-global.com/article/fair-distribution-efficiency-gainssupply/42117?camid=4v1 This title is available in InfoSci-Journals, InfoSci-Journal Disciplines Business, Administration, and Management. Recommend this product to your librarian: www.igi-global.com/e-resources/libraryrecommendation/?id=2"""	autonomous robot;built-in self-test;fairness measure;game theory;graph isomorphism problem;information systems journal;information science;librarian;mind;point of view (computer hardware company);relevance;safety stock;schmidt decomposition;stable marriage problem;web page	Stephan Zelewski;Malte L. Peters	2010	IJISSCM	10.4018/jisscm.2010040101	non-cooperative game;game theory;economics;t-statistic;microeconomics;mathematical economics;welfare economics	AI	-6.238042600755481	-3.6293563896274526	141070
08dc7c5e278a177802e11e118485de4ed1275753	using causal models in heterogeneous information fusion to detect terrorists		We describe basic research that uses a causal, uncertainty-sensitive computational model rooted in qualitative social science to fuse disparate pieces of threat information. It is a cognitive model going beyond rational-actor methods. Having such a model has proven useful when information is uncertain, fragmentary, indirect, soft, conflicting, and even deceptive. Inferences from fusion must then account for uncertainties about the model, the credibility of information, and the fusion methods - i.e. we must consider both structural and parametric uncertainties, including uncertainties about the uncertainties. We use a novel combination of (1) probabilistic and parametric methods, (2) alternative models and model structures, and (3) alternative fusion methods that include nonlinear algebraic combination, variants of Bayesian inference, and a new entropy-maximizing approach. Initial results are encouraging and suggest that such an analytically flexible and model-based approach to fusion can simultaneously enrich thinking, enhance threat detection, and reduce harmful false alarms.	bayesian approaches to brain function;causal filter;cognitive model;computation;computational model;experiment;interaction;linear algebra;nonlinear system;oracle fusion middleware;prototype;tails;threat (computer)	Paul K. Davis;David Manheim;Walter L. Perry;John Hollywood	2015	2015 Winter Simulation Conference (WSC)		intelligence analysis;uncertainty;computer science;artificial intelligence;machine learning;mathematical model;data mining;mathematics;probabilistic logic;terrorism;computational model;statistics	AI	-17.667451319369363	-3.95198617727332	141154
65a4d6e24342d896486ab7a3bcaeaa0b3cc54c47	player splitting in extensive form games	maastricht university;player splitting;equilibrium refinements;extensive form games;workingpaper;nash equilibria;satisfiability;digital archive;invariance;open access;stable set;solution concept;publication;scientific;institutional repository	-------------------------------------------------------------By a player splitting we mean a mechanism that distributes the information sets of a player among so-called agents. A player splitting is called independent if each path in the game tree contains at most one agent of every player. Following Mertens (1989), a solution is said to have the player splitting property if, roughly speaking, the solution of an extensive form game does not change by applying independent player splittings. We show that Nash equilibria, perfect equilibria, Kohlberg-Mertens stable sets and Mertens stable sets have the player splitting property. An example is given to show that the proper equilibrium concepts does not satisfy the player splitting property. Next, we give a definition of invariance under (general) player splittings which is an extension of the player splitting property to the situation where we also allow for dependent player splittings. We come to the conclusion that none of the solutions above are invariant under any dependent player splitting. The results are used to give several characterizations of the class of independent player splittings and the class of single appearance structures by means of invariance of solution concepts under player splittings.	intelligent agent;nash equilibrium	Andrés Perea;Mathijs Jansen;Dries Vermeulen	2000	Int. J. Game Theory	10.1007/s001820000051	bayesian game;economics;extensive-form game;strategic dominance;invariant;publication;mathematics;strategy;correlated equilibrium;microeconomics;mathematical economics;solution concept;nash equilibrium;satisfiability	Theory	-5.914601391589385	-1.8749829917535588	141313
58f5298ae96e6300aeb2da8a4c04c1ed69ffd812	the nash bargaining solution in general n-person cooperative games	subgame perfect equilibrium;nash bargaining solution noncooperative bargaining coalition externality core n person cooperative games;cooperative game;nash bargaining solution	We present a noncooperative foundation for the Nash bargaining solution for an n-person cooperative game in strategic form. The Nash bargaining solution should be immune to any coalitional deviations. Our noncooperative approach yields a new core concept, called the Nash core, for a cooperative game based on a consistency principle. We prove that the Nash bargaining solution can be supported (in every subgame) by a stationary subgame perfect equilibrium of the bargaining game if and only if the Nash bargaining solution belongs to the Nash core. JEL classification: C71, C72, C78	fixed-point theorem;nash equilibrium;rays linux;stationary process	Akira Okada	2010	J. Economic Theory	10.1016/j.jet.2010.07.001	price of stability;bargaining problem;game theory;epsilon-equilibrium;best response;trembling hand perfect equilibrium;economics;subgame;non-credible threat;folk theorem;repeated game;correlated equilibrium;microeconomics;risk dominance;normal-form game;mathematical economics;stackelberg competition;subgame perfect equilibrium;welfare economics;solution concept;nash equilibrium;centipede game	ECom	-5.628624483517308	-1.7267837417794625	141419
08bc1f9575c4fd10832f4bee7e19a3e98815c205	a new understanding of prediction markets via no-regret learning	learning algorithm;cost function;market scoring rules;scoring rule;online learning;conference paper;prediction with expert advice;prediction market;follow the regularized leader;theory;prediction markets;learning problems;algorithms;economics	We explore the striking mathematical connections that exist between market scoring rules, cost function based prediction markets, and no-regret learning. We first show that any cost function based prediction market can be interpreted as an algorithm for the commonly studied problem of learning from expert advice by equating the set of outcomes on which bets are placed in the market with the set of experts in the learning setting, and equating trades made in the market with losses observed by the learning algorithm. If the loss of the market organizer is bounded, this bound can be used to derive an O(√T) regret bound for the corresponding learning algorithm. We then show that the class of markets with convex cost functions exactly corresponds to the class of Follow the Regularized Leader learning algorithms, with the choice of a cost function in the market corresponding to the choice of a regularizer in the learning problem. Finally, we show an equivalence between market scoring rules and prediction markets with convex cost functions. This implies both that any market scoring rule can be implemented as a cost function based market maker, and that market scoring rules can be interpreted naturally as Follow the Regularized Leader algorithms. These connections provide new insight into how it is that commonly studied markets, such as the Logarithmic Market Scoring Rule, can aggregate opinions into accurate estimates of the likelihood of future events.	aggregate data;algorithm;image organizer;loss function;machine learning;regret (decision theory);turing completeness	Yiling Chen;Jennifer Wortman Vaughan	2010		10.1145/1807342.1807372	scoring rule;mathematical optimization;actuarial science;economics;artificial intelligence;machine learning;data mining;microeconomics;mathematical economics;theory	ECom	-7.012732549476819	-5.255641627258335	141428
3228e38b2373b0e794105ed8af1603d2c07e2093	the psychology and rationality of decisions from experience	choice;risk;rationality;decisions from experience;rarity	Most investigations into how people make risky choices have employed a simple drosophila: monetary gambles involving stated outcomes and probabilities. People are asked to make decisions from description. When people decide whether to back up their computer hard drive, cross a busy street, or go out on a date, however, they do not enjoy the convenience of stated outcomes and probabilities. People make such decisions either in the void of ignorance or in the twilight of their own often limited experience of such real-world options. In the latter case, they make decisions from experience. Recent research has consistently documented that decisions from description and decisions from experience can lead to substantially different choices. Key in this description–experience gap is people’s treatment of rare events. In this paper, I briefly review studies that have documented the description–experience gap, offer several explanations for this gap, and discuss to what extent people’s decisions from experience are in conflict with benchmarks of rationality.	backup;catastrophic interference;extreme value theory;hard disk drive;rare events;rationality;the void (virtual reality)	Ralph Hertwig	2011	Synthese	10.1007/s11229-011-0024-4	actuarial science;management science	HCI	-9.334145283879524	-4.5245210826051006	141631
5ba47890fcdfab8f6124c50d2e9a955f73710252	open columns: a carbon dioxide (co2) responsive architecture	responsive environments;carbon dioxide;design criteria;interactive architecture;co2 emission;responsive environment	This paper describes the use of composite urethane elastomers for constructing responsive structures at an architectural scale. It explains the underlying material research and design criteria for constructing deployable columns that are responsive to carbon dioxide (CO2) emissions and are used to reconfigure and pattern the space of inhabitation.	column (database);responsive architecture	Omar Khan	2010		10.1145/1753846.1754232	carbon dioxide	HCI	-13.130817235218323	-3.382730505599508	141636
dbc08d290190db70efff80989adac824198712f5	the shapley value as the maximizer of expected nash welfare	nash bargaining solution;shapley value;maximizing expected nash welfare	We provide an alternative interpretation of the Shapley value in TU games as the unique maximizer of expected Nash welfare. Copyright Springer-Verlag Berlin Heidelberg 2014	decibel;nash equilibrium;stable marriage problem	Anirban Kar;Arunava Sen	2014	Int. J. Game Theory	10.1007/s00182-013-0398-2	bargaining problem;epsilon-equilibrium;best response;economics;microeconomics;shapley value;mathematical economics;welfare economics	ECom	-5.386417470021583	-1.9919296108580977	142098
312072e6f6ee52cb326d2a4bdd2836035a172707	centralized super-efficiency and yardstick competition - incentives in decentralized organizations			centralized computing	Armin Varmaz;Andreas Varwig;Thorsten Poddig	2010		10.1007/978-3-642-20009-0_9		Crypto	-8.5210222495018	-5.691172947550519	142278
e1b38ef847ceb66131a41280d99307a602fd58f0	prospect theory: much ado about nothing?	stochastic dominance;prospect theory;expected utility;utility function;markowitz stochastic dominance;certainty equivalence;financial market;value function;prospect stochastic dominance	Prospect theory is a paradigm challenging the expected utility paradigm. One of the fundamental components of prospect theory is the S-shaped value function. The value function is mainly justified by experimental investigation of the certainty equivalents of prospects confined either to the negative or to the positive domain, but not of mixed prospects, which characterize most actual investments. We conduct an experimental study withmixed prospects, using, for the first time, recently developed investment criteria called Prospect Stochastic Dominance (PSD) and Markowitz Stochastic Dominance (MSD). We reject the S-shaped value function, showing thatat least 62%--76% of the subjects cannot be characterized by such preferences. We find support for the Markowitz utility function, which is a reversed S-shaped function--exactly the opposite of the prospect theory value function. It is possible that the previous results supporting the S-shaped value function are distorted because the prospects had only positive or only negative outcomes, presenting hypothetical situations which individuals do not usually face, and which are certainly not common in financial markets.	ado.net	Moshe Levy;Haim Levy	2002	Management Science	10.1287/mnsc.48.10.1334.276	financial economics;prospect theory;economics;expected utility hypothesis;stochastic dominance;certainty effect;finance;bellman equation;mathematical economics;welfare economics;financial market	Theory	-5.252100356631	-6.723446348356898	142985
889d2ad36364b6b7e0ea8064586255d2e7bdf003	experiments in distributed game-tree searching				Jonathan Schaeffer	1987	ICGA Journal	10.3233/ICG-1987-10107	game tree;artificial intelligence;machine learning;computer science	Crypto	-13.378932211671291	-8.086154223309604	142998
9fa0fef10942202c0d92e898af16be81cdd21c3e	truthful mechanism for crowdsourcing task assignment		As an emerging human-solving paradigm, crowdsourcing has attracted much attention where requesters want to employ reliable workers to complete the specific task. Task assignment is a vital branch in crowdsourcing. Most existing works in crowdsourcing haven't taken self-interested individuals' strategy into account. To guarantee truthfulness, auction has been regarded as a promising form to charge requesters and reward workers. In this paper, we consider an online task assignment scenario, where each worker has a set of experienced skills, whereas specific task is budget-constrained and requires certain skill. Under this scenario, we model the crowdsourcing task assignment as a reverse auction in which requesters are buyers and workers are sellers. Specially, our paper studies simple task case where the requester ask for single skill. We propose TMC-VCG and TMC-ST and prove the related properties for the mechanisms theoretically. Meanwhile, through extensive simulations, we verify the truthfulness and also evaluate other performance.	crowdsourcing;programming paradigm;simulation;test-and-set;time complexity;traffic message channel	Haiyan Qin;Yonglong Zhang;Bin Li	2017	2017 IEEE 10th International Conference on Cloud Computing (CLOUD)	10.1109/CLOUD.2017.72	mathematical optimization;theoretical computer science;computer science;crowdsourcing	DB	-9.98561191740807	-8.349330249859912	143553
b6cd1b742f4984dbab56348f05cb690a44c882b5	the deontological conception of epistemic justification: a reassessment	epistemology;epistemic justification;the deontological conception of epistemic justification;epistemic deontologism;william alston	This paper undertakes two projects: Firstly, it offers a new account of the so-called deontological conception of epistemic justification (DCEJ). Secondly, it brings out the basic weaknesses of DCEJ, thus accounted for. It concludes that strong reasons speak against its acceptance. The new account takes it departure from William Alston’s influential work. Section 1 argues that a fair account of DCEJ is only achieved by modifying Alston’s account and brings out the crucial difference between DCEJ and the less radical position of epistemic deontologism. Section 2 starts by setting up two fundamental problems for proponents of DCEJ to solve. It argues further that proponents of DCEJ may not convincingly solve those problems by appeal to a notion of permissible belief. Section 3 investigates, whether an appeal to the notion of blameless belief may help DCEJ overcome its central problems. It argues that, even if an appeal to the notion of blameless belief has advantages over an appeal to the notion of permissible belief, DCEJ cannot convincingly overcome the problems set up for it. Further, it is brought out that DCEJ commits its proponents to a problematic non-standard view regarding the intrinsic value of epistemic justification. Section 4 concludes that DCEJ is not the natural conception of epistemic justification, that Alston takes it to be. However, its problems do not leave a scratch on epistemic deontologism, properly conceived.	intension;jargon;modal logic;non-repudiation;syntactic predicate	Nikolaj Nottelmann	2011	Synthese	10.1007/s11229-011-9967-8	philosophy;epistemology;mathematics	AI	-13.190728214776236	3.5389234389006186	143601
d44aff29fbb2ab57775c1319b07a03821c9adf33	efficient and stable collective choices under gregarious preferences	subgame perfect equilibrium;user agent;documento de trabajo;bens publics;public goods gregarious preferences subgame perfect implementation;satisfiability;gregarious preferences;collective choice;public goods;subgame perfect implementation;public good	We consider collective choice problems where a set of agents have to choose an alternative from a finite set and agents may or may not become users of the chosen alternative. An allocation is a pair given by the chosen alternative and the set of its users. Agents have gregarious preferences over allocations: given an allocation, they prefer that the set of users becomes larger. We require that the final allocation be efficient and stable (no agent can be forced to be a user and no agent who wants to be a user can be excluded). We propose a two-stage sequential mechanism whose unique subgame perfect equilibrium outcome is an efficient and stable allocation which also satisfies a maximal participation property. © 2008 Elsevier Inc. All rights reserved. JEL classification: D62; D71; H41	fax;humberto maturana;sandro corsaro	Jordi Massó;Antonio Nicolò	2008	Games and Economic Behavior	10.1016/j.geb.2007.12.007	public good;economics;microeconomics;mathematical economics;subgame perfect equilibrium;welfare economics	AI	-5.835852932777064	-3.650981324556861	143694
57de2df8986a8597b4604c518b9ea5cf082eb5c4	capacitive touch communication: a technique to input data through devices' touch screen	software;touch sensitive screens;radio receivers;biometrics access control;authentication;smart phones;mobile communications authentication receivers hardware software capacitance smart phones capacitive screen capacitive touch communication touchscreen authentication identification;capacitive screen;radio transmitters;receivers;touchscreen;authentication receivers hardware software capacitance smart phones;touch sensitive screens biometrics access control message authentication mobile communication radio receivers radio transmitters smart phones;identification;capacitive touch communication smartphones swipe patterns pin codes signal generator parental control applications multiplayer tablet games data rate firmware modification hardware modification tablet receiver prototype transmitter signet ring low power continuous transmitter human body signal receiver capacitive touchscreens user authentication user identification post pc devices;mobile communication;capacitive touch communication;capacitance;message authentication;mobile communications;hardware	As we are surrounded by an ever-larger variety of post-PC devices, the traditional methods for identifying and authenticating users have become cumbersome and time consuming. In this paper, we present a capacitive communication method through which a device can recognize who is interacting with it. This method exploits the capacitive touchscreens, which are now used in laptops, phones, and tablets, as a signal receiver. The signal that identifies the user can be generated by a small transmitter embedded into a ring, watch, or other artifact carried on the human body. We explore two example system designs with a low-power continuous transmitter that communicates through the skin and a signet ring that needs to be touched to the screen. Experiments with our prototype transmitter and tablet receiver show that capacitive communication through a touchscreen is possible, even without hardware or firmware modifications on a receiver. This latter approach imposes severe limits on the data rate, but the rate is sufficient for differentiating users in multiplayer tablet games or parental control applications. Controlled experiments with a signal generator also indicate that future designs may be able to achieve data rates that are useful for providing less obtrusive authentication with similar assurance as PIN codes or swipe patterns commonly used on smartphones today.	authentication;capacitive sensing;code;data rate units;embedded system;experiment;firmware;interaction;laptop;low-power broadcasting;mobile device;multi-user;parental controls;post-pc era;prototype;seal (emblem);smartphone;tablet computer;touchscreen;transmitter;wearable computer	Tam Vu;Akash Baid;Simon Gao;Marco Gruteser;Richard E. Howard;Janne Lindqvist;Predrag Spasojevic;Jeffrey Walling	2014	IEEE Transactions on Mobile Computing	10.1109/TMC.2013.116	identification;embedded system;mobile telephony;computer hardware;telecommunications;computer science;authentication;capacitance	Mobile	-15.500808356548552	-2.5377970129895586	143715
db3d9265c817ed9df38f6571e2daa02177de2c91	on fuzzy projection-based utility decomposition in compound multi-agent negotiations	fuzzy set;service utilization;utility function;swinburne;satisfiability	In the process of compound multi-agent negotiation a number of agents concurrently negotiate with one or more counterparts in order to satisfy the individual preferences that lead to the collective maximization of the overall utility function imposed on the compound service. In order to perform this task the overall utility function has to be decomposed into individual single-service utility functions. This problem is not trivial, especially in compound multi-agent negotiations involving more complex aggregation patters of negotiated issues. In this paper we propose an approach for derivation of the individual utility functions based of the principles of fuzzy set projection. We also propose a way of modifying the initially generated utility functions in the case where the agreement was not reached with those functions, what allows for reaching an agreement in repeated negotiation.		Jakub Brzostowski;Ryszard Kowalczyk	2007		10.1007/978-3-540-72950-1_74	mathematical optimization;computer science;artificial intelligence;mathematics;fuzzy set;satisfiability	AI	-9.553427117649623	-7.005937020596444	143716
dd3a38999614119729a66a888ad1be357a32ba46	simulation of customers behaviour as a method for explaining certain market characteristic	simulation;price dispersion;customer buying behaviour	Price dispersion is an observed variation of price of the same (or similar) product among different sellers. This paper provides a possible explanation of the observed shape of the dispersion, proven by simulations of an agent based customer-seller environment. Proposed models for both seller and customer, based on current marketing knowledge are included. It turns out that the observed shape is achieved when some key factors influencing customers' buying behaviour are taken into account; e.g. the social context, communication, a limited memory of a customer and the cost of crossing the distance between agents. As a benefit of the proposed model, it allows for speculation on how the future commerce may look like - in an Internet world where distances matter little.	simulation	Marek Zachara;Ewa Majchrzyk-Zachara;Cezary Piskor-Ignatowicz	2013	Trans. Computational Collective Intelligence	10.1007/978-3-642-36815-8_10	marketing;advertising;business;commerce	ECom	-4.648035637612827	-9.446496525040887	143892
217fca2e480202e094fa6617f9be14d99cdffefb	is economic rationality in the head?	behavioral economics;extended mind;market rationality;economic rationality;subjective transaction costs;economic agency;heuristics and biases	Many economic theorists hold that social institutions can lead otherwise irrational agents to approximate the predictions of traditional rational choice theory. But there is little consensus on how institutions do so. I defend an economic internalist account of the institution-actor relationship by explaining economic rationality as a feature of individuals whose decision-making is aided by institutional structures. This approach, known as the subjective transaction costs theory, represents apparently irrational behavior as a rational response to high subjective transaction costs of thinking and deciding. The theory has two attractive features. First, it reconciles rational choice theory with the increasing body of evidence cataloguing putative errors in economic decision-making. Second, it vindicates the explanatory power of individual choice against externalist challenges; the subjective transaction costs theory keeps economic rationality in the head.	approximation algorithm;rationality;stereotype (uml);theory	Kevin Vallier	2015	Minds and Machines	10.1007/s11023-015-9386-6	ecological rationality;rationality;social psychology;behavioral economics;rational choice theory;bounded rationality	ECom	-10.694907517076437	-3.180327969159922	143942
59a915c1b4439b8ec767eb3abc3bd01a80dcbb23	on the preservation of context-free languages in a level-based system	recent discovery;level-based system;greater naturalness;context-free language;theoretical implication;natural language;machine analysis;level-oriented model;revised version;context free language	In this paper, a recently proposed level-oriented model for machine analysis and synthesis of natural languages is investigated° Claims concerning the preservation of context-free (CF) languages in such a system are examined and shown to be unjustified. Furthermorep it is ~hown that even a revised version of the mode1 (incorporating some recent discoveries) will not be CF-preservlngo Finallyp some theoretical implications of these findings are explored: in partlcular~ claims of greater naturalness and the question of recurslvltyo ® Over the yearsp and especlally during the pr~waratlon of this paper t I have had the pleasure of many enlightening discussions with Stani%y Peters, for which I am glad to thank hlm.	context-free language;natural language	Jacob Mey	1969			natural language processing;computer science;context-free language;linguistics;natural language;algorithm	NLP	-13.87889794125233	-0.508118559512289	144021
168e2abf42a29c5702eaa91b6b47ccd0cd2b6f48	perfect information stochastic games and related classes	perfect information stochastic game;related class;game theory;publication	"""For n-person perfect information stochastic games and for n-person stochastic games with Additive Rewards and Additive Transitions (ARAT) we show the existence of pure limiting average equilibria. Using a similar approach we also derive the existence of limiting average """"-equilibria for two-person switching control stochastic games. The order eld property holds for each of the classes mentioned, and algorithms to compute equilibria are pointed out."""	additive model;algorithm	Frank Thuijsman;Thirukkannamangai E. S. Raghavan	1997	Int. J. Game Theory	10.1007/BF01263280	game theory;economics;theoretical computer science;publication;mathematics;microeconomics;mathematical economics	AI	-5.402002259575521	-1.3588619572317369	144031
5f6c1b76809269ad2a796b51909a73db2009020d	criteria for indefeasible knowledge: john mcdowell and ‘epistemological disjunctivism’	humanidades;filosofia etica;h social sciences general	Duncan Pritchard has recently defended a view he calls ‘epistemological disjunctivism’, largely inspired by John McDowell. I argue that Pritchard is right to associate the view with McDowell, and that McDowell’s ‘inference-blocking’ argument against the sceptic succeeds only if epistemological disjunctivism is accepted. However, Pritchard also recognises that epistemological disjunctivism appears to conflict with our belief that genuine and illusory experiences are indistinguishable (the ‘distinguishability problem’). Since the indistinguishability of experiences is the antecedent in the inference McDowell intends to block, I suggest that his argument rests on an inconsistent set of premises. In support of this, I show that Pritchard’s response to the distinguishability problem is incompatible with the conclusion of the ‘inference-blocking’ argument, and that the response available in McDowell’s work relies on a mistaken conception of fallibility. Either McDowell must deny the sceptic’s premise that perceptual experiences are indistinguishable, or he must give up his conclusion that perceptual warrant can be indefeasible.	accessibility;corner case;dalton pritchard;defeasible reasoning;disjunctive normal form;experience	Peter Dennis	2014	Synthese	10.1007/s11229-014-0516-0	philosophy;epistemology;mathematics	AI	-13.26415177210939	3.4872243806160057	144157
0b1275eef863b5fbc2e6649f3b124169d01970d6	constraining influence diagram structure by generative planning: an application to the optimization of oil spill response	emergency response;ordered set;oil spill;levels of abstraction;influence diagram	We develop and extend existing decision-theoretic methods for troubleshooting a nonfunctioning device. Traditionally, diagnosis with Bayesian networks has focused on belief updating| determining the probabilities of various faults given current observations. In this paper, we extend this paradigm to include taking actions. In particular, we consider three classes of actions: (1) we can make observations regarding the behavior of a device and infer likely faults as in traditional diagnosis, (2) we can repair a component and then observe the behavior of the device to infer likely faults, and (3) we can change the con guration of the device, observe its new behavior, and infer the likelihood of faults. Analysis of latter two classes of troubleshooting actions requires incorporating notions of persistence into the belief-network formalism used for probabilistic inference.	bayesian network;influence diagram;naruto shippuden: clash of ninja revolution 3;persistence (computer science);programming paradigm;semantics (computer science);theory	John Mark Agosta	1996			mathematical optimization;simulation;influence diagram;artificial intelligence;machine learning	AI	-19.064049825098877	-3.687258725485049	144286
131ee6266fb4962c35757f3cfda2b719cfadd249	a quantum model for the ellsberg and machina paradoxes	machina paradox;quantum modeling;ambiguity aversion;ellsberg paradox	The Ellsberg and Machina paradoxes reveal that expected utility theory is problematical when real subjects take decisions under uncertainty. Suitable generalizations of expected utility exist which attempt to solve the Ellsberg paradox, but none of them provides a satisfactory solution of the Machina paradox. In this paper we elaborate a quantum model in Hilbert space describing the Ellsberg situation and also the Machina situation, and show that we can model the specific aspect of the Machina situation that is unable to be modeled within the existing generalizations of expected utility.	machina (company);quantum	Diederik Aerts;Sandro Sozzo;Jocelyn Tapia	2012		10.1007/978-3-642-35659-9_5	ambiguity aversion;ellsberg paradox;mathematics;mathematical economics;welfare economics	NLP	-8.781866710092938	-1.143744822230319	144402
5c3f80a777af5ae3ab7501dabff337cf9eea5877	agreeing on decisions: an analysis with counterfactuals		Moses & Nachum ([7]) identify conceptual flaws in Bacharach’s generalization ([3]) of Aumann’s seminal “agreeing to disagree” result ([1]). Essentially, Bacharach’s framework requires agents’ decision functions to be defined over events that are informationally meaningless for the agents. In this paper, we argue that the analysis of the agreement theorem should be carried out in information structures that can accommodate for counterfactual states. We therefore develop a method for constructing such “counterfactual structures” (starting from partitional structures), and prove a new agreement theorem within such structures. Furthermore, we show that our approach also resolves the conceptual flaws in the sense that, within our framework, decision functions are always only defined over events that are informationally meaningful for the agents.	counterfactual conditional;counterfactual definiteness;fits;mind;moses	Bassel Tarbush	2013	CoRR		artificial intelligence;mathematics;mathematical economics;algorithm	Logic	-13.653298719126452	3.514563514110299	144457
a0e9fb7f2960aee8b734d9a793e7e03c4e28fec3	an effective approach for distributed program allocation	performance measure;simulated annealing algorithm;distributed programs;simulated annealing;distributed architecture;task allocation	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	H. B. Zhou	1994	Parallel Algorithms Appl.	10.1080/10637199408962526	distributed algorithm;real-time computing;simulated annealing;computer science;theoretical computer science;distributed computing;adaptive simulated annealing	Robotics	-14.355851413365913	-5.178257418031373	144614
a6738108d84ae7d06aca1acb6a2537f01c13a5f5	personal business continuity planning	disaster recover planning;sudden death;impact analysis;business continuity;high priority;natural disaster;risk assessment;age groups;business process	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	business continuity planning;francis;primary source;scott continuity	Avinash W. Kadam	2010	Information Security Journal: A Global Perspective	10.1080/19393550903577657	risk assessment;natural disaster;business process;operations research;computer security;demographic profile;disaster recovery	Robotics	-15.502772934903026	-5.5331119966984845	144726
42193396ec7271912380e2cf2d92167af8ae919c	extraordinary cryptology collection	sale;cryptology collection;louis kruh	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	cryptography;francis;primary source	Brian J. Winkel	2003	Cryptologia	10.1080/0161-110391891847	theoretical computer science;cryptography;computer science	Robotics	-15.157671815798178	-5.5849586940804725	144767
2d8075e51f444f374b55925e5f7f91bdf1672f65	assisting seller pricing strategy selection for electronic auctions	pricing consumer electronics internet taxonomy computer science search engines simple object access protocol electronic commerce predictive models xml;extensible markup language;electronic commerce;selected works;search engines;intelligent auction registry engine;pricing;seller pricing strategy;consumer electronics;null;business model;internet;risk preference;knowledge based systems pricing electronic commerce access protocols xml;taxonomy;electronic auctions;xml;access protocols;simple object access protocol seller pricing strategy electronic auctions bidder risk preferences business model intelligent auction registry engine extensible markup language;bepress;predictive models;profitability;computer science;simple object access protocol;technology choice;knowledge based systems;bidder risk preferences	This research examines sellers' pricing strategy oriented to bidders' risk preferences, within the context of an electronic auction. A business model is then proposed to predict the probability of potential bidders' risk preference, which sellers can consider to set pricing strategy to maximize their profits. A tailored intelligent auction registry engine is constructed based upon an initial version of our business model. Because the extensible markup language (XML)-based simple object access protocol (SOAP) is utilized to support interactions with other resources, our engine is decoupled from the underlying technology choices; therefore, it can be deployed to different platforms.	interaction;markup language;soap;xml	Jia Zhang;Ning Zhang;Jen-Yao Chung	2004	Proceedings. IEEE International Conference on e-Commerce Technology, 2004. CEC 2004.	10.1109/ICECT.2004.1319714	xml;computer science;taxonomy;forward auction	Robotics	-7.824238775712232	-8.843392096721416	144813
54ba4bdabaf9a24a2b68c377a6755e04370d43b0	excessive abundance of common resources deters social responsibility	game theory;social responsibility;models theoretical;cooperative behavior	We study the evolution of cooperation in the collective-risk social dilemma game, where the risk is determined by a collective target that must be reached with individual contributions. All players initially receive endowments from the available amount of common resources. While cooperators contribute part of their endowment to the collective target, defectors do not. If the target is not reached, the endowments of all players are lost. In our model, we introduce a feedback between the amount of common resources and the contributions of cooperators. We show that cooperation can be sustained only if the common resources are preserved but never excessively abound. This, however, requires a delicate balance between the amount of common resources that initially exist, and the amount cooperators contribute to the collective target. Exceeding critical thresholds in either of the two amounts leads to loss of cooperation, and consequently to the depletion of common resources.	biologic preservation;depletion region;feedback;game theory;social characteristics;the evolution of cooperation	Xiaojie Chen;Matjaž Perc	2014		10.1038/srep04161	game theory;social responsibility	HCI	-11.029515673411199	-9.77109916432329	144909
45bbb4c0040511eff6c7f55b59af0aca09caf938	projection, symmetry, and natural kinds	humanidades;filosofia etica	Scientific practice involves two kinds of induction. In one, generalizations are drawn about the states of a particular system of variables. In the other, generalizations are drawn across systems in a class. We can discern two questions of correctness about both kinds of induction: (P1) what distinguishes those systems and classes of system that are ‘projectible’ in Goodman’s (Fact, fiction and forecast, 1955) sense from those that are not, and (P2) what are the methods by which we are able to identify kinds that are likely to be projectible? In answer to the first question, numerous theories of ‘natural kinds’ have been advanced, but none has satisfactorily addressed both questions simultaneously. I propose a shift in perspective. Both essentialist and cluster property theories have traditionally characterized kinds directly in terms of the causally salient properties their members possess. Instead, we should focus on ‘dynamical symmetries’, transformations of a system to which the causal structure of that system is indifferent. I suggest that to be a member of natural kind it is necessary and sufficient to possess a particular collection of dynamical symmetries. I show that membership in such a kind is in turn necessary and sufficient for the presence of the sort of causal structure that accounts for success in both kinds of induction, thus demonstrating that (P1) has been answered satisfactorily. More dramatically, I demonstrate that this new theory of ‘dynamical kinds’ provides an answer to (P2) with methodological implications concerning the discovery of projectible kinds.	causal filter;chaos theory;correctness (computer science);counterfactual conditional;dynamical system;mathematical induction	Benjamin C. Jantzen	2014	Synthese	10.1007/s11229-014-0637-5	philosophy;epistemology;pure mathematics;mathematics;algorithm	AI	-13.75884612197848	1.8893127908334482	145153
5b705b63d2519d5299c8467243eb5ce33442332d	dynamic data driven methods for self-aware aerospace vehicles	statistical inference	A self-aware aerospace vehicle can dynamically adapt the way it performs missions by gathering information about itself and its surroundings and responding intelligently. Achieving this DDDAS paradigm enables a revolutionary new generation of self-aware aerospace vehicles that can perform missions that are impossible using current design, flight, and mission planning paradigms. To make self-aware aerospace vehicles a reality, fundamentally new algorithms are needed that drive decision-making through dynamic response to uncertain data, while incorporating information from multiple modeling sources and multiple sensor fidelities. In this work, the specific challenge of a vehicle that can dynamically and autonomously sense, plan, and act is considered. The challenge is to achieve each of these tasks in real time—executing online models and exploiting dynamic data streams—while also accounting for uncertainty. We employ a multifidelity approach to inference, prediction and planning—an approach that incorporates information from multiple modeling sources, multiple sensor data sources, and multiple fidelities.	algorithm;dynamic data driven applications systems;exploit (computer security);programming paradigm;real-time computing;self-awareness;uncertain data	Douglas L. Allaire;George Biros;J. Chambers;Omar Ghattas;D. Kordonowy;Karen Willcox	2012		10.1016/j.procs.2012.04.130	real-time computing;simulation;data mining	Robotics	-18.427278554264635	-3.2728362254081342	145242
9eda4783065de2b866f3ceaded68cd0582c2a00f	convergence of aspirations and (partial) cooperation in the prisoner's dilemma	aspiration level;game theory;learning;prisoner s dilemma;cooperation;dynamic model;aspirations cooperation bounded rationality;bounded rationality;hb economic theory;economia y empresa generalidades;economia y empresa;information;aspirations	"""This paper proposes an aspiration-based model for (anonymous) cooperation where a large population of agents are re-matched every period to playa Prisoner's Dilemma. At each point in time, agents hold a certain common aspiration level which is updated on the basis of population-average experience. On the other hand, those agents who (relative to current aspiration) feel """"dissatisfied"""" switch actions at a rate which is increasing in the magnitude of the dissatisfaction. The induced process is shown to converge in the long run under quite general conditions. Moreover, if agents are responsive enough, the long-run social state is seen to display some extent of cooperation, a constant positive fraction of the population (always less than halt) choosing to cooperate in every period."""	prisoner's dilemma	Frédéric Palomino;Fernando Vega-Redondo	1999	Int. J. Game Theory	10.1007/s001820050120	superrationality;game theory;information;economics;microeconomics;mathematical economics;welfare economics;cooperation;prisoner's dilemma;bounded rationality	Logic	-5.32341604348805	-5.496452903337651	145337
0e936fa74431ffc0e3229bcfc54b8555b71bd6e1	crash risk assessment using intelligent transportation systems data and real-time intervention strategies to improve safety on freeways	it strategy;traffic simulation;traffic speed;accident black spots;pedestrian safety;microscopic traffic simulation;poison control;intelligent transport system;its data;high accident locations;injury prevention;intelligent transportation systems;real time;real time traffic;microscopic simulation;safety literature;traffic flow;real time information;traffic management;traffic safety;injury control;ramp metering;home safety;injury research;speed limits;safety abstracts;human factors;accident prone locations;occupational safety;loop detectors;safety;traffic;microscopic traffic flow;hotspot;risk assessment;safety research;accident prevention;variable speed limit;regression analysis;freeways;violence prevention;bicycle safety;traffic data;highway factors in crashes;traffic crashes;poisoning prevention;falls;high speed;real time crash prediction;ergonomics;logistic regression model;suicide prevention;proactive traffic management;variable speed limits;ramps interchanges	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source;real-time transcription;risk assessment	Mohamed A. Abdel-Aty;Anurag Pande;Chris Lee;Vikash V. Gayah;Cristina dos Santos	2007	J. Intellig. Transport. Systems	10.1080/15472450701410395	risk assessment;real-time data;intelligent transportation system;active traffic management;hotspot;computer science;engineering;suicide prevention;human factors and ergonomics;injury prevention;machine learning;traffic flow;transport engineering;logistic regression;forensic engineering;computer security;regression analysis;statistics	Robotics	-15.312256627933538	-6.609754345276971	145616
4aa2b04f14af2736290ea9797eb5786d4759e980	subsidies, stability, and restricted cooperation in coalitional games	various restriction;coalitional game;tight bound;minimal demand relaxation;required subsidy;restricted cooperation;minimal subsidy;extended core;minimal external subsidy;various artificial intelligence application;game theory	Cooperation among automated agents is becoming increasingly important in various artificial intelligence applications. Coalitional (i.e., cooperative) game theory supplies conceptual and mathematical tools useful in the analysis of such interactions, and in particular in the achievement of stable outcomes among self-interested agents. Here, we study the minimal external subsidy required to stabilize the core of a coalitional game. Following the Cost of Stability (CoS) model introduced by Bachrach et al. [2009a], we give tight bounds on the required subsidy under various restrictions on the social structure of the game. We then compare the extended core induced by subsidies with the least core of the game, proving tight bounds on the ratio between the minimal subsidy and the minimal demand relaxation that each lead to stability.	applications of artificial intelligence;game theory;interaction;linear programming relaxation;social structure	Reshef Meir;Jeffrey S. Rosenschein;Enrico Malizia	2011		10.5591/978-1-57735-516-8/IJCAI11-060	mathematical economics	AI	-5.607082582354576	-0.5926039263014713	145850
b943523e070551d03a16718e298369208dc94176	market equilibrium for bundle auctions and the matching core of nonnegative tu games	matching core. jel classifications:;matching;multi-unit auctions;nonnegative tu game;constrained equilibrium;market equilibrium;bundle auctions	We discuss bundle auctions within the framework of an integer allocation problem. We show that for multi-unit auctions, of which bundle auctions are a special case, market equilibrium and constrained market equilibrium are equivalent concepts. This equivalence, allows us to obtain a computable necessary and sufficient condition for the existence of constrained market equilibrium for bundle auctions. We use this result to obtain a necessary and sufficient condition for the existence of market equilibrium for multi-unit auctions. After obtaining the induced bundle auction of a nonnegative TU game, we show that the existence of market equilibrium implies the existence of a possibly different market equilibrium as well, which corresponds very naturally to an outcome in the matching core of the TU game. Consequently we show that the matching core of the nonnegative TU game is non-empty if and only if the induced market game has a market equilibrium.	computable function;darknet market;decibel;turing completeness	Somdeb Lahiri	2006	CoRR		matching;economics;microeconomics;mathematical economics;welfare economics;equilibrium selection	ECom	-4.6218907320390406	-1.9129562198658931	145977
71806f75715932405eabdb6bce1fbd701c33fb7b	non-iterative privacy preservation for online lotteries	electronic commerce;t technology general;noniterative privacy preservation;privacy preservation;online lottery game;online lottery game internet noniterative privacy preservation electronic lottery method;internet;data privacy;internet computer games data privacy electronic commerce;electronic lottery method;computer games	Unlike gambling, lottery games can exist in a lawful form to raise funds for charitable institutions. Owing to the expeditious development of network technology, lotteries over the Internet have become an inevitable trend. Since the Internet allows people to communicate with each other without direct contact, it is more difficult to guarantee the security and fairness of online lotteries than for conventional lottery games. However, electronic lottery methods can also provide something that conventional lottery mechanisms cannot: they allow players to purchase tickets at any time and in any place where they can access the Internet. The authors propose an online lottery mechanism that can confirm the propositions of general lottery games. Specifically, this novel method not only allows players to make t-out-of-n numbers in lotteries without iterative selection but also preserves the privacy of players' choices, making the system more similar to traditional lottery games.	iterative method;privacy	Jung-San Lee;Chi-Shiang Chan;Chin-Chen Chang	2009	IET Information Security	10.1049/iet-ifs.2008.0104	e-commerce;the internet;information privacy;computer science;internet privacy;computer security	Crypto	-8.06768705287095	-7.755194837685425	146028
61bb112a7777c859b4f572e45ebdb9c557b42754	delay in the alternating-offers model of bargaining	bargaining;breakdown	Alternating offers bargaining has been extensively used to model two-sided negotiations. The celebrated model of Rubinstein (1982) has provided a formal justification for equitable payoff division. A typical assumption of these models under risk is that the termination event means a complete and irrevocable breakdown in negotiations. In this paper, the meaning of termination is reinterpreted as the imposition to finish negotiations immediately. Specifically, bargaining terminates when the last offer becomes definitive. While Rubinstein’s model predicts an immediate agreement with stationary strategies, we show that the same payoff allocation is attainable under non-stationary strategies. Moreover, the payoffs in delayed equilibria are potentially better for the proposer than those in which agreement is immediately reached.	stationary process	Juan J. Vidal-Puga	2008	Int. J. Game Theory	10.1007/s00182-008-0128-3	chemical decomposition;economics;operations management;microeconomics;mathematical economics;welfare economics	AI	-6.455630693644547	-4.203285579957239	146178
102bccd5e1fcc9e5b34370bf62e8501282e695be	a-brain: a general system for solving data analysis problems	a brain;decision maker;data analysis;multi expression programming;adaptive system;intelligent systems;intelligent system;symbolic regression;numerical experiment;problem solving	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	algorithm;experiment;francis;genetic programming;media-embedded processor;numerical analysis;primary source;solver;symbolic regression;system call	Mihai Oltean	2007	J. Exp. Theor. Artif. Intell.	10.1080/09528130701416835	decision-making;mathematical optimization;simulation;computer science;artificial intelligence;adaptive system;machine learning;data analysis	Robotics	-13.633663511616083	-5.735071582950968	146390
c8c27baaff499dbb43b4c87dcc6bb96f10ffd531	war of the benchmark means: time for a truce	benchmarking;goodness of fit;confidence level;normal distribution;lognormal distribution;skewed distribution;geometric mean;sampling methods;performance ratio;log normal distribution;gaussian distribution	"""For decades, computer benchmarkers have fought a War of Means. Although many have raised concerns with the geometric mean (GM), it continues to be used by SPEC and others. This war is an unnecessarymisunderstanding due to inadequately articulated implicit assumptions, plus confusio namong populations, their parameters, sampling methods, and sample statistics. In fact, all the Means have their uses, sometimes in combination. Metrics may be algebraically correct, but statistically irrelevant or misleading if applied to population distributions for which they are inappropriate. Normal (Gaussian) distributions are so useful that they are often assumed without question,but many important distributions are not normal.They require different analyses, most commonly by finding a mathematical transformations that yields a normal distribution,computing the metrics, and then back-transforming to the original scale. Consider the distribution of relative performance ratios of programs on two computers. The normal distribution is a good fit only when variance and skew are small, but otherwise generates logical impossibilities and misleading statistical measures. A much better choice is the lognormal (or log-normal) distribution, not just on theoretical grounds, but through the (necessary) validation with real data. Normal and lognormal distributions are similar for low variance and skew, but the lognormal handles skewed distributions reasonably, unlike the normal. Lognormal distributions occur frequently elsewhere are well-understood, and have standard methods of analysis.Everyone agrees that """"Performance is not a single number,"""" ... and then argues about which number is better. It is more important to understanding populations, appropriate methods, and proper ways to convey uncertainty. When population parameters are estimated via samples, statistically correct methods must be used to produce the appropriate means, measures of dispersion, Skew, confidence levels, and perhaps goodness-of-fit estimators. If the wrong Mean is chosen, it is difficult to achieve much. The GM predicts the mean relative performance of programs, not of workloads. The usual GM formula is rather unintuitive, and is often claimed to have no physical meaning. However, it is the back-transformed average of a lognormal distribution, as can be seen by the mathematical identity below. Its use is not onlystatistically appropriate in some cases, but enables straightforward computation of other useful statistics.<display equation>""""If a man will begin in certainties, he shall end in doubts, but if he will be content to begin with doubts, he shall end with certainties.""""  — Francis Bacon, in Savage."""	benchmark (computing);computation;computer;francis;natural language understanding;performance;population;relevance;sampling (signal processing);savage	John R. Mashey	2004	SIGARCH Computer Architecture News	10.1145/1040136.1040137	normal distribution;log-normal distribution	DB	-11.815161328432632	0.7701316976952202	146438
03a936bc5b2e453f6bff359e3ff272ad6c912840	dynamically formed heterogeneous robot teams performing tightly-coordinated tasks	dynamically formed heterogeneous robot teams;pickup team challenge;path planning;market based task allocation;mobile robots;software agents mobile robots multi robot systems path planning;software agents;robot kinematics service robots robot vision systems robot sensing systems gas insulated transmission lines computer science robustness software agents multirobot systems humans;multi robot systems;cost effectiveness;treasure hunt task dynamically formed heterogeneous robot teams tightly coordinated tasks pickup team challenge market based task allocation synchronized task execution;tightly coordinated tasks;synchronized task execution;treasure hunt task;task allocation;robot team	As we progress towards a world where robots play an integral role in society, a critical problem that remains to be solved is the pickup team challenge; that is, dynamically formed heterogeneous robot teams executing coordinated tasks where little information is known a priori about the tasks, the robots, and the environments in which they would operate. Successful solutions to forming pickup teams would enable researchers to experiment with larger numbers of robots and enable industry to efficiently and cost-effectively integrate new robot technology with existing legacy teams. In this paper, we define the challenge of pickup teams and propose the treasure hunt domain for evaluating the performance of pickup teams. Additionally, we describe a basic implementation of a pickup team that can search and discover treasure in a previously unknown environment. We build on prior approaches in market-based task allocation and plays for synchronized task execution, to allocate roles amongst robots in the pickup team, and to execute synchronized team actions to accomplish the treasure hunt task	robot;task manager	Edward Gil Jones;Brett Browning;M. Bernardine Dias;Brenna Argall;Manuela M. Veloso;Anthony Stentz	2006	Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.	10.1109/ROBOT.2006.1641771	mobile robot;real-time computing;simulation;cost-effectiveness analysis;computer science;engineering;artificial intelligence;software agent;distributed computing;motion planning	Robotics	-18.47206052712429	-9.118814027186817	146643
d9e8d8a9e2b1d163804acec7e10709d890fc62e3	interactive tv for the home: an ethnographic study on users' requirements and experiences	human computer interaction;t technology general;design and development;usability study;field trial;interactive tv;lessons learned	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source;requirement	Marianna Obrist;Regina Bernhaupt;Manfred Tscheligi	2008	Int. J. Hum. Comput. Interaction	10.1080/10447310701821541	simulation;human–computer interaction;computer science;multimedia;interactive television;world wide web	Robotics	-16.100676346350262	-5.992751635157123	146683
5043e30753ddc81180dd0f2a125639e58acf039a	probability logic		"""Introduction Among logicians it is well-known that Leibniz was the first to conceive of a mathematical treatment of logic. Much less known, however, was his insistence that there was need for a new kind of logic that would treat of degrees of probability. Although it isn't clear what Leibniz had in mind for such a logic—understandably, since the subject of probability had just begun in his lifetime and the florescence of modern logic was not to begin until the 19th century—he did envision that it would be a means for estimating likelihoods and a way of proof leading not to certainty but only to probability (see his Nouveaux Essais, pp. 372-373). Beginning in his day, and extending through the present century, a number of mathematicians and logicians, e. made either forays or detailed attacks on establishing such a logic, but with differing conceptions as to its nature. A few brief remarks will give some idea as to what these were like. To Bernoulli (as also to Leibniz) probability was degree of certainty, differing from it as part to whole. In his Λrs Conjectandi (Part IV, Chapter III) he considers the various kinds of arguments (i.e., grounds) for a conclusion (opinion or conjecture) and the problem of estimating their weights so as to compute the probability of the conclusion. Situations involving arguments are divided into three types: those in which the argument is necessarily the case but indicates (proves) the conclusion only contingently; those in which the argument is contingent but when present necessarily proves the conclusion; and those in which the presence of the argument and its proving of the conclusion are both contingent. The * 'proving power"""" of an argument is determined by the number of cases in which the argument is, or is not, present and also by the number of"""	bernoulli polynomials;contingency (philosophy);process (computing)	Theodore Hailperin	1984	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1093870625		AI	-12.815342063383554	3.7048080305834894	146694
0ea9a0a66973bc8943a734770a0e43040f144927	ɛ-consistent equilibrium in repeated games	repeated game;stochastic games	We introduce the concept of e-consistent equilibrium where each player plays a e-best response after every history reached with positive probability. In particular, an e-consistent equilibrium induces an e-equilibrium in any subgame reached along the play path. The existence of e-consistent equilibrium is examined in various repeated games. The main result is the existence in stochastic games with absorbing states.		Ehud Lehrer;Sylvain Sorin	1998	Int. J. Game Theory	10.1007/s001820050069	markov perfect equilibrium;epsilon-equilibrium;sequential equilibrium;economics;subgame;repeated game;microeconomics;mathematical economics;subgame perfect equilibrium;welfare economics;equilibrium selection;solution concept;symmetric equilibrium	ECom	-4.8975493156847	-1.7584776486340221	146751
88c0e1a5db6307df7b2c59290a0d1b258d78181d	graphical modeling and analysis software for state space-based optimization of discrete event systems		In view of the particular research objectives rather than the system’s characteristics, almost all systems can be discretized regardless of original continuous or discrete pattern. Modeling oriented to discrete-event system (DES) represents the dynamics of a system as a series of discrete events that perform changes in the state of the system, constituting the state space which supports the further analysis for scheduling and optimization. In this paper, the graphical modeling and analysis software (GMAS) as a platform for modeling DES is introduced with the basic notions and a general perspective on the systems approach. It clearly provides the graphic modeling and analysis interface. Besides, the system evolution process is recorded and represented by state space, transforming the optimization problem into a search-based issue in the reachability tree of finding the optimal or near-optimal sequence of function component activations from some initial state to the goal state. To validate its efficacy and practicability, a causal encounter model of traffic collision avoidance system operations is proposed in the GMAS formalism. The model has been proved to not only provide a better comprehension of the potential collision occurrences for risk assessment by representing the cause–effect relationship of each action but also aid the crews in the involved aircraft to make a cooperative and optimal option.	causality;discretization;mathematical optimization;optimization problem;program optimization;reachability;risk assessment;scheduling (computing);semantics (computer science);state space;traffic collision avoidance system	Jun Tang;Feng Zhu	2018	IEEE Access	10.1109/ACCESS.2018.2852324	theoretical computer science;traffic collision avoidance system;computer science;distributed computing;collision;state space;scheduling (computing);discretization;reachability;software;optimization problem	Robotics	-12.973388696066744	-7.746067417498855	146879
ea8052f611befb84f173d5d8fc835192e911821d	a contrast between two decision rules for use with (convex) sets of probabilities: γ-maximin versus e-admissibilty	recueil d articles;admissibilite;theorie de la connaissance;probability;utility;regle;independance;utilite;maximization;uncertainty;independence;ensemble;bayesianism;probabilite;decision;set of articles;knowledge theory;incertitude;set;rule;levi i;maximisation;bayesianisme	This paper offers a comparison between two decision rules for use when uncertainty is depicted by a non-trivial, convex2 set of probability functions . This setting for uncertainty is different from the canonical Bayesian decision theory of expected utility, which uses a singleton set, just one probability function to represent a decision maker’s uncertainty. Justifications for using a non-trivial set of probabilities to depict uncertainty date back at least a half century (Good 1952) and a foreshadowing of that idea can be found even in Keynes’ (1921), where he allows that not all hypotheses may be comparable by qualitative probability – in accord with, e.g., the situation where the respective intervals of probabilities for two events merely overlap with no further (joint) constraints, so that neither of the two events is more, or less, or equally probable compared with the other. Here, I will avail myself of the following simplifying assumption: Throughout, I will avoid the complexities that ensue when the decision maker’s values for outcomes also are indeterminate and, in parallel with her or his uncertainty, are then depicted by a set of (cardinal) utilities. That is, for this discussion, I will contrast two decision rules when the decision maker’s uncertainties, but not her/his values are indeterminate. The more familiar decision rule of the pair under discussion, Maximin,3 requires that the decision maker ranks an option by its lower expected value, taken with respect to the convex set of probabilities, , and then to choose an option whose lower expected value is maximum. This decision rule (as simplified by the two assumptions, above) was given a representation in terms of a binary preference relation over Anscombe– Aumann horse lotteries (Gilboa and Schmeidler 1989), has been discussed	admissible heuristic;convex function;convex set;database normalization;decision problem;decision theory;dilation (morphology);expected utility hypothesis;failure;ibm notes;indeterminacy in concurrent computation;joint constraints;lexicographical order;loss function;minimax;multitier architecture;rejection sampling;rule 90;savage;subgraph isomorphism problem;trojan horse (computing)	Teddy Seidenfeld	2004	Synthese	10.1023/B:SYNT.0000029942.11359.8d	set;independence;ensembl;uncertainty;bayesian probability;probability;data mining;mathematics;utility;statistics	AI	-9.104279658098703	-0.764798192965436	147056
59d1a3d2b6b238e14080888923141ae4ff70eea6	probability as a theory dependent concept	mecanique quantique;distribution;behaviorism;concept;stochastique;probability;philosophy of language;indistinguabilite;equiprobabilite;possible;hypothese;logic;frequence;theorie du jeu;axiom;empirical;possibility measure;dependance;test;satisfiability;subjectif vs objectif;value;recherche;occurence;strategy;dependency;frequency of occurrence;kolmogorov a;subjective vs objective;behaviorisme;quantum mechanics;theory;probabilite;success rate;philosophy;epistemology;measure;metaphysics;valeur;interpretation;evenement;event;propensite;mesure;hypothesis;frequency;assumption;assomption;strategie;prediction;a priori;propensity;empirique;theorie;axiome	It is argued that probability should be defined implicitly by the distributions of possible measurement values characteristic of a theory. These distributions are tested by, but not defined in terms of, relative frequencies of occurrences of events of a specified kind. The adoption of an a priori probability in an empirical investigation constitutes part of the formulation of a theory. In particular, an assumption of equiprobability in a given situation is merely one hypothesis inter alia, which can be tested, like any other assumption. Probability in relation to some theories – for example quantum mechanics – need not satisfy the Kolmogorov axioms. To illustrate how two theories about the same system can generate quite different probability concepts, and not just different probabilistic predictions, a team game for three players is described. If only classical methods are allowed, a 75% success rate at best can be achieved. Nevertheless, a quantum strategy exists that gives a 100% probability of winning.	kolmogorov complexity;quantum mechanics;theory	David Atkinson;Jeanne Peijnenburg	1999	Synthese	10.1023/A:1005242414754	dependency;distribution;probability and statistics;behaviorism;hypothesis;imprecise probability;conditional probability;measure;prediction;probability measure;event;philosophy;epistemology;interpretation;strategy;philosophy of language;probability interpretations;mean-preserving spread;frequency;metaphysics;calculus;circular distribution;probability;a priori probability;mathematics;axiom;concept;logic;theory;algorithm;satisfiability	ML	-11.001846875910301	1.5122300844998313	147638
1b0c9168071676230383c7667afaf159402f5d6e	conceptual role semantics	representation;concept;semantics;synthetic;mental content;translation;symbol;meaning;compositionality;analytic;holism;language;indication;chinese room;information;inference;thought;conceptual role	In the philosophy of language, conceptual role semantics (hereafter CRS) is a theory of what constitutes the meanings possessed by expressions of natural languages, or the propositions expressed by their utterance. In the philosophy of mind, it is a theory of what constitutes the contents of psychological attitudes, such as beliefs or desires.  CRS comes in a variety of forms, not always clearly distinguished by commentators. Such versions are known variously as functional/causal/computational role semantics, and more broadly as use-theories of meaning. Nevertheless, all are united in seeking the meaning or content of an item, not in what it is made of, nor in what accompanies or is associated with it, but in what is done with it, the use it is put to. Roughly, according to CRS, the meaning or propositional content of an expression or attitude is determined by the role it plays in a person’s language or in her cognition.  Currently, many view CRS as the main rival to theories that take notions such as truth or reference as central (for example, Davidson 2001), although the relationship between the two is not straightforward. The following outlines the main varieties of CRS, provides a cursory survey of its history, introduces the central arguments offered in its favor, and provisionally assesses how the variants fair against a number of prominent criticisms.		Gilbert Harman	1982	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1093883628	psychology;epistemology;communication;social psychology	Logic	-12.881489035793344	3.1653895513681687	147781
f8c1062a7ca42d31d9eb6135c7a5bde691c96d1b	an electronic marketplace based on reputation and learning	tecnologia electronica telecomunicaciones;electronic commerce;market model;agent based;agent modeling;reinforcement learning;reputation;electronic marketplace;grupo c;electronic commerce agents;profitability;tecnologias;product quality	In this paper, we propose a market model which is based on reputation and reinforcement learning algorithms for buying and selling agents. Three important factors: quality, price and delivery-time are considered in the model. We take into account the fact that buying agents can have different priorities on quality, price and delivery-time of their goods and selling agents adjust their bids according to buying agents preferences. Also we have assumed that multiple selling agents may offer the same goods with different qualities, prices and delivery-times. In our model, selling agents learn to maximize their expected profits by using reinforcement learning to adjust product quality, price and delivery-time. Also each selling agent models the reputation of buying agents based on their profits for that seller and uses this reputation to consider discount for reputable buying agents. Buying agents learn to model the reputation of selling agents based on different features of goods: reputation on quality, reputation on price and reputation on delivery-time to avoid interaction with disreputable selling agents. The model has been implemented with Aglet and tested in a large-sized marketplace. The results show that selling/buying agents that model the reputation of buying/selling agents obtain more satisfaction rather than selling/buying agents who only use the reinforcement learning.	algorithm;machine learning;reinforcement learning	Omid Roozmand;Mohammad Ali Nematbakhsh;Ahmad Baraani-Dastjerdi	2007	JTAER		e-commerce;public relations;buying agent;reputation;computer science;marketing;world wide web;reinforcement learning;commerce;profitability index	AI	-7.574799089915068	-8.517125095908154	147949
f1dc8e82f607f08814cbc0b7d3bdb1d6f2240015	p-spline estimation of functional classification methods for improving the quality in the food industry	62g05;functional principal components analysis;p splines;functional partial least squares;62h25;functional linear discriminant analysis;functional logit regression	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	approximation;b-spline;francis;http cookie;kerrison predictor;linear discriminant analysis;logistic regression;nonlinear system;odds algorithm;primary source;principal component analysis;receiver operating characteristic;smoothing;spline (mathematics);statistical classification;t-spline	M. C. Aguilera-Morillo;Ana M. Aguilera	2015	Communications in Statistics - Simulation and Computation	10.1080/03610918.2013.804555	econometrics;mathematics;statistics	Robotics	-14.126963317748027	-5.858934884995167	148212
45c99406734863be7b0c88526c8802110891b7aa	welfarist evaluations of decision rules for boards of representatives	b philosophy general;voting power;decision rule	We consider a decision board with representatives who vote on proposals on behalf of their constituencies. We look for decision rules that realize utilitarian and (welfarist) egalitarian ideals. We set up a simple model and obtain roughly the following results. If the interests of people from the same constituency are uncorrelated, then a weighted rule with square root weights does best in terms of both ideals. If there are perfect correlations, then the utilitarian ideal requires proportional weights, whereas the egalitarian ideal requires equal weights. We investigate correlations that are in between these extremes and provide analytic arguments to connect our results to Barbera and Jackson (J Polit Econ 114(2):317–339, 2006) and to Banzhaf voting power.		Claus Beisbart;Luc Bovens	2007	Social Choice and Welfare	10.1007/s00355-007-0246-z	economics;decision rule;mathematics;microeconomics;mathematical economics;social psychology;law;welfare economics	ECom	-7.5890710024980965	-3.471348670694586	148439
f076a37debd73da9e017e7e1d453285de5443806	prediction of the opponent's preference in bilateral multi-issue negotiation through bayesian learning	bilateral;learning;opponent;multi;bayesian;issue;preference;prediction;negotiation	In multi-issue negotiation, agents’ preferences are extremely important factors for reaching mutual beneficial agreements. However, agents would usually keeping their preferences in secret in order to avoid be exploited by their opponents during a negotiation. Thus, preference modelling has become an important research direction in the area of agent-based negotiation. In this paper, a bilateral multi-issue negotiation approach is proposed to help both negotiation agents to maximise their utilities under a setting that the opponent agent’s preference is private information. In the proposed approach, Bayesian learning is employed to analyse the opponent’s historical offers and approximately predicate the opponent’s preference over negotiation issues. Besides, a counter-offer proposition algorithm is integrated in our approach to help agents to generate mutual beneficial offers based on the preference learning result. Also, the experimental results indicate the good performance of the proposed approach in aspects of utility gain and negotiation efficiency.	agent-based model;algorithm;bilateral filter;personally identifiable information;preference learning	Jihang Zhang;Fenghui Ren;Minjie Zhang	2014		10.1007/978-3-319-30307-9_1	artificial intelligence;machine learning;social psychology	AI	-9.294345605764796	-7.942624815314999	148594
42f3477a942aab7f0cafdbc6273ff04fd4741c08	trade reduction vs. multi-stage: a comparison of double auction design approaches	commerce electronique;emerging market;comercio electronico;sintesis mecanismo;strategy proof mechanism;market structure;electronic markets;synthese mecanisme;scenario;argumento;script;subasta;bidding;enchere;estructura mercado;mechanism synthesis;mechanism design;structure marche;high efficiency;double auction;electronic trade	With the growth of electronic markets, designing double auction mechanisms that are applicable to emerging market structures has become an important research topic. In this paper, we investigate two truthful double auction design approaches, the Trade Reduction Approach and the Multi-Stage Design Approach, and compare their resulting mechanisms in various exchange environments. We find that comparing with the Trade Reduction Approach, the Multi-Stage Design Approach offers mechanisms applicable to more complicated exchange environments. Furthermore, for the known trade reduction mechanisms, we prove that the corresponding mechanisms under the multi-stage design approach are superior in terms of both social efficiency and individual payoffs, in each exchange environment of interest. Our computational tests show that the mechanisms under the multi-stage design approach achieve very high efficiency in various scenarios. 2006 Published by Elsevier B.V.	computation;electronic markets	Leon Yang Chu;Zuo-Jun Max Shen	2007	European Journal of Operational Research	10.1016/j.ejor.2006.04.015	industrial organization;mechanism design;economics;bidding;scenario;marketing;operations management;double auction;market structure;emerging markets;commerce	AI	-5.487433141721049	-8.479333087660601	149212
030209a3edb536d1d174ecc560046e508630a123	probabilistic interpretations of integrability for game dynamics	evolutionary game theory;learning in games;integrability;decision rules	In models of evolution and learning in games, a variety of proofs of convergence rely on the assumption that the players’ choice functions are integrable. This assumption does not have an obvious game-theoretic interpretation. We address this question by introducing probability models defined in terms of piecewise smooth closed curves through Rn; these curves describe cycles in the performances of the available actions. We establish that a choice function is integrable if and only if in the probability model induced by each such curve, the rate at which players switch to a randomly drawn action is uncorrelated with a certain binary signal. The binary signal specifies whether the performance of the randomly drawn action is improving or worsening, and can also be interpreted as a signal about the performances of actions other than the one randomly drawn.	game theory;performance;randomness	William H. Sandholm	2014	Dynamic Games and Applications	10.1007/s13235-013-0082-y	evolutionary game theory;discrete mathematics;economics;decision rule;mathematics;microeconomics;mathematical economics;statistics	ML	-4.795608212797401	-1.0284180759916113	149478
ff480e658cd0fb0dcff5ee8ef07d2e924a1b1da6	check in or not? a stochastic game for privacy preserving in point-of-interest recommendation system		With the growing popularity of mobile social networks, point-of-interest (POI) recommendation, which utilizes users’ check-in data to suggest interesting places for users, has attracted much attention in recent years. The check-in data, containing time and location information, are closely related to the user’s personal life. Due to privacy concerns, users are reluctant to share check-in data with the service provider (SP), which causes a negative effect on recommendations. It is important for the user to find a balance between privacy and recommendation quality. In this paper, we consider a POI recommendation scenario where an adversary can access the data that a user reports to the SP. The user sequentially decides whether to check in for the POI he has visited. A stochastic game model is proposed to analyze the interaction between the user and the adversary. To find a good policy for the user, two value iteration algorithms are applied. The proposed game has a large state set, which makes it difficult for policy learning. To deal with this problem, we use some tricks when implementing the minimax Q-learning algorithm, and a set of neural networks are trained to approximate the Q-functions. To evaluate the performance of the learning algorithms, we conduct a series of simulations by using real-world check-in data. Simulation results show that the proposed learning algorithms can help the user to make good decisions, in the sense that the user can get a high long-term return.	adversary (cryptography);approximation algorithm;artificial neural network;gradient;inference attack;iteration;machine learning;markov decision process;minimax;mobile social network;point of interest;privacy;q-learning;randomized algorithm;recommender system;reinforcement learning;simulation	Lei Xu;Chunxiao Jiang;Nengqiang He;Yi Qian;Yong Ren;Jianhua Li	2018	IEEE Internet of Things Journal	10.1109/JIOT.2018.2847302	recommender system;point of interest;data mining;distributed computing;computer science;approximation algorithm;artificial neural network;adversary;minimax;stochastic game;information privacy	AI	-11.40494045546319	-7.532900548765448	149547
8240518b4722920c15120ccc9c35028ce1b3e12b	partially-specified large games	anonymity;equilibrio nash;game theory;nash equilibrium;structural robustness;teoria juego;metagames;theorie jeu;nash equilibria;strategie nash;anonymat;equilibre nash;internet;estrategia nash;nash strategy;anonimato;ex post nash	The sensitivity of Nash equilibrium to strategic and informational details presents a di¢ culty in applying it to games which are not fully specied. Structurally-robust Nash equilibria are less sensitive to such details. Moreover, they arrise naturally in important classes of games that have many semi-	nash equilibrium	Ehud Kalai	2005		10.1007/11600930_2	game theory;epsilon-equilibrium;simulation;best response;economics;microeconomics;mathematical economics;equilibrium selection;nash equilibrium	ECom	-5.1662851035190664	-2.0186860743372397	149719
9bafbecb28b2f055a90ec413a8754d9fa4e53e52	arguments from experience: the padua protocol	argumentation;protocol design;classification;roach;dialogue games;knowledge base;knowledge engineering	In this paper we describe PADUA, a protocol designed to enabl e gents to debate an issue drawing arguments not from a knowledge bas of facts, rules and priorities but directly from a dataset of records of inst a ces in the domain. This is particularly suited to applications which have larg e, possibly noisy, datasets, for which knowledge engineering would be difficult. Direct u se of data requires a different style of argument, which has many affinities to ca se based reasoning. Following motivation and a discussion of the requirement of this form of reasoning, we present the protocol and illustrate its use with a case stu dy. We conclude with a discussion of some significant issues highlighted by our app oach.	knowledge engineering;stu-i	Maya Wardeh;Trevor J. M. Bench-Capon;Frans Coenen	2008			knowledge base;biological classification;computer science;knowledge management;artificial intelligence;knowledge engineering;data mining;algorithm	Visualization	-16.11340239654216	3.107095949989569	149773
be2ce15722faa16ebfe81c92c6fe56eb2b2314de	can self-organisation emerge through dynamic neural fields computation?	self organisation;dynamic neural fields;cognitive systems;dynamic neural ﬁelds;self organization	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	a-normal form;algorithm;autonomous robot;born–haber cycle;bump mapping;cognition;computation;computer science;distance (graph theory);dynamical system;emergence;experiment;francis;lateral computing;lateral thinking;modal logic;nist hash function competition;nl (complexity);nonlinear system;online and offline;primary source;recurrent neural network;relevance;requirement;self-organization;simulation;situated;teuvo kohonen;thermal copper pillar bump;thierry coquand	Lucian Alecu;Hervé Frezza-Buet;Frédéric Alexandre	2011	Connect. Sci.	10.1080/09540091.2010.526194	self-organization;simulation;computer science;artificial intelligence;machine learning	Robotics	-15.05342498005067	-7.044986169071126	150317
329376fddd022f222cb2b2c749b5cd7d51024a27	the irreducible complexity of objectivity	philosophy of science;complexite;interaction;experience;epistemologie;neutralite;complexity;individu;objectivite;value;individual;processus;epistemology;valeur;neutrality;evidence;social;process;subjectivite;subjectivity;objectivity	The terms ``objectivity'' and ``objective'' are among the mostused yet ill-defined terms in the philosophy of science and epistemology. Common to all thevarious usages is the rhetorical force of ``I endorse this and you should too'', orto put it more mildly, that one should trust the outcome of the objectivity-producing process.The persuasive endorsement and call to trust provide some conceptual coherenceto objectivity, but the reference to objectivity is hopefully not merely an attemptat persuasive endorsement. What, in addition to epistemological endorsement,does objectivity carry with it? Drawing on recent historical and philosophical work,I articulate eight operationally accessible and distinct senses of objectivity.While there are links among these senses, providing cohesion to the concept, I argue thatnone of the eight senses is strictly reducible to the others, giving objectivity itsirreducible complexity.	irreducibility;irreducible complexity;objectivity/db	Heather Douglas	2004	Synthese	10.1023/B:SYNT.0000016451.18182.91	philosophy of science;complexity;interaction;philosophy;epistemology;objectivity;mathematics;subjectivity;social;process	AI	-12.538988927755748	3.778224629678299	150346
6b542d9c9478e2f5bdddb6e72487e39dead0d21c	development and comparison of aerial photograph aided visualization pipelines for lidar datasets	nonparametric statistics;mining;experiment design;visualization;feature extraction;lidar	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	aerial photography;francis;pipeline (computing);primary source	Suddhasheel Ghosh;Bharat Lohani	2015	Int. J. Digital Earth	10.1080/17538947.2014.897386	nonparametric statistics;lidar;computer vision;mining;visualization;geography;feature extraction;computer science;data mining;design of experiments;statistics;remote sensing	Robotics	-14.869615461061377	-6.190402680894204	150817
728fc5a5d55c9ddb017f9174ffe8c788d52afc15	preference trees: a language for representing and reasoning about qualitative preferences	knowledge representation and reasoning;preferences	We introduce a novel qualitative preference representation language, preference trees, or P-trees. We show that the language is intuitive to specify preferences over combinatorial domains and it extends existing preference formalisms such as LP-trees, ASO-rules and possibilistic logic. We study reasoning problems with P-trees and obtain computational complexity results.	app store optimization;cp/m;computational complexity theory;decision tree;pareto efficiency;preference learning	Xudong Liu;Miroslaw Truszczynski	2014			natural language processing;knowledge representation and reasoning;preference learning;computer science;artificial intelligence	AI	-8.498229100598653	2.668311887693279	150846
dcae2027e1db74b094923b6293b31e6ccc1ce46c	neutrally stable outcomes in cheap-talk coordination games	pareto efficiency;nash equilibrium;evolutionary stability;coordination game;limit set;journal of economic literature;cheap talk	This paper examines equilibrium and stability in symmetric two-player cheaptalk games and specifically characterizes the set of neutrally stable outcomes in cheap-talk 2 × 2 coordination games. With a finite message set, this set is finite. As the number of messages goes to infinity, the set expands toward a countable limit. The Pareto efficient Nash equilibrium payoff is its unique cluster point. By contrast, the corresponding limit set of strategically stable outcomes is dense in the interval spanned by the Nash equilibrium payoffs of the underlying game. Journal of Economic Literature Classification Number: C70. © 2000 Academic Press	nash equilibrium;pareto efficiency	Abhijit Banerjee;Jörgen W. Weibull	2000	Games and Economic Behavior	10.1006/game.1999.0756	limit set;cheap talk;epsilon-equilibrium;mathematical optimization;best response;coordination game;economics;repeated game;mathematics;microeconomics;mathematical economics;welfare economics;equilibrium selection;nash equilibrium	ECom	-4.766507472483353	-0.9797204987649913	150864
9c1f7ad3f303b7f8a2e907f1f14204aeda4a06ef	strategy-proof rules for two public goods: double median rules	two public goods;strategy proof;single peaked preferences;double median rules	We consider the problem of selecting the locations of two (identical) public goods on an interval. Each agent has preferences over pairs of locations, which are induced from single-peaked rankings over single locations: each agent compares pairs of locations by comparing the location he ranks higher in each pair. We introduce a class of “double median rules” and characterize it by means of continuity, anonymity, strategy-proofness, and users only. To each pair of parameter sets, each set in the pair consisting of (n + 1) parameters, is associated a rule in the class. It is the rule that selects, for each preference profile, the medians of the peaks and the parameters belonging to each set in the pair. We identify the subclasses of the double median rules satisfying group strategy-proofness, weak efficiency, and double unanimity (or efficiency), respectively. We also discuss the classes of “multiple median rules” and “non-anonymous double median rules”.	scott continuity	Eun Jeong Heo	2013	Social Choice and Welfare	10.1007/s00355-012-0713-z	mathematical optimization;mathematics;welfare economics;statistics	ECom	-6.731745823047942	-2.6903402214742718	150982
4a6aa045bbd6e9ba1d9c7bd9fff8a274d630b56d	decision-making under uncertainty processed by lattice-valued possibilistic measures		The notion and theory of statistical decision functions are re-considered and modified to the case when the uncertainties in question are quantified and processed using lattice-valued possibilistic measures, so emphasizing rather the qualitative than the quantitative properties of the resulting possibilistic decision functions. Possibilistic variants of both the minimax (the worst-case) and the Bayesian optimization principles are introduced and analyzed.		Ivan Kramosil	2006	Kybernetika		fuzzy logic;bayesian optimization;mathematical optimization;mathematics;lattice (order);decision support system;minimax;complete lattice;decision theory;possibility theory	Robotics	-9.300031453419802	-0.2755432723450178	151404
47a3b2d682650106b3ddf1f2d357d861c91a6809	medical error detection based on insurance claims			error detection and correction	Huanmei Wu;Adam Thomas Jones;Ekta Agrawal	2011			computer science;data mining;error detection and correction	Robotics	-15.764139243063008	-5.125530046330456	151513
e44a902cee7a4c81ec6af009fcba8606f46726ba	social choice theory, belief merging, and strategy-proofness	bepress selected works;era2012;intelligent agent;knowledge integration;belief merging;social choice theory	Intelligent agents have to be able to merge informational inputs received from different sources in a coherent and rational way. Several proposals have been made for information merging in which it is possible to encode the preferences of sources [5,4,19,24,25,1]. Information merging has much in common with social choice theory, which aims to define operations reflecting the preferences of a society from the individual preferences of the members of the society. Given this connection, frameworks for information merging should provide satisfactory resolutions of problems raised in social choice theory. We investigate the link between the merging of epistemic states and some results in social choice theory. This is achieved by providing a consistent set of properties-akin to those used in Arrow's theorem [2]-for merging. It is shown that in this framework there is no Arrow-like impossibility result. By extending this to a consistent framework which includes properties corresponding to the notion of being strategy-proof, we show that results due to Gibbard and Satterthwaite [13,31,32] and others [6,3] do not hold in merging frameworks.		Samir Chopra;Aditya K. Ghose;Thomas Andreas Meyer	2006	Information Fusion	10.1016/j.inffus.2005.05.003	social choice theory;knowledge integration;computer science;artificial intelligence;mathematics;management science;intelligent agent	NLP	-12.660786396273679	-0.9008312091695831	151543
f8e2d652c93fb1283c04797d642b39f3cd997b24	an agent-based global economy	selected works;agent based;global economy;computational science and engineering;bepress	Many people are buying music CDs over the Web, but with multiple retail sites offering the same CDs, how do you decide which retailer to buy from? Typically, you visit a few sites and choose the one with the lowest price. Shopbotsshopping agents that automatically search the Internet to obtain information about prices and other attributes of goods and servicesare ideal helpers for such a task.1 The better ones can visit hundreds of sites, giving price-conscious consumers a powerful tool that could work to the detriment of some retailers.	global serializability	Michael N. Huhns	2000	IEEE Internet Computing	10.1109/MIC.2000.10029	computer science;computational science and engineering;world wide web;computer security	Visualization	-7.860013491932313	-8.77964770730588	151655
213e4aeeb33091e981217cf84a8f560e7c32503b	an extended automata to efficiently match counting constraints patterns	compression algorithm;memory management;counting constraints patterns;network security;automata doped fiber amplifiers construction industry pattern matching memory management intrusion detection classification algorithms;construction industry;compressed algorithm;intrusion detection;snortmemory space requirement extended automata counting constraints patterns regular expression matching network security deterministic finite automata bitmap shift finite automata;deep packet inspection;automata;regular expressions;pattern matching;deterministic finite automata;regular expression matching;security of data finite automata pattern matching;classification algorithms;bitmap shift finite automata;finite automata;dfa;extended automata;regular expressions dfa compressed algorithm deep packet inspection;security of data;snortmemory space requirement;regular expression;doped fiber amplifiers	Regular expression matching is an important application in the area of network security. In this paper, we analyse the limitation of the existing methods and show the classification of counting constraints patterns. We then present a new representation for deterministic finite automata, called Bitmap Shift Finite Automata (Bs-FA), which introduces condition function and bitmap structure. Bs-FA handles counting constraints patterns effectively and considerably reduces memory space requirement of states by evaluating this method on signature sets used in Snort. Overall, for all signature sets and compression methods evaluated of counting constraints patterns, Bs-FA offers the best memory reduction.	automata theory;bitmap;dspace;deterministic finite automaton;finite-state machine;intrusion detection system;network security;regular expression;requirement;rewrite (programming);rewriting;snort	Yao Yuan;Liu Peng;Zhang Zheng;Wang Hui;Gou Cheng Cheng	2010	2010 International Conference on E-Business and E-Government	10.1109/ICEE.2010.510	discrete mathematics;computer science;theoretical computer science;network security;machine learning;regular expression;algorithm	DB	-18.360768176140624	-0.8546978204600105	151795
a403c23b00c4ca52f374ac01f4c0e254fc9ca138	shaping digital earth applications through open innovation - setting the scene for a digital earth living lab	living laboratories;governance methodologies;development;virtual globes;open innovation;info eu repo semantics article;stakeholder engagement;sustainable development	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;living lab;nl (complexity);noise shaping;open innovation;otto h. schade;primary source;virtual world	Sven Schade;Carlos Granell	2014	Int. J. Digital Earth	10.1080/17538947.2013.804600	simulation;geography;knowledge management;open innovation;sustainable development	Robotics	-15.75177675165342	-6.060458767698135	151856
062b73bd23bd0bce931ec081278b657e6b546925	the marginal operators for games on convex geometries	supermodular game;subject classification 91a12;quasi supermodular games;convex geometry;marginal operators	In this work we study situations in which communication among the players is not complete and it is represented by a family of subsets of the set of players. Although several models of partial cooperation have been proposed, we shall follow a model derived from the work of Faigle and Kern. We define the games on convex geometries and introduce marginal worth vectors and quasi-supermodular games. Furthermore, we analyze some properties of the marginal operators on the space of games on convex geometries.	antimatroid;marginal model;supermodular function	Jesús Mario Bilbao;N. Jiménez;Esperanza A. Lebrón;J. J. López	2006	IGTR	10.1142/S0219198906000837	convex geometry;mathematical optimization;combinatorics;discrete mathematics;mathematics	ECom	-6.207269140287898	-0.21889288932946274	152068
a42a00d98fec62df0023b46a2f6d9df39d995e10	random path to stability in a decentralized market with contracts		For a many-to-many matching model with contracts in which the preferences of all hospitals satisfy substitutability and the preferences of all doctors satisfy substitutability, the law of aggregate demand and q-congruence, we show the existence of a convergent blocking path. In other words, we start from an arbitrary allocation and build a finite sequence of allocations leading to a stable outcome, with the special feature that each allocation can be obtained from the previous one by satisfying a unilateral or a bilateral blocking contract. As a consequence, we prove that the process of allowing randomly selected blocks to be satisfied eventually leads to a stable outcome. This explains the fact that some markets with contracts reach stable assignments by means of decentralized decisions.		Beatriz Millán;Eliana Pepa Risma	2018	Social Choice and Welfare	10.1007/s00355-018-1108-6	welfare economics;economics;aggregate demand	AI	-4.998641823110343	-3.8261029980477166	152197
9aa2c3f8df29b0b5ef0bfd7501c3abe2db883afc	reasoning beyond predictive validity: the role of plausibility in decision-supporting social simulation		Practical and philosophical arguments speak against predictability in social systems, and consequently against the predictive validity of social simulations. This deficit is tolerable for description, exploration, and theory construction but serious for all kinds of decision support. The value of plausibility, however, as the most obvious substitute for predictive validity, is disputed for good reasons: it lacks the solid grounds of objectivity. Hence, on the one hand, plausibility seems to be in contradiction to scientific inquiry in general. On the other hand, plausibility is paramount and ubiquitous in practical decision making. The article redefines plausibility in order to render it more precise than colloquial usage. Based on the experiences with military applications different lines of reasoning with plausible trajectories based on computer simulation are analyzed. It is argued that the rationale behind such reasoning is often substantially stronger than a mere subjective expert opinion can be.	computer simulation;decision problem;decision support system;decision theory;definition;design rationale;objectivity/db;plausibility structure;relevance;social simulation;social system	Marko A. Hofmann	2015	2015 Winter Simulation Conference (WSC)		computer simulation;simulation;cognition;visualization;computer science;engineering;knowledge management;artificial intelligence;trajectory;mathematics;knowledge;business intelligence	AI	-14.365555828532328	1.7792916669239882	152220
5c8b38034539787384ea8e235e4dd705a8e83fb1	the telegraph, espionage, and cryptology in nineteenth century iran	great game;garbles;nineteenth century;codes;telegraph;iran	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	cryptography;francis;primary source	Michael Rubin	2001	Cryptologia	10.1080/0161-110191889752	computer science;mathematics;telegraphy;code;computer security	Robotics	-15.356766632535304	-5.511835835798457	152386
905fc4c0fa5fd819b6390f1b17bfc77d80522aed	uncertainty analysis of rule-based expert systems with dempster-shafer mass assignments	rule based;uncertainty analysis;dempster shafer;expert system	Abstract#R##N##R##N#This article extends Dempster-Shafer Theory (DST) mass probability assignments to Boolean algebra and considers how such probabilities can propagate through a system of Boolean equations, which form the basis for both rule-based expert systems and fault trees. the advantage of DST mass assignments over classical probability methods is the ability to accommodate when necessary uncommitted probability belief. This paper also examines rules in the context of a probabilistic logic, where a given rule itself may be true with some probability in the interval [0,1]. When expert system knowledge bases contain rules which may not always hold, or rules that occasionally must be operated upon with imprecise information, the DST mass assignment formalism is shown to be a suitable methodology for calculating probability assignments throughout the system.	expert system;logic programming	Michael A. S. Guth	1988	Int. J. Intell. Syst.	10.1002/int.4550030203	imprecise probability;uncertainty analysis;dempster–shafer theory;computer science;artificial intelligence;machine learning;data mining;mathematics;expert system;algorithm;statistics	ML	-16.160784141805625	0.38899903887358334	152615
ffa314175c4e547b7d6c8a80b5edf18485fc814d	no regrets about no-regret	minimisation;minimization;adaptability;adaptabilite;multiagent system;regret minimization;game theory;securite;videojuego;teoria juego;minimizacion;intelligence artificielle;theorie jeu;video game;adaptabilidad;jeu video;agent intelligent;safety;intelligent agent;artificial intelligence;agente inteligente;inteligencia artificial;jeu ordinateur;computer games;sistema multiagente;multi agent learning;seguridad;systeme multiagent	No-regret is described as one framework that game theorists and computer scientists have converged upon for designing and evaluating multi-agent learning algorithms. However, Shoham, Powers, and Grenager also point out that the framework has serious deficiencies, such as behaving sub-optimally against certain reactive opponents. But all is not lost. With some simple modifications, regret-minimizing algorithms can perform in many of the ways we wish multi-agent learning algorithms to perform, providing safety and adaptability against reactive opponents. We argue that the research community should have no regrets about no-regret methods. © 2007 Elsevier B.V. All rights reserved.	algorithm;computer scientist;machine learning;multi-agent system;regret (decision theory)	Yu-Han Chang	2007	Artif. Intell.	10.1016/j.artint.2006.12.007	game theory;minimisation;adaptability;simulation;computer science;artificial intelligence;operations research;intelligent agent	AI	-11.673681155077553	-7.387027725471509	152652
a5b85fa2a9db27528e127e925d72a3fb76db6e90	non-preemptive preferences in multi-agent task processing	task execution;multi-agent task processing;object creation;task type;best solution;non-preemptive preferences;highest number;query processing;non-preemptive allocation;consumable resource;multi-agent environment;cooperative design;multi agent system;distributed processing;multi agent systems;resource allocation	Preferences are assigned on consumable resources during task execution. We have presented here a scheme for handling such preferences in nonpreemptive allocations for two types of task in a multiagent environment with the object of finding a solution that preserves the highest number of preferences. The task types are: distributed query processing (such as a travel plan) and object creation (such as cooperative design). In both cases, convergence can be guaranteed because of nonpreemption, but the quickest path for the best solution is harder to identify.	agent-based model;constraint (mathematics);consumability;database;mathematical optimization;multi-agent system;nonlinear system;obedience (human behavior);object lifetime	S. Misbah Deen	2004	Proceedings. 15th International Workshop on Database and Expert Systems Applications, 2004.	10.1109/DEXA.2004.1333474	query optimization;real-time computing;resource allocation;computer science;artificial intelligence;multi-agent system;database;distributed computing	AI	-13.048859639452038	-2.646225978401968	152749
111db3fb33faea774ee968e5a6fc2064e6e1e302	building the fundamentals of granular computing: a principle of justifiable granularity	granular computing;fuzzy sets;intervals;information granules;justifiable granularity;design of granules	The study introduces and discusses a principle of justifiable granularity, which supports a coherent way of designing information granules in presence of experimental evidence (either of numerical or granular character). The term ''justifiable'' pertains to the construction of the information granule, which is formed in such a way that it is (a) highly legitimate (justified) in light of the experimental evidence, and (b) specific enough meaning it comes with a well-articulated semantics (meaning). The design process associates with a well-defined optimization problem with the two requirements of experimental justification and specificity. A series of experiments is provided as well as a number of constructs carried for various formalisms of information granules (intervals, fuzzy sets, rough sets, and shadowed sets) are discussed as well.	granular computing	Witold Pedrycz;Wladyslaw Homenda	2013	Appl. Soft Comput.	10.1016/j.asoc.2013.06.017	interval;granular computing;computer science;artificial intelligence;machine learning;mathematics;fuzzy set;algorithm	Logic	-15.62709196073541	1.9331953881311312	152816
b3a07aac7d3a3ec9d54250b9d0e712f2de236378	a method of rule match conflict resolution for product configuration in manufacturing	evaluation function;manufacture;input and output matrices;conflict resolution;control strategy;product configuration;rule based reasoning	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	algorithm;francis;failure;input/output;knowledge representation and reasoning;knowledge-based configuration;logic programming;mathematical optimization;personalization;primary source;programming paradigm;recursion (computer science);relevance;requirement	Niya Li;Dayou Liu;Jian Zhang;Yuhua Chen	2009	Int. J. Computer Integrated Manufacturing	10.1080/09511920802216350	conflict resolution strategy;computer science;engineering;artificial intelligence;operations management;conflict resolution;evaluation function;management science;manufacturing;business rule;operations research	Robotics	-15.154518847251564	-5.032166601939894	153087
1349c24c5a8a6f495ae8f9ef7b796f4a79b23e8b	self-organizing systems	psychology	This paper applies basic economic principles which have been developed in financial markets to a future smart grid scenario. Our method allows for autonomous bidding for electricity units to create an emerging market price for electricity. We start with replicating the popular Zero-Intelligence-Plus algorithm and setting it in a electricity supplier-consumer scenario. We identify significant weaknesses of applying this in an electricity market especially when intermittent sources of energy are present or when the supplier to consumer ratio is very small. A new algorithm (ZIP-260) is proposed which includes a measure of fairness based on minimising the deviation across all un-matched demand for a given period. This approach means that no consumer in the system is constantly experiencing an electricity supply deficit. We show and explain how market conditions can lead to collective bargaining of consumers and monopolistic behaviour of suppliers and conclude with observations on automated trading for smart grids.	algorithm;algorithmic trading;autonomous robot;fairness measure;uninterruptible power supply	Wilfried Elmenreich Falko Dressler Vittorio Loreto	2013		10.1007/978-3-642-54140-7	knowledge management;self-organization;computer science	AI	-6.4619295329678055	-8.793430188354934	153148
a5b92dac8f06dd6d485882559b7496bee155fcd0	an interest and belief-based model of explicit communication		The paper outlines an inferential model of explicit communication. The first section presents the main notions involved in the model, that is, the speaker’s intended meaning and addressee’s intended meaning. The first notion is centred on the speaker’s interest in situation: a speaker intends the meaning of an utterance on the basis of a partial order of preferences with respect to a set of contextually plausible meanings. The second notion is centred on the addressee’s communicative inference, which is based on the addressee’s belief about the speaker’s interest and on the linguistic form of a sentence. In the following sections, the paper presents the phenomenon of semantic underdeterminacy, introduces the preferences partial order, and the communicative inferences. The paper takes the notion of interest from cognitive social theory, the notion of meaning from semantics, the notion of explicit meaning from pragmatics, and uses the notion of communicative inference for explicit meaning in partial accordance with Relevant Theory. Finally, the paper discusses some examples taken from the pragmatist literature.	inferential programming	Marco Cruciani	2015			natural language processing;semantics;inference;artificial intelligence;pragmatics;social cognitive theory;underdeterminacy;mathematics;utterance;sentence;phenomenon	NLP	-14.987517790238808	3.999906750121309	153184
9700dacd61956351cce87e96a0b50f1e28be5da0	european needs and attitudes towards information security	information security;proprietary algorithms;satisfiability;western europe;regulation	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;information security;primary source	Richard I. Polis	1988	Cryptologia	10.1080/0161-118891862981	regulation;actuarial science;computer science;information security;computer security;satisfiability	Mobile	-15.38719521536666	-5.58898389587014	153215
862fa0feb54b7b45957e2776a6030d25f7893f70	memory-based hardware-accelerated system for high-speed human detection	euclidean distance coprocessor;hardware software co design;human detection;multi prototype learning system	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	algorithm;artificial neural network;benchmark (computing);ccir system a;cmos;computer science;electrical engineering;francis;hardware acceleration;image processing;institute of electronics, information and communication engineers;machine learning;nl (complexity);pipeline (computing);primary source;prototype;real-time clock;real-time transcription;semiconductor device;smart card;support vector machine;very-large-scale integration	Indra Bagus Wicaksono;Fengwei An;Hans Jürgen Mattausch	2014	Advanced Robotics	10.1080/01691864.2013.870494	real-time computing;simulation;computer science;artificial intelligence;theoretical computer science	Robotics	-14.706805829573247	-4.866947087186764	153323
b4aa1b329e4f706f33107ba7d2b39f1cbebd7f3a	ternary dialectical informatics		Modern binary informatics with its so-called ―classical‖ two-valued logic admits to create an artificial intellect and suppresses the natural intellect of students and other thinking people. Logic that based on dogmatic law of the excluded middle is incompatible with dialectical principle of opposition coexistence. Such logic is deprived of fundamental logical relation – the content consequence, and then cannot reach a conclusion. Aristotle‘s syllogistics includes the content consequence as common affirmative premise ―All x are y‖. However, binarity misinterprets it as a paradoxical material implication that is not a relation at all. Lewis Carroll‘s ―Symbolic logic‖ correctly represented syllogistics, but it is not intelligible for modern binary logic. Our paper reveals the essence of content consequence; it explains Carroll‘s intentional judgments and syllogistic relations submitted to opposite coexistence.	artificial intelligence;coexist (image);informatics;intellect	Nikolay Petrovich Brusentsov;Julia Sergeevna Vladimirova	2006		10.1007/978-3-642-22816-2_11	epistemology;artificial intelligence;mathematics;algorithm;philosophy of logic	AI	-12.712963415980367	3.189639979406826	153530
d72de6a4d3349847c6037ff775975b62a78e45b7	delegating via quitting games		Delegation allows an agent to request that another agent completes a task. In many situations the task may be delegated onwards, and this process can repeat until it is eventually, successfully or unsuccessfully, performed. We consider policies to guide an agent in choosing who to delegate to when such recursive interactions are possible. These policies, based on quitting games and multi-armed bandits, were empirically tested for effectiveness. Our results indicate that the quitting game based policies outperform those which do not explicitly account for the recursive nature of delegation.		Juan Afanador;Nir Oren;Murilo S. Baptista	2018	CoRR		machine learning;computer science;artificial intelligence;recursion;delegation;delegate	AI	-10.243960957406156	-5.240920321994636	153600
526bab6e47f67effe3b8a1608a4416fae695205a	earth sciences systems		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Karl H. Wolf	2008	Int. J. General Systems	10.1080/03081070802203934	earth system science	Robotics	-15.045425272341445	-5.8038034452976435	153637
d56ff0f238b28ceb7a1cf9e8fed079e5e914c7d4	redefinition of belief distorted nash equilibria for the environment of dynamic games with probabilistic beliefs		In this paper, a new concept of equilibrium in dynamic games with incomplete or distorted information is introduced. In the games considered, players have incomplete information about crucial aspects of the game and formulate beliefs about the probabilities of various future scenarios. The concept of belief distorted Nash equilibrium combines optimization based on given beliefs and self-verification of those beliefs. Existence and equivalence theorems are proven, and this concept is compared to existing ones. Theoretical results are illustrated using several examples: extracting a common renewable resource, a large minority game, and a repeated Prisoner’s Dilemma.	mathematical optimization;nash equilibrium;prisoner's dilemma;turing completeness	Agnieszka Wiszniewska-Matyszkiel	2017	J. Optimization Theory and Applications	10.1007/s10957-016-1034-7	price of stability;game theory;epsilon-equilibrium;mathematical optimization;best response;sequential equilibrium;folk theorem;repeated game;mathematical economics;equilibrium selection;solution concept	AI	-8.095291401131032	-1.5410620248046611	153857
69bb9b741d3193f5c82871862c65e601c4261510	neural networks for identifying civil pilot's operation sequences	neural networks;operation behavior;coding system;human error	"""""""Human Error"""", as we all know, is inevitable during the flight process of civil aircraft. It is one of the most significant reasons for civil aircraft accidents and incidents. Therefore, to identify and avoid """"Human Error"""" is becoming more and more urgent.#R##N##R##N#In order to restrict the influence of """"Human Error"""", the wrong sequence of civil pilot's operation must be detected and a warning should be provided for pilot or intelligent action to correct the wrong sequence of operations.#R##N##R##N#A set of effective behavior coding system is developed for expressing the pilot's operations. Pilot's operation behaviors can be quantized and operation sequences can be coded. And the set of effective pilot's behavior coding system plays an important role in reducing the probability of flight accidents caused by """"Human Error"""".#R##N##R##N#For identifying whether the pilot's operation sequence is right, a database of codes of pilot's operation sequences should be built. By comparing with the codes in the database, a wrong operation sequence can be detected. Generally speaking, the database containing codes of all possible correct and wrong operation sequences is difficult to set up. As a matter of fact, the database we can develop is just a part of all possible codes of operation sequences. Therefore, those naturally correct operation sequences but not in the database may be detected as wrong ones by comparing with the correct codes in the database. This paper adopts neural networks to identify any codes of operation sequences (in database and not in database) accurately. The incomplete database is trained by neural networks to find the rule for identifying whether a specific operation sequence is correct. If the specific pilot's operation sequence disobeys the rule, a warning will be provided for pilot to rectify the operation, which reduces the probability of accidents caused by """"Human Error"""" and realizes the intelligent identifying function."""	neural networks	Zhuoyuan Jiang;Qin Lu;Yuandong Liang;Bin Chen	2014		10.1007/978-3-319-07863-2_24	simulation;engineering;data mining;computer security	Theory	-18.183942795382606	-1.2444715464115952	154002
74c807bdf4d369be16e0b54d128062323d7a2911	nash implementation in private good economies with single-plateaued preferences and in matching problems	matching problems;nash implementation;private good economies with single plateaued preferences	In this paper we explore the problem of Nash implementation providing two new properties called I-monotonicity and I-weak no-veto power. These properties form together with unanimity a new sufficient condition for a social choice correspondence to be implementable in Nash equilibria with at least three agents. As applications, we prove that: (1) In private good economies with single-plateaued preferences, each solution of the problem of fair division that has a full range is Nash implementable if and only if it satisfies Maskin monotonicity, (2) In matching problems, Maskin monotonicity is a necessary and sufficient condition for implementation.	nash equilibrium	Ahmed Doghmi;Abderrahmane Ziad	2015	Mathematical Social Sciences	10.1016/j.mathsocsci.2014.11.002	epsilon-equilibrium;best response;economics;microeconomics;mathematical economics;welfare economics	Theory	-5.662181372566077	-1.8905858258938333	154048
212de6807897ff646646dab2c4c3c9e534072cef	scenario-based decision-making for power systems investment planning		The optimization of power systems involves complex uncertainties, such as technological progress, political context, geopolitical constraints. Negotiations at COP21 are compl icated by the huge number of scenarios that various people want to consider; these scenarios correspond to many uncertaint ies. These uncertainties are difficult to modelize as probabilit ies, due to the lack of data for future technologies and due to partialy adversarial geopolitical decision makers. Tools for such d ifficult decision making problems include Wald and Savage criteria, possibilistic reasoning and Nash equilibria. We investiga te the rationale behind the use of a two-player Nash equilibrium approach in such a difficult context; we show that the computati onal cost is indeed smaller than for simpler criteria. Moreover, it naturally provides a selection of decisions and scenarios, and it has a natural interpretation in the sense that Nature does not make decisions taking into account our own decisions. Th e algorithm naturally provides a matrix of results, namely the matrix of outcomes in the most interesting decisions and forthe most critical scenarios. These decisions and scenarios are also equipped with a ranking. I. I NTRODUCTION: DECISION MAKING IN UNCERTAIN	algorithm;design rationale;ibm power systems;mathematical optimization;nash equilibrium;the matrix	Jialin Liu;Olivier Teytaud	2016	CoRR		simulation;management science;welfare economics	ML	-9.598143269364783	-2.8574677495189107	154135
935eb4b7e4dcf0d6ba29050498821355f84d82f8	decision-support for optimizing supply chain formation based on cset model	electronic trading;make to order;decision support;make to order supply chain model;electronic commerce;cost function;raw materials;software agent;simulation;dynamic supply chain;resource management;software agent supply chain formation optimization problem decision support e commence e marketplace online trade dynamic supply chain cset model make to order supply chain model profit maximization java based simulator supply chain planning;simulation cset model automated negotiation supply chain;supply chains supply chain management resource management java companies visualization computational modeling cost function optimization methods business;companies;supply chains;software agents;online trade;local knowledge;visualization;computational modeling;internet;cset model;e marketplace;business;production facilities;manufacturing;supply chain planning;supply chain formation optimization problem;supply chain management decision making electronic commerce electronic trading internet profitability software agents;fabrics;supply chain;java based simulator;profitability;e commence;supply chain management;profit maximization;automated negotiation;model simulation;java;optimization methods	Supply chain formation problem is one of the important research topics in e-Commence. In an e-Marketplace where buyers and sellers meet and trade online, dynamic supply chains can be formed among them by mediating agents. SET and CSET are two typical make-to-order supply chain models. CSET represents a scenario that has a central authority in charge of the formation, management and dissolution of a sup-ply chain. The principal authority selects the partners under certain principles which may either aim for maximizing profits of the whole supply chain or for ensuring every partner to re-ceive a job for communal prosperity. In SET, every supply chain partner uses local knowledge to compete for jobs at each supply chain level. We have implemented a Java-based simu-lator for simulating the process of dynamic supply chain for-mation. The simulator can operate in both modes whose results may be useful in decision-support in supply chain planning.	automated planning and scheduling;benchmark (computing);chart;e-commerce;intelligent agent;interaction;java;job stream;optimizing compiler;pareto efficiency;simulation;software agent;supply chain attack	Yang Hang;Simon Fong;Zhuang Yan	2009	2009 IEEE Conference on Commerce and Enterprise Computing	10.1109/CEC.2009.51	supply chain management;service management;computer science;resource management;software agent;supply chain	AI	-8.662977986442193	-7.2756910613152765	154185
0886e182f8e06f14378111d1c996bdf46a2fd1e2	coevolutionary opinion formation games	opinions;price of anarchy;games	We present game-theoretic models of opinion formation in social networks where opinions themselves co-evolve with friendships. In these models, nodes form their opinions by maximizing agreements with friends weighted by the strength of the relationships, which in turn depend on difference in opinion with the respective friends. We define a social cost of this process by generalizing recent work of Bindel et al., FOCS 2011. We tightly bound the price of anarchy of the resulting dynamics via local smoothness arguments, and characterize it as a function of how much nodes value their own (intrinsic) opinion, as well as how strongly they weigh links to friends with whom they agree more.	anarchy;game theory;social network;symposium on foundations of computer science	Kshipra Bhawalkar;Sreenivas Gollapudi;Kamesh Munagala	2013		10.1145/2488608.2488615	games;opinion;mathematics;price of anarchy	Theory	-5.502094387235765	0.18620118024938273	154204
89cac81a9d68b502d8210ca7231ee2eaf5f4b02c	approximate reasoning about generalized conditional independence with complete random variables		The implication problem of conditional statements about the independence of finitely many sets of random variables is studied in the presence of controlled uncertainty. Uncertainty refers to the possibility of missing data. As a control mechanism random variables can be de- clared complete, in which case data on these random variables cannot be missing. While the implication of conditional independence statements is not axiomatizable, a finite Horn axiomatization is established for the expressive class of saturated conditional independence statements under controlled uncertainty. Complete random variables allow us to balance the expressivity of sets of saturated statements with the efficiency of de- ciding their implication. This ability can soundly approximate reasoning in the absence of missing data. Delobel's class of full first-order hierarchi- cal database decompositions are generalized to the presence of controlled uncertainty, and their implication problem shown to be equivalent to that of saturated conditional independence.		Sebastian Link	2013		10.1007/978-3-642-40381-1_21	econometrics;discrete mathematics;conditional variance;mathematics;statistics	AI	-9.971102291378665	0.7857215413206533	154251
88fd9d513501e3d9995ba8f0ef209a5096ae4612	private agenda and re-election incentives		Consider a politician who has to take several decisions during his term in office. For each decision, the politician faces a trade-off between taking what he believes to be the right choice, thus increasing his chances of re-election, and taking the decision that increases his private gain but is likely to decrease his chances of re-election. Within this setting, we consider how different factors affect the incentives of the politician to take the right choice. In our results we find, among others, that the behavior such that the politician follows his private interests in the first periods of his term in office and then tries to please the electorate when elections approach is optimal if and only if he has either very high or very low decision making skills. Furthermore, we find that it is not true that a more demanding electorate produces more socially motivated politicians. JEL Classification: D72, D81.		Javier Rivas	2016	Social Choice and Welfare	10.1007/s00355-015-0941-0	public relations;actuarial science;public economics	ECom	-5.718921835909433	-5.812254667461614	154287
bc2a94e56ac40644c89c39dc7dc531c8165f2e54	probability as a measure of information added	probability;comparative probability;journal article;koopman;cox;information	"""My aim is to present a new interpretation of a calculus of conditional probability, to be understood as governing a rescaling of measures of information added. My aim is to present a new interpretation of a calculus of conditional probability, to be understood as governing a rescaling of measures of information added. In information theory, one starts with a statistical probability distribution P over signal elements and takes − log P as a measure of the information (in the sense of """" surprise value """") of the signal elements. Similarly, Carnap and Bar-Hillel used the same measure, but based on an inductive probability, as a measure of semantic information. My aim is to present a new interpretation of a calculus of conditional probability, to be understood as governing a rescaling of measures of information added. In information theory, one starts with a statistical probability distribution P over signal elements and takes − log P as a measure of the information (in the sense of """" surprise value """") of the signal elements. Similarly, Carnap and Bar-Hillel used the same measure, but based on an inductive probability, as a measure of semantic information. I want to turn this way of conceiving the relationship beween probability and information on its head. I plan to start from qualitative considerations on the information added by a proposition α to a body of propositions Γ, and hence to obtain quantitative measures of information. We find that there are at least two viable notions of information-added: one goes by the novelty value of the added information, the other, very roughly, by the (weighted) proportion of consequences left open by the body of propositions that the added proposition rules out—this is information-added as an additional resource in drawing consequences. Aim and Structure We find that there are at least two viable notions of information-added: one goes by the novelty value of the added information, the other, very roughly, by the (weighted) proportion of consequences left open by the body of propositions that the added proposition rules out—this is information-added as an additional resource in drawing consequences. Making the sort of plenitude assumptions common in the area of representation theorems, we find that any precise measure of information-added as novelty value can be rescaled as a probability-like function and any measure of information-added as additional resource rescales as a unique Popper function. (What follows borrows heavily from the …"""	agi-plan;inductive probability;information theory	Peter Milne	2012	Journal of Logic, Language and Information	10.1007/s10849-011-9142-0	information;conditional probability;probability measure;epistemology;computer science;artificial intelligence;regular conditional probability;probability;mathematics;conditional mutual information;algorithm;statistics	ML	-11.287594850498833	1.936139474928226	154336
60704a00913ac51f6a303d84446d2d5c1cf79f2e	a new monotonic, clone-independent, reversal symmetric, and condorcet-consistent single-winner election method	pareto efficiency;public interest;satisfiability	In recent years, the Pirate Party of Sweden, the Wikimedia Foundation, the Debian project, the “Software in the Public Interest” project, the Gentoo project, and many other private organizations adopted a new single-winner election method for internal elections and referendums. In this paper, we will introduce this method, demonstrate that it satisfies e.g. resolvability, Condorcet, Schwartz, Smith-IIA, Pareto, reversal symmetry, monotonicity, prudence, and independence of clones and present an O(C^3) algorithm to calculate the winner, where C is the number of alternatives.	algorithm;debian;gentoo linux;pareto efficiency;pirate party	Markus Schulze	2011	Social Choice and Welfare	10.1007/s00355-010-0475-4	economics;schulze method;mathematics;microeconomics;mathematical economics;law;welfare economics;algorithm;statistics;satisfiability	AI	-6.716369609035421	0.5219930444740605	154614
2b2a80f6d9c417722a27d56e490968d2b55a46d2	inter-causal independence and heterogeneous factorization		It is well known that conditional indepen­ dence can be used to factorize a joint prob­ ability into a multiplication of conditional probabilities. This paper proposes a con­ structive definition of intercausal indepen­ dence, which can be used to further factorize a conditional probability. An inference algo­ rithm is developed, which makes use of both conditional independence and intercausal in­ dependence to reduce inference complexity in Bayesian networks.	bayesian network;causal filter;independence day: resurgence;ply (game theory)	Nevin Lianwen Zhang;David L. Poole	1994				AI	-9.36182544157221	2.7998660096245955	154692
203301f4226c6e3e1115cadaf60b1ca3ac925c05	scenario update applied to causal reasoning	causal reasoning;satisfiability;information value;possible worlds	In this paper, we propose to define the update of a scenario (sequence of observations at different time points) by a piece of information (value of a fluent or event occurrence) at a given time point. This operation computes the possible world evolutions (called trajectories) satisfying this piece of information that are the most in accordance with the initial scenario. It enables us to identify the consequences that a modification may involve on the evolution of the world. Updating scenarios allows us to define formally the counterfactual aspect of causation: to check if an event is a cause in a given scenario amounts to update this scenario by the nonoccurrence of this event.	algorithm;causal filter;causality;computation;counterfactual conditional;extrapolation;possible world;semantics (computer science);sequence alignment;software agent;temporal logic	Florence Bannay	2008			causal reasoning;artificial intelligence;value of information;data mining;possible world;algorithm;satisfiability	AI	-18.433327630014098	2.8826347007888993	154754
1a6be9c5b4e4dfa5b9a6b052386d58cee7dbcb1b	integrating hierarchical and analogical planning	analogical planning;integrating hierarchical	Both hierarchical plmming and analogical phmning (:a~l separately reduce the amount of search a planner must perform to find a solution to a partitular problem. We believe that these two methods cazl bc integrated mtd that the combined planner will have ,-m even smaller average search space them either method would produce ahme. In this work: we present our ideas for integrating the two met hods; focusing on the opportunities for beneficial interactions between analogical plazming and hierarchical plamfing.	analogical modeling;automated planning and scheduling;interaction	Billy Harris;Diane J. Cook	1998			computer science;artificial intelligence;machine learning;algorithm	AI	-18.2246391151504	-6.9191467923603565	154866
801c1352510b51dbfdd2741147d3e6265629945b	negotiating with bounded rational agents in environments with incomplete information using an automated agent	modelizacion;multiagent system;automated agent;negociation;informacion incompleta;juego de funciones;bounded rationality;intelligence artificielle;jeu role;bilateral negotiation;modelisation;incomplete information;systeme incertain;negociacion;information incomplete;bargaining;preferencia;artificial intelligence;preference;inteligencia artificial;sistema multiagente;role playing;sistema incierto;modeling;uncertain system;automated negotiation;systeme multiagent	Many tasks in day-to-day life involve interactions among several people. Many of these interactions involve negotiating over a desired outcome. Negotiation in and of itself is not an easy task, and it becomes more complex under conditions of incomplete information. For example, the parties do not know in advance the exact tradeoff of their counterparts between different outcomes. Furthermore information regarding the preferences of counterparts might only be elicited during the negotiation process itself. In this paper we propose a model for an automated negotiation agent capable of negotiating with bounded rational agents under conditions of incomplete information. We test this agent against people in two distinct domains, in order to verify that its model is generic, and thus can be adapted to any domain as long as the negotiators’ preferences can be expressed in additive utilities. Our results indicate that the automated agent reaches more agreements and plays more effectively than its human counterparts. Moreover, in most of the cases, the automated agent achieves significantly better agreements, in terms of individual utility, than the human counterparts playing the same role. © 2007 Elsevier B.V. All rights reserved.	artificial neural network;experiment;genetic algorithm;interaction;make;rational agent;utility functions on indivisible goods;web crawler	Raz Lin;Sarit Kraus;Jonathan Wilkenfeld;James Barry	2008	Artif. Intell.	10.1016/j.artint.2007.09.007	systems modeling;computer science;artificial intelligence;complete information;negotiation;bounded rationality	AI	-9.957758542351257	-6.433372348762461	155257
65070a3eef6502ea3e1ccb682eaa6187ebc466cf	are imperfect reviews helpful in social learning?	tail probability social learning imperfect reviews information cascades agent review cascade asymptotic properties markov analysis review quality martingale analysis;multi agent systems learning artificial intelligence markov processes;markov processes history information theory sociology statistics noise measurement electronic mail	Social learning encompasses situations in which agents attempt to learn from observing the actions of other agents. It is well known that in some cases this can lead to information cascades in which agents blindly follow the actions of others, even though this may not be optimal. Having agents provide reviews in addition to their actions provides one possible way to avoid “bad cascades.” In this paper, we study one such model where agents sequentially decide whether or not to purchase a product, whose true value is either good or bad. If they purchase the item, agents also leave a review, which may be imperfect. Conditioning on the underlying state of the item, we study the impact of such reviews on the asymptotic properties of cascades. For a good underlying state, using Markov analysis we show that depending on the review quality, reviews may in fact increase the probability of a wrong cascade. On the other hand, for a bad underlying state, we use martingale analysis to bound the tail-probability of the time until a correct cascade happens.	feedback;heart rate variability;information cascade;markov chain	Tho Ngoc Le;Vijay G. Subramanian;Randall Berry	2016	2016 IEEE International Symposium on Information Theory (ISIT)	10.1109/ISIT.2016.7541667	computer science;artificial intelligence;data mining;statistics;information cascade	Theory	-8.857092322185682	-4.185576884667972	155271
b9b8d037102e01f654a11ef04a90b220614d9d42	on the likelihood of condorcet's profiles	game theory;majority rule;economic model;satisfiability;economic models;condorcet s profiles	Consider a group of individuals who have to collectively choose an outcome from a finite set of feasible alternatives. A scoring or positional rule is an aggregation procedure where each voter awards a given number of points, wj , to the alternative she ranks in j position in her preference ordering; the outcome chosen is then the alternative that receives the highest number of points. A Condorcet or majority winner is a candidate who obtains more votes than her opponents in any pairwise comparison. Condorcet [4] showed that all positional rules fail to satisfy the majority criterion. Furthermore, he supplied a famous example where all the positional rules select simultaneously the same winner while the majority rule picks another one. Let P ? be the probability of such events in three-candidate elections. We apply the techniques of Merlin, Tataru and Valognes [17] to evaluate P ? for a large population under the Impartial Culture condition. With these assumptions, such a paradox occurs in 1.808 % of the cases.		Vincent Merlin;Monica Tataru;Fabrice Valognes	2002	Social Choice and Welfare	10.1007/s355-002-8332-y	game theory;dodgson's method;economics;voting paradox;economic model;approval voting;mathematics;mathematical economics;ranked pairs;welfare economics;condorcet method;algorithm;condorcet's jury theorem;kemeny–young method	AI	-7.18859984013077	-3.2011032111575632	155402
92f98f3ddeb5e3f0f627c7cb422796ab7451cfc2	towards a fair distribution mechanism for asylum		It has been suggested that the distribution of refugees over host countries can be made more fair or efficient if policy makers take into account not only numbers of refugees to be distributed but also the goodness of the matches between refugees and their possible host countries. There are different ways to design distribution mechanisms that incorporate this practice, which opens up a space for normative considerations. In particular, if the mechanism takes countries’ or refugees’ preferences into account, there may be trade-offs between satisfying their preferences and the number of refugees distributed. This article argues that, in such cases, it is not a reasonable policy to satisfy preferences. Moreover, conditions are given which, if satisfied, prevent the trade-off from occurring. Finally, it is argued that countries should not express preferences over refugees, but rather that priorities for refugees should be imposed, and that fairness beats efficiency in the context of distributing asylum. The framework of matching theory is used to make the arguments precise, but the results are general and relevant for other distribution mechanisms such as the relocations currently in effect in the European Union.	asylum;control theory;fairness measure;matching (graph theory);relocation (computing)	Philippe van Basshuysen	2017	Games	10.3390/g8040041	refugee;economics;normative;welfare economics;relocation;european union	ECom	-5.755390908121112	-3.75053272725463	155690
4ec1953fe59bcc4580772cecdbba9b3c728bf3ec	the cyclical majority problem	voting paradox;simulation;voter s paradox;arrow s paradox;cyclical majority	The problem of the cyclical majority is presented and some new, simulated results for 3, 4, 5, … , 40 issues and 3, 5, 7, … , 37 judges are reported.	majority problem (cellular automaton)	John Pomeranz;Roman L. Weil	1970	Commun. ACM	10.1145/362258.362282	voting paradox;mathematical economics	Theory	-9.043337423060072	-2.269092239582728	155826
eddedd4c60f132933194dcb61b42591cdb9eff62	dealing with external actions in belief causal networks	interventions	Graphical models are efficient and simple ways to represent dependencies between variables. We introduce in this paper the so-called belief causal networks where dependencies are uncertain causal links and where the uncertainty is represented by belief masses. Through these networks, we propose to represent the results of passively observing the spontaneous behavior of the system and also evaluate the effects of external actions. Interventions are very useful for representing causal relations, we propose to compute their effects using a generalization of the “do” operator. Even if the belief chain rule is different from the Bayesian chain rule, we show that the joint distributions of the altered structures to graphically describe interventions are equivalent. This paper also addresses new issues that are arisen when handling interventions: we argue that in real world applications, external manipulations may be imprecise and show that they have a natural encoding under the belief function framework.	causal filter	Imen Boukhris;Zied Elouedi;Salem Benferhat	2013	Int. J. Approx. Reasoning	10.1016/j.ijar.2013.02.009	artificial intelligence;machine learning;data mining;mathematics	AI	-17.83610071517353	-3.8628085454332197	155949
4b25051366d3031f5368d225def9560f9e4d19e0	committees with farsighted voters: a new interpretation of stable sets	voting game;stable set	The interpretation of von Neumann-Morgenstern stable sets in voting games has been debated by most political scientists. The present paper addresses the issue in a model that consists of an in nite sequence of repetitions of the standard committee game. The analysis of equilibrium processes leads to the following conclusion: When voters are farsighted, an alternative is the limit of an absorbing equilibrium process if and only if it belongs to some stable set of the underlying committee game. While the traditional interpretation of the core implicitly assumes myopic voters, we also demonstrate that the core of a strong committee is the unique limit of all absorbing equilibrium processes, provided that voters are arbitrarily patient. We nally proceed to an analysis of the Condorcet Paradox in this dynamic context. JEL classi cation: C71, C73, D71	moravec's paradox;nash equilibrium	Vincent Anesi	2006	Social Choice and Welfare	10.1007/s00355-006-0146-7	independent set;economics;mathematical economics;social psychology;law;welfare economics	AI	-8.156052351350406	-2.7836504015106844	156040
ef5d016ddeecfbaae03c0938c9e9d08b9251cbc3	coalitional stability with a credibility constraint	credibly consistent sets;solution concept;simple game;proper simple games	Following Chwe (1994) we introduce a similar notion of coalitional stability for proper simple games with the additional desirable property that a coalition moves to an outcome stable with respect to this notion only when the outcome is not dominated by another stable outcome. We call a set of outcomes stable with respect to this notion a credibly consistent set. We examine the issue of the existence and non-emptiness of such sets and investigate their relation to some other solution concepts prevalent in the literature concerning coalitional stability.  2002 Elsevier Science B.V. All rights reserved.	information sciences institute;typescript	Anindya Bhattacharya	2002	Mathematical Social Sciences	10.1016/S0165-4896(01)00078-6	mathematical optimization;economics;mathematics;microeconomics;mathematical economics;welfare economics;solution concept	AI	-6.89563611164673	-1.8843802517793262	156226
33e7ccd8d765ff2f2d6afd60d053a3d78cba6fa8	can we trust subjective logic for information fusion?	formal logic;probability;sl fusion rule;belief assignment;beta probability density functions;information fusion;subjective logic;information fusion;subjective logic;belief functions	In this paper, we provide a deep examination of the main bases of Subjective Logic (SL) and reveal serious problems with them. A new interesting alternative way for building a normal coarsened basic belief assignment from a refined one is also proposed. The defects in the SL fusion rule and the problems in the link between opinion and Beta probability density functions are also analyzed. Some numerical examples and related analyses are provided to justify our viewpoints.	interpretation (logic);numerical analysis;sl (complexity);state space	Jean Dezert;Albena Tchamova;Deqiang Han;Jean-Marc Tacnet	2014	17th International Conference on Information Fusion (FUSION)			Robotics	-15.068914537034056	0.6977610316156974	156382
a4877a7bbab3a38f91dbf10c00bb2701e83a968d	minimum-effort coordination games: stochastic potential and logit equilibrium	nash equilibria;coordination game;journal of economic literature;potential function;choice function	This paper revisits the minimum-effort coordination game with a continuum of Paretoranked Nash equilibria. Noise is introduced via a logit probabilistic choice function. The resulting logit equilibrium distribution of decisions is unique and maximizes a stochastic potential function. In the limit as the noise vanishes, the distribution converges to an outcome that is analogous to the risk-dominant outcome for 2×2 games. In accordance with experimental evidence, logit equilibrium efforts decrease with increases in effort costs and the number of players, even though these parameters do not affect the Nash equilibria. JEL Classifications: C72, C92	apache continuum;markov chain;nash equilibrium	Simon P. Anderson;Jacob K. Goeree;Charles A. Holt	2001	Games and Economic Behavior	10.1006/game.2000.0800	price of stability;epsilon-equilibrium;best response;choice function;coordination game;economics;correlated equilibrium;microeconomics;risk dominance;mathematical economics;welfare economics;equilibrium selection;nash equilibrium	ECom	-4.7089247438784065	-1.444241326087082	156626
a9712cbd17608f4f72c514906a59ba2ed4ceb25a	the q-exponential decay of subjective probability for future reward: a psychophysical time approach	econophysics;finance;q exponential function;tsallis thermostatistics;neuroeconomics	This study experimentally examined why subjective probability for delayed reward decays non-exponentially (“hyperbolically”, i.e., q ˂ 1 in the q-exponential discount function) in humans. Our results indicate that nonlinear psychophysical time causes hyperbolic time-decay of subjective probability for delayed reward. Implications for econophysics and neuroeconomics are discussed.	experiment;nonlinear system;time complexity	Taiki Takahashi;Shinsuke Tokuda;Masato Nishimura;Ryo Kimura	2014	Entropy	10.3390/e16105537	neuroeconomics;econophysics;mathematical economics	ML	-4.598250633665429	-5.350677282656779	156838
07c9c41569e4975cb57ac3f69d7843e1de355516	rationalizability and minimal complexity in dynamic games	maastricht university;nash equilibrium;dynamic game;utility function;microeconomics;digital archive;open access;backward induction;forward induction;publication;scientific;institutional repository	"""This paper presents a formal epistemic framework for dynamic games in which players, during the course of the game, may revise their beliefs about the opponents' utility functions. We impose two key conditions upon the players' beliefs: (a) throughout the game, every move by the opponent should be interpreted as a rational move, and (b) the belief about the opponents' relative utilities between two terminal nodes should only be revised if you are sure that the opponent has decided to avoid one of these nodes. Common belief about these events leads to the concept of persistent rationalizability. It is shown that persistent rationalizability implies the backward induction procedure in generic games with perfect information. We next focus on persistently rationalizable types having beliefs with """"minimal complexity"""", resulting in the concept of minimal rationalizability. For two-player simultaneous move games, minimal rationalizability is equivalent to the concept of Nash equilibrium strategy. In every outside option game, as defined by van Damme (1989), minimal rationalizability uniquely selects the forward induction outcome."""	backward induction;emoticon;nash equilibrium;persistent data structure	Andrés Perea	2003		10.1145/846241.846243	simulation;economics;epistemology;computer science;artificial intelligence;publication;rationalizability;mathematics;microeconomics;mathematical economics;sequential game;backward induction;algorithm;nash equilibrium	AI	-8.817132141406526	-2.48492374266946	156907
c17c6875686d8fcdc0817abc57baf25024b42968	discovering expert's knowledge from sequences of discrete event class occurrences		This paper is concerned with the discovery of expert’s knowledge from a sequence of alarms provided by a knowledge based system monitoring a dynamic process. The discovering process is based on the principles and the tools of the Stochastic Approach framework where a sequence is represented with a Markov chain from which binary relations between discrete event classes can be find and represented as abstract chronicle models. The problem with this approach is to reduce the search space as close as possible to the relations between the process variables. To this aim, we propose an adaptation of the J-Measure to the Stochastic Approach framework, the BJ-Measure, to build an entropic based heuristic that help in finding abstract chronicle models revealing strong relations between the process variables. The result of the application of this approach to a real world system, the Sachem system that controls the blast furnace of the Arcelor-Mittal Steel group, is provided in the paper, showing how the combination of the Stochastic Approach and the Information Theory allows finding the a priori expert’s knowledge between blast furnace variables from a sequence of alarms.	algorithm;b. j. fogg;blast;causal filter;heuristic;information theory;knowledge-based systems;markov chain;stochastic matrix;system monitor;world-system	Marc Le Goc;Nabil Benayadi	2008				ML	-18.131491124112486	-4.562458287707305	157181
37e9973c1ef14708604ee8247bc158f9c1502854	reconciling justificatory internalism and content externalism	justification internalism;content externalism;language of thought hypothesis	At first pass, internalism about justification is the view that there is no justificatory difference without an internal difference. Externalism about mental content is the view that there are differences in mental content without an internal difference. Assuming (complete) mental contents are the primary bearers of justificatory features, the two views are in obvious tension. The goal of this paper is to determine how the tension is best resolved.	externalism;mind	Chris Tillman	2010	Synthese	10.1007/s11229-010-9827-y	internalism and externalism	Theory	-12.58673942685273	3.100152294629878	157247
09bd0e179eedbcbd1c1cef3fff182d074de24165	a novel algorithmic approach to the integration of posterior knowledge into condition-monitoring systems	condition monitoring	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	algorithm;francis;primary source	Shaun Marriott;Robert F. Harrison	2000	Int. J. Systems Science	10.1080/002077200418423	reliability engineering;engineering;data mining;mathematics	Robotics	-14.78051823919081	-5.565430860725503	157424
3aa08167bb8e84f415245f8d9bd87cd24e01dff9	learning in games with risky payoffs	learning process;learning in games;learning model;reinforcement learning;stochastic payoffs;coordination game;noncooperative games;experiments;fictitious play	This paper analyzes learning in multi-player noncooperative games with risky payoffs. The goal of the paper is to assess the relative importance of stochastic payoffs and expected payoffs in the learning process. A general learning model which nests several variations of reinforcement learning, belief-based learning, and experience-weighted attraction learning is used to analyze behavior in coordination game and prisonerʼs dilemma experiments with probabilistic payoffs. In all experiments, some subjects learn from past lottery outcomes, though the importance of these stochastic payoffs relative to expected payoffs depends on the game. Stochastic payoffs are less important when posted probabilities are equal to expected payoffs and more important when subjects are informed how much they would have earned from foregone strategies.	interdependence;network of cancer genes;prisoner's dilemma;regret (decision theory);reinforcement learning	Aric P. Shafran	2012	Games and Economic Behavior	10.1016/j.geb.2011.09.001	coordination game;economics;microeconomics;mathematical economics;welfare economics;reinforcement learning;symmetric game;fictitious play	ECom	-6.726647366212543	-5.6566393616650705	157455
46e77cb09f2be8e1cd2408db3932046c41d5eafd	stochastic equilibria under imprecise deviations in terminal-reward concurrent games		We study the existence of mixed-strategy equilibria in concurrent games played on graphs. While existence is guaranteed with safety objectives for each player, Nash equilibria need not exist when players are given arbitrary terminal-reward objectives, and their existence is undecidable with qualitative reachability objectives (and only three players). However, these results rely on the fact that the players can enforce infinite plays while trying to improve their payoffs. In this paper, we introduce a relaxed notion of equilibria, where deviations are imprecise. We prove that contrary to Nash equilibria, such (stationary) equilibria always exist, and we develop a PSPACE algorithm to compute one.	algorithm;lagrangian relaxation;nash equilibrium;pspace;reachability;stationary process;undecidable problem	Patricia Bouyer;Nicolas Markey;Daniel Stan	2016		10.4204/EPTCS.226.5	mathematical optimization;discrete mathematics;mathematics;mathematical economics	Theory	-5.464187549283813	3.2101193889478212	157463
61c6eb07a3bc5c49c154d02d65975bed97d380f6	space debris removal: learning to cooperate and the price of anarchy		In this paper we study space debris removal from a game-theoretic perspective. In particular we focus on the question whether and how self-interested agents can cooperate in this dilemma, which resembles a tragedy of the commons scenario. We compare centralised and decentralised solutions and the corresponding price of anarchy, which measures the extent to which competition approximates cooperation. In addition we investigate whether agents can learn optimal strategies by reinforcement learning. To this end, we improve on an existing high fidelity orbital simulator, and use this simulator to obtain a computationally efficient surrogate model that can be used for our subsequent game-theoretic analysis. We study both singleand multi-agent approaches using stochastic (Markov) games and reinforcement learning. The main finding is that the cost of a decentralised, competitive solution can be significant, which should be taken into consideration when forming debris removal strategies.	algorithmic efficiency;anarchy;centralisation;game theory;markov chain;molecular orbital;multi-agent system;reinforcement learning;simulation;surrogate model	Richard Klíma;Daan Bloembergen;Rahul Savani;Karl Tuyls;Alexander Wittig;Andrei Sapera;Dario Izzo	2018	Front. Robotics and AI	10.3389/frobt.2018.00054	dilemma;machine learning;artificial intelligence;computer science;operations research;tragedy of the commons;reinforcement learning;price of anarchy;markov decision process;high fidelity;space debris;markov chain	AI	-11.60613218525257	-8.792836707474928	157465
64597b1568d4a024e3e88f9b9545774655b1e295	a risk-based bidding strategy for continuous double auctions	zero intelligence;continuous double auction;software agent;bidding strategies;competitive equilibrium	We develop a novel bidding strategy that software agents can use to buy and sell goods in Continuous Double Auctions (CDAs). Our strategy involves the agent forming a bid or ask by assessing the degree of risk involved and making a prediction about the competitive equilibrium that is likely to be reached in the marketplace. We benchmark our strategy against two of the most common strategies for CDAs, namely the Zero-Intelligence and the ZeroIntelligence Plus strategies, and we show that our agents outperform these benchmarks. Specifically, our agents win in 100% of the simulations against the ZI agents and, on average, 75% of the games against the ZIP agents.	benchmark (computing);converge;data integrity field;nash equilibrium;population;s/pdif;simulation;software agent;software bug;traders	Perukrishnen Vytelingum;Rajdeep K. Dash;Esther David;Nicholas R. Jennings	2004			computer science;artificial intelligence;software agent	AI	-8.420485649594257	-8.90124168525148	157494
a686fdc8d0bc04b0b719243c4fe477796857365e	dynamic path consistency for spatial reasoning	dynamic qualitative spatial reasoning;topology;electronic mail;constraint propagation spatial information consistency constraint satisfaction techniques spatial knowledge consistency spatial reasoning dynamic environment spatial ontology spaceontology topological relations distance relations dynamic path consistency algorithm randomly generated spatial constraint problem;spatial reasoning;constraint satisfaction problem dynamic qualitative spatial reasoning dynamic temporal reasoning;earthquakes;constraint satisfaction problems;ontologies artificial intelligence;heuristic algorithms cognition tv robots electronic mail geographic information systems earthquakes;geographic information systems;heuristic algorithms;robots;cognition;random processes;constraint satisfaction problem;dynamic temporal reasoning;tv;topology constraint satisfaction problems ontologies artificial intelligence random processes spatial reasoning	Dealing with spatial knowledge requires the consistency of spatial information. This consistency is usually enforced by constraint satisfaction techniques including constraint propagation through arc and path consistency. While theses techniques often assume that spatial information are static, this is in general not the case in the real world. Our goal is to propose an approach to maintain the consistency of spatial knowledge in a dynamic environment. To our best knowledge no work in spatial reasoning has addressed this issue. In this paper we use a spatial ontology called Space Ontology to describe both objects and spatial relations namely topological and distance relations between these objects. Based on a dynamic path consistency algorithm, our proposed method maintains the consistency of spatial information after adding new instances of topological relations described by Space Ontology of a given environment. In order to evaluate the performance of our dynamic path consistency method, we conducted several tests on instantiations of Space Ontology in addition to randomly generated spatial constraint problems. The results of these tests demonstrate the efficiency of our method to deal with large size problems in a dynamic environment.	algorithm;constraint satisfaction;geographic information system;information needs;local consistency;procedural generation;software propagation;spatial–temporal reasoning	Lamia Belouaer;Maroua Bouzid;Malek Mouhoub	2012	2012 IEEE 24th International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2012.142	stochastic process;mathematical optimization;weak consistency;computer science;artificial intelligence;theoretical computer science;consistency model;machine learning;data mining;mathematics;constraint satisfaction problem;local consistency	DB	-13.157724553368706	-2.122229112935678	157967
052da21705affc5da742718f4e18189e7d4e9d1d	managing the concurrent execution of dependent product development stages	analytical models;overlapping;pareto optimisation;learning effectiveness;concurrent computing;computer crashes;costing;search problems concurrent engineering costing pareto optimisation product development;common mode;handset design company concurrent execution management dependent product development stage concurrent product development concurrent process nonnegative upstream evolution total development cost rework cost binary search pareto optimal overlapping strategy;companies;sensitivity;product development concurrent computing sensitivity analytical models computer crashes logic gates companies;process modeling concurrent engineering new product development overlapping;opportunity cost;logic gates;process modeling;binary search;search problems;time to market;process model;new product development;logic gate;concurrent process;pareto optimality;analytical model;concurrent engineering;product development	Concurrent product development, the practice of executing dependent product development stages simultaneously, has become the common mode of new product development because of the increasing importance of time-to-market. However, such simultaneous execution of dependent stages may substantially increase the total amount of rework. This research addresses the trade-offs involved in concurrent process, presents analytical models to determine the optimal priority ordering of initial development and rework, and the optimal overlapping duration. We first show that initial development is prior to rework when learning effect is taken into account. Then, based on the general assumption of nonnegative upstream evolution and the clear definition of maximum concurrency, we prove that the total development cost (including rework cost and opportunity cost of time) is convex with respect to the overlapping duration, and so the optimal overlapping duration can be identified by a simple binary search. After that we investigate the Pareto-optimal overlapping strategies for the cases where budget is given or the time to market is predetermined. Finally, the methodology is illustrated with a case study at a handset design company.	binary search algorithm;concurrency (computer science);dependent ml;new product development;parallel computing;pareto efficiency;rework (electronics)	Jun Lin;Yanjun Qian;Wentian Cui	2012	IEEE Transactions on Engineering Management	10.1109/TEM.2011.2132761	simulation;concurrent computing;economics;logic gate;engineering;marketing;operations management;process modeling;engineering drawing;new product development	DB	-12.235919015443667	-3.254331080727064	158080
6912097ed7caae8ffabf03e1f34e86d2a07ea31d	more on preference and freedom	freedom of choice	The paper seeks to formalize the notion of effective freedom or freedom to realize meaningful choices. The definition of meaningful choice used in this paper is based on the preference orderings of a reasonable person in a society. 1 argue that only the alternatives that can be selected by a reasonable person from the set of all possible alternatives provide a meaningful choice. 1 discuss this approach and provide an axiomatization of the cardinal rule and two lexicographic versions of this rule in this context.	axiomatic system;lexicographical order	Antonio Romero-Medina	2001	Social Choice and Welfare	10.1007/PL00007180	economics;mathematics;social psychology;welfare economics	AI	-8.47859110514149	-1.865853332238344	158106
716187542f455bdc36e9909717ee4c1cffee86b8	analogue neuromorphic systems		This thesis addresses a new area of science and technology, that of neuromorphic systems, namely the problems and prospects of analogue neuromorphic systems. The subject is subdivided into three chapters. Chapter 1 is an introduction. It formulates the oncoming problem of the creation of highly computationally costly systems of nonlinear information processing (such as artificial neural networks and artificial intelligence systems). It shows that an analogue technology could make a vital contribution to the creation such systems. The basic principles of creation of analogue neuromorphic systems are formulated. The importance will be emphasised of the principle of orthogonality for future highly efficient complex information processing systems. Chapter 2 reviews the basics of neural and neuromorphic systems and informs on the present situation in this field of research, including both experimental and theoretical knowledge gained up-to-date. The chapter provides the necessary background for correct interpretation of the results reported in Chapter 3 and for a realistic decision on the direction for future work. Chapter 3 describes my own experimental and computational results within the framework of the subject, obtained at De Montfort University. These include: the building of (i) Analogue Polynomial Approximator/lnterpolatoriExtrapolator, (ii) Synthesiser of orthogonal functions, (iii) analogue real-time video filter (performing the homomorphic filtration), (iv) Adaptive polynomial compensator of geometrical distortions of CRTmonitors, (v) analogue parallel-learning neural network (backpropagation algorithm). Thus, this thesis makes a dual contribution to the chosen field: it summarises the present knowledge on the possibility of utilising analogue technology in up-to-date and future computational systems, and it reports new results within the framework of the subject. The main conclusion is that due to its promising power characteristics, small sizes and high tolerance to degradation, the analogue neuromorphic systems will playa more and more important role in future computational systems (in particular in systems of artificial intelligence).		Vyacheslav Chesnokov	2001			applied mathematics;mathematics education;computer science;neuromorphic engineering	AI	-16.61944872014139	-1.5221678445193518	158192
a297402ffee9d882c797a35c0ced539400f600e2	hermeneutical philosophy and pragmatism: a philosophy of science	abduction;philosophy of science;explanation;ontologie;explication;philosophy of language;comparaison;logic;experience;monde de la vie;hermeneutical circle;peirce c s;comparison;pragmatisme;recherche;action;theory vs praxis;hermeneutics;heidegger m;culture;philosophy;pragmatism;epistemology;science philosophy;metaphysics;interpretation;theorie vs pratique;cercle hermeneutique;dewey j;ontology;philosophie des sciences;hermeneutique	Two philosophical traditions with much in common, (classical) pragmatism and (Heidegger's) hermeneutic philosophy, are here compared with respect to their approach to the philosophy of science. Both emphasize action as a mode of interpreting experience. Both have developed important categories – inquiry, meaning, theory, praxis, coping, historicity, life-world – and each has offered an alternative to the more traditional philosophies of science stemming from Descartes, Hume, and Comte. Pragmatism's abduction works with the dual perspectives of theory (as explanation) and praxis (as culture). The hermeneutical circle depends in addition on the lifeworld as background source of ontological meaning and resource for strategies of inquiry. Thus a hermeneutical philosophy of research involves three components: lifeworld (as ontological and strategic), theory (as explanatory), and praxis (as constitutive of culture).	altran praxis;hume (programming language);pdf/a;stemming;unix philosophy	Patrick A. Hellan	1998	Synthese	10.1023/A:1005032631417	philosophy of science;philosophy;pragmatism;epistemology;interpretation;philosophy of language;metaphysics;ontology;hermeneutics;logic;culture	AI	-11.346580580787121	3.278135654977872	158289
38bec736791a61b2204fc3db9115ab96de045a84	normative theories of argumentation: are some norms better than others?	argumentation;bayesian probability;reasoning;norms;psychological sciences	Norms—that is, specifications of what we ought to do—play a critical role in the study of informal argumentation, as they do in studies of judgment, decision-making and reasoning more generally. Specifically, they guide a recurring theme: are people rational? Though rules and standards have been central to the study of reasoning, and behavior more generally, there has been little discussion within psychology about why (or indeed if) they should be considered normative despite the considerable philosophical literature that bears on this topic. In the current paper, we ask what makes something a norm, with consideration both of norms in general and a specific example: norms for informal argumentation. We conclude that it is both possible and desirable to invoke norms for rational argument, and that a Bayesian approach provides solid normative principles with which to do so.	bayesian approaches to brain function;bayesian programming;design rationale;ingo wegener;jackson;requirement;semantic reasoner;theory;while	Adam Corner;Ulrike Hahn	2012	Synthese	10.1007/s11229-012-0211-y	philosophy;epistemology;bayesian probability;mathematics;reason;norm	AI	-13.920996724685924	2.219597471186294	158603
b7a0ae12f437a972193d3ab24b1794c4633b6a12	cluster-based asp solving with claspar	faculty of science environment engineering and technology;280301;pre2009 programming techniques	We report on three recent advances in the distributed ASP solver claspar. First, we describe its flexible architecture supporting various search strategies, including competitive search using a portfolio of solver configurations. Second, we describe claspar’s distributed learning capacities that allow for sharing learned nogoods among solver instances. Finally, we discuss claspar’s approach to distributed optimization.	mathematical optimization;solver	Martin Gebser;Roland Kaminski;Benjamin Kaufmann;Torsten Schaub;Bettina Schnor	2011		10.1007/978-3-642-20895-9_42	simulation;computer science;knowledge management;artificial intelligence;management science	AI	-16.879247519621586	-8.804129275156324	159235
03e88c07145da9b6f7073b0b8e74883053159f06	a nonparametric test for deviation from randomness with applications to stock market index data	tukey s g and h distributions;time series;62g10;binomial;percentiles;variance ratio tests	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	autocorrelation;ct scan;emoticon;francis;jock d. mackinlay;nl (complexity);population;primary source;pseudo-random number sampling;randomness;requirement;sensor;simulation;time series	Alicia Graziosi Strandberg;Boris Iglewicz	2013	Communications in Statistics - Simulation and Computation	10.1080/03610918.2011.654030	percentile;econometrics;sign test;time series;randomness tests;mathematics;binomial test;binomial;statistics	Mobile	-14.104631823317904	-5.736865313507135	159360
6ab8246470ce6823f320e8eac2f2006625e3d808	sufficient conditions for unique stable sets in three agent pillage games	stable sets;algorithm;core;cooperative game theory;co operative game theory	Pillage games (Jordan, 2006a) have two features that make them richer than cooperative games in either characteristic or partition function form: they allow power externalities between coalitions; they allow resources to contribute to coalitions’ power as well as to their utility. Extending von Neumann and Morgenstern’s analysis of three agent games in characteristic function form to anonymous pillage games, we characterise the core for any number of agents; for three agents, all anonymous pillage games with an empty core represent the same dominance relation. When a stable set exists, and the game also satisfies a continuity and a responsiveness axiom, it is unique and contains no more than 15 elements, a tight bound. By contrast, stable sets in three agent games in characteristic or partition function form may not be unique, and may contain continua. Finally, we provide an algorithm for computing the stable set, and can easily decide non-existence. Thus, in addition to offering attractive modelling possibilities, pillage games seem well behaved and analytically tractable, overcoming a difficulty that has long impeded use of cooperative game theory’s flexibility. © 2014 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/3.0/).	algorithm;characteristic function (convex analysis);cobham's thesis;game theory;partition function (mathematics);responsiveness;scott continuity	Colin Rowat;Manfred Kerber	2014	Mathematical Social Sciences	10.1016/j.mathsocsci.2014.02.003	bondareva–shapley theorem;combinatorial game theory;core;discrete mathematics;mathematics;mathematical economics;welfare economics	ECom	-5.860151855654878	-1.6839907395244986	159439
5a1829379b104effb817bc25e26944b243209a8d	rescue robot cul	mobile robot;following mode mobile robot;following mode;robotic follower;rescue robot	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source;rescue robot	Kenichi Tokuda;Koichi Osuka;Toshiro Ono	1998	Advanced Robotics	10.1163/156855399X00757	mobile robot;robot end effector;rescue robot;computer science;artificial intelligence;social robot;robot control	Robotics	-15.008070741687265	-6.849274254421427	160044
51b1641e2623680f395a9a48a2f755e9da21b1e5	a scientific psychologistic foundation for theories of meaning	natural language;logical form	I propose, develop and defend the view that theories of meaning — for instance, a theory specifying the logical form or truth conditions of natural language sentences — should be naturalized to scientific psychological inquiry. This involves both psychologism — the claim that semantics characterizes psychological states — and scientific naturalism — the claim that semantics will depend on the data and theories of scientific psychology. I argue that scientific psychologism is more plausible than the traditional alternative, the view that a theory of meaning is a priori. After defending scientific psychologism against several objections, I offer a speculative proposal that shows how a theory of meaning can be integrated into scientific psychology.	metaphysical naturalism;natural language;speculative execution;theory	Lawrence J. Kaye	1995	Minds and Machines	10.1007/BF00974743	psychology;superseded scientific theories;psychologism;logical form;philosophy;epistemology;computer science;artificial intelligence;natural language;social psychology;algorithm;cognitive science	NLP	-13.647149187989163	4.0558610776406585	160270
6bf26d971fb1f3094884c6d9c1538d0547e8e7b7	non-existence of subgame-perfect \(\varepsilon \) -equilibrium in perfect information games with infinite horizon	subgame perfect equilibrium;perfect information games;infinite horizon;non existence	Every finite extensive-form game with perfect information has a subgameperfect equilibrium. In this note we settle to the negative an open problem regarding the existence of a subgame-perfect ε-equilibrium in perfect information games with infinite horizon and Borel measurable payoffs, by providing a counter-example. We also consider a refinement called strong subgame-perfect ε-equilibrium, and show by means of another counter-example, with a simpler structure than the previous one, J. Flesch Department of Quantitative Economics, Maastricht University, P.O. Box 616, 6200 MD Maastricht, The Netherlands e-mail: j.flesch@maastrichtuniversity.nl J. Kuipers · G. Schoenmakers · K. Vrieze Department of Knowledge Engineering, Maastricht University, P.O. Box 616, 6200 MD Maastricht, The Netherlands e-mail: kuipers@maastrichtuniversity.nl G. Schoenmakers e-mail: gm.schoenmakers@maastrichtuniversity.nl K. Vrieze e-mail: koosvrieze@gmail.com A. Mashiah-Yaakovi · E. Shmaya · E. Solan (B) School of Mathematical Sciences, Tel Aviv University, Tel Aviv 69978, Israel e-mail: eilons@post.tau.ac.il A. Mashiah-Yaakovi e-mail: ayalam@post.tau.ac.il E. Shmaya Kellogg School of Management, Northwestern University, 2001 Sheridan Road, Evanston, IL 60208, USA e-mail: e-shmaya@kellogg.northwestern.edu	email;geforce 6 series;knowledge engineering;refinement (computing)	János Flesch;Jeroen Kuipers;Ayala Mashiah-Yaakovi;Gijs Schoenmakers;Eran Shmaya;Eilon Solan;Koos Vrieze	2014	Int. J. Game Theory	10.1007/s00182-014-0412-3	combinatorics;discrete mathematics;economics;mathematics;microeconomics;mathematical economics;subgame perfect equilibrium	Theory	-5.472170987780408	-0.6655662913740278	160403
0f0125b9f5afa5b0ad6546f8ec05c16543f8ee26	e-points in extensive games with complete information	existence theorem	In this paper we characterize a general existence theorem concerning E-points for n-persons extensive games with complete information. We provide a sufficient and necessary condition for the existence of such E-points.		Ezio Marchi	2006	IGTR	10.1142/S0219198906001193	bondareva–shapley theorem;combinatorics;discrete mathematics;economics;mathematics;mathematical economics	ECom	-5.799025555661262	-1.073683955358188	160432
275b4505052dc6343a2755ea5a67b45e39371f8a	extensionally defining principles and cases in ethics: an ai model	case base reasoning;computer model;a search;decision maker;extensional definitions;interpretive case based reasoning;empirical evidence;engineering ethics;professional ethics;analogy;text retrieval;ethical principles;structural mapping;operationalization	Principles are abstract rules intended to guide decision-makers in making normative judgments in domains like the law, politics, and ethics. It is difficult, however, if not impossible to define principles in an intensional manner so that they may be applied deductively. The problem is the gap between the abstract, open-textured principles and concrete facts. On the other hand, when expert decision-makers rationalize their conclusions in specific cases, they often link principles to the specific facts of the cases. In effect, these expert-defined associations between principles and facts provide extensional definitions of the principles. The experts operationalize the abstract principles by linking them to the facts. This paper discusses research in which the following hypothesis was empirically tested: extensionally defined principles, as well as cited past cases, can help in predicting the principles and cases that might be relevant in the analysis of new cases. To investigate this phenomenon computationally, a large set of professional ethics cases was analyzed and a computational model called SIROCCO, a system for retrieving principles and past cases, was constructed. Empirical evidence is presented that the operationalization information contained in extensionally defined principles can be leveraged to predict the principles and past cases that are relevant to new problem situations. This is shown through an ablation experiment, comparing SIROCCO to a version of itself that does not employ operationalization information. Further, it is shown that SIROCCO’s extensionally defined principles and case citations help it to outperform a full-text retrieval program that does not employ such information. Artificial Intelligence Journal, Volume 150, November 2003, pp. 145-181	computation;computational model;document retrieval;extension (semantics);intensional logic;sirocco	Bruce M. McLaren	2003	Artif. Intell.	10.1016/S0004-3702(03)00135-8	professional ethics;decision-making;empirical evidence;analogy;operationalization;computer science;knowledge management;artificial intelligence;mathematics;management science	AI	-16.315363291794096	1.7913063910515248	160466
5a01e4ec8bf9f4950aef0e0681b4ba7201a0648a	learning models from temporal-logic properties via explanations.	learning model;temporal logic;satisfiability;incremental learning	Given a model and a property expressed in temporal logic, a model checker normally produces a counterexample in case the model does not satisfy the property. This counterexample is meant to serve as a guide for manually modifying the model so that the new model does satisfy the property. We observe that basing the modification of a model on negative information (why a formula is not true) can have limitations, and we present a method employing positive information instead. Our method incrementally learns a subformula and marks the part of the model that makes the already learned subformula true (i.e. an explanation). Next, our method attempts to learn the rest of the formula without altering the marked part of the model.	model checking;quantum information;temporal logic;well-formed formula	Miguel Carrillo;David A. Rosenblueth	2007			temporal difference learning;natural language processing;unsupervised learning;instance-based learning;algorithmic learning theory;temporal logic;computer science;artificial intelligence;machine learning;inductive transfer;leabra;computational learning theory;algorithm;satisfiability	Logic	-15.98902229793073	2.7319327699164013	160474
dcd5f9b0c82b26f96f2efd3ea7e8e69cd92b80d5	on optimal solutions of answer set optimization problems		In 2003, Brewka, Niemela and Truszczynski introduced answer-set optimization problems. They consist of two components: a logic program and a set of preference rules. Answer sets of the program represent possible outcomes; preferences determine a preorder on them. Of interest are answer sets of the program that are optimal with respect to the preferences. In this work, we consider computational problems concerning optimal answer sets. We implement and study several methods for the problems of computing an optimal answer set; computing another one, once the first one is found; and computing an optimal answer set that is similar to respectively, dissimilar from a given interpretation. For the problems of the existence of similar and dissimilar optimal answer set we establish their computational complexity.	mathematical optimization;program optimization	Ying Zhu;Miroslaw Truszczynski	2013		10.1007/978-3-642-40564-8_55	answer set programming;data mining;algorithm	Vision	-7.452319722616923	2.8972279870236255	160603
fb4489f5472a78d853b4190811d448cd2b6dd586	formation of preferences and strategic analysis	transportation	In some situations, players have to start analyzing the game before they have formed their preferences over the outcomes of the game. However, strategic analysis naturally depends on preferences. This dilemma will be discussed and some ideas for resolving it will be presented.		Nimrod Megiddo	2010		10.1145/1807406.1807476	non-cooperative game;knowledge management;political science;management science;social psychology	ECom	-9.305058340193806	-2.2440312457488956	160649
321eddca55666abb9e2c041863c0e2205ca14ce3	the cryptologic origin of braille		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	David Kahn	1995	Cryptologia	10.1080/0161-119591883845	theoretical computer science;computer science;braille	Robotics	-15.123095221939417	-5.70726075345555	160653
8659ea37c8ddee2ee1254e0413ac59ff1ea170c9	commitment strategies in planning: a comparative analysis	total order;comparative analysis;partial order	"""In this paper we compare the ut i l i ty of different commitment strategies in planning. Under a """"least commitment strategy"""", plans are represented as part ia l orders and operators are ordered only when interactions are detected. We investigate claims of the inherent advantages of planning w i t h part ia l orders, as compared to planning w i t h tota l orders. By focusing our analysis on the issue of operator ordering commi tment , we are able to carry out a rigorous comparative analysis of two planners. We show that partial-order planning can be more efficient than total-order planning, but we also show that this is not necessarily so."""	automated planning and scheduling;commitment ordering;interaction;partial-order planning;qualitative comparative analysis	Steven Minton;John L. Bresina;Mark Drummond	1991			partially ordered set;qualitative comparative analysis;operations research;total order	AI	-11.682387038177069	-1.8520779048207072	160864
5886111231089c86409f60e2916c83ac7becacbe	epistemic logic, relevant alternatives, and the dynamics of context	relevant alternative;context sensitivity;epistemic logic;ra theory;known implication;ra theorist;david lewis;context change;closure principle;dynamic epistemic logic;knowledge attribution	Abstract. According to the Relevant Alternatives (RA) Theory of knowledge, knowing that something is the case involves ruling out (only) the relevant alternatives. The conception of knowledge in epistemic logic also involves the elimination of possibilities, but without an explicit distinction, among the possibilities consistent with an agent’s information, between those relevant possibilities that an agent must rule out in order to know and those remote, far-fetched or otherwise irrelevant possibilities. In this article, I propose formalizations of two versions of the RA theory. Doing so clarifies a famous debate in epistemology, pitting Fred Dretske against David Lewis, about whether the RA theorist should accept the principle that knowledge is closed under known implication, familiar as the K axiom in epistemic logic. Dretske’s case against closure under known implication leads to a study of other closure principles, while Lewis’s defense of closure by appeal to the claimed context sensitivity of knowledge attributions leads to a study of the dynamics of context. Having followed the first lead at length in other work, here I focus more on the second, especially on logical issues associated with developing a dynamic epistemic logic of context change over models for the RA theory.	counterfactual conditional;dynamic epistemic logic;epistemic closure;epistemic modal logic;fred (chatterbot);logic theorist;relevance;theory;traffic collision avoidance system;eric	Wesley H. Holliday	2011		10.1007/978-3-642-31467-4_8	epistemic modal logic;epistemology;artificial intelligence;mathematics;algorithm	AI	-13.89897276744071	3.4198758209526514	161438
ea384cc6a66a6db3ba97a86942954f71ecbe974b	solving the multiagent selection and scheduling problem	multiagent selection;scheduling problem	Consider Ann’s morning scheduling problem. Ann is a graduate student, who, among many other objectives, would like to both exercise and work on research during her morning. I summarize possible morning activities in Table 1. Even for these two simple objectives, selecting a feasible schedule from the many possible schedules (run then generate experimental results, write then bike, swim then read some related work, etc.) may be non-trivial. However, suppose Ann wants to run with her friend Bill, who notoriously oversleeps his alarm. Additionally, suppose also that Ann must coordinate the use of her lab’s computational cluster with her lab mate Claire. Without further information from Bill and Claire, it is impossible for Ann to determine which candidate schedules will successfully achieve her morning goals. One option for Ann would be to myopically select her schedule anyway, with the risk that her attempt to run with Bill or to use the computational cluster could result in a failed goal. As another option, Ann could also volunteer to collect the scheduling constraints of both Bill and Claire and generate a single joint morning schedule. However, this puts additional scheduling burden on Ann while requiring both Bill and Claire to reveal other scheduling commitments they may prefer to keep private. Even if Ann employed a centralized computational agent to solve this global scheduling problem, the resulting combinatorics may limit the scalability of such a centralized approach. Instead, the pervasiveness of personal computational devices, coupled with desires for scalability and privacy, argue for decentrally solving such problems using multiagent algorithms. My thesis focuses on providing scalable, multiagent algorithms for solving rich, complex multiagent activity scheduling and selection problems, while retaining as much privacy as possible on behalf of the human users. My approach is distinct from other recent multiagent scheduling approaches (Hunsberger 2002; Smith et al. 2007; Shah, Conrad, and Williams 2009) in that it uses a constraint-based representation of selection (finite-domain) aspects of scheduling problems in addition to the scheduling aspects. I proceed by introducing the Multiagent Selection and Scheduling Problem	agent-based model;autonomy;computation;half-life 2: episode one;privacy;scheduling (computing)	James Boerkoel	2011		10.5591/978-1-57735-516-8/IJCAI11-464	dynamic priority scheduling	AI	-10.502593189116007	-5.405876711852765	161604
f302d06ea52bda0b2daf5bd5ac7c3ae523e069d1	scheduling a cascade with opposing influences		Adoption or rejection of ideas, products, and technologies in a society is often governed by simultaneous propagation of positive and negative influences. Consider a planner trying to introduce an idea in different parts of a society at different times. How should the planner design a schedule considering this fact that positive reaction to the idea in early areas has a positive impact on probability of success in later areas, whereas a flopped reaction has exactly the opposite impact? We generalize a well-known economic model which has been recently used by Chierichetti, Kleinberg, and Panconesi (ACM EC’12). In this model the reaction of each area is determined by its initial preference and the reaction of early areas. We model the society by a graph where each node represents a group of people with the same preferences. We consider a full propagation setting where news and influences propagate between every two areas. We generalize previous works by studying the problem when people in different areas have various behaviors. We first prove, independent of the planner’s schedule, influences help (resp., hurt) the planner to propagate her idea if it is an appealing (resp., unappealing) idea. We also study the problem of designing the optimal non-adaptive spreading strategy. In the non-adaptive spreading strategy, the schedule is fixed at the beginning and is never changed. Whereas, in adaptive spreading strategy the planner decides about the next move based on the current state of the cascade. We demonstrate that it is hard to propose a non-adaptive spreading strategy in general. Nevertheless, we propose an algorithm to find the best non-adaptive spreading strategy when probabilities of different behaviors of people in various areas drawn i.i.d from an unknown distribution. Then, we consider the influence propagation phenomenon when the underlying influence network can be any arbitrary graph. We show it is #P -complete to compute the expected number of adopters for a given spreading strategy. However, we design a polynomial-time algorithm for the problem of computing the expected number of adopters for a given schedule in the full propagation setting. Last but not least, we give a polynomial-time algorithm for designing an optimal adaptive spreading strategy in the full propagation setting.	algorithm;graph (discrete mathematics);rejection sampling;schedule (computer science);schedule (project management);software propagation;time complexity	Mohammad Taghi Hajiaghayi;Hamid Mahini;Anshul Sawant	2013		10.1007/978-3-642-41392-6_17	simulation;computer science;artificial intelligence;operations management;mathematics;mathematical economics;algorithm	AI	-6.537113386618423	-6.774567905431585	161874
1a14fc181c7eaf66b602698dda9d5850216f2fd5	existence of equilibria in quantum bertrand-edgeworth duopoly game	bertrand edgeworth duopoly game;edgeworth paradox;quantum game;price competition;quantum physics;existence of nash equilibria;continuous quantum variables;pure strategy nash equilibria	Both classical and quantum version of two models of price competition in duopoly market, the one is realistic and the other is idealized, are investigated. The pure strategy Nash equilibria of the realistic model exists under stricter condition than that of the idealized one in the classical form game. This is the problem known as Edgeworth paradox in economics. In the quantum form game, however, the former converges to the latter as the measure of entanglement goes to infinity.	bertrand (programming language)	Yohei Sekiguchi;Kiri Sakahara;Takashi Sato	2012	Quantum Information Processing	10.1007/s11128-011-0276-4	price of stability;game theory;best response;coordination game;non-credible threat;repeated game;edgeworth conjecture;chicken;edgeworth paradox;normal-form game;mathematical economics;nash equilibrium;quantum mechanics;centipede game	Theory	-4.916272064685818	-1.5583949463505982	161993
e8ed783b211dd696726205e1afc89eee8da8886d	geschichte der anwendung quantitativer verfahren in der russischen sprach- und literaturwissenschaft		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Haitao Liu	2010	Journal of Quantitative Linguistics	10.1080/09296174.2010.512171	linguistics;computer science	Robotics	-15.168285778981392	-5.649159024584257	162050
bc288872fbf2e51b5a10c2e928edf9d19609cbba	limit representations of intergenerational equity		Equal treatment of all generations is a fundamental ethical principle in intertemporal welfare economics. This principle is expressed in anonymity axioms of orderings on the set of infinite utility streams. We first show that an ordering satisfies finite anonymity, uniform Pareto,weak non-substitution, and sup continuity if and only if it is representedby an increasing, continuous function that is a natural extensionof the limit function. We then show that whenever such an ordering is infinitely anonymous, it depends only on the liminf and limsup of any utility stream. Our results imply that in ethically ranking utility streams, reflecting only infinitely long-run movements is possible, with liminf and limsup particularly essential, but it is impossible to respect finite generations.	pareto efficiency;scott continuity	Toyotaka Sakai	2016	Social Choice and Welfare	10.1007/s00355-016-0973-0	mathematical optimization;economics;mathematics;microeconomics;mathematical economics;welfare economics	ECom	-6.827277127311269	-1.6286659806656136	162068
2d533fa4ec9305d9e51640048a5d929f6f6d53f2	frugal bribery in voting	bribery;algorithm;frugal;voting;theory;manipulation;computational social choice	Bribery in elections is an important problem in computational social choice theory. We introduce and study two important special cases of the classical $BRIBERY problem, namely, FRUGAL-BRIBERY and FRUGAL-$BRIBERY where the briber is frugal in nature. By this, we mean that the briber is only able to influence voters who benefit from the suggestion of the briber. More formally, a voter is vulnerable if the outcome of the election improves according to her own preference when she accepts the suggestion of the briber. In the FRUGAL-BRIBERY problem, the goal is to make a certain candidate win the election by changing only the vulnerable votes. In the FRUGAL-$BRIBERY problem, the vulnerable votes have prices and the goal is to make a certain candidate win the election by changing only the vulnerable votes, subject to a budget constraint. We further formulate two natural variants of the FRUGAL-$BRIBERY problem namely UNIFORM-FRUGAL-$BRIBERY and NONUNIFORM-FRUGAL-$BRIBERY where the prices of the vulnerable votes are, respectively, all the same or different. The FRUGAL-BRIBERY problem turns out to be a special case of sophisticated $BRIBERY as well as SWAPBRIBERY problems. Whereas the FRUGAL-$BRIBERY problem turns out to be a special case of the $BRIBERY problem. We show that the FRUGAL-BRIBERY problem is polynomial time solvable for the k-approval, k-veto, and plurality with run off voting rules for unweighted elections. These results establish success in finding practically appealing as well as polynomial time solvable special cases of the sophisticated $BRIBERY and SWAP-BRIBERY problems. On the other hand, we show that the FRUGAL-BRIBERY problem is NP-complete for the Borda voting rule and the FRUGAL-$BRIBERY problem is NP-complete for most of the voting rules studied here barring the plurality and the veto voting rules for unweighted elections. Our hardness results of the FRUGAL-BRIBERY and the FRUGAL-$BRIBERY problems thus subsumes and strengthens the hardness results of the $BRIBERY problem from the literature. For the weighted elections, we show that the FRUGAL-BRIBERY problem is NP-complete for all the voting rules studied here except the plurality voting rule even when the number of candidates is as low as 3 (for the STV and the plurality with run off voting rules) or 4 (for the maximin, the Copeland with α ∈ [0, 1), and the	decision problem;list of sega arcade system boards;minimax;np-completeness;time complexity;uniform resource identifier	Palash Dey;Neeldhara Misra;Y. Narahari	2016		10.1016/j.tcs.2017.02.031	bullet voting;voting;approval voting;cardinal voting systems;preferential block voting;anti-plurality voting;condorcet method;theory;algorithm;disapproval voting	AI	-7.401213032822118	1.4550823751444828	162271
04efafa2c9c401e6d5eaff1a4159cb67c9645564	on bitcoin and red balloons	information propagation;bitcoin;mechanism design	In this letter we present a brief report of our recent research on information distribution mechanisms in networks [Babaioff et al. 2011]. We study scenarios in which all nodes that become aware of the information compete for the same prize, and thus have an incentive not to propagate information.  Examples of such scenarios include the 2009 DARPA Network Challenge (finding red balloons), and raffles. We give special attention to one application domain, namely Bitcoin, a decentralized electronic currency system. We propose reward schemes that will remedy an incentives problem in Bitcoin in a Sybil-proof manner, with little payment overhead.	application domain;bitcoin;computation;darpa network challenge;digital currency;overhead (computing);sybil attack	Moshe Babaioff;Shahar Dobzinski;Sigal Oren;Aviv Zohar	2011	SIGecom Exchanges	10.1145/2325702.2325704	mechanism design;economics;advertising;internet privacy;mathematical economics;world wide web;computer security	ECom	-8.845303684006304	-5.489268273128835	163118
69bcba5a5882926d1acbd9de735bd379958d9ef6	on cooperative games related to market situations and auctions	market games;91a12;one object auction situations;cooperative game;90b05;ring games;peer group games;market game;solution concept;big boss games	We consider a market situation with two corners. One corner consists of a single seller with one object, and the other corner consists of potential buyers who all want the object. We suppose that the valuations of the object for the different buyers are known by all of them. Then two types of cooperative games, which we call market games and ring games, corresponding to such market situations are considered.Market games are related to special total big boss games, while ring games are related to special convex games, the peer group games. It turns out that there exists a duality relation between the market game and the ring game arising from the same two-corner market situation. For both classes of games relevant solution concepts are studied.		Rodica Branzei;Vito Fragnelli;Ana Meca;Stef Tijs	2009	IGTR	10.1142/S0219198909002443	bondareva–shapley theorem;video game design;combinatorial game theory;game theory;economics;turns, rounds and time-keeping systems in games;emergent gameplay;game mechanics;advertising;mathematical economics;sequential game;global game;solution concept;commerce	ECom	-5.5042047081424155	-3.007875500343554	163379
334c7930fa9e757c693e847a354c9e776fa1ef48	perspectives on the validation and verification of machine learning systems in the context of highly automated vehicles		Algorithms incorporating learned functionality play an increasingly important role for highly automated vehicles. Their impressive performance within environmental perception and other tasks central to automated driving comes at the price of a hitherto unsolved functional verification problem within safety analysis. We propose to combine statistical guarantee statements about the generalisation ability of learning algorithms with the functional architecture as well as constraints about the dynamics and ontology of the physical world, yielding an integrated formulation of the safety verification problem of functional architectures comprising artificial intelligence components. Its formulation as a probabilistic constraint system enables calculation of low risk manoeuvres. We illustrate the proposed scheme on a simple automotive scenario featuring unreliable environmental perception.		Werner Damm;Martin Fränzle;Sebastian Gerwinn;Paul Kröger	2018			machine learning;artificial intelligence;ontology;functionalism (architecture);verification and validation;probabilistic logic;perception;automotive industry;computer science;functional verification;generalization	AI	-18.432046388472273	-1.8058847673857543	163513
8ed1fec32e33096bbbe7901d5db5617db05e4085	negotiation exploiting reasoning by projections	distributed agents;utility function;satisfiability	We present a framework that allows two self-motivated distributed agents to perform, in an efficient way, negotiations aiming at achieving mutually satisfactory agreements, when privacy of information is an issue, and no central authority could be used. In particular, each agent has her own constraints to satisfy, as well as her own utility function. Such issues are kept private, and cannot be disclosed to the counterpart. Negotiation is hence carried out by exchanging proposals and by performing sophisticated forms of reasoning on the remote agent’s offers, by trying to infer some characteristics of the counterpart, in order to achieve efficient process convergence.	utility	Toni Mancini	2009		10.1007/978-3-642-00487-2_35	theoretical computer science;machine learning;mathematics;algorithm	AI	-9.742131211762588	-7.26059539380367	163754
0bac93f15299aba5ebde6e692efc9ca5b9d9f155	darwin’s solution to the species problem	genetique;general lineage concept;species;philosophie de la biologie;espece;darwin;ressemblance;category;species problem;species category;family resemblance;famille;biology;genetics;kind;categorie;darwin c;family;philosophy of biology;science philosophy;homeostatic property cluster theory;resemblance;philosophie des sciences;biologie	Biologists and philosophers that debate the existence of the species category fall into two camps. Some believe that the species category does not exist and the term ‘species’ should be eliminated from biology. Others believe that with new biological insights or the application of philosophical ideas, we can be confident that the species category exists. This paper offers a different approach to the species problem. We should be skeptical of the species category, but not skeptical of the existence of those taxa biologists call ‘species.’ And despite skepticism over the species category, there are pragmatic reasons for keeping the word ‘species.’ This approach to the species problem is not new. Darwin employed a similar strategy to the species problem 150 years ago.	darwin	Marc Ereshefsky	2009	Synthese	10.1007/s11229-009-9538-4	category;philosophy;epistemology;darwin;mathematics;kind;philosophy of biology	Logic	-11.124807400229187	3.403603920119547	163839
2bd5b94142dbb3d62c6ce4f45ae8b18d6a4382b1	consistency, converse consistency, and aspirations in tu-games	tu games;hb economic theory;satisfiability;converse consistency;reduced games;consistency;aspirations	In problems of choosing ‘aspirations’ for TU-games, we study two axioms, ‘MW-consistency’ and ‘converse MW-consistency.’ In particular, we study which subsolutions of the aspiration correspondence satisfy MW-consistency and/or converse MW-consistency. We also provide axiomatic characterizations of the aspiration kernel and the aspiration nucleolus.  2002 Elsevier Science B.V. All rights reserved.	microwave	Toru Hokari;Özgür Kibris	2003	Mathematical Social Sciences	10.1016/S0165-4896(02)00085-9	mathematics;linguistics;mathematical economics;consistency;welfare economics;satisfiability	AI	-6.743355203383206	-1.1158300374536072	163934
216e2a63517d8f087c32a6a06d7acf7060c287db	algorithmic collusion in cournot duopoly market: evidence from experimental economics		Algorithmic collusion is an emerging concept. Whether algorithmic collusion is a creditable threat remains as an argument. In this paper, we propose an algorithm which can extort its human rival to collude in a Cournot duopoly competing market. In experiments, we show that, the algorithm can successfully extort its human rival and gets higher profit in long-run, meanwhile the human rival will fully collude with the algorithm. As a result, the social welfare declines rapidly and stably. Both in theory and in experiment, our work confirms that, algorithmic collusion can be a creditable threat. In application, we hope, the models of the algorithm design as well as the experiment environment illustrated in this work, can be an incubator or a test bed for researchers and policymakers to handle the emerging algorithmic collusion.	algorithm design;experiment;nash equilibrium;testbed;threat (computer)	Nan Zhou;Xiang Lin;Shijian Li;Zhijian Wang	2018	CoRR		cournot competition;collusion;algorithm design;experimental economics;microeconomics;economics	NLP	-5.403478701490753	-7.65813671697971	164052
6043b139cc58e4f2d21079233da131c7b2cb98ad	prolegomena to any future artificial moral agent	moral agency computational ethics moral turing test;turing test;arti cial intelligence;autonomous agent;computer ethics	As arti® cial intelligence moves ever closer to the goal of producing fully autonomous agents, the question of how to design and implement an arti® cial moral agent (AMA) becomes increasingly pressing. Robots possessing autonomous capacities to do things that are useful to humans will also have the capacity to do things that are harmful to humans and other sentient beings. Theoretical challenges to developing arti® cial moral agents result both from controversies among ethicists about moral theory itself, and from computational limits to the implementation of such theories. In this paper the ethical disputes are surveyed, the possibility of a `moral Turing Test ' is considered and the computational di culties accompanying the diOE erent types of approach are assessed. Human-like performance, which is prone to include immoral actions, may not be acceptable in machines, but moral perfection may be computationally unattainable. The risks posed by autonomous machines ignorantly or deliberately harming people and other sentient beings are great. The development of machines with enough intelligence to assess the eOE ects of their actions on sentient beings and act accordingly may ultimately be the most important task faced by the designers of arti® cially intelligent automata. 1. Introduction A good web server is a computer that e ciently serves up html code. A good chess program is one that wins chess games. There are some grey areas and fuzzy edges, of course. Is a good chess program one that wins most games or just some ? Against just ordinary competitors or against world class players ? But in one sense of the question, it is quite clear what it means to build a good computer or write a good program. A good one is one that ful® lls the purpose we had in building it. However, if you wanted to build a computer or write a program that is good in a moral sense, that is a good moral agent, it is much less clear what would count as success. Yet as arti® cial intelligence moves ever closer to the goal of producing fully autonomous agents, the question of how to design and implement an arti® cial moral agent becomes increasingly pressing. Robots possessing autonomous capacities to do things that are useful to humans will also have the capacity to do things that are harmful to humans and other sentient beings. How to curb these capacities for harm is …	automata theory;automaton;autonomous agent;autonomous robot;character encodings in html;chess engine;cobham's thesis;computation;computational complexity theory;computer performance;deep blue (chess computer);lisp machine;sentience;server (computing);top-down and bottom-up design;turing test;web server	Colin Allen;Gary Varner;Jason Zinser	2000	J. Exp. Theor. Artif. Intell.	10.1080/09528130050111428	turing test;computer science;artificial intelligence;autonomous agent;moral disengagement;computer ethics	AI	-11.724381146335661	-2.60516259657941	164056
3340dbe99a8a951d0d940b03fc659b58c4592036	perfect cryptography, s5 knowledge, and algorithmic knowledge	bayesian games;multiagent system;unawareness;cryptographic protocol;type space;interactive epistemology;agreement;incomplete information;speculative trade;awareness;equilibrium;common prior;knowledge base	We propose a principled approach to model secrecy in multiagent systems, by defining a set of possible observations and providing agents with algorithms used to distinguish the possible states of the system. Our approach fits naturally within a knowledge-based account of secrecy. By adjusting both the kind of observations and the capabilities of the agents, we can capture in a natural way different forms of secrecy in the presence of perfect cryptography. In particular, we show how to model extraction secrecy. Our formalization suggests a unified definition of secrecy for cryptographic protocols and for systems that seek to prevent inadmissible flows of information.	agent-based model;algorithm;cryptographic protocol;cryptography;dolev–yao model;fits;instance (computer science);multi-agent system;norm (social);yao graph	Sabina Petride;Riccardo Pucella	2007		10.1145/1324249.1324281	knowledge base;awareness;computer science;knowledge management;artificial intelligence;theoretical computer science;cryptographic protocol;mathematics;complete information;algorithm	AI	-17.164993755515646	3.6944877617096235	164162
2e507e918a2e1520f725264aef708721d7cc7e72	individual behavior of first-price auctions: the importance of information feedback in computerized experimental markets	experimental economics;independent private values;nash equilibrium;risk aversion;choice experiment;first price auction;computer simulation	This article reports the results of an individual choice experiment designed to test the Nash equ predictions of the first-price sealed-bid auction. A subject faced in 100 auctions always the same value and competed with computer-simulated bids. The design used between-subjects variation volved information feedback as the treatment variable. Earlier experimental work on first price auctio frequently reported an overbidding relative to the risk neutral Nash equilibrium. Our data provide ev that overbidding can be fostered by the standard information feedback in auction experiments, whic each auction, reveals the winning bid only. By means of learning direction theory we explain the vidual bidding dynamics in our experiment. Finally we apply impulse balance theory and make lo predictions of individual bidding behavior.  2005 Elsevier Inc. All rights reserved. JEL classification:C12; C13; C72; C92; D44	balance theory;computer simulation;experiment;nash equilibrium;real-time bidding	Tibor Neugebauer;Reinhard Selten	2006	Games and Economic Behavior	10.1016/j.geb.2005.10.001	industrial organization;computer simulation;vickrey auction;combinatorial auction;generalized second-price auction;risk aversion;economics;unique bid auction;vickrey–clarke–groves auction;common value auction;revenue equivalence;english auction;double auction;microeconomics;mathematical economics;experimental economics;welfare economics;auction theory;nash equilibrium	AI	-5.778345602993694	-6.122985081829122	164512
7da0629a976c05e5334ea08385a3454157f4d60f	fuzzy nash-pareto equilibrium: concepts and evolutionary detection	game theory;nash equilibrium;cournot duopoly;rational agent	Standard game theory relies on the assumption that players are rational agents that try to maximize their payoff. Experiments with human players indicate that Nash equilibrium is seldom played. The goal of proposed approach is to explore more nuance equilibria by allowing a player to be biased towards different equilibria in a fuzzy manner. Several classes of equilibria (Nash, Pareto, Nash-Pareto) are defined by using appropriate generative relations. An evolutionary technique for detecting fuzzy equilibria is considered. Experimental results on Cournot’ duopoly game illustrate evolutionary detection of proposed fuzzy equilibria.	nash equilibrium;pareto efficiency	Dumitru Dumitrescu;Rodica Ioana Lung;Tudor Dan Mihoc;Réka Nagy	2010		10.1007/978-3-642-12239-2_8	price of stability;bargaining problem;implementation theory;game theory;epsilon-equilibrium;traveler's dilemma;best response;trembling hand perfect equilibrium;coordination game;economics;non-credible threat;folk theorem;repeated game;correlated equilibrium;chicken;microeconomics;risk dominance;normal-form game;mathematical economics;welfare economics;equilibrium selection;solution concept;nash equilibrium;symmetric equilibrium	AI	-9.157417698120321	-9.079246109698886	164706
0bc91347b3db853eda283ca99573f66b4bdd922f	on an extension of the concept of tu-games and their values		We propose a new more general approach to TU-games and their efficient values, significantly different from the classical one. It leads to extended TU-games described by a triplet (N , v, ), where (N , v) is a classical TU-game on a finite grand coalition N , and ∈ R is a game worth to be shared between the players in N . Some counterparts of the Shapley value, the equal division value, the egalitarian Shapley value and the least square prenucleolus are defined and axiomatized on the set of all extended TU-games. As simple corollaries of the obtained results, we additionally get some new axiomatizations of the Shapley value and the egalitarian Shapley value. Also the problem of independence of axioms is widely discussed.	decibel;emoticon;stable marriage problem;triplet state	Tadeusz Radzik	2017	Math. Meth. of OR	10.1007/s00186-017-0587-z	mathematical optimization;mathematics;shapley value;mathematical economics	ECom	-6.7903597356443335	-1.3101814481305534	164728
8e13612ba21aa305b520ed647c54b76c4b1bc362	special issue: optimization under fuzzy and possibilisitic uncertainty	possibilisitic uncertainty;special issue		optimizing compiler	Weldon A. Lodwick;Masahiro Inuiguchi	2007	Fuzzy Sets and Systems	10.1016/j.fss.2007.04.002	probabilistic-based design optimization	Robotics	-12.218889767005379	-6.009301041209416	165038
dd6aba15df49db78eee4ac4a2dc00391077ef02f	cloud-based production logistics synchronization mechanism and method	collaborative optimization;management	It is inevitable that the dynamic is occurs in the operation of the production logistics (PL). Production logistics synchronization (PLS) can solve the dynamic of production logistics operation process by collaboration the both or among of the production, shipping and storage. Cloud manufacturing mode can quickly respond to the shortage of resources in the production logistics operation process, and provide cheap cloud resource services, such as cloud forklift. This paper is aimed at the problem for the dynamic of production logistics operation, and the dynamic of production logistics is classified. The PLS mechanism and cloud-based PLS information framework are established. By using collaboration optimization (CO) method, the optimization method of PLS is put forward. Finally, the PLS optimization model of production and storage is presented with an industrial case, and the effectiveness is also demonstrated and analyzed.	logistics	ShuiPing Lei;Ting Qu;Zongzhong Wang;Xin Chen;Hao Luo;George Q. Huang	2015		10.1007/978-3-319-22759-7_53	operations management;economic shortage;information framework;cloud computing;synchronization;business;cloud manufacturing	Robotics	-12.297725730662993	-3.0309502435098814	165113
e205a23449d83330cacbc61e4d38529f46b8b7b0	a code problem: a book review		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Louis Kruh	1981	Cryptologia	10.1080/0161-118191855913	cognitive science	Robotics	-15.126769978395096	-5.741708619242505	165145
8c361ece4418101b8a8d14be8128387dca95a387	strategic equilibria and decisive set structures for social choice mechanisms	social choice	The connections between ,\trategy selecting functions for social choice mechanisms and the power structure of the associated collections of decisive sets are investigated. Equilibrium conditions are presented that give rise IO filters (oligarchies), prefilters (collegial polities), and acyclic majorities. ‘ihe results broaden the dictatorship theorems of Gibbard and Satterthwaite and resolve open questions about conditions that lead to more democratic power structures.	directed acyclic graph	Edward W. Packel;Donald G. Saari	1982	Mathematical Social Sciences	10.1016/0165-4896(82)90030-0	social choice theory;economics;mathematics;microeconomics;mathematical economics;welfare economics	Theory	-5.823213448496969	-2.22268427979505	165225
c292351e88eca1799b31571bf6748ca1cb2f60fc	a review on select models for supply chain formation	incentive compatibility;supply chain formation;negotiation function;social choice function;scm;contract theory;belief merging;supply chain management;supplier selection	Supply chain formation is becoming a new and challenging topic of research in supply chain management. It basically deals with selection of suppliers or group of suppliers, which are rational in nature, to make them partners in the supply chain. Because of this rationality a game is induced between the suppliers. This paper is an attempt to compare mechanism design, negotiation, and contract theory where the issue of rationality is well handled, which serves as an input for the comparative analysis of the incentives provided to participants/suppliers. Also, this paper highlights some of the essential issues, challenges and complexities in both mechanism design and negotiation model which will be useful for researchers and supply chain practitioners working in this interesting and challenging area. Finally, this paper comes up with the future directions of developing the optimal strategies, addressing the identified issues, challenges and complexities associated with supply chain formation.	knowledge base;qualitative comparative analysis;rationality;regular language description for xml;simulation;supply chain attack	Gajanan Panchal;Vipul Jain	2011	IJBPSCM	10.1504/IJBPSCM.2011.041374	supply chain risk management;contract theory;supply chain management;economics;incentive compatibility;service management;marketing;operations management;supply chain;business;commerce	AI	-8.732545429708216	-8.87413596972915	165336
bc4da4c68ae3aa979833bc170a43f4d8a8786f6f	on the effects of heterogeneity on one-way flow network formation	network formation	In this work, the problem of one-way flow social network formation is studied allowing for a general type of agents' heterogeneity: Agents do not have an a priori opinion on the relative importance of the benefits they conjecture to get from connections with the others. Two different models of network formation are introduced, corresponding to different kinds of disutilities of establishing direct connections. These models are games with vector-valued payoffs, and “stable” network structures are here characterized by considering the concept of Pareto–Nash equilibrium and its refinements. © 2009 Wiley Periodicals, Inc.	flow network;network formation;one-way function	Giuseppe De Marco	2010	Int. J. Intell. Syst.	10.1002/int.20398	network formation;computer science;artificial intelligence;mathematics;mathematical economics;operations research	AI	-5.340791529301384	-0.24867954456563784	165354
985aedfdb0143e81c92aef2b15c024da663e9ff1	intelligent escalation and the principle of relativity		Escalation is the fact that in a game (for instance in an auction), the agents play forever. The 0, 1-game is an extremely simple infinite game with intelligent agents in which escalation arises. It shows at the light of research on cognitive psychology the difference between intelligence (algorithmic mind) and rationality (algorithmic and reflective mind) in decision processes. It also shows that depending on the point of view (inside or outside) the rationality of the agent may change which is proposed to be called the principle of relativity.	intelligent agent;jeanne w. ross;numerical relativity;privilege escalation;rationality	Pierre Lescanne	2014	CoRR		artificial intelligence;mathematics;mathematical economics	AI	-10.805643220721796	-1.808680249234152	165452
82b317dc8c4e83d903618c93d133230315e6ab37	conformity versus manipulation in reputation systems	actual opinion;convergence;history;nash equilibrium;motion pictures;cost function;conformity;reputation system;voting;games;manipulation;virtual opinion	In this paper, we consider a reputation system, where a number of individuals express their opinions, modeled by discrete scalars in the interval [0,1], about an object and the object's score (reputation) is determined as the arithmetic mean of all expressed opinions. An individual's expressed opinion may or may not be consistent with her actual opinion, a continuous scalar in [0,1], for a variety of reasons. In this paper, we address in a unified, game-theoretic framework the influence of two opposing social behaviors, namely conformity and manipulation, on the outcome of a reputation system. For the purposes of this paper, conformity as a social behavior refers to the tendency of an individual to express an opinion that matches the public opinion, whereas manipulation refers to the tendency of an individual to express an opinion so as to manipulate the public opinion toward her actual opinion.	conformity;game theory;human body weight;markov chain;markov model;reputation system;stationary process	Seyed Rasoul Etesami;Sadegh Bolouki;Angelia Nedic;Tamer Basar	2016	2016 IEEE 55th Conference on Decision and Control (CDC)	10.1109/CDC.2016.7798945	games;conformity;convergence;voting;computer science;nash equilibrium	Robotics	-12.026098184835817	-9.565049675321585	165455
e3197fdb9464582b79de5502aa4f322d547f4fec	computer-supported negotiations: an experimental study of bargaining in electronic commerce	business to business;electronic commerce;negotiation support system;e commerce;nash bargaining solution;negotiation support;laboratory experiment;profitability;b2b e commerce	The expanding business-to-business (B2B) e-commerce market has created a need for firms to negotiate business deals online. Negotiation support tools are likely to play a more critical role in B2B e-commerce. Notwithstanding their importance, the impacts of negotiation support tools (especially automated bargaining agents) are not well understood. This research addresses this gap by conducting a series of laboratory experiments to investigate the impact of web-based electronic messaging, web-based negotiation support systems (NSS), and autonomous electronic bargaining agents (EBA) on the outcomes of a multi-issue, ecommerce negotiation. Two types of bargaining situation were investigated: integrative and distributive bargaining. Negotiation outcomes were assessed using joint profit/utility outcome, contract balance, and the closeness to the efficient/Pareto frontier and the Nash bargaining solution. Findings show that web-based NSS can significantly improve efficiency and fairness in remote integrative negotiations but not in distributive negotiations. EBA were found to achieve outcomes comparable to but not significantly better than unassisted human dyads. Implications for NSS and EBA implementation and research were drawn.	algorithm;autonomous robot;centrality;data logger;experiment;fairness measure;nash equilibrium;pareto efficiency;relevance;types of e-commerce;web application	Khim-Yong Goh;Hock-Hai Teo;Haixin Wu;Kwok Kee Wei	2000			e-commerce;bargaining problem;computer science;microeconomics;world wide web;commerce;profitability index	AI	-5.997524714495547	-8.215643070701312	165482
667b3b95aef51cfe73ff4f05cc3d7917c41335ce	a majoritarian representative voting system		We present an alternative voting system that aims at bridging the gap between proportional representative systems and majoritarian, single winner election systems. The system lets people vote for multiple parties, but then assigns each ballot to a single party. This opens a whole range of possible systems, all representative. We show theoretically that this space is convex. Then among the possible parliaments we present an algorithm to produce the most majoritarian result. We then test the system and compare the results with a pure proportional and a majoritarian voting system showing how the results are comparable with the majoritarian system. Then we simulate the system and show how it tends to produce parties of exponentially decreasing size with always a first, major party. Finally we describe how the system can be used in a context of a parliament made up of two separate houses.	algorithm;bridging (networking);convex function;simulation	Pietro Speroni di Fenizio;Daniele A. Gewurz	2016	CoRR		public relations;parallel voting;preferential block voting	AI	-5.86611690855066	-5.116848542684715	165645
13b0747f90e3b0300b491b0aa67cc625172a1863	learning dynamic preferences in multi-agent meeting scheduling	negotiations;learning machines;symposia;multi agent systems;dynamic scheduling calendars processor scheduling training data computer science decision trees learning meetings intelligent agent data mining;scheduling multi agent systems;scheduling;software tools;adaptive training;spread out preferences dynamic preferences learning multiagent meeting scheduling systems static time of day preferences back to back preferences	Multi-agent meeting scheduling systems in which each person has an agent that negotiates with other agents to schedule meetings have the potential to save computer users large amounts of time. Such agents need to model the scheduling preferences of their users. We consider that a user's preferences over meeting times are of two kinds: static time-of-day preferences and dynamic preferences which change as meetings are added to a calendar. We present an algorithm that effectively learns static time-of-day preferences, as well as two important classes of dynamic preferences: back-to-back preferences and spread-out preferences (i.e. preferences for having gaps between meetings).	algorithm;crew scheduling;multi-agent system;scheduling (computing);user (computing)	Elisabeth Crawford;Manuela M. Veloso	2005	IEEE/WIC/ACM International Conference on Intelligent Agent Technology	10.1109/IAT.2005.94	real-time computing;simulation;dynamic priority scheduling;computer science;knowledge management;two-level scheduling	AI	-13.198648413372059	-9.05756950419069	165848
4eb8d494a1fa1b028afa80959f6866d2e2ad2b61	deriving consensus for conflict situations with respect to its susceptibility		  In this paper we consider several problems related to susceptibility to consensus for conflict situations. Firstly, we present  the ontology of consensus, which contains information about conflict situations, conflict profiles, and the notion of consensus.  Next we present the notion of susceptibility to consensus for conflict profiles and show some results which allow to answer  the question: When for a conflict profile is it worth to choose a consensus?    		Ngoc Thanh Nguyen;Michal Malowiecki	2004		10.1007/978-3-540-30133-2_157	political science;management science;communication;social psychology	Logic	-14.721509397261299	-1.4055868820307877	165890
e62491ac001fcb6061bca6bfa93793102f9c631f	two-sided market situations with existing contracts	profitability;two sided market	The main aim of this paper is to study two-sided market situations where there are existing contracts which are exogenously given. These existing contracts could come from a previous competitive period or from any other circumstances. In any case, all these existing contracts provide an initial feasible solution for the two-sided market situation, perhaps non optimal in the sense the agents do not obtain jointly the maximum profit that they could get by cooperation. Therefore, the agents could be interested in improving their results through cooperation but taking into account the existing bilateral contracts. Thus, taking as starting point what each agent has got with the existing contracts, they have to distribute among themselves the extra amount that they could get by cooperation. For this kind of cooperative situation we propose different models and prove some results about the nonemptiness of the core and its relationship with the Owen set.		Joaquín Sánchez-Soriano;Vito Fragnelli	2010	Social Choice and Welfare	10.1007/s00355-009-0397-1	economics;operations management;microeconomics;commerce;profitability index	AI	-5.156123647953244	-4.453908625710607	165963
13210c1630dc534aaf9fdfaff4c25979b1fd0854	game theoretic approach to threat prediction and situation awareness	stochastic game theory;belief networks;level 2;decentralized markov game model;bayesian network;threat assessment;game theory;force sensors;information security;bayesian methods;threat prediction;adversarial decision process;decentralized markov game model stochastic game theory threat prediction data fusion joint directors of laboratories dfig model bayesian network adversarial decision process asymmetric threat detection knowledge infrastructure adaptive threats intelligent agent hierarchical entity aggregation;data fusion;performance metric;game theory predictive models stochastic processes intelligent agent data security information security force sensors automation terminology bayesian methods;dfig model;prediction theory;stochastic processes;hierarchical entity aggregation;knowledge infrastructure;adaptive signal detection;stochastic games adaptive signal detection belief networks knowledge based systems markov processes prediction theory sensor fusion;intelligent agent;asymmetric threat detection;situation awareness;joint directors of laboratories;terminology;decision process;predictive models;markov processes;sensor fusion;asymmetric threats;situation assessment;asymmetric threats data fusion game theory situation assessment threat assessment situation awareness;knowledge based systems;adaptive threats;stochastic games;data security;automation	The strategy of data fusion has been applied in threat prediction and situation awareness and the terminology has been standardized by the Joint Directors of Laboratories (JDL) in the form of a so-called JDL data fusion model, which currently called DFIG model. Higher levels of the DFIG model call for prediction of future development and awareness of the development of a situation. It is known that Bayesian network is an insightful approach to determine optimal strategies against asymmetric adversarial opponent. However, it lacks the essential adversarial decision processes perspective. In this paper, a highly innovative data-fusion framework for asymmetric-threat detection and prediction based on advanced knowledge infrastructure and stochastic (Markov) game theory is proposed. In particular, asymmetric and adaptive threats are detected and grouped by intelligent agent and hierarchical entity aggregation in level 2 and their intents are predicted by a decentralized Markov (stochastic) game model with deception in level 3. We have verified that our proposed algorithms are scalable, stable, and perform satisfactorily according to the situation awareness performance metric	adversary (cryptography);algorithm;bayesian network;cpu cache;game theory;intelligent agent;markov chain;scalability;simulation;threat (computer)	Genshe Chen;Dan Shen;Chiman Kwan;Jose B. Cruz;Martin Kruger;Erik Blasch	2006	2006 9th International Conference on Information Fusion	10.1109/ICIF.2006.301670	game theory;simulation;computer science;information security;machine learning;data mining;sensor fusion;intelligent agent;statistics	AI	-17.674093766913455	-5.526003968632428	166548
f6f577cc286d757fb8ce497aa9cd6c6f79377415	an adaptable human-agent collaboration information system in manufacturing (hacism)	learning;neural nets;adaptable human agent collaboration;uncertainty handling;manufacturing data processing software agents user interfaces textile industry uncertainty handling learning artificial intelligence decision support systems fuzzy logic neural nets;software agents;fuzzy logic;garment manufacturing industry;business environment;collaboration information systems clothing intelligent agent manufacturing industries management information systems artificial neural networks fuzzy logic programming application software;intelligent agents;end users;beliefs desires and intentions;manufacturing data processing;decision support systems;uncertainty handling adaptable human agent collaboration learning information system intelligent agents deliberative agent theories beliefs desires and intentions knowledge base garment manufacturing industry end users decision making rules fuzzy logic artificial neural network;intelligent agent;human agent interaction;manufacturing industry;profitability;information system;learning artificial intelligence;user interfaces;rules;textile industry;artificial neural network;deliberative agent theories;neural network;knowledge base	Intelligent agents are built based on theories, architectures and languages. The basic deliberative agent theories include beliefs, desires and intentions. However the deliberative agents lack flexibility to manage unseen problems. It is because the existing knowledge base does not cover the unanticipated circumstances. This research project extends and enhances the features of the deliberative agents, demonstrating the usefulness applied to the manufacturing industry. This paper focuses on the human-agent interactions and how management can optimise profit for the organisation. The garment manufacturing industry in Hong Kong is selected for demonstrating how an existing industry can utilise different types of agents to enhance its transactions and manufacturing procedures to adapt to a dynamic business environment. These agents learn new facts from the end-users to generate suggestions to assist end-users in decision-making for the quota management and material usage by using rules, fuzzy logic and an artificial neural network. Thus, agents can provide flexible human-agent collaboration.	information system	Grace SauLan Loo;Bondic C. P. Tang;Lech J. Janczewski	2000		10.1109/DEXA.2000.875064	fuzzy logic;end user;textile industry;computer science;knowledge management;artificial intelligence;software agent;data mining;manufacturing;user interface;information system;artificial neural network;profitability index	Robotics	-10.118090777360498	-9.741741208429314	166581
41983d65d780ebb5ca72eaef9778bf9e4510e583	an improved constraint ordering heuristics for compiling configuration problems		Constraint programming techniques are widely used to model and solve interactive decision problems, an especially configuration problems. In this type of application, the configurable product is described by means of a set of constraint bearing on the configuration variables. The user then interactively solves the CSP by assigning (and possibly, relaxing) the configuration variables according to her preferences. The aim of the system is then to keep the domains of the other variables consistent with these choices. Since maintaining of the global inverse consistency is generally not tractable, the domains are instead filtered according to some level of local consistency, e.g. arc-consistency. In the present work, we aim at offering a more convenient interaction by providing the user with possible alternative values for each of the already assigned variables i.e. the values that could replace the current one without leading to the violation of some constraint. We thus present the new concept of alternative domains in a (possibly) partially assigned CSP. We propose a propagation algorithm that computes all the alternative domains in a single step. Its worst case complexity is comparable with the one of the naive algorithm that would run a full propagation for each variable, but its experimental efficiency is much better.	algorithm;best, worst and average case;cobham's thesis;constraint programming;decision problem;heuristic (computer science);interactivity;local consistency;software propagation;worst-case complexity	Benjamin Matthes;Christoph Zengler;Wolfgang Küchlin	2012			compiler;binary constraint;testbed;heuristics;binary decision diagram;mathematical optimization;mathematics	AI	-7.644691413663898	3.763372597936884	166603
75454c03c9e471db0148265b224f13350c471195	demonstrative reference and cognitive significance	identite;cognitif;philosophy of science;rigid designator;belief;internalisme;demonstratif;descriptivisme;epistemologie;cognitive;croyance;designateur rigide;reference;internalism;descriptivism;meaning;perry j;signification;epistemology;indexicaux;indexicals;identity;demonstrative	my belief, if true, is informative. After all, for all that I know I may have gotten it wrong. For all that I know, the car entering and the car leaving the tunnel, though of the same type, may be numerically different. Yet if my belief is true, it is necessarily true. Everything, I take it, is necessarily identical with itself. And demonstratives designate rigidly. Once their reference is fixed, they – or better, anaphoric expressions depending on them – can be used over and over again to pick out the object referred to, and only it, in all the possible states of affaires in which that object figures.1 Given rigid designation plus each thing’s necessary identity with itself, my cognitively significant demonstrative identity belief, if true, is necessarily true. Partially in order to account for rigid designation, the Direct Reference Theory holds that the components of demonstrative thoughts responsible	anaphora (linguistics);information;numerical analysis;rigid designator;tunneling protocol	Ronald Loeffler	2001	Synthese	10.1023/A:1011918626632	philosophy of science;cognition;philosophy;epistemology;belief;meaning;rigid designator;internalism and externalism;linguistic description	AI	-12.400625209049434	3.4043478742245283	166672
2d062e714276e257a03fb4fc3871e098aa24ebaf	resource sharing in multi-agent systems through multi-stage negotiation	location problem;multi agent system;resource sharing	In this paper we try to solve the Temporal Resource Reallocation Problem (TRR-P) in multi-agent systems by having the agents negotiate periods of time during which they can have use of resources. Our work is based on and extends a previous work in which a multistage negotiation framework that defines the way agents negotiate over resources and time is defined. The approach in this framework suggests that the negotiating agents use a sequence of stages, each characterized by an increased chance of success and amount of exchanged information. In the same work two negotiation stages are defined each of which solves a specific range of TRR-P instances. In this paper we propose a third negotiation stage which is more sophisticated than the first two and is able to solve a larger range of TRR-P instances.	multi-agent system;multistage amplifier	Panos Alexopoulos	2007		10.1007/978-0-387-74161-1_7	shared resource;computer science;knowledge management;artificial intelligence;multi-agent system	AI	-10.006690165114366	-6.830161132815868	166783
361131b59291d1e6e2263314bc8fff1f3f5a0c17	triple-acyclicity in majorities based on difference in support	reciprocal preference relations;voting system;voting systems;reciprocal preference relation;majorities based on difference in support;triple acyclicity;d7 analysis of collective decision making	In this paper we study to what extent majorities based on difference in support leads to triple-acyclic collective decisions. These majorities, which take into account voters’ intensities of preference between pairs of alternatives through reciprocal preference relations, require to the winner alternative to exceed the support for the other alternative in a difference fixed before the election. Depending on that difference, i.e., on the threshold of support, and on some requirements on the individual rationality of the voters, we provide necessary and sufficient conditions for avoiding cycles of three alternatives on the collective decision.	directed acyclic graph;rationality;requirement	Bonifacio Llamazares;Patrizia Pérez-Asurmendi	2015	Inf. Sci.	10.1016/j.ins.2014.11.049	economics;public economics;social psychology;welfare economics	Web+IR	-7.677031290508548	-2.610870996132118	167154
402154dcf5097d5d1f84941ebaa701191ef862c8	introducing situational signs in qualitative probabilistic networks	raisonnement probabiliste;reseau probabiliste;computacion informatica;loi probabilite;ley probabilidad;raisonnement qualitatif;reseau probabiliste qualitatif;reseau bayes;probabilistic net;ordered by external client;influence non monotone;red bayes;ciencias basicas y experimentales;probability distribution;bayes network;graphical model;razonamiento calitativo;qualitative reasoning;grupo a;probabilistic reasoning;wiskunde en informatica wiin;probabilistic network	A qualitative probabilistic network is a graphical model of the probabilistic influences among a set of statistical variables in which each influence is associated with a qualitative sign. A non-monotonic influence between two variables is associated with the ambiguous sign ’?’, which indicates that the actual sign of the influence depends on the state of the network. The presence of such ambiguous signs is undesirable as it tends to lead to uninformative results upon inference. In this paper, we argue that, in each specific state of the network, the effect of a non-monotonic influence is unambiguous. To capture the current effect of the influence, we introduce the concept of situational sign. We show how situational signs can be used upon inference and how they are updated as the state of the network changes. By means of a real-life qualitative network in oncology, we show that the use of situational signs can effectively forestall uninformative results upon inference.	graphical model;non-monotonic logic;problem domain;real life;situational application	Janneke H. Bolt;Linda C. van der Gaag;Silja Renooij	2005	Int. J. Approx. Reasoning	10.1016/j.ijar.2004.05.009	probability distribution;qualitative reasoning;computer science;artificial intelligence;machine learning;bayesian network;mathematics;graphical model;probabilistic logic;statistics	AI	-18.927074488549916	-0.7375265625359912	167169
d36f356b88a893eef050315173d3a2eda08d67f7	discretionary rewards as a feedback mechanism	feedback mechanism;moral hazard;universiteitsbibliotheek;information content;discretionary rewards feedback self confidence subjective performance moral hazard	This paper studies the use of discretionary rewards in a finitely repeated principal-agent relationship with moral hazard. The key aspect is that rewards have informational content. When the principal obtains a private subjective signal about the agentu0027s performance, she may pay discretionary bonuses to provide credible feedback to the agent. In accordance with the often observed compression of ratings, we show that in equilibrium the principal communicates the agentu0027s interim performance imperfectly, i.e., she does not fully differentiate good and bad performance. Furthermore, we show that small rewards can have a large impact on the agentu0027s effort, provided that the principalu0027s stake in the project is small.	discretionary access control;feedback	Anton Suvorov;Jeroen van de Ven	2009	Games and Economic Behavior	10.1016/j.geb.2009.03.002	public relations;actuarial science;self-information;economics;feedback;statistics	ECom	-5.654043195135441	-6.288841917279803	167276
143afcaedc97a555583e293000122088d3bb6792	solving distributed csps probabilistically	naming game;distributed constraint satisfaction problem	Constraint solving problems (CSPs) are the formalization of a large range of problems that emerge from computer science. The solving methodology described here is based on the naming game. The two main features that distinguish this methodology from most distributed constraint solving problem (DCSPs) methods are: the system can react to small instance changes, and it does not require pre-agreed agent/variable ordering. The naming game was introduced to represent N agents that have to bootstrap an agreement on a name to give to an object. The agents do not have a hierarchy, and use a minimal protocol. Still they converge to a consistent state by using a distributed strategy. For this reason, the naming game can be used to untangle DCSPs. It was shown that a distributed system of uniform finite state machines does not solve the ring ordering problem in all the algorithm executions. Our algorithm is a distributed uniform system of agents able to perform random decisions when presented with equivalent alternatives. We show that this algorithm solves the ring ordering problem with a probability one.		Stefano Bistarelli;Giorgio Gosti	2010	Fundam. Inform.	10.3233/FI-2010-358	computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;distributed computing;algorithm	AI	-13.01415092430536	-1.957449143955247	167661
4ad2396971d4f44283d77a17e7a3eeaa601df0f5	mechanism design		A social choice function: A function that determines a social choice according to players’ preferences over the different possible alternatives. A mechanism: a game in incomplete information, in which player strategies are based on their private preferences. A mechanism implements a social choice function f if the equilibrium strategies yield an outcome that coincides with f . Dominant strategies: An equilibrium concept where the strategy of each player maximizes her utility, no matter what strategies the other players choose. Bayesian-Nash equilibrium: An equilibrium concept that requires the strategy of each player to maximize the expected utility of the player, where the expectation is taken over the types of the other players. VCG mechanisms: A family of mechanisms that implement in dominant strategies the social choice function that maximizes the social welfare.	expected utility hypothesis;nash equilibrium	Ron Lavi	2009		10.1007/978-0-387-30440-3_327		ECom	-5.865468489026309	-2.758106229166907	167698
26b6db953f3547402d15b40f75f5db17ef200689	vote buying detection via independent component analysis		Electoral fraud can be committed along several stages. Different tools have been applied to detect the existence of such undesired actions. One particular undesired activity is that of vote-buying. It can be thought of as an economical influence of a candidate over voters that in other circumstances could have decided to vote for a different candidate, or not to vote at all. Instead, under this influence, some citizens cast their votes for the suspicious candidate. We propose in this contribution that intelligent data analysis tools can be of help in the identification of this undesired behavior. We think of the results obtained in the affected ballots as a mixture of two signals. The first signal is the number of votes for the suspicious candidate, which includes his/her actual supporters and the voters affected by an economic influence. The second mixed signal is the number of citizens that did not vote, which is affected also by the bribes or economic incentives. These assumptions allows us to apply an instance of blind source separation, independent component analysis, in order to reconstruct the original signals, namely, the actual number of voters the candidate may have had and the actual number of no voters. As a case of study we applied the proposed methodology to the case of presidential elections in Mexico in 2012, obtained by analyzing public data. Our results are consistent with the findings of inconsistencies through other electoral forensic means.	independent component analysis	Antonio Neme;Omar Neme	2016		10.1007/978-3-319-46349-0_20	data mining;artificial intelligence;independent component analysis;pattern recognition;blind signal separation;computer science;electoral fraud;presidential system;incentive	ML	-5.059938431719009	-7.300825621475962	168043
3d6c3534e3e7033cf42b015cefbccc9f172435f3	platform competition as network contestability	competition;platform;ambiguity;market intermediation;two sided market;middlemen	Recent research in industrial organisation has investigated the essential place that middlemen—viewed as platform providers—have in the networks that make up our global economy. In this paper we aempt to understand how such middlemen compete with each other. We model a purposely abstract and reduced model of one middleman who provides a two-sided platform, mediating surplus-creating interactions between two users. e middleman evaluates uncertain outcomes under positional ambiguity, taking into account the possibility of the emergence of an alternative middleman oering intermediary services to the two users. We ndmany situations in which themiddlemanwill purposely extract maximal gains from her position. Only if there is relatively low probability of devastating loss of business under competition, themiddlemanwill adopt amore competitive aitude and extract less from her position.	ambiguity function;emergence;game theory;industrial pc;interaction;maximal set	Robert P. Gilles;Dimitrios Diamantaras	2013	CoRR		industrial organization;competition;economics;microeconomics;economy;mathematical economics;platform;commerce	AI	-6.982006822983554	-7.976378480022148	168956
77aa4b6c78907e35cdbcafda4540dbd90575f352	the dot bombs	security engineering;information security;financial market;profitability	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Jeffrey L. Ott	2001	Information Systems Security	10.1201/1086/43316.10.3.20010701/31722.1	security through obscurity;security engineering;computer science;information security;corporate security;computer security;financial market;profitability index	Robotics	-15.276849186911289	-5.556595525364909	169230
1ca7ad969593e300c5de47f2f41e1c74ad7fea24	a simple budget-balanced mechanism		In the private values single object auction model, we construct a  satisfactory mechanism—a dominant strategy incentive compatible and budget-balanced mechanism satisfying equal treatment of equals. Our mechanism allocates the object with positive probability to only those agents who have the highest value and satisfies ex-post individual rationality. This probability is at least \((1-\frac{2}{n})\), where n is the number of agents. Hence, our mechanism converges to efficiency at a linear rate as the number of agents grow. Our mechanism has a simple interpretation: a fixed allocation probability is allocated using a second-price Vickrey auction whose revenue is redistributed among all the agents in a simple way. We show that our mechanism maximizes utilitarian welfare among all satisfactory mechanisms that allocate the object only to the highest-valued agents.		Debasis Mishra;Tridib Sharma	2018	Social Choice and Welfare	10.1007/s00355-017-1078-0	strategic dominance;economics;mathematical economics;vickrey auction;welfare economics;rationality;incentive compatibility;revenue;welfare;microeconomics	ECom	-4.847954549951694	-3.396397412802976	169280
04c60b20fd94546e0517416d8c437dbca529a0d9	monetary discount strategies for real-time promotion campaign	online shopping;user behavior modeling;discount giving strategy;real time promotion	The e↵ectiveness of monetary promotions has been well reported in the literature to a↵ect shopping decisions for products in real life experience [3]. Nowadays, e-commerce retailers are facing more fierce competition on price promotion in that consumers can easily use a search engine to find another merchant selling an identical product for comparing price. We study e-commerce data — shopping receipts collected from email accounts, and conclude that for non-urgent products like books or electronics, buyers are price sensitive and are willing to delay the purchase for better deals. We then present a real-time promotion framework, called the RTP system: a one-time promoted discount price is o↵ered to allure a potential buyer making a decision promptly. To achieve more e↵ectiveness on real-time promotion in pursuit of better profits, we propose two discount-giving strategies: an algorithm based on Kernel density estimation, and the other algorithm based on Thompson sampling strategy. We show that, given a pre-determined discount budget, our algorithms can significantly acquire better revenue in return than classical strategies with simply fixed discount on label price. We then demonstrate its feasibility to be a promising deployment in e-commerce services for real-time promotion.	algorithm;book;e-commerce;email;kernel density estimation;real life;real-time clock;real-time transcription;sampling (signal processing);software deployment;thompson sampling;web search engine	Ying-Chun Lin;Chi-Hsuan Huang;Chu-Cheng Hsieh;Yu-Chen Shu;Kun-Ta Chuang	2017		10.1145/3038912.3052616	thompson sampling;software deployment;data mining;marketing;computer science;profit (economics);kernel density estimation;search engine;revenue	AI	-7.272208069945685	-9.221630377884438	169555
0a31cc1d56961605509f475af0197d9e4f24ecda	using the max-sum algorithm for supply chain formation in dynamic multi-unit environments	supply chain formation;loopy belief propagation	The max-sum loopy belief propagation (LBP) algorithm was shown in [4] to produce strong results in a simple decentralised supply chain formation (SCF) scenario where goods are traded in single units. In this paper, we demonstrate the performance of LBP in a multi-unit SCF scenario with additional constraints. We also provide experimental analysis of LBP’s performance in dynamic scenarios where the properties and composition of participants are altered while the algorithm is running. Our results suggest that LBP continues to produce strong solutions in multi-unit scenarios, and that performance remains solid in a dynamic setting.	algorithm;belief propagation;casio loopy;local binary patterns;software propagation	Michael Winsper;Maria Chli	2012			simulation;computer science;belief propagation	ML	-11.513793455587518	-8.777357477533368	169625
1bf4d388f01025afccc08b0753fa75f781fd74e7	voting with rank dependent scoring rules	scoring rules;ordered weighted average;voting;social choice;computational social choice	Positional scoring rules in voting compute the score of an alternative by summing the scores for the alternative induced by every vote. This summation principle ensures that all votes contribute equally to the score of an alternative. We relax this assumption and, instead, aggregate scores by taking into account the rank of a score in the ordered list of scores obtained from the votes. This defines a new family of voting rules, rank-dependent scoring rules (RDSRs), based on ordered weighted average (OWA) operators, which include all scoring rules, and many others, most of which of new. We study some properties of these rules, and show, empirically, that certain RDSRs are less manipulable than Borda voting, across a variety of statistical cultures and on real world data from skating competitions.	aggregate data;ordered weighted averaging aggregation operator	Judy Goldsmith;Jérôme Lang;Nicholas Mattei;Patrice Perny	2014			social choice theory;voting;data mining;cardinal voting systems;positional voting system;anti-plurality voting;statistics	AI	-7.744125386479986	-3.3748710201069105	169686
01df716d6b4e4fe657c4f35f2ff6bacccc12ff36	intelligent aggregation of purchase orders in e-procurement	entity relationship model;electronic commerce;negotiation engine intelligent aggregation e procurement enterprise procurement cost reduction rule based aggregation engine purchase order processing corporate agreement system;information model;decision aid;procurement;rule based;cost reduction;costs procurement search engines aggregates timing pricing supply chain management supply chains imaging phantoms warranties;satisfiability;purchasing;order processing;negotiation support systems;aggregate demand;negotiation support systems procurement order processing purchasing electronic commerce cost reduction	A large enterprise generates millions of purchase orders (PO) each year buying various types of goods and services. Each PO has a cost associated with it. This cost comprises multiple elements including the price of the good or service, the shipping and handling of the purchase, and the overhead in initiating, generating, tracking, and managing the PO. To reduce the cost of doing business, it is imperative to reduce the total cost of POs in enterprise e-procurement in an automated fashion. One way to reduce enterprise procurement cost is to aggregate demands so that the total cost of a bunch of POs is reduced by a better price, a lowered shipping and handling fee, and a reduced overhead. The cost of goods and services often depend on several factors including volume, timing, and other business objectives. This paper describes an intelligent aggregation approach for automatically aggregating demands to reduce procurement cost in enterprise e-procurement. Our aggregation approach for e-procurement consists of an information model for representing products (goods or services) and representing purchase orders for such products, a corporate agreement system, a negotiation engine, and a rule-based aggregation engine. The information model is based on an extension of the classic entity-relationship model. The extension enables association of rules and constraints with and among attributes. These rules and constraints must be satisfied during PO aggregation and thus ensure the aggregate PO to be consistent with original individual POs. A rule-based aggregation engine examines POs as they arrive and interact with other decision aids to determine whether aggregation of a particular bunch of POs makes any business sense. Aggregation can happen in two business scenarios, one for POs constrained by existing corporate agreements and another for POs to be refined by online negotiations. The aggregation engine interacts with a corporate agreement system to obtain supplier policies in the first scenario. For the second scenario, it interacts with the negotiation engine to obtain supplier's policies during iterations of the negotiation process. Relevant policies are those that define product pricing, shipping and handing, and post-sale sendees as well as warranties and returns. Examples are given to demonstrate how automated intelligent aggregation of purchases is performed and how it reduces cost in enterprise e-procurement.	aggregate data;e-procurement;enterprise resource planning;entity–relationship model;imperative programming;information model;intelligent agent;iteration;logic programming;overhead (computing);procurement;purchasing	Guijun Wang;Stephen Miller	2005	Ninth IEEE International EDOC Enterprise Computing Conference (EDOC'05)	10.1109/EDOC.2005.19	e-commerce;rule-based system;procurement;entity–relationship model;information model;computer science;systems engineering;data mining;database;aggregate demand;management;computer security;order processing;satisfiability	DB	-7.603375859400435	-8.823145297595874	169706
3e78afb58b15c65bb6a2bc674722b0674deab2b1	a runtime goal conflict resolution model for agent systems	goal conflicts runtime goal conflict resolution model agent systems goal oriented agent programming bdi model proactive behaviors simultaneous mode rational agent goal state transition structure goal deliberation mechanism extended event calculus;goal deliberation mechanism;extended discrete event calculus;extended discrete event calculus bdi agent system goal conflict goal deliberation mechanism;bdi agent system;temporal logic mathematical programming multi agent systems;goal conflict	The goal-oriented agent programming based on BDI model is obtaining increasing attentions, because it allows us to design proactive behaviors for an agent. Generally, an agent does pursue multiple goals not only in a sequential way, but in a simultaneous mode. Accordingly, a rational agent requires that the goals pursued by it are consistent and do not conflict or even deadlock with each other. Existing strategies of goal deliberation are still not sufficient to deal with this task. This paper proposes a runtime goal conflict resolution model for agent systems, which consists of a goal state transition structure and a goal deliberation mechanism based on an extended event calculus. Besides the elaboration on the theoretical parts, a case study is also given to show how this model works. Finally the evaluation of the model is introduced, and the comparing result shows that the goal deliberation time of the agent can be decreased and fewer goal conflicts could happen in the runtime.	deadlock;event calculus;rational agent;state transition table	Xiaogang Wang;Jian Cao;Jie Wang	2012	2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2012.56	soft goal;knowledge management;artificial intelligence	AI	-18.02518661846978	-8.57500500678673	169723
120c2f15a5137f858d76d07b6c912b7a0fae4345	analogy, similarity and factors	case base reasoning;weighing values;factors;similarity;analogy;evaluation;rule application	Analogy has been considered in AI and law primarily in relation to reasoning from precedent cases rather than reasoning from statutes. Where a statutory provision does not apply to a case, the principle of e contrario, that if the case is not covered by the rule the negation of the conclusion can be taken as established, has typically been assumed to apply. There are, however, cases where analogy is an appropriate way to bring a case under a statutory rule. In this paper we discuss using analogy in reasoning with states, and where this should be avoided and e contrario followed. Our account will be based on the notion of factors as developed in AI and law case based reasoning.	case-based reasoning	Michal Araszkiewicz	2011		10.1145/2018358.2018372	case-based reasoning;similarity;analogy;artificial intelligence;evaluation;deductive reasoning;algorithm	AI	-13.872623638440192	3.60589396635282	169766
e1575086a92f6070981b445c4ba306728ad6b30a	matching with stochastic arrival	market design;dynamic matching;public housing allocation	We study matching in a dynamic setting, with applications to the allocation of public housing. In our model, objects of different types that arrive stochastically over time must be allocated to agents in a queue. For the case that the objects share a common priority ordering over agents, we introduce a strategy-proof mechanism that satisfies certain fairness and efficiency properties. More generally, we show that the mechanism continues to satisfy these properties if and only if the priority relations satisfy an acyclicity condition. We then turn to an application of the framework by evaluating the procedures that are currently being used to allocate public housing. The estimated welfare gains from adopting the new mechanism are substantial, exceeding $5,000 per applicant.	fairness measure	Neil Thakral	2015		10.1145/2764468.2764483	mechanism design;mathematical optimization;actuarial science;economics;operations management;microeconomics;mathematical economics;welfare economics	ECom	-5.088736462777409	-3.5443087087751306	169864
623fff2fabd24a90340ed16021417a4507f0ede8	lattice theory and the consumer's problem	superextremal;lattice theory;income effects;consumer s problem	This paper explores and explains the application of the lattice theoretic approach to classic comparative statics in consumer theory. Through examples of preferences that are not quasiconcave, or not differentiable, or not continuous, the approach is shown to characterize income effects more powerfully than the standard approach. The underlying partial order is key to applying the method. Therefore, several adapted partial orders are introduced and discussed.		Leonard J. Mirman;Richard Ruble	2008	Math. Oper. Res.	10.1287/moor.1070.0290	mathematical optimization;lattice;mathematics;mathematical economics	Theory	-6.974414940858699	-1.1460155676638009	170288
130cd8f11c579180bdb42b08f10559f4e3c86e1e	measuring voting power in games with correlated votes using bahadur's parametrisation		We introduce a method of measuring voting power in simple voting games with correlated votes using the Bahadur parameterisation. With a method for measuring voting power with correlated votes, we can address a question of practical importance. Given that most of the applied power analysis is carried out with either the Penrose-Banzhaf or the ShapleyShubik measures of power, what happens when you use these two measures in games with correlated votes? Simulations of all possible voting games with up to six players show that both measures tend to overestimate power when the votes are positively correlated. Yet, in most voting scenarios, the Shapley-Shubik index is closer to the probability of criticality than the Penrose-Banzhaf measure. This also holds for the power distribution in the EU Council of Ministers. Based on these simulations, we conclude that, while the PenroseBanzhaf measure may be ideal for designing constitutional assemblies, the Shapley-Shubik index is better suited for the analysis of power distributions beyond the constitutional stage. JEL-Codes: D72	code;computer simulation;self-organized criticality	Serguei Kaniovski;Sreejith Das	2015	Social Choice and Welfare	10.1007/s00355-014-0831-x	public economics;cardinal voting systems;mathematical economics;weighted voting;anti-plurality voting;statistics	ECom	-8.08637223452129	-3.440049035078784	170355
59ee61efaa35afec6a097fea278934d27823e30a	perfect folk theorems. does public randomization matter?	mixed strategy;folk theorem	I consider two player games, where player 1 can use only pure strategies, and player 2 can use mixed strategies. I indicate a class of such games with the property that under public randomization both the discounted and the undiscounted finitely repeated perfect folk theorems do hold, but the discounted theorem does not without public randomization. Further, I show that the class contains games such that without public randomization the undiscounted theorem does not hold, as well as games such that without public randomization the undiscounted theorem does hold.		Wojciech Olszewski	1998	Int. J. Game Theory	10.1007/BF01243200		ECom	-5.408105255786315	-2.2941452780003884	170679
6ac9ba30d42a6188c56a1082dc91cdb512a26f36	reasons for a careful design of fuzzy sets	modelscollectivesdesign	This paper is, basically, a reflection on modeling, starting by asking what happens when the meaning of a predicate P is interpreted by the membership function of a fuzzy set that, labeled P , is designed with the information available to the designer on the use of the predicate in the corresponding universe of discourse. In it, the modification of the meaning that P can suffer by its identification with the membership function of the fuzzy set, is analyzed and discussed. It is argued that what can be done for reaching a least possible modification of the meaning, is just a careful design of the membership function. This reflection is further continued at the level of linguistic variables, fuzzy if-then rules, and rule-based systems. It is concluded that as argued in the case of fuzzy sets, the involved operations in rules and systems should be at least carefully chosen, if not specially designed.	computing with words and perceptions;conditional (computer programming);domain of discourse;dynamical system;fuzzy concept;fuzzy control system;fuzzy logic;fuzzy rule;fuzzy set;logic programming;logical connective;natural language;rule-based system	Enric Trillas;Claudio Moraga	2013		10.2991/eusflat.2013.20	management science	OS	-15.89977400417339	1.6159189917770473	170847
8f6b683b4a2d58eb8a86d0a8d5942bfeaa9ab9da	regret minimization in non-zero-sum games with applications to building champion multiplayer computer poker agents		In two-player zero-sum games, if both players minimize their average external regret, then the average of the strategy profiles converges to a Nash equilibrium. For n-player general-sum games, however, theoretical guarantees for regret minimization are less understood. Nonetheless, Counterfactual Regret Minimization (CFR), a popular regret minimization algorithm for extensiveform games, has generated winning three-player Texas Hold’em agents in the Annual Computer Poker Competition (ACPC). In this paper, we provide the first set of theoretical properties for regret minimization algorithms in non-zero-sum games by proving that solutions eliminate iterative strict domination. We formally define dominated actions in extensive-form games, show that CFR avoids iteratively strictly dominated actions and strategies, and demonstrate that removing iteratively dominated actions is enough to win a mock tournament in a small poker game. In addition, for two-player non-zero-sum games, we bound the worst case performance and show that in practice, regret minimization can yield strategies very close to equilibrium. Our theoretical advancements lead us to a new modification of CFR for games with more than two players that is more efficient and may be used to generate stronger strategies than previously possible. Furthermore, we present a new three-player Texas Hold’em poker agent that was built using CFR and a novel game decomposition method. Our new agent wins the three-player events of the 2012 ACPC and defeats the winning three-player Email address: rggibson@cs.ualberta.ca (Richard Gibson) URL: http://cs.ualberta.ca/~rggibson/ (Richard Gibson) Preprint submitted to Artificial Intelligence May 2, 2013 ar X iv :1 30 5. 00 34 v1 [ cs .G T ] 3 0 A pr 2 01 3 programs from previous competitions while requiring less resources to generate than the 2011 winner. Finally, we show that our CFR modification computes a strategy of equal quality to our new agent in a quarter of the time of standard CFR using half the memory.	algorithm;artificial intelligence;best, worst and average case;counterfactual conditional;dominating set;email;iterative method;mock object;nash equilibrium;regret (decision theory)	Richard Gibson	2013	CoRR		simulation;artificial intelligence;operations management;mathematical economics;algorithm;regret	AI	-4.935494771088904	1.560672685845614	170894
76f93eea0771346f30a9ce4a000cc24d6cf29a6b	a relation between nash equilibria and correlated equilibria	maastricht university;correlated equilibria;nash equilibria;digital archive;projection;open access;publication;scientific;institutional repository	In this paper we investigate the set of correlated equilibria of bimatrix games. These equilibria are interesting, because they can result in outcome profiles that are not feasible as a result of Nash equilibria. After giving an example to illustrate the various concepts, we present a Projection Theorem which relates the two types of equilibria. Some lemmas are provided to clarify and extend this theorem.		Ruud Hendrickx;Ronald J. A. P. Peeters;Jos A. M. Potters	2002	IGTR	10.1142/S021919890200077X	econometrics;best response;economics;projection;publication;correlated equilibrium;mathematical economics;operations research;nash equilibrium	ECom	-6.1691511948360205	-1.0493718131863268	171387
1ccb3a0f2d96466158ccd75f6621be16ae7cf163	age-related differences in eye tracking and usability performance: website usability for older adults		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	data web;data visualization;eye tracking;francis;fastest;hyperlink;internet;next-generation network;peripheral;primary source;relevance;the quality of life;usability testing;user experience;web 2.0;web page;web usability	Jennifer C. Romano Bergstrom;Erica L. Olmsted-Hawala;Matt E. Jans	2013	Int. J. Hum. Comput. Interaction	10.1080/10447318.2012.728493	human–computer interaction;multimedia;world wide web	Mobile	-16.19002194553118	-6.026829871341924	171658
c0637e3d7310c765a02f6c47f518118f025594e5	set of experience knowledge structure (soeks) and decisional dna (ddna): past, present and future	set of experience knowledge set soeks;decisional dna ddna;virtual engineering object;industrial and manufacturing design	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	dna computing;francis;primary source	Syed Imran Shafiq;Cesar Sanín;Edward Szczerbicki	2014	Cybernetics and Systems	10.1080/01969722.2014.874830	knowledge management;artificial intelligence;management science	Robotics	-15.288245534262328	-5.025153855506908	171981
a512d78dcfcf07228a06e8ef0c4d94c2539dd4c7	non-additive anonymous games	nash equilibrium;uncertainty;qa mathematics;existence theorem;set valued mapping;nash equilibria;hb economic theory;anonymous game;model uncertainty;upper semicontinuous;non additive measure;capacity	This paper introduces a class of non-additive anonymous games where agents are assumed to be uncertain (in the sense of Knight) about opponents’ strategies and about the initial distribution over players’ characteristics in the game. We model uncertainty by non-additive measures or capacities and prove the Cournot-Nash equilibrium existence theorem for this class of games. Equilibrium distribution can be symmetrized under milder conditions than in the case of additive games. In particular, it is not required for the space characteristics to be atomless under capacities. The set-valued map of the Cournot-Nash equilibria is upper-semicontinuous as a function of initial beliefs of the players for non-additive anonymous games.	approximation;fear, uncertainty and doubt;map;multi-agent system;nash equilibrium;semi-continuity;uncertainty principle;utility functions on indivisible goods	Roman Kozhan	2011	Int. J. Game Theory	10.1007/s00182-010-0235-9	game theory;epsilon-equilibrium;mathematical optimization;best response;economics;folk theorem;repeated game;mathematics;mathematical economics;welfare economics;equilibrium selection;nash equilibrium	ECom	-5.001831918334885	-1.50661203368451	172066
dad28061a6d21350a913836558658e6cb6773a8e	isomorphic strategy spaces in game theory	non cooperative game theory;c02 mathematical methods;isomorphic probability spaces;c72 noncooperative games	This book summarizes ongoing research introducing probability space isomorphic mappings into the strategy spaces of game theory. This approach is motivated by discrepancies between probability theory and game theory when applied to the same strategic situation. In particular, probability theory and game theory can disagree on calculated values of the Fisher information, the log likelihood function, entropy gradients, the rank and Jacobian of variable transforms, and even the dimensionality and volume of the underlying probability parameter spaces. These differences arise as probability theory employs structure preserving isomorphic mappings when constructing strategy spaces to analyze games. In contrast, game theory uses weaker mappings which change some of the properties of the underlying probability distributions within the mixed strategy space. Here, we explore how using strong isomorphic mappings to define game strategy spaces can alter rational outcomes in simple games . Specific example games considered are the chain store paradox, the trust game, the ultimatum game, the public goods game, the centipede game, and the iterated prisoner's dilemma. In general, our approach provides rational outcomes which are consistent with observed human play and might thereby resolve some of the paradoxes of game theory.	fisher information;game theory;gradient;iteration;jacobian matrix and determinant;prisoner's dilemma;spaces	Michael J. Gagen	2013	CoRR		bondareva–shapley theorem;non-cooperative game;combinatorial game theory;implementation theory;game theory;minimax;positive political theory;combinatorics;example of a game without a value;simultaneous game;repeated game;mathematics;strategy;screening game;normal-form game;simulations and games in economics education;mathematical economics;algorithmic game theory;sequential game;symmetric game	ECom	-8.243748084322613	0.4275326168461718	172097
c717be627097cda603b6916405fb8f761bae0464	the effect of an extra object on the linguistic apprehension of the spatial relationship between two objects	spatial relationships	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Takatsugu Kojima;Takashi Kusumi	2006	Spatial Cognition & Computation	10.1207/s15427633scc0602_2	spatial relation;psychology;computer vision;computer science;mathematics;communication;social psychology	Robotics	-15.270809304211705	-6.013300544486553	172123
460921850288a7ea2a0b9988bbfe6f31bccd5814	in the long-run we are all dead: on the benefits of peer punishment in rich environments	altruistic punishment;counter punishment;public good game;330 wirtschaft;feuds;working paper	We investigate whether peer punishment is an e¢ cient mechanism for enforcing cooperation in an experiment with a long time horizon. Previous evidence suggests that the costs of peer punishment can be outweighed by the benets of higher cooperation, if (i) there is a su¢ ciently long time horizon and (ii) punishment cannot be avenged. However, in most instances in daily life, when individuals interact for an extended period of time, punishment can be retaliated. We use a design that imposes minimal restrictions on who can punish whom or when, and allows participants to employ a wide range of punishment strategies including retaliation of punishment. Similar to previous research, we nd that, when punishment cannot be avenged, peer punishment leads to higher earnings relative to a baseline treatment without any punishment opportunities. However, in the more general setting, we nd no evidence of group earnings increasing over time relative to the baseline treatment. Our results raise questions under what conditions peer punishment can be an e¢ cient mechanism for enforcing cooperation. JEL Classication: C92, D70, H41		Dirk Engelmann;Nikos Nikiforakis	2015	Social Choice and Welfare	10.1007/s00355-015-0884-5	socioeconomics;social psychology;welfare economics	ML	-5.969998961589122	-6.580627529791643	172215
434f6754dc131e1af272e8a94c516a579089e5d2	interactive reconfiguration in power supply restoration	diagrama binaria decision;diagramme binaire decision;restoration;satisfiability;power supply;resolucion problema;alimentation electrique;restauration;alimentacion electrica;problem solving;resolution probleme;binary decision diagram;restauracion	Interactive configuration is the problem of assisting a user in selecting values for parameters that respect given constraints. The problem was introduced as a problem of product configuration with the emergence of the mass-customization paradigm in product manufacturing but has also been applied to other application areas. Examples include specifying a product (a PC or a car), a service (a plane ticket or an insurance) or setting up equipment (a VCR or heating controller). Intuition is that in these situations, there is no definable unique best solution, and therefore a user should instead be guided in selecting the appropriate values for the parameters while at the same time obeying the constraints and meeting user preferences. The guidance takes the form of immediate feedback on the consequences of choices. There are three main important features required of an implementation of interactive configuration: It should be complete, backtrack-free, and provide real-time performance. It is a computational challenge to obtain all three simultaneously. In this paper we look at interactive reconfiguration, where the starting point is a full valid configuration, which for external reasons becomes inconsistent and therefore has to be changed back to a consistent configuration. We take the approach of determining a small set of parameters that need to be changed and on these perform interactive configuration to get back to a consistent configuration. We present two BDD-based precompilation algorithms for solving the problem. One based on a monolithic BDDrepresentation of the solution space and another using a set of BDDs. We carry out experiments on a set of power supply restoration benchmarks and show that the set-of-BDDs algorithm scales well. In fact, we are able to perform interactive reconfiguration on examples where interactive configuration is not possible due to explosions in the size of the corresponding monolithic BDDs. This shows that even systems that are too large for full interactive configuration could be amenable to reconfiguration.	algorithm;backtrack;bitonic sorter;circuit restoration;compiler;computation;constraint (mathematics);emergence;experiment;feasible region;heuristic (computer science);knowledge-based configuration;obedience (human behavior);personal computer;power supply;programming paradigm;real-time clock;requirement;response time (technology);scalability;user (computing);videocassette recorder	Tarik Hadzic;Henrik Reif Andersen	2005		10.1007/11564751_61	computer science;artificial intelligence;mathematics;binary decision diagram;algorithm;satisfiability	Robotics	-7.8141666913941545	3.3578538383202607	172230
4d2c74a6de4e8ddc1ac53faeed741c03df1b6a39	necessitarian propositions	semantics;propositions;times;worlds	The eternalist holds that all propositions specify the needed time information, and so are eternally true if true at all. The necessitarian holds the parallel view for worlds: she holds that all propositions specify the needed world information, and so are necessarily true if true at all. I will argue that the considerations for both views run parallel: the necessitarian can mimic the whole case for eternalism.	modal logic	Jonathan Schaffer	2012	Synthese	10.1007/s11229-012-0097-8		ML	-13.229310209940381	4.109589821664236	172515
d5c3609caacadbc8a7d44b0cb1e1c1ca705b901a	representational innovation and mathematical ontology	realism;objet;representation;ontologie;mathematics;naturalism;innovaciňn;metaphysique;object;naturalisme;infinitesimal;integral;innovation;matematicas;integrale;realisme;metaphysics;ontologia;ontology;mathematiques;metafisica	Notorious difficulties confront those who study the ontology of mathematics. On the one hand, there is the ancient and widespread intuition that, if mathematical objects are real, they must exist wholly outside the physical world as eternal and immutable abstract entities. On the other hand, the history of mathematics has seen profound changes of consensus as to which mathematical objects exist and which are the most general, fundamental, and permissible. Reactions to this tension between our intuitions and our heritage vary. Some resolve it by demarcating mathematical objects sharply from our imperfect knowledge of them: There are such things as mathematical objects, but we cannot know precisely what they are. Current mathematical practice offers the best, though incomplete, guide to the inhabitants of the mathematical universe. Others resolve this tension by decreeing the futility of mathematical ontology: There are no mathematical objects as such, only mathematical methods. Current mathematical practice illustrates this fact insofar as it calls our attention towards inferential structure and away from mathematical content. Despite significant and obvious differences, both resolutions (as sketched here) take two premises for granted. First, they both accept the conditional hypothesis that if mathematical entities are real, they must be eternal, immutable, and “causally inert beasts that exist outside space and time” (Resnik 1995, xiii). Second, they both privilege contemporary mathematical practice as a metaphysical touchstone. These two premises are mutually sustaining; rejecting one challenges the plausibility of the other. Study the history of mathematics for ontological insight and we are led to doubt the canon of formal properties attributed to mathematical objects. Deny that the reality of mathematical objects requires the postulation of entities that exist beyond the bounds of lived experience and we are led to study the past for clues as to their nature over time. Perhaps both premises should be rejected. I suggest there are such things as mathematical objects and, further, we really do know, with varying degrees of precision, what they are. Does a robust commitment to mathematical realism commit us to the existence of immutable and eternal	entity;immutable object;inferential programming;plausibility structure;touchstone file	Madeline Muntersbjorn	2003	Synthese	10.1023/A:1022139715092	naturalism;upper ontology;innovation;infinitesimal;integral;philosophy;epistemology;ontology;object;metaphysics;ontology;mathematics;realism;ontology-based data integration;representation;process ontology;suggested upper merged ontology	Web+IR	-12.174400716543076	3.1910154762567107	172538
09fc67d6d1ffc461a5f7b834a2ade44a9cd7ba91	l.s. penrose's limit theorem: proof of some special cases	limit theorems;article letter to editor;ternary weighted games;indexation;majority games;jc political theory;voting power;weighted voting games;limit theorem;large classes	L.S. Penrose was the first to propose a measure of voting power (which later came to be known as ‘the [absolute] Banzhaf (Bz) index’). His limit theorem—which is implicit in his booklet (1952) and for which he gave no rigorous proof—says that in simple weighted voting games (WVGs), if the number of voters increases indefinitely while the quota is pegged at half the total weight, then— under certain conditions—the ratio between the voting powers (as measured by him) of any two voters converges to the ratio between their weights. We conjecture that the theorem holds, under rather general conditions, for large classes of variously defined WVGs, other values of the quota, and other measures of voting power. We provide proofs for some special cases. D 2003 Elsevier B.V. All rights reserved.	automated theorem proving	Ines Lindner;Moshé Machover	2004	Mathematical Social Sciences	10.1016/S0165-4896(03)00069-6	discrete mathematics;economics;arrow's impossibility theorem;penrose square root law;mathematics;mathematical economics;condorcet method;statistics	AI	-7.770251677086031	-3.1235613898367642	172878
fcfa099cbaec147671893f1669ff2a61258b48f6	markov task network: a framework for service composition under uncertainty in cyber-physical systems	hierarchical task networks;cyber physical systems;uncertainty reasoning;markov logic networks;ontology	In novel collaborative systems, cooperative entities collaborate services to achieve local and global objectives. With the growing pervasiveness of cyber-physical systems, however, such collaboration is hampered by differences in the operations of the cyber and physical objects, and the need for the dynamic formation of collaborative functionality given high-level system goals has become practical. In this paper, we propose a cross-layer automation and management model for cyber-physical systems. This models the dynamic formation of collaborative services pursuing laid-down system goals as an ontology-oriented hierarchical task network. Ontological intelligence provides the semantic technology of this model, and through semantic reasoning, primitive tasks can be dynamically composed from high-level system goals. In dealing with uncertainty, we further propose a novel bridge between hierarchical task networks and Markov logic networks, called the Markov task network. This leverages the efficient inference algorithms of Markov logic networks to reduce both computational and inferential loads in task decomposition. From the results of our experiments, high-precision service composition under uncertainty can be achieved using this approach.	algorithm;automated planning and scheduling;cyber-physical system;entity;experiment;hl7publishingsubsection <operations>;hierarchical task network;high- and low-level;inference;inferential theory of learning;mrln gene;markov chain;markov logic network;ontology;physical object;reasoning;service composability principle;synthetic data	Abdul-Wahid Mohammed;Yang Xu;Haixiao Hu;Brighter Agyemang	2016		10.3390/s16091542	computer science;knowledge management;machine learning;ontology;data mining;cyber-physical system;hierarchical task network	AI	-18.746690982452137	-3.047857267233679	173013
37a370e1ec05f4fafa9982efee77a72228dbc8f9	a fuzzy model for it security investments	330 wirtschaft;ddc 330;it security;000 allgemeines wissenschaft;ddc 000;fuzzy model	This paper presents a fuzzy set based decision support model for taking uncertainty into account when making security investment decisions for distributed systems. The proposed model is complementary to probabilistic approaches and useful in situations where probabilistic information is either unavailable or not appropriate to reliably predict future conditions. We first present the specification of a formal security language that allows to specify under which conditions a distributed system is protected against security violations. We show that each term of the security language can be transformed into an equivalent propositional logic term. Then we use propositional logic terms to define a fuzzy set based decision model. This optimization model incorporates uncertainty with regard to the impact of investments on the achieved security levels of components of the distributed system. The model also accounts for budget and security constraints, in order to be applicable in practice.	computer security;decision support system;distributed computing;fuzzy set;mathematical optimization;propositional calculus;term (logic)	Guido Schryen	2010			computer security model;computer science;artificial intelligence;operations management;data mining	Security	-17.542269380573696	-0.628834837427318	173147
c8462b39407d974ef96774670733d79e5b05480c	optimizing locations of social and health care service centers using location-allocation tools				Timo Widbom;Tarmo Lipping	2018		10.3233/978-1-61499-933-1-530		HCI	-12.379069210188286	-7.86535011564534	173177
5b1c0f36272375fbdb1718589bb6172574871da1	multi-criteria decision making with existential rules using repair techniques		In this paper, we explain how to benefit from the reasoning capabilities of existential rules for modelling an MCDM problem as an inconsistent knowledge base. The repairs of this knowledge base represent the maximally consistent point of views and inference strategies can be used for decision making.		Nikos Karanikolas;Madalina Croitoru;Pierre Bisquert;Christos Kaklamanis;Rallou Thomopoulos;Bruno Yun	2018		10.1007/978-3-030-04191-5_15	knowledge representation and reasoning;multiple-criteria decision analysis;knowledge base;existentialism;inference;computer science;artificial intelligence	AI	-18.81302633799514	1.5345754478319666	173888
879844ca1e5f163f3503eb57fc532ef61e2e3777	influence-based policy abstraction for weakly-coupled dec-pomdps	sequential decision making;multiagent planning under uncertainty;interaction structure	Decentralized POMDPs are powerful theoretical models for coordinating agents’ decisions in uncertain environments, but the generally-intractable complexity of optimal joint policy construction presents a significant obstacle in applying DecPOMDPs to problems where many agents face many policy choices. Here, we argue that when most agent choices are independent of other agents’ choices, much of this complexity can be avoided: instead of coordinating full policies, agents need only coordinate policy abstractions that explicitly convey the essential interaction influences. To this end, we develop a novel framework for influence-based policy abstraction for weakly-coupled transition-dependent Dec-POMDP problems that subsumes several existing approaches. In addition to formally characterizing the space of transition-dependent influences, we provide a method for computing optimal and approximately-optimal joint policies. We present an initial empirical analysis, over problems with commonly-studied flavors of transition-dependent influences, that demonstrates the potential computational benefits of influence-based abstraction over state-of-the-art optimal policy search methods.	autonomous decentralized system;partially observable markov decision process	Stefan J. Witwicki;Edmund H. Durfee	2010			simulation;computer science;knowledge management;artificial intelligence;management science	AI	-17.395413325625956	-9.130246639809714	174112
db8956541a67c4b305641d17a65c85c36bd2be5f	quantitative and qualitative information in computations		In this paper two notions of information content for the characteristic sequences of sets are compared. One is the minimal-program complexity of the sequences and represents a quantitative information content, and the other is the degree of unsolvability of the underlying set and represents a qualitative information content. The major conclusion from this work is that with few exceptions these measures of information content are unrelated. Various tradeotis between these measures are also demonstrated.	self-information;turing degree	Robert P. Daley	1980	Information and Control	10.1016/S0019-9958(80)90615-4	discrete mathematics;theoretical computer science;mathematics;algorithm	AI	-9.631470528895996	3.959635831142696	174141
bba77e0b867727f137f2adb72cd580e06c238159	quality dimensions in e-commerce software tools: an empirical analysis of north american and japanese markets	electronic commerce software tools;software tool;empirical analysis;e commerce;north american;customer satisfaction;cross cultural study;bayesian analysis	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	e-commerce;francis;formal specification;primary source	Mayuram S. Krishnan;Ramanath Subramanyam	2004	J. Org. Computing and E. Commerce	10.1207/s15327744joce1404_1	e-commerce;economics;bayesian probability;computer science;marketing;advertising;customer satisfaction;world wide web;commerce	Robotics	-15.34153621464022	-5.223705962878326	174350
369012c4b8342a25c140835f14f63b57d4aeb635	learning relational decision trees for guiding heuristic planning	evaluation function;decision tree;scaling up;hill climbing;off the shelf;domain specificity;problem solving	The current evaluation functions for heuristic planning are expensive to compute. In numerous domains these functions give good guidance on the solution, so it worths the computation effort. On the contrary, where this is not true, heuristics planners compute loads of useless node evaluations that make them scale-up poorly. In this paper we present a novel approach for boosting the scalability of heuristic planners based on automatically learning domain-specific search control knowledge in the form of relational decision trees. Particularly, we define the learning of planning search control as a standard classification process. Then, we use an off-theshelf relational classifier to build domain-specific relational decision trees that capture the preferred action in the different planning contexts of a planning domain. These contexts are defined by the set of helpful actions extracted from the relaxed planning graph of a given state, the goals remaining to be achieved, and the static predicates of the planning task. Additionally, we show two methods for guiding the search of a heuristic planner with relational decision trees. The first one consists of using the resulting decision trees as an action policy. The second one consists of ordering the node evaluation of the Enforced Hill Climbing algorithm with the learned decision trees. Experiments over a variety of domains from the IPC test-benchmarks reveal that in both cases the use of the learned decision trees increase the number of problems solved together with a reduction of the time spent.	algorithm;computation;decision tree;evaluation function;heuristic (computer science);hill climbing;scalability	Tomás de la Rosa;Sergio Jiménez Celorrio;Daniel Borrajo	2008			mathematical optimization;computer science;artificial intelligence;hill climbing;machine learning;decision tree;evaluation function;data mining;mathematics;algorithm	AI	-18.05796817111633	-6.578719177881976	174438
2f71576d248c0e193443f8b48f370a067744bd9c	automated negotiation	automated negotiation	tion engine then returns a list of products that satisfy all of the shopper’s hard constraints in order of how well they satisfy the shopper’s soft constraints. Tête-à-Tête uses comparable techniques to recommend complex products based on multiattribute utility theory. However, unlike PersonaLogic, Tête-à-Tête also assists buyers and sellers in the merchant-brokering and negotiation stages. Like PersonaLogic, Firefly (www.firefly.com) and other systems based on collaborative filtering [4] help consumers find products (see Figure 1). However, instead of filtering products based on features, Firefly recommends products through an automated “wordof-mouth” recommendation mechanism called “collaborative filtering.” The system first compares a shopper’s product ratings with those of other shoppers. After identifying the shopper’s “nearest neighbors,” or users with similar taste, the system recommends the products the neighbors rated highly but which the shopper may not yet have rated, possi-	collaborative filtering;firefly;utility	Tuomas Sandholm	1999	Commun. ACM	10.1145/295685.295866	theoretical computer science;natural language processing;computer science;negotiation;artificial intelligence	Web+IR	-7.622798307713967	-8.820520051358207	174597
0867ab484621a04958de289cbafdcea993505736	propensities and frequencies: inference to the best explanation	abduction;philosophy of science;regularite;explanation;ontologie;explication;hypothetico deductif;frequence;epistemologie;regularity;nomique;induction;nomic;epistemology;hypothetico deductive;propensite;frequency;law of nature;prediction;loi de la nature;ontology;propensity;inference	An approach to inference to the best explanation integrating a Popperianconception of natural laws together with a modified Hempelian account of explanation, one the one hand, and Hacking's law of likelihood (in its nomicguise), on the other, which provides a robust abductivist model of sciencethat appears to overcome the obstacles that confront its inductivist,deductivist, and hypothetico-deductivist alternatives.This philosophy of scienceclarifies and illuminates some fundamental aspects of ontology and epistemology, especially concerning the relations between frequencies and propensities. Among the most important elements of this conception is thecentral role of degrees of nomic expectability in explanation, prediction,and inference, for which this investigation provides a theoretical defense.	abductive reasoning	James H. Fetzer	2002	Synthese	10.1023/A:1019614716405	philosophy of science;prediction;philosophy;epistemology;frequency;ontology;natural law	Vision	-11.447594180107853	3.381775750631454	174625
007edf6d65cff3670adc780d24a6fb1108cbfbec	the cost of segregation in (social) networks		This paper investigates the private provision of public goods in segregated societies. While most research agrees that segregation undermines public provision, the findings are mixed for private provision: social interactions, being strong within groups and limited across groups, may either increase or impede voluntary contributions. Moreover, although efficiency concerns generally provide a rationale for government intervention, surprisingly, little light is shed in the literature on the potential effectiveness of such intervention in a segregated society. This paper first develops an index based on social interactions, which, roughly speaking, measures the welfare impact of income redistribution in an arbitrary society. It then shows that the proposed index vanishes when applied to large segregated societies, which suggests an “asymptotic neutrality” of redistributive policies. JEL classification: C72, D31, H41.		Nizar Allouch	2017	Games and Economic Behavior	10.1016/j.geb.2017.08.012	economic interventionism;public economics;turnover;economics;neutrality;public good;social network;welfare;redistribution of income and wealth	AI	-5.604231803002763	-5.789404444522948	174705
7c5925ba4735689d27299847119e6b8a5b03f629	mixed strategy equilibrium in a downsian model with a favored candidate: a comment	spatial competition;candidate quality;mixed strategies	This note complements Aragones and Palfrey (2002) [2] by providing upper and lower bounds of the equilibrium payoff of the advantaged (disadvantaged) candidate for any symmetric distribution of the median voterʼs ideal policy and any (even or odd) number of equidistant locations. These bounds point to a negative (positive) relationship between the equilibrium payoff of the (dis)advantaged candidate and the uncertainty regarding the median voterʼs preferences.		Dimitrios Xefteris	2012	J. Economic Theory	10.1016/j.jet.2011.11.008	mathematical optimization;mathematics;mathematical economics;welfare economics	ECom	-5.381006759908395	-4.169113778247859	175242
e5f01ff6d05a53a5b3ff1cc2c5add0921a30b3cd	sufficient conditions for the solution existence in general coalition games	game theory;theorie jeu;condition suffisante;condicion vida;coalition;condition existence;sufficient condition;existence condition	On montre que la notion de jeu de coalition convexe peut etre generalisee au niveau de jeu de coalition general		Milan Mares	1985	Kybernetika		game theory;mathematical economics	AI	-6.42789381233996	1.1051885936328183	175269
18e4d576fa2b666e2d8b98b5b892515a77076685	equilibria in bundle-reducing strategies for combinatorial auctions	bundle-reducing strategies;combinatorial auctions;preferred bundle;agent-based computational simulator;combinatorial auction;reduced subset;extensive experimentation;adverse auction result;electronic commerce;software agents	We search for equilibria on a reduced subset of the set of strategies a bidder may use in a combinatorial auction. The strategies allow the bidders to reduce their preferred bundle in response to adverse auction results which are known after each round. Extensive experimentation using an agent-based computational simulator was conducted after which it is recognized that efficiency of the auction can be increased when bidders are allowed to reformulate their bids during the auction.	agent-based model;computation;simulation	Fernando Beltrán	2007	2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Workshops		computer simulation;auction algorithm;simulation;combinatorial auction;computer science;artificial intelligence;software agent;auction theory	AI	-8.567031236214795	-8.497236648799104	175428
78cb2f9c2a47facddaa87159ce762c1c80cea754	preference change	preference;cp-nets;decision making;observing unknown possibilities	Most models of rational action assume that all possible states and actions are pre-defined and that preferences change only when beliefs do. But several decision and game problems lack these features, calling for a dynamic model of preferences: preferences can change when unforeseen possibilities come to light or when there is no specifiable or measurable change in belief. We propose a formally precise dynamic model of preferences that extends an existing static model (Boutilier et al, 2004). Our axioms for updating preferences preserve consistency while minimising change, like Hansson’s (1995). But unlike prior models of preference change, ours supports default reasoning with partial preference information, which is essential to handle decision problems where the decision tree isn’t surveyable. We also show that our model avoids problems for other models of preference change discussed in Spohn (2009).	approximation algorithm;belief revision;cp/m;computation;computational model;decision problem;decision tree;default logic;informatics;lempel–ziv–stac;mathematical model;petri net;reinforcement learning;vocabulary;word lists by frequency	Anaïs Cadilhac;Nicholas Asher;Alex Lascarides;Farah Benamara	2015	Journal of Logic, Language and Information	10.1007/s10849-015-9221-8	artificial intelligence	AI	-18.467233565867833	3.329421045238683	175562
32de4ed81aef85af3cab34fdc46a948386443ef9	solidarity in games with a coalition structure	owen value;solidarity;the two step shapley value;games with a coalition structure;shapley value;games with a coalition structure owen value the two step shapley value solidarity	A new axiomatic characterization of the two-step Shapley value (Kamijo, 2009) is presented based on a solidarity principle of the members of any union: when the game changes due to the addition or deletion of players outside the union, all members of the union will share the same gains/losses.		Emilio Calvo;Esther Gutiérrez	2010	Mathematical Social Sciences	10.1016/j.mathsocsci.2010.08.004	economics;shapley value;mathematical economics;market economy;welfare economics	ECom	-5.938607404855649	-2.2018271635072546	175631
5f6d67cb6bd1455b644f3f98f9c28662d61a02a2	heterogeneous mixture-of-experts for fusion of locally valid knowledge-based submodels		Real-world applications often require the joint use of datadriven and knowledge-based models. While data-driven models are learned from available process data, knowledge-based models are able to provide additional information not contained in the data. In this contribution, we propose a method to divide the input space on the basis of the validity ranges of the knowledge-based models. By doing so they are only active in those domains they are designed for. The data-driven models complete the coverage of the input space. We demonstrate the benefits of our approach on a real-world application for the energy management of a hybrid electric vehicle.	knowledge-based systems	Jörg Beyer;Kai Heesche;Werner Hauptmann;Clemens Otte	2009			machine learning;artificial intelligence;electric vehicle;computer science;energy management;mixture of experts	AI	-17.533738262318543	-3.7290042884134116	175845
072eab2ebf42810b052a0f026b363fc3ac075013	a tool for autonomous ground-based rover planning	automated planning;cycle time;resource constraint;prototypes;autonomy;satisfiability;planning and scheduling;automatic generation;proof of concept;performance metric;artificial intelligent;proving;sequencing;temporal constraints;scheduling;automated planning scheduling rover operations artificial intelligence;artificial intelligence;planning	"""This paper discusses a proof-of-concept prototype for ground-based automatic generation of validated rover command sequences from high-level science and engineering activities. This prototype is based on ASPEN, the Automated Scheduling and Planning Environment. This Artificial Intelligence (AI) based planning and scheduling system will automatically generate a command sequence that will execute within resource constraints and satisfy flight rules. Commanding the rover to achieve mission goals requires significant knowledge of the rover design, access to the low-level rover command set, and an understanding of the performance metrics rating the desirability of alternative sequences. It also requires coordination with external events such as orbiter passes and day/night cycles. An automated planning and scheduling system encodes this knowledge and uses search and reasoning techniques to automatically generate low-level command sequences while respecting rover operability constraints, science and engineering preferences,and also adhering to hard temporal constraints.Enabling goal-driven commanding of planetary rovers by engineering and science personnel greatly reduccs the requirements for highly skilled rover engineering personnel and Rover Science Team time. This in turn greatly reduces mission operations costs. In addition, goal-driven commanding permits a faster response to changes in rover state (e.g., faults) or science discoveries by removing the time consuming manual sequence validation process, allowing rapid """"what-iV’ analyses, and thus reducing overall cycle times."""	artificial intelligence;automated planning and scheduling;automation;data validation;high- and low-level;information;instrument flight rules;operability;planetary scanner;prototype;requirement;rover (the prisoner);scheduling (computing)	Rob Sherwood;Andrew Mishkin;Tara A. Estlin;Steve A. Chien;Barbara Engelhardt;Brian K. Cooper;Gregg Rabideau	2001			planning;simulation;cycle time variation;computer science;artificial intelligence;sequencing;prototype;autonomy;proof of concept;scheduling;satisfiability	AI	-19.102485447796845	-7.452361998345946	176188
22b5915acda603b9eed134965f9760e0053ab76f	a non-cooperative interpretation of the f-just rules of bankruptcy problems	bilateral consistency;converse consistency;f -just rule;bankruptcy problem;d63;c72	First, we propose an axiomatic characterization of the f-just rules. Second, based on the result, a game is designed and a non-cooperative interpretation of the f-just rules is provided.		Chih Chang;Cheng-Cheng Hu	2008	Games and Economic Behavior	10.1016/j.geb.2007.10.005	semantics of business vocabulary and business rules;data mining;mathematics;welfare economics	ECom	-7.209814191055188	-0.6410979351672753	176261
12e97cbb5781859e834da8c56fc03ab119ed6126	perfect equilibria in a model of bargaining with arbitration	journal of economic literature	We consider an alternating offer bargaining model in which the players may agree to call in an arbitrator in case of disagreement. The main message of our study is that the mere presence of an arbitrator—who can only become active with the consent of both parties—in the background of negotiations may entirely drive their outcome. We compare our results with those obtained in models with outside options. Journal of Economic Literature classification Number: C78.  2001 Academic Press		Paola Manzini;Marco Mariotti	2001	Games and Economic Behavior	10.1006/game.2000.0823	actuarial science;economics;public economics;microeconomics;mathematical economics	ECom	-6.595679926034483	-4.1854600274035345	176308
ebbb9d2d9cf32e329a0dc48c802afe6888fc7f29	objectivity and proof in a classical indian theory of number	realism;rationalite;prueba;ontogenesis;philosophy of language;logic;semantics;psychology;semantique;arithmetique;objectivite;mathematics philosophy;aritmetica;preuve;philosophy;realisme;epistemology;metaphysics;arithmetic;number;psychologie;ontogenese;nombre;multiple;proof;rationality;india;numero;psicologia;inde;philosophie des mathematiques;objectivity	The theory of number serves as a highly instructive example of the nature of rationality, theory-change and philosophical thinking in India. I will show how the processes of theory rejection and theory modification are driven by perceived explanatory failure rather than by discovery of foundational incoherence. I will argue too that the source of structure, order and pattern is seen to reside in the natural world, and not in any domain of abstract objects or in the empirically unconstrained working of a priori reflection. The discussion of number broaches fundamental questions about the nature of objectivity and the correspondence theory of meaning, and it explores the basic principles underlying the construction of a formal ontology. At the end, I will make some comparisons between Indian philosophers’ theory of number and the Indian mathematicians’ description of the epistemology of mathematics and the nature and function of mathematical proof. The philosophical study of number in India specifically, the theory of the realist Nyāya-Vaiśes. ika school, 1 belongs with the analysis of the structure of empirical knowledge. This theory of number is an account of the semantics and epistemology of contingent numerical judgements, judgements whose subject matter concerns questions of numerical quantity. Such judgements are typically expressed by statements like ‘there are two pots in this room’ or ‘there are nine planets in the solar system’. The study of number is therefore an enquiry into the role of number within empirical and scientific knowledge. Like much classical philosophy in India, the Nyāya-Vaiśes. ika theory of number is broadly empiricist: facts about the numerical properties of objects do not differ in kind from facts about their colour, shape or extension. Number is part of the natural world. While these philosophers generally do not discuss explicitly the nature of mathematical truth, they implicitly favour an empiricist philosophy of	contingency (philosophy);formal ontology;numerical analysis;objectivity/db;rationality;rejection sampling;subject matter expert turing test	Jonardon Ganeri	2001	Synthese	10.1023/A:1013144411940	philosophy;rationality;epistemology;philosophy of language;metaphysics;proof;objectivity;semantics;realism;logic;multiple	AI	-11.880781536220306	3.914795037223364	176367
38964d9f61f8015cc1369bef48c111e44301f172	planning games	computational problem;basic model;strips-like model;local interaction;acyclic interaction structure;automated planning setting;efficient algorithm;stable solution;single-agent planning;game-theoretic solution concept	We introduce planning games, a study of interactions of self-motivated agents in automated planning settings. Planning games extend STRIPS-like models of single-agent planning to systems of multiple self-interested agents, providing a rich class of structured games that capture subtle forms of local interactions. We consider two basic models of planning games and adapt game-theoretic solution concepts to these models. In both models, agents may need to cooperate in order to achieve their goals, but are assumed to do so only in order to increase their net benefit. For each model we study the computational problem of finding a stable solution and provide efficient algorithms for systems exhibiting acyclic interaction structure.	algorithm;automated planning and scheduling;computation;computational problem;directed acyclic graph;game theory;interaction;strips	Ronen I. Brafman;Carmel Domshlak;Yagil Engel;Moshe Tennenholtz	2003			simulation;computer science;artificial intelligence;management science	AI	-10.995295803500943	-5.9569103862275	176462
fb786de67c5e7e216b8b62a8e6c5649f9f2004f4	control costs and potential functions for spatial games	potentials;control costs;evolutionary game;evolutionary games;spatial games;weighted graph;potential function;game playing;markov chain	Van Damme and Weibull (1998, 2000) model the noise in games as endogenously determined tremble probabilities, by assuming that with some effort players can control the probability of implementing the intended strategy. Following their methodology, we derive the logit model for games played on quasi-symmetric weighted graphs and explore the properties of the ensuing Markov chain.		Richard Baron;Jacques Durieu;Hans Haller;Philippe Solal	2003	Int. J. Game Theory	10.1007/s001820300138	combinatorial game theory;game theory;markov chain;mathematics;mathematical economics;sequential game;statistics	AI	-4.648487843992908	-0.7576138671578265	176515
9a20bbda202a9642dc3f54cb7789608eb9fa8631	statistical analysis and data display: an intermediate course with examples in s-plus, r, and sas		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source;s-plus;sas	Theresa L. Utlaut	2005	Technometrics	10.1198/tech.2005.s285	statistics	Robotics	-14.926102555529408	-5.797176510319737	176526
876c08fea372a5ce5ce9df91d150dce248a9f22b	statistics for dummies		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	David L. Rumpf	2004	Technometrics	10.1198/tech.2004.s204	econometrics;mathematics;algorithm	Robotics	-14.767023075213812	-5.701904682604531	176630
5b860e2cace6d4ec32a83db53f474231e63a8427	an approximate subgame-perfect equilibrium computation technique for repeated games	subgame perfect equilibrium;game theory;computational techniques;repeated game;repeated games;finite automata;agent systems;multiagent systems;computation	This paper presents a technique for approximating, up to any precision, the set of subgame-perfect equilibria (SPE) in discounted repeated games. The process starts with a single hypercube approximation of the set of SPE. Then the initial hypercube is gradually partitioned on to a set of smaller adjacent hypercubes, while those hypercubes that cannot contain any point belonging to the set of SPE are simultaneously withdrawn. Whether a given hypercube can contain an equilibrium point is verified by an appropriate mathematical program. Three different formulations of the algorithm for both approximately computing the set of SPE payoffs and extracting players’ strategies are then proposed: the first two that do not assume the presence of an external coordination between players, and the third one that assumes a certain level of coordination during game play for convexifying the set of continuation payoffs after any repeated game history. A special attention is paid to the question of extracting players’ strategies and their representability in form of finite automata, an important feature for artificial agent systems.	algorithm;approximation;automata theory;computation;continuation;finite-state machine;intelligent agent	Andriy Burkov;Brahim Chaib-draa	2010	CoRR		game theory;mathematical optimization;combinatorics;discrete mathematics;economics;computer science;repeated game;mathematics;microeconomics;mathematical economics;subgame perfect equilibrium;algorithm	AI	-4.636919757663033	-2.193056973066606	177171
e266f82446586a71e54c33dab222bb6b881944c8	social influence as a voting system: a complexity analysis of parameters and properties		In this paper we propose a new way to analyze influence in networks. There is a common perception that influence is relevant to determine the global behavior of the social network and thus it can be used to determine some normative behavioral rules. From this point of view we propose to analyze influence in networks as a voting systems. For doing so we introduce a new family of simple games: the influence games whose definition is based on the linear threshold model for influence propagation. We show first that any simple game can be described by an influence game and concentrate in the cases in which such construction can be done in polynomial time. We study computational problems related to parameters and properties of simple games. We characterize the computational complexity of various problems for general influence graphs. Finally, we analyze those problems for some particular extremal cases, with respect to the propagation of influence, showing tighter complexity characterizations.	analysis of algorithms;computational complexity theory;computational problem;social network;software propagation;threshold model;time complexity	Xavier Molinero;Fabián Riquelme;Maria J. Serna	2012	CoRR		simulation;management science	AI	-7.015114483248211	1.6605481514494365	177503
9e0b8f09186caaff753ebec92b6fb7d16c8edf0e	an agent-based framework for assessing missile defense doctrine and policy	mathematics;formal specification;time measurement;formal specification missiles command and control systems multi agent systems;measures of effectiveness;agent based;missile defense doctrine;government;ballistic missile defense;missiles;multi agent systems;agents;permission;command and control;missile defense policy;interaction pattern;performance analysis;ballistic missile defense systems agent based framework missile defense doctrine missile defense policy command and control;missiles weapons command and control systems government timing permission time measurement target tracking mathematics performance analysis;ballistic missile defense systems;target tracking;command and control systems;ballistic missile defense policy;agent based framework;weapons;ballistic missile defense policy agents;timing	BMD-agents (BMDA) is a framework for specifying entities and their interaction patterns used in the command and control (C2) of ballistic missile defense (BMD) systems. In BMDA, doctrine is encoded as rules of individual agents and C2 policies are encoded as combinations of agents. BMDA can be used to analyze measures of effectiveness and performance of BMD policies.	agent-based model;ballistic missile;entity	Duminda Wijesekera;James Bret Michael;Anil Nerode	2005	Sixth IEEE International Workshop on Policies for Distributed Systems and Networks (POLICY'05)	10.1109/POLICY.2005.4	simulation;engineering;operations research;computer security	Metrics	-17.757561041083093	-5.587176242716386	177504
1bbe7afaee8afc912e8a03b66f3b83afa786b930	arrow and gibbard-satterthwaite	proof	This article formalizes two proofs of Arrow’s impossibility theorem due to Geanakoplos and derives the Gibbard-Satterthwaite theorem as a corollary. One formalization is based on utility functions, the other one on strict partial orders. For an article about these proofs see http://www.in.tum.de/∼nipkow/pubs/ arrow.pdf. 1 Arrow’s Theorem for Utility Functions theory Arrow-Utility imports Complex-Main begin This theory formalizes the first proof due to Geanakoplos [1]. In contrast to the standard model of preferences as linear orders, we model preferences as utility functions mapping each alternative to a real number. The type of alternatives and voters is assumed to be finite. typedecl alt typedecl indi axiomatization where alt3 : ∃ a b c::alt . distinct [a,b,c] and finite-alt : finite(UNIV :: alt set) and finite-indi : finite(UNIV :: indi set) lemma third-alt : a 6= b =⇒ ∃ c::alt . distinct [a,b,c] 〈proof 〉 lemma alt2 : ∃ b::alt . b 6= a 〈proof 〉 type-synonym pref = alt ⇒ real type-synonym prof = indi ⇒ pref	axiomatic system	Tobias Nipkow	2008	Archive of Formal Proofs		mathematical economics;arrow;mathematics	Theory	-7.527251571044235	-1.4218845375274862	177570
a672a8aacd57f6b9b848f4652f701ade09dc5393	simulating new markets by introducing new accepting policies for the conventional continuous double auction	trading agents;continuous double auction;multi agent system;stock market;online auction;transaction success rate and cat competition;market design;multi agent systems;autonomous agent;success rate;accepting policy;new york stock exchange	In recent years a huge number of online auctions that use Multi Agent systems have been created. As a result there are numerous auctions that provide the same product. In this case each customer can buy a product with the lowest possible price. But searching between auctions in terms of finding the suitable product can be time consuming for consumers and also providing products in different markets is a difficult task for suppliers. So the need for an autonomous agent in these types of markets is deeply felt. On the other side the structure of an auction mechanism that provides the environment for traders to operate their trades is vital.  Despite all the research that has been done about online auctions, most of them were about single markets. But in real world the stocks and commodities of companies are listed and traded in different markets. There is a growing tendency towards research about online auctions and Market Design. Particularly in recent years CAT (CATallactics) game has provided an important opportunity to develop and test new techniques in this field. In this paper after introducing CAT game and PersianCAT agent, we want to challenge the conventional accepting policy used in stock markets like New York Stock Exchange and provide a better solution that improves the general performance of the markets.	.cda file;autonomous agent;autonomous robot;catallactics;electronic trading;experiment;gd-rom;probabilistic database;traders;traffic sign recognition	Sina Honari;Maziar Gomrokchi;Mojtaba Ebadi;Amin Fos-hati;Jamal Bentahar	2008			variable pricing;combinatorial auction;computer science;auto auction;artificial intelligence;market system;autonomous agent;common value auction;multi-agent system;auction theory;forward auction	ECom	-7.748283749745855	-8.973549041187407	178001
0cf7dbc4d201984f4ae0c3567741c6c6046fc6bf	concepts and fuzzy sets: misunderstandings, misconceptions, and oversights	modelizacion;fuzzy set;computacion informatica;concepts;psychology of concepts;logique floue;conceptual analysis;conjunto difuso;logica difusa;ensemble flou;intelligence artificielle;analisis conceptual;fuzzy set theory;fuzzy sets;fuzzy logic;modelisation;ciencias basicas y experimentales;artificial intelligence;actitud;inteligencia artificial;analyse conceptuelle;grupo a;modeling;attitude	The psychology of concepts has been undergoing significant changes since the early 1970s, when the classical view of concepts was seriously challenged by convincing experimental evidence that conceptual categories never have sharp boundaries. Some researchers recognized already in the early 1970s that fuzzy set theory and fuzzy logic were potentially suitable for modeling of concepts and obtained encouraging results. This positive attitude abruptly changed in the early 1980s, and since that time fuzzy set theory and fuzzy logic have been portrayed as problematic and unsuitable for representing and dealing with concepts. Our aim in this paper is to identify some of the most notorious claims regarding fuzzy set theory and fuzzy logic that have propagated through the literature on psychology of concepts and to show that they are, by and large, false. We trace the origin and propagation of these claims within the literature in this area. It is shown in detail that these claims are consistently erroneous and that they are based on various misunderstandings, misconceptions, and oversights. The ultimate purpose of this paper is to document these various erroneous claims. 2009 Elsevier Inc. All rights reserved.	correctness (computer science);fuzzy logic;fuzzy set;prototype;set theory;software engineer;software propagation	Radim Belohlávek;George J. Klir;Harold W. Lewis;Eileen C. Way	2009	Int. J. Approx. Reasoning	10.1016/j.ijar.2009.06.012	computer science;artificial intelligence;machine learning;mathematics;fuzzy set;algorithm	AI	-17.69570664301799	0.6732006962882314	178170
301abb3da31be7548f963a639c8c7590c44dbfbd	who is afraid of scientific imperialism?		In recent years, several authors have debated about the justifiability of so-called scientific imperialism. To date, however, widespread disagreements remain regarding both the identification and the normative evaluation of scientific imperialism. In this paper, I aim to remedy this situation by making some conceptual distinctions concerning scientific imperialism and by providing a detailed assessment of the most prominent objections to it. I shall argue that these objections provide a valuable basis for opposing some instances of scientific imperialism, but do not yield cogent reasons to think that scientific imperialism in general is objectionable or unjustified. I then highlight three wide-ranging implications of this result for the ongoing philosophical debate about the justifiability of scientific imperialism.	hadamard transform;information;normative social influence;petri net;plausibility structure;shadowrun;word lists by frequency	Roberto Fumagalli	2017	Synthese	10.1007/s11229-017-1411-2	philosophy;epistemology	HPC	-13.894337376738466	2.6594425662492416	178396
74c47eb6939ec72b7f8ecbb26618aa062b62484a	negotiation-based price discrimination for information goods	information good;multiagent system;art;electronic commerce;multi agent system;pricing;system performance;software agents;dynamic pricing;transaction databases;internet;system testing;pricing multiagent systems art internet transaction databases costs system testing system performance electronic commerce software agents;price discrimination;multiagent systems	We present a mechanism for trading database tuples in a multiagent system. The mechanism enables negotiation and evaluation of database-based information goods. As part of the work we propose various policies for dynamic pricing of information goods. We have developed a testbed that simulates a multi-agent system where agents use the offered mechanism and evaluated the system performance when sellers use different pricing policies in competitive and non-competitive environments. The investigated pricing policies include two novel pricing policies that implement negotiation and price discrimination across buyers. These were compared to two policies known in the art, which implement dynamic posted pricing. We have empirically demonstrated the superiority of the offered policies in maximizing sellers' gains. We have additionally identified equilibria profiles of these policies.	agent-based model;electronic markets;experiment;multi-agent system;testbed	Gabi Koifman;Onn Shehory;Avigdor Gal	2004	Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.	10.1109/AAMAS.2004.194	pricing;variable pricing;the internet;computer science;artificial intelligence;software agent;multi-agent system;information good;system testing;price discrimination	AI	-8.300808665396577	-8.812421208711225	178654
52191be6c31fa2f6e8b01ada04354e413a8ea884	vehicular coordination via a safety kernel in the gulliver test-bed	inter vehicle communications;level of service;trajectory planning algorithm;communication capabilities;vehicular systems;vehicular ad hoc networks traffic engineering computing;distributed computer systems;trajectory planning vehicular coordination safety kernel gulliver test bed cooperative vehicular system uncertain intervehicle communication system architectural concept local dynamic map;cooperative communication;information management;intelligent vehicle highway systems;architectural concepts;sensory information;vehicles;local dynamics;equipment testing;vehicles safety kernel trajectory sensors roads vehicle dynamics	Cooperative vehicular systems base their coordination on inherently uncertain inter-vehicle communications. Safe solutions that do not properly manage uncertainty, lead to inefficient outcomes. We consider that cooperative functions can be executed with several service levels, and we use the system architectural concept of safety kernel for managing the service level that achieves the best possible performance while keeping the system safe. We use the Gulliver test-bed for demonstrating the safety kernel concept by means of a pilot system implementation on scaled vehicles with sensors and communication capabilities. The demonstrated architecture incorporates: (1) a local dynamic map (LDM) that uses local and remote sensory information for calculating the location of nearby objects, (2) a safety kernel to manage the service levels, (3) a cooperative level of service evaluator that allows vehicles to reach agreement on a common service level and, finally, (4) a driver manager that executes in accordance to the cooperative level of service when determining how to calculate the trajectory. This paper explains how the different components considered in the architectural concept operate, and shows how it is possible to use (similar to existing) trajectory planning algorithms when implementing the concept.	algorithm;interpreter (computing);kernel (operating system);sensor;test case;testbed	Antonio Casimiro;Oscar Morales Ponce;Thomas Petig;Elad Michael Schiller	2014	2014 IEEE 34th International Conference on Distributed Computing Systems Workshops (ICDCSW)	10.1109/ICDCSW.2014.25	sensory system;simulation;computer science;distributed computing;information management;level of service;computer security;computer network	Robotics	-18.376447095765055	-8.110777666886682	178672
0f641cc7ab54a10fd8d4222c6da0d325d9b6a975	rigorous computer analysis of the chow-robbins game	publikationer;konferensbidrag;artiklar;rapporter	Flip a coin repeatedly, and stop whenever you want. Your payoff is the proportion of heads, and you wish to maximize this payoff in expectation. This so-called Chow-Robbins game is amenable to computer analysis, but while simple-minded number crunching can show that it is best to continue in a given position, establishing rigorously that stopping is optimal seems at first sight to require “backward induction from infinity”. We establish a simple upper bound on the expected payoff in a given position, allowing efficient and rigorous computer analysis of positions early in the game. In particular we confirm that with 5 heads and 3 tails, stopping is optimal. 1 The Chow-Robbins game The following game was introduced by Yuan-Shih Chow and Herbert Robbins [1] in 1964: We toss a coin repeatedly, and stop whenever we want. Our payoff is the proportion of heads up to that point, and we assume that we want to maximize the expected payoff. Basic properties of this game, like the fact that there is an optimal strategy that stops with probability 1, were established in [1]. Precise asymptotical results were obtained by Aryeh Dvoretzky [2] and Larry Shepp [4].	asymptote;backward induction;robbins v. lower merion school district;tails	Olle Häggström;Johan Wästlund	2013	The American Mathematical Monthly	10.4169/amer.math.monthly.120.10.893	operations research	Theory	-11.102242909112137	-2.717300803671304	178702
d5f0ad717cd393028d4ae25403bd64c914f57961	on the price of stability of fractional hedonic games	algorithmic game theory;fractional hedonic games;nash stability;clustering formation	We consider fractional hedonic games, where self-organized groups (or clusters) are created as a result of the strategic interactions of independent and selfish players and the happiness of each player in a group is the average value she ascribes to its members. We adopt Nash stable outcomes, that is states where no player can improve her utility by unilaterally changing her own group, as the target solution concept. We study the quality of the best Nash stable outcome and refer to the ratio of its social welfare to the one of an optimal clustering as to the price of stability. We remark that a best Nash stable outcome has a natural meaning of stability since it is the optimal solution among the ones which can be accepted by selfish users. We provide upper and lower bounds on the price of stability for games played on different network topologies. In particular, we give an almost tight bound (up to a 0.026 additive factor) for bipartite graphs and suitable bounds on more general families of graphs.	algorithmics;closing (morphology);cluster analysis;computation;graph (discrete mathematics);interaction;nash equilibrium;network topology;numerical stability;price of stability;self-organization;unrestricted grammar;utility functions on indivisible goods	Vittorio Bilò;Angelo Fanelli;Michele Flammini;Gianpiero Monaco;Luca Moscardelli	2015			price of stability;epsilon-equilibrium;mathematical optimization;best response;computer science;algorithmic game theory;nash equilibrium	ECom	-4.788069773049611	-0.49528439746259534	178862
3a09f238187f1cb373f166835f87babdac069718	on the benefits of party competition	hb economic theory;incomplete information;strategic voting	Article history: Received 29 March 2006 Available online 30 October 2008 JEL classification: C72 C73 D72 D78 We study the role of parties in a citizen-candidate repeated-elections model in which voters have incomplete information. We first identify a novel “party competition effect” in a setting with two opposing parties. Compared with “at large” selection of candidates, party selection makes office-holders more willing to avoid extreme ideological stands, and this benefits voters of all ideologies. We then allow for additional parties. With strategic voting, citizens benefit most when the only two parties receiving votes are more moderate. With sincere voting, even with three parties, extreme parties can thrive at the expense of a middle party; and whether most citizens prefer two or three parties varies with model parameters. © 2008 Elsevier Inc. All rights reserved.	brookgpu;design rationale;screening effect	Dan Bernhardt;Larissa Campuzano;Francesco Squintani;Odilon Câmara	2009	Games and Economic Behavior	10.1016/j.geb.2008.10.007	split-ticket voting;public relations;economics;public economics;complete information	AI	-5.306153955933648	-5.989574361527005	179276
40abaa554c30effdc439a18d71d16ebe4b3b1d6b	variable temptations and black mark reputations	trust;reputation mechanisms;ratings;reputation	In a world of imperfect information, reputations often guide the sequential decisions to trust and to reward trust. We consider two-player situations, where one player — the truster — decides whether to trust, and the other player — the temptee — has a temptation to betray when trusted. The strength of the temptation to betray varies from encounter to encounter. We refer to a recorded betrayal as a black mark and focus on mechanisms that only reveal the number of black marks of a temptee. We show that the greater the number of black marks, the less likely the temptee is to betray. We then study the different equilibria that emerge, depending on which side of the market has the ability to specify the equilibrium. In closing, we generalize to cases where the number of encounters is also recorded.	closing (morphology);emergentism;reputation	Christina Aperjis;Richard J Zeckhauser;Yali Miao	2014	Games and Economic Behavior	10.1016/j.geb.2014.04.003	public relations;express trust;reputation;advertising;trustworthy computing	ECom	-5.680871110181288	-6.871429488645206	179292
0e43f761d8456e92b2ad118a70b66d6e096b23df	complexity of manipulation, bribery, and campaign management in bucklin and fallback voting	complexity theory;bribery;fallback voting;bucklin voting;campaign management;manipulation;computational social choice;voting theory	A central theme in computational social choice is to study the extent to which voting systems computationally resist manipulative attacks seeking to influence the outcome of elections, such as manipulation (i.e., strategic voting), control, and bribery. Bucklin and fallback voting are among the voting systems with the broadest resistance (i.e., NP-hardness) to control attacks. However, only little is known about their behavior regarding manipulation and bribery attacks. We comprehensively investigate the computational resistance of Bucklin and fallback voting for many of the common manipulation and bribery scenarios; we also complement our discussion by considering several campaign-management problems for these two voting rules.	best, worst and average case;np-hardness;resource bounded measure	Piotr Faliszewski;Yannick Reisch;Jörg Rothe;Lena Schend	2014	Autonomous Agents and Multi-Agent Systems	10.1007/s10458-014-9277-x	bullet voting;cardinal voting systems;computer security;bucklin voting;disapproval voting	AI	-7.405042321634128	1.5672279455529388	179376
45648b4b01cfa81cd3206feaee5b75a39e9cb50d	game-theoretic design of optimal two-sided rating protocols for service exchange dilemma in crowdsourcing		Despite the increasing popularity and successful examples of crowdsourcing, it is stripped of aureole when collective efforts are derailed or severely hindered by elaborate sabotage. A service exchange dilemma arises when there is non-cooperation among self-interested users, and zero social welfare is obtained at myopic equilibrium. Traditional rating protocols are not effective to overcome the inefficiency of the socially undesirable equilibrium due to specific features of crowdsourcing: a large number of anonymous users having asymmetric service requirements, different service capabilities, and dynamically joining/leaving a crowdsourcing platform with imperfect monitoring. In this paper, we develop the first game-theoretic design of the two-sided rating protocol to stimulate cooperation among self-interested users, which consists of a recommended strategy and a rating update rule. The recommended strategy recommends a desirable behavior from three predefined plans according to intrinsic parameters, while the rating update rule involves the update of ratings of both users, and uses differential punishments that punish users with different ratings differently. By quantifying necessary and sufficient conditions for a sustainable social norm, we formulate the problem of designing an optimal two-sided rating protocol that maximizes the social welfare among all sustainable protocols, provide design guidelines for optimal two-sided rating protocols and a low-complexity algorithm to select optimal design parameters in an alternate manner. Finally, evaluation results show the validity and effectiveness of our protocol designed for service exchange dilemma in crowdsourcing.		Jianfeng Lu;Yun Xin;Zhao Zhang;Xinwang Liu;Keqin Li	2018	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2018.2834318	task analysis;optimal design;artificial intelligence;management science;dilemma;computer science;pattern recognition;popularity;inefficiency;norm (social);server;crowdsourcing	Metrics	-9.639936543476756	-8.282529543655238	179410
ae94ac5b93929c97a3aae34c505aff291117b9d8	context dependence in multiagent resource allocation	multiagent resource allocation;satisfiability;decision problem;context dependent	A standard assumption in studies of multiagent resource allocation problems is that the value an individual agent places on its assignment remains unchanged by any redistribution of the remaining resources among the other agents. This assumption renders impossible analyses of scenarios where the utility an agent attaches to a particular set of resources is determined by factors other than the resource set itself. Thus an agent’s perception of what its allocation is worth may be tempered by its view of what other agents in the system may own, e.g. if working within a coalition a particular allocation may assume a greater value if other coalition members hold certain resources. In this paper we develop a model for examining such context dependent valuations and consider various decision problems related to the existence of context dependent allocations satisfying various criteria.	agent-based model;computational complexity theory;decision problem;embedded system;relevance;rendering (computer graphics)	Paul E. Dunne	2004			resource allocation;computer science;knowledge management;context-dependent memory;decision problem;management science;satisfiability	AI	-8.115342491295682	-4.138519152680008	179696
5b056967374a7a6a4fc6835fa0fab5987ea2c1ff	string rearrangement metrics: a survey	relative position;cost function;computer architecture;pattern matching;video on demand;computational biology	A basic assumption in traditional pattern matching is that the order of the elements in the given input strings is correct, while the description of the content, i.e. the description of the elements, may be erroneous. Motivated by questions that arise in Text Editing, Computational Biology, Bit Torrent and Video on Demand, and Computer Architecture, a new pattern matching paradigm was recently proposed by [2]. In this model, the pattern content remains intact, but the relative positions may change. Several papers followed the initial definition of the new paradigm. Each paper revealed new aspects in the world of string rearrangement metrics. This new unified view has already proven itself by enabling the solution of an open problem of the mathematician Cayley from 1849. It also gave better insight to problems that were already studied in different and limited situations, such as the behavior of different cost functions, and enabled deriving results for cost functions that were not yet sufficiently analyzed by previous research. At this stage, a general understanding of this new model is beginning to coalesce. The aim of this survey is to present an overview of this recent new direction of research, the problems, the methodologies, and the state-of-the-art.	bittorrent;computation;computational biology;computer architecture;magma;null (sql);pattern matching;programming paradigm	Amihood Amir;Avivit Levy	2010		10.1007/978-3-642-12476-1_1	computer science;theoretical computer science;algorithm	Theory	-16.393342835368117	-2.3450687933358862	179824
4632c056460e5fe9c059535f0a44cf8e53f8ebd7	an argumentation model of evidential reasoning with variable degrees of justification				QingYin Liang;Bin Wei	2012		10.3233/978-1-61499-167-0-71	evidential reasoning approach;argumentation theory;computer science;artificial intelligence	AI	-15.553807792142207	0.5296478355416894	180015
37dcb9bebe390a814fda39f051b3042daa94a197	provision of public goods on networks: on existence, uniqueness, and centralities		We consider the provision of public goods on networks of strategic agents. We study different effort outcomes of these network games, namely, the Nash equilibria, Pareto efficient effort profiles, and semi-cooperative equilibria (resulting from interactions among coalitions of agents). We identify necessary and sufficient conditions on the structure of the network for the uniqueness of the Nash equilibrium by using a connection between these outcomes and linear complementarity problems. We show that our finding unifies, and extends, existing results in the literature. We also identify conditions for the existence of Nash equilibria for two subclasses of games at the two extremes of our model, namely games of strategic complements and games of strategic substitutes. We provide a graph-theoretical interpretation of agents’ efforts at the Nash equilibrium, as well as the Pareto efficient outcomes and semi-cooperative equilibria, by linking an agent's decision to her centrality in the interaction network. Using this connection, we separate the effects of incoming and outgoing edges on agents’ efforts and uncover an alternating effect over walks of different length in the network.	centrality;complementarity theory;interaction network;linear complementarity problem;nash equilibrium;nonlinear system;pareto efficiency;semiconductor industry	Parinaz Naghizadeh;Mingyan Liu	2018	IEEE Transactions on Network Science and Engineering	10.1109/TNSE.2017.2755003	epsilon-equilibrium;best response;economics;microeconomics;mathematical economics;welfare economics;nash equilibrium	ECom	-4.723130296723396	-1.1177735651214042	180024
45062dce2a2edba2a98b95a13517a826fcb98e02	discount - a distributed and learning equational prover	distributed theorem proving;competition;learning;reactive planning;theorem proving;theorem prover;discount;user interaction;automated theorem proving;control strategy;knowledge base	The DISCOUNT system is a distributed equational theorem prover based on the teamwork method for knowledge-based distribution. It uses an extended version of unfailing Knuth–Bendix completion that is able to deal with arbitrarily quantified goals. DISCOUNT features many different control strategies that cooperate using the teamwork approach. Competition between multiple strategies, combined with reactive planning, results in an adaptation of the whole system to given problems, and thus in a very high degree of independence from user interaction. Teamwork also provides a suitable framework for the use of control strategies based on learning from previous proof experiences. One of these strategies forms the core of the expert global_learn, which is capable of learning from successful proofs of several problems. This expert, running sequentially, was one of the entrants in the competition (DISCOUNT/GL), while a distributed DISCOUNT system running on two workstations was another en trant.	automated theorem proving;database;experience;heuristic (computer science);inference engine;knuth–bendix completion algorithm;lisp;reactive planning;rewriting;synergetics (fuller);throughput;workstation	Jörg Denzinger;Martin Kronenburg;Stephan Schulz	1997	Journal of Automated Reasoning	10.1023/A:1005879229581	knowledge base;discrete mathematics;computer science;artificial intelligence;theoretical computer science;mathematics;automated theorem proving;algorithm	AI	-16.761663983727722	-8.679278154523091	180077
29781ef6a30297f47e89c8e175344f8514d99f0d	pairwise liquid democracy		In a liquid democracy, voters can either vote directly or delegate their vote to another voter of their choice. We consider ordinal elections, and study a model of liquid democracy in which voters specify partial orders and use several delegates to refine them. This flexibility, however, comes at a price, as individual rationality (in the form of transitive preferences) can no longer be guaranteed. We discuss ways to detect and overcome such complications. Based on the framework of distance rationalization, we introduce novel variants of voting rules that are tailored to the liquid democracy context.	emergent democracy;external ray;graph edit distance;microsoft outlook for mac;ordinal data;rationality	Markus Brill;Nimrod Talmon	2018		10.24963/ijcai.2018/19	artificial intelligence;democracy;machine learning;pairwise comparison;computer science	AI	-9.554108609344082	-2.196079142336141	180226
2b03907ef4906de992c842a381ba241e9ea6453b	an extension of the tau-value to games with coalition structures	allocation rule;game theory;τ value;tu games;bankruptcy problems;satisfiability;airport games	We introduce the coalitional s-value, which is an extension of the s-value for TU-games to games with a coalition structure. We identify a class of TU-games that satisfy the property that for every game in this class and every coalition structure on its player set it holds that the coalitional s-value can be defined for the corresponding game with a coalition structure. We study properties of the coalitional s-value and provide an axiomatic characterization of this allocation rule. We use the coalitional s-value to study bankruptcy problems and the determination of aircraft landing fees. 2002 Elsevier Science B.V. All rights reserved.		Balbina Casas-Méndez;Ignacio García-Jurado;Anne van den Nouweland;Margarita Vázquez-Brage	2003	European Journal of Operational Research	10.1016/S0377-2217(02)00426-5	game theory;simulation;economics;mathematical economics;welfare economics;satisfiability	AI	-6.123869952952939	-1.5251777130029283	180256
9abf394241173821d84b8d1e33a7704434c2c883	design of software agent-populated electronic negotiation system and evaluation of human - to - agent negotiations	experimental studies;electronic negotiations;software agents;design theory	Negotiation is a flexible mechanism for facilitating effective economic exchanges. Electronic negotiations allow participants to negotiate online and use analytical support tools in making their decisions. Software agents offer the possibility of automating negotiation process using these tools. The purpose of this work is to make progress towards outlining design-theoretical principles for agent-enhanced negotiation systems (AENS). This paper describes an electronic marketplace named DIANA (Deal-making system Incorporating Agents in Negotiations and Auctions) that allows involving software agents in negotiations. It also presents the results of experiments in agent-to-human negotiations. Various types of agents have been configured and paired up with human counterparts for negotiating product sale. The paper discusses the results and presents a set of rules for the design of AENS.	population;software agent	Rustam M. Vahidov;Gregory E. Kersten	2012		10.1007/978-3-642-29863-9_30	engineering;knowledge management;operations management;management science	HCI	-8.927992827053432	-8.399460212140445	180269
ee819b7d7ff91a3565cc51539d1fc3eb6a9fb7a6	auction-based variety formation and steering for mass customization	variety steering;multi agent system;mass customization;auction mechanism;variety formation	Large product variety in mass customization induces a high internal complexity level inside a company's operations, as well as a high external complexity level from a customer's perspective. Both complexity problems should be addressed respectively within the scope of variety formation and variety steering tasks. To support these tasks, an information system based on an electronic market is identified as a suitable solution approach. Therefore, we consider the main product building blocks as autonomous rational agents participating in an auction market where they compete by bidding to form product variants with the best chances to suit customers' requirements. In this market, the agents with the best bidding strategies are capable of ensuring their self‐preservation. Thus, a framework for supporting the auction market, the main agents in this market, as well as an appropriate coordination mechanism are proposed in order to support variety formation and variety steering tasks.		Thorsten Blecker;Nizar Abdelkafi;Gerold Kreutler;Bernd Kaluza	2004	Electronic Markets	10.1080/1019678042000245245	simulation;economics;mass customization;computer science;marketing;operations management;multi-agent system	ECom	-8.59048024948508	-8.290131388973572	180415
f76cec2dcc2885a7ede99b92ba103e419bfbefe1	two-person ex post implementation	incentive compatibility;implementation;ex post equilibrium;satisfiability;social choice;two person	This paper investigates the ex post implementation of a social choice set with two agents. A social choice set F is ex post implementable with two agents if F satis es the conditions of ex post incentive compatibility (EPIC) and ex post monotonicity (EM) in an environment that is economic and has a bad outcome. Furthermore, if F is a social choice function, (EPIC), (EM), and an economic environment are su cient.		Yoshihiro Ohashi	2012	Games and Economic Behavior	10.1016/j.geb.2011.09.006	public relations;social choice theory;economics;incentive compatibility;microeconomics;implementation;welfare economics;satisfiability	ECom	-5.89709216501833	-2.279618780918482	180536
d74c53263ec739107817059469c1a9a4d54d8fdd	improving trust estimates in planning domains with rare failure events	dagification;p circa;importance sampling	In many planning domains, it is impossible to construct plans that are guaranteed to keep the system completely safe. A common approach is to build probabilistic plans that are guaranteed to maintain system with a sufficiently high probability. For many such domains, bounds on system safety cannot be computed analytically, but instead rely on execution sampling coupled with a plan verification techniques. While probabilistic planning with verification can work well, it is not adequate in situations in which some modes of failure are very rare, simply because too many execution traces must be sampled (e.g., 10) to ensure that the rare events of interest will occur even once. The P-CIRCA planner seeks to solve planning problems while probabilistically guaranteeing safety. Our domains frequently involve verifying that the probability of failure is below a low threshold ( < 0.01). Because the events we sample have such low probabilities, we use Importance sampling (IS) (Hammersley and Handscomb 1964; Clarke and Zuliani 2011) to reduce the number of samples required. However, since we deal with an abstracted model, we cannot bias all paths individually. This prevents IS from achieving a correct bias. To compensate for this drawback we present a concept ofDAGificationto partially expand our representation and achieve a better bias.	artificial intelligence;autonomous system (internet);biasing;computation;edmund m. clarke;flowchart;graph rewriting;importance sampling;intelligent control;interaction;nondeterministic algorithm;rare events;real-time clock;real-time computing;real-time transcription;sampling (signal processing);state space;system safety;tracing (software);verification and validation	Colin M. Potts;Kurt D. Krebsbach;Jordan Tyler Thayer;David J. Musliner	2013			simulation;importance sampling;data mining	AI	-17.553820106025405	-1.32385271000228	180634
dcbcceefcd694f71ae736d9751bdde75b70c4558	cooperation and computability in n-player games	pareto efficiency;equilibrium selection;repeated game;backward induction	Abstract   A Common Interest game is a game that has a unique vector of payoffs that strictly Pareto-dominates all other payoffs. We consider the undiscounted repeated game obtained by the infinite repetition of such an  n -player Common Interest game. We restrict supergame strategies to be computable within Church’s thesis, and we introduce computable trembles on these strategies. If the trembles have sufficiently large support, the only equilibrium vector of payoffs that survives is the Pareto-efficient one. The result is driven by the ability of the players to use the early stages of the game to communicate their intention to play cooperatively in the future. The players take turns to reveal their cooperative intentions, and the result is proved by backwards induction on the set of players. We also show that our equilibrium selection result fails when there are a countable infinity of players.	computability	Luca Anderlini;Hamid Sabourian	2001	Mathematical Social Sciences	10.1016/S0165-4896(00)00068-8	non-cooperative game;bayesian game;cheap talk;example of a game without a value;economics;extensive-form game;simultaneous game;folk theorem;information set;repeated game;mathematics;stochastic game;screening game;microeconomics;normal-form game;mathematical economics;outcome;sequential game;welfare economics;backward induction;complete information;equilibrium selection;symmetric game;solution concept;nash equilibrium;centipede game	Theory	-5.2946023637872095	-1.6626309249250861	180645
eb58a15cbe52709bf867f04883216a356894cd16	research and application of one-dimensional optimal cutting stock system oriented to manufacturability	forward optimization;multi threading;bin packing;cutting plan;raw materials;software system;two stage optimization method;optimal method;one dimensional optimal cutting stock system;software systems;cutting stock problem;data mining;production engineering computing;production planning bin packing multi threading production engineering computing;raw materials design optimization computer aided manufacturing manufacturing processes optimization methods production costs manufacturing industries product design collaborative work;manufacturability;manufacturing;one dimensional cutting stock problem;production;production planning;multithread technology;reverse optimization;two stage optimization manufacturability one dimensional cutting stock problem;software system one dimensional optimal cutting stock system manufacturability one dimensional cutting stock problem cutting plan multithread technology two stage optimization method forward optimization reverse optimization;optimization methods;two stage optimization	The generalized concept of cutting stock problem was proposed, and the meaning of the cutting stock problem oriented to manufacturability was discussed. For one-dimensional cutting stock problem, the influencing factors and the processing methods of the manufacturability of cutting plan was researched. The function and structure of the manufacturability oriented one-dimensional optimal cutting stock system based on multithread technology was put forward. Then, processing ways for one-dimensional cutting stock problem oriented to manufacturability was studied and a two-stage optimization method combined forward and reverse optimization was given. Finally, a software system for the cutting stock problem studied above was developed and an example was given to verify the practicality and efficiency of the system.	design for manufacturability	Chunping Yan;Tianfeng Song;Zhaoxia Zhang;Fei Liu	2009		10.1109/CSIE.2009.581	software system	NLP	-12.233753247971958	-4.327495679625816	180663
37da8aa828d7a73e2e9d7d7c7075883e6534d48c	on finding curb sets in extensive games	subgame perfect equilibrium;nash equilibria;rationalizability;finite horizon;external research report;stochastic stability	We characterize strategy sets that are closed under rational behavior (curb) in extensive games of perfect information and finite horizon. It is shown that any such game possesses only one minimal curb set, which necessarily includes all its subgame perfect Nash equilibria. Applications of this result are twofold. First, it lessens computational burden while computing minimal curb sets. Second, it implies that the profile of subgame perfect equilibrium strategies is always stochastically stable in a certain class of games. April 10, 2003	computation;nash equilibrium	Vitaly Pruzhansky	2003	Int. J. Game Theory	10.1007/s001820300146	markov perfect equilibrium;mathematical optimization;sequential equilibrium;trembling hand perfect equilibrium;economics;subgame;folk theorem;rationalizability;mathematics;microeconomics;mathematical economics;stackelberg competition;subgame perfect equilibrium;welfare economics;nash equilibrium	AI	-4.724019070516717	-1.1449851297139404	180719
27d8b3d09fc1ee7488bf4ab8a0e416ca4dcce341	characterization of coherent conditional probabilities as a tool for their assessment and extension	probability assessment and extensions;coherence;conditional probability	A major purpose of this paper is to show the broad import and applicability of the theory of probability as proposed by de Finetti, which differs radically from the usual one (based on a measure-theoretic framework). In particular, with reference to a coherent conditional probability, we prove a characterization theorem, which provides also a useful algorithm for checking coherence of a given assessment. Moreover it allows to deepen and generalise in useful directions de Finetti’s extension theorem (dubbed as “the fundamental theorem of probability”), emphasising its operational aspects in many significant applications.	coherence (physics)	Giulianella Coletti;Romano Scozzafava	1996	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S021848859600007X	combinatorics;discrete mathematics;imprecise probability;coherence;conditional probability;conditional expectation;regular conditional probability;chain rule;mathematics;conditional mutual information;statistics	SE	-10.997083158386044	1.293593877851814	180982
045ebf9894d3af7d39c2b866857d887c95ce1f16	qualitative versus quantitative notions of speaker and hearer belief: implementation and theoretical extensions		This paper investigates the notion of the Stalnakerean acceptance in relation to common ground, exploring alternative contexts where the Stalnakerean view of acceptance is insufficient (2002). Thus, this paper extends the Stalnakerean notion of acceptance through the introduction of qualitative as well as quantitative notions of beliefs into Discourse Representation Theory (DRT), corresponding to a weaker degree of belief than contexts requiring a stronger commitment beliefwise on the hearer’s part. In addition, emphasis is placed on viewing presuppositions through agents’ beliefs instead of the vague notion of common ground. These qualitative and quantitative notions of beliefs are then implemented as part of a speaker/ hearer model of dialogue.	vagueness	Yafa Al-Raheb	2006				AI	-15.471485753434129	3.902671318647478	180991
494c8154f6da5551aec30d30ab6bdb0d99957db3	anonymous pricing of efficient allocations in combinatorial economies	interface agents;pricing resource management multiagent systems permission computer science autonomous agents sufficient conditions distributed computing instruments;embodied conversational agents;multiagent system;instruments;pricing;coordination mechanisms;information interface and presentation;resource management;distributed computing;satisfiability;sufficient conditions;qualitative study;permission;evaluation methodology;necessary and sufficient condition;wizard of oz technique;computer science;autonomous agents;empirical evaluation;user interfaces;multiagent systems	Auctions and exchanges are important coordination mechanisms for multiagent systems. Most multi-good markets are combinatorial in that the agents have preferences over bundles of goods. We study the possibility of determining prices so as to support (efficient) allocations in combinatorial economies where a seller (or arbitrator) wants to implement an efficient allocation and the prices are required to be anonymous. Conditions on the existence of equilibria are presented and a particularly attractive pricing scheme is studied in detail. The relation of minimal equilibrium prices to Vickrey payments is analyzed. A procedure based on the controlled formation of alliances is suggested that shrinks economies to ensure the existence of prices coherent with the preferred pricing scheme.	agent-based model;coherence (physics);multi-agent system	Wolfram Conen;Tuomas Sandholm	2004	Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.	10.1109/AAMAS.2004.65	pricing;simulation;wizard of oz experiment;computer science;qualitative research;artificial intelligence;autonomous agent;resource management;multi-agent system;distributed computing;user interface;satisfiability	AI	-5.704729902912706	-2.0332969635044043	181043
12097744c70075c248ceac4bd6727aaa1b3ec2e4	australia's public library system: its needs and potential	survey form;data gathering;australian public libraries;public libraries;statistics directory	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source;public library	Alan Bundy	2010	Public Library Quarterly	10.1080/01616846.2010.525384	computer science;world wide web;information retrieval;statistics;data collection	Robotics	-15.254618314616392	-5.852719020104587	181068
2a71bdf0676b3ba201b0b04444aa54041cdf52ef	book review: prolog: the standard reference manual by e deransart, a. ed-dbali, and l. cervoni (springer-verlag, 1996)	standard reference manual;l. cervoni;book review;long time;prolog inference engine;computational model;standards group;sound understanding;language ii;iso prolog standard;programming language;clear description;major service;e deransart	The ISO Prolog standard took 10 ars to produce. For a language II that, on its face, is both simple and logical this seems like a long time to spend on standardization. While this book is in no sense a history of the standardization of Prolog, it does provide some insight into why the process took so long. Anyone who has tried to learn Prolog knows that to use the language effectively one must have a sound understanding of the computational model. (Of course, this is true of any programming language, but for Protog it seems to be especially so.) The standards group was to find plenty of room for dispute over the computational model. A major service that this book performs is to provide a clear description of the Prolog inference engine.	computation;computational model;inference engine;programming language;prolog;springer (tank)	J. P. E. Hodgson	1998	SIGART Bulletin	10.1145/294828.1067912	computer science;artificial intelligence;programming language;algorithm	PL	-17.95868709559171	2.3133106009667985	181083
b0d2667ded19dab3727395fa62df78e88d0467bb	solving fuzzy distributed csps: an approach with naming games		Constraint Satisfaction Problems (CSPs) are the formalization of a large range of problems that emerge from computer science. The solving methodology described here is based on Naming Games (NGs). NGs were introduced to represent N agents that have to bootstrap an agreement on a name to give to an object (i.e., a word). In this paper we focus on solving both Fuzzy NGs and Fuzzy Distributed CSPs (Fuzzy DCSPs) with an algorithm inspired by NGs. In this framework, each proposed solution is associated with a preference represented as a fuzzy score. We want the agents to find the solution, which is associated with the highest preference value among all solutions. The two main features that distinguish this methodology from classical Fuzzy DCSPs algorithms are that i) the system can react to small instance changes, and ii) the fact the algorithm does not require a pre-agreed agent/variable ordering.	cryptographic service provider	Stefano Bistarelli;Giorgio Gosti;Francesco Santini	2012		10.1007/978-3-642-37890-4_7	fuzzy logic;computer science;natural language processing;machine learning;artificial intelligence;constraint satisfaction problem	Logic	-12.967355481687774	-1.6759524162259423	181165
005cdb9d805bd6f23d2196e9bc3a2801e6053779	analysis of bidding behavior on ebay auctions	electronic commerce;affiliated valuation;stopping rule;bidding strategies;online auction;online auction site;late bidding strategy;affiliated valuation bidding behavior analysis ebay auctions online auction site ipv early bidding strategy late bidding strategy optimal bidding;ipv;ebay auctions;optimal bidding;cost accounting educational institutions electronic commerce nash equilibrium ethics counting circuits history;early bidding strategy;bidding behavior analysis	eBay is the most successful online auction site. In this paper, we study bidding behavior on eBay auctions. First, we show that under the assumption of IPV, the truthful and early bidding is approximately optimal. Then, we compare the truthful and early bidding strategy with the late bidding strategy and point out that the optimal bidding strategies rely on the influence of bidders' valuations, i.e., IPV or AV (affiliated valuations). In the following, we analyze the advantages of the truthful and early bidding strategy from different aspects and the influence of auction components on bidding strategies, such as proxy bidding, bid increment, the number of bidders, and stopping rules. Finally, we discuss preliminary their optimal bidding behavior under the assumption of the hybrid valuations of IPV and AV and obtain some conclusions	av-comparatives;av-test;proxy server	Li Du;Qiying Hu	2006	2006 IEEE International Conference on e-Business Engineering (ICEBE'06)	10.1109/ICEBE.2006.26	e-commerce;bidding;real-time bidding;computer science;ebidding;world wide web	Robotics	-5.044080991327351	-9.537408895471547	181192
1ae660dc317b37a452f2524ae8a87778379c67b9	is three better than one? simulating the effect of reviewer selection and behavior on the quality and efficiency of peer review		This paper looks at the effect of multiple reviewers and their behavior on the quality and efficiency of peer review. By extending a previous model, we tested various reviewer behavior, fair, random and strategic, and examined the impact of selecting multiple reviewers for the same author submission. We found that, when reviewer reliability is random or reviewers behave strategically, involving more than one reviewer per submission reduces evaluation bias. However, if scientists review scrupulously, multiple reviewers require an abnormal resource drain at the system level from research activities towards reviewing. This implies that reviewer selection mechanisms that protect the quality of the process against reviewer misbehavior might be economically unsustainable.	experiment;fairness measure;peer-to-peer;simulation;trusted computer system evaluation criteria	Federico Bianchi;Flaminio Squazzoni	2015	2015 Winter Simulation Conference (WSC)			AI	-10.92606118873777	-9.636181693637965	181697
d45c2d8d806f99c0bf1e23d57d8490455b185f17	inheritance of properties in communication situations	cooperative game;shapley value;communication situations;communication situations properties inheritance;properties;inheritance	In this paper we consider cooperative games in which the possibilities for cooperation between the players are restricted because communication between the players is restricted. The bilateral communication possibilities are modeled by means of a (communication) graph. We are interested in how the communication restrictions influence the game. In particular, we investigate what conditions on the communication graph guarantee that certain appealing properties of the original game are inherited by the graph-restricted game, the game that arises once the communication restrictions are taken into account. We study inheritance of the following properties: average convexity, inclusion of the Shapley value in the core, inclusion of the Shapley values of a game and all its subgames in the corresponding cores, existence of a population monotonic allocation scheme, and the property that the extended Shapley value is a population monotonic allocation scheme.		Marco Slikker	2000	Int. J. Game Theory	10.1007/s001820000039	bondareva–shapley theorem;non-cooperative game;example of a game without a value;economics;simultaneous game;repeated game;mathematics;stochastic game;screening game;shapley value;property;mathematical economics;sequential game;welfare economics	NLP	-6.142195455943176	-1.3987076835138768	181899
e4670e3c9d7be34d32e93d7d926789c06f51a08b	the social will-testing game and its solution		We examine a two-person game we call Will-Testing in which the strategy space for both players is a real number. It has no equilibrium. When an infinitely large set of players plays this in all possible pairings, there is an equilibrium for the distribution of strategies which requires all players to use different strategies. We conjecture this solution could underlie some phenomena observed in animals. 1 The Will-Testing Game This game is played by two people in continuous time through a fixed time interval. Either player may capitulate at any time, thereby ending the play. The payoff for the non-capitulator is some constant times the amount of time remaining in the game; the capitulator gets half as much as the non-capitulator. The tension in the game is that both players want someone to capitulate in a hurry, but both want the other one to do so, and hence they both stall. There is no equilibrium to this game. The maximum sum of payoffs is 1 + .5, so players could judge their earnings by how much of that they obtained; many people would judge 0.75 to be a satisfactory take-away pay. In the case where this game is played repeatedly with the same player, there is an opportunity to split the 1.5 quite evenly in half, but achieving this requires the development of trust. How humans or chickens or other animals play this game is an area open to behavioral studies. The tension changes as we alter the fraction that the capitulator gets. Clearly if his fraction is 1, then it does not matter who capitulates first, and both will do so immediately. If his fraction very small (say 1/100) then there 1 ar X iv :1 20 6. 61 48 v1 [ cs .G T ] 2 7 Ju n 20 12 is a huge incentive to stall. If his fraction is close to 1 then it does not matter very much who goes first, and it’s relatively easy to make the sacrifice to go first. Since the fraction makes a big difference, we will parameterize the game and call the capitulator’s fraction ρ where 0 ≤ ρ ≤ 1. In spite of its description as being played in continuous time, we will formalize it as a one-shot game where each player i selects a real number si in the fixed interval [0, T ]. Then the payoffs for players 1 and 2 respectively are:  (T − s2) and ρ× (T − s2) if s1 > s2 ρ× (T − s1) and ρ× (T − s1) if s1 = s2 ρ× (T − s1) and (T − s1) if s1 < s2 The social version of the game has a large set of players who each choose a strategy once; their payoffs are the average of the payoffs using that strategy against all other players. This captures the idea of a group of social animals that play the game repeatedly with partners picked uniformly at random from the same group. There is no fixed pure equilibrium for this game when there are a finite number of players. In the limit of infinite players there is a density function for the equilibria, but it does not specify which player uses which strategy so all permutations are equivalent. Finding this distribution involves an equation that makes the usual assumption that all players get equal payoffs, so the players are quite indifferent to which permutation occurs. One strong interesting feature of the equilibria is that all players must choose different strategies. Birds and mammals and many other animals are generally perceived as being individuals who are all slightly different, populating a spectrum of behavior space. They are also widely seen as having social structures like dominance hierarchies wherein pairwise relations between individuals can be characterized. We view the social version of Will-Testing to be an example of a game that produces a similar ordered structure where the existence of differences in individual behaviors is a straightforward requirement of the game’s solution. In the case of this particular game, the social dynamic will be interpretable as a total ordering (like a pecking order) simply because the strategy space is real and one dimensional. 2 Deriving the Equilibria We seek a density over the strategy space, den(t) ≥ 0 and ∫ T 0 den(t)dt = 1 . To find the average over all possible pairwise interactions in the social version of the game, we must integrate over that space. Define the pay of strategy s	dominance drawing;interaction;population;social structure;utility	Leonid Gurvits;J. Stephen Judd	2012	CoRR		non-cooperative game;combinatorics;simulation;sequential equilibrium;repeated game;rationalizability;mathematics;mathematical economics;outcome;complete information;equilibrium selection;solution concept;nash equilibrium;symmetric equilibrium	ECom	-5.222005984377943	-4.933012992776937	182006
a56ff218ecf5084f57c1244016a258688a58ae10	game theoretic analysis of the bargaining process over a long-term replenishment contract	modelizacion;equilibrio nash;nuevo abastecimiento;game theory;nash equilibrium;negociation;relation client fournisseur;realimentation;equilibrio juego;teoria juego;theorie jeu;long terme;contrato;refeeding;long term;modelisation;ciencias economicas;equilibre nash;largo plazo;contract;negociacion;theoretical analysis;supplier buyer model;bargaining;realimentacion;relacion cliente proveedor;replenishment contract;equilibre jeu;sciences economiques;contrat;economics;replenishment;game equilibrium;modeling;reapprovisionnement;supplier customer relationship	This paper presents supplier–buyer models to describe the bargaining process between a supplier and a buyer over a long-term replenishment contract. Two different models are developed: one for the situation where the supplier has superior bargaining power over the buyer, and the other for the reverse situation. For each model, a method is derived that employs game theory-based analysis to determine the best strategy for each agent. A computational experiment is conducted to estimate the efficiency of the methods and to determine the economic implications of the results. The result indicates that each algorithm is very efficient compared to other strategies. We also verify that the solutions derived from each model are Nash equilibrium. Significantly improved outcomes are obtained for both agents by agreeing to the terms generated by the algorithms over the terms selected in the usual manner.	game theory	J. S. Kim;T. C. Kwak	2007	JORS	10.1057/palgrave.jors.2602183	contract;bargaining problem;game theory;economics;operations management;mathematical economics;management;operations research;law;welfare economics;nash equilibrium	HCI	-5.260356751716429	-8.404225386889197	182025
6d5d9223b4a3a3106e4eb0e5781ae7047cb3e684	preference elicitation for participatory budgeting		Participatory budgeting enables the allocation of public funds by collecting and aggregating individual preferences; it has already had a sizable real-world impact. But making the most of this new paradigm requires a rethinking of some of the basics of computational social choice, including the very way in which individuals express their preferences. We analytically compare four preference elicitation methods — knapsack votes, rankings by value or value for money, and threshold approval votes — through the lens of implicit utilitarian voting, and find that threshold approval votes are qualitatively superior. This conclusion is supported by experiments using data from real participatory budgeting elections.	computation;experiment;participatory monitoring;preference elicitation;programming paradigm	Gerdus Benade;Swaprava Nath;Ariel D. Procaccia;Nisarg Shah	2017			participatory budgeting;machine learning;management science;artificial intelligence;computer science;preference elicitation	AI	-7.885585681355595	-6.2226304746610035	182072
1e4e712a036c0b303611241ab9775427ed1e6043	truthful outcomes from non-truthful position auctions	sponsored search;conference paper;auction theory	We exhibit a property of the VCG mechanism that can help explain the surprising rarity with which it is used even in settings with unit demand: a relative lack of robustness to inaccuracies in the choice of its parameters. For a standard position auction environment in which the auctioneer may not know the precise relative values of the positions, we show that under both complete and incomplete information a non-truthful mechanism supports the truthful outcome of the VCG mechanism for a wider range of these values than the VCG mechanism itself. The result for complete information concerns the generalized second-price mechanism and lends additional theoretical support to the use of this mechanism in practice. Particularly interesting from a technical perspective is the case of incomplete information, where a surprising combinatorial equivalence helps us to avoid confrontation with an unwieldy differential equation.	turing completeness	Paul Dütting;Felix A. Fischer;David C. Parkes	2016		10.1145/2940716.2940731	economics;microeconomics;mathematical economics;welfare economics;auction theory	ECom	-4.818748180584093	-6.1121117114891	182130
ec5e5abb38b9ab3f33b97089ede93fceea0673ea	the pairwise egalitarian solution	assignment;optimal solution;game theory;transportation problem;satisfiability;shapley value;transportation;economics;solution concept	The main aim of this paper is to introduce an ad hoc solution concept for transportation games. This solution is related to the structure of the underlying transportation problem in the game, in particular to the optimal solutions for it. This is significantly called the pairwise egalitarian solution. We compare the new solution with the Core and the Shapley value of the game. Finally, we study the properties satisfied by the pairwise egalitarian solution and provide two axiomatic characterizations of it. 2002 Elsevier Science B.V. All rights reserved.	hoc (programming language);pairwise summation;transportation theory (mathematics)	Joaquín Sánchez-Soriano	2003	European Journal of Operational Research	10.1016/S0377-2217(02)00503-9	transportation theory;game theory;transport;mathematical optimization;economics;assignment;mathematics;shapley value;mathematical economics;welfare economics;solution concept;satisfiability	AI	-6.0636104667250414	-1.122493732551991	182332
e4081a916c39a4205f465e994f57bc5046c5e7ba	risk-averse autonomous route guidance by a constrained a* search	demand response;advanced driver information systems;travel time;autonomous vehicle;uncertainty;efficient algorithm;risk aversion;algorithm;navigation;traffic congestion;risk;performance analysis;guidance;algorithms;a;navigation system	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	a* search algorithm;autonomous robot;broadcasting (networking);experiment;francis;primary source;real-time clock;real-time web;requirement;risk aversion;search problem;shortest path problem;simulation;veracity	Yanyan Chen;Michael G. H. Bell;Klaus Bogenberger	2010	J. Intellig. Transport. Systems	10.1080/15472450.2010.484753	navigation;simulation;risk aversion;uncertainty;engineering;risk;transport engineering	Robotics	-14.838619915725982	-6.60672957237537	182383
8fc0554d253bef30639f481b57008206a9444d03	the structure of signals: causal interdependence models for games of incomplete information		Traditional economic models typically treat private information, or signals, as generated from some underlying state. Recent work has explicated alternative models, where signals correspond to interpretations of available information. We show that the difference between these formulations can be sharply cast in terms of causal dependence structure, and employ graphical models to illustrate the distinguishing characteristics. The graphical representation supports inferences about signal patterns in the interpreted framework, and suggests how results based on the generated model can be extended to more general situations. Specific insights about bidding games in classical auction mechanisms derive from qualitative graphical models.	causal filter;causality;graphical model;interdependence;personally identifiable information;r-cast	Michael P. Wellman;Lu Hong;Scott E. Page	2011			econometrics;machine learning;mathematics;statistics	ML	-11.896545246772627	-9.473868422119644	182418
539bdcff2b666accb049b17fc03f6dc408cc90eb	how the invisible hand is supposed to adjust the natural thermostat: a guide for the perplexed	carbon pricing;climate change economics;equity versus efficiency;normative uncertainty;social discount rate	"""Mainstream climate economics takes global warming seriously, but perplexingly concludes that the optimal economic policy is to almost do nothing about it. This conclusion can be traced to just a few """"normative"""" assumptions, over which there exists fundamental disagreement amongst economists. This paper explores two axes of this disagreement. The first axis (""""market vs. regulation"""") measures faith in the invisible hand to adjust the natural thermostat. The second axis expresses differences in views on the efficiency and equity implications of climate action. The two axes combined lead to a classification of conflicting approaches in climate economics. The variety of approaches does not imply a post-modern """"anything goes"""", as the contradictions between climate and capitalism cannot be wished away."""	apache axis;conflict (psychology);global warming;thermostat device component	Servaas Storm	2017		10.1007/s11948-016-9780-3	psychology;medicine;economics;philosophy;engineering;operations management;sociology;management;law;welfare economics	ML	-10.912987679709417	-3.1480728403572815	182478
3996c12eb5a2557d931c752f02acfc26a2f67548	ab testing for process versions with contextual multi-armed bandit algorithms		Business process improvement ideas can be validated through sequential experiment techniques like AB Testing. Such approaches have the inherent risk of exposing customers to an inferior process version, which is why the inferior version should be discarded as quickly as possible. In this paper, we propose a contextual multi-armed bandit algorithm that can observe the performance of process versions and dynamically adjust the routing policy so that the customers are directed to the version that can best serve them. Our algorithm learns the best routing policy in the presence of complications such as multiple process performance indicators, delays in indicator observation, incomplete or partial observations, and contextual factors. We also propose a pluggable architecture that supports such routing algorithms. We evaluate our approach with a case study. Furthermore, we demonstrate that our approach identifies the best routing policy given the process performance and that it scales horizontally.	a/b testing;algorithm;multi-armed bandit	Suhrid Satyal;Ingo Weber;Hye-Young Paik;Claudio Di Ciccio;Jan Mendling	2018		10.1007/978-3-319-91563-0_2	business process;performance indicator;data mining;computer science;architecture;business process management;algorithm;multi-armed bandit;inherent risk (accounting)	SE	-17.77258737430109	-7.37947175069607	182484
45bfd4eafe96cffb7489254d3622fc628c383ddd	challenges in causal inference from personal monitoring devices			causal filter;causal inference	Tomasz Wiktorski	2018		10.15439/2018F378	data mining;machine learning;causal inference;artificial intelligence;computer science	NLP	-18.103032426958684	-4.8394445636954195	182574
2ecca260b6e375ee4a4175469323128a4db543cd	the core and the steady bargaining set for convex games	cooperative game;convex games;core;bargaining set	Within the class of zero-monotonic and grand coalition superadditive cooperative games with transferable utility, the convexity of a game is characterized by the coincidence of its core and the steady bargaining set. As a consequence it is proved that convexity can also be characterized by the coincidence of the core of a game and the modified Zhou bargaining set à la Shimomura.	linear algebra	Josep M. Izquierdo;Carles Rafels	2018	Int. J. Game Theory	10.1007/s00182-017-0576-8	mathematical optimization;economics;convexity in economics;microeconomics;mathematical economics	ECom	-5.7989568445170425	-1.1289951353702203	182608
b914939c2ebd8bf17a05ba2cd6a56081b622e3fa	a new approach to inference under uncertainty for knowledge based systems	knowledge based system	Uncertainties Expert systems for diagnosis, decision support systems, robot and real time control, engineering design etc. must necessarily use incomplete, contaminated and imprecise knowledge. A database of diseases with associated symptoms may not register presence or absence of a given symptom for a given disease. We can say that the symptom may or may not be present. We may be able to express this incompleteness in terms of probabilities. This is a form of incomplete information and is common in real application areas. Incompleteness can arise because of complete absence of knowledge or through only knowing approximately the answer. In the latter case this might be expressed in words with imprecise meaning such as tail, strong, probable etc. This imprecision can be taken into account by expressing it by means of fuzzy sets, ZADEH 1965, 75, 83. These fuzzy sets impose a possibility distribution over the set of possible values a variable can take, ZADEH 1978. This in turn imposes a family of possible probability distributions for the value of the variable. Contaminated knowledge arises from noise present in the system, perhaps through the unreliability of a communication channel or the unreliability of a measuring device etc. This form of uncertainty can be represented in probabilistic terms, perhaps as a family of conditional probability distributions, but may also require the use of fuzzy sets to express imprecision once again. Imprecise knowledge is fuzzy. Most everyday concepts cannot be precisely defined by means of necessary and sufficient conditions. The meaning of concepts which are acquired through examples of their usage and their applicability to new situations which do not perfectly match with those used previously cannot be deduced with certainty.	knowledge-based systems	James F. Baldwin	1991		10.1007/3-540-54659-6_74	uncertainty analysis;computer science;artificial intelligence;knowledge-based systems	SE	-16.495386392093188	0.5943006087899966	182722
9b45f37aa93e2cbb23aaf5660a51ca8357a7893b	contextual deliberation and preference construction		Choices can be context dependent. This empirical finding is usually invoked to suggest that preferences are constructive and susceptible to decision environment. Yet preference construction can be systematic and endogenous. This paper develops the theory of contextual deliberation as a potential explanation for behavioral phenomena of preference construction. When preference ordering in a choice set is ex ante unknown and state dependent, decision makers can engage in information acquisition activities i.e., deliberation before choice to improve knowledge about the state-dependent preference ordering. Choice context can thus influence ex post preference ordering through affecting the incentive to deliberate. Consequently, contextual deliberation may lead to preference construction and give rise to seemingly irrational behavioral phenomena such as the compromise effect and the choice overload effect. The theory of contextual deliberation also yields predictions that can be empirically tested to identify from other alternative explanations. This paper was accepted by Eric Anderson, marketing.		Liang Guo	2016	Management Science	10.1287/mnsc.2015.2290	public relations;context effect;welfare economics	Logic	-5.796061079896112	-7.437214232508218	183005
6449cdc8c2fe8bd86740370fdb5725296e8dcef7	wittgenstein on knowledge: a critique	humanidades;filosofia etica	My goal here is to assess whether Wittgenstein’s metaphilosophical conception of a descriptive philosophy is in accordance with his philosophical practice. I argue that Wittgenstein doesn’t really limit himself to description when he criticizes Moore’s use of the verb “to know”. In On Certainty, Wittgenstein argues that Moore’s claims of knowledge (such as “I know I have two hands”) are at odds with the everyday use of the verb “to know”, because, among other things, they don’t allow the possibility of justification. That is, Wittgenstein considers that proper, everyday claims of knowledge require the possibility of justification. What I try to show is that this idea cannot be derived from the mere observation and description of knowledge claims in ordinary language. I conclude that Wittgenstein’s treatment of the verb “to know” constitutes an inconsistency between his metaphilosophical posture and his philosophical practice.	poor posture	Raquel Krempel	2014	Synthese	10.1007/s11229-014-0593-0	philosophy;epistemology;mathematics	HCI	-13.136125328392557	3.5002955429663856	183284
d77c940b8abf0c48f4eb018ce1c6c3bcca25073f	multiplicative risk apportionment	higher order multiplicative risk apportionment;bepress selected works;expected utility;higher order relative risk aversion;journal;higher order;higher order relative risk aversion higher order multiplicative risk apportionment;relative risk aversion	This paper examines changes in a multiplicative risk and how these changes affect preferences in an expected-utility setting. We define higher-order multiplicative risk apportionment, via a particular classes of lottery preference and show it is equivalent to the magnitude of higher-order relative risk aversion.		Jianli Wang;Jingyuan Li	2010	Mathematical Social Sciences	10.1016/j.mathsocsci.2010.03.003	financial economics;econometrics;higher-order logic;risk aversion;economics;expected utility hypothesis;finance;welfare economics	ECom	-7.1804865351537694	-1.714377959754641	183340
52505d2f295717cb548fe34f285608eabddc415a	note on vcg vs. price raising for matching markets		In [1] the use of VCG in matching markets is motivated by saying that in order to compute market clearing prices in a matching market, the auctioneer needs to know the true valuations of the bidders. Hence VCG and corresponding personalized prices are proposed as an incentive compatible mechanism. The same line of argument pops up in several lecture sheets and other documents related to courses based on Easley and Kleinberg’s book, seeming to suggest that computing market clearing prices and corresponding assignments were not incentive compatible. Main purpose of our note is to observe that, in contrast, assignments based on buyer optimal market clearing prices are indeed incentive compatible. keywords: matching markets, market clearing prices, VCG mechanism.	personalization	Walter Kern;Bodo Manthey;Marc Uetz	2016	CoRR		industrial organization;economics;microeconomics;mathematical economics;market clearing;commerce	ECom	-5.5546945115663195	-3.867355953089127	183396
25142f03282142c22ba2552a0836c73c517dea96	the no miracles argument without the base rate fallacy	philosophy of science;munich center for mathematical philosophy mcmp;ddc 100	According to an argument by Colin Howson, the no-miracles argument (NMA) is contingent on committing the base-rate fallacy and is therefore bound to fail. We demonstrate that Howson’s argument only applies to one of two versions of the NMA. The other version, which resembles the form in which the argument was initially presented by Putnam and Boyd, remains unaffected by his line of reasoning. We provide a formal reconstruction of that version of the NMA and show that it is valid. Finally, we demonstrate that the use of subjective priors is consistent with the realist implication of the NMA and show that a core worry with respect to the suggested form of the NMA can be dispelled.	argument map;base rate;contingency (philosophy);putnam model	Richard Dawid;Stephan Hartmann	2017	Synthese	10.1007/s11229-017-1408-x	philosophy of science;philosophy;epistemology;mathematics	NLP	-13.187644966123008	3.4905638321796553	183622
6df6dd400cdb7c148330d3e73051769251f2ca95	gender codes: why women are leaving computing, edited by thomas j. misa. hoboken, nj: ieee computer society, 2010, 306 pp. $31.95 paper. isbn-10 0470597194, isbn-13 978-0470597194 (paper)	thomas j. misa;ieee computer society;gender codes	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	code;francis;international standard book number;primary source	Bärbel Bohr	2013	Inf. Soc.	10.1080/01972243.2013.800412	law and economics	Robotics	-15.273163116497265	-5.640460374714786	183696
1284401725bcf01b1960ab8c975aaa3ba7894985	a dual-motive heuristic for member information initiation in group decision making: managing risk and commitment	social risk;information exchange;group decision making;status processes;group decision	Information exchange in the decision making of interactive groups is examined at the level of individual group members. I recognize the dual or competing motives of members who act as both individuals and group members, and propose a two-stage heuristic for decisions on the type and amount of information they initiate. At the first stage, individual members intuit or solve the problem of maintaining their status in the group through information initiations that minimize the probability of receiving negative evaluations weighted by the sender's status. In the second stage, the member accepts some increment to this minimum to contribute to group decision quality. The increment the member accepts is proportional to his or her status and is the basis for the initiation of ideas and negative evaluations. The explicit forms that are proposed for the solution of the two-stage problem allow the quality-maximizing probability of an idea to be expressed in terms of a member's status. This result is used to examine a conjecture on status distributions and the probability of idea initiations that maximize the quality of a group decision. Initial evidence from recent studies that supports assumptions of this work is presented, and the capability of procedures in computer-mediated information exchange to maintain the exchange of ideas and negative evaluations at quality maximizing rates is noted.	decision quality;heuristic;information exchange;knowledge spillover	Steven D. Silver	1995	Decision Support Systems	10.1016/0167-9236(94)00050-3	group decision-making;information exchange;operations management;operations research	Web+IR	-6.355666216924254	-5.379075035353629	183750
906536befe9092b5a95250756905e67363c38534	cooperative equilibrium: a solution predicting cooperative play		Nash equilibrium (NE) assumes that players always make a bes t response. However, this is not always true; sometimes people cooperate even it is not a best response to do so. For example, in the Prisoner’s Dilemma, people often cooperate. Are there r ules underlying cooperative behavior? In an effort to answer this question, we propose a new equilib rium concept:perfect cooperative equilibrium(PCE), and two related variants: max-PCEandcooperative equilibrium. PCE may help explain players’ behavior in games where cooperation is obs erved in practice. A player’s payoff in a PCE is at least as high as in any NE. However, a PCE does not alw ays exist. We thus consider α-PCE, whereα takes into account the degree of cooperation; a PCE is a 0-PCE . Every game has a Pareto-optimal max-PCE (M-PCE) ; that is, anα-PCE for a maximumα. We show that M-PCE does well at predicting behavior in quite a few games of inter est. We also consider cooperative equilibrium(CE), another generalization of PCE that takes punishment i nto account. Interestingly, all Pareto-optimal M-PCE are CE. We prove that, in 2-player g ames, both a PCE (if it exists) and a M-PCE can be found in polynomial time, using bilinear progr amming. This is a contrast to Nash equilibrium, which is PPAD complete even in 2-player games [ Chen, Deng, and Teng 2009]. We compare M-PCE to thecoco value[Kalai and Kalai 2009], another solution concept that tries to capture cooperation, both axiomatically and in terms of an a lgebraic characterization, and show that the two are closely related, despite their very different de finitions.	bilinear filtering;bilinear transform;entity–relationship model;nash equilibrium;pareto efficiency;prisoner's dilemma;time complexity;turing completeness	Nan Rong;Joseph Y. Halpern	2014	CoRR		simulation;economics;mathematical economics;equilibrium selection	ECom	-7.446447119654549	-2.9389873548006245	183908
8a6be3587e3b7c59d58a93b29026f7c765d426d4	effective selection of abstract plans for multi-agent systems	multi agent system;estimation method;conflict resolution	This paper proposes a situation-based conflict estimation method that efficiently generates quality plans for multi-agent  systems (MAS) by appropriately selecting abstract plans in hierarchical planning (HP). In HP, selecting a plan at an abstract  level affects planning performance because an abstract plan restricts the scope of concrete-level (or primitive) plans and  thus can reduce the planning cost. However, if all primitive plans under the selected abstract plan have serious and difficult-to-resolve  conflicts with the plans of other agents, the final plan after conflict resolution will be inefficient or of low quality.  This issue originates in the uncertainty of MAS, where other agents also have individual plans for their own goals and it  is difficult to clearly anticipate which abstract plan will cause fewer conflicts with other agents’ plans. In the proposed  method, by introducing conflict patterns that express the situations of conflicts among agents’ plans, agents learn and estimate  which abstract plans are less likely to cause conflicts or which conflicts will be easy to resolve; thus, after conflict resolution,  they can induce probabilistically higher-utility primitive plans. This paper also describes an experiment to evaluate our  method. The results indicate that our method can improve the efficiency of plan execution.  	multi-agent system	Toshiharu Sugawara;Satoshi Kurihara;Toshio Hirotsu;Kensuke Fukuda;Toshihiro Takada	2007		10.1007/978-1-84800-094-0_17	geography;knowledge management;operations management;management science	AI	-18.39251912248286	-7.700503618272286	183986
88d73660218e25a8b2c822c0b4c966cae9d932f4	is envy one of the possible evolutionary roots of charity?	clytemnestra s strategy;spitefulness;other regarding preference;egalitarianism;ess;charity;evolutionary game	"""We introduce an evolutionary game in which envy and charity can be considered as a consequence of Darwinian competition, i.e. individuals aim at increasing their own proportion rather than their absolute contribution to the next generation, and other-regarding-preference is a """"method"""" for that. If the damage is additive and its cost is low, an envious strategy defeats a neutral strategy (rational strategy maximizing its own income). If damage is multiplicative, then coexistence of neutral and envious strategists is possible. Envy is a conditional spiteful strategy, thus in envious groups there is less damage than in spiteful groups, so envy decreases the total cost of spiteful competition. Although envy can be selected out in a mixed population of envious, neutral and spiteful individuals when damage is additive, the envious-spiteful strategists (envious within its kin and spiteful outside its kin) outperform neutral and spiteful ones in a simple kin-selection scenario. Furthermore, Clytemnestra's strategy, namely donating to richer people to evokes envious attacks on them, can spread through an envious group. Moreover, the envious-donator strategy (giving to the poorest and damaging the richer individuals) can spread in the envious group. Charity is a """"buy-off"""" behaviour, since the donator can decrease its own damage from envy, thus charity further decreases the total cost of spiteful competition."""	coexist (image);microsoft kin;nist hash function competition;next-generation network;plant roots;utility functions on indivisible goods;envy	József Garay;Tamás F. Móri	2011	Bio Systems	10.1016/j.biosystems.2011.06.004	egalitarianism	AI	-4.7012740935563775	-6.156173033689096	184095
d7df85b7461196b09ffa4e6bc2eef0fa2c371153	a knowledge based semantics of messages	logic;semantics;knowledge;messages;knowledge base;protocol	We investigate the semantics of messages, and argue that the meaning of a message is naturally and usefully given in terms of how it affects the knowledge of the agents involved in the communication. We see that the semantics depends on the protocol used by the agents, and this leads us to knowledge based specification of protocols. While these notions are natural for distributed computations, we suggest that the considerations discussed here may be relevant in more general linguistic contexts.	computation;intelligent agent	Rohit Parikh;Ramaswamy Ramanujam	2003	Journal of Logic, Language and Information	10.1023/A:1025007018583	natural language processing;knowledge base;protocol;formal semantics;philosophy;epistemology;computer science;theoretical computer science;formal semantics;semantics;linguistics;knowledge;well-founded semantics;operational semantics;logic;computational semantics	Logic	-16.534574889960904	3.9829947712875837	184166
7f3926bae082228391d5b9e95b9e367d39f0b293	stakeholders in bilateral conflict	timing game;public interest;stakeholders;incomplete information;war of attrition;bargaining	The resolution of a conflict often has an impact which extends beyond the remits of the parties directly involved in the confrontation (e.g. labour negotiations in sectors of public interest, where a strike would impact on the public at large). Once this is recognised, models addressing negotiations in such situations ought to account for the role and interests of the stakeholder—a third party whose stake is linked to the original negotiations. In this paper we address the strategic role of stakeholders in bilateral confrontations that take the form of a war of attrition; we assume that the bilateral confrontation runs concurrently with the parties interaction with the stakeholder, that chooses strategically her timing to intervene and take action to promote agreement. We show that under complete information the interplay of different interests in this tripartite timing game results in delayed outcomes. We also explore the role of incomplete information and show that asymmetries of information do not necessarily translate in increased inefficiency. D 2005 Elsevier B.V. All rights reserved.	attrition (website);bilateral filter;serializability	Paola Manzini;Clara Ponsati	2005	Mathematical Social Sciences	10.1016/j.mathsocsci.2005.01.007	public relations;war of attrition;stakeholder;actuarial science;economics;microeconomics;mathematical economics;law;complete information	AI	-5.645119552583132	-6.431583320305602	184231
f2376e43b18a7e528a0c44298edd56dac4a8fc34	stochastic stability in one-way flow networks	null	This paper considers one-way flow network formation games in which transmission through a series of agents is subject to decay. We analyze the myopic best-response dynamics of network formation games, occasionally perturbed by state-dependent random noises. Specifically, if an agent is isolated or has a direct neighbor who is better paid, it is more likely that the agent will make mistakes. Our main result identifies that only empty and wheel networks are candidates for long-term outcomes.	one-way function	Zhiwei Cui;Shouyang Wang;Jin Zhang;Lei Zu	2013	Mathematical Social Sciences	10.1016/j.mathsocsci.2013.09.003	operations management;mathematical economics	Theory	-5.419051562005466	-5.216749500764559	184401
69797e341f94d78a6b2b7edcf653aa76bf41c188	temporal preference optimization as weighted constraint satisfaction	disjunctive temporal problem;optimal solution;search space;efficient algorithm;search strategy;constraint satisfaction;expressive power;temporal constraints;linear program;sat solver;temporal reasoning	We present a new efficient algorithm for obtaining utilitarian optimal solutions to Disjunctive Temporal Problems with Preferences (DTPPs). The previous state-of-the-art system achieves temporal preference optimization using a SAT formulation, with its creators attributing its performance to advances in SAT solving techniques. We depart from the SAT encoding and instead introduce the Valued DTP (VDTP). In contrast to the traditional semiring-based formalism that annotates legal tuples of a constraint with preferences, our framework instead assigns elementary costs to the constraints themselves. After proving that the VDTP can express the same set of utilitarian optimal solutions as the DTPP with piecewise-constant preference functions, we develop a method for achieving weighted constraint satisfaction within a meta-CSP search space that has traditionally been used to solve DTPs without preferences. This allows us to directly incorporate several powerful techniques developed in previous decision-based DTP literature. Finally, we present empirical results demonstrating that an implementation of our approach consistently outperforms the SAT-based solver by orders of	algorithm;boolean satisfiability problem;constraint logic programming;constraint satisfaction;disjunctive normal form;distributed transaction;mathematical optimization;pollack's rule;semantics (computer science);solver	Michael D. Moffitt;Martha E. Pollack	2006			mathematical optimization;constraint satisfaction;computer science;linear programming;machine learning;boolean satisfiability problem;expressive power;algorithm	AI	-7.644119862880369	3.8076559076385266	184458
5072917bb74aad579fcbc0f4504da36bb0113092	auctions with severely bounded communication	asymmetric auction;communication complexity;profit maximization	We study au tions with severe bounds on the ommuniation allowed: ea h bidder may only transmit t bits of information to the au tioneer. We onsider both welfaremaximizing and revenue-maximizing au tions under this ommuni ation restri tion. For both measures, we determine the optimal au tion and show that the loss in urred relative to un onstrained au tions is mild. We prove non-surprising properties of these kinds of au tions, e.g. that dis rete pri es are informationally eÆ ient, as well as some surprising properties, e.g. that asymmetri au tions are better than symmetri ones.	limbo;rete algorithm	Liad Blumrosen;Noam Nisan;Ilya Segal	2007	J. Artif. Intell. Res.	10.1613/jair.2081	combinatorial auction;unique bid auction;computer science;common value auction;communication complexity;forward auction	Theory	-5.032995619446554	-0.04536246517094661	184618
1933a0735a1a4fb2d7469d1b5c7450dca67c7671	stable marriage and roommate problems with individual-based stability	coalition formation;teamwork;game theory cooperative and non cooperative;coordination	Research regarding the stable marriage and roommate proble m has a long and distinguished history in mathematics, computer sc i nce and economics. Stability in this context is predominantly core stability o r ne of its variants in which each deviation is by a group of players. There has been l ittle focus in matching theory on stability concepts such as Nash stabilit y and individual stability in which the deviation is by a single player. Such stab ility concepts are suitable especially when trust for the other party is limite d, complex coordination is not feasible, or when only unmatched agents can be app ro ched. Furthermore, weaker stability notions such as individual stabilit y may in principle circumvent the negative existence and computational complexi ty results in matching theory. We characterize the computational complexity of ch ecking the existence and computing individual-based stable matchings for the ma rriage and roommate settings. One of our key computational results for the stabl e marriage setting also carries over to di fferent classes of hedonic games for which individual-based s tability has already been of much interest.	cns;computation;computational complexity theory;decision problem;np-completeness;nash equilibrium;non-functional requirement;numerical stability;stable marriage problem;time complexity	Haris Aziz	2013			simulation;teamwork	AI	-5.374469100253147	-0.27683686528929546	184746
4aaf2ee083b7365fe6d4d7b78f5fd02df269a2b4	morvam: a reverse vickrey auction system for mobile commerce	expert systems;bid privacy;reverse vickrey auction;mobile commerce;internet auction;vickrey auction;mobile agent;automated negotiation;expert system;internet auctions	Internet auctions bring buyers and sellers together for the purpose of trading goods and services online. In order to get the goods, a buyer must search for items through several auction sites. When the auction starts, the buyer needs to connect to these auction sites frequently so that he/she can monitor the bid states and re-bid. In this paper, we propose an automated negotiation model between two participants, for mobile commerce, using collaborative mobile agents called MoRVAM, which mediates between the buyer and the sellers, and executes bidding asynchronously and autonomously. A new RVT protocol is also implemented to achieve unconditional bid privacy. Advantages of the RVT protocol are addressed as well. All the bidding process can be implemented without revealing losing bid and unnecessary information.		Dong-Her Shih;Binshan Lin;Shin-Yi Huang	2007	Expert Syst. Appl.	10.1016/j.eswa.2006.02.012	spectrum auction;auction sniping;eauction;vickrey auction;combinatorial auction;mobile commerce;generalized second-price auction;unique bid auction;computer science;artificial intelligence;reverse auction;vickrey–clarke–groves auction;proxy bid;revenue equivalence;mobile agent;english auction;double auction;ebidding;bid shading;expert system;auction theory;forward auction	Mobile	-8.103994900280254	-8.51688903122322	184923
f1d1186ee6602b5457768f91272a1652effe2514	pure strategy equilibria of single and double auctions with interdependent values	monotonic equilibrium;asymmetric auction;interdependent values;pure strategy equilibrium;equilibrium existence in auctions;market for lemmons;positive probability of trade;winner s curse;tie breaking rule;winners curse;equilibrium existence in auctions pure strategy nash equilibrium monotonic equilibrium tie breaking rule positive probability of trade market for lemmons winners curse;double auction;pure strategy nash equilibrium	We prove the existence of monotonic pure strategy equilibrium for many types of asymmetric auctions among n bidders with unitary demands, interdependent values and independent types. The assumptions require monotonicity only in the own bidder's type and the payments can be function of all bids. Thus, we provide a new equilibrium existence result for asymmetrical double auctions.	interdependence	Aloisio Araujo;Luciano Irineu de Castro	2009	Games and Economic Behavior	10.1016/j.geb.2007.10.006	epsilon-equilibrium;trembling hand perfect equilibrium;economics;winner's curse;common value auction;correlated equilibrium;microeconomics;mathematical economics;welfare economics;equilibrium selection;symmetric equilibrium;statistics	ECom	-4.7160389909148765	-2.8468596812602778	185017
99d3b07ac9bc966d0fc3d1d131a1731215ab7a21	a note on the risk aversion of informed newsvendors		AbstractThe order behaviour of newsvendors has been extensively analysed in the behavioural operations literature and a robust observation has been that average order quantities are between expected-profit-maximising quantities and mean demand. This “pull-to-center” effect has been explained by anchoring, demand-chasing, inventory error minimisation, and other decision heuristics and biases. Risk preferences have been ruled out as an explanation of order behaviour, which we believe might have been premature. Risk preferences vary between people and understanding the effect of risk preferences on ordering requires an analysis at the individual level and not only on the group level, which is the dominant approach used in the literature. In a controlled laboratory experiment, we measure individual risk preferences and analyse how they relate to order quantities. We find a significant correlation between individual risk preferences and order quantities, which indicates that risk preferences affect order behav...	risk aversion	Michael Becker-Peth;Ulrich Wilhelm Thonemann;Torsten Gully	2018	JORS	10.1080/01605682.2017.1390525	management science;anchoring;computer science;welfare economics;newsvendor model;decision support system;risk aversion;heuristics;minimisation (psychology)	NLP	-4.792100705866936	-8.691168108743003	185169
69f745402a73f66358ef4cf14e0d1979307b99c2	fuzzy logic for rule-based formant speech synthesis	fuzzy logic;rule based;speech synthesis	Fuzzy set theory and fuzzy logic has been initiated by Zadeh back in 1965 to permit the treatment of vague, imprecise, and ill-defined knowledge in an concise manner. One of the unique advantages of fuzzy logic is that it is capable of directly incorporating and utilizing qualitative and heuristic knowledge in the form of causal if-then production rules for reasoning and inference. On the other hand, rule-based speech synthesis based on formants makes considerable use of rules for numerous of the tasks it involves, e.g. graphemic to phonemic transcription, coarticulation, concatenation, and duration rules etc. These rules also take the if-then form with their antecedent (condition) part describing the context of the rule and their decedent an appropriate action to be taken. The main motivation for introducing fuzzy logic in the synthesis-by-rule paradigm, is its ability to host and treat uncertainty and imprecision both in the condition part of the rule as well as its decedent part. This may be argued to significantly reduce the number of required rules while rendering them more meaningful and human-like.	causal filter;concatenation;display resolution;fuzzy control system;fuzzy logic;fuzzy set;heuristic;level of measurement;logic programming;natural language;numerical analysis;programming paradigm;set theory;speech synthesis;tract (literature);transcription (software);vagueness	Spyros Raptis;George Carayannis	1997			rule-based system;speech recognition;artificial intelligence;fuzzy logic;pattern recognition;fuzzy set;fuzzy associative matrix;inference;heuristic;fuzzy control system;computer science;fuzzy classification	AI	-18.98159145983111	1.0038182708586514	185189
70ba8fd0638ed0f287b4c70dc62d9c6c7efc78e5	dsp: digital sound processing				Bill Sack	2003	Computer Music Journal	10.1162/comj.2003.27.1.92	audio signal processing	Graphics	-14.558520317677887	-8.522830075871735	185224
431b1464d2fe6b72d56d2848f3d96a8809c2505e	a reputation management approach for resource constrained trustee agents	servicing capacity;constraint optimization;existing trust model;collective capacity;reputation management approach;dynamic multi-agent environment;resulting reputation score;trustee agent;reputable trustee;dynamic multi-agent system	Trust is an important mechanism enabling agents to self-police open and dynamic multi-agent systems (ODMASs). Trusters evaluate the reputation of trustees based on their past observed performance, and use this information to guide their future interaction decisions. Existing trust models tend to concentrate trusters’ interactions on a small number of highly reputable trustees to minimize risk exposure. When a trustee’s servicing capacity is limited, such an approach may cause long delays for trusters and subsequently damage the reputation of trustees. To mitigate this problem, we propose a reputation management approach for trustee agents based on distributed constraint optimization. It helps a trustee to make situation-aware decisions on which incoming requests to serve and prevent the resulting reputation score from being affected by factors out of the trustee’s control. The approach is evaluated through theoretical analysis and within a simulated, highly dynamic multi-agent environment. The results show that it can achieve close to optimally efficient utilization of the trustee agents’ collective capacity in an ODMAS, promotes fair treatment of trustee agents based on their behavior, and significantly outperforms related work in enhancing social welfare.	block cipher mode of operation;constrained optimization;crowdsourcing;digital media;distributed constraint optimization;e-commerce;interaction;interactivity;mathematical optimization;multi-agent system;quantum fluctuation;reputation management;trust management (information system);user experience	Han Yu;Chunyan Miao;Bo An;Cyril Leung;Victor R. Lesser	2013			actuarial science	AI	-10.79918371052295	-9.536206098599736	185260
3dc52d1f16d150d3c391745fbcae309f379b6217	the myerson value for complete coalition structures	complete coalition structure;potential;the myerson value;post print;union stable structure	In order to describe partial cooperation structures, this paper introduces complete coalition structures as sets of feasible coalitions. A complete coalition structure has a property that, for any coalition, if each pair of players in the coalition belongs to some feasible coalition contained in the coalition then the coalition itself is also feasible. The union stable structures, which constitute the domain of the Myerson value, are a special class of the complete coalition structures. As an allocation rule on complete coalition structures, this paper proposes an extension of the Myerson value for complete coalition structures and provides an axiomatization. JEL classification: C71.	axiomatic system	Takashi Ui;Hiroyuki Kojima;Atsushi Kajii	2011	Math. Meth. of OR	10.1007/s00186-011-0371-4	potential;core;mathematical economics	AI	-6.044381626665673	-2.1119499294302706	185372
e1b43740ba41a582d64568ea228d079ae79da98a	on the protection of private information in machine learning systems: two recent approches		The recent, remarkable growth of machine learning has led to intense interest in the privacy of the data on which machine learning relies, and to new techniques for preserving privacy. However, older ideas about privacy may well remain valid and useful. This note reviews two recent works on privacy in the light of the wisdom of some of the early literature, in particular the principles distilled by Saltzer and Schroeder in the 1970s.	algorithm;machine learning;needham–schroeder protocol;privacy	Martín Abadi;Úlfar Erlingsson;Ian J. Goodfellow;H. Brendan McMahan;Ilya Mironov;Nicolas Papernot;Kunal Talwar;Li Zhang	2017	2017 IEEE 30th Computer Security Foundations Symposium (CSF)	10.1109/CSF.2017.10	privacy by design;machine learning;computer science;artificial intelligence;private information retrieval;privacy software;training set;information privacy;data modeling	Security	-13.834616012840456	-0.7369689481090025	185558
0f748cbb9023ad96663b3b535bc2c92ba54810e5	a time-series eye-fixation analysis of the similarity-compromise effect in multi-alternative choice		In decision-making tasks with two attributes and three alternatives, the similarity effect implies that, if the total expected utility is equal between two opposite alternatives (i.e., the target and competitor), the probability of the target being chosen decreases with the addition of the decoy similar to the target. This study demonstrated the similarity-compromise effect, wherein the probability of the target being chosen increased with the addition of the decoy, under the same conditions as the similarity effect, by setting all attribute values of three alternatives to broken numbers rather than rounded numbers. To determine the mechanism underlying this effect, we examined information acquisition patterns using eye-movement data collected from 37 undergraduates who made 10 hypothetical purchase tasks with two attributes and three alternatives. Timeseries analysis of fixation time for the three alternatives revealed dynamic temporal features distinct from those of attraction and compromise effects observed in our previous research.	expected utility hypothesis;mcgurk effect;time series	Takashi Tsuzuki;Itsuki Chiba	2017			psychology;social psychology;cognitive psychology;fixation (visual);compromise	HCI	-5.496443658675404	-7.909716409268782	185608
9885199491cecd427420dedf88ac7d09ede623dd	hr - automatic concept formation in finite algebras		We are investigating how and why mathematicians invent new concepts while developing a theory, and implementing our ideas into the HR system, which automatically produces, assesses and displays concepts in finite algebras, such as finite group theory. We fit’st determined a reason for HI~ to produce concepts to classify a given set of groups up to isomorphism. Doing so would involve inventing concepts which help describe groups, so a classification can occur, and inventing concepts which help generate new examples of groups, so that improvements to the classification are necessitated, perpetuating the process. Next, we developed measures to tell us how interesting the concepts produced were (see Colton 1997). This helped us determine the kinds of concepts HR. should produce and with this in mind, we implemented production rules taking one (or two) concepts as input and outputting a new concept.	concept learning;mind	Simon Colton	1998				AI	-15.831473269876916	1.7139534409014658	185654
6c29a37982a3342d9cd0f9c78535b5dd477729ed	false but slow: evaluating statements with non-referring definites		One central debate in the analysis of denite descriptions concerns the truthvalue of sentences where there is no entity that meets the description in the denite. Classical Russellian accounts predict them to be plain false, whereas presuppositional accounts predict them to be infelicitous. Recent discussions have homed in on the factors that aect actual judgment behavior in relation to the underlying status posited by dierent accounts. This paper presents experimental evidence for a presuppositional view based on response times for judging statements with non-referring denites to be ‘false’, which were longer relative to control statements where existence was asserted. I discuss the theoretical implications of these results, as well as of other ndings from the literature, arguing that they support a presuppositional view of denites that sees the existence presupposition as conventionally encoded. The paper also makes a methodological contribution, as systematic evidence on speakers’ judgments in these cases turns out to be hard to come by. Finally, the results inform the more general issue of the online processes involved in the interpretation of presupposed, as opposed to asserted, content.		Florian Schwarz	2016	J. Semantics	10.1093/jos/ffu019	epistemology;social psychology	PL	-13.485615879490577	3.0249071368257403	185665
96d29ff0bbdb244816dd219da3b7122c51c639fd	computer facility centralization/ decentralization: a multiobjective analysis model		The trend toward centralization of computer facilities appears to have been reversed with recent substantial deereases in hardware costs. Nevertheless, this trend is often fought vigorousIy by firmly entrenched centralized computer departments. This paper presents an application of goal programming to the multiple conflicting objectives of proponents and opponents of decentralization. The model analyzes the effect of a decision to decentralize on revenue to the central facility, the impact on staffing levels, the impact on users remaining on the central facility after decentralization, and the level of computer resources available to the various users.	centralized computing;goal programming	Sang Min Lee;Susan J. Wilkins	1983	Computers & OR	10.1016/0305-0548(83)90024-2	management science;operations research	Networks	-8.421526314652773	-5.773989205145901	185681
3b5952b08ce79d6f8f49511a6ebc6875b1a8ce05	the impact of verbal interaction on driver lateral control: an experimental assessment	experimental design;vehicle control;traffic accident;auditory interface;auditory interfaces;public safety;driving simulator;cognitive distraction;intelligent vehicles;game based simulation;collision avoidance;young adult;auditory distraction;article;national highway traffic safety administration	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;lateral thinking;primary source	Nikolaos Gkikas;John Richardson	2012	Behaviour & IT	10.1080/0144929X.2010.518247	simulation;young adult;engineering;forensic engineering;design of experiments;computer security;statistics	Robotics	-15.54090513916188	-6.391668986844017	185792
e32c133ca46f495a23cd36e543e6cd09e58e5057	the miracle of applied mathematics	realism;matematicas aplicadas;fictionnalisme;mathematiques appliquees;physique;electromagnetisme;fisica;physics;mathematics philosophy;antirealism;fictionalism;quine w v o;antirealisme;philosophy;loi scientifique;realisme;epistemology;field h;philosophy and religious studies;expanding knowledge;electromagnetism;electromagnetismo;applied mathematics;scientific law;prediction;new physics;expanding knowledge in philosophy and religious studies;philosophie des mathematiques;wigner e p	Mathematics has a great variety ofapplications in the physical sciences.This simple, undeniable fact, however,gives rise to an interestingphilosophical problem:why should physical scientistsfind that they are unable to evenstate their theories without theresources of abstract mathematicaltheories? Moreover, theformulation of physical theories inthe language of mathematicsoften leads to new physical predictionswhich were quite unexpected onpurely physical grounds. It is thought by somethat the puzzles the applications of mathematicspresent are artefacts of out-dated philosophical theories about thenature of mathematics. In this paper I argue that this is not so.I outline two contemporary philosophical accounts of mathematics thatpay a great deal of attention to the applicability of mathematics and showthat even these leave a large part of the puzzles in question unexplained.	theory	Mark Colyvan	2001	Synthese	10.1023/A:1010309227321	physics beyond the standard model;prediction;electromagnetism;philosophy;epistemology;scientific law;fictionalism;mathematics;realism	HCI	-11.259809719130628	3.656056459102798	186020
6408214b81d5ca4938b5dec91de1e1a31d7d8118	efficiency of competitive equilibria in economies with time-dependent preferences	hyperbolic discounting;time dependent preferences;competitive equilibrium;first fundamental welfare theorem	This paper focuses on welfare properties of equilibria in exchange economies with time-dependent preferences. We reintroduce the notion of time-consistent overall Pareto efficiency proposed by Herings and Rohde (2006) and show that, whenever all agents in the economy are sophisticated, any equilibrium allocation is efficient in this sense. Therefore, we present a version of the First Fundamental Welfare Theorem for this class of economies.		Pawel Dziewulski	2015	J. Economic Theory	10.1016/j.jet.2015.07.003	economics;macroeconomics;microeconomics;mathematical economics;welfare economics	ECom	-5.156490301129059	-2.14305933285746	186086
9204bc4f98f3edc7c959aa55c306d856442ebd50	computation of completely mixed equilibrium payoffs in bimatrix games	jel classification number c72;two person games;equilibrium;computation;bimatrix game	Computing the (Nash) equilibrium payoffs in a given bimatrix game (i.e., a finite two-person game in strategic form) is a problem of considerable practical importance. One algorithm that can be used for this purpose is the Lemke-Howson algorithm (Lemke and Howson (1964); von Stengel (2002)), which is guaranteed to find one equilibrium. Another, more elementary, approach is to compute the equilibrium payoffs by: (1) “guessing” the support of the equilibrium, i.e., the set of pure strategies each player uses with positive probability (which can always be done by systematically checking all possibilities, if necessary); (2) finding a pair of mixed strategies with this support, such that all the pure strategies designated to either player give that player the same payoff against the other player’s mixed strategy; (3) computing the corresponding payoffs; and, finally, (4) checking that none of the other pure strategies of either player gives that player a higher payoff. For an equilibrium with known support, the equilibrium payoffs are easier to compute. Indeed, it is shown below that, in the case of known support, steps (1), (2), and (4) can essentially be dispensed with. Examples of bimatrix games having equilibria with known supports include generalized rock–scissors–paper games, such as the (symmetric) game with payoff matrix	computation;lemke's algorithm;lemke–howson algorithm;nash equilibrium	Igal Milchtaich	2006	IGTR	10.1142/S021919890600103X	markov perfect equilibrium;game theory;mathematical optimization;sequential equilibrium;lemke–howson algorithm;computation;repeated game;mathematics;mathematical economics;subgame perfect equilibrium;equilibrium selection;symmetric game;symmetric equilibrium	ECom	-5.641572309053588	-1.9174346436775573	186150
0bf0b9c6fd476a79da5ab47f9c7cc567b7dac861	sports strategy analytics using probabilistic reasoning	sport formal verification inference mechanisms markov processes;probabilistic logic games analytical models cognition computational modeling markov processes reachability analysis;markov decision process sports strategy analytics probabilistic reasoning dynamic behavior formal method probabilistic model checking technique tennis;conference item;sport probabilistic model checking strategy consulting mdp;probabilistic model checking;strategy consulting;mdp;sport	The advance of analytics technology has attracted more attention and adoption from sports, although modeling and analyzing the dynamic (and uncertain) behaviors of sports are challenging. Formal methods have been strongly recommended to deal with complex systems by their rigorous semantics and powerful reasoning capabilities. In this paper, we present our initiative as the first to apply probabilistic model checking techniques to strategy analytics for tennis based on Markov Decision Processes (MDP). Our approach can derive insights such as prediction of winning chances and identification of best improvement. We evaluate the effectiveness of our approach through real-life case study.	complex systems;formal methods;markov chain;markov decision process;model checking;overhead (computing);real life;reasoning system;statistical model	Jin Song Dong;Ling Shi;Le Vu Nguyen Chuong;Kan Jiang;Jing Sun	2015	2015 20th International Conference on Engineering of Complex Computer Systems (ICECCS)	10.1109/ICECCS.2015.28	simulation;probabilistic relevance model;computer science;artificial intelligence;sport;machine learning;data mining;probabilistic logic	SE	-18.288167524197572	-1.7727080354159683	186156
62bd1a56b186b04c4870b2d17ba076b3f5b925f5	social choice without the pareto principle: a comprehensive analysis		This article provides a systematic analysis of social choice theory without the Pareto principle, by revisiting the method of Murakami Yasusuke. This article consists of two parts. The first part investigates the relationship between rationality of social preference and the axioms that make a collective choice rule either Paretian or anti-Paretian. In the second part, the results in the first part are applied to obtain impossibility results under various rationality requirements of social preference, such as S-consistency, quasi-transitivity, semi-transitivity, the interval-order property, and acyclicity. Copyright Springer-Verlag 2012	pareto efficiency	Susumu Cato	2012	Social Choice and Welfare	10.1007/s00355-011-0564-z	economics;mathematics;microeconomics;mathematical economics;social psychology;welfare economics	ECom	-7.455868081731986	-1.4890441991544825	186351
2e26baa934a8b0599b36edf31f0d1c7d91b97666	stable coalition structures under restricted coalitional changes	stability;coalition formation;farsighted players;cournot oligopoly;common pool resource	In this paper, we examine whether farsighted players form the e¢ cient grand coalition structure in coalition formation games. We propose a stability concept for a coalition structure, called sequentially stability, when only bilateral mergers of two separate coalitions are feasible because of high negotiation costs. We provide an algorithm to check the sequential stability of the grand coalition structure as well as su¢ cient conditions for which the e¢ cient grand coalition structure is sequentially stable. We also illustrate out results by means of common pool resource games and Cournot oligopoly games. JEL classication codes: C70; C71; D62.	algorithm;bilateral filter;code	Yukihiko Funaki;Takehiko Yamato	2014	IGTR	10.1142/S0219198914500066	cournot competition;common-pool resource;stability;economics;core;microeconomics;mathematical economics;welfare economics;statistics	AI	-5.210085659978444	-2.860221598135872	186379
fd68f8e85f9dc7f51d4650610a7f46ae830d7804	towards a model of goal autonomous agents	economic model;autonomous agent	In this paper we sketch a model in which agents are autonomous not only because they can choose among alternative courses of action (executive autonomy), but also because they can select and pursue new goals on the basis of endogenously generated interests (goal autonomy). We use economic models to argue that goal autonomy can be defined by introducing a precise notion of an agent’s identity. 1 Executive vs. goal autonomy The notion of autonomy is central to many definition of (human and software) agent. However, in many agent models – including BDI models – autonomy is thought of as goal-directed behavior and as the possibility of choosing among alternative courses of action. This notion of autonomy, which Castelfranchi [1] calls executive autonomy, is very weak, and some authors would argue that it is not autonomy at all. Executive autonomy is strictly related to traditional theories of choice, based on the paradigm that choosing implies deciding the best course of action in order to achieve a goal [5]. As March pointed out [2, 3], this means-ends paradigm presupposes an anticipatory, causative, consequential type of rationality. The strong limitation of this view is that it presupposes that preferences and goals, once set, cannot conflict with the interest of the agent. However, this form of rationality produces rational behavior only if the environment is stable (or changes in a predictable way), and an agent has complete knowledge about it. Otherwise, it may happen that agents, with no control on their goals, can irrationally pursue unrealistic goals or evaluate goals on the basis of unrealistic preferences. To overcome these limitations, we suggest that an agent must be goal autonomous, namely must have the possibility to decide not only how to achieve a goal, but also which goals are to be preferred and pursued on the basis of an endogenously generated interest. The main object of our research is to propose an agent model in which the source of such an interest is found in a very general principle, which we call the principle of sunk costs, which in turn is strongly related to a form of rationality, which March [4] calls ex-post rationalization. 2 Identity and the principle of sunk costs March suggests that rational agents are entities that not only can set appropriate courses of action (including sub-goals) to achieve a given goal, but can also change their mind about their top level goals and preferences when planned achievements become unrealistic. The research question now is: is there any principled way in which we can explain when and how agents should adopt new goals or change their preferences? Our research theses are: that such a principled explanation is possible; that is based on the notion of an agent’s identity; that an agent’s identity can be defined in terms of economical principles, namely economies of scale and irreversibility of investments. In short, the idea is the following. First of all, it is clear that agents sustain costs to acquire a capability or the right to use a resource. Not always these costs are (completely) reversible, and this generates sunk costs. Therefore, the more such a capability (resource) is used, the more its costs are amortized (economies of scale effect). Under this respect, we believe that acquired capabilities (resources) are an essential part of an agent identity (what an agent is), and play a crucial role in deciding what goals are to be preferred and pursued on the basis of endogenously generated interests (not using an available capability, especially when it is not reversible, implies a loss of value generated by the lost opportunity of a cost saving!!). The conclusion is that rational agents should consider not only the current costs of achieving a goal, but also the losses generated by the non-use of sunk investments. Now the point is that in a non predictable environment, circumstances can lead an agent to develop and acquire resources that, to some extent, have no use in order to achieve the current goal. Our thesis is that sometimes the cost of changing one’s mind about what is desirable is lower than the cost of going on in the pursuit of current intentions. This happens when, in the decision function, the weight of sunk costs overcomes the weight of current opportunities. In such a situation, instead of reasoning about means necessary to achieve ends that happen to be irrational, rational agents may rationalize their current state as an end which is appropriate to his means, and to change their preferences accordingly. In this sense the sunk cost effect is an attempt to demonstrate the rationality of behaviors that are otherwise not explained and thus labelled as “irrational” by traditional theories of rationality.	agent-based model;amortized analysis;autonomous robot;autonomy;cristiano castelfranchi;entity;mind;programming paradigm;rational agent;rationality;reversible computing;software agent;theory	Matteo Bonifacio;Paolo Bouquet;Roberta Ferrario;Diego Ponte	2002			autonomous agent;autonomy;sketch;economic model;political science;knowledge management	AI	-11.976406220895871	-1.277279399227732	186440
aa488ffe83923cd213e4700c2ed7e66396b30806	on neutrality with multiple private and public goods	bepress selected works;public goods neutrality private provision equilibrium voluntary contributions	We obtain an analogue of the neutrality result of Warr (1983) and Bergstrom et al. (1986) for economies with both multiple private and public goods.		Marta Faias;Emma Moreno-García;Myrna Holtz Wooders	2015	Mathematical Social Sciences	10.1016/j.mathsocsci.2015.04.005	public good;economics;public economics;microeconomics	ECom	-4.8351435287636315	-4.326574692330223	186545
fbc9a81e7df3e437d8a1e924f258a353975ea95f	pascal poncelet, florent masseglia, maguelonne teisseire: successes and new directions in data mining	data mining	Successes and New Directions in Data Mining, is a fairly recent book, being published in late 2007, well presented with around 360 pages divided into 13 independent chapters, and although it has some editing errors, such as the table of contents not matching all chapters, and also grammar mistakes in some of the chapters, these are really minor and do not compromise the understanding or novelty of the work. Its stated goal is to provide theoretical frameworks and present challenges and their possible solutions concerning knowledge extraction, in other words the book attempts to provide a critical understanding of such frameworks to the reader with alternative solutions to common problems. As also stated, it aims at providing an overall view of the recent existing solutions for data mining with a particular emphasis on the potential real world applications. A common critique to books with such a wide goal is that it is very difficult, if possible, to fully accomplish the goal in a limited amount of pages with such an active research community. In terms of content, the first chapter is entitled Why Fuzzy Set Theory is Useful in Data Mining and it presents a considerably easy to understand overview of fuzzy sets and demonstrates through simple examples how fuzzy logic can be used to avoid the (argued) issues related to the inadequacy of classical logic to some problems. It presents some interesting arguments such as that many features and patterns of interest are inherently fuzzy and modelling them in a nonfuzzy way will inevitably lead to unsatisfactory results and that the increased expressiveness of fuzzy methods is useful for both feature expression and dependency analysis. The Chapter aims to provide convincing evidence for the assertion that fuzzy set theory can contribute to data mining in a substantial way, but unfortunately it does not make a full cover in the differences of imprecision and uncertainty, which as explained by Almond (1995) are related to fuzzy logic (when it is not possible to accurately predict the behaviour of the average) and probability theory (when it is not known what it is going to happen in a	assertion (software development);book;data mining;dependence analysis;fuzzy logic;fuzzy set;pascal;set theory	Renato Cordeiro de Amorim	2008	Information Retrieval	10.1007/s10791-008-9068-6	computer science;data science;data mining;database	ML	-14.497907733264881	0.7453392012314489	186578
2d548312fbbf60ac1b83ba3138f2205bd7fa3bfe	single-crossing, strategic voting and the median choice rule	strategyproofness;strategy proofness;median voter;order restriction;strategic voting;neue politische okonomie;social choice;majority voting;medianwahler modell;single crossing;working paper	This paper studies the strategic foundations of the Representative Voter Theorem (Rothstein, 1991), also called the “second version” of the Median Voter Theorem. As a by-product, it also considers the existence of non-trivial strategy-proof social choice functions over the domain of single-crossing preference profiles. The main result presented here is that single-crossing preferences constitute a domain restriction on the real line that allows not only majority voting equilibria, but also non-manipulable choice rules. In particular, this is true for the median choice rule, which is found to be strategy-proof and group-strategic-proof not only over the full set of alternatives, but also over every possible policy agenda. The paper also shows the close relation between single-crossing and order-restriction. And it uses this relation together with the strategy-proofness of the median choice rule to prove that the collective outcome predicted by the Representative Voter Theorem can be implemented in dominant strategies through a simple mechanism in which, first, individuals select a representative among themselves, and then the representative voter chooses a policy to be implemented by the planner. JEL Classification: D70, D71, D78.	voter model	Alejandro Saporiti;Fernando A. Tohmé	2006	Social Choice and Welfare	10.1007/s00355-006-0098-y	bullet voting;may's theorem;majority rule;social choice theory;economics;arrow's impossibility theorem;public economics;approval voting;microeconomics;cardinal voting systems;mathematical economics;law;welfare economics;anti-plurality voting;condorcet method;disapproval voting	ECom	-7.256741537012116	-1.7534674546308753	187130
0bbbf558854984135ae597693b1ce37eddb8fc7e	failure mode and effect analysis for abductive diagnosis		Diagnosis, i.e., fault localization in case of observing an unexpected behavior, is an important practical problem. During the past decades researchers have suggested several approaches for using models of the systems directly for identifying the root causes of failure. This model-based diagnosis approaches are either based on retaining consistency or on abduction. Despite their advantages both approaches are only rarely used in practical applications. In this paper, we focus on bringing abductive diagnosis closer to its application. In particular, we describe how failure mode and effect analysis, a technique of growing interest in applications, can be directly mapped to abductive diagnosis models. We discuss the basic foundations, and also problems that occur and how to deal with them. A direct conversion of FMEAs to abductive diagnosis problems would increase the use of abduction in practice because of avoiding writing logical theories directly.	abductive reasoning;algorithm;application domain;computation;failure cause;failure mode and effects analysis;fundamental fysiks group;goto;knowledge base;theory;usability	Franz Wotawa	2014			reliability engineering;failure mode and effects analysis;mathematics	AI	-18.22773389579039	0.3327111762624436	187377
6f714b8f8a84b42c5cfbab0df38575fe477d6f39	revisiting agent-based models of algorithmic trading strategies		Algorithmic trading (AT) strategies aim at executing large orders discretely, in order to minimize the order’s impact, whilst also hiding the traders’ intentions. The contribution of this paper is twofold. First we presented a method for identifying the most suitable market simulation type, based on the specific market model to be investigated. Then we proposed an extended model of the Bayesian execution strategy. We implemented and assessed this model using our tool AlTraSimBa (ALgorithmic TRAding SIMulation BAcktesting) against the standard Bayesian execution strategy and naive execution strategies, for momentum, random and noise markets, as well as against historical data. Our results suggest that: (i) momentum market is the most suitable model for testing AT strategies, since it quickly fills the Limit Order book and produces results comparable to those of a liquid stock; (ii) the priors estimation method proposed in this paper (-) within the Bayesian adaptive agent model (-) can be advantageous in relatively stable markets, when trading patterns in consecutive days are strongly correlated, and (iii) there exists a trade-off between the frequency of decision making and more complex decision criteria, on one side, and the negative outcome of lost trading on the agents’ side due to them not participating actively in the market for some of the execution steps.	agent-based model;algorithmic trading	Natalia Ponomareva;Ani Calinescu	2014	Trans. Computational Collective Intelligence	10.1007/978-3-662-44871-7_4	financial economics	ECom	-6.550128565259497	-9.013387556458552	187506
744f2849920e4a5e74d0208e2b5b5875e31cfdb8	social choice and electoral competition in the general spatial model	social preferences;b economie et finance;electoral competition;social choice;simple game;spatial model	This paper extends the theory of the core, the uncovered set, and the related undominated set to a general set of alternatives and an arbitrary measure space of voters. We investigate the properties of social preferences generated by simple games; we extend results on generic emptiness of the core; we prove the general nonemptiness of the uncovered and undominated sets; and we prove the upper hemicontinuity of these correspondences when the voters’ preferences are such that the core is nonempty and externally stable. Finally, we give conditions under which the undominated set is lower hemicontinuous. © 2004 Elsevier Inc. All rights reserved.	hemicontinuity;norman margolus;semi-continuity	Jeffrey S. Banks;John Duggan;Michel Le Breton	2006	J. Economic Theory	10.1016/j.jet.2004.08.001	social choice theory;economics;social preferences;public economics;microeconomics;mathematical economics;welfare economics	AI	-6.25103651208869	-1.7178828546832885	187641
73a357810d86bd7fc56e86886ed323a2a8e5a8a9	an approach to threat assessment based on evidential networks	local computations;theory of evidence;joint binary tree;threat assessment;belief function;evidential networks;data fusion;uncertainty bayesian methods australia decision making computer networks binary trees humans knowledge management expert systems possibility theory;valuation based system;theory of evidence threat assessment data fusion valuation based system local computations;sensor fusion decision making military computing;commander decision making;joint binary tree threat assessment evidential networks information fusion system commander decision making valuation based system theory of evidence;information fusion system;local computation;information fusion;sensor fusion;military computing;binary tree	The paper develops an information fusion system that aims at supporting a commander's decision making by providing an assessment of threat, that is an estimate of the extent to which an enemy platform poses a threat based on evidence about its intent and capability. Threat is modelled in the framework of the valuation-based system (VBS), by a network of entities and relationships between them. The uncertainties in the relationships are represented by belief functions as defined in the theory of evidence. Hence the resulting network for reasoning is referred to as an evidential network. Local computations in the evidential network are carried out by inward propagation on the underlying joint binary tree. This allows the dynamic nature of the external evidence, which drives the evidential network, to be taken into account by recomputing only the affected paths in the joint binary tree.	algorithm;automated reasoning;binary tree;computation;direct inward dial;entity;refinement (computing);software propagation;threat model;vbscript;value (ethics)	Alessio Benavoli;Branko Ristic;Alfonso Farina;Martin Oxenham;Luigi Chisci	2007	2007 10th International Conference on Information Fusion	10.1109/ICIF.2007.4408020	engineering;artificial intelligence;machine learning;data mining;evidential reasoning approach	Robotics	-17.67048819208943	-5.46256448694471	187671
a244399eab6c2c4660671dbfb7048ce720042762	the minimum approval mechanism implements the efficient public good allocation theoretically and experimentally	allokation;assumed equilibrium concepts;offentliche guter;test;multiple implementation;approval mechanism;nash gleichgewicht;nichtkooperatives spiel;public good experiment;pareto optimum;working paper	We propose the minimum approval mechanism (MAM) for a standard linear public good environment with two players. Players simultaneously and privately choose their contributions to the public good in the first stage. In the second stage, they simultaneously decide whether to approve the other’s choice. Both contribute what they choose in the first stage if both players approve; otherwise, both contribute the minimum of the two choices in the first stage. The MAM implements the Pareto-efficient allocation in backward elimination of weakly dominated strategies (BEWDS) and is unique under plausible conditions. Contributions in the MAM experiment overall averaged 94.9%. The data support BEWDS rather than subgame perfect Nash equilibria. Quantifying subjects’ responses to the questionnaire showed that the majority of subjects in the MAM found a heuristic or an algorithm named diagonalization and supported the notions of minimax regret and iterated best response, all of which mimic BEWDS outcomes. JEL Classification: C72; C92; D74; H41; P43	algorithm;experiment;heuristic;iteration;minimax;nash equilibrium;pareto efficiency;regret (decision theory);stepwise regression	Takehito Masuda;Yoshitaka Okano;Tatsuyoshi Saijo	2014	Games and Economic Behavior	10.1016/j.geb.2013.10.003	economics;operations management;mathematics;software testing;microeconomics;mathematical economics;welfare economics;statistics	AI	-6.177961481193694	-5.32422438561445	187930
7a8222f23fe96a3fd4ae78e8a557ebf9704c6bbb	sensitivity to communicative norms when deceiving without lying		Much of our interpersonal communication conforms to Gricean-style norms governing the truthfulness, informativeness and relevance of the information exchanged. But we also experience untruthful, uninformative, and misleading communication when these norms are violated. How do people draw upon this experience when attempting to conceal the truth? We introduce a computational model which predicts how people should best conceal the truth when required to reveal information to another (and lying is not an option). We argue that when placed in such situations, people will take into account the other’s expectations of whether Gricean norms apply. This notion is incorporated in our model, which we test with an experiment that manipulates people’s assumptions in this regard. Results show that revealing informative but misleading information is an acceptable strategy when the other expects cooperation; otherwise, being uninformative is overwhelmingly preferred. We analyse how our model and alternatives account for these results.	computational model;experiment;information;relevance;t-norm	Keith Ransom;Wouter Voorspoels;Amy Perfors;Daniel J. Navarro	2015			social psychology;cognitive psychology;psychology;lying	HCI	-13.867780236434724	1.8752062735880146	188455
0dd358e573b9ec6497c23a3812650c273c43fda6	obtaining the consensus and inconsistency among a set of assertions on a qualitative attribute	inconsistency measurement;consensus;hierarchy;information extraction;qualitative data;outlier;assertion;fuzzy logic;confusion;qualitative value;classical logic;inconsistency	It is well understood how to compute the average or centroid of a set of numeric values, as well as their variance. In this way we handle inconsistent measurements of the same property. We wish to solve the analogous problem on qualitative data: How to compute the “average” or consensus of a set of affirmations on a non-numeric fact, as reported for instance by different Web sites? What is the most likely truth among a set of inconsistent assertions about the same attribute? Given a set (a bag, in fact) of statements about a qualitative feature, this paper provides a method, based in the theory of confusion, to assess the most plausible value or “consensus” value. It is the most likely value to be true, given the information available. We also compute the inconsistency of the bag, which measures how far apart the testimonies in the bag are. All observers are equally credible, so differences arise from perception errors, due to the limited accuracy of the individual findings (the limited information extracted by the examination method from the observed reality). Our approach differs from classical logic, which considers a set of assertions to be either consistent (True, or 1) or inconsistent (False, or 0), and it does not use Fuzzy Logic. 1. Previous work and problem statement Assume several measurements are performed on the same property (for instance, the length of a table). One measurer some distance away asserted “3m.” Another person with the help of a meter said “3.13m”. A lady with a micrometer reported “3.1427m.” From these, it is possible to obtain the most likely value (=3.09m, the average length) as well as the dispersion of these measurements (, the variance), perhaps disregarding some outliers. For quantitative measurements we know how to take into account contradicting facts, and we do not regard them necessarily as inconsistent. We just assume that the observers’ gauges have different precisions or accuracies. Let us now consider several asseverations on a single-valued non-numeric variable (such as the killer is) that ranges on qualitative values (such as dog, cat, German Shepherd, Schnauzer...) that can be arranged in a hierarchy (Figure 1). That is, observer 1 reports that the killer is a dog, observer 2 reports that the killer is a cat... Can we find the consensus value or most likely value for the assassin? The “centroid” or “average” of the reported animals? Or, we know that Ossama Bin Laden is reported to hide in {Afghanistan; Beirut; Irak; Kabul; Middle East; Afghanistan; Syria}. What is the most likely value to be true? Intuitively, this is the value that minimizes the sum of disagreements or discomforts for all the observers when they learn of the value chosen as the consensus value. 1 We shall assume that only one value is possible – no two or more killers in our example.	computation;consensus (computer science);fuzzy logic	Adolfo Guzmán-Arenas;Adriana Jimenez-Contreras	2010	Expert Syst. Appl.	10.1016/j.eswa.2009.05.010	fuzzy logic;qualitative property;outlier;classical logic;consensus;assertion;computer science;machine learning;data mining;mathematics;information extraction;algorithm;hierarchy;statistics	AI	-12.083455565068297	1.1118010904386704	188604
c6e9a5b88e298bf32c47c7160d78f5c14980fddd	directions in uncertainty reasoning	uncertainty reasoning;practical performance issue;fundamental philosophical inquiry;ai community;real-world model	‘Uncertainty reasoning” refers in a general way to problems discussed by that subset of the AI community interested in representing and reasoning with knowledge that cannot be expressed as certainties. The range of problems discussed runs the gamut from fundamental philosophical inquiry into the nature of uncertainty and how (if at all) it can be measured and modelled, to practical performance issues arising from the (automatic) construction of real-world models and making inferences from such models.		Eric Neufeld	1997	Knowledge Eng. Review		uncertainty analysis;artificial intelligence;data mining;management science	AI	-15.389904995209374	0.8335033136911999	188695
eb7b8c008d6d790e3f458aa19e28df188f96cec1	inequality, asymmetry and social welfare, and their relationship with the median-mean ratio		Inequality, asymmetry, and the median-mean ratio are related but different concepts, although they have been frequently treated as equivalent in the literature. In this paper, we find important connections between these three concepts under particular conditions. We show that the Atkinson family and the generalized entropy class of inequality indices are simple functions of the medianmean ratio when a distribution is symmetrizable by a power transformation. In such a case, the coefficient of asymmetry can be interpreted as the aversion inequality parameter of a social planner, and a social evaluation function a la Kolm-Atkinson is shown to be equivalent to median income. A social evaluation function that depends only on mean income and inequality of opportunity is found as a by-product. In addition, the Hannah-Kay family of concentration indices (of which the Herfindahl-Hirschman is a special case) is also obtained as a function of the median-mean ratio. We illustrate these results empirically using the C...	social inequality	Juan Gabriel Rodríguez;Rafael Salas	2016	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488516400079	inequality;discrete mathematics;social planner;mathematics;simple function;welfare economics;evaluation function;special case;welfare;social welfare;median income	Robotics	-7.413429760147779	-1.9720910135869407	188915
0f826a4cc37d7de6c4950bd8f1caa3dfdb93e2d9	the reasonable effectiveness of mathematics: partial structures and the application of group theory to physics	mecanique quantique;teoria de los grupos;mathematics;physique;reasonableness;weyl h;efficiency;group theory;physics;theorie des groupes;quantum mechanics;theory of groups;efficacite;structure;mathematiques;raisonnable	Wigner famously referred to the `unreasonable effectiveness' of mathematics in its application to science. Using Wigner's own application of group theory to nuclear physics, I hope to indicate that this effectiveness can be seen to be not so unreasonable if attention is paid to the various idealising moves undertaken. The overall framework for analysing this relationship between mathematics and physics is that of da Costa's partial structures programme.		Steven French	2000	Synthese	10.1023/A:1005246608001	structure;philosophy;epistemology;calculus;pure mathematics;mathematics;efficiency;group theory;quantum mechanics	ML	-10.740953128364646	3.676812660181343	189056
696d137cb41ed7699d7c5fa7b1a14d58d73d50b2	credit-based repeated game model applied in transfer decision of opportunistic network	game theory;credit;transfer decision making;opportunistic network	Because of the characteristic of network topology and application domain, the data transmission in opportunistic network depends on the ‘Store-Transfer’ operation between nodes in it. And there isn’t the fix route or path in it. So the fitness transmitter-node choice is a key for the success of data transmission in network. But the decision-making for transmission is not an easy work. As a smart unit, the nodes do cheating operations for avoiding accept the data transmitter job. To reduce such cheating behavior of nodes, the idea of credit-cooperation and repeated games are involved. For a node, every game is considered as a part of repeated-game in its lifecycle. If it did a cheating-operation in a game, it would face the punishment with long time. And the profit gained from the cheating-operation would be counteracted mutually from the loss of punishment time. Moreover, usually the punishment is bigger than profits in fact. By such method, it minimizes the probability of cheating happened in the game theory and makes the game more efficient.	application domain;game theory;network topology;transmitter	Cheng Zhang;Qingsheng Zhu;Ziyu Chen	2011	JSW		non-cooperative game;cheap talk;simulation;economics;simultaneous game;win-win game;information set;repeated game;strategy;screening game;normal-form game;simulations and games in economics education;algorithmic game theory;sequential game;social psychology;welfare economics	AI	-7.449147272603328	-6.147122558538458	189556
f8d9cf927eef4ed810600b4e633ac3d330fb58e3	cooperation and sharing costs in a tandem queueing network		Abstract We consider a tandem network of queues with a Poisson arrival process to the first queue. Service times are assumed to be exponential. In cases where they are not, we additionally assume a processor sharing service discipline in all servers. Consecutive servers may cooperate by pooling resources which leads to the formation of a single combined server that satisfies the aggregated service demands with a greater service rate. On this basis we define a cooperative game with transferable utility, where the cost of a coalition is the steady-state mean total number of customers in the system formed by its members. We show that the game is subadditive, leading to full cooperation being socially optimal. We then show the non-emptiness of the core, despite the characteristic function being neither monotone, nor concave. Finally, we derive several well-known solution concepts, including the Shapley value, the Banzhaf, value and the nucleolus, for the case where servers have equal mean service demands. In particular, we show that all three values coincide in this case.	queueing theory;tandem computers	Dan Bendel;Moshe Haviv	2018	European Journal of Operational Research	10.1016/j.ejor.2018.04.049	mathematical optimization;pooling;mathematics;queueing theory;queue;transferable utility;cost allocation;shapley value;processor sharing;server	Metrics	-4.647968607539319	-3.4353741682272814	189568
6f2ff94360bcf361e57a022a6c788c0f28f001bb	converse consistent enlargements of the unit-level-core of the multi-choice games	91a;converse consistency;multi choice games;consistency	We extend the reduced games introduced by Moulin (J Econ Theory 36:120–148, 1985) to multi-choice games, and define related properties of consistency and its converse. Since the unit-level-core proposed by Hwang and Liao (J Glob Optim 2009) violates (weak) converse consistency, we propose to minimally enlarge the unit-level-core so as to recover (weak) converse consistency.	glob (programming)	Yu-Hsien Liao	2012	CEJOR	10.1007/s10100-011-0201-z	mathematics;mathematical economics;consistency;algorithm	AI	-6.562408179431097	-1.0611288259084253	189627
b64e716eb0d44eee5f7bcca5ad967f13d1102ec8	lexicographic composition of simple games	fairness;set partitions;semi value;decisiveness;voting game;voting;simple games;indexation;game composition;game decomposition;simple game	"""A two-house legislature can often be modelled as a proper simple game whose outcome depends on whether a coalition wins, blocks or loses in two smaller proper simple games. It is shown that there are exactly ve ways to combine the smaller games into a larger one. This paper focuses on one of the rules, lexicographic composition, where a coalition wins in G1 ) G2 when it either wins in G1, or blocks in G1 and wins in G2. It is the most decisive of the ve. A lexicographically decomposable game is one that can be represented in this way using components whose player sets partition the whole set. Games with veto players are not decomposable, and anonymous games are decomposable if and only if they are decisive and have two or more players. If a players benet is assessed by any semi-value, then for two isomorphic games a player is better o¤ from having a role in the rst game than having the same role in the second. Lexicographic decomposability is sometimes compatible with equality of roles. A relaxation of it is suggested for its practical benets. Acknowledgements: We would like to thank Xingwei Hu for his help. Barry ONeill is grateful to the Cowles Foundation and the Russell Sage Foundation for their support. Section 1. Introduction Group decision rules have been modeled by simple games, dened as those in which a coalition either wins or not, with no outcomes in between (von Neumann and Morgenstern 1944, Shapley and Shubik 1954, Peleg 1984, Taylor and Zwicker 1999, Peleg and Sudholter 2003). One stream of research developed an algebra in which a coalition wins in the product (sum) of two smaller games whenever it wins in both (either) of them (Shapley 1964, Owen 1964, Billera 1980, and others). Shapley (1967) dened the more general idea of a committee, a group of players who can be treated as a decision unit within the larger game. This paper considers a class of composition rules, then focuses on one of them. It shows that there are ve ways to combine two proper games into a third one, where success in the combined game depends only on whether a coalition wins, blocks or loses without blocking in the components. Among the ve is """"lexicographic composition,"""" where a rst group of players has the right to decide, but if there is a deadlock the power passes to a separate group. The name lexicographicis suggested by the ordering of words in the dictionary, which puts azurebefore babblethat is, considers the words rst letter, then its second, etc. The analogy for games is that a coalitions power in the second game, however great, is irrelevant if its rivals win the rst game. The lexicographic rule is especially interesting because it has been overlooked theoretically, has practical advantages and is occasionally used. One attractive feature is associativity, which allows simple games to be strung together without consideration to their grouping, and another is that among the ve rules it is least prone to stalemate. Section 2 gives denitions and example and Section 3 gives some basic properties of the lexicographic rule, showing that there are only ve ways of combining two proper games to make a proper game. The lexicographic rule is the most decisive of them. Section 4 relates power and the order of play: if the two components are identical a player would prefer to be in the rst game, when interests are measured by any semi-value. The next two sections treat decomposability. Even if a game is not played as a physical sequence it may be representable in this way since the criterion is the games winning coalitions, not its realization in the world. We ask when a certain game, even though played as a unit, is equivalent to a decomposed game, and investigate the case where the component games"""	barry boehm;blocking (computing);deadlock;dictionary;game theory;holographic principle;lexicographical order;lexicography;linear programming relaxation;operator associativity;relevance;rule 184;semiconductor industry;stable marriage problem	Barry O'Neill;Bezalel Peleg	2008	Games and Economic Behavior	10.1016/j.geb.2007.06.005	bondareva–shapley theorem;non-cooperative game;combinatorial game theory;bayesian game;game theory;example of a game without a value;voting;game tree;extensive-form game;simultaneous game;win-win game;information set;metagaming;repeated game;mathematics;stochastic game;screening game;normal-form game;mathematical economics;sequential game;welfare economics;symmetric game	AI	-6.054008963504887	-2.580633296389229	189666
92fc27a64d72818067505c8f1d9e0df62fed8ff4	diffusion, seeding, and the value of network information		Identifying the optimal set of individuals to first receive information (`seeds') in a social network is a widely-studied question in many settings, such as the diffusion of information, microfinance programs, and new technologies. Numerous studies have proposed various network-centrality based heuristics to choose seeds in a way that is likely to boost diffusion. Here we show that, for some frequently studied diffusion processes, randomly seeding S + x individuals can prompt a larger cascade than optimally targeting the best S individuals, for a small x. We prove our results for large classes of random networks, but also show that they hold in simulations over several real-world networks. This suggests that the returns to collecting and analyzing network information to identify the optimal seeds may not be economically significant. Given these findings, practitioners interested in communicating a message to a large number of people may wish to compare the cost of network-based targeting to that of slightly expanding initial outreach.	centrality;flow network;heuristic (computer science);optimal control;randomness;simulation;social network	Mohammad Akbarpour;Suraj Malladi;Amin Saberi	2018		10.1145/3219166.3219225	artificial intelligence;computer science;seeding;machine learning;outreach;word of mouth;emerging technologies;heuristics;social network;microfinance	Metrics	-10.464001716996254	-4.745770383334804	189718
165be43bd24e688cc8ba4ef36edefaf71397e433	coalition formation with uncertain heterogeneous information	service provider;rfp;coalition formation;incomplete information;heterogeneous information;common knowledge;request for proposal;experimental evaluation;experimental measurement;experimentation;task allocation;time constraint	Coalition formation methods allow agents to join together and are thus necessary in cases where tasks can only be performed cooperatively by groups. This is the case in the Request For Proposal (RFP) domain, where some requester business agent issues an RFP - a complex task comprised of sub-tasks - and several service provider agents need to join together to address this RFP. In such environments the value of the RFP may be common knowledge, however the costs that an agent incurs for performing a specific sub-task are unknown to other agents. Additionally, time for addressing RFPs is limited. These constraints make it hard to apply traditional coalition formation mechanisms, since those assume complete information, and time constraints are of lesser significance there.To address this problem, we have developed a protocol that enables agents to negotiate and form coalitions, and provide them with simple heuristics for choosing coalition partners. The protocol and the heuristics allow the agents to form coalitions in the face of time constraints and incomplete information. The overall payoff of agents using our heuristics is very close to an experimentally measured optimal value, as our extensive experimental evaluation shows.	experiment;fairness measure;heuristic (computer science);marginal model;mike lesser;optimization problem;request for proposal	Sarit Kraus;Onn Shehory;Gilad Taase	2003		10.1145/860575.860577	simulation;request for proposal;artificial intelligence;computer security	AI	-9.216134133852421	-6.982199513146457	189759
3ce13fe1cb3852fffc52b10bb58e0e5eab58ba65	on the possibilistic-based decision model: characterization of preference relations under partial inconsistency	decision models;case based decision;utility function;decision problem;decision under uncertainty;expected utility theory;possibility theory;partial inconsistency;possibility distribution	A qualitative counterpart to Von Neumann and Morgenstern's Expected Utility Theory of decision under uncertainty was recently proposed by Dubois and Prade. In this model, belief states are represented by normalised possibility distributions over an ordinal scale of plausibility, and the utility (or preference) of consequences of decisions are also measured in an ordinal scale. In this paper we extend the original Dubois and Prade's decision model to cope with partially inconsistent descriptions of belief states, represented by non-normalised possibility distributions. Subnormal possibility distributions frequently arise when adopting the possibilistic model for case-based decision problems. We consider two qualitative utility functions, formally similar to the original ones up to modifying factors coping with the inconsistency degree of belief states. We provide axiomatic characterizations of the preference orderings induced by these utility functions.	axiomatic system;decision problem;expected utility hypothesis;game-maker;level of measurement;ordinal data;plausibility structure;t-norm	Lluis Godo;Adriana Zapico	2001	Applied Intelligence	10.1023/A:1011203021670	possibility theory;decision model;optimal decision;decision theory;two-moment decision model;expected utility hypothesis;computer science;artificial intelligence;decision tree;decision problem;decision rule;subjective expected utility;evidential reasoning approach;von neumann–morgenstern utility theorem	AI	-9.49383230107749	-0.1785948671474481	190124
da99f2868bca335397fbc2e47adbd03a8afae8ef	hurford conditionals		Compare the following conditionals: Bad #If John is not in Paris, he is in France. Good If John is in France, he is not in Paris. Good sounds entirely natural, whereas Bad sounds quite strange. This contrast is puzzling, because Bad and Good have the same structure at a certain level of logical abstraction:	john collison	Matthew Mandelkern;Jacopo Romoli	2018	J. Semantics	10.1093/jos/ffx022		PL	-11.587245087319	4.10765780762089	190172
3e44b525f6fa30bd5335e9a43af1ed6e987859f3	inference in bayesian networks	bayesian network	In rare cases, primarily involving terminological information or other artificially constructed domains, one has the opportunity to determine categorically the truth of a proposition based on prior knowledge and current observation. Often, truth is more elusive, and categorical statements can only be made by judgment of the likelihood or other ordinal attribute of competing propositions. Probability theory is the oldest and best-understood theory for representing and reasoning about such situations, but early AI experimental efforts at applying probability theory were disappointing and only confirmed a belief among AI researchers that those who worried about numbers were “missing the point.”1 The point, so succinctly stated in Newell and Simon’s physical symbol system hypothesis,2 was that structure was the key, not the numeric details. The problem: The core around which a probabilistic approach revolves is the joint-probability distribution (JPD). Unfortunately, for domains described by a set of discrete parameters, the size of this object and the complexity of reasoning with it directly can both be exponential in the number of parameters. A popular simplification was the naïve Bayes’s model. This model assumes that the probability distribution for each observable parameter (that is, the probability of each value in the domain of the parameter) depends only on the root cause and not on the other parameters. Simplifying assumptions such as naïve Bayes’s permitted tractable reasoning but were too extreme: They again provided no mechanism for representing the qualitative structure of a domain. About 10 years ago, probability, and especially decision theory, began to attract renewed interest within the AI community, which was the result of a felicitous combination of obstacle and opportunity: The issue of ordering possible beliefs, both for belief revision and for action selection, was seen as increasingly important and problematic, and at the same time, dramatic new developments in computational probability and decision theory directly addressed perceived shortcomings. The key development was the discovery that a relationship could be established between a welldefined notion of conditional independence in probability theory and the absence of arcs in a directed acyclic graph (DAG). This relationship made it possible to express much of the structural information in a domain independently of the detailed numeric information, in a way that both simplifies knowledge acquisition and reduces the computational complexity of reasoning. The resulting graphic models have come to be known as Bayesian networks. An example Bayesian network is shown in figure 1. I shortly examine the formal semantics of the graph, but intuitively, it is a representation of the following model: Sneezing can be “caused by” cold or an allergic reaction,3 allergic reaction can be “caused by” the presence of a cat, and furniture scratches can be “caused by” a cat. For reasons we see later, a Bayes’s net must be a DAG. Still, we could add several more arcs without violating the acyclicity requirement. The missing arcs encode information: The chance I have a cold is unrelated to the presence of a cat, colds don’t cause furniture scratches, cat’s don’t cause sneezing except through initiation of an allergic reaction, and sneezing and scratches are unrelated except through their shared possible causes. We have described the structure of the model Articles	action selection;bayesian network;belief revision;cobham's thesis;computation;computational complexity theory;decision theory;directed acyclic graph;encode;knowledge acquisition;level of detail;naive bayes classifier;naivety;observable;ordinal data;physical symbol system;semantics (computer science);time complexity	Bruce D'Ambrosio	1999	AI Magazine		biological network inference;variable-order bayesian network;frequentist inference;bayesian programming;causal markov condition;bayesian network;graphical model;bayesian hierarchical modeling;bayesian statistics;dynamic bayesian network	AI	-16.413434175857347	0.8874866606418711	190212
772dc82592abd0a503ee07384640a5dbe93ddc32	aggregating conditionally lexicographic preferences using answer set programming solvers	answer set programming;conditionally lexicographic preferences;social choice theory;positional scoring voting rules	We consider voting over combinatorial domains, where alternatives are binary tuples. We assume that votes are specified as conditionally lexicographic preferences, or LP trees. We study aggregation problems of LP tree votes for several positional scoring rules. Our main goal is to demonstrate that answer-set programming can be effective in solving the winner and the evaluation problems for instances of practical sizes.	answer set programming;lexicographic preferences;lexicographical order;stable model semantics	Xudong Liu;Miroslaw Truszczynski	2013		10.1007/978-3-642-41575-3_19	mathematical optimization;discrete mathematics;social choice theory;computer science;answer set programming;mathematics	AI	-8.353221511525458	2.5892314779198418	190213
ea5f86539ef6796b704aaf8cd7b8b4bace6ebb10	a priori knowledge in bolzano, conceptual truths, and judgements		According to Kant, a true judgement can be called a priori in case it can take place absolutely (schlechterdings) independent of experience. Propositions that are knowable in this way are called a priori propositions by him (Kant 1787 B, 3–4). As is well known, the class of those a priori propositions that are synthetic was particularly important for Kant. In contrast to analytic propositions, they are supposed to contain nontrivial information about the world and yet be irrefutable by experience. Not many of his critics were satisfied with Kant’s way of drawing this distinction. Peter Strawson, for example, writes in his commentary on the Critique of Pure Reason:		Stefan Roski	2013		10.1007/978-94-007-5137-8_8	psychology;aesthetics;epistemology;social psychology	NLP	-12.96268097451192	3.6630795799867903	190579
51e690f74fdb53333202818093e6564d4193fed9	information integration via hierarchical and hybrid bayesian networks	belief networks;bayesian network;software tool;counterterrorism;hidden markov model;safety systems;information integration bayesian networks counterterrorism decision making hidden markov models;information analysis belief networks hidden markov models adaptive systems monitoring terrorism decision making safety systems;information integration;markov model;hidden markov models;safety analysis;monitoring;adaptive systems;indexation;bayesian methods hidden markov models terrorism decision making random variables information analysis information filters data mining collaboration software tools;random variable;decision making information integration hierarchical bayesian network hybrid bayesian networks hidden markov models adaptive safety analysis safety monitoring system terrorist attack scenario;information analysis;terrorism;open source;bayesian networks	"""A collaboration scheme for information integration among multiple agencies (and/or various divisions within a single agency) is designed using hierarchical and hybrid Bayesian networks (HHBNs). In this scheme, raw information is represented by transactions (e.g., communication, travel, and financing) and information entities to be integrated are modeled as random variables (e.g., an event occurs, an effect exists, or an action is undertaken). Each random variable has certain states with probabilities assigned to them. Hierarchical is in terms of the model structure and hybrid stems from our usage of both general Bayesian networks (BNs) and hidden Markov models (HMMs, a special form of dynamic BNs). The general BNs are adopted in the top (decision) layer to address global assessment for a specific question (e.g., """"Is target A under terrorist threat?"""" in the context of counterterrorism). HMMs function in the bottom (observation) layer to report processed evidence to the upper layer BN based on the local information available to a particular agency or a division. A software tool, termed the adaptive safety analysis and monitoring (ASAM) system, is developed to implement HHBNs for information integration either in a centralized or in a distributed fashion. As an example, a terrorist attack scenario gleaned from open sources is modeled and analyzed to illustrate the functionality of the proposed framework."""	bayesian network;centralized computing;entity;hidden markov model;markov chain;programming tool	Haiying Tu;Jefferey Allanach;Satnam Singh;Krishna R. Pattipati;Peter Willett	2006	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2005.859180	computer science;artificial intelligence;data science;machine learning;bayesian network;data mining;hidden markov model;statistics	AI	-18.034694800843404	-4.91133001848117	190789
dab8c48c38845bb22d258146e6d25646cb084a68	a basic firewall configuration strategy for the protection of development-related computer networks and subnetworks	computer network;industrial relations;source code	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;firewall (computing);primary source	Sujan Maskey;Brittany Jansen;Dennis Guster;Charles Hall	2007	Information Systems Security	10.1080/10658980701744853	application firewall;simulation;computer science;artificial intelligence;industrial relations;computer security;source code	Mobile	-15.357398789616948	-5.40022392577722	191104
4f60db57655e19c79818083379ba3d28470898ce	coalition formation strategies for multiagent hedonic games	proposal acceptance strategy;protocols;multiagent system;probability;history;proposals games context protocols multiagent systems waste materials history;waste materials;satisfying utility;hedonic games multiagent systems coalition formation;probability multi agent systems;proposal selection strategy;satisfiability;coalition formation;strategic behavior;multi agent systems;autonomous agent;games;coordination method;multiagent hedonic games;coalition formation process;autonomous agents;coalition formation process coalition formation strategy multiagent hedonic games multiagent system coordination method autonomous agents strategical behaviors proposal acceptance strategy proposal selection strategy probability satisfying utility;coalition formation strategy;proposals;strategical behaviors;context;hedonic games;multiagent systems	In a multiagent system, coalition formation is a coordination method for agents aiming to form groups of interest. In this paper, we focus on the particular context of hedonic games. In hedonic games, the objective of the agents is to form coalitions, which are groups of agents. The satisfaction of an agent depends on other members of its coalition. In this context, autonomous agents need strategical behaviors to efficiently form their coalitions. In this article, we describe two types of strategies which could be used by agents: proposal acceptance strategies, used by agents to decide to join a coalition and proposal selection strategies, based on the analysis of the history of a negotiation, used by agents to select interesting coalitions to propose to other agents. Then we present our experiments and discuss the results we have obtained. We underline that a compromise between high and low selectivity allows agents to obtain a higher probability to form coalitions with a satisfying utility. Our proposal selection strategies allow agents to reduce the number of proposals to send during the coalition formation process without losing much utility. This speeds up considerably the process.	agent-based model;autonomous agent;autonomous robot;experiment;game theory;multi-agent system;selectivity (electronic)	Thomas Génin;Samir Aknine	2010	2010 22nd IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2010.72	simulation;computer science;knowledge management;artificial intelligence;autonomous agent;multi-agent system;management science	AI	-8.898529411963638	-7.319290960716563	191131
c0513ac138f70921f3d664c93671a1edc0f219d2	competitive machine learning: best theoretical prediction vs optimization		Machine learning is often used in competitive scenarios: Participants learn and fit static models, and those models compete in a shared platform. The common assumption is that in order to win a competition one has to have the best predictive model, i.e., the model with the smallest out-sample error. Is that necessarily true? Does the best theoretical predictive model for a target always yield the best reward in a competition? If not, can one take the best model and purposefully change it into a theoretically inferior model which in practice results in a higher competitive edge? How does that modification look like? And finally, if all participants modify their prediction models towards the best practical performance, who benefits the most? players with inferior models, or those with theoretical superiority? The main theme of this paper is to raise these important questions and propose a theoretical model to answer them. We consider a study case where two linear predictive models compete over a shared target. The model with the closest estimate gets the whole reward, which is equal to the absolute value of the target. We characterize the reward function of each model, and using a basic game theoretic approach, demonstrate that the inferior competitor can significantly improve his performance by choosing optimal model coefficients that are different from the best theoretical prediction. This is a preliminary study that emphasizes the fact that in many applications where predictive machine learning is at the service of competition, much can be gained from practical (back-testing) optimization of the model compared to static prediction improvement.		Amin Khajehnejad;Shima Hajimirza	2018	CoRR		machine learning;competitive advantage;mathematical optimization;predictive modelling;artificial intelligence;mathematics;absolute value	ML	-10.93381182690862	-7.613881258733238	191210
9b13ae05d670a3e809c6032921ac8c1cab14a892	is proximity preservation rational in social choice theory?	social choice;social welfare function;social choice theory	We establish a strong impossibility theorem of a rational social choice that the proximity preservation (also called weak proximorphismWPX) and the diagonal surjectivity are logically inconsistent. The result is valid for finite or infinite alternatives, discrete or continuous. It generalizes the Baigent theorem, largely weakening his antecedent. For continuum set of alternatives, we clarify the notion of WPX by showing (1) WPX almost implies the continuity, (2)WPX is almost rigid. These observations raise the issue whetherWPX is a natural condition for a social welfare function. A splitting reformulation of the proximity preservation which is weaker but rational is suggested.	scott continuity;triune continuum paradigm	Wu-Hsiung Huang	2004	Social Choice and Welfare	10.1007/s00355-003-0224-z	social choice theory;economics;mathematics;microeconomics;mathematical economics;welfare economics	ECom	-7.488382009138791	-0.8174247320490847	191276
d197de4f7e14fbfc0b8f1f806106fd442b5fca8e	graphic solution of a linear transformation cipher	linear transformation	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	cipher;francis;primary source	Greg E. Mellen	1981	Cryptologia	10.1080/0161-118191855733	discrete mathematics;computer science;theoretical computer science;mathematics;linear map;affine cipher;algebra	Robotics	-14.94021775527601	-5.581446542025639	191520
f7797a04735231c629c824e7964b90477f8c72f9	cooperative computer mediated working		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Robert W. Lawler	1992	Intelligent Tutoring Media	10.1080/14626269209408319	multimedia;computer science;human–computer interaction	Robotics	-15.51815398519635	-5.9019774449002265	191647
ad31cd2cf720b6aa17375d8e1676e8ae22e83bbb	model for renegotiation in infrastructure projects with government guarantee	analytical models;game theory;government guarantee;finance;procurement;renegotiation;government;government games contracts analytical models game theory procurement economics;contracts;public private partnership government guarantee dynamic game theory government renegotiation model ppp model management;government finance game theory;games;economics;infrastructure projects;infrastructure projects government guarantee renegotiation	This paper aims to study how to renegotiate with the government guarantee again, and the impact of the government guarantee to negotiation. We use dynamic game theory to describe on the government renegotiation model, give the equilibrium path to the renegotiation and gives the corresponding results. The research in this paper can effectively increase the PPP model management, to provide the theoretical basis, and can also to provide framework and method to understand the behavior of both parties.	game theory	Li Gong;Jin Tian	2012	2012 Fifth International Conference on Business Intelligence and Financial Engineering	10.1109/BIFE.2012.140	public economics;microeconomics;business	SE	-7.087622736500583	-7.449510125947022	192087
90481d6a605a994e979312d86e0c9b6aa5b04413	the newcomb problem: an unqualified resolution	probability;newcomb s paradox;deliberation;prisoner s dilemma;rationnel;rational;conditional expectation;causalite;probabilidad;probabilite;decision;evidence;paradoxe du prisonnier;paradoxe de newcomb;causality;causalidad	The Newcomb problem is analysed here as a type ofcommon cause problem. Inrelation to such problems, if you take the dominatedoption your expected outcomewill be good and if you take the dominant optionyour expected outcome will be notso good. As is explained, however, these arenot conventional conditional expectedoutcomes but `conditional evidence expectedoutcomes' and while in the deliberationprocess, the evidence on which they are based isonly hypothetical evidence.Conventional conditional expected outcomes aremore sensitive to your currentepistemic state in that they are based purely onactual evidence which is available toyou during the deliberation process. So althoughthey are conditional on a certain actbeing performed, they are not based on evidencethat you would have only if that actis performed. Moreover, for any given epistemicstate during the deliberationprocess, your conventional conditional expectedoutcome for the dominant option willbe better than that for the dominated option. Theprinciple of dominance is thus inperfect harmony with the conventional conditionalexpected outcomes. In relation tothe Newcomb problem then, the evidence unequivocallysupports two-boxing as therational option. Yet what is advanced here isnot simply a two-boxing strategy. Tosee why, two stages to the problem need to berecognised. The first stage is thatwhich occurs before the information used by thepredictor in making his predictionshas been gained. The second stage is after thispoint. Provided that you are still inthe first stage, you have an opportunity toinfluence whether or not the predictorplaces the $1m in the opaque box. To maximisethe probability that it is, you need tocommit yourself to one-boxing.	boxing	Simon Burgess	2004	Synthese	10.1023/B:SYNT.0000013243.57433.e7	causality;philosophy;rationality;epistemology;conditional expectation;probability;mathematics;mathematical economics;prisoner's dilemma;statistics	ML	-9.380292294642203	-2.0216436366461714	192095
53016e434d6c34fd9f309f66479908dd16781caa	the scholarship assignment problem	assignment problem;nash equilibrium;journal of economic literature;common knowledge;article;graduate student	There are n graduate students and n faculty members. Each student will be assigned a scholarship by the joint faculty. The socially optimal outcome is that the best student should get the most prestigious scholarship, the second-best student should get the second most prestigious scholarship, and so on. The socially optimal outcome is common knowledge among all faculty members. Each professor wants one particular student to get the most prestigious scholarship and wants the remaining scholarships to be assigned according to the socially optimal outcome. We consider the problem of finding a mechanism such that in equilibrium, all scholarships are assigned according to the socially optimal outcome. Journal of Economic Literature Classification Numbers: D70, D78.  2002 Elsevier Science	assignment problem;philips 68070	Pablo Amorós;Luis C. Corchón;Bernardo Moreno	2002	Games and Economic Behavior	10.1006/game.2001.0852	economics;assignment problem;microeconomics;management;common knowledge;nash equilibrium	ECom	-5.880912879577394	-4.425171883493505	192200
3ea5583c422911585c1ea49b0e6b17b1ca6ad509	examining spillovers between long and short repeated prisoner’s dilemma games played in the laboratory	prisoner’s dilemma;cooperation;dictator game;learning;repeated games;spillovers	We had participants play two sets of repeated Prisoner's Dilemma (RPD) games, one with a large continuation probability and the other with a small continuation probability, as well as Dictator Games (DGs) before and after the RPDs. We find that, regardless of which is RPD set is played first, participants typically cooperate when the continuation probability is large and defect when the continuation probability is small. However, there is an asymmetry in behavior when transitioning from one continuation probability to the other. When switching from large to small, transient higher levels of cooperation are observed in the early games of the small continuation set. Conversely, when switching from small to large, cooperation is immediately high in the first game of the large continuation set. We also observe that response times increase when transitioning between sets of RPDs, except for altruistic participants transitioning into the set of RPDs with long continuation probabilities. These asymmetries suggest a bias in favor of cooperation. Finally, we examine the link between altruism and RPD play. We find that small continuation probability RPD play is correlated with giving in DGs played before and after the RPDs, whereas high continuation probability RPD play is not.	continuation;income;interaction;no-communication theorem;preparation;prisoner's dilemma;probability;random number generation;randomness;software bug;unit;video games	Antonio A. Arechar;Maryam Kouchaki;David G. Rand	2018		10.3390/g9010005	dilemma;continuation;microeconomics;mathematical economics;altruism;prisoner's dilemma;dictator;dictator game;social psychology;repeated game;psychology	ECom	-5.343715473979789	-5.605142295295727	192699
cf757ae48767360bb02083e57252427079038406	network architecture and traffic flows: experiments on the pigou-knight-downs and braess paradoxes	braess paradox;reinforcement learning;traffic flow;route choice;network architecture	This paper presents theory and experiments to investigate how network architecture influences route-choice behavior. We consider changes to networks that, theoretically, exhibit the PigouKnight-Downs and Braess Paradoxes. We show that these paradoxes are specific examples of more general classes of network change properties that we term the “least congestible route” and “size” principles, respectively. We find that technical improvements to networks induce adjustments in traffic flows. In the case of network changes based on the Pigou-Knight-Downs Paradox, these adjustments undermine short-term payoff improvements. In the case of network changes based on the Braess Paradox, these adjustments reinforce the counter-intuitive, but theoretically predicted, effect of reducing payoffs to network users. Although aggregate traffic flows are close to equilibrium levels, we see some systematic deviations from equilibrium. We show that the qualitative features of these discrepancies can be accounted for by a simple reinforcement learning model. Acknowledgements We are grateful for the extremely helpful comments of three anonymous referees as well as the Action Editor. We also thank seminar participants at the University of Leicester, the Institute for Transport Studies, University of Leeds, and at the Amsterdam 2004 International Meeting of the Economic Science Association for their comments. a. Haas School of Business and Department of Economics, University of California, Berkeley, CA 94720-1900. email: morgan@haas.berkeley.edu b. School of Economics, University of Nottingham, Nottingham, NG7 2RD, United Kingdom. e-mail: henrik.orzen@nottingham.ac.uk c. School of Economics, University of Nottingham, Nottingham, NG7 2RD, United Kingdom. e-mail: martin.sefton@nottingham.ac.uk	aggregate data;epr paradox;email;experiment;moravec's paradox;network architecture;precedence effect;reinforcement learning;traffic exchange	John Morgan;Henrik Orzen;Martin Sefton	2009	Games and Economic Behavior	10.1016/j.geb.2008.04.012	network architecture;traffic flow;mathematical economics;reinforcement learning	ECom	-5.441175504594416	1.9123683772787967	192728
0bf509916f1a1bfa09a3216f1d1d6dfe27b7953a	a comparative evaluation of user preferences for extra-user interfaces	institutional repositories;fedora;vital;vtls;ils	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	emoticon;francis;nl (complexity);numerical aperture;primary source;qualitative comparative analysis;user (computing);user experience;user interface	Jérémie Melchior;Jean Vanderdonckt;Peter Van Roy	2012	Int. J. Hum. Comput. Interaction	10.1080/10447318.2012.715544	user interface design;simulation;human–computer interaction;computer science;user interface;world wide web	Robotics	-16.016483478052557	-5.92777221184233	192735
22456cfc5175581b548dfaba650963cd651764e0	narrow-scoping for wide-scopers	humanidades;filosofia etica	Many philosophers think that requirements of rationality are “wide-scope”. That is to say: they are requirements to make true some material conditional, such that one counts as satisfying the requirement iff one either makes the conditional’s antecedent false or makes its consequent true. These contrast with narrow-scope requirements, where the ‘requires’ operator takes scope only over the consequent of the conditional. Many of the philosophers who have preferred wide-scope requirements to narrow-scope requirements have also endorsed a corresponding semantic claim, namely that ordinary talk about rationality, despite appearances to the contrary, expresses wide-scope claims. In doing so, they seek to avoid attributing massive error to ordinary speakers. However, it is becoming increasingly clear that the wide-scope semantics inadequately captures the meaning of ordinary talk about rationality. It seems, then, that we are left with a dilemma: either give up the view that requirements of rationality are wide-scope, or accept an implausible semantics for ordinary talk about rationality, or attribute massive error to speakers. In this paper, I argue that this dilemma is only apparent, since we can appeal to a standard kind of contextualist semantics for modals to explain why narrow-scope talk comes out true in virtue of the wide-scope requirements. My view, then, combines wide-scoping about the explanatorily fundamental requirements of rationality with a contextualist variant of a narrow-scope semantics. I argue that this view gives us the best of both worlds, as well as solving related puzzles and challenges for the extant views in the literature.	pc bruno;rationality;requirement;scope (computer science);user (computing)	Alex Worsnip	2015	Synthese	10.1007/s11229-015-0681-9	ecological rationality;philosophy;epistemology;mathematics;algorithm	AI	-12.986006861453324	3.778900551459871	192995
ab86a7037d768d919358e6b3143b83375e8ccf46	superreplication under model uncertainty in discrete time	91b25;93e20;superreplication;nondominated model;hahn banach theorem;60g42;martingale measure;knightian uncertainty;medial limit	We study the superreplication of contingent claims under model uncertainty in discrete time. We show that optimal superreplicating strategies exist in a general measure-theoretic setting; moreover, we characterize the minimal superreplication price as the supremum over all continuous linear pricing functionals on a suitable Banach space. The main ingredient is a closedness result for the set of claims which can be superreplicated from zero capital; its proof crucially relies on medial limits.	contingency (philosophy);medial graph;theory	Marcel Nutz	2014	Finance and Stochastics	10.1007/s00780-014-0238-7	mathematical optimization;mathematical analysis;discrete mathematics;hahn–banach theorem;mathematics	Theory	-7.3007343585096525	-0.5447651894238007	193374
0c23f3297eae1a7c8281a36940eead5aff5a55bd	argumentation and persuasion in the cognitive coherence theory: preliminary report	generic model;agent communication;conference paper;computer software and services not elsewhere classified;social psychology;other artificial intelligence	This paper presents a coherentist approach to argumentation that extends previous proposals on cognitive coherence based agent communication pragmatics (inspired from social psychology) and propose (1) an alternative view on argumentation that is (2) part of a more general model of communication. In this approach, the cognitive aspects associated to both the production, the evaluation and the integration of arguments are driven by calculus on a formal characterization of cognitive coherence.	agent architecture;cognition;cognitive science;computation	Philippe Pasquier;Iyad Rahwan;Frank Dignum;Liz Sonenberg	2006		10.1007/978-3-540-75526-5_12	computer science;knowledge management;artificial intelligence;social psychology;artificial psychology	AI	-16.573453557126896	3.4920410888304008	193528
2d858d7b49ce2551bfe91964de20a6ddce7e4942	failure handling in a dialogue system	dialogue system	Failure can be defined as a state that occurs after the violation of an expectation; a common example of a dialogue failure would be a misunderstanding. Our first observation is that dialogue failure can be just as complex as any other sort of failure. Some dialogue failures require sophisticated analytic mechanisms to isolate their causes, and in some cases even detecting a failure will require substantial effort. In addition, dialogue failure introduces some complexities of its own. For example, it is necessary to distinguish between failures occurring in dialogue and those merely reported there. On the face of it this seems an obvious enough distinction, but in analyzing dialogue protocols there is a temptation to conflate the two. Many reports of non-dialogue failures are also dialogue failures since the reports violate some of the hearer's expectations about the course that the dialogue will take. (The report may come as an interruption of, say, a question-answer sequence.) One should not conclude from this, however, that all reports of external failure are necessarily dialogue failures as well; to do so would be to adopt an oversimplified model that will lead to confusion.	dialog system;dialog tree;interrupt;sensor;state (computer science)	Gretchen Brown	1977			computer science	NLP	-18.05711230480456	-1.0732419983924069	193624
890773682f58500deba6a1faa17a93b2f36265fb	rationality and indeterminate probabilities	keywords bayesianism;regularity;journal article;indeterminate probabilities;principal principle;interpretivism;bayesianism;chance;pasadena game;rationality	We argue that indeterminate probabilities are not only rationally permissible for a Bayesian agent, but they may even be rationally required. Our first argument begins by assuming a version of interpretivism: your mental state is the set of probability and utility functions that rationalize your behavioral dispositions as well as possible. This set may consist of multiple probability functions. Then according to interpretivism, this makes it the case that your credal state is indeterminate. Our second argument begins with our describing a world that plausibly has indeterminate chances. Rationality requires a certain alignment of your credences with corresponding hypotheses about the chances. Thus, if you hypothesize the chances to be indeterminate, your will inherit their indeterminacy in your corresponding credences. Our third argument is motivated by a dilemma. Epistemic rationality requires you to stay open-minded about contingent matters about which your evidence has not definitively legislated. Practical rationality requires you to be able to act decisively at least sometimes. These requirements can conflict with each other-for thanks to your open-mindedness, some of your options may have undefined expected utility, and if you are choosing among them, decision theory has no advice to give you. Such an option is playing Nover and Hájek’s Pasadena Game, and indeed any option for which there is a positive probability of playing the Pasadena Game. You can serve both masters, epistemic rationality and practical rationality, with an indeterminate credence to the prospect of playing the Pasadena game. You serve epistemic rationality by making your upper probability positive-it ensures that you are open-minded. You serve practical rationality by making your lower probability 0-it provides guidance to your decision-making. No sharp credence could do both.	bayesian network;contingency (philosophy);decision problem;decision theory;expected utility hypothesis;indeterminacy in concurrent computation;mental state;nondeterministic algorithm;philosophy of mind;rational agent;rationality;requirement;statically indeterminate;undefined behavior	Alan Hájek;Michael Smithson	2011	Synthese	10.1007/s11229-011-0033-3	philosophy;rationality;epistemology;bayesian probability;mathematics;mathematical economics	AI	-9.003650186806704	-2.7701352985153496	193708
eba269e85cf67ccf93545599d0e99d1c95d19e5e	enhanced qualitative probabilistic networks for resolving trade-offs	modelizacion;distributed system;systeme reparti;detail level;abstraction;programmation stochastique;intelligence artificielle;aprendizaje probabilidades;raisonnement qualitatif;probabilistic approach;abstraccion;niveau detail;modelisation;sistema repartido;enfoque probabilista;approche probabiliste;inferencia;apprentissage probabilites;trade off resolution;artificial intelligence;razonamiento calitativo;inteligencia artificial;qualitative reasoning;information system;probabilistic logic;scientific;stochastic programming;logique probabiliste;modeling;programacion estocastica;systeme information;inference;probability learning;probabilistic reasoning;probabilistic network;sistema informacion;nivel detalle	Qualitative probabilistic networks were designed to overcome, to at least some extent, the quantification problem known to probabilistic networks. Qualitative networks abstract from the numerical probabilities of their quantitative counterparts by using signs to summarise the probabilistic influences between their variables. One of the major drawbacks of these qualitative abstractions, however, is the coarse level of representation detail that does not provide for indicating strengths of influences. As a result, the trade-offs modelled in a network often remain unresolved upon inference. We present an enhanced formalism of qualitative probabilistic networks to provide for a finer level of representation detail. An enhanced qualitative probabilistic network differs from a basic qualitative network in that it distinguishes between strong and weak influences. Now, if a strong influence is combined, upon inference, with a conflicting weak influence, the sign of the net influence may be readily determined. Enhanced qualitative networks are purely qualitative in nature, as basic qualitative networks are, yet allow for resolving more trade-offs upon inference.	ambiguous grammar;apollonian network;causal filter;computation;directed graph;expressive power (computer science);genetic algorithm;hans l. bodlaender;hans moravec;interval propagation;linked list;mental representation;numerical analysis;real life;robustness (computer science);semantics (computer science);sign extension;signed zero;software propagation;stepwise regression;subject-matter expert;vertex-transitive graph;visual basic[.net]	Silja Renooij;Linda C. van der Gaag	2008	Artif. Intell.	10.1016/j.artint.2008.04.001	computer science;artificial intelligence;machine learning;probabilistic logic;algorithm	AI	-18.98093247548904	-0.7096132635203444	193757
e10fc970905e6e2ebfc384a11be7d6afdfbad037	liar liar, pants on fire; or how to use subjective logic and argumentation to evaluate information from untrustworthy sources	multi-agent systems;non-prioritized belief revision;car-to-car communication;information fusion	This paper presents a non-prioritized belief change operator, designed specifically for incorporating new information from many heterogeneous sources in an uncertain environment. We take into account that sources may be untrustworthy and provide a principled method for dealing with the reception of contradictory information. We specify a novel Data-Oriented Belief Revision Operator, that uses a trust model, subjective logic, and a preference-based argumentation framework to evaluate novel information and change the agent’s belief set accordingly. We apply this belief change operator in a collaborative traffic scenario, where we show that (1) some form of trust-based non-prioritized belief change operator is necessary, and (2) in a direct comparison between our operator and a previous proposition, our operator performs at least as well in all scenarios, and significantly better in some.	argumentation framework;belief revision;experiment;robustness (computer science);tempest (codename)	Andrew Koster;Ana L. C. Bazzan;Marcelo de Souza	2016	Artificial Intelligence Review	10.1007/s10462-016-9499-1	belief structure;artificial intelligence;machine learning;algorithm	AI	-17.911670971165385	3.1716756986186545	193797
22c873f4fe9e79e6a379cf6602df1e8026608fb8	graph monotonic values	myerson value;shapley value;community structure;solution concept	A disturbing feature of most of the solution concepts for TU games with incomplete communication is that payments of players may decrease when they activate a new link. That can be considered as a drawback which does not occur for the Myerson value (Math Oper Res 2:225–229, 1977) of superadditive games. The present article offers a new axiomatic characterization of the Myerson value. We show that the Myerson value is the unique solution for games with communication structures verifying a set of properties including monotonicity with respect to the graph and coinciding with the Shapley value when the communication is complete.		Gérard Hamiache	2011	Social Choice and Welfare	10.1007/s00355-010-0494-1	mathematical optimization;discrete mathematics;economics;mathematics;shapley value;mathematical economics;solution concept;community structure	ECom	-6.744359823540383	-2.1825859442217506	193912
00e9b33ad86b20cbfd6f7d27461451fdca8d8fe7	hans reichenbach's probability logic		Any attempt to characterize Reichenbach’s approach to inductive reasoning must take into account some of the core influences that set his work apart from more traditional or standard approaches to inductive reasoning. In the case of Reichenbach, these influences are particularly important as otherwise Reichenbach’s views may be confused with others that are closely related but different in important ways. The particular influences on Reichenbach also shift the strengths and weaknesses of his views to areas different from the strengths and weaknesses of other approaches, and from the point of some other approaches Reichenbach’s views would seem quite unintelligible if not for the particular perspective he has. Reichenbach’s account of inductive reasoning is fiercely empirical. More than perhaps any other account it takes its lessons from the empirical sciences. In Reichenbach’s view, an inductive logic cannot be built up entirely from logical principles independent of experience, but must develop out of the reasoning practiced and useful to the natural sciences. This might already seem like turning the whole project of an inductive logic on its head: We want an inductive inference system built on some solid principles (whatever they may be) to guide our scientific methodology. How could an inference procedure that draws on the methodologies of science supply in any way a normative foundation for an epistemology in the sciences? For Reichenbach there are two reasons for this “inverse” approach. We will briefly sketch them here, but return with more detail later in the text: First, Reichenbach was deeply influenced by Werner Heisenberg’s results, including the uncertainty principle, that called into question whether there is a fact to the matter – and consequently whether there can be certain knowledge about – the truth of propositions specifying a particular location and velocity for an object in space and time. If there necessarily always remains residual uncertainty for such propositions (which prior to Heisenberg seemed completely innocuous or at worst subject to epistemic limitations), then – according to Reichenbach – this is reason for more general caution about the goals of induction. Maybe the conclusions any inductive logic can aim for when applied to the sciences are significantly limited. Requiring soundness of an inference – preservation of truth with certainty – may not only	inductive reasoning;inference engine;mathematical induction;soundness (interactive proof);uncertainty principle;velocity (software development)	Frederick Eberhardt;Clark Glymour	2011		10.1016/B978-0-444-52936-7.50010-0	epistemology;artificial intelligence;mathematics;algorithm	AI	-13.090879490923513	2.238519876712159	193965
c367df8eb9ddaba7474957ca32c10ab7e6a25f5a	structure and significance of analogical reasoning	analogical reasoning	Abstract   Analogical reasoning is suggested as a useful new mechanism for the manipulation and derivation of some kinds of deep knowledge. The starting-point for the suggestion is the observation that experts often express parts of their non-shallow knowledge in terms of ‘cases’ and reason by trying to identify relevant similarities between past cases and a current problem. We describe case-based reasoning as a particular form of analogical reasoning, and set both types of reasoning within a new seven-stage model. An example of reasoning applied to similarities between a case of heart disease and the malfunctioning of a shower system is given and explained.	knowledge representation and reasoning	J. A. Campbell;John Wolstencroft	1990	Artificial Intelligence in Medicine	10.1016/0933-3657(90)90032-M	knowledge representation and reasoning;opportunistic reasoning;case-based reasoning;analytic reasoning;causal reasoning;qualitative reasoning;verbal reasoning;computer science;artificial intelligence;model-based reasoning;psychology of reasoning;reasoning system;deductive reasoning;algorithm	AI	-16.60569701292286	2.0015980070667236	194211
71ebb501d23f97d563cd380024f23cc275132132	exploiting fuzzy clustering and case-based reasoning for autonomic managers	wildfires case based reasoning decision making emergency management environmental science computing fuzzy set theory knowledge based systems learning artificial intelligence optimisation pattern clustering problem solving;knowledge based systems cognition monitoring clustering algorithms autonomic systems computational modeling machine learning algorithms;autonomic forest fire fuzzy clustering case based reasoning cbr autonomic manager self management algorithm machine learning decision making algorithm optimization knowledge base system problem solver	Designing efficient self-management algorithms for autonomic managers has been an ongoing and evolving research area. In literature, many machine learning paradigms have been proposed and exploited to make effective decisions in autonomic managers. This position paper proposes to optimize the decision making algorithm closer to the nature inspired decision making process. Core of the proposed framework is case-based reasoning which is an incremental learning mechanism for solving new system problems using the past experience. Fuzzy clustering has been proposed to maintain the knowledge-base of past problems in autonomic managers. New monitored problem in autonomic manager seeks fuzzy memberships amongst different clusters in the knowledge-base and problem solver in autonomic manager exploits existing solutions in different clusters with respect to their fuzzy membership value. It has been tested on a simulated case-study of Autonomic Forest Fire Application and up to 88% accurate predictions have been observed.	algorithm;autonomic computing;autonomic networking;case-based reasoning;cluster analysis;fuzzy clustering;machine learning;self-management (computer science);solver	Cynthia Khan;Malik Jahan Khan	2016	2016 IEEE International Conference on Autonomic Computing (ICAC)	10.1109/ICAC.2016.24	computer science;artificial intelligence;machine learning;data mining;database	Robotics	-15.70587110693106	-9.827891688824334	194341
accd78eabdce6669c24ecd87d58521f00cc8112a	on analysis of coalition formation with cooperative game theory and conflict analysis	game theory;benefit allocation rules coalition formation cooperative game theory conflict analysis;satisfiability;coalition formation;cooperative game theory;characteristic function;profitability	As an approach to examine coalition formation in a group, we propose a way to transform an expression of cooperation in the group with a game in characteristic function form into a model with which not only cooperation but also competition in the group can be analyzed. By using the way, once we have a game in characteristic function form with the set of all possible coalition structures and the rules of benefit allocation in possible coalitions, we can transform it into the model that can be analyzed in the framework of conflict analysis. Examining examples, we show that the grand coalition, that is, the coalition that all of the members in a group participate, is not always formed, even if it is satisfied that the bigger a coalition is, the bigger the profit of the coalition is.		Takehiro Inohara	2001		10.1109/ICSMC.2001.973086	non-cooperative game;game theory;characteristic function;win-win game;core;mathematical economics;statistics;profitability index;satisfiability	AI	-5.248502211756761	-3.262737550119986	194375
2b719f4320061f430d7c2606c738a81c85557889	spatial statistics in geographical information science: from interpolation to probabilistic robotics	remote sensing image;geographic information science;interpolation;spatial data quality;spatial data;robotics;image mining;image analysis;spatial statistics;object identification;field study	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;geographic information system;information science;interpolation;primary source;robotics;spatial analysis	Alfred Stein	2010	Annals of GIS	10.1080/19475683.2010.539986	computer vision;image analysis;geography;computer science;data science;machine learning;data mining;mathematics;spatial analysis;robotics;statistics	Robotics	-14.947588609194103	-6.315359123770756	194436
